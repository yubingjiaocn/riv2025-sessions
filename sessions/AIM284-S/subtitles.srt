1
00:00:00,660 --> 00:00:03,258
Hello, good morning, everyone. Welcome

2
00:00:03,258 --> 00:00:05,658
to day 4 of

3
00:00:05,658 --> 00:00:06,839
Reinvent. Um,

4
00:00:07,179 --> 00:00:09,419
so just a quick note, I know you guys saw the session

5
00:00:09,419 --> 00:00:11,569
title about building intelligent agents.

6
00:00:11,939 --> 00:00:14,060
Uh, the punch line is this is still the

7
00:00:14,060 --> 00:00:16,170
same session. It's just I put the

8
00:00:16,170 --> 00:00:17,879
answer like right there,

9
00:00:18,539 --> 00:00:19,239
um.

10
00:00:20,589 --> 00:00:22,589
So, uh, just to queue up

11
00:00:22,589 --> 00:00:25,568
really quick, this session is going to be a little bit more

12
00:00:25,699 --> 00:00:27,489
educational and less specific about

13
00:00:27,789 --> 00:00:29,370
how to apply Mongo DB.

14
00:00:29,839 --> 00:00:31,949
And voyage, uh, for building agents,

15
00:00:32,158 --> 00:00:34,359
I encourage you guys to come hit me up. I'll

16
00:00:34,359 --> 00:00:36,389
be outside the hall, uh, to answer any

17
00:00:36,389 --> 00:00:38,579
questions and also we have a team that's at the booth

18
00:00:38,880 --> 00:00:41,250
where we can do a more in-depth demo. But,

19
00:00:41,598 --> 00:00:43,598
uh, for this session we felt it was really important to

20
00:00:43,598 --> 00:00:45,918
honestly do a primer on agent memory

21
00:00:45,918 --> 00:00:48,118
because if you're looking at improving the quality

22
00:00:48,118 --> 00:00:49,259
of your agents,

23
00:00:49,598 --> 00:00:51,259
um, or rag pipelines,

24
00:00:51,719 --> 00:00:53,978
architecting and how you architect your agent memory

25
00:00:53,978 --> 00:00:55,740
is a really good place to go.

26
00:00:57,798 --> 00:00:59,918
Um, so we all

27
00:00:59,918 --> 00:01:01,418
recall, uh,

28
00:01:01,759 --> 00:01:03,319
this MIT report,

29
00:01:03,598 --> 00:01:05,799
uh, I believe, about how 95%

30
00:01:05,799 --> 00:01:07,838
of organizations are seeing zilch,

31
00:01:08,040 --> 00:01:08,959
right, value,

32
00:01:09,480 --> 00:01:10,189
um,

33
00:01:10,599 --> 00:01:13,079
and there's been a lot of reasons posited.

34
00:01:13,198 --> 00:01:15,329
I think all of us kind of feel this a little bit

35
00:01:15,329 --> 00:01:17,480
that we're still not quite at the

36
00:01:17,480 --> 00:01:20,000
point where we have a high conversion of

37
00:01:20,000 --> 00:01:21,939
like POCs to production ready

38
00:01:22,680 --> 00:01:23,859
apps, right,

39
00:01:24,319 --> 00:01:24,888
um.

40
00:01:25,808 --> 00:01:28,079
And this study was, you know,

41
00:01:28,418 --> 00:01:30,418
it had about 300, uh, samples

42
00:01:30,418 --> 00:01:31,260
that they looked at.

43
00:01:31,579 --> 00:01:33,719
They spanned across all different industries,

44
00:01:34,138 --> 00:01:36,219
um, and they found that it was pretty

45
00:01:36,219 --> 00:01:38,659
consistent in terms of the challenges

46
00:01:38,659 --> 00:01:39,659
that businesses had,

47
00:01:39,969 --> 00:01:40,579
um,

48
00:01:40,900 --> 00:01:42,599
but they also had a,

49
00:01:43,019 --> 00:01:43,638
uh,

50
00:01:43,980 --> 00:01:44,558
quote about

51
00:01:44,859 --> 00:01:46,819
the 5% who were successful.

52
00:01:47,680 --> 00:01:50,409
All right. Um,

53
00:01:50,569 --> 00:01:52,609
now, their claim was that, and they call

54
00:01:52,609 --> 00:01:53,629
this the dividing line,

55
00:01:54,290 --> 00:01:56,528
was that it wasn't really model

56
00:01:56,528 --> 00:01:58,569
quality, regulation or

57
00:01:58,569 --> 00:02:00,909
infrastructure. Uh, it was actually the design

58
00:02:00,909 --> 00:02:03,308
of these agentic workflows and systems.

59
00:02:04,439 --> 00:02:05,079
Um,

60
00:02:05,400 --> 00:02:07,558
something that we've all experienced, for example, is,

61
00:02:07,680 --> 00:02:10,278
you know, agents losing track of user goals,

62
00:02:10,599 --> 00:02:12,800
uh, they forget earlier details,

63
00:02:12,960 --> 00:02:15,300
uh, provide inconsistent answers,

64
00:02:15,719 --> 00:02:16,819
and repeat work.

65
00:02:17,649 --> 00:02:19,699
Uh, there was a fun paper from Carnegie

66
00:02:19,699 --> 00:02:22,159
Mellon called The Agent Company,

67
00:02:22,618 --> 00:02:25,250
uh, something like, something like that, and,

68
00:02:25,258 --> 00:02:27,338
uh, what they found was when they try

69
00:02:27,338 --> 00:02:29,379
to replace like human tasks with

70
00:02:29,379 --> 00:02:30,118
agents,

71
00:02:30,419 --> 00:02:31,679
uh, it ended up.

72
00:02:32,639 --> 00:02:34,159
Running the business to the ground,

73
00:02:34,520 --> 00:02:36,800
uh, these agents would lie,

74
00:02:37,038 --> 00:02:39,050
um, and they struggled with opening pop-up

75
00:02:39,050 --> 00:02:41,080
windows. So, in case

76
00:02:41,080 --> 00:02:43,159
anyone's wondering if your job is safe, if you can open a

77
00:02:43,159 --> 00:02:45,500
pop-up window, you are like so good,

78
00:02:45,599 --> 00:02:46,118
um.

79
00:02:47,110 --> 00:02:49,308
You know, so, but all these failure,

80
00:02:49,429 --> 00:02:51,460
failures, they're not reasoning failures necessarily.

81
00:02:51,550 --> 00:02:52,088
Um,

82
00:02:52,550 --> 00:02:54,729
we would argue that they are memory failures

83
00:02:55,389 --> 00:02:57,500
because at the end of the day, most agents today are

84
00:02:57,500 --> 00:02:59,849
kind of stateless systems pretending to be stateful.

85
00:03:01,969 --> 00:03:02,788
Um,

86
00:03:03,088 --> 00:03:05,129
now, there's a ton of really

87
00:03:05,129 --> 00:03:07,288
interesting papers that have been published. These are a few of

88
00:03:07,288 --> 00:03:07,830
my

89
00:03:08,240 --> 00:03:10,710
favorite, um, there's more, but

90
00:03:11,360 --> 00:03:13,278
if we take a look at this, um,

91
00:03:13,649 --> 00:03:15,189
you know, when we examine long

92
00:03:15,558 --> 00:03:17,689
horizon behavior, um, the data is

93
00:03:17,689 --> 00:03:19,808
pretty clear. Performance degrades

94
00:03:19,808 --> 00:03:22,669
significantly over multi-turn or multi-session interactions.

95
00:03:22,960 --> 00:03:25,250
Um, well, all these studies would show that

96
00:03:25,250 --> 00:03:27,368
anywhere between 20 to 40% accuracy drops

97
00:03:27,368 --> 00:03:28,569
in long tasks,

98
00:03:29,008 --> 00:03:31,210
uh, your retrieval quality decays over

99
00:03:31,210 --> 00:03:33,849
time. And these agents frequently,

100
00:03:33,939 --> 00:03:35,629
uh, contradict earlier inputs.

101
00:03:36,149 --> 00:03:38,389
So, real-world systems, you know, are consistently

102
00:03:38,389 --> 00:03:40,389
exposing the same pattern that without

103
00:03:40,389 --> 00:03:41,149
engineered memory,

104
00:03:41,419 --> 00:03:43,550
agent performance collapses as you add

105
00:03:43,550 --> 00:03:44,669
on more interactions.

106
00:03:47,770 --> 00:03:50,338
Go. Um,

107
00:03:50,758 --> 00:03:52,879
two of my favorite charts

108
00:03:52,879 --> 00:03:53,800
in particular,

109
00:03:54,159 --> 00:03:56,278
uh, when people ask, well, why is it important

110
00:03:56,278 --> 00:03:56,868
to,

111
00:03:57,240 --> 00:03:59,719
you know, think about like how do you architect,

112
00:03:59,879 --> 00:04:02,110
uh, engineer your context,

113
00:04:02,360 --> 00:04:04,338
and so on and so forth, uh, is the,

114
00:04:04,679 --> 00:04:06,939
uh, Lawson Mill paper and,

115
00:04:07,038 --> 00:04:08,599
uh, context brought from Chroma.

116
00:04:09,308 --> 00:04:11,508
Um, so, you know, independent research from

117
00:04:11,508 --> 00:04:12,788
companies like Microsoft,

118
00:04:13,088 --> 00:04:14,080
Stanford, Chroma

119
00:04:14,349 --> 00:04:16,699
showed that, uh, two, there's two insights, right?

120
00:04:16,988 --> 00:04:19,389
One is that elements get lost when sequences

121
00:04:19,389 --> 00:04:20,028
get long.

122
00:04:20,798 --> 00:04:21,540
Um

123
00:04:22,278 --> 00:04:23,899
And then they,

124
00:04:24,278 --> 00:04:26,559
you know, lose these like constraints and

125
00:04:26,559 --> 00:04:28,600
drift from the original post, right? Uh, so this

126
00:04:28,600 --> 00:04:30,678
happens even when the context window is large enough

127
00:04:30,678 --> 00:04:32,399
to contain the relevant information.

128
00:04:32,759 --> 00:04:34,879
Um, and the other aspect as well, it's where

129
00:04:34,879 --> 00:04:36,119
that information is located,

130
00:04:36,480 --> 00:04:38,588
right? Um, the position of that

131
00:04:38,588 --> 00:04:40,759
information matters in terms of how likely it is

132
00:04:40,759 --> 00:04:41,980
to actually be retrieved.

133
00:04:42,439 --> 00:04:44,809
Um, so the problem isn't just missing tokens,

134
00:04:45,079 --> 00:04:47,250
it's CLM's inability to maintain the stable

135
00:04:47,250 --> 00:04:49,519
state across time, and this is why context

136
00:04:49,519 --> 00:04:50,319
alone isn't memory.

137
00:04:53,290 --> 00:04:54,329
Um.

138
00:04:55,559 --> 00:04:58,250
Now the cost of poor memory engineering is

139
00:04:58,548 --> 00:05:01,028
these systems become expensive, unreliable,

140
00:05:01,108 --> 00:05:02,028
and unscalable. And we'll,

141
00:05:02,389 --> 00:05:04,730
I'll dig in a little bit deeper into

142
00:05:05,189 --> 00:05:07,338
why you can't just get a bigger LLM to

143
00:05:07,338 --> 00:05:08,290
kind of solve this problem.

144
00:05:09,569 --> 00:05:10,230
Um

145
00:05:10,949 --> 00:05:13,470
And you know there is a cost, right, because

146
00:05:13,470 --> 00:05:15,588
when users are using your product, whether

147
00:05:15,588 --> 00:05:17,689
it's internal users or external users,

148
00:05:18,048 --> 00:05:20,309
um, they eventually start kind of losing trust

149
00:05:20,309 --> 00:05:22,350
like getting that experience

150
00:05:22,350 --> 00:05:24,750
of your early, early POCs is really, really

151
00:05:24,750 --> 00:05:25,699
important, um,

152
00:05:26,928 --> 00:05:28,949
because eventually sometimes like you just don't get that second chance.

153
00:05:31,910 --> 00:05:32,559
Now let's,

154
00:05:32,829 --> 00:05:34,470
let's dig in a bit to.

155
00:05:35,290 --> 00:05:37,540
The MIT report where they said that

156
00:05:37,540 --> 00:05:38,230
uh

157
00:05:38,809 --> 00:05:41,048
the issue is really once again it goes back to

158
00:05:41,048 --> 00:05:42,199
most AI systems

159
00:05:42,488 --> 00:05:44,559
do not retain feedback. They will not adapt to

160
00:05:44,559 --> 00:05:47,108
context and uh

161
00:05:47,250 --> 00:05:49,290
they generally don't improve over time, which is why you

162
00:05:49,290 --> 00:05:50,028
have like

163
00:05:50,329 --> 00:05:52,528
2 like $20 200 dollars consumer

164
00:05:52,528 --> 00:05:54,588
apps are consistently beating out $200,000

165
00:05:54,588 --> 00:05:57,189
to $80,000 enterprise implementations.

166
00:05:57,769 --> 00:06:00,269
Some of these implications are even more expensive, um.

167
00:06:01,009 --> 00:06:03,129
You know, so that's why we kind of wanted

168
00:06:03,129 --> 00:06:04,889
to dig into this primer a little bit because,

169
00:06:05,209 --> 00:06:07,209
uh, I work with a lot of our customers. Um,

170
00:06:07,329 --> 00:06:09,649
some of them are more like cutting edge like

171
00:06:09,649 --> 00:06:10,790
AI native startups,

172
00:06:11,119 --> 00:06:13,028
uh, but a lot of them tend to be

173
00:06:13,410 --> 00:06:15,528
more enterprisy, um,

174
00:06:15,730 --> 00:06:18,069
customers who maybe have legacy systems.

175
00:06:18,488 --> 00:06:19,869
Uh, they're not AI native,

176
00:06:20,250 --> 00:06:21,988
but they would like to be AI fluent.

177
00:06:23,548 --> 00:06:25,949
And they tend to kind of circle

178
00:06:25,949 --> 00:06:27,988
around some of the same questions in terms of,

179
00:06:28,269 --> 00:06:30,108
well, can't we just use a bigger LM?

180
00:06:30,428 --> 00:06:31,040
Can't we just,

181
00:06:31,309 --> 00:06:32,809
you know, um,

182
00:06:33,309 --> 00:06:34,939
throw money at the problem. And it's like, no.

183
00:06:35,238 --> 00:06:37,350
Eventually you have to throw engineering, good

184
00:06:37,350 --> 00:06:38,350
engineering at the problem.

185
00:06:40,470 --> 00:06:42,910
Um, now, let me just

186
00:06:43,149 --> 00:06:44,488
sort of refresh

187
00:06:45,069 --> 00:06:47,829
everyone on what are the limitations of LMs

188
00:06:47,829 --> 00:06:49,670
and relying on them for everything.

189
00:06:51,209 --> 00:06:53,230
Um, and this is really important because once again,

190
00:06:53,488 --> 00:06:55,500
I do get questions about

191
00:06:55,500 --> 00:06:57,850
like the usage of LLMs as embedding

192
00:06:57,850 --> 00:06:59,988
models, things like that, and they are different.

193
00:07:00,649 --> 00:07:02,889
Um, but let's talk about LLMs, right, which are

194
00:07:02,889 --> 00:07:05,048
the core reasoning brain of most

195
00:07:05,048 --> 00:07:05,670
agents.

196
00:07:07,889 --> 00:07:08,928
Uh, so there's

197
00:07:09,250 --> 00:07:09,949
3

198
00:07:10,369 --> 00:07:12,649
limitations of LMs.

199
00:07:12,970 --> 00:07:15,009
Um, so LMs are powerful reasoners,

200
00:07:15,119 --> 00:07:17,290
but they are fundamentally limited when it comes to memory

201
00:07:17,290 --> 00:07:17,949
and knowledge.

202
00:07:19,088 --> 00:07:21,369
Uh, for example, we

203
00:07:21,369 --> 00:07:23,470
all kind of know that they only store

204
00:07:23,470 --> 00:07:25,588
information in their parametric memory,

205
00:07:25,838 --> 00:07:28,129
i.e., their model weights, the stuff they were trained

206
00:07:28,129 --> 00:07:30,170
on. Um, so they can't learn or

207
00:07:30,170 --> 00:07:31,949
accumulate experience after training.

208
00:07:35,000 --> 00:07:37,350
The other aspect as well is that their context

209
00:07:37,350 --> 00:07:39,399
window is temporary, so anything outside that

210
00:07:39,399 --> 00:07:41,459
window disappears immediately after a generation

211
00:07:41,459 --> 00:07:43,678
unless you have saved it someplace, you've persisted

212
00:07:43,678 --> 00:07:45,920
it. um, and critically LMs

213
00:07:45,920 --> 00:07:47,988
have no notion of persistent state across turns

214
00:07:47,988 --> 00:07:49,759
or recessions unless you build that.

215
00:07:53,129 --> 00:07:53,889
Now,

216
00:07:54,250 --> 00:07:56,369
can't you just get a bigger boat? Uh,

217
00:07:56,470 --> 00:07:58,569
can we not just expand the size of

218
00:07:58,569 --> 00:07:59,428
LMs?

219
00:07:59,850 --> 00:08:02,108
Um, and many, many teams try to fix,

220
00:08:02,470 --> 00:08:04,528
uh, the issues that they have with agents

221
00:08:04,528 --> 00:08:06,528
by just getting a, an LM with a bigger

222
00:08:06,528 --> 00:08:07,298
context window.

223
00:08:07,689 --> 00:08:09,889
Um, but larger windows arguably

224
00:08:09,889 --> 00:08:12,048
don't function as memory. You could say like, well,

225
00:08:12,088 --> 00:08:14,309
I've trained more data into

226
00:08:14,309 --> 00:08:15,088
the LM.

227
00:08:15,528 --> 00:08:17,869
So does it not have more knowledge? Yeah, it does.

228
00:08:18,410 --> 00:08:20,488
Um, but let's think back again to

229
00:08:20,488 --> 00:08:22,569
the charts that we saw earlier, right? Which

230
00:08:22,569 --> 00:08:23,238
is one,

231
00:08:23,689 --> 00:08:25,988
the longer your context gets,

232
00:08:26,329 --> 00:08:29,108
uh, you still have that positional

233
00:08:29,358 --> 00:08:30,829
accuracy and retrieval problem.

234
00:08:32,109 --> 00:08:32,879
Um,

235
00:08:33,149 --> 00:08:35,048
and then the other thing as well is.

236
00:08:36,308 --> 00:08:37,889
My favorite saying in the world, a 1 million

237
00:08:38,149 --> 00:08:40,308
token context window does not equal reliable

238
00:08:40,308 --> 00:08:41,989
performance at 1 million tokens.

239
00:08:43,508 --> 00:08:45,749
Um, now the other thing I didn't

240
00:08:45,749 --> 00:08:46,707
really mention,

241
00:08:47,288 --> 00:08:49,347
uh, about the Chroma paper, uh, that

242
00:08:49,347 --> 00:08:50,028
I should have

243
00:08:50,467 --> 00:08:52,629
was this insight. So who wants

244
00:08:52,629 --> 00:08:53,749
to guess, um.

245
00:08:54,759 --> 00:08:56,830
Throw up your hands, I'm gonna toss some numbers out.

246
00:08:57,129 --> 00:08:59,678
Uh, the percent of the effective

247
00:08:59,678 --> 00:09:01,969
context window that you have

248
00:09:02,168 --> 00:09:03,908
for, let's say a 1 million,

249
00:09:04,250 --> 00:09:05,529
uh, token LLM.

250
00:09:06,129 --> 00:09:08,428
Who here thinks you get to use like

251
00:09:09,048 --> 00:09:09,928
80%?

252
00:09:10,678 --> 00:09:11,408
Of the LM.

253
00:09:14,229 --> 00:09:16,109
What about 60%?

254
00:09:19,619 --> 00:09:20,719
It's not like one hand,

255
00:09:21,580 --> 00:09:22,359
40%.

256
00:09:24,460 --> 00:09:26,019
Yeah, OK, what about 20%?

257
00:09:27,359 --> 00:09:29,369
Awesome. Split the middle. It's like someplace

258
00:09:29,369 --> 00:09:31,489
in between. It's like 20 to 30%.

259
00:09:31,649 --> 00:09:34,288
That's actually the effective, uh, context

260
00:09:34,288 --> 00:09:36,330
like tokens that you get to use

261
00:09:36,330 --> 00:09:36,960
for an LLM,

262
00:09:37,450 --> 00:09:39,859
um. Because

263
00:09:40,840 --> 00:09:42,219
Let's go back to this, right?

264
00:09:43,428 --> 00:09:44,658
Let's think about this again, right?

265
00:09:44,918 --> 00:09:45,479
Um.

266
00:09:46,500 --> 00:09:48,500
That earlier chart where I showed that input

267
00:09:48,500 --> 00:09:50,538
output into the context window, right? That's

268
00:09:50,538 --> 00:09:53,158
kind of how we think of LMs,

269
00:09:53,538 --> 00:09:55,619
uh, but what you don't see is all

270
00:09:55,619 --> 00:09:58,139
the junk that gets added into the context

271
00:09:58,139 --> 00:10:00,298
for tool calls, system prompt

272
00:10:00,298 --> 00:10:01,700
information, all that good stuff.

273
00:10:02,979 --> 00:10:03,960
Um,

274
00:10:04,489 --> 00:10:06,489
now, we can solve this,

275
00:10:06,658 --> 00:10:08,899
uh, historically as human beings. So

276
00:10:08,899 --> 00:10:10,899
my background is actually in anthropology. I did

277
00:10:10,899 --> 00:10:12,899
not study computer science. I was late to

278
00:10:12,899 --> 00:10:14,899
the game, uh, but it's shocking how

279
00:10:14,899 --> 00:10:16,940
many of those like concepts in anthropology,

280
00:10:17,099 --> 00:10:19,129
the study of humans and how we've evolved

281
00:10:19,129 --> 00:10:19,750
as a

282
00:10:20,099 --> 00:10:22,519
socio cultural group of animals, uh, applies

283
00:10:22,519 --> 00:10:23,038
to.

284
00:10:24,099 --> 00:10:24,719
Jenner of AI,

285
00:10:25,038 --> 00:10:25,899
who knew, right?

286
00:10:26,678 --> 00:10:27,918
Um, so,

287
00:10:28,320 --> 00:10:30,469
here's the thing, right, humans, we've always extended

288
00:10:30,469 --> 00:10:32,769
our intelligence by building external

289
00:10:32,769 --> 00:10:33,869
memory systems.

290
00:10:34,369 --> 00:10:36,570
For example, we had oral traditional, oral

291
00:10:36,570 --> 00:10:37,469
traditions in writing.

292
00:10:39,479 --> 00:10:41,690
Uh, we came up with books and libraries

293
00:10:41,690 --> 00:10:43,710
and archives, partially as a way

294
00:10:43,710 --> 00:10:46,129
to further persist whatever information we

295
00:10:46,129 --> 00:10:48,529
had, but also to share and collaborate that

296
00:10:48,529 --> 00:10:50,629
information. And

297
00:10:50,629 --> 00:10:52,219
then we had the internet.

298
00:10:52,599 --> 00:10:54,558
Oh, what a great time. This was fun,

299
00:10:54,820 --> 00:10:55,779
right? So,

300
00:10:56,158 --> 00:10:58,239
these are all essentially exocor cortices that

301
00:10:58,239 --> 00:11:00,399
we've built, so information can persist

302
00:11:00,399 --> 00:11:02,719
beyond our short biological memory limits.

303
00:11:03,509 --> 00:11:04,129
Um,

304
00:11:04,509 --> 00:11:06,798
we can kind of think of civil civilization

305
00:11:06,798 --> 00:11:09,178
itself as like humans,

306
00:11:09,469 --> 00:11:11,710
humanity's externalized like long-term

307
00:11:11,710 --> 00:11:13,908
memory. It's a a system for storing

308
00:11:13,908 --> 00:11:15,928
knowledge, organizing it, transmitting

309
00:11:15,928 --> 00:11:18,149
it, and letting future generations benefit

310
00:11:18,149 --> 00:11:19,109
from past experiences.

311
00:11:21,450 --> 00:11:23,590
Um, so once again, we figured out

312
00:11:23,590 --> 00:11:25,710
the answer to how we give agents

313
00:11:25,710 --> 00:11:27,869
the right knowledge at the right moment and to

314
00:11:27,869 --> 00:11:29,710
preserve what they've learned over time.

315
00:11:31,139 --> 00:11:33,239
Um, AI agents need the same thing.

316
00:11:34,099 --> 00:11:34,658
Uh,

317
00:11:34,940 --> 00:11:37,099
the LM is a reasoning engine like the human brain, but

318
00:11:37,099 --> 00:11:39,178
the memory system is the exocortex that

319
00:11:39,178 --> 00:11:40,408
holds durable knowledge,

320
00:11:40,700 --> 00:11:42,119
experiences, and state.

321
00:11:45,460 --> 00:11:47,580
Um, now one of the biggest misconceptions today

322
00:11:47,580 --> 00:11:49,359
actually is treating context as memory,

323
00:11:49,700 --> 00:11:51,599
and I wanna dig into more about like what,

324
00:11:52,058 --> 00:11:54,340
how those are actually kind of different and also how they work

325
00:11:54,340 --> 00:11:55,580
together. Right?

326
00:11:57,259 --> 00:11:59,619
Um, so it's helpful to

327
00:11:59,629 --> 00:12:01,710
kind of just baseline on like what a

328
00:12:01,710 --> 00:12:03,830
context window is, what context is, and

329
00:12:03,830 --> 00:12:04,830
what kind of goes into it.

330
00:12:05,149 --> 00:12:07,149
Going back to the like the question about

331
00:12:07,149 --> 00:12:09,469
the effective percent of your,

332
00:12:09,869 --> 00:12:11,869
um, of a 1 million

333
00:12:11,869 --> 00:12:13,288
model that you get to use,

334
00:12:13,590 --> 00:12:15,658
right? Uh, so

335
00:12:15,658 --> 00:12:18,099
context is the information assembled

336
00:12:18,099 --> 00:12:19,129
for, uh,

337
00:12:19,418 --> 00:12:20,639
an LM call, right?

338
00:12:20,940 --> 00:12:23,298
Uh, it's the fixed amount of texts or tokens

339
00:12:23,298 --> 00:12:25,719
that an LM can process in a single interaction.

340
00:12:25,979 --> 00:12:28,058
Essentially the LLM's working memory, right? And this

341
00:12:28,058 --> 00:12:30,190
is true also if it's a multi-modal like

342
00:12:30,190 --> 00:12:30,960
model as well.

343
00:12:31,808 --> 00:12:32,590
Um,

344
00:12:32,928 --> 00:12:34,869
so it's, it's what exists for that like

345
00:12:35,229 --> 00:12:38,599
moment. Um,

346
00:12:39,019 --> 00:12:41,389
so agent memory, uh, this is our

347
00:12:41,389 --> 00:12:43,899
definition of agent memory that is,

348
00:12:44,119 --> 00:12:45,759
should be similar to what other people have said,

349
00:12:46,298 --> 00:12:48,418
um, and that is agent memory is

350
00:12:48,418 --> 00:12:50,570
persistent memory management systems that

351
00:12:50,570 --> 00:12:52,639
transform stateless AI agents into

352
00:12:52,639 --> 00:12:54,798
intelligent entities capable

353
00:12:54,798 --> 00:12:56,729
of learning, adapting,

354
00:12:57,099 --> 00:12:59,259
and maintaining continually cross interactions.

355
00:12:59,379 --> 00:13:01,619
I apologies, guys. I've done like 3 talks

356
00:13:01,619 --> 00:13:03,729
this week and booth duty. I'm like,

357
00:13:03,739 --> 00:13:05,058
my voice is kinda.

358
00:13:05,739 --> 00:13:07,808
Dying, um, so apologies once again.

359
00:13:08,149 --> 00:13:09,288
Um, so

360
00:13:09,710 --> 00:13:11,788
memory is durable. It persists across turns,

361
00:13:11,859 --> 00:13:14,029
sessions, and tasks, and it can evolve as the agent

362
00:13:14,029 --> 00:13:14,849
interacts with the world,

363
00:13:15,308 --> 00:13:16,229
depending on how you architect it.

364
00:13:17,940 --> 00:13:18,639
Um,

365
00:13:19,658 --> 00:13:21,519
Now, let's talk about

366
00:13:22,340 --> 00:13:24,379
the relationship between, let's introduce the

367
00:13:24,379 --> 00:13:26,619
idea of the relationship between memory

368
00:13:26,619 --> 00:13:27,879
engineering and context engineering, right?

369
00:13:28,308 --> 00:13:30,658
Um, so they work, they do work hand in hand.

370
00:13:31,019 --> 00:13:31,719
We're not saying

371
00:13:32,058 --> 00:13:34,058
only build memory and neglect context

372
00:13:34,058 --> 00:13:34,678
engineering.

373
00:13:34,979 --> 00:13:37,340
Um, there is a process you should go through, right?

374
00:13:37,619 --> 00:13:39,940
Similar to how everyone started with like

375
00:13:39,940 --> 00:13:41,519
prompting and prompt engineering,

376
00:13:41,820 --> 00:13:43,899
and then you start moving towards working with

377
00:13:43,899 --> 00:13:44,798
context,

378
00:13:45,178 --> 00:13:47,298
and then you wanna start thinking about memory

379
00:13:47,298 --> 00:13:48,599
engineering, right?

380
00:13:49,279 --> 00:13:51,349
Um, so retrieval builds

381
00:13:51,349 --> 00:13:53,658
context, but retrieval does not create memory.

382
00:13:53,918 --> 00:13:56,359
Uh, for memory, you need writing, storage,

383
00:13:56,489 --> 00:13:58,599
summarization, embedding, and evolution over

384
00:13:58,599 --> 00:13:59,298
time, right?

385
00:14:00,009 --> 00:14:00,719
Um,

386
00:14:01,009 --> 00:14:03,288
and this is all instructions, tools,

387
00:14:03,330 --> 00:14:05,408
knowledge, all that gets dumped in the context window.

388
00:14:05,690 --> 00:14:07,629
And wouldn't it be nice if you could,

389
00:14:07,889 --> 00:14:10,038
I don't know, like curate the

390
00:14:10,038 --> 00:14:10,908
information that gets pulled in.

391
00:14:14,038 --> 00:14:15,500
Um, now for

392
00:14:16,200 --> 00:14:18,418
developers, this means you can't rely on Yella Malone

393
00:14:18,418 --> 00:14:20,538
to maintain state or accumulate experience.

394
00:14:20,918 --> 00:14:23,139
You need to build these explicit memory layers.

395
00:14:24,500 --> 00:14:27,029
This includes context engineering to surface relevant

396
00:14:27,029 --> 00:14:29,029
information, memory engineering

397
00:14:29,029 --> 00:14:31,210
to store and organize long-term state,

398
00:14:31,830 --> 00:14:33,989
and of course evalves to ensure the memory

399
00:14:33,989 --> 00:14:36,070
system is accurate, stable, and non-drifting.

400
00:14:37,609 --> 00:14:39,830
Um, once you understand the elements and limitations,

401
00:14:39,918 --> 00:14:41,969
it becomes clear that memory isn't optional. It's

402
00:14:41,969 --> 00:14:44,009
the foundation for reliable long

403
00:14:44,009 --> 00:14:46,009
horizon agent behavior, and I

404
00:14:46,009 --> 00:14:47,389
would pause it to argue that

405
00:14:47,769 --> 00:14:50,129
the more you're willing to invest in

406
00:14:50,129 --> 00:14:52,769
like your context engineering and memory engineering practices,

407
00:14:53,250 --> 00:14:55,570
the more freedom and autonomy you give

408
00:14:55,570 --> 00:14:57,570
yourself as a developer, as a development

409
00:14:57,570 --> 00:14:59,979
team. To not

410
00:14:59,979 --> 00:15:01,389
constantly rely on,

411
00:15:01,710 --> 00:15:03,908
for example, a Frontier lab creating the next

412
00:15:03,908 --> 00:15:04,629
new model,

413
00:15:05,029 --> 00:15:07,269
right? You don't have to wait on some

414
00:15:07,269 --> 00:15:08,529
external entity

415
00:15:08,989 --> 00:15:10,570
to improve the accuracy

416
00:15:11,029 --> 00:15:13,058
and the quality of the

417
00:15:13,058 --> 00:15:15,219
agents or the agenttic workflows you build. You, you

418
00:15:15,219 --> 00:15:17,548
have that control in your hands. It's a wonderful,

419
00:15:17,629 --> 00:15:18,570
beautiful thing,

420
00:15:18,908 --> 00:15:19,450
um.

421
00:15:20,649 --> 00:15:21,599
And

422
00:15:22,759 --> 00:15:24,139
Yeah, and like once again like.

423
00:15:24,940 --> 00:15:27,288
If there's a question about the place

424
00:15:27,288 --> 00:15:29,418
of software engineering in the time of Generative

425
00:15:29,418 --> 00:15:30,908
AI, um.

426
00:15:31,840 --> 00:15:33,960
Like someone said, there is only, we

427
00:15:33,960 --> 00:15:36,038
should look at the growth opportunities. The

428
00:15:36,038 --> 00:15:38,058
growth opportunities for us as

429
00:15:38,239 --> 00:15:39,548
technical professionals.

430
00:15:39,830 --> 00:15:42,239
How can we apply the best practices we've accumulated

431
00:15:42,639 --> 00:15:43,820
to improve these systems.

432
00:15:46,000 --> 00:15:46,710
Um,

433
00:15:47,119 --> 00:15:48,859
now, let me talk about

434
00:15:49,599 --> 00:15:50,599
the anatomy of an agent.

435
00:15:51,359 --> 00:15:53,590
Right, this might seem kind of basic,

436
00:15:53,719 --> 00:15:54,928
but it is really important

437
00:15:55,239 --> 00:15:57,279
because once again, I, I still get questions about, well, what's

438
00:15:57,279 --> 00:15:59,668
the difference between like an LM versus an embedding

439
00:15:59,668 --> 00:16:02,219
model versus like a re-ranking model.

440
00:16:02,460 --> 00:16:03,109
Um,

441
00:16:03,440 --> 00:16:04,070
you know,

442
00:16:04,359 --> 00:16:04,899
like,

443
00:16:05,239 --> 00:16:07,440
what if we get to the point where we have a

444
00:16:07,440 --> 00:16:08,658
super, super

445
00:16:09,000 --> 00:16:11,158
like insanely amazing large

446
00:16:11,158 --> 00:16:13,200
model? Can we not just drop that

447
00:16:13,200 --> 00:16:14,000
in and replace it?

448
00:16:15,019 --> 00:16:16,129
So it's important actually

449
00:16:16,538 --> 00:16:17,178
to go over this.

450
00:16:18,259 --> 00:16:18,918
Um,

451
00:16:19,178 --> 00:16:20,359
so AI agents.

452
00:16:21,149 --> 00:16:23,229
Definition there, but, uh, we

453
00:16:23,229 --> 00:16:26,009
can think of agents in terms of 4 faculties.

454
00:16:26,940 --> 00:16:29,099
Um, the first one is cognition, which is

455
00:16:29,099 --> 00:16:30,399
where the element plays a role.

456
00:16:30,940 --> 00:16:32,940
Uh, the second is action, which

457
00:16:32,940 --> 00:16:34,200
is provided by tools.

458
00:16:35,450 --> 00:16:37,500
Uh, the third is perception, which is

459
00:16:37,500 --> 00:16:38,840
like inputs essentially.

460
00:16:39,298 --> 00:16:41,769
Um, and then finally, memory

461
00:16:41,769 --> 00:16:43,259
in terms of short and long term.

462
00:16:46,190 --> 00:16:48,190
Um, now, to be clear, an LM becomes

463
00:16:48,190 --> 00:16:50,229
an agent when it's embedded in a loop where it

464
00:16:50,229 --> 00:16:50,928
can observe,

465
00:16:51,269 --> 00:16:53,308
think, act, and update its

466
00:16:53,308 --> 00:16:54,090
understanding of the world.

467
00:16:54,349 --> 00:16:56,808
So instead of just generating text, an agent performs

468
00:16:56,908 --> 00:16:59,379
the structured reasoning, calls tools,

469
00:16:59,548 --> 00:17:01,428
and manipulates external systems.

470
00:17:02,379 --> 00:17:03,119
To a certain degree,

471
00:17:03,379 --> 00:17:05,459
um, depends on how, you know, risky you want to

472
00:17:05,459 --> 00:17:07,719
play and it responds to this environment.

473
00:17:08,739 --> 00:17:10,779
The perception, reasoning, action,

474
00:17:10,890 --> 00:17:12,900
and feedback cycle is what differentiates stag

475
00:17:12,900 --> 00:17:13,880
chatbots from

476
00:17:14,380 --> 00:17:16,338
adaptive interactive agents.

477
00:17:18,750 --> 00:17:20,368
Um, so this is kind of

478
00:17:20,789 --> 00:17:22,380
typically what an agentic,

479
00:17:22,959 --> 00:17:24,489
uh, system looks like.

480
00:17:24,868 --> 00:17:26,989
Um, so if we kind of follow the

481
00:17:26,989 --> 00:17:29,108
flow, um, every agent request

482
00:17:29,108 --> 00:17:30,880
breaks down into distinct phases.

483
00:17:31,219 --> 00:17:33,430
Uh, we have the environment which sends an observation.

484
00:17:34,828 --> 00:17:36,828
Usually through a human or maybe even another

485
00:17:36,828 --> 00:17:38,848
agent. Um, we

486
00:17:38,848 --> 00:17:40,848
have the system that builds context. We have

487
00:17:40,848 --> 00:17:42,489
them all that performs reasoning.

488
00:17:42,890 --> 00:17:44,890
We have tools that could be invoked, and we

489
00:17:44,890 --> 00:17:46,289
have results that are written back to memory.

490
00:17:47,259 --> 00:17:49,059
So understanding this anatomy,

491
00:17:49,420 --> 00:17:51,618
uh, clarifies where memory reads and writes

492
00:17:51,618 --> 00:17:53,660
occur and where you as a developer must

493
00:17:53,660 --> 00:17:54,318
intervene,

494
00:17:54,900 --> 00:17:57,209
right? Um,

495
00:17:57,368 --> 00:17:59,449
but yeah, once again, like, you know,

496
00:17:59,689 --> 00:18:00,348
just to

497
00:18:00,769 --> 00:18:03,309
summarize, we have data, that data gets

498
00:18:03,969 --> 00:18:06,049
processed, uh, you can chunk it, you

499
00:18:06,049 --> 00:18:06,949
can embed it,

500
00:18:07,289 --> 00:18:09,289
um, that gets dropped into

501
00:18:09,289 --> 00:18:11,650
a vector database, and

502
00:18:11,650 --> 00:18:13,068
information from that is retrieved.

503
00:18:14,259 --> 00:18:15,969
So I've gotten questions from people going like,

504
00:18:16,368 --> 00:18:18,529
oh, doesn't agents, don't agents

505
00:18:18,529 --> 00:18:20,598
negate the need for rag? It's like, no, rag

506
00:18:20,598 --> 00:18:21,269
could be a tool.

507
00:18:22,059 --> 00:18:24,358
And you could do semantic search, you could do full text search,

508
00:18:24,500 --> 00:18:25,199
um,

509
00:18:25,799 --> 00:18:26,719
doesn't matter, right?

510
00:18:30,269 --> 00:18:31,088
Um,

511
00:18:31,509 --> 00:18:33,699
I didn't mention, I, I said this was gonna be more educational,

512
00:18:33,779 --> 00:18:35,910
but, uh, just to do my duty

513
00:18:35,910 --> 00:18:37,049
of pointing out

514
00:18:37,309 --> 00:18:39,588
in green, uh, where Mongodibi Atlas

515
00:18:39,588 --> 00:18:40,930
and Voyage lives,

516
00:18:41,309 --> 00:18:41,828
um.

517
00:18:42,759 --> 00:18:44,759
So, modern agents rely very

518
00:18:44,759 --> 00:18:47,229
heavily on tool use, calling APIs,

519
00:18:47,519 --> 00:18:48,509
databases,

520
00:18:48,920 --> 00:18:51,059
code execution, search systems,

521
00:18:51,160 --> 00:18:52,318
sectional services.

522
00:18:52,799 --> 00:18:55,078
Um, so the reasoning loop looks like react,

523
00:18:55,400 --> 00:18:57,400
right? The mall thinks, it decides,

524
00:18:57,469 --> 00:18:58,680
it performs a tool action,

525
00:18:59,078 --> 00:19:00,739
receives feedback, and continues reasoning.

526
00:19:01,979 --> 00:19:03,118
Um, now,

527
00:19:03,529 --> 00:19:05,699
where MongudiB fits, right, is

528
00:19:05,699 --> 00:19:07,858
we provide an operational database,

529
00:19:07,939 --> 00:19:09,199
we provide a vector store,

530
00:19:09,539 --> 00:19:11,769
um, and your agent, for example, would

531
00:19:11,769 --> 00:19:13,559
interact with that vector store,

532
00:19:13,858 --> 00:19:15,969
uh, with embedding malls provided by

533
00:19:15,969 --> 00:19:18,118
Voyage and with,

534
00:19:18,259 --> 00:19:20,368
for example, uh, retrieve

535
00:19:20,368 --> 00:19:21,318
documents,

536
00:19:21,578 --> 00:19:23,828
um, being re-scored through

537
00:19:23,828 --> 00:19:24,779
our re-ranking malls.

538
00:19:25,910 --> 00:19:27,130
And we also know I'm Speer.

539
00:19:28,318 --> 00:19:29,019
Um

540
00:19:30,250 --> 00:19:32,338
Let me kind of continue on because I'll

541
00:19:32,338 --> 00:19:33,140
drive you guys through

542
00:19:33,858 --> 00:19:35,939
what memory types actually

543
00:19:35,939 --> 00:19:38,289
mean. Um, now we've

544
00:19:38,469 --> 00:19:40,750
talked about like, um, agents,

545
00:19:40,910 --> 00:19:41,809
right? Single one.

546
00:19:43,229 --> 00:19:45,269
and now we have like multi-agent systems, right?

547
00:19:45,430 --> 00:19:47,670
Which are basically just collections of

548
00:19:47,670 --> 00:19:48,489
agents,

549
00:19:49,029 --> 00:19:51,269
which means a couple of things. Uh, the

550
00:19:51,269 --> 00:19:52,848
same problems of single agent

551
00:19:53,529 --> 00:19:54,328
still apply.

552
00:19:55,209 --> 00:19:57,750
But then you also develop kind of new problems,

553
00:19:57,959 --> 00:19:59,900
namely around coordination of these agents.

554
00:20:01,858 --> 00:20:04,059
Now, we would argue that

555
00:20:04,059 --> 00:20:06,500
these coordination problems could actually be solved

556
00:20:06,500 --> 00:20:07,118
by.

557
00:20:08,449 --> 00:20:09,979
A good shared state.

558
00:20:10,890 --> 00:20:11,608
Um,

559
00:20:11,930 --> 00:20:13,108
that these guys can access.

560
00:20:14,759 --> 00:20:16,868
This shared data substrate, you could say,

561
00:20:17,160 --> 00:20:19,239
which keeps all agents synchronized and

562
00:20:19,239 --> 00:20:20,279
prevents freezing drift.

563
00:20:24,199 --> 00:20:25,459
Um, now,

564
00:20:26,559 --> 00:20:27,219
what,

565
00:20:27,479 --> 00:20:28,539
what, for example,

566
00:20:29,239 --> 00:20:30,439
does short-term memory look like?

567
00:20:31,848 --> 00:20:32,489
Uh

568
00:20:33,299 --> 00:20:35,420
It could look like persisting checkpoints

569
00:20:35,420 --> 00:20:36,358
in Mongo DB.

570
00:20:36,739 --> 00:20:38,088
So for example, we have a,

571
00:20:38,358 --> 00:20:40,019
an integration with Lang graph,

572
00:20:40,420 --> 00:20:42,039
um, where if you want to build

573
00:20:42,459 --> 00:20:44,838
like an agent or a multi-agent system,

574
00:20:45,219 --> 00:20:45,920
um,

575
00:20:46,539 --> 00:20:48,539
you could essentially persist a short-term memory as

576
00:20:48,539 --> 00:20:50,459
a checkpoint through the Mong DB saver.

577
00:20:51,910 --> 00:20:54,029
And you know, the fun part is the way to

578
00:20:54,029 --> 00:20:56,410
check that it's working is, uh, you

579
00:20:56,670 --> 00:20:58,250
run the agent chain once

580
00:20:58,549 --> 00:21:01,130
and then you go and you delete these checkpoints

581
00:21:01,130 --> 00:21:02,500
and you ask the agent,

582
00:21:02,910 --> 00:21:04,729
what did we talk about and they'll say

583
00:21:05,029 --> 00:21:06,390
hello there, good man.

584
00:21:06,858 --> 00:21:09,088
It, I mean, maybe not, it'll say hello, uh.

585
00:21:09,818 --> 00:21:10,640
How are you?

586
00:21:10,979 --> 00:21:11,920
I don't remember.

587
00:21:12,789 --> 00:21:13,959
So yeah

588
00:21:14,848 --> 00:21:17,229
Um, so agents need a way to take snapshots

589
00:21:17,229 --> 00:21:19,420
of their state, what tools they've called, what

590
00:21:19,420 --> 00:21:21,420
steps they've completed, and what their current

591
00:21:21,420 --> 00:21:22,108
objectives are.

592
00:21:22,588 --> 00:21:23,559
And so

593
00:21:23,858 --> 00:21:25,979
essentially as each step or node in this graph

594
00:21:25,979 --> 00:21:28,180
is executed, uh, new checkpoints containing

595
00:21:28,180 --> 00:21:30,549
the checkpoint ID and the threat ID that belongs to

596
00:21:30,939 --> 00:21:31,789
would be ran into MogdiB

597
00:21:32,719 --> 00:21:34,489
or, you know, any database provider,

598
00:21:34,779 --> 00:21:36,759
right? But that's essentially how these checkpoints work.

599
00:21:40,140 --> 00:21:40,880
Dang,

600
00:21:41,189 --> 00:21:41,949
that did not

601
00:21:42,430 --> 00:21:43,699
turn out well, but

602
00:21:44,108 --> 00:21:46,229
nevertheless, uh, this just goes

603
00:21:46,229 --> 00:21:48,368
back to reinforces the anatomy of an agent.

604
00:21:49,098 --> 00:21:51,209
Um, and also the four characteristics that

605
00:21:51,209 --> 00:21:53,368
we, we kind of want, um, our

606
00:21:53,368 --> 00:21:54,439
agents to,

607
00:21:54,848 --> 00:21:56,910
to feel like and what users expect,

608
00:21:57,559 --> 00:21:59,549
namely that agents are reflective, we've

609
00:21:59,969 --> 00:22:01,229
mentioned this several times,

610
00:22:01,689 --> 00:22:03,549
um, that they're interactive.

611
00:22:04,969 --> 00:22:07,400
That they're reactive and proactive,

612
00:22:07,598 --> 00:22:09,640
so rather than you have having to like prompt them for

613
00:22:09,640 --> 00:22:10,390
every little bit,

614
00:22:10,680 --> 00:22:12,309
um, they give you a little bit more,

615
00:22:12,809 --> 00:22:15,430
um, and some level of autonomy,

616
00:22:15,608 --> 00:22:16,420
although.

617
00:22:17,479 --> 00:22:20,068
This is questionable in terms of

618
00:22:20,098 --> 00:22:22,049
where you you individually

619
00:22:22,348 --> 00:22:24,739
would want to like set your autonomy level,

620
00:22:25,029 --> 00:22:27,229
right? There is a lot of talk about fully autonomous

621
00:22:27,229 --> 00:22:29,348
agents, but, you know, in my experience, most

622
00:22:29,348 --> 00:22:30,969
companies don't want that. Um,

623
00:22:31,469 --> 00:22:33,509
what they want is what we call almost

624
00:22:33,509 --> 00:22:35,160
like a level 2, level 3 agent,

625
00:22:35,430 --> 00:22:37,779
where you have a mix of like some determinism,

626
00:22:38,068 --> 00:22:40,068
um, and then in key areas, you might have

627
00:22:40,068 --> 00:22:41,199
some non-determinism,

628
00:22:41,789 --> 00:22:43,009
but even then it's.

629
00:22:44,009 --> 00:22:46,170
Not, not, people are really not like

630
00:22:46,170 --> 00:22:47,809
my experience building fully autonomous agents.

631
00:22:49,318 --> 00:22:50,019
Um,

632
00:22:51,789 --> 00:22:53,009
I'm so sorry you guys.

633
00:22:53,630 --> 00:22:55,709
It, it too is feeling the day for reinvent

634
00:22:55,709 --> 00:22:56,328
Energy.

635
00:22:56,989 --> 00:22:59,529
All right, um, but understanding where,

636
00:22:59,709 --> 00:23:00,328
uh,

637
00:23:00,979 --> 00:23:03,348
each memory layer fits, uh,

638
00:23:03,390 --> 00:23:05,949
is key to building these stable agents. And

639
00:23:05,949 --> 00:23:07,989
each type of memory solves a different class of

640
00:23:07,989 --> 00:23:08,660
problems.

641
00:23:09,029 --> 00:23:11,108
So we'll build on this in the next act as we move

642
00:23:11,108 --> 00:23:13,108
from context engineering to

643
00:23:13,108 --> 00:23:14,568
full memory engineering, what that looks like.

644
00:23:15,969 --> 00:23:18,328
Also, you can connect with me uh

645
00:23:18,328 --> 00:23:20,489
after the session and on LinkedIn and

646
00:23:20,489 --> 00:23:22,068
I will send you these slides.

647
00:23:22,890 --> 00:23:24,279
And they will be very pretty.

648
00:23:27,150 --> 00:23:29,358
Uh, OK, so I've, we've

649
00:23:29,358 --> 00:23:30,660
described, um,

650
00:23:30,959 --> 00:23:33,118
the anatomy of an agent in a multi-agent system. Uh,

651
00:23:33,199 --> 00:23:35,219
we've outlined the problem

652
00:23:35,219 --> 00:23:37,900
of why LMs misbehave,

653
00:23:38,318 --> 00:23:38,959
um.

654
00:23:39,858 --> 00:23:41,930
It's helpful to actually talk about some application patterns,

655
00:23:42,259 --> 00:23:43,000
right, uh,

656
00:23:43,299 --> 00:23:45,420
specifically to explain how different agent application

657
00:23:45,420 --> 00:23:47,699
modes would shape the memory requirements.

658
00:23:49,910 --> 00:23:52,108
So an application mode describes how an agent

659
00:23:52,108 --> 00:23:54,380
interacts with this environment, its

660
00:23:54,380 --> 00:23:54,949
operational rhythm.

661
00:23:55,910 --> 00:23:58,029
Um, each mode drives very specific

662
00:23:58,029 --> 00:24:00,309
memory requirements, uh, and no matter

663
00:24:00,309 --> 00:24:02,348
what the industry is, their use cases can

664
00:24:02,348 --> 00:24:04,410
be clustered into a few recurring patterns, and

665
00:24:04,410 --> 00:24:05,588
we call these application modes.

666
00:24:07,930 --> 00:24:10,009
Um, now these are the three.

667
00:24:10,390 --> 00:24:12,368
Feel free to, uh, take a picture if you'd like.

668
00:24:13,299 --> 00:24:15,229
Especially since the formatting on this is much nicer.

669
00:24:16,519 --> 00:24:17,219
Um

670
00:24:18,088 --> 00:24:20,250
So I'll just kind of talk through them a little bit. Um,

671
00:24:20,368 --> 00:24:22,029
but the three key modes are,

672
00:24:22,489 --> 00:24:24,078
um, assistant mode agents,

673
00:24:24,880 --> 00:24:26,588
which typically need conversational continuity.

674
00:24:27,680 --> 00:24:29,750
You have uh workflow mode agents,

675
00:24:29,838 --> 00:24:32,140
which needs step by step procedural recall.

676
00:24:33,269 --> 00:24:35,660
And autonomous or multi-agent systems

677
00:24:35,660 --> 00:24:37,769
which need shared memory to coordinate

678
00:24:37,769 --> 00:24:40,289
decisions. And

679
00:24:40,588 --> 00:24:42,890
for everyone, that's, that's a deep research agent,

680
00:24:43,088 --> 00:24:44,309
typically. Um,

681
00:24:44,930 --> 00:24:47,130
so understanding these modes helps us design

682
00:24:47,130 --> 00:24:49,368
the memory architecture that matches the agent's

683
00:24:49,368 --> 00:24:51,509
operational power rather than taking a one size fits all approach.

684
00:24:53,828 --> 00:24:56,088
Um, so the first one is

685
00:24:56,549 --> 00:24:57,219
conversational assistant.

686
00:24:57,549 --> 00:24:59,868
So in assistant mode, the agent's memory

687
00:24:59,868 --> 00:25:02,578
must pres must preserve the user's preferences,

688
00:25:02,910 --> 00:25:05,529
past conversations, and long-run objectives.

689
00:25:05,828 --> 00:25:07,910
And this is where short-term memory, semantic

690
00:25:07,910 --> 00:25:10,068
caching, and episodic recall matter

691
00:25:10,068 --> 00:25:12,189
most. Um, the agent should

692
00:25:12,189 --> 00:25:13,779
remember what the user cares about,

693
00:25:14,068 --> 00:25:15,709
not just repeat steps or lose thread.

694
00:25:16,539 --> 00:25:18,578
So this mode, uh, Hadley exercises

695
00:25:18,578 --> 00:25:20,660
context engineering and continuity benchmarks

696
00:25:20,660 --> 00:25:21,219
like locomo.

697
00:25:23,509 --> 00:25:25,618
In workflow mode, um, the agent

698
00:25:25,618 --> 00:25:28,108
executes a series of predefined

699
00:25:28,108 --> 00:25:29,630
or dynamically generated steps.

700
00:25:30,519 --> 00:25:32,868
Uh, so here memory isn't about

701
00:25:32,868 --> 00:25:34,630
conversation, it's about procedure.

702
00:25:35,118 --> 00:25:36,750
You preserve outputs,

703
00:25:37,118 --> 00:25:38,209
intermediate states,

704
00:25:38,640 --> 00:25:41,098
decisions, and toll results, so the graph

705
00:25:41,098 --> 00:25:43,439
can continue, can continue deterministically.

706
00:25:44,348 --> 00:25:46,868
Um, procedural memory and state logging

707
00:25:46,868 --> 00:25:48,949
become the backbone of this pattern. Uh, so

708
00:25:48,949 --> 00:25:50,410
we can think of the,

709
00:25:50,750 --> 00:25:52,930
you know, like no code, low code tools,

710
00:25:53,338 --> 00:25:54,529
uh, for example,

711
00:25:54,949 --> 00:25:56,489
a partner of ours, NAN.

712
00:25:57,759 --> 00:25:59,299
would be a pretty good example of this.

713
00:25:59,880 --> 00:26:01,969
They have, uh, a workflow agent or

714
00:26:01,969 --> 00:26:02,949
an agent

715
00:26:03,358 --> 00:26:05,439
where you can describe kind of what you're looking for

716
00:26:05,439 --> 00:26:08,019
and they'll output this workflow, right?

717
00:26:10,689 --> 00:26:13,009
Um, and in deep research mode, the

718
00:26:13,009 --> 00:26:15,170
system operates as a coordinated collective of

719
00:26:15,170 --> 00:26:16,180
specialist agents,

720
00:26:16,650 --> 00:26:17,949
each one taking on

721
00:26:18,250 --> 00:26:20,279
like a research role such as, uh,

722
00:26:20,289 --> 00:26:22,689
retrieval, synthesis, critique, or

723
00:26:22,689 --> 00:26:23,568
cross validation.

724
00:26:24,739 --> 00:26:26,789
Um, and here memory isn't conversational

725
00:26:26,789 --> 00:26:29,189
or procedural as much as it is about

726
00:26:29,189 --> 00:26:30,029
being collaborative.

727
00:26:31,439 --> 00:26:34,318
Um, agents contribute findings, hypotheses,

728
00:26:34,400 --> 00:26:36,598
citations, and evaluations into a

729
00:26:36,598 --> 00:26:37,818
shared research record

730
00:26:38,160 --> 00:26:39,160
that evolves over time.

731
00:26:40,269 --> 00:26:42,828
The quality of the research depends entirely on the stability,

732
00:26:42,910 --> 00:26:45,098
accuracy, and structure of the shared memory

733
00:26:45,098 --> 00:26:47,199
layer. Which becomes

734
00:26:47,199 --> 00:26:49,608
a single source of truth that agents iterate on, refine,

735
00:26:49,719 --> 00:26:52,059
and challenge as the investigation deepens.

736
00:26:52,348 --> 00:26:54,019
Um, now, just to be clear.

737
00:26:54,979 --> 00:26:57,098
You can have all of these actually in

738
00:26:57,098 --> 00:26:59,338
the same feature or in the same company

739
00:26:59,338 --> 00:27:00,358
or as the same product.

740
00:27:00,858 --> 00:27:02,900
Um, we're not saying that these are mutually exclusive, but

741
00:27:02,900 --> 00:27:04,939
it's just that if you kind of zoom in on a part

742
00:27:04,939 --> 00:27:05,660
of the process,

743
00:27:05,939 --> 00:27:08,019
you can usually kind of fit it into one of these buckets.

744
00:27:11,189 --> 00:27:11,949
Um

745
00:27:13,140 --> 00:27:15,299
Now let's just dig in a little bit deeper into

746
00:27:15,299 --> 00:27:15,979
context and journey.

747
00:27:18,318 --> 00:27:20,318
So what is context engineering?

748
00:27:21,229 --> 00:27:23,358
Um, it's the discipline of controlling exactly

749
00:27:23,358 --> 00:27:25,160
what the LMCs at inference time.

750
00:27:26,509 --> 00:27:28,979
Since the model's internal memory is limited and volatile,

751
00:27:29,098 --> 00:27:31,578
we need to construct the optimal input context

752
00:27:31,578 --> 00:27:32,279
for every step.

753
00:27:33,670 --> 00:27:35,939
This means retrieving the right information, shaping

754
00:27:35,939 --> 00:27:37,949
it, ordering it, and trimming it

755
00:27:37,949 --> 00:27:40,029
so the all understands the task, the state, and

756
00:27:40,029 --> 00:27:40,868
what matters right now.

757
00:27:42,189 --> 00:27:44,699
So if memory in engineering is about what the agent retains

758
00:27:45,068 --> 00:27:47,088
and offloads, um, context engineering

759
00:27:47,088 --> 00:27:48,489
is about what the agent focuses on.

760
00:27:51,779 --> 00:27:53,479
Um, now let's just talk about

761
00:27:53,900 --> 00:27:54,608
retrieval,

762
00:27:54,939 --> 00:27:55,858
the retrieval pipeline.

763
00:27:57,019 --> 00:27:59,250
So modern context engineering starts

764
00:27:59,299 --> 00:27:59,939
with retrieval.

765
00:28:01,039 --> 00:28:02,189
A retrieval pipeline

766
00:28:02,489 --> 00:28:04,660
transforms your raw data into context the

767
00:28:04,660 --> 00:28:05,789
model can actually use.

768
00:28:06,250 --> 00:28:08,489
It'll chunk docs, embed the chunks,

769
00:28:08,729 --> 00:28:10,729
store them in a vector index, and retrieve the

770
00:28:10,729 --> 00:28:12,689
top few candidates based on semantic similarity.

771
00:28:14,140 --> 00:28:16,180
Um, this pipeline is the difference between an agent

772
00:28:16,180 --> 00:28:18,180
that guesses and an agent that grounds its

773
00:28:18,180 --> 00:28:20,318
responses in real information. So common

774
00:28:20,318 --> 00:28:22,420
question like I get is how do we cut down

775
00:28:22,420 --> 00:28:24,660
on hallucinations of LMs? And

776
00:28:24,660 --> 00:28:26,759
one of the ways you can do that is you provide the LM

777
00:28:26,759 --> 00:28:28,759
with enough grounded information

778
00:28:29,098 --> 00:28:31,098
that it doesn't need to make up stuff on the

779
00:28:31,098 --> 00:28:32,759
spot. Right?

780
00:28:33,299 --> 00:28:35,420
Because that's, they're post-trained to be

781
00:28:35,420 --> 00:28:36,059
helpful,

782
00:28:36,400 --> 00:28:38,000
and helpful is not,

783
00:28:38,279 --> 00:28:40,180
yeah, nah dude, you're on your own.

784
00:28:41,130 --> 00:28:43,130
Helpful is giving some kind of answer, even if

785
00:28:43,130 --> 00:28:44,289
it's not the best answer.

786
00:28:47,380 --> 00:28:49,559
Um, now when we talk about

787
00:28:49,699 --> 00:28:50,719
zooming in even further,

788
00:28:51,059 --> 00:28:52,630
uh, chunking, indexing, and me.

789
00:28:54,029 --> 00:28:56,368
So in this retrieval process, uh,

790
00:28:56,380 --> 00:28:57,809
chunking and metadata

791
00:28:58,348 --> 00:29:00,309
profoundly affect retrieval quality.

792
00:29:03,699 --> 00:29:05,598
Two large chunks create noise,

793
00:29:05,858 --> 00:29:07,539
and two small chunks lose meaning.

794
00:29:08,680 --> 00:29:10,118
Metadata like

795
00:29:10,598 --> 00:29:12,848
topic type, time stamp or project

796
00:29:13,199 --> 00:29:15,539
lets agents filter and target the right information,

797
00:29:16,000 --> 00:29:17,118
especially when data sets grow.

798
00:29:20,088 --> 00:29:22,209
And this becomes really important, right, because for example,

799
00:29:22,289 --> 00:29:24,368
if you're exposing a certain number of like tool

800
00:29:24,368 --> 00:29:26,549
calls to an agent and there's no description

801
00:29:26,549 --> 00:29:28,509
about what those tools actually do besides the name,

802
00:29:28,868 --> 00:29:29,430
um,

803
00:29:29,969 --> 00:29:31,410
yeah, it's not gonna do very well.

804
00:29:32,348 --> 00:29:33,789
You gotta, you gotta help the agent

805
00:29:34,108 --> 00:29:36,989
help you. And

806
00:29:36,989 --> 00:29:39,229
this is also where retrieval precision either succeeds

807
00:29:39,229 --> 00:29:39,890
or collapses.

808
00:29:41,309 --> 00:29:42,108
So,

809
00:29:42,509 --> 00:29:44,920
one powerful technique that the Voyage team,

810
00:29:45,039 --> 00:29:47,088
um, has researched and looked into,

811
00:29:47,670 --> 00:29:49,799
uh, they released a model called Voyage Context

812
00:29:49,799 --> 00:29:50,449
3,

813
00:29:51,479 --> 00:29:53,689
is contextualized,

814
00:29:53,880 --> 00:29:54,750
uh, chunking.

815
00:29:55,559 --> 00:29:57,239
Or contextualized context, um.

816
00:29:57,920 --> 00:30:00,180
And instead of inventing the user's raw query,

817
00:30:00,588 --> 00:30:01,180
uh,

818
00:30:01,559 --> 00:30:03,380
what you could do actually, for example,

819
00:30:03,799 --> 00:30:05,979
alongside with the Voyage conduct 3 model,

820
00:30:06,239 --> 00:30:08,400
um, is you could rewrite the query to

821
00:30:08,400 --> 00:30:10,719
reflect the intent, task, context, and domain

822
00:30:10,719 --> 00:30:11,259
language,

823
00:30:11,650 --> 00:30:13,739
and this process will dramatically improve

824
00:30:13,739 --> 00:30:15,559
retrieval quality for long horizon tasks.

825
00:30:18,410 --> 00:30:20,900
Now let me dig in a little bit deeper actually. um,

826
00:30:21,368 --> 00:30:22,150
so.

827
00:30:23,118 --> 00:30:25,180
What are some additional ways that you can improve

828
00:30:25,358 --> 00:30:27,858
like the quality and performance

829
00:30:28,239 --> 00:30:29,269
of your agent?

830
00:30:30,400 --> 00:30:31,318
We've talked about the,

831
00:30:31,598 --> 00:30:34,078
we dug in a little bit about the embedding and chunking

832
00:30:34,078 --> 00:30:36,118
process, um, but there's something that you can

833
00:30:36,118 --> 00:30:37,229
also do on the query level,

834
00:30:37,598 --> 00:30:39,900
right? So one

835
00:30:39,900 --> 00:30:41,739
thing you take a look at is query augmentation.

836
00:30:42,838 --> 00:30:44,949
Um, and this is a process of improving a user's

837
00:30:44,949 --> 00:30:45,759
raw query,

838
00:30:46,118 --> 00:30:47,979
uh, before retrieval happens.

839
00:30:48,479 --> 00:30:50,549
And the goal is simple, make the query better

840
00:30:50,549 --> 00:30:52,338
aligned with how knowledge is stored,

841
00:30:52,759 --> 00:30:54,759
so the agent retrieves richer, more relevant information.

842
00:30:55,920 --> 00:30:58,039
And there are 3 major families of augmentation.

843
00:30:59,358 --> 00:31:00,939
So the first one is query rewriting.

844
00:31:01,689 --> 00:31:04,219
And this is where the model restates the question more explicitly,

845
00:31:04,328 --> 00:31:06,900
adding missing context or clarifying

846
00:31:06,900 --> 00:31:10,390
intent. The

847
00:31:10,390 --> 00:31:12,789
second one is query expansion.

848
00:31:13,979 --> 00:31:16,068
Um, and this is where the mall generates multiple

849
00:31:16,068 --> 00:31:17,630
variants of the same question,

850
00:31:17,949 --> 00:31:20,229
using synonyms, related terms, and

851
00:31:20,229 --> 00:31:22,489
alternative phrasings to increase recall.

852
00:31:25,729 --> 00:31:27,910
And finally, you have query decomposition,

853
00:31:28,489 --> 00:31:30,608
where a complex multi-part question is broken

854
00:31:30,608 --> 00:31:32,650
into smaller targeted sub-queries

855
00:31:32,890 --> 00:31:34,049
that can be answered independently.

856
00:31:35,789 --> 00:31:37,868
Together these techniques dramatically enhance

857
00:31:37,868 --> 00:31:39,009
retrievable quality,

858
00:31:39,390 --> 00:31:41,848
and they can reduce hallucinations

859
00:31:42,549 --> 00:31:44,549
because you have a clear question, you get a clear

860
00:31:44,549 --> 00:31:46,789
answer. And they can help

861
00:31:46,789 --> 00:31:47,509
increase grounding.

862
00:31:47,868 --> 00:31:49,650
Combine this with

863
00:31:50,259 --> 00:31:52,608
improved retrieval and you're in a good spot

864
00:31:52,989 --> 00:31:55,209
so far. Um,

865
00:31:57,920 --> 00:32:00,078
Now, how do you know that

866
00:32:00,078 --> 00:32:02,430
your efforts in context engineering

867
00:32:02,430 --> 00:32:02,959
are working?

868
00:32:04,049 --> 00:32:06,219
Um, now consequ engineering isn't guesswork.

869
00:32:06,338 --> 00:32:07,358
You can measure it.

870
00:32:08,150 --> 00:32:10,348
And to evaluate or to validate

871
00:32:10,348 --> 00:32:12,549
that retrieval is genuinely helping them all,

872
00:32:12,910 --> 00:32:13,890
we need to evaluate 3 things.

873
00:32:14,699 --> 00:32:16,750
Um, the first one is how relevant their tri

874
00:32:16,750 --> 00:32:17,410
context is.

875
00:32:18,670 --> 00:32:20,709
The second is how well the album actually uses that

876
00:32:20,709 --> 00:32:21,299
context.

877
00:32:22,019 --> 00:32:24,059
And the third is whether the context supports

878
00:32:24,059 --> 00:32:24,799
multi-t reasoning.

879
00:32:25,650 --> 00:32:27,930
And this is where you might hear benchmarks

880
00:32:27,930 --> 00:32:29,489
like context bench,

881
00:32:29,759 --> 00:32:30,880
uh, rag,

882
00:32:31,209 --> 00:32:31,818
uh,

883
00:32:32,150 --> 00:32:34,449
rag Truth, and locomo come in. Crag is actually

884
00:32:34,449 --> 00:32:35,989
more of a technique, but there are some

885
00:32:36,449 --> 00:32:37,969
data sets that are used for evaluating it.

886
00:32:38,818 --> 00:32:41,529
Um, so each of these data sets and benchmarks

887
00:32:41,529 --> 00:32:43,828
will test a different dimension of context quality.

888
00:32:47,959 --> 00:32:48,900
Um,

889
00:32:49,279 --> 00:32:51,180
now, common retrieval metrics are

890
00:32:51,640 --> 00:32:53,309
still very useful, actually,

891
00:32:53,598 --> 00:32:55,500
in terms of evaluating,

892
00:32:56,068 --> 00:32:57,660
uh, context engineering efforts.

893
00:32:59,088 --> 00:33:01,098
And it's helpful to just kind of talk

894
00:33:01,098 --> 00:33:03,318
about this before we get into more advanced metrics.

895
00:33:03,699 --> 00:33:05,858
Um, so precision at

896
00:33:05,858 --> 00:33:06,900
K, recall at K.

897
00:33:07,959 --> 00:33:10,199
Precision at K measures how much of the retrieved context

898
00:33:10,199 --> 00:33:11,088
is actually relevant,

899
00:33:11,568 --> 00:33:13,689
and recall at K measures how much of the

900
00:33:13,689 --> 00:33:15,410
important context we successfully retrieved.

901
00:33:16,318 --> 00:33:18,439
So high precision reduces hallucinations

902
00:33:18,439 --> 00:33:20,640
and high recall ensures the agent gets all the

903
00:33:20,640 --> 00:33:21,279
necessary evidence.

904
00:33:21,890 --> 00:33:24,390
And balancing the two is the foundation of good concent engineering.

905
00:33:24,809 --> 00:33:26,969
Um, some people will, a lot of people

906
00:33:26,969 --> 00:33:28,989
will actually use a harmonic mean

907
00:33:29,170 --> 00:33:31,250
F1K as well, and it's a single metric

908
00:33:31,250 --> 00:33:33,328
that attempts to capture the trade-off between these two.

909
00:33:36,500 --> 00:33:38,598
Now I'd mentioned some, uh,

910
00:33:38,979 --> 00:33:41,068
benchmarks and some papers and evaluation data

911
00:33:41,068 --> 00:33:43,689
sets. Um, so a little bit more

912
00:33:43,689 --> 00:33:44,598
information about these three.

913
00:33:45,358 --> 00:33:47,358
Um, so valuing retrieval isn't just about

914
00:33:47,358 --> 00:33:48,269
the embeddings alone.

915
00:33:48,598 --> 00:33:51,180
You need to validate the entire retrieval pipeline.

916
00:33:51,680 --> 00:33:53,709
And benchmarks help us understand whether the

917
00:33:53,709 --> 00:33:55,799
embedding model encodes the right semantics.

918
00:33:56,509 --> 00:33:59,088
Uh, whether our re-ranker is improving grounding

919
00:33:59,670 --> 00:34:02,150
and whether the system retrieves consistent context

920
00:34:02,150 --> 00:34:03,430
across multi-tenant interactions.

921
00:34:04,709 --> 00:34:06,049
Now the three I've listed above,

922
00:34:06,430 --> 00:34:07,729
um, just a

923
00:34:08,148 --> 00:34:10,648
quick summary of each of them, uh, so context bench

924
00:34:10,909 --> 00:34:12,909
measures retrieval accuracy across different

925
00:34:12,909 --> 00:34:14,010
task categories.

926
00:34:14,938 --> 00:34:15,719
Um,

927
00:34:15,978 --> 00:34:18,539
Craig evaluates grounding and hallucination

928
00:34:18,539 --> 00:34:19,639
under realistic workloads.

929
00:34:20,789 --> 00:34:22,909
And locomo stresses the retrieval system over

930
00:34:22,909 --> 00:34:24,929
multi-turn dialogues to test context stability

931
00:34:24,929 --> 00:34:27,188
over time. So when you hear these guys mentioned,

932
00:34:27,269 --> 00:34:28,610
this is what, uh,

933
00:34:29,110 --> 00:34:30,010
they're used for.

934
00:34:31,108 --> 00:34:31,759
But

935
00:34:32,228 --> 00:34:34,028
arguably, uh, you,

936
00:34:34,309 --> 00:34:36,387
when you are doing evals in terms of what is

937
00:34:36,387 --> 00:34:38,387
working for you guys, you should test on

938
00:34:38,387 --> 00:34:38,967
your data

939
00:34:39,268 --> 00:34:41,509
because performance on benchmarks does not

940
00:34:41,509 --> 00:34:43,628
always correspond to real world

941
00:34:43,628 --> 00:34:45,068
performance for your production systems.

942
00:34:47,929 --> 00:34:49,889
Now, getting into memory injury.

943
00:34:50,800 --> 00:34:52,840
Because essentially that's what Talk of this is about. This is where I've been

944
00:34:52,840 --> 00:34:53,458
driving to.

945
00:34:54,579 --> 00:34:55,800
Um, so let's define

946
00:34:56,659 --> 00:34:58,159
what memory engineering actually is.

947
00:35:00,300 --> 00:35:01,039
Um,

948
00:35:01,500 --> 00:35:03,000
now let's go back to this

949
00:35:03,500 --> 00:35:04,079
one.

950
00:35:05,619 --> 00:35:08,099
so memory engineering is a discipline of designing the systems

951
00:35:08,099 --> 00:35:10,300
that allow these agents to accumulate experiences,

952
00:35:10,369 --> 00:35:12,469
retain knowledge over time, and develop continually

953
00:35:12,469 --> 00:35:13,179
cross stacks.

954
00:35:13,969 --> 00:35:16,059
There's a collection of methods that churn these raw

955
00:35:16,059 --> 00:35:18,079
agent interactions into structured,

956
00:35:18,099 --> 00:35:19,969
retrievable, durable memory objects,

957
00:35:20,228 --> 00:35:21,679
so the agent becomes smarter with use.

958
00:35:23,320 --> 00:35:25,559
Memory entering is about ensuring the right information is available

959
00:35:25,559 --> 00:35:27,929
later. Persistent

960
00:35:27,929 --> 00:35:29,409
and clean structured forms.

961
00:35:30,320 --> 00:35:32,688
Whereas context engineering is about retrieving the right

962
00:35:32,688 --> 00:35:33,519
information now for the agent.

963
00:35:35,898 --> 00:35:37,349
Um,

964
00:35:37,820 --> 00:35:38,519
so,

965
00:35:39,099 --> 00:35:41,378
let me talk you through the hierarchy of memory types

966
00:35:41,378 --> 00:35:42,800
that we've been seeing.

967
00:35:43,739 --> 00:35:45,369
For our users, for our customers,

968
00:35:45,820 --> 00:35:48,188
and that also been talked about, um,

969
00:35:48,300 --> 00:35:49,280
by various researchers.

970
00:35:50,688 --> 00:35:52,829
And when we talk about memory for agents, we're not just referring

971
00:35:52,829 --> 00:35:53,708
to like one thing.

972
00:35:56,208 --> 00:35:58,110
There's kind of 3 types.

973
00:35:58,929 --> 00:36:01,219
Um, there's short term memory, long-term memory,

974
00:36:01,329 --> 00:36:03,199
and shared memory,

975
00:36:03,619 --> 00:36:04,239
um.

976
00:36:05,059 --> 00:36:07,378
And these are kind of like the three core layers. So short-term

977
00:36:07,378 --> 00:36:09,360
memory is the agent's working space.

978
00:36:09,688 --> 00:36:11,780
It's the information that's held inside the reasoning loop

979
00:36:11,780 --> 00:36:14,139
for the current step or immediate sequence

980
00:36:14,139 --> 00:36:16,418
of steps. It helps agents stay focused on the local

981
00:36:16,418 --> 00:36:16,958
objective.

982
00:36:17,579 --> 00:36:19,860
Long-term memory is durable knowledge stored

983
00:36:19,860 --> 00:36:20,599
outside the model,

984
00:36:20,898 --> 00:36:22,570
enabling the agent to remember facts,

985
00:36:22,938 --> 00:36:24,478
decisions, experiences.

986
00:36:25,208 --> 00:36:27,398
And finally, in multi-agent systems,

987
00:36:27,438 --> 00:36:28,800
we also introduce shared memory.

988
00:36:29,958 --> 00:36:32,159
The gold layer that every agent reads and writes

989
00:36:32,159 --> 00:36:34,329
from, ensuring coordination, consistency,

990
00:36:34,360 --> 00:36:35,260
and collective progress.

991
00:36:35,559 --> 00:36:36,780
So these three layers

992
00:36:37,360 --> 00:36:39,719
will form the backbone of most reliable

993
00:36:39,719 --> 00:36:40,800
agentic architectures.

994
00:36:43,539 --> 00:36:45,659
Um, now we do get a lot of questions about,

995
00:36:46,219 --> 00:36:48,300
OK, well then how does, how does data and how

996
00:36:48,300 --> 00:36:50,219
these interactions become memory.

997
00:36:51,878 --> 00:36:52,679
Um,

998
00:36:53,079 --> 00:36:55,398
so for an interaction to turn into

999
00:36:55,398 --> 00:36:57,760
usable memory, it must move through a transformation

1000
00:36:57,760 --> 00:36:58,320
pipeline.

1001
00:36:59,000 --> 00:37:01,039
And the sequence looks like this. Uh,

1002
00:37:01,159 --> 00:37:03,349
first you have kind of raw inputs, right, which is

1003
00:37:03,349 --> 00:37:05,898
the queries, actions, docs, and observations.

1004
00:37:06,918 --> 00:37:08,619
These are transformed.

1005
00:37:09,039 --> 00:37:11,389
They can be summarized, they can be

1006
00:37:11,750 --> 00:37:12,679
extracted,

1007
00:37:13,000 --> 00:37:14,260
they can be embedded

1008
00:37:14,800 --> 00:37:16,760
and or normalized into a structure format.

1009
00:37:18,829 --> 00:37:19,849
They're stored.

1010
00:37:20,309 --> 00:37:22,769
They're persisted in the right collection or memory table.

1011
00:37:23,309 --> 00:37:25,188
Conversations, long-term knowledge,

1012
00:37:25,510 --> 00:37:27,610
personas, tool outputs,

1013
00:37:28,070 --> 00:37:30,188
um, and really this is kind of like

1014
00:37:30,188 --> 00:37:31,329
a circulating

1015
00:37:31,829 --> 00:37:33,250
set of stages, right?

1016
00:37:34,780 --> 00:37:35,648
We index it.

1017
00:37:36,030 --> 00:37:38,179
We make it searchable with vectors,

1018
00:37:38,300 --> 00:37:40,079
metadata, and keys so it can be

1019
00:37:40,539 --> 00:37:41,159
accessed later.

1020
00:37:42,309 --> 00:37:43,789
Um, and then retrieved.

1021
00:37:44,208 --> 00:37:46,289
We serviced it at the right moment to influence for

1022
00:37:46,289 --> 00:37:46,849
future reasoning.

1023
00:37:48,260 --> 00:37:50,760
Um, so this loop, the input, transform,

1024
00:37:50,958 --> 00:37:53,079
store, index, retrieve, is the moment when data

1025
00:37:53,079 --> 00:37:54,619
becomes memory. Um,

1026
00:37:55,119 --> 00:37:56,938
if you skip this life cycle, you're

1027
00:37:57,239 --> 00:37:59,280
not really building memory, you're just kind of like accumulating

1028
00:37:59,280 --> 00:37:59,898
logs.

1029
00:38:00,840 --> 00:38:02,958
Which are still a very crucial ingredient,

1030
00:38:03,079 --> 00:38:03,590
right,

1031
00:38:04,079 --> 00:38:05,438
for building long-term memory.

1032
00:38:07,878 --> 00:38:10,128
But memory engineering at the end of the day, it's intentional. Um,

1033
00:38:10,250 --> 00:38:12,369
you need to decide what matters, uh,

1034
00:38:12,409 --> 00:38:14,510
compress the rest and make everything else retrievable.

1035
00:38:17,659 --> 00:38:19,679
Um, now let's just talk a little bit

1036
00:38:19,679 --> 00:38:21,590
about the steps in a memory life cycle.

1037
00:38:24,269 --> 00:38:26,369
Um, so once you understand the types of memory and agent

1038
00:38:26,369 --> 00:38:28,789
needs, the next step is building the memory life cycle.

1039
00:38:29,188 --> 00:38:31,199
That operational pipeline that we talked

1040
00:38:31,199 --> 00:38:31,739
about,

1041
00:38:32,260 --> 00:38:34,619
that creates memory and ensures agents are

1042
00:38:34,619 --> 00:38:35,280
RBC.

1043
00:38:36,199 --> 00:38:37,800
Reliable, believable, capable.

1044
00:38:38,119 --> 00:38:39,458
Um, so if you're trying to figure out,

1045
00:38:39,789 --> 00:38:41,800
you know, whether you're like agent or agenttic

1046
00:38:41,800 --> 00:38:42,478
work flows

1047
00:38:42,918 --> 00:38:44,019
or agent-like system,

1048
00:38:44,519 --> 00:38:45,699
um, is working,

1049
00:38:46,280 --> 00:38:48,320
you can pull your users and just even

1050
00:38:48,320 --> 00:38:49,478
ask them along these lines.

1051
00:38:51,510 --> 00:38:53,668
You can also, for example, curate your own

1052
00:38:53,668 --> 00:38:54,789
eval data set,

1053
00:38:55,148 --> 00:38:55,750
um,

1054
00:38:56,030 --> 00:38:58,070
and essentially try to create

1055
00:38:58,070 --> 00:38:59,728
kind of measures or calculations

1056
00:39:00,030 --> 00:39:01,550
that can capture these things.

1057
00:39:06,090 --> 00:39:08,469
Um, now let's just talk about like

1058
00:39:09,010 --> 00:39:10,769
a little bit deeper into the stage of the life cycle.

1059
00:39:11,599 --> 00:39:13,949
Um, so the life cycle, like we talked about, it's,

1060
00:39:14,250 --> 00:39:16,579
you know, you write, transform, summarize,

1061
00:39:16,860 --> 00:39:17,398
embed,

1062
00:39:17,679 --> 00:39:18,639
store, index, and retrieve.

1063
00:39:20,590 --> 00:39:22,750
Um, so this loop kind of always needs to run

1064
00:39:22,750 --> 00:39:24,898
continuously because you need to ensure that the memory is always

1065
00:39:24,898 --> 00:39:25,648
up to date

1066
00:39:25,918 --> 00:39:27,969
and reflects kind of the current state.

1067
00:39:29,489 --> 00:39:30,610
Of all interactions

1068
00:39:32,099 --> 00:39:34,300
Um, now the first thing is

1069
00:39:34,458 --> 00:39:35,840
consolidation summarization, right?

1070
00:39:36,168 --> 00:39:37,000
What is that?

1071
00:39:37,418 --> 00:39:37,978
Um,

1072
00:39:38,300 --> 00:39:40,938
so consolidation is a step where noisy, raw interactions

1073
00:39:40,938 --> 00:39:42,300
become structured high signal memory.

1074
00:39:43,889 --> 00:39:46,010
Summarization extracts the meaningful parts of an

1075
00:39:46,010 --> 00:39:48,398
event, preserves the context, and removes irrelevant

1076
00:39:48,398 --> 00:39:50,769
details. Similar to how human memory compresses

1077
00:39:50,769 --> 00:39:51,289
experiences.

1078
00:39:52,579 --> 00:39:55,030
Um, without consolidation, you can definitely

1079
00:39:55,030 --> 00:39:57,179
get bloated, redundant, unusable memory.

1080
00:39:57,510 --> 00:39:59,659
Not everything's important, right? When we're, for

1081
00:39:59,659 --> 00:40:01,409
example, interacting, talking with an agent,

1082
00:40:01,789 --> 00:40:02,349
um,

1083
00:40:02,708 --> 00:40:04,780
there's definitely been times where I've had to

1084
00:40:04,780 --> 00:40:06,780
have the agent like rerun its runs,

1085
00:40:07,110 --> 00:40:09,429
because the answer it provided was not satisfactory.

1086
00:40:09,668 --> 00:40:11,750
Does the agent really need the context from

1087
00:40:11,750 --> 00:40:12,769
all those,

1088
00:40:13,309 --> 00:40:15,329
um, times that it failed?

1089
00:40:15,989 --> 00:40:18,148
Or could we structure it so that it just

1090
00:40:18,148 --> 00:40:20,188
gets like the gist of all the times

1091
00:40:20,188 --> 00:40:20,929
that it succeeded?

1092
00:40:21,668 --> 00:40:23,349
And it can continue along that path.

1093
00:40:26,159 --> 00:40:26,860
Now something

1094
00:40:27,519 --> 00:40:28,148
that

1095
00:40:28,599 --> 00:40:30,599
most of my family would argue I am amazing

1096
00:40:30,599 --> 00:40:32,860
at is uh intelligent forgetting.

1097
00:40:34,579 --> 00:40:36,599
Um, so good memory systems

1098
00:40:36,599 --> 00:40:38,958
don't store everything, they store the right things.

1099
00:40:39,418 --> 00:40:40,679
That's what I tell my husband all the time.

1100
00:40:41,840 --> 00:40:43,918
Um, so with intelligent forgetting, what that

1101
00:40:43,918 --> 00:40:46,340
means is removing low value, outdated,

1102
00:40:46,360 --> 00:40:48,478
or overwritten information so the memory

1103
00:40:48,478 --> 00:40:49,478
stays coherent and efficient.

1104
00:40:50,780 --> 00:40:53,780
Um, and obviously this can help prevent drift,

1105
00:40:53,898 --> 00:40:55,809
contamination, and contradictions,

1106
00:40:56,219 --> 00:40:58,809
especially as the agent operates across long horizons,

1107
00:40:59,019 --> 00:41:01,250
and there's a couple of different ways you could cut this, you know, you could do

1108
00:41:01,250 --> 00:41:03,599
like, um, a time to live index.

1109
00:41:03,898 --> 00:41:06,139
Um, there's a number of like ways

1110
00:41:06,139 --> 00:41:07,360
you can consider

1111
00:41:07,860 --> 00:41:08,699
intelligent forgetting.

1112
00:41:11,659 --> 00:41:12,349
Um,

1113
00:41:12,668 --> 00:41:13,889
now with retrieval.

1114
00:41:15,648 --> 00:41:16,500
What can we do here?

1115
00:41:18,409 --> 00:41:19,269
Um, so

1116
00:41:19,769 --> 00:41:21,769
even the best engineer memory is useless unless

1117
00:41:21,769 --> 00:41:23,809
the agent can find, rank, and select the right

1118
00:41:23,809 --> 00:41:24,750
pieces at the right time.

1119
00:41:25,579 --> 00:41:27,628
Retrieval determines whether the agent's next action

1120
00:41:27,628 --> 00:41:29,909
is grounded in relevant history, past

1121
00:41:29,909 --> 00:41:32,228
successes, domain knowledge, not

1122
00:41:32,228 --> 00:41:33,829
just whatever text happens to be in the prompt.

1123
00:41:35,079 --> 00:41:37,320
Um, and this is also where most memory failures actually

1124
00:41:37,320 --> 00:41:39,478
appear. Not because memory wasn't stored,

1125
00:41:39,559 --> 00:41:41,079
but because it wasn't retrieved intelligently.

1126
00:41:41,958 --> 00:41:44,059
So for example, um, I was working

1127
00:41:44,059 --> 00:41:46,079
with a user slash customer.

1128
00:41:46,398 --> 00:41:48,889
They were building a no code low code agent, right?

1129
00:41:49,438 --> 00:41:51,840
And they're like, yeah, we have to expose

1130
00:41:51,840 --> 00:41:54,039
like 100 to 200 tools

1131
00:41:54,039 --> 00:41:55,780
at runtime to our agent, and

1132
00:41:56,159 --> 00:41:58,239
they're dealing with two things. They're doing that with massive

1133
00:41:58,239 --> 00:42:00,510
tool exposure and they're also dealing

1134
00:42:00,510 --> 00:42:01,719
with, um.

1135
00:42:03,179 --> 00:42:04,360
The cold start problem,

1136
00:42:04,938 --> 00:42:07,639
which is a user comes to their app and

1137
00:42:08,019 --> 00:42:10,139
the user needs to build some kind of workflow,

1138
00:42:10,340 --> 00:42:11,760
right? Let's say for example,

1139
00:42:12,059 --> 00:42:14,418
the workflow is I need to,

1140
00:42:14,780 --> 00:42:16,360
I'm a ride sharing app

1141
00:42:16,628 --> 00:42:18,978
and I need to onboard, uh, new drivers

1142
00:42:18,978 --> 00:42:20,978
in the state of New York. I need to make

1143
00:42:20,978 --> 00:42:23,280
sure they have the background checks, all that good stuff,

1144
00:42:23,579 --> 00:42:26,000
um, and the poor like HR,

1145
00:42:26,378 --> 00:42:26,898
um.

1146
00:42:27,579 --> 00:42:28,260
IT,

1147
00:42:28,590 --> 00:42:29,719
uh, employee

1148
00:42:30,139 --> 00:42:31,958
has this like blank window,

1149
00:42:32,418 --> 00:42:34,918
100 tools and 100 different notes they could call,

1150
00:42:35,500 --> 00:42:37,239
and they're like, where do we begin?

1151
00:42:37,619 --> 00:42:40,000
And so there are two ways we could kind of solve that, right?

1152
00:42:40,579 --> 00:42:42,719
So we go back to some of the stuff I described.

1153
00:42:43,019 --> 00:42:45,059
Uh, one way was,

1154
00:42:45,139 --> 00:42:45,780
um.

1155
00:42:46,648 --> 00:42:48,869
Servicing up only like 20 tools at runtime.

1156
00:42:49,789 --> 00:42:50,938
But it's like the 20

1157
00:42:51,260 --> 00:42:53,389
most uh useful, relevant,

1158
00:42:53,519 --> 00:42:55,719
specific tools or nodes for building

1159
00:42:55,719 --> 00:42:56,639
that workflow.

1160
00:42:57,000 --> 00:42:59,289
And the way you can do that is you can

1161
00:42:59,289 --> 00:43:01,438
have good tool metadata, and you can

1162
00:43:01,438 --> 00:43:02,619
perform semantic search.

1163
00:43:03,119 --> 00:43:05,269
You have that user queryer prompt,

1164
00:43:05,719 --> 00:43:08,119
and you can essentially go find all the tools that are relevant

1165
00:43:08,119 --> 00:43:10,599
for ride sharing, they're relevant for uh

1166
00:43:10,599 --> 00:43:11,800
driver verification.

1167
00:43:12,559 --> 00:43:13,378
All the good stuff.

1168
00:43:14,119 --> 00:43:16,050
And the second way we had also

1169
00:43:16,389 --> 00:43:17,570
potentially tackled it

1170
00:43:18,030 --> 00:43:20,628
was something I had learned actually when I was at MailChimp.

1171
00:43:21,030 --> 00:43:23,309
I was working as an MLOps engineer there and we were first

1172
00:43:23,309 --> 00:43:25,438
deploying, uh, an agent to help,

1173
00:43:26,070 --> 00:43:28,409
um, small, medium sized business owners

1174
00:43:28,869 --> 00:43:30,989
create email marketing campaigns. Same deal,

1175
00:43:31,070 --> 00:43:33,228
they have a blank window. They have no

1176
00:43:33,228 --> 00:43:34,750
idea what they should put down there.

1177
00:43:35,978 --> 00:43:38,030
So one of the things that we did and that

1178
00:43:38,030 --> 00:43:40,228
one of our partners NN is amazing at

1179
00:43:40,579 --> 00:43:42,539
is, uh, we create templates.

1180
00:43:43,418 --> 00:43:45,418
Now at the time, we also, we were dealing with like hidden

1181
00:43:45,418 --> 00:43:47,719
Markov chains to kind of do this email sequencing.

1182
00:43:48,179 --> 00:43:48,820
Um,

1183
00:43:49,219 --> 00:43:51,519
but that was one of the easiest things we did was we curated

1184
00:43:51,519 --> 00:43:54,079
like a library of high quality templates

1185
00:43:54,550 --> 00:43:56,320
and essentially the user would come in,

1186
00:43:56,699 --> 00:43:57,958
um, they'd put in their,

1187
00:43:58,458 --> 00:43:59,030
you know,

1188
00:43:59,409 --> 00:44:00,458
what their need was,

1189
00:44:00,739 --> 00:44:03,039
uh, that workflow template would get serviced

1190
00:44:03,039 --> 00:44:05,099
up. And then, so rather than the agent kind

1191
00:44:05,099 --> 00:44:06,639
of recreating everything from scratch.

1192
00:44:07,099 --> 00:44:09,219
What they could do is they could just iterate on that existing

1193
00:44:09,219 --> 00:44:09,789
template

1194
00:44:10,059 --> 00:44:12,168
with the, with the agent and figure out

1195
00:44:12,168 --> 00:44:13,079
what did or didn't work.

1196
00:44:13,860 --> 00:44:15,208
Right, so

1197
00:44:15,530 --> 00:44:17,769
a lot of these techniques we're applying

1198
00:44:17,769 --> 00:44:19,510
today to help a lot of our users,

1199
00:44:20,090 --> 00:44:22,159
um, and it's really helpful to kind of think about, right,

1200
00:44:22,208 --> 00:44:24,760
when you're working on your own agentic, uh,

1201
00:44:24,769 --> 00:44:28,478
applications. Um,

1202
00:44:28,780 --> 00:44:30,840
now I showed you that kind of hierarchy.

1203
00:44:31,300 --> 00:44:33,378
I'll just do a quick deep dive into what each of those types of

1204
00:44:33,378 --> 00:44:35,418
memory are, um, just so that you kind of have them in

1205
00:44:35,418 --> 00:44:36,019
your toolkit.

1206
00:44:38,340 --> 00:44:40,969
And once again these memory types, and the agents,

1207
00:44:41,059 --> 00:44:43,099
um, these are the categories, right? These

1208
00:44:43,099 --> 00:44:44,820
are the patterns of memories that you can implement.

1209
00:44:48,099 --> 00:44:49,168
Uh

1210
00:44:50,349 --> 00:44:52,418
huh. Right, OK,

1211
00:44:52,579 --> 00:44:54,739
so, so you guys can see this and you can take a picture

1212
00:44:54,739 --> 00:44:56,019
of it. Right,

1213
00:44:56,320 --> 00:44:58,110
uh, for that coordination, shared memory,

1214
00:44:58,519 --> 00:45:00,570
just, it's shared memory. It's shared memory that is

1215
00:45:00,570 --> 00:45:01,800
solving the coordination problem.

1216
00:45:02,329 --> 00:45:04,438
Sorry, I didn't get to fixing that.

1217
00:45:04,530 --> 00:45:06,648
Um, but this is the family

1218
00:45:06,648 --> 00:45:09,079
tree of memory that we've seen being applied,

1219
00:45:09,250 --> 00:45:11,329
memory types that we've seen being applied, and also,

1220
00:45:11,628 --> 00:45:13,688
uh, that have been validated through papers

1221
00:45:13,688 --> 00:45:14,769
and so on and so forth.

1222
00:45:16,760 --> 00:45:17,539
Um,

1223
00:45:17,840 --> 00:45:19,898
so, first of all, let's talk about short-term memory.

1224
00:45:20,398 --> 00:45:22,510
Um, we've already kind of discussed what this is.

1225
00:45:22,918 --> 00:45:24,958
Humans, it works, it maps to working memory.

1226
00:45:26,639 --> 00:45:28,840
Um, so there's kind of two main types.

1227
00:45:29,898 --> 00:45:31,010
Three main types, sorry.

1228
00:45:31,590 --> 00:45:33,958
Uh, so the first one

1229
00:45:33,958 --> 00:45:36,050
is, uh, semantic cash

1230
00:45:36,668 --> 00:45:37,929
is something that we highly recommend.

1231
00:45:39,309 --> 00:45:41,389
Um, semantic cash is a scratch

1232
00:45:41,389 --> 00:45:43,708
pad for storing recent computations or answers.

1233
00:45:46,918 --> 00:45:48,969
If the agent has already generated a response, it can

1234
00:45:48,969 --> 00:45:51,289
retrieve it instantly without recomputing, reducing

1235
00:45:51,289 --> 00:45:53,110
reducing lane scene cost, so.

1236
00:45:53,958 --> 00:45:55,639
First thing to take a look at for short-term memory.

1237
00:45:57,949 --> 00:45:58,849
Um,

1238
00:45:59,309 --> 00:46:01,628
now going back to that RBC, reliability, believability

1239
00:46:01,628 --> 00:46:03,668
capability, um, if you're trying to

1240
00:46:03,668 --> 00:46:04,329
solve, for example,

1241
00:46:04,619 --> 00:46:06,809
reliability, you should take a look at short-term memory.

1242
00:46:07,389 --> 00:46:09,458
Um, it is kind of, we see it as sort of the

1243
00:46:09,458 --> 00:46:11,829
foundation of reliability because it keeps dialogue

1244
00:46:11,829 --> 00:46:13,918
coherent and prevents constant resets, at least in the short

1245
00:46:13,918 --> 00:46:14,570
term, right?

1246
00:46:15,269 --> 00:46:15,869
Um.

1247
00:46:18,409 --> 00:46:19,789
Now, long-term memory,

1248
00:46:20,208 --> 00:46:22,750
uh, this is what persists across sessions,

1249
00:46:22,860 --> 00:46:23,989
and it has 3 branches.

1250
00:46:26,849 --> 00:46:29,168
Uh, the three main types, and I

1251
00:46:29,168 --> 00:46:31,228
believe some of the announcements that actually hit on

1252
00:46:31,409 --> 00:46:32,668
some of these types as well.

1253
00:46:33,050 --> 00:46:35,449
Um, so we have procedural memory, which

1254
00:46:35,449 --> 00:46:36,949
captures how to do things,

1255
00:46:37,449 --> 00:46:39,148
uh, both workflow memory

1256
00:46:39,449 --> 00:46:41,610
and what we call toolbox memory. So what

1257
00:46:41,610 --> 00:46:43,648
tools exist now and you use them, going back to that.

1258
00:46:44,628 --> 00:46:45,820
User case study I

1259
00:46:46,389 --> 00:46:46,909
had mentioned.

1260
00:46:48,780 --> 00:46:51,019
We have episodic memory, and this records

1261
00:46:51,019 --> 00:46:51,579
what happened.

1262
00:46:52,478 --> 00:46:55,389
So it's the raw conversation episodes and their summarizations,

1263
00:46:55,639 --> 00:46:57,800
so the agent can recall prior interactions

1264
00:46:57,800 --> 00:46:58,519
without prompt bloat.

1265
00:47:00,188 --> 00:47:02,289
And then we have semantic memory, which encodes what is

1266
00:47:02,289 --> 00:47:04,418
true. The knowledge base of facts

1267
00:47:04,418 --> 00:47:06,639
and docs, um, entity memory,

1268
00:47:06,829 --> 00:47:08,398
so people, accounts and assets,

1269
00:47:08,659 --> 00:47:09,840
and persona memory,

1270
00:47:10,300 --> 00:47:11,938
uh, tone, goals, preferences.

1271
00:47:13,369 --> 00:47:15,110
We think of procedural as driving capability,

1272
00:47:15,519 --> 00:47:17,648
episodic provides continuity and semantic grounds,

1273
00:47:17,728 --> 00:47:19,070
answers, and sustains identity.

1274
00:47:22,269 --> 00:47:24,550
So this is an example of workful memory, um.

1275
00:47:25,829 --> 00:47:27,978
We see that this trail kind of lets agents

1276
00:47:27,978 --> 00:47:30,199
resume at step 8

1277
00:47:30,199 --> 00:47:30,820
of 10

1278
00:47:31,079 --> 00:47:32,079
and improve the next run.

1279
00:47:35,679 --> 00:47:38,228
And finally, just talking about shared memory,

1280
00:47:38,409 --> 00:47:40,409
um, once again, like it's

1281
00:47:40,409 --> 00:47:42,769
the coordination memory for multi-agent systems,

1282
00:47:43,168 --> 00:47:45,208
uh, common workplace where agents write,

1283
00:47:45,409 --> 00:47:46,309
read, and align.

1284
00:47:46,610 --> 00:47:48,610
Uh, you can think of this as like

1285
00:47:48,610 --> 00:47:49,590
whiteboard memory.

1286
00:47:49,849 --> 00:47:51,949
It's kind of how I treat it. So,

1287
00:47:52,449 --> 00:47:54,489
like one of the favorite parts of working as

1288
00:47:54,489 --> 00:47:55,469
an engineer, right, is

1289
00:47:55,889 --> 00:47:58,059
being in a room with a whiteboard, um, and

1290
00:47:58,059 --> 00:48:00,128
working with your peers and kind of solving

1291
00:48:00,128 --> 00:48:01,728
answers, um,

1292
00:48:02,000 --> 00:48:04,050
tracking all the interim steps, um, by

1293
00:48:04,050 --> 00:48:05,429
having it in a shared place.

1294
00:48:05,889 --> 00:48:07,628
And then getting to take a,

1295
00:48:07,929 --> 00:48:08,688
you know,

1296
00:48:09,050 --> 00:48:09,628
a

1297
00:48:10,289 --> 00:48:12,148
picture of it with your iPhone or your Android,

1298
00:48:12,929 --> 00:48:15,030
share it. Um, so that's

1299
00:48:15,030 --> 00:48:16,110
kind of what shared memory is.

1300
00:48:18,829 --> 00:48:21,409
Um, and the thing that's also tricky, right, is

1301
00:48:21,409 --> 00:48:23,530
this multi-agent memory is an individual,

1302
00:48:23,610 --> 00:48:25,030
it's shared across all agents.

1303
00:48:25,289 --> 00:48:25,989
Um,

1304
00:48:26,489 --> 00:48:27,349
so

1305
00:48:28,010 --> 00:48:30,309
this requires kind of strict scheme of discipline,

1306
00:48:30,769 --> 00:48:31,929
versioning and checks

1307
00:48:32,878 --> 00:48:35,010
to avoid drift or fragmentation across, uh,

1308
00:48:35,090 --> 00:48:35,708
agents

1309
00:48:35,969 --> 00:48:38,329
and making also you're taking care of, uh, race

1310
00:48:38,329 --> 00:48:39,090
conditions, right?

1311
00:48:41,619 --> 00:48:42,378
So,

1312
00:48:42,739 --> 00:48:43,898
once again, if we zoom out,

1313
00:48:44,300 --> 00:48:46,418
short-term memory is what keeps interactions

1314
00:48:46,418 --> 00:48:48,699
coherent. Long-term memory

1315
00:48:48,699 --> 00:48:50,829
carries continuity across sessions, and

1316
00:48:50,829 --> 00:48:52,909
shared memory enables multi-agent collaboration. So this

1317
00:48:52,909 --> 00:48:54,708
is the family tree of memory types.

1318
00:48:57,679 --> 00:48:59,188
Um, now,

1319
00:48:59,478 --> 00:49:01,679
just a quick bit, I'm not gonna go too deeply

1320
00:49:01,679 --> 00:49:03,860
into the evaluation of memory systems. We do have

1321
00:49:03,860 --> 00:49:05,820
a bunch of like technical material on that.

1322
00:49:06,228 --> 00:49:08,659
Um, but I just wanna kind of drop some

1323
00:49:08,969 --> 00:49:09,659
key terms,

1324
00:49:10,239 --> 00:49:11,639
um, in your toolbox.

1325
00:49:13,300 --> 00:49:15,579
Um, now, I mentioned RBC.

1326
00:49:15,849 --> 00:49:17,878
This is how we actually define it.

1327
00:49:18,500 --> 00:49:19,639
Go ahead and let you guys,

1328
00:49:20,139 --> 00:49:20,699
uh,

1329
00:49:20,978 --> 00:49:22,000
take a snap of that.

1330
00:49:22,340 --> 00:49:23,070
Um,

1331
00:49:23,469 --> 00:49:24,519
not to be, so.

1332
00:49:25,409 --> 00:49:27,599
To define it further, when we say reliable,

1333
00:49:27,809 --> 00:49:30,360
we mean agents that don't lose track mid-task,

1334
00:49:30,929 --> 00:49:33,059
and memory can help make their reasoning repeatable

1335
00:49:33,059 --> 00:49:33,659
and stable.

1336
00:49:34,869 --> 00:49:36,869
Believable is agents that feel

1337
00:49:36,869 --> 00:49:38,969
consistent with personas that persist

1338
00:49:38,969 --> 00:49:40,289
in interactions that adapt.

1339
00:49:40,869 --> 00:49:42,978
And finally, capable is agents that

1340
00:49:42,978 --> 00:49:44,610
expand skills over time,

1341
00:49:44,938 --> 00:49:47,269
um, by remembering tools, workflows and outcomes,

1342
00:49:47,378 --> 00:49:49,668
and also maybe introducing, a lot of times, introducing

1343
00:49:49,668 --> 00:49:51,708
things that you, the user, did not

1344
00:49:51,708 --> 00:49:53,869
expect that sort of delight.

1345
00:49:54,579 --> 00:49:56,699
Um, and memory is a through line across

1346
00:49:56,699 --> 00:49:58,699
all three. This is how you kind of get agents to be

1347
00:49:58,699 --> 00:50:02,458
RBC. Um,

1348
00:50:02,739 --> 00:50:05,079
now, why should we evaluate memory?

1349
00:50:05,260 --> 00:50:05,849
Um,

1350
00:50:06,110 --> 00:50:08,478
because there's been a lot of talk about this like online,

1351
00:50:08,898 --> 00:50:10,599
and, you know.

1352
00:50:12,079 --> 00:50:12,978
Memory is not

1353
00:50:13,679 --> 00:50:15,250
inherently like

1354
00:50:15,840 --> 00:50:17,918
the way you need to apply memory to your use

1355
00:50:17,918 --> 00:50:19,958
case is gonna be very specific. We

1356
00:50:19,958 --> 00:50:22,039
had, we have talked about those application modes that are

1357
00:50:22,039 --> 00:50:22,679
very common.

1358
00:50:22,958 --> 00:50:24,500
We've talked about different memory types,

1359
00:50:24,918 --> 00:50:27,199
um, but at the end of the day, you know, like.

1360
00:50:27,929 --> 00:50:30,199
You guys are the domain, the domain experts.

1361
00:50:30,458 --> 00:50:33,010
Um, if you're kind of building competitive

1362
00:50:33,010 --> 00:50:33,599
solutions,

1363
00:50:33,938 --> 00:50:35,938
the competitive solution isn't that you're using the same

1364
00:50:35,938 --> 00:50:37,860
LM that everyone, every, you know.

1365
00:50:38,878 --> 00:50:41,030
John Jane next to you is using, um,

1366
00:50:41,159 --> 00:50:43,458
but it's that you're personalizing it with your own unique blend

1367
00:50:43,458 --> 00:50:45,760
of data of, um,

1368
00:50:45,840 --> 00:50:47,958
domain knowledge, and so on and so forth.

1369
00:50:49,369 --> 00:50:50,820
But these are definitely

1370
00:50:51,398 --> 00:50:53,260
some of the challenges that you'll face,

1371
00:50:53,570 --> 00:50:54,119
um.

1372
00:50:55,010 --> 00:50:57,269
If, for example, you or your leadership

1373
00:50:57,780 --> 00:51:00,070
decides we don't need to invest in,

1374
00:51:00,389 --> 00:51:02,659
um, context engineering or memory engineering,

1375
00:51:03,079 --> 00:51:04,559
let's just, you know,

1376
00:51:04,918 --> 00:51:06,188
hit an LMN point,

1377
00:51:06,599 --> 00:51:08,148
ship it, call a day,

1378
00:51:08,530 --> 00:51:10,728
and then, you know, Pikachu face when

1379
00:51:10,728 --> 00:51:12,090
people aren't rushing to go try it.

1380
00:51:15,079 --> 00:51:16,978
Um, so I'd mentioned some earlier,

1381
00:51:17,438 --> 00:51:20,000
uh, benchmarks and evaldes,

1382
00:51:20,079 --> 00:51:21,978
but another two I just kind of want to introduce to you,

1383
00:51:23,300 --> 00:51:25,039
uh, is Long Me Eval and Memory Bank.

1384
00:51:26,128 --> 00:51:28,148
Um, so there are dozens of benchmarks,

1385
00:51:28,519 --> 00:51:30,539
uh, but if you kind of do

1386
00:51:30,539 --> 00:51:32,679
the 6 I've mentioned in total, that should

1387
00:51:32,679 --> 00:51:35,228
kind of give you a pretty good picture of memory performance,

1388
00:51:35,849 --> 00:51:37,909
um, from short-term context coherence

1389
00:51:37,909 --> 00:51:39,969
to long-term retention and belief

1390
00:51:39,969 --> 00:51:40,628
stability.

1391
00:51:41,829 --> 00:51:43,860
Um, so Long Memoval, uh,

1392
00:51:43,909 --> 00:51:46,148
evaluates long horizon memory retention

1393
00:51:46,148 --> 00:51:47,769
across many steps or sessions,

1394
00:51:48,148 --> 00:51:50,550
and memory bank tests structured versus unstructured

1395
00:51:50,550 --> 00:51:51,510
memory retrieval.

1396
00:51:55,219 --> 00:51:56,219
Um,

1397
00:51:56,619 --> 00:51:58,648
now, if you're doing the memory engineering

1398
00:51:58,648 --> 00:52:00,878
thing and context engineering thing really well,

1399
00:52:01,239 --> 00:52:02,878
uh, ideally you

1400
00:52:03,340 --> 00:52:04,760
should not experience these things,

1401
00:52:05,619 --> 00:52:07,619
right? The Four Horsemen of memory failure.

1402
00:52:08,820 --> 00:52:09,579
Um

1403
00:52:10,208 --> 00:52:12,530
And, you know, just a quick summary,

1404
00:52:12,610 --> 00:52:14,719
right, drift occurs when these memory

1405
00:52:14,719 --> 00:52:16,929
summaries evolve away from the source truths.

1406
00:52:17,489 --> 00:52:19,570
Overload happens when memory becomes too noisy

1407
00:52:19,570 --> 00:52:21,550
or too large for retrieval to be effective.

1408
00:52:22,500 --> 00:52:24,820
Overload happens when memory

1409
00:52:24,820 --> 00:52:25,648
becomes too noisy.

1410
00:52:26,179 --> 00:52:27,639
I'm so sorry guys, I'm tired.

1411
00:52:28,659 --> 00:52:30,688
Uh, fragmentation occurs when, uh,

1412
00:52:30,699 --> 00:52:32,699
relayed memories scatter across the store and can't be

1413
00:52:32,699 --> 00:52:33,878
assembled coherently.

1414
00:52:34,179 --> 00:52:36,539
And contamination is the most dangerous. Uh,

1415
00:52:36,610 --> 00:52:38,619
it's when incorrect or corrupted memories enter

1416
00:52:38,619 --> 00:52:40,800
the system and prompt gain to future easing. So,

1417
00:52:41,280 --> 00:52:43,079
these are kind of the four horsemen, memory failure.

1418
00:52:43,378 --> 00:52:45,119
Um, if you see them.

1419
00:52:45,559 --> 00:52:47,699
Uh, if you see this behavior,

1420
00:52:47,878 --> 00:52:50,079
then you know that you can take a look at, for example,

1421
00:52:50,199 --> 00:52:51,179
how you can architect,

1422
00:52:51,478 --> 00:52:53,739
uh, your memory pipeline, your context and memory pipeline,

1423
00:52:54,269 --> 00:52:56,360
um, and also where are the memory patterns you can

1424
00:52:56,360 --> 00:52:57,639
use for your application modes.

1425
00:53:00,349 --> 00:53:02,510
Um, now I do need to do

1426
00:53:02,510 --> 00:53:04,340
a little spiel, right, about MogadiB.

1427
00:53:04,909 --> 00:53:07,168
Um, I've given you guys kind of hopefully all the,

1428
00:53:07,349 --> 00:53:09,519
uh, insights and all the

1429
00:53:09,519 --> 00:53:11,570
kind of conceptual foundational knowledge you need

1430
00:53:11,570 --> 00:53:13,628
to understand agent memory regardless of whether it comes

1431
00:53:13,628 --> 00:53:15,769
from Mango DB or anyone else, right?

1432
00:53:17,099 --> 00:53:18,719
Um, so,

1433
00:53:19,340 --> 00:53:21,458
building stateful correction ready agents. So take all

1434
00:53:21,458 --> 00:53:22,800
the information that I had given you.

1435
00:53:23,869 --> 00:53:26,000
And um let's see where Mogadi

1436
00:53:26,000 --> 00:53:26,878
be in Lake Voyage Finn.

1437
00:53:29,559 --> 00:53:31,599
Um, so now that we understand what memory

1438
00:53:31,599 --> 00:53:32,619
engineing requires,

1439
00:53:33,030 --> 00:53:35,039
uh, let's talk about how you could implement it.

1440
00:53:36,168 --> 00:53:38,369
So, ModiB works well as a memory

1441
00:53:38,369 --> 00:53:40,478
substrate because it gives agents three things they

1442
00:53:40,478 --> 00:53:41,228
quickly need.

1443
00:53:41,728 --> 00:53:43,809
A flexible operational database for

1444
00:53:43,809 --> 00:53:44,789
structured memory,

1445
00:53:45,369 --> 00:53:47,849
native vector search for semantic retrieval, people

1446
00:53:47,849 --> 00:53:48,628
seem to forget

1447
00:53:49,050 --> 00:53:51,250
that Atlas has vector search. I'm

1448
00:53:51,250 --> 00:53:52,989
here to tell you that it does.

1449
00:53:54,070 --> 00:53:56,228
And you can do some fun things with it, including a

1450
00:53:56,228 --> 00:53:56,929
hybrid

1451
00:53:57,188 --> 00:53:59,418
search, which is full text plus

1452
00:53:59,418 --> 00:54:01,579
vector search in the same aggregation pipeline.

1453
00:54:01,829 --> 00:54:02,398
So cool.

1454
00:54:03,030 --> 00:54:03,728
Um,

1455
00:54:04,110 --> 00:54:06,590
and the third thing is high quality embeddings and re-ranking

1456
00:54:06,590 --> 00:54:08,688
models through Voyage for accuracy and grounding.

1457
00:54:09,228 --> 00:54:11,429
So instead of stitching together multiple systems, uh,

1458
00:54:11,579 --> 00:54:13,449
LongDB lets you unify long-term memory,

1459
00:54:13,860 --> 00:54:15,188
semantic context retrieval,

1460
00:54:15,628 --> 00:54:17,010
and agent state in one place.

1461
00:54:18,110 --> 00:54:20,688
And we also have a number of integrations,

1462
00:54:20,989 --> 00:54:23,059
uh, some with Asian orchestration frameworks,

1463
00:54:23,269 --> 00:54:25,530
but others with libraries like Mem Zero.

1464
00:54:29,378 --> 00:54:30,320
Um,

1465
00:54:30,648 --> 00:54:32,898
yeah, so once again, uh, something I commonly

1466
00:54:32,898 --> 00:54:35,019
get is people saying like, oh, I

1467
00:54:35,019 --> 00:54:37,260
used Atlas, uh, 8

1468
00:54:37,260 --> 00:54:39,260
years ago. I didn't know you guys could do

1469
00:54:39,260 --> 00:54:41,458
that. Yes, we can. Um, and we'll

1470
00:54:41,458 --> 00:54:43,458
continue to evolve as the, as the

1471
00:54:43,458 --> 00:54:44,320
needs require.

1472
00:54:45,429 --> 00:54:46,019
Um,

1473
00:54:46,320 --> 00:54:46,860
so

1474
00:54:47,119 --> 00:54:49,429
agents need schemas that match memory types,

1475
00:54:49,599 --> 00:54:51,699
and because of the flexible document model,

1476
00:54:51,719 --> 00:54:54,119
you can kind of evolve these schemas as the agent learns

1477
00:54:54,119 --> 00:54:55,019
new things over time

1478
00:54:55,398 --> 00:54:56,389
or as as the programs.

1479
00:54:57,119 --> 00:54:59,119
So I did a session yesterday with, uh, Jeff

1480
00:54:59,119 --> 00:54:59,860
Singer from,

1481
00:55:00,119 --> 00:55:02,199
uh, DraftKings, and I really loved

1482
00:55:02,199 --> 00:55:03,340
what he said, which is that

1483
00:55:04,000 --> 00:55:06,519
Modi has been really helpful in cases where, honestly

1484
00:55:06,519 --> 00:55:08,619
they've had to like evolve the schema,

1485
00:55:08,840 --> 00:55:10,840
and those requirements have changed really quickly

1486
00:55:10,840 --> 00:55:13,000
over time. And so I thought that was really delightful

1487
00:55:13,000 --> 00:55:18,219
here. Um,

1488
00:55:18,300 --> 00:55:20,378
and everything we've talked about, retrieval,

1489
00:55:20,458 --> 00:55:22,619
memory layers, in indexing, and eval

1490
00:55:22,619 --> 00:55:24,800
comes together in a unified memory engine.

1491
00:55:25,530 --> 00:55:27,070
ModiB supports his entire life cycle.

1492
00:55:27,938 --> 00:55:29,969
Um, and with Voyage,

1493
00:55:30,269 --> 00:55:31,090
uh,

1494
00:55:31,458 --> 00:55:33,668
you know, on the left, this is probably

1495
00:55:33,668 --> 00:55:35,128
what your pipelines look like now,

1496
00:55:35,869 --> 00:55:38,228
um, but on the right is kind of this

1497
00:55:38,228 --> 00:55:40,429
world that we're running towards, um, and

1498
00:55:40,429 --> 00:55:42,469
that a lot of our customers are adapting

1499
00:55:42,469 --> 00:55:44,648
to, which is essentially having all

1500
00:55:44,648 --> 00:55:45,550
this under the same hood.

1501
00:55:49,099 --> 00:55:51,418
Um, some malls that we offer, I'd highly

1502
00:55:51,418 --> 00:55:53,510
recommend you guys check out context 3, especially

1503
00:55:53,510 --> 00:55:55,539
if you have like long document

1504
00:55:55,539 --> 00:55:56,099
issues,

1505
00:55:56,458 --> 00:55:58,559
um, and also multi-modal 3 as well.

1506
00:56:01,668 --> 00:56:03,869
So, to end, uh, so memory injuring is

1507
00:56:03,869 --> 00:56:05,159
now a real discipline.

1508
00:56:05,510 --> 00:56:07,469
It really is. It's not just cos I want it to be.

1509
00:56:08,309 --> 00:56:10,648
Um, try to not make fetch a thing, right?

1510
00:56:10,829 --> 00:56:11,409
Um,

1511
00:56:11,789 --> 00:56:14,030
but memorying forces us to think beyond prompting

1512
00:56:14,030 --> 00:56:16,110
context size and instead design systems that

1513
00:56:16,110 --> 00:56:18,269
allow agents to accumulate this experience to

1514
00:56:18,269 --> 00:56:19,929
help your POCs or your,

1515
00:56:20,349 --> 00:56:21,550
uh, you know,

1516
00:56:22,309 --> 00:56:24,219
projects become that 5% that succeed.

1517
00:56:25,789 --> 00:56:27,909
And this is how we shift from LM applications to

1518
00:56:27,909 --> 00:56:30,059
actual intelligent agents. So I didn't lie to you guys,

1519
00:56:30,309 --> 00:56:32,510
right? Uh, the question, the answer

1520
00:56:32,510 --> 00:56:34,610
to the question of the original title was

1521
00:56:34,610 --> 00:56:35,289
agent memory.

1522
00:56:36,849 --> 00:56:38,228
Um, now,

1523
00:56:38,530 --> 00:56:39,599
3 key takeaways.

1524
00:56:39,889 --> 00:56:41,889
Uh, first, context is not memory, you kind of need

1525
00:56:41,889 --> 00:56:42,628
to separate,

1526
00:56:42,929 --> 00:56:44,309
uh, visibility from durability.

1527
00:56:45,099 --> 00:56:47,168
Uh, second, memory must be engineered.

1528
00:56:47,800 --> 00:56:49,469
Schemas, pipelines, and eval loops.

1529
00:56:50,840 --> 00:56:52,918
And 3rd, agents only become reliable and

1530
00:56:52,918 --> 00:56:55,079
useful when their memory systems are reliable

1531
00:56:55,079 --> 00:56:59,398
and useful. Yeah.

1532
00:57:01,239 --> 00:57:03,398
Cool. Um, so if you wanna learn more, uh,

1533
00:57:03,559 --> 00:57:04,800
so you scan the QR code,

1534
00:57:05,159 --> 00:57:07,659
uh, we do a blog post, um,

1535
00:57:07,958 --> 00:57:09,340
about shared memory,

1536
00:57:09,639 --> 00:57:11,668
but that will lead you to other resources around

1537
00:57:11,668 --> 00:57:12,559
ancient memory as well.

1538
00:57:12,918 --> 00:57:14,579
So highly encourage you to check it out.

1539
00:57:17,840 --> 00:57:19,918
And thank you for attending and

1540
00:57:19,918 --> 00:57:21,139
our lovely host would

1541
00:57:21,398 --> 00:57:23,398
love for me to remind you guys that you can take a

1542
00:57:23,398 --> 00:57:25,599
session survey, so if you absolutely hated

1543
00:57:25,599 --> 00:57:26,780
it, just let me know.

1544
00:57:27,280 --> 00:57:29,519
But if you loved it, no, I'm kidding, uh, don't,

1545
00:57:29,679 --> 00:57:31,719
don't let me know if you hated it, but if you did

1546
00:57:31,719 --> 00:57:33,889
love it, please do fill out the survey. Um,

1547
00:57:34,039 --> 00:57:36,239
we love to do these kinds of

1548
00:57:36,239 --> 00:57:37,820
educational sessions and we hope to do more,

1549
00:57:38,438 --> 00:57:38,639
so.

