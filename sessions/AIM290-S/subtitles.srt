1
00:00:00,810 --> 00:00:03,330
- Thank you for joining our session today.

2
00:00:03,330 --> 00:00:05,850
The best session of today, of course.

3
00:00:05,850 --> 00:00:08,280
What is it that we will be
discussing today, right?

4
00:00:08,280 --> 00:00:10,110
So I'm more than happy
to introduce myself.

5
00:00:10,110 --> 00:00:12,120
My name is Diego, I'm part of Intel,

6
00:00:12,120 --> 00:00:15,420
and I'm part of the AWS
team responsible for AIML.

7
00:00:15,420 --> 00:00:18,150
And our session today and
what we thought about,

8
00:00:18,150 --> 00:00:20,130
what we want actually to speak about,

9
00:00:20,130 --> 00:00:21,990
and get also our customers and partners

10
00:00:21,990 --> 00:00:24,630
to a little bit educate
everyone here in the room

11
00:00:24,630 --> 00:00:29,100
is how do we build fast, cost
efficient, and also important,

12
00:00:29,100 --> 00:00:33,599
sovereign influence
platforms using Intel CPUs.

13
00:00:33,599 --> 00:00:36,990
And to do that, I think I'm
really excited to announce today

14
00:00:36,990 --> 00:00:39,360
that we have a really amazing
lineup of speakers, right?

15
00:00:39,360 --> 00:00:44,010
So we will be having a session
of three different use cases.

16
00:00:44,010 --> 00:00:47,700
We will be having Caitlin
Anderson, who is the VP from Intel

17
00:00:47,700 --> 00:00:49,710
and general manager for American sales.

18
00:00:49,710 --> 00:00:52,830
We have Mickey that will
be joining us from AWS

19
00:00:52,830 --> 00:00:55,560
and who is a director
for enterprise technology

20
00:00:55,560 --> 00:00:58,890
specifically for gen AI
in the public sector.

21
00:00:58,890 --> 00:01:01,140
We will be then having
three different customers

22
00:01:01,140 --> 00:01:04,440
which will be Deloitte, Bob
Simmons with us on stage,

23
00:01:04,440 --> 00:01:09,120
Amit Gupta, VP from e& Enterprise,
and at last also Renato,

24
00:01:09,120 --> 00:01:11,193
who is Head of Technology from Articul8.

25
00:01:12,060 --> 00:01:14,310
What can you expect from
our session today, right?

26
00:01:14,310 --> 00:01:16,590
So we have 60 minutes,
we have a long time,

27
00:01:16,590 --> 00:01:19,200
and I think we hope we make it as best

28
00:01:19,200 --> 00:01:20,910
and as efficient as possible.

29
00:01:20,910 --> 00:01:22,970
We will introduce actually
four different flows, right?

30
00:01:22,970 --> 00:01:25,020
So on the one side, I
think we want to give

31
00:01:25,020 --> 00:01:27,840
a little reminder Intel and AWS, right?

32
00:01:27,840 --> 00:01:29,400
So what is it exact that we do?

33
00:01:29,400 --> 00:01:31,140
How do we actually want to achieve

34
00:01:31,140 --> 00:01:34,800
inference performance
on Intel CPUs with AWS?

35
00:01:34,800 --> 00:01:36,810
And then secondly, we will deep dive

36
00:01:36,810 --> 00:01:39,330
into three different use cases.

37
00:01:39,330 --> 00:01:41,880
On the one side, we will be
doing an analysis actually

38
00:01:41,880 --> 00:01:45,930
about how to use EC2 Intel-powered
instances for inference.

39
00:01:45,930 --> 00:01:49,410
On the second side, we will
elaborate of how to develop

40
00:01:49,410 --> 00:01:52,740
a sovereign inference platform
jointly with e& Enterprise.

41
00:01:52,740 --> 00:01:55,860
And at last, we'll talk
about AI innovation

42
00:01:55,860 --> 00:01:57,570
actually with Articul8.

43
00:01:57,570 --> 00:02:00,120
And now having said all
of this, I'm super happy

44
00:02:00,120 --> 00:02:02,520
to announce today actually
on stage, Caitlin,

45
00:02:02,520 --> 00:02:03,960
who will be talking a little bit more

46
00:02:03,960 --> 00:02:07,050
about the partnership
between Intel and AWS.

47
00:02:07,050 --> 00:02:08,370
Welcome, Caitlin.

48
00:02:08,370 --> 00:02:09,840
- Thank you, Diego.

49
00:02:09,840 --> 00:02:10,980
Wonderful.

50
00:02:10,980 --> 00:02:14,310
Well, thank you so much, and
thank you for joining us today.

51
00:02:14,310 --> 00:02:16,530
We're gonna talk a little bit
just about the partnership,

52
00:02:16,530 --> 00:02:17,880
and I'll start there.

53
00:02:17,880 --> 00:02:20,790
Again, I've been at Intel almost 24 years,

54
00:02:20,790 --> 00:02:25,260
so basically our
partnership with AWS spans

55
00:02:25,260 --> 00:02:28,050
my entire career, almost 20 years.

56
00:02:28,050 --> 00:02:31,050
A lot of the work that we do,
and many people don't think

57
00:02:31,050 --> 00:02:33,630
about kind of the complexity
of the relationship

58
00:02:33,630 --> 00:02:34,893
that we have together.

59
00:02:35,970 --> 00:02:38,040
Really, from an
infrastructure perspective,

60
00:02:38,040 --> 00:02:39,477
a lot of people would think about Intel

61
00:02:39,477 --> 00:02:43,740
and the hardware that we build
and design and manufacture,

62
00:02:43,740 --> 00:02:46,710
and obviously sell into AWS.

63
00:02:46,710 --> 00:02:51,710
We have over 400 instances of
EC2 that are running Intel,

64
00:02:52,740 --> 00:02:56,880
which is really... we have
this huge breadth of compute

65
00:02:56,880 --> 00:02:59,190
and technology when you
talk about the hardware

66
00:02:59,190 --> 00:03:00,750
and the infrastructure.

67
00:03:00,750 --> 00:03:05,010
But even more important,
especially in the world of AI now,

68
00:03:05,010 --> 00:03:08,700
is a lot of the optimization
we do with the software,

69
00:03:08,700 --> 00:03:11,610
with the third-party
companies, like many of you

70
00:03:11,610 --> 00:03:14,370
in the room, and that's what
we're gonna talk about today,

71
00:03:14,370 --> 00:03:17,160
to basically optimize those instances

72
00:03:17,160 --> 00:03:20,550
to run really well on Intel,

73
00:03:20,550 --> 00:03:23,310
AI workloads, as well as general compute,

74
00:03:23,310 --> 00:03:26,310
I should say, across all of the services

75
00:03:26,310 --> 00:03:27,933
that Amazon delivers.

76
00:03:28,950 --> 00:03:30,390
And then, of course, the devices,

77
00:03:30,390 --> 00:03:33,570
so the breadth by which
we design our chips

78
00:03:33,570 --> 00:03:38,570
and our products cover
the edge to the AIPC,

79
00:03:38,910 --> 00:03:41,130
to a lot of the cloud, and of course,

80
00:03:41,130 --> 00:03:42,720
data center on-prem as well.

81
00:03:42,720 --> 00:03:47,400
So as we think about AWS and
the span of all the devices

82
00:03:47,400 --> 00:03:49,287
and the services that they offer across

83
00:03:49,287 --> 00:03:52,530
the breadth of the
ecosystem, Intel participates

84
00:03:52,530 --> 00:03:53,640
in every one of those things.

85
00:03:53,640 --> 00:03:56,400
So it's a really dynamic
and important partnership.

86
00:03:56,400 --> 00:03:57,990
It's growing by the minute.

87
00:03:57,990 --> 00:04:00,180
We have a lot that we do
together strategically

88
00:04:00,180 --> 00:04:03,390
across both of our companies,
and I'm super excited

89
00:04:03,390 --> 00:04:04,833
for what's ahead as well.

90
00:04:06,750 --> 00:04:08,945
Let's roll the video and talk about it.

91
00:04:08,945 --> 00:04:12,870
I want you to hear it from our
CEO, Lip-Bu, and Dave Brown,

92
00:04:12,870 --> 00:04:14,797
who's the VP of Compute for AWS.

93
00:04:32,456 --> 00:04:35,340
- AWS has a longstanding
relationship with Intel,

94
00:04:35,340 --> 00:04:37,890
actually dating back to
the launch of Amazon EC2

95
00:04:37,890 --> 00:04:39,960
all the way in 2006.

96
00:04:39,960 --> 00:04:42,630
- Intel relationship with AWS span more

97
00:04:42,630 --> 00:04:45,540
than 18 years and continue to grow.

98
00:04:45,540 --> 00:04:48,570
Our collaboration runs deep.

99
00:04:48,570 --> 00:04:52,140
- Today, I'm excited to talk
about our most recent launch,

100
00:04:52,140 --> 00:04:55,383
AWS 8th generation Intel-based instances.

101
00:04:57,570 --> 00:05:01,470
- Our latest Xeon 6 with P-Core technology

102
00:05:01,470 --> 00:05:05,700
delivers reliability and
performance to cloud providers

103
00:05:05,700 --> 00:05:08,043
like AWS and their customers.

104
00:05:10,320 --> 00:05:13,530
- The new Intel instances
offer up to 20% performance

105
00:05:13,530 --> 00:05:16,590
improvement over previous
generation Intel-based instances

106
00:05:16,590 --> 00:05:18,600
across diverse workloads.

107
00:05:18,600 --> 00:05:20,610
For real-world applications, we believe

108
00:05:20,610 --> 00:05:22,743
the performance gains will be even higher.

109
00:05:25,230 --> 00:05:27,540
- This isn't just about migrating

110
00:05:27,540 --> 00:05:29,940
existing workloads to the cloud.

111
00:05:29,940 --> 00:05:33,150
It is about unlocking new possibilities

112
00:05:33,150 --> 00:05:37,143
through nimble and data-driven
development pipelines.

113
00:05:39,210 --> 00:05:41,607
- With the 8th generation Intel instances,

114
00:05:41,607 --> 00:05:44,730
AWS is adding support for
Instance Bandwidth Configuration,

115
00:05:44,730 --> 00:05:48,180
or IBC, a new feature
that we launched last year

116
00:05:48,180 --> 00:05:50,730
with our 8th generation
Graviton instances.

117
00:05:50,730 --> 00:05:53,730
IBC will give x86
customers the flexibility

118
00:05:53,730 --> 00:05:56,670
to optimize their network
and storage performance

119
00:05:56,670 --> 00:05:59,100
based on their specific workload needs.

120
00:05:59,100 --> 00:06:02,070
Our 8th generation custom
Intel Xeon 6 processor

121
00:06:02,070 --> 00:06:05,580
CM&R instances offer new Flex variants

122
00:06:05,580 --> 00:06:07,380
that have been designed
for customers who workloads

123
00:06:07,380 --> 00:06:09,720
have variable compute patterns.

124
00:06:09,720 --> 00:06:11,670
They offer a cost-effective alternative

125
00:06:11,670 --> 00:06:14,550
to standard instances and
provide the easiest way

126
00:06:14,550 --> 00:06:17,150
for customers to achieve
price performance benefits.

127
00:06:18,420 --> 00:06:21,240
- The future of cloud
computing will require

128
00:06:21,240 --> 00:06:26,240
even more powerful, flexible,
and efficient solutions.

129
00:06:26,280 --> 00:06:28,620
And Intel is proud to be driving

130
00:06:28,620 --> 00:06:31,533
that transformation alongside AWS.

131
00:06:33,120 --> 00:06:35,610
- We're excited to see how
our customers will use them

132
00:06:35,610 --> 00:06:37,310
to drive their businesses forward.

133
00:06:43,980 --> 00:06:45,360
- Wonderful.

134
00:06:45,360 --> 00:06:46,530
So thank you very much.

135
00:06:46,530 --> 00:06:50,310
And also, I mentioned this
is Intel's AI strategy,

136
00:06:50,310 --> 00:06:53,190
but really a little bit of
our philosophy as a company.

137
00:06:53,190 --> 00:06:55,830
For those that have worked
with Intel for many years,

138
00:06:55,830 --> 00:07:00,150
we have always had an open
ecosystem approach to innovation.

139
00:07:00,150 --> 00:07:01,950
That's a big part of our promise.

140
00:07:01,950 --> 00:07:04,560
It's a big part of actually,
we're spending a lot of money

141
00:07:04,560 --> 00:07:09,360
as a company now to invest
in the x86 ecosystem

142
00:07:09,360 --> 00:07:11,490
to make sure that it's strong and healthy.

143
00:07:11,490 --> 00:07:14,970
And that means that we
always really kind of start

144
00:07:14,970 --> 00:07:17,700
with an open ecosystem approach a lot.

145
00:07:17,700 --> 00:07:19,350
And we'll talk a lot about the software

146
00:07:19,350 --> 00:07:22,980
that we are developing
to deliver and contribute

147
00:07:22,980 --> 00:07:24,660
to the open source community as well.

148
00:07:24,660 --> 00:07:25,983
When you talk about AI.

149
00:07:27,450 --> 00:07:29,760
'Cause at the end of the day,

150
00:07:29,760 --> 00:07:31,710
we've been through many
cycles of innovation

151
00:07:31,710 --> 00:07:34,080
and AI is certainly one.

152
00:07:34,080 --> 00:07:36,690
We liken it sometimes to
the wireless innovation

153
00:07:36,690 --> 00:07:39,360
when things change in the PC dramatically.

154
00:07:39,360 --> 00:07:42,660
All of that required the
ecosystem to come along.

155
00:07:42,660 --> 00:07:46,410
One company cannot deliver
a movement like that.

156
00:07:46,410 --> 00:07:47,610
And that's what AI is.

157
00:07:47,610 --> 00:07:50,970
All the entire ecosystem,
when we really have

158
00:07:50,970 --> 00:07:53,280
this open approach and
not these verticalized

159
00:07:53,280 --> 00:07:56,220
kind of locked in systems,
that's when innovation

160
00:07:56,220 --> 00:07:59,130
will thrive and go faster and scale.

161
00:07:59,130 --> 00:08:02,220
And that's what we believe
will, kind of at the beginning

162
00:08:02,220 --> 00:08:05,490
stages of this when you
talk about enterprise AI.

163
00:08:05,490 --> 00:08:07,500
Of course, we want it to be efficient.

164
00:08:07,500 --> 00:08:10,770
We want the TCO to be very
good for your companies.

165
00:08:10,770 --> 00:08:13,200
And I know that you're
facing that as you're making

166
00:08:13,200 --> 00:08:16,530
your own decisions on how to
develop AI for yourselves,

167
00:08:16,530 --> 00:08:19,140
but also to deploy it to your customers.

168
00:08:19,140 --> 00:08:20,610
And of course, we want it to be secure.

169
00:08:20,610 --> 00:08:23,700
We have a long history
in the enterprise market.

170
00:08:23,700 --> 00:08:25,560
We know how to drive security

171
00:08:25,560 --> 00:08:28,170
and how to deliver that for
our enterprise customers,

172
00:08:28,170 --> 00:08:30,750
and that's a bedrock of what we do across

173
00:08:30,750 --> 00:08:32,583
both the hardware and the software.

174
00:08:34,590 --> 00:08:37,140
Okay, and then many people
may or may not know,

175
00:08:37,140 --> 00:08:39,436
we've been working a lot.

176
00:08:39,436 --> 00:08:42,270
We've always had a strong ISV ecosystem,

177
00:08:42,270 --> 00:08:46,200
so part of that ecosystem work
that we do has been to invest

178
00:08:46,200 --> 00:08:49,410
in a lot of the software
providers for many years

179
00:08:49,410 --> 00:08:51,300
to develop on Intel.

180
00:08:51,300 --> 00:08:54,000
But as AI has grown both on the PC

181
00:08:54,000 --> 00:08:55,740
and in our data center business,

182
00:08:55,740 --> 00:08:59,460
we have strengthened and
really tried to invest

183
00:08:59,460 --> 00:09:01,380
a lot more time and energy and money

184
00:09:01,380 --> 00:09:06,380
into enabling and
supporting our ISV customers

185
00:09:07,140 --> 00:09:11,272
across to be not only
just enabled and developed

186
00:09:11,272 --> 00:09:15,330
on Intel and optimized
for Intel instances,

187
00:09:15,330 --> 00:09:17,370
but also to work with AWS as well.

188
00:09:17,370 --> 00:09:19,920
And there's a lot of really great joint

189
00:09:19,920 --> 00:09:22,680
three-way partnerships
that we have as we think

190
00:09:22,680 --> 00:09:24,630
about solutions selling into the market.

191
00:09:24,630 --> 00:09:27,868
So Adobe is just one example.

192
00:09:27,868 --> 00:09:30,390
Integrating AI into the creative tools,

193
00:09:30,390 --> 00:09:35,370
we work very closely with
Adobe on all of their use cases

194
00:09:35,370 --> 00:09:38,850
and making sure that it
can run the best on Intel.

195
00:09:38,850 --> 00:09:43,080
SAP is a huge customer of
ours as we think about the AI

196
00:09:43,080 --> 00:09:45,990
solutions that they're
delivering to market.

197
00:09:45,990 --> 00:09:48,840
And Arqit may be one
you know or don't know,

198
00:09:48,840 --> 00:09:51,570
but really quantum safe encryption,

199
00:09:51,570 --> 00:09:53,550
some a lot in security.

200
00:09:53,550 --> 00:09:57,030
And these are just a few
examples of companies

201
00:09:57,030 --> 00:09:59,130
where we've been partnering
to create programs

202
00:09:59,130 --> 00:10:01,830
through our channel programs to make sure

203
00:10:01,830 --> 00:10:05,760
that they can get access
to tools and technology,

204
00:10:05,760 --> 00:10:07,890
but also to programs and marketing

205
00:10:07,890 --> 00:10:10,890
so they can scale their efforts.

206
00:10:10,890 --> 00:10:13,560
And many of these are also AWS customers.

207
00:10:13,560 --> 00:10:16,080
So a lot of work in the ISV ecosystem.

208
00:10:16,080 --> 00:10:18,870
You're gonna hear from a
few of them today as well.

209
00:10:18,870 --> 00:10:21,693
And so we can bring a little
of those stories to life.

210
00:10:23,430 --> 00:10:25,920
And then lastly, I know it probably sounds

211
00:10:25,920 --> 00:10:28,200
really convenient that we would say CPUs

212
00:10:28,200 --> 00:10:31,920
can be used for AI because
we obviously develop CPUs.

213
00:10:31,920 --> 00:10:34,470
But really at the end of the day,

214
00:10:34,470 --> 00:10:36,900
and I've been tracking this,

215
00:10:36,900 --> 00:10:39,507
I started out really
kind of thinking about

216
00:10:39,507 --> 00:10:43,353
the AIPC space and moving
over into data center as well.

217
00:10:44,430 --> 00:10:46,560
You know, the AI revolution right now

218
00:10:46,560 --> 00:10:48,240
has been heavy on training.

219
00:10:48,240 --> 00:10:51,300
There's a lot of a handful of companies

220
00:10:51,300 --> 00:10:53,763
really developing these amazing models,

221
00:10:54,600 --> 00:10:57,330
training and doing some
of this hardcore AI work

222
00:10:57,330 --> 00:10:59,310
to build the foundation.

223
00:10:59,310 --> 00:11:03,030
But we are at this point
now where enterprise scale

224
00:11:03,030 --> 00:11:04,290
really hasn't happened yet.

225
00:11:04,290 --> 00:11:06,840
Enterprise AI in the way that I think

226
00:11:06,840 --> 00:11:08,640
that we all envisioned for the future.

227
00:11:08,640 --> 00:11:09,960
And it's just beginning.

228
00:11:09,960 --> 00:11:13,710
And a lot of that is gonna
be on inference workloads.

229
00:11:13,710 --> 00:11:15,630
When you think about
the next kind of phase

230
00:11:15,630 --> 00:11:18,870
or chapter of AI, we believe a few things.

231
00:11:18,870 --> 00:11:21,300
We believe that inference workloads

232
00:11:21,300 --> 00:11:24,753
will definitely be growing
at a significant rate.

233
00:11:26,340 --> 00:11:29,010
We certainly believe that agentic AI

234
00:11:29,010 --> 00:11:31,770
is gonna be a really
big piece of the theme

235
00:11:31,770 --> 00:11:33,720
of the next couple of years.

236
00:11:33,720 --> 00:11:36,450
And that we also believe that it will run

237
00:11:36,450 --> 00:11:39,150
across a variety of compute.

238
00:11:39,150 --> 00:11:41,310
That it's not just a GPU workload

239
00:11:41,310 --> 00:11:42,990
at the end of the day.

240
00:11:42,990 --> 00:11:46,800
That you're gonna need all
sorts of types of hardware

241
00:11:46,800 --> 00:11:49,440
to drive the right TCO and ROI to run

242
00:11:49,440 --> 00:11:50,760
these inference workloads.

243
00:11:50,760 --> 00:11:53,190
And there is a lot of great examples

244
00:11:53,190 --> 00:11:56,760
of how inference workloads
can run best on Xeon.

245
00:11:56,760 --> 00:12:00,090
And we've just launched
our Xeon 6 processor,

246
00:12:00,090 --> 00:12:02,250
which is our latest generation.

247
00:12:02,250 --> 00:12:05,760
And we'll show you
actually how you can run

248
00:12:05,760 --> 00:12:08,520
a very simple inference
workload directly on Xeon.

249
00:12:08,520 --> 00:12:10,280
I know I can say that,
but we wanted to show you

250
00:12:10,280 --> 00:12:12,063
so you have the visual.

251
00:12:13,140 --> 00:12:15,750
So just behind me, this is just an example

252
00:12:15,750 --> 00:12:20,750
of just a simple chat kind of use case

253
00:12:20,970 --> 00:12:23,370
where they're searching for information.

254
00:12:23,370 --> 00:12:26,013
And this is all running directly on Xeon.

255
00:12:29,338 --> 00:12:30,171
If we go back...

256
00:12:35,730 --> 00:12:37,923
I think it's a video.

257
00:12:42,900 --> 00:12:43,733
Nope.

258
00:12:45,480 --> 00:12:48,240
Well, it is running a
video in the background,

259
00:12:48,240 --> 00:12:50,283
but we'll have to keep moving there.

260
00:12:52,410 --> 00:12:56,250
But simply, I think from
a lot of the use cases

261
00:12:56,250 --> 00:12:57,870
that you're thinking
about for your company,

262
00:12:57,870 --> 00:13:00,000
we're gonna talk to you
about how they can run

263
00:13:00,000 --> 00:13:01,470
on Xeon as well.

264
00:13:01,470 --> 00:13:04,950
And for that, I wanted
to invite Miki on stage

265
00:13:04,950 --> 00:13:07,800
from AWS to share a little
bit of his perspective

266
00:13:07,800 --> 00:13:08,750
on the partnership.

267
00:13:09,690 --> 00:13:11,315
Welcome.

268
00:13:11,315 --> 00:13:12,276
- Thank you, Caitlin.

269
00:13:12,276 --> 00:13:15,037
- Thank you.

270
00:13:16,770 --> 00:13:21,770
- The Intel and AWS partnership
has been strong since 2006.

271
00:13:23,610 --> 00:13:27,720
And we've been collaborating to develop

272
00:13:27,720 --> 00:13:30,480
transformative technology and innovation

273
00:13:30,480 --> 00:13:34,173
that supports our customers'
enterprise workloads.

274
00:13:36,360 --> 00:13:40,750
AWS has launched 400-plus
Intel-based instances

275
00:13:41,910 --> 00:13:44,700
to serve the diverse
needs of its customers

276
00:13:44,700 --> 00:13:47,430
across all of its regions.

277
00:13:47,430 --> 00:13:51,393
And this partnership continues
to grow and to scale.

278
00:13:52,560 --> 00:13:54,633
Back in September 2024,

279
00:13:55,590 --> 00:13:59,520
we signed a collaborative multi-billion,

280
00:14:01,290 --> 00:14:06,290
multi-year strategic agreement
to invest in chip-making.

281
00:14:07,620 --> 00:14:10,743
And that partnership
is delivering results.

282
00:14:12,360 --> 00:14:17,033
I'm very excited to talk
about the custom Xeon 6

283
00:14:18,270 --> 00:14:22,530
on Intel 3 that is now generally available

284
00:14:22,530 --> 00:14:25,803
on Amazon EC2 8i instances.

285
00:14:27,660 --> 00:14:32,660
This is across the C, the R,
and the M family of instances.

286
00:14:34,500 --> 00:14:37,260
And I really want to talk about the fact

287
00:14:37,260 --> 00:14:39,120
that this is custom.

288
00:14:39,120 --> 00:14:42,510
There is great engineering collaboration

289
00:14:42,510 --> 00:14:46,350
that's gone into developing a purposeful,

290
00:14:46,350 --> 00:14:51,350
purpose-built chip that allows
us to bring unique advantages

291
00:14:51,960 --> 00:14:56,580
to our customers, giving
them the highest performance

292
00:14:56,580 --> 00:14:59,640
as well as the fastest memory bandwidth

293
00:14:59,640 --> 00:15:01,233
and delivering value to them.

294
00:15:02,280 --> 00:15:06,210
Notably, this is 20% faster

295
00:15:06,210 --> 00:15:08,193
from a compute performance perspective.

296
00:15:09,900 --> 00:15:13,170
And when it comes to inferencing-based

297
00:15:13,170 --> 00:15:18,120
recommendation models, it's
showing 40% better performance

298
00:15:18,120 --> 00:15:19,833
to the older generations.

299
00:15:20,670 --> 00:15:22,440
This is truly fascinating.

300
00:15:22,440 --> 00:15:27,440
We have the fastest Intel chip
on the cloud running in AWS.

301
00:15:29,490 --> 00:15:34,490
But I also wanna talk about
how there are AI capabilities

302
00:15:35,550 --> 00:15:39,183
and acceleration capabilities
in this new technology.

303
00:15:40,650 --> 00:15:45,650
The AMX Xeon 6 Intel chip uses
advanced matrix extension.

304
00:15:50,940 --> 00:15:55,500
And what this does is it
allows you to do optimization

305
00:15:55,500 --> 00:15:59,640
of the matrix multiplication
that provides much,

306
00:15:59,640 --> 00:16:03,420
much faster inferencing and
much, much faster training

307
00:16:03,420 --> 00:16:08,280
and it also opens up the array,
a wide array of workloads

308
00:16:08,280 --> 00:16:10,740
through which inferencing can be done.

309
00:16:10,740 --> 00:16:13,950
And this really opens up the business

310
00:16:13,950 --> 00:16:16,053
to a large set of industries.

311
00:16:16,980 --> 00:16:19,530
It leads to faster training
and faster inferencing

312
00:16:19,530 --> 00:16:23,580
that I mentioned, but it
also helps conserve energy

313
00:16:23,580 --> 00:16:25,473
and scales beautifully.

314
00:16:26,880 --> 00:16:31,710
Now, what we have done
with this EC2-8i instance

315
00:16:31,710 --> 00:16:35,760
is that we have introduced it
over these different families

316
00:16:35,760 --> 00:16:39,900
which are the M, the
C, and the R families.

317
00:16:39,900 --> 00:16:42,450
But as you saw in Dave Brown's video,

318
00:16:42,450 --> 00:16:47,450
we also have the flex
option that's available.

319
00:16:47,880 --> 00:16:51,621
And with this flex option,
you can have customers

320
00:16:51,621 --> 00:16:54,573
match the pricing to their demand.

321
00:16:55,470 --> 00:16:57,990
So another thing that we've done

322
00:16:57,990 --> 00:17:00,930
is that we've launched these in the US

323
00:17:00,930 --> 00:17:04,140
and the Spain region
and we are now working

324
00:17:04,140 --> 00:17:07,803
on launching these to
additional AWS regions as well.

325
00:17:09,150 --> 00:17:12,600
A great example of our
collaboration with Intel

326
00:17:12,600 --> 00:17:15,810
is what Technology
Innovation Institute has done

327
00:17:15,810 --> 00:17:20,430
in UAE by using this technology to develop

328
00:17:20,430 --> 00:17:22,953
their Falcon large language model.

329
00:17:24,330 --> 00:17:27,990
We also have many other
partners and customers

330
00:17:27,990 --> 00:17:30,243
that are benefiting
from these technologies.

331
00:17:31,350 --> 00:17:34,290
Next, to hear stories
of one of our partners

332
00:17:34,290 --> 00:17:36,810
who've done fascinating work in this area,

333
00:17:36,810 --> 00:17:39,450
I would like to invite
Bob Simmons from Deloitte

334
00:17:39,450 --> 00:17:40,680
to the stage.

335
00:17:40,680 --> 00:17:41,513
Bob.

336
00:17:46,800 --> 00:17:47,633
- Thanks.

337
00:17:51,000 --> 00:17:53,250
Hi, my name's Bob Simmons from Deloitte.

338
00:17:53,250 --> 00:17:56,880
I'm in the government and
public service sector.

339
00:17:56,880 --> 00:17:58,350
So what I'm here to
talk about is some work

340
00:17:58,350 --> 00:18:00,940
that we've been doing
with Intel on running

341
00:18:02,289 --> 00:18:04,710
LLMs and SLMs on a CPU.

342
00:18:04,710 --> 00:18:08,310
So the Deloitte AI
strategy really comes down

343
00:18:08,310 --> 00:18:10,680
to we invest heavily
into our practitioners,

344
00:18:10,680 --> 00:18:13,530
training them, and heavily into
developing new capabilities,

345
00:18:13,530 --> 00:18:15,063
which this is gonna be one of.

346
00:18:16,680 --> 00:18:20,610
What we're looking at from
two different perspectives

347
00:18:20,610 --> 00:18:23,340
is how can AI win in the market

348
00:18:23,340 --> 00:18:26,040
and then how can clients win with AI?

349
00:18:26,040 --> 00:18:27,900
So what does the market want?

350
00:18:27,900 --> 00:18:31,950
They want a cost efficient
way of doing inferencing.

351
00:18:31,950 --> 00:18:34,770
They need to be able to scale.

352
00:18:34,770 --> 00:18:38,340
So most of the projects
don't deliver on the ROI,

353
00:18:38,340 --> 00:18:42,390
whether it be cost savings,
whether it be reduction

354
00:18:42,390 --> 00:18:45,753
in labor, whether it be an
increased customer satisfaction.

355
00:18:46,650 --> 00:18:48,390
When it goes into a prototype,

356
00:18:48,390 --> 00:18:52,920
it doesn't go into production.

357
00:18:52,920 --> 00:18:55,680
And from production,
sometimes it's hard to scale,

358
00:18:55,680 --> 00:18:57,537
especially if you're looking
at trying to use GPUs

359
00:18:57,537 --> 00:18:58,740
and there's not enough out there.

360
00:18:58,740 --> 00:19:03,330
Maybe you don't have the
funds to cover a GPU 24-7,

361
00:19:03,330 --> 00:19:05,370
but you can a CPU.

362
00:19:05,370 --> 00:19:09,300
So what we're looking
at is running on a CPU

363
00:19:09,300 --> 00:19:13,020
and being able to reduce
the cost and able to scale.

364
00:19:13,020 --> 00:19:15,333
So how can the clients win with AI?

365
00:19:16,230 --> 00:19:20,250
One way is to use CPUs with
SLMs and compressed LLMs,

366
00:19:20,250 --> 00:19:21,930
and we'll get into that in a little bit.

367
00:19:21,930 --> 00:19:24,690
And most clients have
a large infrastructure

368
00:19:24,690 --> 00:19:29,690
of CPU-based EC2s, which
makes it easier to scale.

369
00:19:29,820 --> 00:19:33,060
It decreases the cost by about 50%,

370
00:19:33,060 --> 00:19:35,613
50% plus if you're looking at the OEM,

371
00:19:37,620 --> 00:19:39,420
excuse me, the O&M.

372
00:19:39,420 --> 00:19:43,800
And then it enables scaling
and also increases security.

373
00:19:43,800 --> 00:19:45,900
Now, what I mean by increases security

374
00:19:45,900 --> 00:19:50,900
is in the GPS, they will
have a full-up AWS cloud

375
00:19:51,000 --> 00:19:53,790
on the unclassified government network.

376
00:19:53,790 --> 00:19:58,350
They have a full-up AWS
cloud on the secret network

377
00:19:58,350 --> 00:20:02,100
and a full-up AWS cloud
on the top secret network.

378
00:20:02,100 --> 00:20:04,863
Three full clouds not
connected to the internet.

379
00:20:06,240 --> 00:20:11,240
So the security is when you're on-premise,

380
00:20:11,640 --> 00:20:14,340
you can have different
organizations that don't want

381
00:20:14,340 --> 00:20:16,020
to share data within the same agency,

382
00:20:16,020 --> 00:20:20,370
thinks of human resources and benefits.

383
00:20:20,370 --> 00:20:24,810
I don't wanna send PHI
to the overall ChatGPT

384
00:20:24,810 --> 00:20:26,913
that the agency uses.

385
00:20:26,913 --> 00:20:31,913
If I'm in contracts, I don't
wanna send Deloitte rates

386
00:20:32,910 --> 00:20:36,750
to ChatGPT to say fill
out this form for me.

387
00:20:36,750 --> 00:20:41,750
So what this makes possible
is, within your own VPC,

388
00:20:41,900 --> 00:20:45,390
to have your own gen AI to do
your work that never leaves.

389
00:20:45,390 --> 00:20:48,783
And for a lot of my customers,
that's top priority.

390
00:20:53,730 --> 00:20:58,730
On this, the current
landscape is AI data centers

391
00:20:59,700 --> 00:21:01,380
are estimated to really grow.

392
00:21:01,380 --> 00:21:03,570
I think they've already talked about this,

393
00:21:03,570 --> 00:21:06,300
but it's growing fast and
GPUs aren't out there,

394
00:21:06,300 --> 00:21:09,600
at least for my client, getting the GPUs

395
00:21:09,600 --> 00:21:11,703
onto their networks.

396
00:21:12,660 --> 00:21:14,490
So that's a problem.

397
00:21:14,490 --> 00:21:19,290
They have a large infrastructure
of EC2s, TPU-based.

398
00:21:19,290 --> 00:21:22,320
So going down to CPUs
such as the Intel Xeon,

399
00:21:22,320 --> 00:21:26,250
it offers a cheaper localized
alternative to GPUs, alright?

400
00:21:26,250 --> 00:21:29,130
For the GPUs, costs are surging because

401
00:21:29,130 --> 00:21:31,083
of the high demand for them.

402
00:21:32,040 --> 00:21:34,523
There's massive large
language models on GPUs.

403
00:21:34,523 --> 00:21:38,100
There's a lot of times
overkill for what you need,

404
00:21:38,100 --> 00:21:41,130
but that may be the only
thing that's available.

405
00:21:41,130 --> 00:21:44,520
The CPU-powered models
offer a budget-friendly way

406
00:21:44,520 --> 00:21:47,430
to a broader adoption,
and what I'm gonna say,

407
00:21:47,430 --> 00:21:50,670
democratizing it across the agencies.

408
00:21:50,670 --> 00:21:54,230
So going forward, the small
language model we're seeing

409
00:21:54,230 --> 00:21:56,373
is like 10 billion parameters or less.

410
00:21:57,484 --> 00:22:01,680
The model compression tools,
such as the Intel's OpenVINO,

411
00:22:01,680 --> 00:22:04,830
and this is kind of crucial,
and some new technologies

412
00:22:04,830 --> 00:22:07,770
for compression, which
can compress models down

413
00:22:07,770 --> 00:22:12,270
without losing much
accuracy, and it helps expand

414
00:22:12,270 --> 00:22:14,373
the use cases that you can automate.

415
00:22:18,810 --> 00:22:22,230
Alright, so where are we on this?

416
00:22:22,230 --> 00:22:27,230
So we are working with the Intel Alliance.

417
00:22:27,840 --> 00:22:32,840
We're testing out running
models on GPUs and CPUs.

418
00:22:33,090 --> 00:22:38,090
We had a phase one where we
were running an LLM and an SLM,

419
00:22:38,100 --> 00:22:42,630
both on GPUs and CPUs, and
looking at their performance.

420
00:22:42,630 --> 00:22:45,630
And then we're saying,
okay, we found a company

421
00:22:45,630 --> 00:22:47,910
that does compression in a way,

422
00:22:47,910 --> 00:22:50,910
they call it quantum-inspired compression,

423
00:22:50,910 --> 00:22:53,130
that allows them to compress it down,

424
00:22:53,130 --> 00:22:58,130
say a 70 billion model or
larger, to fit on a CPU,

425
00:22:58,380 --> 00:23:00,750
and we wanted to know what
was the decrease in accuracy.

426
00:23:00,750 --> 00:23:01,923
That's phase two.

427
00:23:03,060 --> 00:23:06,120
And then we went to a
phase three when we said,

428
00:23:06,120 --> 00:23:08,400
okay, that compressed model didn't have

429
00:23:08,400 --> 00:23:10,110
a large enough context window.

430
00:23:10,110 --> 00:23:13,710
So when we're running it
from the GPU to the CPU

431
00:23:13,710 --> 00:23:16,020
on phase one, of course we saw,

432
00:23:16,020 --> 00:23:19,230
and we got SLMs to work uncompressed,

433
00:23:19,230 --> 00:23:21,990
but not the LLMs, just too large, right?

434
00:23:21,990 --> 00:23:26,460
But you can get a 56% reduction
in O&M infrastructure costs,

435
00:23:26,460 --> 00:23:28,020
as I mentioned earlier.

436
00:23:28,020 --> 00:23:30,000
And it outperformed in
terms of efficiency,

437
00:23:30,000 --> 00:23:32,970
not necessarily speed, but
when you're looking at energy

438
00:23:32,970 --> 00:23:37,620
and cost and availability for
scaling, it outperformed it.

439
00:23:37,620 --> 00:23:42,620
On phase two, the compressed
models run almost twice

440
00:23:43,440 --> 00:23:46,890
as fast as the uncompressed
models, which also increased,

441
00:23:46,890 --> 00:23:49,620
so if you don't want to buy more hardware,

442
00:23:49,620 --> 00:23:54,620
you can license a compressed
model and do twice the work.

443
00:23:54,750 --> 00:23:59,750
We went to phase 3A, and
we're on the phase two

444
00:23:59,760 --> 00:24:03,750
on this chat bot, we were
getting 71.3% accuracy

445
00:24:03,750 --> 00:24:06,330
and the client wanted 75%.

446
00:24:06,330 --> 00:24:11,010
We went from just a larger
context window to 80.7%,

447
00:24:11,010 --> 00:24:15,170
and note that the
uncompressed model had 81.7%,

448
00:24:15,170 --> 00:24:18,990
so we lost 1%, which was...
I think it was like a...

449
00:24:18,990 --> 00:24:23,130
so you're keeping like
98.5% of the accuracy

450
00:24:23,130 --> 00:24:24,430
of the uncompressed model.

451
00:24:26,130 --> 00:24:29,400
We saw this and said,
alright, we've now shown

452
00:24:29,400 --> 00:24:31,360
that you can run large language models

453
00:24:32,700 --> 00:24:35,640
on CPUs and small language models,

454
00:24:35,640 --> 00:24:37,410
and that you can run them faster,

455
00:24:37,410 --> 00:24:41,853
and going on to the next slide, actually.

456
00:24:43,380 --> 00:24:48,380
So on the GPUs, Amazon, we
were using Amazon EC2 8i's.

457
00:24:48,540 --> 00:24:52,380
This is the Xeon 4, and then
the monthly cost for the GPU

458
00:24:52,380 --> 00:24:54,750
and the monthly cost for the CPU,

459
00:24:54,750 --> 00:24:58,473
you're looking at about
a 56% reduction in cost.

460
00:24:59,400 --> 00:25:02,070
What you're seeing on the
lower left and the lower right

461
00:25:02,070 --> 00:25:05,610
is the tokens per second on the left,

462
00:25:05,610 --> 00:25:07,713
and then the latency on the right.

463
00:25:08,880 --> 00:25:13,880
The OpenVINO model server
is optimized for the Xeon.

464
00:25:15,660 --> 00:25:19,290
So we're using a Xeon that has 48 CPUs.

465
00:25:19,290 --> 00:25:22,470
When it loads it, it actually
does parallel processing

466
00:25:22,470 --> 00:25:25,140
between those CPUs, so
on the bottom scale,

467
00:25:25,140 --> 00:25:28,170
you'll see one user,
two users going on up.

468
00:25:28,170 --> 00:25:31,860
These are simultaneous users,
and you see the CPU is flat.

469
00:25:31,860 --> 00:25:34,290
And actually, when you
get up to a certain point,

470
00:25:34,290 --> 00:25:37,470
the CPU outperforms the
GPUs, but then the latency

471
00:25:37,470 --> 00:25:39,265
is getting up there.

472
00:25:39,265 --> 00:25:41,850
And the tokens per
second, you can also see

473
00:25:41,850 --> 00:25:44,970
where it's flat on the tokens per second,

474
00:25:44,970 --> 00:25:48,000
and then finally right around 40,

475
00:25:48,000 --> 00:25:50,670
it starts to drop off.

476
00:25:50,670 --> 00:25:52,740
So this also depends on your use case.

477
00:25:52,740 --> 00:25:55,140
If you're not really time-dependent,

478
00:25:55,140 --> 00:25:58,380
you can use this, or
a GPU if you're really

479
00:25:58,380 --> 00:25:59,880
time-dependent and need speed.

480
00:26:03,764 --> 00:26:07,170
Alright, the target use
case areas for the SLM

481
00:26:07,170 --> 00:26:09,483
is basically when we need security,

482
00:26:10,410 --> 00:26:12,573
when we need infrastructure for scaling,

483
00:26:14,040 --> 00:26:16,410
when we need to be on-premise,

484
00:26:16,410 --> 00:26:19,983
where CPUs are prevalent and
GPUs aren't as prevalent.

485
00:26:21,060 --> 00:26:23,250
Multi-dentric use cases,
one of the next phases

486
00:26:23,250 --> 00:26:25,980
we're gonna be doing is
a more complex use case

487
00:26:25,980 --> 00:26:27,750
where there's gonna be
three different agents

488
00:26:27,750 --> 00:26:31,023
running on an EC2, CPU-based EC2,

489
00:26:32,160 --> 00:26:35,040
and that works very well, actually.

490
00:26:35,040 --> 00:26:39,270
And then on-edge devices,
say satellites, drones.

491
00:26:39,270 --> 00:26:40,680
Let's say you're in manufacturing

492
00:26:40,680 --> 00:26:43,770
and you want LLM out on
the manufacturing floor

493
00:26:43,770 --> 00:26:45,570
on different machines.

494
00:26:45,570 --> 00:26:47,793
So these are the use
cases we're looking at.

495
00:26:50,280 --> 00:26:52,040
So one of the use cases
we're gonna be doing next

496
00:26:52,040 --> 00:26:53,550
is say you have a sensitive document.

497
00:26:53,550 --> 00:26:54,630
I'm just gonna call it,

498
00:26:54,630 --> 00:26:56,703
it's an intelligence report, right?

499
00:26:57,540 --> 00:26:59,250
If we want to send that to some allies,

500
00:26:59,250 --> 00:27:01,230
we've got to remove some information.

501
00:27:01,230 --> 00:27:04,170
So one agent's gonna find the information

502
00:27:04,170 --> 00:27:06,300
and then show it to the Intel analyst

503
00:27:06,300 --> 00:27:07,800
and say, is this correct?

504
00:27:07,800 --> 00:27:09,480
Then it's gonna remove the information

505
00:27:09,480 --> 00:27:12,570
and rewrite it as a second
agent and then show it

506
00:27:12,570 --> 00:27:15,510
to the analyst and say, is this right?

507
00:27:15,510 --> 00:27:17,970
And then it's gonna go back
and it's gonna reclassify

508
00:27:17,970 --> 00:27:22,050
the document based off of what
information it pulled out.

509
00:27:22,050 --> 00:27:25,860
So this is just a multi-agentic
system doing a more complex,

510
00:27:25,860 --> 00:27:27,360
now granted this is gonna be using

511
00:27:27,360 --> 00:27:29,400
a 70 billion parameter
that's been compressed down

512
00:27:29,400 --> 00:27:32,313
to fit on a 8i instance.

513
00:27:39,990 --> 00:27:43,113
Alright, well actually,
I need to hand this off.

514
00:27:52,140 --> 00:27:55,182
- Okay, so we talked a little
bit about... thank you, Bob.

515
00:27:55,182 --> 00:27:58,410
What I love about this is
it's nice to have partners

516
00:27:58,410 --> 00:28:00,330
like Deloitte who are
really coming alongside

517
00:28:00,330 --> 00:28:02,220
to bring these use cases to life to see

518
00:28:02,220 --> 00:28:06,000
how they come to fruition
with real customers.

519
00:28:06,000 --> 00:28:08,790
I said that I'll talk a
little bit about software too.

520
00:28:08,790 --> 00:28:10,590
Many people don't know
that we have thousands

521
00:28:10,590 --> 00:28:13,140
of software engineers at
Intel and we've been working

522
00:28:13,140 --> 00:28:15,480
in this ecosystem for many years.

523
00:28:15,480 --> 00:28:18,120
But we talked about this open approach.

524
00:28:18,120 --> 00:28:21,030
Certainly with any large
framework providers,

525
00:28:21,030 --> 00:28:23,250
any of the large language model providers,

526
00:28:23,250 --> 00:28:24,800
we're working with all of them.

527
00:28:26,820 --> 00:28:28,320
For ourselves, we wanna make sure

528
00:28:28,320 --> 00:28:31,260
as any version drops of any
of these models as well,

529
00:28:31,260 --> 00:28:33,210
we have a very short turnaround

530
00:28:33,210 --> 00:28:35,253
that they're up and running on Intel.

531
00:28:36,150 --> 00:28:38,340
So that's been really successful

532
00:28:38,340 --> 00:28:39,570
and we're gonna continue this work.

533
00:28:39,570 --> 00:28:41,940
There's a depth of those relationships

534
00:28:41,940 --> 00:28:43,680
that it's quite strong.

535
00:28:43,680 --> 00:28:45,180
We also have our own software.

536
00:28:45,180 --> 00:28:47,490
We talked about oneAPI.

537
00:28:47,490 --> 00:28:49,470
That really is optimized for Intel

538
00:28:49,470 --> 00:28:52,440
and it's available to all of you.

539
00:28:52,440 --> 00:28:54,450
We've been actually working with oneAPI

540
00:28:54,450 --> 00:28:56,700
in our edge business for many, many years

541
00:28:56,700 --> 00:28:58,110
across all the verticals.

542
00:28:58,110 --> 00:29:00,330
We can talk about optimizing.

543
00:29:00,330 --> 00:29:02,820
That's where that started.

544
00:29:02,820 --> 00:29:06,480
But it's expanded dramatically with AI.

545
00:29:06,480 --> 00:29:10,320
So oneAPI is really a great tool.

546
00:29:10,320 --> 00:29:12,720
Encourage you all to take a look at that.

547
00:29:12,720 --> 00:29:16,200
And then of course, we have open standards

548
00:29:16,200 --> 00:29:19,740
and protocols that we also
develop within the ecosystem.

549
00:29:19,740 --> 00:29:23,370
So UCIe, CXL is some of those as well.

550
00:29:23,370 --> 00:29:26,250
So a lot of work in terms
of how we contribute

551
00:29:26,250 --> 00:29:29,520
to this open ecosystem software approach,

552
00:29:29,520 --> 00:29:33,120
both with companies and also
our own contributions as well.

553
00:29:33,120 --> 00:29:36,300
And then lastly, we actually
just announced Intel AI

554
00:29:36,300 --> 00:29:38,162
for Enterprise Inference.

555
00:29:38,162 --> 00:29:41,070
Let's try to say that a couple times fast.

556
00:29:41,070 --> 00:29:44,280
But really the intent there is simple,

557
00:29:44,280 --> 00:29:47,340
easy to use software deployment

558
00:29:47,340 --> 00:29:50,370
for this inference workload
that we've been discussing.

559
00:29:50,370 --> 00:29:54,000
The beauty of having the AWS instance

560
00:29:54,000 --> 00:29:56,190
in the cloud is that
this is just available

561
00:29:56,190 --> 00:29:59,160
when you choose the EC2 AI instance.

562
00:29:59,160 --> 00:30:04,020
- You get this software as well
as part of that experience.

563
00:30:04,020 --> 00:30:06,180
But really, it goes all
the way down the stack

564
00:30:06,180 --> 00:30:08,400
to really think about how you can leverage

565
00:30:08,400 --> 00:30:10,890
and optimize for inference
workloads on Intel.

566
00:30:10,890 --> 00:30:13,738
And we're gonna continue to build on this

567
00:30:13,738 --> 00:30:15,930
and continue that development.

568
00:30:15,930 --> 00:30:19,020
So, as an example of someone that's using

569
00:30:19,020 --> 00:30:24,020
or used this software, I
wanted to invite Amit up

570
00:30:24,450 --> 00:30:27,780
from e& to talk about a
little bit of how he used

571
00:30:27,780 --> 00:30:30,840
this software for development for the UAE.

572
00:30:30,840 --> 00:30:32,103
So, welcome Amit.

573
00:30:33,420 --> 00:30:36,213
- Thank you, Caitlin, for
the kind introduction.

574
00:30:37,590 --> 00:30:41,310
Hi, I'm Amit Gupta, I'm
the VP for e& Enterprise.

575
00:30:41,310 --> 00:30:43,560
I lead the data and AI practice for Asia,

576
00:30:43,560 --> 00:30:45,810
Middle East, Africa, and Europe.

577
00:30:45,810 --> 00:30:48,930
And today I'm going to talk
about how we are collaborating

578
00:30:48,930 --> 00:30:53,930
with Intel to build the
enterprise AI inference stack

579
00:30:54,810 --> 00:30:56,970
and how we are uniquely solving a problem

580
00:30:56,970 --> 00:31:00,060
whereby we create something which is open,

581
00:31:00,060 --> 00:31:02,850
transparent, cost effective, and sovereign

582
00:31:02,850 --> 00:31:04,680
for our UAE customers.

583
00:31:04,680 --> 00:31:06,150
But before that, let me talk a little bit

584
00:31:06,150 --> 00:31:11,150
about what we offer as
e& data and AI practice.

585
00:31:12,240 --> 00:31:16,110
So, we offer a full stack
practice whereby we are

586
00:31:16,110 --> 00:31:20,070
providing AI connectivity,
AI infrastructure,

587
00:31:20,070 --> 00:31:24,030
AI solutions and services,
but also AI devices.

588
00:31:24,030 --> 00:31:26,790
So, we provide the full stack.

589
00:31:26,790 --> 00:31:29,910
We've already delivered a
lot of these AI solutions,

590
00:31:29,910 --> 00:31:34,350
more than 200 plus AI
cases and 200 use cases

591
00:31:34,350 --> 00:31:36,420
across sectors and industries.

592
00:31:36,420 --> 00:31:38,820
We are tech and platform agnostic,

593
00:31:38,820 --> 00:31:41,970
which means we work across AI

594
00:31:41,970 --> 00:31:45,390
and data platforms as
well as hyperscalers.

595
00:31:45,390 --> 00:31:50,390
We have a large team of AI
resources located globally.

596
00:31:50,730 --> 00:31:54,690
And we have developed
in-house AI accelerators

597
00:31:54,690 --> 00:31:57,900
that give value within
weeks instead of months.

598
00:31:57,900 --> 00:32:02,190
And last but not the least, we
also have our own AI Academy

599
00:32:02,190 --> 00:32:05,310
where we train practitioners

600
00:32:05,310 --> 00:32:08,430
and a lot of other leaders on AI.

601
00:32:08,430 --> 00:32:10,590
We have our latest flagship program,

602
00:32:10,590 --> 00:32:11,730
the Chief AI Officer Program,

603
00:32:11,730 --> 00:32:13,833
which we are running very successfully.

604
00:32:15,210 --> 00:32:18,690
Okay, so I'm sure all of you have seen

605
00:32:18,690 --> 00:32:20,940
some of these statistics.

606
00:32:20,940 --> 00:32:25,230
The key point here is inference
market is going to grow.

607
00:32:25,230 --> 00:32:27,750
The AI inference market is
gonna grow exponentially.

608
00:32:27,750 --> 00:32:32,040
You know, you can see that
from 100 billion this year

609
00:32:32,040 --> 00:32:36,270
to more than 250 billion
in the next five years.

610
00:32:36,270 --> 00:32:38,730
And through 2028, what we have seen

611
00:32:38,730 --> 00:32:43,730
is the cost of AI
inference is at least 70%

612
00:32:44,310 --> 00:32:49,310
of the total model
lifetime, which is far more

613
00:32:49,440 --> 00:32:52,500
than the training costs
that are required to do

614
00:32:52,500 --> 00:32:53,550
to train a model.

615
00:32:53,550 --> 00:32:55,320
So as you can see, the inferencing costs

616
00:32:55,320 --> 00:32:56,403
are very, very high.

617
00:32:57,450 --> 00:33:00,300
And some of these stats
talk about that at least 50%

618
00:33:00,300 --> 00:33:03,570
of gen AI projects will
overrun the budgeted costs,

619
00:33:03,570 --> 00:33:04,953
as per Gartner.

620
00:33:06,300 --> 00:33:09,000
Now, how do we build this solution?

621
00:33:09,000 --> 00:33:10,740
Now, the question is, how do we build

622
00:33:10,740 --> 00:33:14,163
the inference AI platform
to meet the customer needs?

623
00:33:15,930 --> 00:33:18,900
Before that, let's address
some of the challenges.

624
00:33:18,900 --> 00:33:21,210
And I'm sure most of you in this room

625
00:33:21,210 --> 00:33:24,120
are also facing these
challenges in your enterprises.

626
00:33:24,120 --> 00:33:27,210
The first one is about costs.

627
00:33:27,210 --> 00:33:30,480
Organizations implementing
AI at enterprise scale today

628
00:33:30,480 --> 00:33:32,970
face a lot of cost constraints,

629
00:33:32,970 --> 00:33:36,570
often leading to more than 300 to 500%

630
00:33:36,570 --> 00:33:39,510
exceeding whatever we had planned.

631
00:33:39,510 --> 00:33:43,020
The second is about
infrastructure management.

632
00:33:43,020 --> 00:33:47,958
And if you know that AI workloads
today have unprecedented

633
00:33:47,958 --> 00:33:50,720
infrastructure demands,
and our traditional

634
00:33:50,720 --> 00:33:52,920
IT systems cannot handle that.

635
00:33:52,920 --> 00:33:56,520
Not only that, they also have
a lot of parallel processing,

636
00:33:56,520 --> 00:34:00,510
as you know, with all the kind
of heterogeneous requirements

637
00:34:00,510 --> 00:34:05,510
like CPUs, GPUs, TPUs, and
AI chips working in tandem.

638
00:34:06,060 --> 00:34:09,210
And lastly, data sovereignty.

639
00:34:09,210 --> 00:34:13,320
We are building AI for UAE from UAE,

640
00:34:13,320 --> 00:34:17,970
which means all our data has
to reside within the country,

641
00:34:17,970 --> 00:34:22,590
which is cost effective,
which doesn't go out,

642
00:34:22,590 --> 00:34:26,160
which is generated,
protected, and preserved

643
00:34:26,160 --> 00:34:28,473
within the country and
meets the local norms.

644
00:34:30,750 --> 00:34:33,603
So how do we tackle
these three challenges?

645
00:34:36,220 --> 00:34:40,680
So it is through our solution
that we call it SLM in a Box,

646
00:34:40,680 --> 00:34:43,110
or Small Language Model in a Box.

647
00:34:43,110 --> 00:34:47,280
And more precisely, we are
leveraging both Intel AI

648
00:34:47,280 --> 00:34:51,930
for enterprise inference and
Intel AI X-rated instances

649
00:34:51,930 --> 00:34:56,103
to power our sovereign
inference as a service platform.

650
00:34:57,120 --> 00:34:59,010
And our goal is twofold.

651
00:34:59,010 --> 00:35:02,520
One is, of course, keeping
everything in the region,

652
00:35:02,520 --> 00:35:05,479
which means that we are
looking to build a solution

653
00:35:05,479 --> 00:35:08,460
which is independent from
all other AWS regions,

654
00:35:08,460 --> 00:35:12,090
meaning that we wanted to
serve only our UAE customers

655
00:35:12,090 --> 00:35:15,150
through the UAE AWS region.

656
00:35:15,150 --> 00:35:19,350
Second is about being
open and cost effective.

657
00:35:19,350 --> 00:35:23,160
So we wanted to address
sovereignty through automation,

658
00:35:23,160 --> 00:35:27,120
through cost, through security,
and through transparency.

659
00:35:27,120 --> 00:35:30,930
And to do so, we are
using Intel 7AI instances

660
00:35:30,930 --> 00:35:34,560
as inference at point, which
is allowing our customers

661
00:35:34,560 --> 00:35:37,680
to not only build use
cases over the platform,

662
00:35:37,680 --> 00:35:40,650
but also offer use cases
which are ready to made

663
00:35:40,650 --> 00:35:41,643
or ready to deploy.

664
00:35:44,040 --> 00:35:44,873
Okay.

665
00:35:46,140 --> 00:35:50,580
Maybe we can go a little
bit more into deep dive

666
00:35:50,580 --> 00:35:52,530
how our solution works.

667
00:35:52,530 --> 00:35:55,140
And as Caitlin mentioned
in the introduction,

668
00:35:55,140 --> 00:36:00,090
the users of GPUs is only for
inference is a myth, right?

669
00:36:00,090 --> 00:36:01,650
We have come ourselves to a conclusion

670
00:36:01,650 --> 00:36:06,300
that when running specifically
small language models,

671
00:36:06,300 --> 00:36:08,820
models with below 10 billion parameters,

672
00:36:08,820 --> 00:36:11,703
Intel instances are a perfect fit.

673
00:36:12,630 --> 00:36:14,790
And it's great to see a
variety of partnerships

674
00:36:14,790 --> 00:36:18,780
in UAE as Mickey also
mentioned about how TII built

675
00:36:18,780 --> 00:36:20,790
through the Falcon model, right?

676
00:36:20,790 --> 00:36:23,070
Now let's deep dive a little
bit more into the solution,

677
00:36:23,070 --> 00:36:27,060
specifically two takeaways
that I want to highlight.

678
00:36:27,060 --> 00:36:29,880
One is we win with open.

679
00:36:29,880 --> 00:36:32,610
As you can see on the right hand side,

680
00:36:32,610 --> 00:36:37,470
our architecture is only
leveraging open source components.

681
00:36:37,470 --> 00:36:39,180
Not only this, it's also helping

682
00:36:39,180 --> 00:36:41,850
in becoming cost effective.

683
00:36:41,850 --> 00:36:44,490
We are advancing using what the
community is building today,

684
00:36:44,490 --> 00:36:47,010
so it remains cost effective.

685
00:36:47,010 --> 00:36:49,380
Two is about flexibility.

686
00:36:49,380 --> 00:36:50,790
That's one thing I really wanted

687
00:36:50,790 --> 00:36:52,290
to point out is flexibility.

688
00:36:52,290 --> 00:36:55,710
So in today's world where
AI is changing everything

689
00:36:55,710 --> 00:36:58,680
so quickly, there's so many
new models getting built in,

690
00:36:58,680 --> 00:37:02,010
this platform helps you choose
the model that you want.

691
00:37:02,010 --> 00:37:04,920
And it can be deployed out of box.

692
00:37:04,920 --> 00:37:07,233
So that's one of the
key advantages of this.

693
00:37:10,320 --> 00:37:12,090
Okay, just to bring it home now,

694
00:37:12,090 --> 00:37:14,640
I think most of the slides
have spoken about the benefits

695
00:37:14,640 --> 00:37:17,910
that we have received, but
let me summarize it for you.

696
00:37:17,910 --> 00:37:21,450
One, it reduces complexity,
which means introducing

697
00:37:21,450 --> 00:37:23,940
a full automated solution.

698
00:37:23,940 --> 00:37:28,350
Two, it's quick time to value
with automated deployment

699
00:37:28,350 --> 00:37:31,860
and management of AI and LLM models.

700
00:37:31,860 --> 00:37:36,030
Three, it's scalable with
embedded auto scaling.

701
00:37:36,030 --> 00:37:38,340
Four, we are talking about
being cost effective,

702
00:37:38,340 --> 00:37:39,930
which is very, very critical

703
00:37:39,930 --> 00:37:41,520
in the overall scheme of things.

704
00:37:41,520 --> 00:37:45,210
We are leveraging Intel
CPUs as inference at point.

705
00:37:45,210 --> 00:37:47,460
Five, it's secure by design.

706
00:37:47,460 --> 00:37:52,410
We are deploying it with
built-in authentication and APIs.

707
00:37:52,410 --> 00:37:54,750
And lastly, it's sovereign by design.

708
00:37:54,750 --> 00:37:57,960
We are deploying it only
for our UAE customers,

709
00:37:57,960 --> 00:38:00,990
leveraging UAE region only, and owing

710
00:38:00,990 --> 00:38:02,670
to our inference stack, infrastructure,

711
00:38:02,670 --> 00:38:04,620
orchestration, everything is happening

712
00:38:04,620 --> 00:38:06,453
in the AWS region in UAE.

713
00:38:07,800 --> 00:38:08,940
So that's it from my side.

714
00:38:08,940 --> 00:38:11,583
Thank you very much, and
over to you, Caitlin.

715
00:38:14,670 --> 00:38:16,860
- Okay, we are just bringing it home here.

716
00:38:16,860 --> 00:38:19,410
So the last, we talked a lot, obviously,

717
00:38:19,410 --> 00:38:21,000
about our AWS partnership.

718
00:38:21,000 --> 00:38:23,010
We talked about customers
that are leveraging

719
00:38:23,010 --> 00:38:25,170
inference workloads on Xeon.

720
00:38:25,170 --> 00:38:27,630
I'm gonna shift a little
bit to a new company

721
00:38:27,630 --> 00:38:31,980
called Articul8, and really,
it's an amazing company.

722
00:38:31,980 --> 00:38:35,490
It actually started at Intel,
and we kind of spun it off.

723
00:38:35,490 --> 00:38:39,450
It was a great startup story,
but I'm really impressed

724
00:38:39,450 --> 00:38:40,283
with their use case.

725
00:38:40,283 --> 00:38:41,580
I'm impressed with their platform.

726
00:38:41,580 --> 00:38:44,340
It's a lot more about using your data,

727
00:38:44,340 --> 00:38:46,770
enterprise data and
insights inside your company

728
00:38:46,770 --> 00:38:49,004
to unlock those insights.

729
00:38:49,004 --> 00:38:54,004
I have Renato here to tell
us more about Articul8,

730
00:38:54,210 --> 00:38:55,773
so welcome onto the stage.

731
00:38:57,840 --> 00:38:58,680
Thank you.

732
00:38:58,680 --> 00:38:59,730
- Thank you, Caitlin.

733
00:39:04,650 --> 00:39:05,483
Hello, everyone.

734
00:39:05,483 --> 00:39:08,610
I'm Renato Nascimento, head
of technology at Articul8.

735
00:39:08,610 --> 00:39:10,350
Thrilled to be here talking

736
00:39:10,350 --> 00:39:13,323
about what we are building at Articul8.

737
00:39:14,490 --> 00:39:18,750
So Articul8 is a platform
that's a smart layer

738
00:39:18,750 --> 00:39:21,900
that helps enterprise to convert complex,

739
00:39:21,900 --> 00:39:24,420
unstructured, or structured data

740
00:39:24,420 --> 00:39:26,880
into hyper-personalized outcomes,

741
00:39:26,880 --> 00:39:29,160
as you can see here on the right.

742
00:39:29,160 --> 00:39:31,320
On this vertical integrated platform,

743
00:39:31,320 --> 00:39:36,320
we help you to run across
different agents and models.

744
00:39:37,620 --> 00:39:41,276
So it's a vertical integrated
platform that works

745
00:39:41,276 --> 00:39:44,550
either on your own
premise or can be deployed

746
00:39:44,550 --> 00:39:48,330
on your own VPC within your AWS account.

747
00:39:48,330 --> 00:39:51,540
So at left, as your data
comes into your platform,

748
00:39:51,540 --> 00:39:55,140
that data gets break
down into its components,

749
00:39:55,140 --> 00:39:57,900
and that components runs through our model

750
00:39:57,900 --> 00:40:01,933
mesh in-house orchestrator that will use

751
00:40:03,150 --> 00:40:05,040
a library of different models,

752
00:40:05,040 --> 00:40:09,750
a combination between traditional
machine learning models,

753
00:40:09,750 --> 00:40:12,780
small language models, and
also large language models

754
00:40:12,780 --> 00:40:15,540
to understand your data,
and most important,

755
00:40:15,540 --> 00:40:18,870
domain-specific models and
also task-specific models,

756
00:40:18,870 --> 00:40:22,260
and we'll talk a little bit
more about that later on.

757
00:40:22,260 --> 00:40:24,960
That gets break down and understood

758
00:40:24,960 --> 00:40:27,330
where we'll build a knowledge graph,

759
00:40:27,330 --> 00:40:30,060
and this knowledge graph here
is a little bit different

760
00:40:30,060 --> 00:40:31,980
than what is done out there.

761
00:40:31,980 --> 00:40:36,150
We also augmented the
content from your data

762
00:40:36,150 --> 00:40:39,840
using the domain specificity
from your own enterprise,

763
00:40:39,840 --> 00:40:42,780
creating and help you to see connections

764
00:40:42,780 --> 00:40:46,683
that are not directly
implied on the data as well.

765
00:40:50,760 --> 00:40:54,810
So what are the key
differentiations of our platform?

766
00:40:54,810 --> 00:40:58,470
One is our intelligent model routing,

767
00:40:58,470 --> 00:41:02,670
where we built an in-house
model orchestrator

768
00:41:02,670 --> 00:41:06,300
that helps you at runtime
autonomously decide

769
00:41:06,300 --> 00:41:08,880
which agent or model
will be used for the task

770
00:41:08,880 --> 00:41:11,400
that you're trying to achieve at hand.

771
00:41:11,400 --> 00:41:15,330
We do that by evaluating
every single of those models,

772
00:41:15,330 --> 00:41:17,610
agents across the tasks that are important

773
00:41:17,610 --> 00:41:21,213
for your use case, and I'll
show a couple examples as well.

774
00:41:22,080 --> 00:41:25,953
All of that with a platform
that gives you observability.

775
00:41:26,940 --> 00:41:30,840
You can audit every single
decision that those actions

776
00:41:30,840 --> 00:41:33,960
are taking on your behalf
on the platform as well,

777
00:41:33,960 --> 00:41:35,970
and that can be run in scales.

778
00:41:35,970 --> 00:41:38,160
We have use cases that have been run

779
00:41:38,160 --> 00:41:40,410
with hundreds of thousands of files,

780
00:41:40,410 --> 00:41:42,900
generating knowledge graphs with millions

781
00:41:42,900 --> 00:41:46,530
of entities running in production.

782
00:41:46,530 --> 00:41:49,320
And more importantly, our platform comes

783
00:41:49,320 --> 00:41:53,850
with a combination of, in the
library, of domain-specific

784
00:41:53,850 --> 00:41:56,850
and task-specific models that helps you

785
00:41:56,850 --> 00:41:59,490
to achieve a complex use case.

786
00:41:59,490 --> 00:42:03,810
And I brought here an
example to help you visualize

787
00:42:03,810 --> 00:42:07,650
why domain-specific models
are important, right?

788
00:42:07,650 --> 00:42:10,290
If you're trying to do
something very specific

789
00:42:10,290 --> 00:42:15,270
or a use case, the general
purpose models out there,

790
00:42:15,270 --> 00:42:18,450
they might fall short
of achieving your goal.

791
00:42:18,450 --> 00:42:20,970
In this example here, we have an image.

792
00:42:20,970 --> 00:42:25,970
This image is a grid, is a
testing system for power cables.

793
00:42:29,160 --> 00:42:31,590
And if you send that to,
for instance, in this case,

794
00:42:31,590 --> 00:42:36,590
to cloud, you see that the
image gets poorly classified.

795
00:42:36,720 --> 00:42:40,020
And then if you use one of
our domain-specific models,

796
00:42:40,020 --> 00:42:44,970
in this case, it's a
domain-specific model,

797
00:42:44,970 --> 00:42:48,540
that one can not only understand
what this image is about,

798
00:42:48,540 --> 00:42:52,830
but also all the minutiae
and details underlying

799
00:42:52,830 --> 00:42:54,780
of that image as well.

800
00:42:54,780 --> 00:42:57,453
So if you have an expert use case,

801
00:42:59,760 --> 00:43:02,760
that data might not be
out there on the public,

802
00:43:02,760 --> 00:43:04,890
or even if your data is
out there in the public,

803
00:43:04,890 --> 00:43:09,090
you might have to heavily prompt engineer

804
00:43:09,090 --> 00:43:11,220
general purpose models
to achieve that goal,

805
00:43:11,220 --> 00:43:14,280
and might be rule-based,
which might also fall short

806
00:43:14,280 --> 00:43:16,893
in corner use cases.

807
00:43:19,830 --> 00:43:23,610
Also, we brought here,
that was an example,

808
00:43:23,610 --> 00:43:26,220
but we also don't believe
in benchmarks internally

809
00:43:26,220 --> 00:43:28,830
with Articul8 what we do as part

810
00:43:28,830 --> 00:43:31,170
of a model orchestrator as well.

811
00:43:31,170 --> 00:43:34,410
We test every single model in agents

812
00:43:34,410 --> 00:43:37,380
that are part of our platform across

813
00:43:37,380 --> 00:43:39,900
a different variety of dimensions.

814
00:43:39,900 --> 00:43:44,850
On this example here, we have
our domain-specific model

815
00:43:46,200 --> 00:43:51,200
being tested across 10
different domains, sorry,

816
00:43:51,390 --> 00:43:54,663
dimensions, created by
experts of the area.

817
00:43:56,070 --> 00:44:00,810
Each of those tasks
contain very curated tasks

818
00:44:02,190 --> 00:44:05,190
that those experts on that
domain want's to achieve,

819
00:44:05,190 --> 00:44:09,870
and on this spider plot, closer to 100%

820
00:44:09,870 --> 00:44:12,930
means how accurate you are on that task.

821
00:44:12,930 --> 00:44:16,650
Here we can see, across
all these 10 domains,

822
00:44:16,650 --> 00:44:20,860
our domain-specific model overperforms

823
00:44:21,780 --> 00:44:25,740
with more than 25% accuracy than ever

824
00:44:25,740 --> 00:44:28,740
other model available there,
either private models,

825
00:44:28,740 --> 00:44:31,080
state-of-the-art, like GPT-5,

826
00:44:31,080 --> 00:44:34,353
or even large open-source models as well.

827
00:44:35,430 --> 00:44:39,900
Not only that, towards running
optimized inference costs

828
00:44:39,900 --> 00:44:43,710
as well, this model is
also a fraction of the size

829
00:44:43,710 --> 00:44:45,707
of every other model there, too.

830
00:44:45,707 --> 00:44:50,707
So you can not only achieve
better results across

831
00:44:50,910 --> 00:44:52,950
different tasks that are specialized,

832
00:44:52,950 --> 00:44:57,360
but also reduce your
overall cost of inference.

833
00:44:57,360 --> 00:45:01,110
And I brought one example here
with the domain-expert model,

834
00:45:01,110 --> 00:45:05,220
but we have data and experiments like that

835
00:45:05,220 --> 00:45:08,340
for all of domain-specific
models out there.

836
00:45:08,340 --> 00:45:12,600
For instance, semiconductor manufacturing,

837
00:45:12,600 --> 00:45:17,400
supply chain, financial services as well.

838
00:45:17,400 --> 00:45:21,750
And that's all embedded and
imbedded on our platform.

839
00:45:21,750 --> 00:45:24,663
It comes as we deploy
the platform as well.

840
00:45:26,580 --> 00:45:31,580
So our platform also brings
with it all the optimizations

841
00:45:32,760 --> 00:45:35,730
and expertise the team
has built over the years,

842
00:45:35,730 --> 00:45:40,730
helping optimize what model
works best with what hardware.

843
00:45:41,850 --> 00:45:45,510
So we have a library of models
from all different sides,

844
00:45:45,510 --> 00:45:49,260
so we need to understand at
the real time which model,

845
00:45:49,260 --> 00:45:52,650
which hardware is gonna
be used to execute that,

846
00:45:52,650 --> 00:45:55,710
either for the small ones
that will be CPU-optimized,

847
00:45:55,710 --> 00:45:58,500
or even from very large
general-purpose models

848
00:45:58,500 --> 00:46:00,723
as well that will run on GPU.

849
00:46:02,850 --> 00:46:07,850
We also are AWS partners,
certified AI on the AI competency.

850
00:46:12,030 --> 00:46:15,300
That means we have a very
tried-and-true solution

851
00:46:15,300 --> 00:46:17,910
that deploys into AWS,
optimizing the resources,

852
00:46:17,910 --> 00:46:20,310
and follow all the best
practices there as well.

853
00:46:27,360 --> 00:46:32,360
And just to close out,
I also wanna talk about

854
00:46:35,370 --> 00:46:37,680
how you can start to see our products.

855
00:46:37,680 --> 00:46:39,720
So our product is a subscription,

856
00:46:39,720 --> 00:46:41,700
either subscribe to our software,

857
00:46:41,700 --> 00:46:44,610
or you can subscribe to our
managed solution as well

858
00:46:44,610 --> 00:46:47,310
if you wanna start before
you get your own deployment

859
00:46:47,310 --> 00:46:50,910
on your own AWS account or your own VPC.

860
00:46:50,910 --> 00:46:55,910
But also, we have our
domain-specific models

861
00:46:56,697 --> 00:47:01,260
and agents available
into the AWS Marketplace

862
00:47:01,260 --> 00:47:02,313
for you to try it.

863
00:47:03,480 --> 00:47:05,910
I'm also proud to announce that today,

864
00:47:05,910 --> 00:47:10,560
not today, this week, we
are launching a new agent

865
00:47:10,560 --> 00:47:15,090
on AWS Marketplace that
does table understanding.

866
00:47:15,090 --> 00:47:19,437
And with that agent, you
can upload very complex

867
00:47:22,080 --> 00:47:26,190
unstructured files that have
data on the type of tables,

868
00:47:26,190 --> 00:47:30,270
either images or normal
table, it doesn't matter.

869
00:47:30,270 --> 00:47:33,120
That agent will understand that table

870
00:47:33,120 --> 00:47:35,340
and send that back to
you, and you only pay

871
00:47:35,340 --> 00:47:36,870
the user's cost cents.

872
00:47:36,870 --> 00:47:39,420
So it's very easy to start using us,

873
00:47:39,420 --> 00:47:42,870
either through our AWS
Marketplace offering

874
00:47:42,870 --> 00:47:45,480
or getting directed with us

875
00:47:45,480 --> 00:47:47,373
using our managed solution as well.

876
00:47:50,790 --> 00:47:52,860
If you want to know more about us,

877
00:47:52,860 --> 00:47:55,050
please check out our website, articul8.ai,

878
00:47:55,050 --> 00:47:56,610
and thank you very much.

879
00:47:56,610 --> 00:47:57,540
Back to you, Caitlin.

880
00:47:57,540 --> 00:47:59,340
- Okay, thank you.

881
00:47:59,340 --> 00:48:01,020
Thank you, Renato.

882
00:48:01,020 --> 00:48:02,340
Okay, we will wrap it up.

883
00:48:02,340 --> 00:48:04,980
So thank you so much
for being with us today.

884
00:48:04,980 --> 00:48:08,160
Just a few key takeaways to take with you.

885
00:48:08,160 --> 00:48:11,940
Certainly, check out the EC2-8i instance

886
00:48:11,940 --> 00:48:13,800
with our Xeon 6.

887
00:48:13,800 --> 00:48:15,720
We know now, after this session,

888
00:48:15,720 --> 00:48:17,850
that you can use these for this instance,

889
00:48:17,850 --> 00:48:19,530
not only for your inference workloads,

890
00:48:19,530 --> 00:48:22,500
but obviously general compute as well.

891
00:48:22,500 --> 00:48:25,050
We will lead and continue
to lead with an open

892
00:48:25,050 --> 00:48:26,670
and trusted AI approach.

893
00:48:26,670 --> 00:48:28,350
We will invest in the ecosystem.

894
00:48:28,350 --> 00:48:30,180
We'd love to partner as well when we talk

895
00:48:30,180 --> 00:48:32,100
about this third part.

896
00:48:32,100 --> 00:48:34,170
If you're a software
provider, you wanna be part

897
00:48:34,170 --> 00:48:36,120
of some of the programs,
if you have interest

898
00:48:36,120 --> 00:48:39,060
in how to get involved,
we'd love to talk to you

899
00:48:39,060 --> 00:48:41,730
after the session in the hall as well.

900
00:48:41,730 --> 00:48:43,380
And then, of course, the more you can do,

901
00:48:43,380 --> 00:48:47,070
there's an Intel booth
in the showcase as well.

902
00:48:47,070 --> 00:48:49,410
We have roundtables on different topics

903
00:48:49,410 --> 00:48:51,900
throughout the next couple of days,

904
00:48:51,900 --> 00:48:53,760
and you're welcome to meet the team

905
00:48:53,760 --> 00:48:55,950
and contact us just through this QR code.

906
00:48:55,950 --> 00:48:58,290
So thank you so much for taking your time

907
00:48:58,290 --> 00:49:00,960
and investing in this
session, and we appreciate it.

908
00:49:00,960 --> 00:49:02,227
Have a great day.

909
00:49:02,227 --> 00:49:04,878
(audience applauds)

