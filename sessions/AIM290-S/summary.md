# AWS re:Invent 2025 技术分享会总结

## 会议概述

本次AWS re:Invent 2025分享会聚焦于如何使用Intel CPU构建快速、成本高效且具有主权性的AI推理平台。会议由Intel和AWS联合举办,展示了双方长达近20年的深度合作关系。会议的核心主题是证明CPU(特别是Intel Xeon 6处理器)在AI推理工作负载中的强大能力,打破了"AI推理必须使用GPU"的传统观念。

会议通过三个实际客户案例深入探讨了Intel CPU在不同场景下的应用:Deloitte展示了在政府和公共部门如何使用CPU运行大语言模型和小语言模型;E& Enterprise分享了在阿联酋构建主权AI推理平台的经验;Articulate介绍了其利用企业数据构建超个性化AI解决方案的创新平台。整场会议强调了开放生态系统、成本效益、数据主权和安全性在企业级AI部署中的重要性。

Intel和AWS共同推出的第八代Intel Xeon 6定制处理器在EC2实例上实现了比上一代提升20%的计算性能,在基于推荐模型的推理任务中性能提升高达40%。该处理器集成了AMX(高级矩阵扩展)技术,专门优化了矩阵乘法运算,显著加速了AI推理和训练过程。

## 详细时间线与关键要点

### 00:00 - 开场介绍
- Diego介绍会议主题:使用Intel CPU在AWS上构建快速、成本高效且具有主权性的AI推理平台
- 介绍演讲嘉宾阵容:Intel副总裁Caitlyn Anderson、AWS公共部门GenAI总监Mickey、以及来自Deloitte、E& Enterprise和Articulate的客户代表
- 会议议程包括四个部分:Intel与AWS合作关系回顾、三个不同客户用例的深入分析

### 05:00 - Intel与AWS合作关系
- Caitlyn Anderson介绍Intel在AWS工作近24年,双方合作关系跨越近20年
- AWS上运行超过400个基于Intel的EC2实例类型
- 合作涵盖硬件基础设施、软件优化和从边缘到云端的全方位设备支持

### 08:00 - 合作视频展示
- Intel CEO Pat Gelsinger和AWS计算副总裁Dave Brown共同介绍合作成果
- 发布AWS第八代Intel实例,基于Intel Xeon 6 P-Core技术
- 新实例相比上一代Intel实例性能提升最高20%,实际应用中预期性能提升更高
- 引入实例带宽配置(IBC)功能,让客户可根据工作负载需求优化网络和存储性能
- 推出Flex变体实例,为计算模式可变的工作负载提供更具成本效益的选择

### 12:00 - Intel AI战略
- 强调开放生态系统方法,这是Intel公司理念的核心
- Intel正在大力投资x86生态系统,确保其强大和健康发展
- 向开源社区贡献AI软件开发成果

### 15:00 - ISV生态系统合作
- Intel与众多独立软件供应商(ISV)建立强大合作关系
- 展示与Adobe、SAP、Arqit等公司的合作案例
- 提供工具、技术和营销支持,帮助ISV在Intel平台上扩展业务

### 18:00 - CPU用于AI推理的理念
- 强调AI革命当前阶段主要集中在训练,但企业级AI规模化部署尚未真正开始
- 推理工作负载将成为AI下一阶段的主要增长点
- 代理式AI(Agentic AI)将成为未来几年的重要主题
- AI推理需要多种类型的计算硬件,不仅仅是GPU

### 21:00 - Xeon 6处理器演示
- 现场展示在Xeon处理器上直接运行简单推理工作负载的聊天应用案例
- 证明CPU完全可以高效运行AI推理任务

### 23:00 - Mickey介绍AWS与Intel合作
- 自2006年以来,Intel与AWS建立了强大的合作关系
- AWS在所有区域推出了400多个基于Intel的实例
- 2024年9月签署了数十亿美元的多年战略协议,投资于芯片制造

### 25:00 - 定制Xeon 6处理器详情
- AWS EC2 8i实例现已正式提供基于Intel 3工艺的定制Xeon 6处理器
- 覆盖C、R和M系列实例
- 强调"定制"的重要性:通过深度工程合作开发专用芯片
- 计算性能提升20%,内存带宽最快
- 在基于推荐模型的推理任务中性能提升40%
- 这是云端运行最快的Intel芯片

### 28:00 - AMX技术介绍
- Xeon 6 Intel芯片使用高级矩阵扩展(AMX)技术
- 优化矩阵乘法运算,提供更快的推理和训练速度
- 支持更广泛的推理工作负载类型
- 有助于节能并实现良好扩展

### 30:00 - 实例部署和可用性
- EC2 8i实例覆盖M、C和R系列
- 提供Flex选项,客户可根据需求匹配价格
- 已在美国和西班牙区域推出,正在向更多AWS区域扩展
- 阿联酋技术创新研究所(TII)使用该技术开发Falcon大语言模型

### 32:00 - Deloitte案例介绍
- Bob Simmons介绍Deloitte在政府和公共服务部门的工作
- 研究在CPU上运行大语言模型(LLM)和小语言模型(SLM)
- Deloitte AI战略:大力投资从业者培训和开发新能力

### 34:00 - 市场需求分析
- 市场需要成本高效的推理方式和可扩展性
- 许多AI项目未能实现ROI,无法从原型进入生产环境
- GPU供应不足且成本高昂,CPU提供了可行的替代方案

### 36:00 - 客户如何通过AI获胜
- 使用CPU配合SLM和压缩的LLM
- 大多数客户拥有大量基于CPU的EC2基础设施,易于扩展
- 成本降低50%以上
- 增强安全性:在政府专用安全网络(GPS)中运行

### 38:00 - 安全性优势
- AWS在非密、秘密和绝密网络上都有完整的云环境,三个云环境互不连接且不连接互联网
- 组织可以在自己的VPC内运行GenAI,数据永不离开,满足高安全要求
- 人力资源、合同等敏感部门可以使用独立的AI系统

### 40:00 - 当前AI数据中心现状
- AI数据中心预计将快速增长
- GPU供应不足,特别是在政府网络中
- 客户拥有大量基于CPU的EC2基础设施
- Intel Xeon CPU提供了更便宜的本地化GPU替代方案

### 42:00 - 技术解决方案
- GPU成本飙升,需求旺大
- 在GPU上运行大型语言模型对许多用例来说过度配置
- CPU驱动的模型提供预算友好的方式,实现更广泛的采用和民主化
- 小语言模型定义为100亿参数或更少
- 使用Intel OpenVINO等模型压缩工具和新压缩技术

### 44:00 - 测试阶段
- 第一阶段:在GPU和CPU上运行LLM和SLM,比较性能
- 第二阶段:测试压缩模型,使用量子启发压缩技术将70亿参数模型压缩到可在CPU上运行
- 第三阶段:解决压缩模型上下文窗口不足的问题

### 46:00 - 测试结果
- 第一阶段:SLM在CPU上运行成功,但未压缩的LLM太大
- 运营和维护基础设施成本降低56%
- 在效率(能源、成本、扩展可用性)方面CPU优于GPU,虽然速度不一定更快
- 第二阶段:压缩模型运行速度几乎是未压缩模型的两倍

### 48:00 - 准确性对比
- 第二阶段聊天机器人准确率71.3%,客户要求75%
- 第三阶段通过增加上下文窗口,准确率提升至80.7%
- 未压缩模型准确率为81.7%,压缩后仅损失1%
- 保留了未压缩模型98.5%的准确性

### 50:00 - 成本对比
- 使用Amazon EC2 8i实例(Xeon 4)
- GPU与CPU的月度成本对比显示成本降低56%
- 展示每秒令牌数和延迟对比图表
- OpenVINO模型服务器针对Xeon优化

### 52:00 - 性能分析
- 使用48个CPU的Xeon处理器
- 加载时在CPU之间进行并行处理
- 随着并发用户数增加(1、2用户等),CPU性能保持稳定
- 在特定点CPU性能超过GPU
- 根据用例,如果不是时间敏感型应用可使用CPU,时间敏感型应用使用GPU

### 54:00 - SLM目标用例
- 需要安全性的场景
- 需要扩展基础设施的场景
- 本地部署,CPU普遍而GPU不常见的环境
- 多代理用例:下一阶段将测试三个不同代理在基于CPU的EC2上运行
- 边缘设备:卫星、无人机、制造车间的机器

### 56:00 - 未来用例示例
- 敏感文档处理(情报报告)
- 多代理系统:第一个代理查找信息并向情报分析员确认
- 第二个代理删除信息并重写文档,再次确认
- 第三个代理根据删除的信息重新分类文档
- 使用压缩到可在8i实例上运行的70亿参数模型

### 58:00 - Intel软件生态系统
- Caitlyn强调Intel拥有数千名软件工程师
- 与所有主要框架提供商和大语言模型提供商合作
- 确保任何模型版本发布后都能快速在Intel上运行
- 介绍One API:针对Intel优化的软件工具
- One API在边缘业务中已使用多年,现在扩展到AI领域

### 60:00 - 开放标准和协议
- Intel开发并贡献开放标准和协议(如UPI、CXL)
- 强调对开放生态系统软件方法的贡献

### 62:00 - Intel AI企业推理软件
- 宣布推出Intel AI for Enterprise Inference
- 简单易用的软件部署,专为推理工作负载设计
- 选择AWS EC2 8i实例时自动包含该软件
- 从底层到顶层全栈优化Intel推理工作负载

### 64:00 - E& Enterprise案例介绍
- Amit Gupta介绍E& Enterprise数据和AI实践
- 覆盖亚洲、中东、非洲和欧洲地区
- 与Intel合作构建企业AI推理栈
- 为阿联酋客户创建开放、透明、成本高效且具有主权性的解决方案

### 66:00 - E& Enterprise服务范围
- 提供全栈服务:AI连接、AI基础设施、AI解决方案和服务、AI设备
- 已交付200多个AI用例,跨越多个行业和部门
- 技术和平台无关,与多个AI和数据平台及超大规模云服务商合作
- 拥有全球AI资源团队
- 开发内部AI加速器,在数周而非数月内创造价值
- 拥有AI学院,培训从业者和领导者,包括首席AI官项目

### 68:00 - 推理市场增长预测
- AI推理市场将呈指数级增长
- 从今年的1000亿美元增长到未来5年的2500亿美元以上
- 到2028年,AI推理成本占模型总生命周期成本的至少70%
- 推理成本远高于训练成本
- Gartner预测至少50%的GenAI项目将超出预算

### 70:00 - 企业面临的挑战
- 成本约束:企业规模化实施AI面临成本限制,通常超出计划300-500%
- 基础设施管理:AI工作负载对基础设施要求前所未有,传统IT系统无法应对
- 需要CPU、GPU、TPU和AI芯片等异构资源协同工作
- 数据主权:为阿联酋构建AI,所有数据必须保留在国内,符合本地规范

### 72:00 - SLM in a Box解决方案
- E& Enterprise的解决方案称为"SLM in a Box"(盒装小语言模型)
- 利用Intel AI for Enterprise Inference和Intel AI加速实例
- 构建主权推理即服务平台
- 双重目标:区域独立性和开放成本效益

### 74:00 - 主权性和成本效益
- 解决方案独立于其他AWS区域,仅服务阿联酋客户
- 通过阿联酋AWS区域提供服务
- 开放且成本高效
- 通过自动化、成本控制、安全性和透明度实现主权性
- 使用Intel 7 AI实例作为推理端点
- 客户可以在平台上构建用例,也可以使用现成的即部署用例

### 76:00 - 技术架构深入分析
- 打破"推理只能用GPU"的误区
- 运行参数低于100亿的小语言模型时,Intel实例是完美选择
- 与阿联酋多个合作伙伴合作,包括TII开发的Falcon模型

### 78:00 - 开放架构优势
- 架构仅使用开源组件
- 利用社区构建的成果,保持成本效益
- 灵活性:AI快速变化的环境中,平台支持选择和部署任何模型
- 开箱即用的模型部署能力

### 80:00 - 解决方案优势总结
- 降低复杂性:引入全自动化解决方案
- 快速实现价值:自动化部署和管理AI及LLM模型
- 可扩展:内置自动扩展功能
- 成本效益:利用Intel CPU作为推理端点
- 安全设计:内置身份验证和API部署
- 主权设计:仅为阿联酋客户部署,仅使用阿联酋区域,所有基础设施编排都在阿联酋AWS区域进行

### 82:00 - Articulate平台介绍
- Renato介绍Articulate是从Intel孵化的初创公司
- Articulate是智能层平台,帮助企业将复杂的非结构化或结构化数据转换为超个性化结果
- 垂直集成平台,支持跨不同代理和模型运行
- 可部署在本地或客户自己的AWS账户VPC中

### 84:00 - Articulate技术架构
- 数据进入平台后被分解为组件
- 通过内部模型网格编排器运行
- 使用模型库:结合传统机器学习模型、小语言模型和大语言模型
- 重点是领域特定模型和任务特定模型
- 理解数据并构建知识图谱

### 86:00 - 会议总结
- 强调Intel与AWS的长期合作关系和技术创新
- 展示了CPU在AI推理工作负载中的强大能力和成本优势
- 通过三个客户案例证明了解决方案的实际价值
- 突出了开放生态系统、数据主权和安全性在企业AI部署中的重要性