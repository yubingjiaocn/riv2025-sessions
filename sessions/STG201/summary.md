# AWS re:Invent 2025 存储服务在AI工作负载中的应用

## 会议概述

本次技术分享由Amazon S3高级技术产品经理Monica Veahore和Amazon FSx首席产品经理Jordan Dolman共同主讲，重点探讨了AWS存储服务如何支持客户构建和扩展AI应用场景。

会议的核心主题是如何让生成式AI真正为企业数据服务，并以合理的成本实现业务价值。演讲者强调，通用AI只能提供通用答案，而企业的独特数据（如客户反馈、使用模式等）才是AI产生真正商业价值的关键。当前最成功的企业能够让AI代理访问相关数据，从而驱动多步骤的AI工作流程，显著提升生产力。

演讲涵盖了从简单到复杂的多种AI实现方法，包括提示工程（Prompt Engineering）、检索增强生成（RAG）、AI代理（Agents）以及模型定制化训练。随着方法复杂度的提升，成本和时间投入会增加，但输出质量也会显著改善。特别值得关注的是本周刚刚正式发布的Amazon S3 Vectors服务，这是首个原生支持向量存储和查询的云对象存储服务，相比传统向量数据库可降低高达90%的成本。会议还介绍了模型上下文协议（MCP）如何标准化工具与AI代理的连接方式，以及S3 Metadata和S3 Tables如何通过元数据增强AI工作流的精准度。

## 详细时间线

### 开场与核心挑战 (00:00 - 05:30)

- **00:00** - Monica Veahore和Jordan Dolman介绍会议主题：AWS存储服务如何支持AI用例的构建和扩展
- **01:15** - 提出核心挑战：如何让AI适配企业自有数据，以合理成本为业务服务
- **02:00** - 强调数据的重要性：企业独特数据（客户反馈、使用模式）是AI产生商业价值的关键
- **02:45** - 指出当前成功企业的共同点：能够让AI代理访问相关数据以驱动多步骤工作流
- **03:30** - 阐述基本问题：大语言模型基于静态数据训练，不了解企业最新信息和业务上下文，容易产生幻觉

### 提示工程 (05:30 - 08:45)

- **05:30** - 介绍最简单的方法：提示工程（Prompt Engineering）
- **06:00** - Monica分享个人案例：使用提示工程撰写Amazon内部PR FAQ文档
- **06:45** - 说明提示工程包含示例、上下文和约束条件来引导模型响应
- **07:30** - 展示效果：原本需要数周的文档初稿现在几分钟即可完成，几天内可获得可审阅的草稿
- **08:15** - 提出提示工程的局限：难以扩展到数百个文档或多个数据源，上下文窗口会溢出导致质量下降

### 检索增强生成 (RAG) (08:45 - 18:30)

- **08:45** - 引入RAG（检索增强生成）作为可扩展解决方案
- **09:30** - 解释RAG工作原理：检索相关信息 → 增强原始提示 → 生成更好的响应
- **10:15** - 详细说明向量化过程：数据转换为向量（数值表示） → 查询也转换为向量 → 使用空间相似度搜索 → 找到相关内容增强提示
- **11:45** - 介绍数据摄取方法：批量摄取（适合不常变化的数据）和实时摄取（适合频繁变化的数据）
- **13:00** - 说明大多数客户使用S3进行数据摄取，因其低成本且可扩展
- **14:00** - 展示向量化管道：数据源选择 → 分块策略 → 嵌入模型选择 → 向量存储选择
- **15:15** - 介绍Bedrock Knowledge Bases可自动管理整个向量化管道
- **16:30** - 指出传统向量数据库的三大痛点：成本高、扩展性差、多租户应用需要数百万索引时成本失控

### Amazon S3 Vectors发布 (18:30 - 23:00)

- **18:30** - 宣布本周正式发布Amazon S3 Vectors（GA）
- **19:00** - 强调这是首个原生支持向量存储和查询的云对象存储服务
- **19:30** - 列举核心优势：上传、存储和查询成本降低高达90%
- **20:00** - 性能指标：热查询延迟100毫秒，冷查询延迟亚秒级
- **20:30** - 容量规格：每个索引最多20亿向量，每个存储桶最多10,000个索引（总计超过20万亿向量）
- **21:15** - 介绍三层定价模型：摄取费用、存储费用、查询费用（按实际使用付费，无需容量规划）
- **22:00** - 分享生物技术公司客户案例：使用S3 Vectors对3000万篇科学文献进行语义搜索，研究时间从数周缩短至实时查询

### 元数据过滤 (23:00 - 27:30)

- **23:00** - 引入元数据过滤概念：缩小搜索空间，提高查询速度和准确性
- **24:00** - 说明可使用AWS Glue对结构化数据编目，为非结构化数据添加自定义元数据
- **25:00** - 强调元数据的三大价值：提供上下文、追溯数据血缘、自动分类敏感信息（如PII）
- **26:30** - 总结到此为止已构建完整RAG系统：数据摄取到S3 → 向量化 → 元数据过滤 → 成本优化的向量存储

### AI代理 (Agents) (27:30 - 35:00)

- **27:30** - 提出更高级需求：处理复杂任务、使用多个工具和数据源、进行推理
- **28:15** - Monica分享路线图规划场景：需要整合客户反馈、产品评论、使用数据，并为每个痛点撰写PR FAQ
- **29:00** - 说明RAG无法满足此需求，需要引入AI代理
- **29:45** - 解释代理的核心能力：自主确定步骤、选择所需数据、选择合适工具
- **30:30** - 展示代理架构：目标+指令+上下文+工具 → 代理使用基础模型执行操作
- **31:30** - 介绍MCP（模型上下文协议）：标准化工具与代理的连接方式，类似HTTP标准化应用与后端通信

### MCP与工具集成 (35:00 - 40:00)

- **35:00** - 说明MCP包含客户端（定义"如何"查询）和服务器（执行查询并应用约束）
- **36:00** - 宣布AWS正在为多个服务构建MCP服务器
- **36:45** - 介绍Bedrock Knowledge Bases的MCP服务器：支持自然语言查询，无需API调用，可过滤特定部分、配置结果大小、重新排序输出
- **38:00** - 介绍S3 Metadata功能：自动生成和搜索结构化及非结构化数据的元数据
- **39:00** - 说明S3 Tables的MCP服务器：使用自然语言与S3表交互，无需SQL

### S3 Metadata与S3 Tables (40:00 - 43:30)

- **40:00** - 详细演示S3 Metadata工作流程：配置源存储桶 → 创建S3表存储桶 → 自动生成元数据表
- **41:00** - 说明元数据表每几分钟自动更新，跟踪新对象和系统元数据
- **42:00** - 展示查询方式：可使用Athena、QuickSight、Spark或任何SQL工具查询元数据
- **42:45** - 强调代理可通过自然语言访问S3 Tables，例如查询"前10名客户的月度使用情况"

### 现代AI技术栈总结 (43:30 - 45:00)

- **43:30** - 回顾完整技术栈：提示工程 → RAG → 向量化 → 元数据过滤 → S3 Vectors → 代理 → MCP
- **44:15** - 强调这是现代AI技术栈，许多客户在此阶段停止，因为已足够强大且可扩展
- **44:45** - 预告接下来由Jordan讲解更高级的工作负载：模型定制化

### 模型定制化基础 (45:00 - 50:00)

- **45:00** - Jordan接手讲解，说明提示工程和RAG的局限：数据和结构存在于上下文窗口（短期记忆）
- **46:00** - 提出需要将内容嵌入模型本身（长期记忆）
- **46:45** - 解释模型训练原理：知识深度嵌入模型权重中
- **47:30** - 根据数据量介绍不同技术：小数据量用标注数据更新部分权重，大数据量可更新更多权重或进行持续预训练
- **49:00** - 介绍两大服务：Amazon Bedrock（面向应用构建者）和Amazon SageMaker AI（面向模型开发者）

### 监督微调 (Supervised Fine-tuning) (50:00 - 54:00)

- **50:00** - 介绍三种标注数据技术：监督微调、蒸馏、对齐
- **50:45** - 详细说明监督微调：提供输入和期望输出的标注数据
- **51:30** - 展示非对话模型的标注数据格式：简单的提示-响应对
- **52:15** - 展示对话模型的标注数据格式：包含用户和助手标签，支持多轮对话
- **53:00** - 展示图像到文本模型的标注数据格式：图像引用+文字说明
- **53:45** - 强调提前考虑数据标注格式的重要性，特别是数据量有限时

### 蒸馏 (Distillation) (54:00 - 56:30)

- **54:00** - 介绍蒸馏技术：将大模型的能力转移到小模型
- **54:45** - 适用场景：喜欢大模型输出但需要更快速度或更低成本
- **55:30** - 工作原理：提供输入提示 → 大模型（教师）生成输出 → 输出与输入结合训练小模型（学生）
- **56:15** - 关键区别：不需要人工提供输出响应，由教师模型自动生成

### 对齐 (Alignment) (56:30 - 58:30)

- **56:30** - 介绍对齐技术：训练语气、合规性和品牌风格，而非知识
- **57:15** - 工作原理：提供提示、上下文以及首选和非首选响应
- **57:45** - 标注数据包含不同响应的评分，教会模型偏好某些输出
- **58:15** - 说明此功能目前仅在SageMaker AI中提供，Bedrock暂不支持

### 持续预训练与从头训练 (58:30 - 结束)

- **58:30** - 介绍持续预训练：拥有大量数据时，深度嵌入新知识、语气和数据关系
- **59:15** - 说明从头训练场景：现有模型的训练数据结构差异太大（如不同语言、非语言模型如天气预报或药物发现模型）
- **60:00** - 强调关键区别：这些方法资源密集，数据、计算和存储的集成至关重要
- **60:45** - 解释数据流：大量数据需高效加载到GPU/加速器实例，定期写入检查点到存储
- **61:30** - 列举检查点用途：故障恢复的保存点、模型评估点
- **62:15** - 提供性能数据：文本模型每GPU约128 MB/s，分布式训练可达数十GB/s；多模态或视频模型需求更高
- **63:00** - 强调IO大小的影响：小文件读取时开销（认证、网络延迟、元数据查找）占主导，低延迟存储可显著缩短作业完成时间
- **64:00** - 强调保持GPU/加速器实例高效运行的重要性，因为训练成本高昂