1
00:00:05,340 --> 00:00:06,300
- All right everybody.

2
00:00:06,300 --> 00:00:10,500
Thank you for joining
us at re:Invent 2025.

3
00:00:10,500 --> 00:00:13,380
It is exciting to see a full room today.

4
00:00:13,380 --> 00:00:15,300
My name is Suraj Sajo,

5
00:00:15,300 --> 00:00:18,060
I'm an AWS Customer Solutions Manager

6
00:00:18,060 --> 00:00:21,390
working with our largest global
financial service customers

7
00:00:21,390 --> 00:00:24,000
on their enterprise
transformation journey.

8
00:00:24,000 --> 00:00:27,000
Welcome to Secure in Milliseconds
where you will learn about

9
00:00:27,000 --> 00:00:31,560
how Visa built an AI
powered fraud defense on AWS

10
00:00:31,560 --> 00:00:34,140
to secure account to account payments.

11
00:00:34,140 --> 00:00:35,670
I'm joined today by Hema Zutshi,

12
00:00:35,670 --> 00:00:38,040
Senior Director of Engineering at Visa

13
00:00:38,040 --> 00:00:40,680
and Aravinda Nagalla, Chief Architect

14
00:00:40,680 --> 00:00:44,190
for the Visa Protect A2A product at Visa.

15
00:00:44,190 --> 00:00:45,840
It's a pleasure to have them here with me.

16
00:00:45,840 --> 00:00:48,210
We've had a phenomenal
partnership over the years

17
00:00:48,210 --> 00:00:50,310
and it is a testament to
their team's hard work

18
00:00:50,310 --> 00:00:53,670
that we're able to share key
learnings from their journey.

19
00:00:53,670 --> 00:00:56,310
Let's go ahead and get into the agenda.

20
00:00:56,310 --> 00:00:59,520
First, I will talk about the
growing payments landscape,

21
00:00:59,520 --> 00:01:02,610
focusing specifically on
account to account payments,

22
00:01:02,610 --> 00:01:06,150
its increased relevance
and the growth and risk.

23
00:01:06,150 --> 00:01:08,400
Then, my colleagues from
Visa will talk about

24
00:01:08,400 --> 00:01:10,920
how they secure this
risk through Visa Protect

25
00:01:10,920 --> 00:01:12,630
for account to account payments,

26
00:01:12,630 --> 00:01:15,480
the high bar of requirements
that they held themselves to

27
00:01:15,480 --> 00:01:18,030
and the journey in building this on AWS.

28
00:01:18,030 --> 00:01:22,440
We will deep dive into how Visa
leveraged AWS Nitro Enclaves

29
00:01:22,440 --> 00:01:26,220
to meet stringent security
requirements, achieve low latency

30
00:01:26,220 --> 00:01:31,050
through Amazon EKS
optimizations and Memory DB

31
00:01:31,050 --> 00:01:33,720
and built for a high bar of resilience.

32
00:01:33,720 --> 00:01:36,390
We will then wrap up by
reflecting on the key takeaways

33
00:01:36,390 --> 00:01:37,790
Visa had from their journey.

34
00:01:40,080 --> 00:01:44,580
So in today's day and age,
consumers and businesses alike

35
00:01:44,580 --> 00:01:47,070
are looking for
flexibility and convenience

36
00:01:47,070 --> 00:01:49,680
when it comes to paying
for goods or services.

37
00:01:49,680 --> 00:01:52,860
For years, the swipe of the
credit card had done the job.

38
00:01:52,860 --> 00:01:55,710
For decades, this had been
the backbone of commerce

39
00:01:55,710 --> 00:01:58,563
and it still continues
to play a critical role.

40
00:01:59,610 --> 00:02:02,010
Then, we saw the rise
of the digital wallet.

41
00:02:02,010 --> 00:02:04,230
All of a sudden with
the tap of your phone,

42
00:02:04,230 --> 00:02:06,690
you can now pay for
your goods and services

43
00:02:06,690 --> 00:02:09,750
in a seamless, contactless
checkout manner.

44
00:02:09,750 --> 00:02:12,250
This was the next stage
in the payments evolution,

45
00:02:13,290 --> 00:02:15,270
but with the rise of digital technology,

46
00:02:15,270 --> 00:02:16,860
consumers and businesses

47
00:02:16,860 --> 00:02:19,200
are looking for even more streamlined

48
00:02:19,200 --> 00:02:21,333
cost optimal methods of payment.

49
00:02:22,410 --> 00:02:24,480
This is where account to account payments

50
00:02:24,480 --> 00:02:27,060
or A2A payments come into play.

51
00:02:27,060 --> 00:02:30,690
A2A payments are secure,
reliable, streamlined,

52
00:02:30,690 --> 00:02:33,990
instant payment method that
moves payments or funds

53
00:02:33,990 --> 00:02:35,610
from the sender's bank account

54
00:02:35,610 --> 00:02:38,640
directly to the recipient's
bank account without the need

55
00:02:38,640 --> 00:02:42,150
for additional intermediaries
like credit card networks

56
00:02:42,150 --> 00:02:44,070
or credit card instruments.

57
00:02:44,070 --> 00:02:46,650
There's various use cases for A2A payments

58
00:02:46,650 --> 00:02:49,320
like person to person, person to business,

59
00:02:49,320 --> 00:02:51,390
business to person and beyond.

60
00:02:51,390 --> 00:02:53,553
There are two types of A2A payments.

61
00:02:54,570 --> 00:02:56,280
First is the push payment.

62
00:02:56,280 --> 00:02:58,320
The push payment is
initiated by the sender

63
00:02:58,320 --> 00:03:01,410
and moves funds directly from
the sender's bank account

64
00:03:01,410 --> 00:03:03,360
to the recipient's bank account.

65
00:03:03,360 --> 00:03:04,860
We've all probably used this.

66
00:03:04,860 --> 00:03:07,140
A great example is let's
say a buddy of mine

67
00:03:07,140 --> 00:03:09,690
covered me for lunch yesterday
and I need to pay him back.

68
00:03:09,690 --> 00:03:12,180
I'll go ahead and send him a push payment.

69
00:03:12,180 --> 00:03:14,640
The next is the pull payment.

70
00:03:14,640 --> 00:03:17,520
The pull payment is
initiated by the recipient

71
00:03:17,520 --> 00:03:19,680
and moves funds from the
sender's bank account

72
00:03:19,680 --> 00:03:21,750
to the recipient's bank account.

73
00:03:21,750 --> 00:03:26,280
Examples of this include a
business that has subscriptions

74
00:03:26,280 --> 00:03:28,080
or recurring payments

75
00:03:28,080 --> 00:03:30,240
that they would like to
receive from their clients

76
00:03:30,240 --> 00:03:32,070
at a agreed upon time interval

77
00:03:32,070 --> 00:03:34,380
and so they initiate a pull payment.

78
00:03:34,380 --> 00:03:35,460
Let's go ahead and look at

79
00:03:35,460 --> 00:03:38,160
how the A2A payment actually flows.

80
00:03:38,160 --> 00:03:42,570
First, the consumer or
business requests the payment

81
00:03:42,570 --> 00:03:44,220
and that request gets routed

82
00:03:44,220 --> 00:03:48,240
to the sending financial
institution who then processes it

83
00:03:48,240 --> 00:03:51,600
and initiates the payment via
a real-time payment network

84
00:03:51,600 --> 00:03:53,460
or RTP network.

85
00:03:53,460 --> 00:03:56,670
Few examples of RTP networks
include Pix in Brazil,

86
00:03:56,670 --> 00:04:01,380
UPI in India, Faster Payments
in the UK and beyond.

87
00:04:01,380 --> 00:04:04,050
The realtime payment network
then forwards the payment

88
00:04:04,050 --> 00:04:08,130
to the receiving financial
institution who then processes it

89
00:04:08,130 --> 00:04:10,893
and posts it to the receiver.

90
00:04:11,790 --> 00:04:14,250
This more streamlined payment method

91
00:04:14,250 --> 00:04:16,713
has seen increased
adoption over the years.

92
00:04:18,090 --> 00:04:22,080
That is attributed primarily
due to the cost optimizations

93
00:04:22,080 --> 00:04:24,690
available through RTP networks.

94
00:04:24,690 --> 00:04:28,080
RTP networks are significantly
more cost efficient

95
00:04:28,080 --> 00:04:29,613
than credit card networks.

96
00:04:31,290 --> 00:04:36,090
Juniper Research is forecasting 83% growth

97
00:04:36,090 --> 00:04:40,440
of the annual number of
transactions in the A2A landscape

98
00:04:40,440 --> 00:04:45,440
by 2029, reaching 1 trillion
annual transactions.

99
00:04:46,110 --> 00:04:49,350
By 2030, the annual transaction value

100
00:04:49,350 --> 00:04:54,350
is expected to grow by
113%, reaching $195 trillion

101
00:04:57,120 --> 00:05:00,210
of transactions annually.

102
00:05:00,210 --> 00:05:03,090
That is tremendous scale
and tremendous growth.

103
00:05:03,090 --> 00:05:05,220
But why does this matter?

104
00:05:05,220 --> 00:05:08,460
Well, as A2A payments grow,
the transaction rates grow,

105
00:05:08,460 --> 00:05:12,663
the transaction value grows,
so does the associated fraud.

106
00:05:13,500 --> 00:05:17,610
Juniper research also
forecasts that the annual value

107
00:05:17,610 --> 00:05:22,610
of fraud related losses is
expected to grow by 153%,

108
00:05:22,950 --> 00:05:25,980
reaching $58 billion

109
00:05:25,980 --> 00:05:28,650
worth of losses in the banking segment.

110
00:05:28,650 --> 00:05:30,990
That is not a negligible amount

111
00:05:30,990 --> 00:05:33,450
and this risk and this fraud

112
00:05:33,450 --> 00:05:36,930
is exactly what customers
like Visa are solving for

113
00:05:36,930 --> 00:05:41,430
by leveraging AWS while
meeting stringent requirements

114
00:05:41,430 --> 00:05:44,310
around security, latency, resilience

115
00:05:44,310 --> 00:05:47,250
and even data localization requirements

116
00:05:47,250 --> 00:05:49,950
while enabling tremendous scale.

117
00:05:49,950 --> 00:05:51,810
And so I invite my colleague Hema

118
00:05:51,810 --> 00:05:55,470
to tell us how Visa secured
account to account payments

119
00:05:55,470 --> 00:05:56,570
and built this on AWS.

120
00:06:03,135 --> 00:06:06,060
- Thank you Suraj for
setting that context.

121
00:06:06,060 --> 00:06:07,680
My name is Hema Zutshi.

122
00:06:07,680 --> 00:06:11,283
I'm a Senior Director
of Engineering at Visa.

123
00:06:12,480 --> 00:06:15,870
Visa is synonymous with trust

124
00:06:15,870 --> 00:06:20,670
and to build that trust Visa
has invested over the years

125
00:06:20,670 --> 00:06:24,300
in core principles around resilience,

126
00:06:24,300 --> 00:06:28,020
high uptime, high availability,

127
00:06:28,020 --> 00:06:32,763
stringent adherence to
cybersecurity and low latency.

128
00:06:34,530 --> 00:06:37,260
And based on these principles

129
00:06:37,260 --> 00:06:41,640
as Suraj was talking about
account to account payments,

130
00:06:41,640 --> 00:06:44,190
growth of account to account payments,

131
00:06:44,190 --> 00:06:47,040
wherever there is a
growth in the payments,

132
00:06:47,040 --> 00:06:49,860
unfortunately fraud follows.

133
00:06:49,860 --> 00:06:53,400
And since these account to
account payments do not use

134
00:06:53,400 --> 00:06:58,400
the existing fraud payment
solutions of the card network,

135
00:06:59,190 --> 00:07:02,520
Visa built a solution specifically

136
00:07:02,520 --> 00:07:05,370
for non-card payments

137
00:07:05,370 --> 00:07:08,010
called Visa Protect Account to Account,

138
00:07:08,010 --> 00:07:11,220
which is aimed at enhancing the security

139
00:07:11,220 --> 00:07:13,590
of non card payments.

140
00:07:13,590 --> 00:07:15,840
It's designed to address the demand

141
00:07:15,840 --> 00:07:19,590
of the greater security
in non card payments

142
00:07:19,590 --> 00:07:21,450
and as Suraj was talking about,

143
00:07:21,450 --> 00:07:25,767
it caters to all kinds of
non card payments B2B, B2C,

144
00:07:25,767 --> 00:07:26,677
C2C and C2B.

145
00:07:28,950 --> 00:07:30,630
To build this solution,

146
00:07:30,630 --> 00:07:34,770
we leveraged the AWS infrastructure

147
00:07:34,770 --> 00:07:37,830
and this infrastructure
allowed us to build

148
00:07:37,830 --> 00:07:40,410
both our scoring engine,

149
00:07:40,410 --> 00:07:44,610
which basically gives the fraud
score back to our customers

150
00:07:44,610 --> 00:07:46,620
and they can make a decision on this

151
00:07:46,620 --> 00:07:50,400
as well as our model
inferencing AIP platform,

152
00:07:50,400 --> 00:07:54,870
which is our backbone for giving the score

153
00:07:54,870 --> 00:07:59,553
back to our customers based
off the account models.

154
00:08:03,570 --> 00:08:06,690
In Suraj's diagram earlier we talked about

155
00:08:06,690 --> 00:08:08,280
account to account payments,

156
00:08:08,280 --> 00:08:12,210
how when a sender receives,
sender send a payment

157
00:08:12,210 --> 00:08:13,500
through its sending FI,

158
00:08:13,500 --> 00:08:16,470
which is sender's Financial Institute,

159
00:08:16,470 --> 00:08:19,050
it goes to RTP networks.

160
00:08:19,050 --> 00:08:21,360
Here to avoid the fraud

161
00:08:21,360 --> 00:08:23,670
between the two sender and receiver,

162
00:08:23,670 --> 00:08:27,330
to avoid that kind of fraud,
we are introducing a call

163
00:08:27,330 --> 00:08:29,940
made to Visa Protect Account to Account.

164
00:08:29,940 --> 00:08:34,940
Here this call lets the sending
FI decide based off a score

165
00:08:37,680 --> 00:08:39,360
that Visa Protect Account to Account

166
00:08:39,360 --> 00:08:40,890
is going to return back.

167
00:08:40,890 --> 00:08:43,470
To make a decision, if the score is high,

168
00:08:43,470 --> 00:08:47,010
it can stop the payment
to go to the RTP network,

169
00:08:47,010 --> 00:08:49,800
thus preventing the fraud from happening.

170
00:08:49,800 --> 00:08:51,510
Now let's say for whatever reason,

171
00:08:51,510 --> 00:08:54,840
if we are not able to make that call

172
00:08:54,840 --> 00:08:56,370
to Visa Protect Account to Account

173
00:08:56,370 --> 00:08:57,690
through the sending FI audit

174
00:08:57,690 --> 00:09:00,090
did not get the high enough score.

175
00:09:00,090 --> 00:09:04,950
There's another opportunity
for us at the receiving FI,

176
00:09:04,950 --> 00:09:08,520
Receiving Financial Institute,
which is a receiver's bank

177
00:09:08,520 --> 00:09:11,310
to make a call to Visa
Protect Account to Account

178
00:09:11,310 --> 00:09:14,070
and stop the fraud from fully happening.

179
00:09:14,070 --> 00:09:17,103
It can reject the payment
from the RTP network.

180
00:09:20,490 --> 00:09:24,030
Visa builds solutions
for its global audiences

181
00:09:24,030 --> 00:09:26,790
for all over various different regions.

182
00:09:26,790 --> 00:09:29,430
Within various regions like UK,

183
00:09:29,430 --> 00:09:33,570
the liability of fraud lies
both on sender and receiver.

184
00:09:33,570 --> 00:09:37,050
So to protect both senders and receivers

185
00:09:37,050 --> 00:09:39,120
from their liability of fraud,

186
00:09:39,120 --> 00:09:41,400
Visa Protect Account to
Account is a solution

187
00:09:41,400 --> 00:09:43,800
that a lot of our customers are using

188
00:09:43,800 --> 00:09:46,023
to prevent fraud from happening.

189
00:09:48,690 --> 00:09:52,000
In the next few slides,
I will be talking about

190
00:09:53,460 --> 00:09:57,360
the requirements that we
had to build this solution

191
00:09:57,360 --> 00:10:00,330
and how did we leverage AWS platform

192
00:10:00,330 --> 00:10:01,920
for building this solution.

193
00:10:01,920 --> 00:10:04,260
Visa has traditionally built its solutions

194
00:10:04,260 --> 00:10:06,330
within its on-prem systems

195
00:10:06,330 --> 00:10:09,540
so a lot of our requirements
were catered towards our

196
00:10:09,540 --> 00:10:10,920
on-prem data centers.

197
00:10:10,920 --> 00:10:13,200
We had to rethink some of that

198
00:10:13,200 --> 00:10:15,240
and make sure we are able to build it

199
00:10:15,240 --> 00:10:19,110
as per the same guidance
as we are building

200
00:10:19,110 --> 00:10:23,523
to protect our customer
data in the on-prem systems.

201
00:10:25,680 --> 00:10:27,150
So the first requirement

202
00:10:27,150 --> 00:10:32,150
is adherence to very stringent
cybersecurity guidelines.

203
00:10:32,220 --> 00:10:35,400
This is not just a buzzword for Visa.

204
00:10:35,400 --> 00:10:38,070
In Visa, this is taken
very, very seriously

205
00:10:38,070 --> 00:10:43,070
because our customers trust
us with their PII information

206
00:10:43,860 --> 00:10:45,780
and with their PII data.

207
00:10:45,780 --> 00:10:48,840
So on top of your regular DSRs,

208
00:10:48,840 --> 00:10:51,210
which is design security requirements

209
00:10:51,210 --> 00:10:53,070
and technical security requirements

210
00:10:53,070 --> 00:10:56,850
and everything that comes
with data protection,

211
00:10:56,850 --> 00:10:59,160
we had a unique problem to solve

212
00:10:59,160 --> 00:11:02,610
as we were taking our solution into cloud,

213
00:11:02,610 --> 00:11:07,610
which was to protect against
in-memory data exposure.

214
00:11:07,860 --> 00:11:12,270
Data is encrypted, protected
when it is in transit

215
00:11:12,270 --> 00:11:14,010
or when it is stored.

216
00:11:14,010 --> 00:11:16,470
It is not protected however,

217
00:11:16,470 --> 00:11:19,020
when it is getting processed in memory

218
00:11:19,020 --> 00:11:22,620
and it is these times that
various threat vectors

219
00:11:22,620 --> 00:11:24,030
and malicious agents

220
00:11:24,030 --> 00:11:27,690
try and steal the sensitive data.

221
00:11:27,690 --> 00:11:31,440
Threat vectors like core
dump and swap files,

222
00:11:31,440 --> 00:11:33,000
exposed data endpoints

223
00:11:33,000 --> 00:11:37,080
and memory scraping hardware,
this is the chance where they,

224
00:11:37,080 --> 00:11:40,650
they can get into the cloud network

225
00:11:40,650 --> 00:11:43,860
and steal customer information.

226
00:11:43,860 --> 00:11:45,900
To protect against that,

227
00:11:45,900 --> 00:11:50,640
not only did we build
another layer of security,

228
00:11:50,640 --> 00:11:52,623
we rethought the architecture.

229
00:11:53,490 --> 00:11:56,340
We went our solution against this threat

230
00:11:56,340 --> 00:11:59,130
was to go with zero trust architecture

231
00:11:59,130 --> 00:12:01,833
and use AWS Nitro Enclaves.

232
00:12:03,030 --> 00:12:05,040
Think about Nitro Enclaves

233
00:12:05,040 --> 00:12:09,030
as a secure vault within a main server

234
00:12:09,030 --> 00:12:12,120
and nobody has access to that secure vault

235
00:12:12,120 --> 00:12:15,000
and all communication to that secure vault

236
00:12:15,000 --> 00:12:17,940
happens through the main
server on a secure channel

237
00:12:17,940 --> 00:12:21,360
and nobody, not even AWS or VISA accounts

238
00:12:21,360 --> 00:12:23,250
have access to that.

239
00:12:23,250 --> 00:12:25,800
My colleague Aravinda is
gonna deep dive on this

240
00:12:25,800 --> 00:12:27,603
in next few segments.

241
00:12:31,080 --> 00:12:33,960
Next very important requirement

242
00:12:33,960 --> 00:12:38,760
to build a solution that
caters to all the regions

243
00:12:38,760 --> 00:12:41,100
that we support is to adhere

244
00:12:41,100 --> 00:12:43,860
to the local regional requirements.

245
00:12:43,860 --> 00:12:46,110
Since we were building this solution

246
00:12:46,110 --> 00:12:50,370
for our European markets,

247
00:12:50,370 --> 00:12:53,250
one of the very important
requirements for us

248
00:12:53,250 --> 00:12:56,820
was to build around GDPR requirements.

249
00:12:56,820 --> 00:13:00,750
Even though GDPR does not
necessarily request you

250
00:13:00,750 --> 00:13:05,750
to have data localized, we are able to

251
00:13:06,300 --> 00:13:09,900
do better with their requirements
if the data is localized.

252
00:13:09,900 --> 00:13:12,990
We also had requirements from our banks

253
00:13:12,990 --> 00:13:16,323
or our customers to keep data locally.

254
00:13:18,187 --> 00:13:20,280
For that specific reason,

255
00:13:20,280 --> 00:13:24,100
our solution to build the
solution to this platform

256
00:13:25,920 --> 00:13:30,920
was to use AWS regions

257
00:13:30,990 --> 00:13:34,140
for both transactional and
model inferencing engines

258
00:13:34,140 --> 00:13:38,317
to be hosted in the
European regions in AWS.

259
00:13:40,590 --> 00:13:43,410
And this not only helped us

260
00:13:43,410 --> 00:13:47,070
with the localization
requirement, this also helps us

261
00:13:47,070 --> 00:13:50,133
with the next requirement
which is around low latency.

262
00:13:52,380 --> 00:13:56,628
Again, low latency is not just,

263
00:13:56,628 --> 00:13:58,800
we don't take it lightly,

264
00:13:58,800 --> 00:14:01,110
it's a very, very serious requirement

265
00:14:01,110 --> 00:14:03,240
in the financial sector.

266
00:14:03,240 --> 00:14:06,120
Transactions happen within few seconds

267
00:14:06,120 --> 00:14:08,910
and any kind of latency

268
00:14:08,910 --> 00:14:11,730
can break the overall user experience.

269
00:14:11,730 --> 00:14:13,440
Take for example,

270
00:14:13,440 --> 00:14:15,570
let's talk about the
example that Suraj took

271
00:14:15,570 --> 00:14:18,420
where two friends are splitting the bill.

272
00:14:18,420 --> 00:14:21,270
Now that is happening
over the RTP networks.

273
00:14:21,270 --> 00:14:26,250
If we now want to add another
layer to it of security

274
00:14:26,250 --> 00:14:28,680
by using Visa Protect Account to Account,

275
00:14:28,680 --> 00:14:32,310
we cannot afford to extend that time

276
00:14:32,310 --> 00:14:34,350
of end-to-end transaction
between these two

277
00:14:34,350 --> 00:14:36,813
because that will break
the overall experience.

278
00:14:37,710 --> 00:14:38,610
To make sure

279
00:14:38,610 --> 00:14:41,760
that we are able to provide
this fraud solution,

280
00:14:41,760 --> 00:14:45,870
we had to make sure the
solution that we are building

281
00:14:45,870 --> 00:14:48,300
is able to give the end-to-end results

282
00:14:48,300 --> 00:14:50,280
in less than 250 milliseconds,

283
00:14:50,280 --> 00:14:53,010
that is a fraction of a second.

284
00:14:53,010 --> 00:14:57,630
Which means that you have
to accept the transaction,

285
00:14:57,630 --> 00:15:02,040
process it, run it through our models

286
00:15:02,040 --> 00:15:05,433
and respond back within 250 milliseconds.

287
00:15:06,450 --> 00:15:11,370
To solution this, we looked
at two different approaches.

288
00:15:11,370 --> 00:15:14,220
One was co-localization,

289
00:15:14,220 --> 00:15:16,560
which is what I talked
about in the previous slide,

290
00:15:16,560 --> 00:15:20,520
where to avoid any kind
of network latency,

291
00:15:20,520 --> 00:15:23,280
any time we are trying to
create a TLS connection,

292
00:15:23,280 --> 00:15:25,350
it goes back and forth a few times.

293
00:15:25,350 --> 00:15:27,420
That takes time to establish.

294
00:15:27,420 --> 00:15:29,610
To avoid that latency,

295
00:15:29,610 --> 00:15:32,730
we made sure that we build our solutions

296
00:15:32,730 --> 00:15:37,350
within the local regions
and we also architected

297
00:15:37,350 --> 00:15:42,333
our solution based off
Amazon Memory DB with Valkey.

298
00:15:43,470 --> 00:15:45,840
Now this is a very interesting solution

299
00:15:45,840 --> 00:15:49,110
because not only does Amazon Memory DB

300
00:15:49,110 --> 00:15:51,840
gives us faster reads,

301
00:15:51,840 --> 00:15:54,930
but it also gives us durability of data,

302
00:15:54,930 --> 00:15:56,760
which is the right combination

303
00:15:56,760 --> 00:16:01,380
to be used in a financial solution

304
00:16:01,380 --> 00:16:03,880
which basically gives us the

305
00:16:05,280 --> 00:16:10,280
faster read time of a cache
and durability of a database.

306
00:16:11,580 --> 00:16:14,490
Aravinda will talk in detail about this.

307
00:16:14,490 --> 00:16:19,490
We also used a lot of TLS
connection pooling mechanisms

308
00:16:19,500 --> 00:16:24,500
to help be able to give us
the 250 milliseconds latency

309
00:16:25,410 --> 00:16:27,483
that Aravinda will be covering as well.

310
00:16:35,940 --> 00:16:39,300
The last business requirement
that I want to talk about

311
00:16:39,300 --> 00:16:42,483
is around resiliency
and high availability.

312
00:16:43,920 --> 00:16:48,120
One of the principles for our
trust are high availability,

313
00:16:48,120 --> 00:16:50,160
resilience and uptime.

314
00:16:50,160 --> 00:16:52,950
Within Visa, there is a categorization

315
00:16:52,950 --> 00:16:54,810
within tiers for our applications,

316
00:16:54,810 --> 00:16:59,160
anything that is transactional
will be considered a tier RT

317
00:16:59,160 --> 00:17:02,970
or a tier zero and to be
able to consider a tier RT

318
00:17:02,970 --> 00:17:04,350
or a tier zero,

319
00:17:04,350 --> 00:17:09,350
one needs to be able to fulfill
at least 99.99% of uptime.

320
00:17:10,260 --> 00:17:14,370
Think about this uptime that
within this an entire year,

321
00:17:14,370 --> 00:17:16,770
application can only go down,

322
00:17:16,770 --> 00:17:21,210
it cannot go down for more
than 52 minutes, 35 seconds.

323
00:17:21,210 --> 00:17:24,450
Beyond that, we will be breaking our SLAs.

324
00:17:24,450 --> 00:17:26,190
There is another important requirement

325
00:17:26,190 --> 00:17:28,560
that we had to cater to.

326
00:17:28,560 --> 00:17:31,350
Within Visa we have our internal

327
00:17:31,350 --> 00:17:34,050
availability disaster
recovery requirements,

328
00:17:34,050 --> 00:17:35,880
which we call ITDR

329
00:17:35,880 --> 00:17:37,800
and to cater to those ITDRs,

330
00:17:37,800 --> 00:17:42,420
we have to build redundancy
in at least dual regions

331
00:17:42,420 --> 00:17:43,890
and within that redundancy,

332
00:17:43,890 --> 00:17:47,940
the regions have to be
more than 60 miles apart.

333
00:17:47,940 --> 00:17:49,980
So to build our solution

334
00:17:49,980 --> 00:17:54,323
to cater to our tier zero
uptime requirements of 99.99,

335
00:17:55,830 --> 00:18:00,830
our solution was to build our
application within two regions

336
00:18:03,030 --> 00:18:05,430
and give them redundancy both in London

337
00:18:05,430 --> 00:18:08,520
and Ireland region with a multi AZ setup,

338
00:18:08,520 --> 00:18:09,900
we are using three AZs,

339
00:18:09,900 --> 00:18:14,490
availability zones within AWS setup.

340
00:18:14,490 --> 00:18:17,790
So this was all about various requirements

341
00:18:17,790 --> 00:18:19,503
that we had to build around.

342
00:18:20,460 --> 00:18:22,980
I'll spend some time talking about

343
00:18:22,980 --> 00:18:24,990
the functional architecture

344
00:18:24,990 --> 00:18:27,180
of Visa Protect Account to Account

345
00:18:27,180 --> 00:18:30,240
and then Aravinda will go into the details

346
00:18:30,240 --> 00:18:33,540
of the architecture.

347
00:18:33,540 --> 00:18:37,320
So Visa Protect Account to
Account is a scoring engine

348
00:18:37,320 --> 00:18:39,780
which basically takes a request

349
00:18:39,780 --> 00:18:43,500
and processes it, runs
it through our models

350
00:18:43,500 --> 00:18:48,030
and gives a score back
to the end customers.

351
00:18:48,030 --> 00:18:50,340
To enable this, we have
two important components,

352
00:18:50,340 --> 00:18:55,340
we have the real time
component which has scoring API

353
00:18:55,440 --> 00:18:57,840
as well as the model inferencing API.

354
00:18:57,840 --> 00:19:01,380
Here, the scoring API talks
to our external clients,

355
00:19:01,380 --> 00:19:03,540
it does all the data processing,

356
00:19:03,540 --> 00:19:06,480
it does all the data
acceptance, security checks,

357
00:19:06,480 --> 00:19:08,730
everything and then decides to,

358
00:19:08,730 --> 00:19:12,563
through a decision service
decides to call the right model

359
00:19:12,563 --> 00:19:14,573
for its inferencing.

360
00:19:14,573 --> 00:19:17,820
Within model inferencing we
have aggregation service,

361
00:19:17,820 --> 00:19:21,540
we have orchestration service,
we have long-term profiles

362
00:19:21,540 --> 00:19:25,320
that run and are able to provide a score,

363
00:19:25,320 --> 00:19:27,903
a risk score for a transaction.

364
00:19:28,860 --> 00:19:30,900
Along with the real time system

365
00:19:30,900 --> 00:19:32,790
which is the core of our platform,

366
00:19:32,790 --> 00:19:36,210
we also have a near real
time offline system,

367
00:19:36,210 --> 00:19:38,725
which uses a scoring API

368
00:19:38,725 --> 00:19:41,190
as well as a feature engineering platform.

369
00:19:41,190 --> 00:19:45,300
Scoring API is an important offline system

370
00:19:45,300 --> 00:19:48,960
which accepts daily files

371
00:19:48,960 --> 00:19:51,810
where daily status files, it accepts the,

372
00:19:51,810 --> 00:19:55,590
It has data consumers that
accepts data from our clients.

373
00:19:55,590 --> 00:19:59,310
It also, any time we are
onboarding a new client,

374
00:19:59,310 --> 00:20:03,630
this is another system that
allows us to get historical data

375
00:20:03,630 --> 00:20:07,870
of the clients and we are
able to build our features

376
00:20:09,450 --> 00:20:11,910
based off that data.

377
00:20:11,910 --> 00:20:13,650
Our feature engineering components

378
00:20:13,650 --> 00:20:16,440
has both longtime profile generation

379
00:20:16,440 --> 00:20:20,620
and short time profile generation features

380
00:20:21,480 --> 00:20:24,060
along with your billing, reporting

381
00:20:24,060 --> 00:20:28,770
and all other components
that comprise a system.

382
00:20:28,770 --> 00:20:32,730
So with that, I'll hand
it over to Aravinda

383
00:20:32,730 --> 00:20:34,880
who will be going deep
on the architecture.

384
00:20:42,210 --> 00:20:43,650
- Thanks Hema.

385
00:20:43,650 --> 00:20:46,380
Hello everyone, my name
is Aravinda Nagalla,

386
00:20:46,380 --> 00:20:48,900
I'm a Solution Architect at Visa.

387
00:20:48,900 --> 00:20:51,780
Next I'll walk through
the technical architecture

388
00:20:51,780 --> 00:20:55,050
for the functional components
that Hema just described.

389
00:20:55,050 --> 00:20:56,280
Later, we will dive deep

390
00:20:56,280 --> 00:20:57,930
into some of the AWS technologies

391
00:20:57,930 --> 00:20:59,523
we used in our architecture.

392
00:21:02,430 --> 00:21:04,260
Let's start by looking closely

393
00:21:04,260 --> 00:21:07,290
at our realtime scoring API workload.

394
00:21:07,290 --> 00:21:10,560
This is the REST API that our clients use

395
00:21:10,560 --> 00:21:13,293
to get their score from our model.

396
00:21:14,280 --> 00:21:17,760
Client requests land in our DMZ VPC.

397
00:21:17,760 --> 00:21:21,940
Think of this as a secure
perimeter facing the internet

398
00:21:23,070 --> 00:21:24,900
where each incoming packet

399
00:21:24,900 --> 00:21:28,140
is inspected by Amazon network firewall

400
00:21:28,140 --> 00:21:30,900
and only the traffic from
white-listed IP addresses

401
00:21:30,900 --> 00:21:32,073
is allowed inside.

402
00:21:33,604 --> 00:21:36,900
Then the request lands
at our load balancer

403
00:21:36,900 --> 00:21:40,860
where we authenticate the
request using mutual TLS.

404
00:21:40,860 --> 00:21:42,600
Successfully authenticated request

405
00:21:42,600 --> 00:21:46,830
travel via transit gateway, VPC endpoints

406
00:21:46,830 --> 00:21:49,200
and then land at our Amazon API gateway,

407
00:21:49,200 --> 00:21:52,053
which is hosted in private spoke VPC.

408
00:21:53,070 --> 00:21:55,770
API Gateway enforces usage plans,

409
00:21:55,770 --> 00:21:58,080
applies throttling per client

410
00:21:58,080 --> 00:21:59,520
and then sends it to the

411
00:21:59,520 --> 00:22:01,290
A2A a gateway application running

412
00:22:01,290 --> 00:22:05,013
inside the Nitro Enclaves
via a network load balancer.

413
00:22:06,360 --> 00:22:08,790
A2A gateway application
receives the request,

414
00:22:08,790 --> 00:22:12,000
terminates the TLS
connection inside the enclave

415
00:22:12,000 --> 00:22:14,910
and then performs some validations

416
00:22:14,910 --> 00:22:17,160
and it also does the
secondary authentication

417
00:22:17,160 --> 00:22:19,170
using token validation

418
00:22:19,170 --> 00:22:22,770
and then it encrypts
the sensitive PIA data

419
00:22:22,770 --> 00:22:25,200
using KMS encryption.

420
00:22:25,200 --> 00:22:30,030
Then it converts the REST
request into GRPC protocol

421
00:22:30,030 --> 00:22:32,100
and then it sends it to our

422
00:22:32,100 --> 00:22:34,620
downstream decision service application.

423
00:22:34,620 --> 00:22:38,910
The decision service runs
on top of Amazon EKS,

424
00:22:38,910 --> 00:22:41,700
it transforms the incoming request

425
00:22:41,700 --> 00:22:43,800
and then it does the duplicate check

426
00:22:43,800 --> 00:22:48,330
by doing lookups into Amazon
ElastiCache for Valkey.

427
00:22:48,330 --> 00:22:51,300
It also does the enrichment
of the incoming payload

428
00:22:51,300 --> 00:22:54,333
by doing some lookups into
the ElastiCache as well.

429
00:22:55,230 --> 00:22:58,173
Then it calls our model
inferencing platform.

430
00:22:59,070 --> 00:23:02,070
Our model inferencing
platform runs on Ray cluster,

431
00:23:02,070 --> 00:23:05,167
which is deployed on top of Amazon EKS.

432
00:23:06,172 --> 00:23:07,005
Model inferencing

433
00:23:07,005 --> 00:23:09,423
mainly consists of three
different services.

434
00:23:10,559 --> 00:23:13,320
LT profile service is fetching

435
00:23:13,320 --> 00:23:17,193
the long term profile features
from Amazon Memory DB.

436
00:23:18,390 --> 00:23:19,660
Aggregation service

437
00:23:20,790 --> 00:23:24,030
performs real time
aggregations by fetching

438
00:23:24,030 --> 00:23:27,210
short term profile features
from the Memory DB as well.

439
00:23:27,210 --> 00:23:32,210
Once the features are ready,
AIas serve runs the model

440
00:23:32,430 --> 00:23:34,380
and generates the risk score.

441
00:23:34,380 --> 00:23:37,350
The score is then sent back
to the decision service

442
00:23:37,350 --> 00:23:40,380
where we have a rule
engine applying the rules

443
00:23:40,380 --> 00:23:45,030
and then transforming the
response by decisioning.

444
00:23:45,030 --> 00:23:47,910
Then, it sends that
response back to the gateway

445
00:23:47,910 --> 00:23:51,390
where we convert the GRPC response to REST

446
00:23:51,390 --> 00:23:54,270
and then that response is
sent back to the client.

447
00:23:54,270 --> 00:23:57,180
So that completes the real time flow.

448
00:23:57,180 --> 00:24:00,450
Both the model inferencing service

449
00:24:00,450 --> 00:24:03,000
and the decisions and the gateway service

450
00:24:03,000 --> 00:24:06,690
are writing to, writing
the request and responses

451
00:24:06,690 --> 00:24:09,900
to Amazon managed streaming for Kafka,

452
00:24:09,900 --> 00:24:13,320
which is fed to our
downstream applications

453
00:24:13,320 --> 00:24:15,540
such as reporting, billing,

454
00:24:15,540 --> 00:24:19,563
and near realtime feature
engineering processes.

455
00:24:20,580 --> 00:24:24,400
So we chose ElastiCache for Valkey

456
00:24:25,500 --> 00:24:28,920
and Memory DB with Valkey Engine

457
00:24:28,920 --> 00:24:32,340
because they provide about 20%,

458
00:24:32,340 --> 00:24:35,490
they're about 20% cheaper than Redis

459
00:24:35,490 --> 00:24:37,170
and they provide the same functionality

460
00:24:37,170 --> 00:24:39,390
and performance as well.

461
00:24:39,390 --> 00:24:41,910
Our applications didn't need to change

462
00:24:41,910 --> 00:24:44,790
because the Redis client libraries work

463
00:24:44,790 --> 00:24:46,653
as is with the Valkey engine.

464
00:24:49,080 --> 00:24:51,210
That's on the realtime side.

465
00:24:51,210 --> 00:24:54,960
Next, let's take a look at
our offline flow pipeline.

466
00:24:54,960 --> 00:24:59,340
This flow enables our
clients to upload daily files

467
00:24:59,340 --> 00:25:02,640
such as entitlements, fraud data

468
00:25:02,640 --> 00:25:05,880
and bank to bank account data

469
00:25:05,880 --> 00:25:09,060
and transaction status files.

470
00:25:09,060 --> 00:25:11,130
This is also used by our clients

471
00:25:11,130 --> 00:25:15,180
to upload one time historical
transactional data,

472
00:25:15,180 --> 00:25:17,700
which gets used for model training

473
00:25:17,700 --> 00:25:21,693
and baseline feature generation.

474
00:25:23,130 --> 00:25:26,190
Clients connect to AWS
transfer family service

475
00:25:26,190 --> 00:25:28,560
running again inside the DMZ VPC,

476
00:25:28,560 --> 00:25:30,570
which is facing the internet

477
00:25:30,570 --> 00:25:34,380
and they upload the PGP encrypted files

478
00:25:34,380 --> 00:25:36,153
using SFTP protocol.

479
00:25:37,860 --> 00:25:40,170
The files land in an S3 bucket

480
00:25:40,170 --> 00:25:43,440
from where we have a scan
process picking up these files

481
00:25:43,440 --> 00:25:48,440
and running AWS GuardDuty
scanning for malware detection.

482
00:25:48,840 --> 00:25:50,670
The successfully scanned files

483
00:25:50,670 --> 00:25:53,190
get moved to another S3 bucket

484
00:25:53,190 --> 00:25:56,013
and an SNS notification is also generated.

485
00:25:56,880 --> 00:25:58,290
This SNS notification

486
00:25:58,290 --> 00:26:01,170
is consumed by our data ingestion process,

487
00:26:01,170 --> 00:26:03,963
which is also running
inside the Nitro Enclaves.

488
00:26:05,430 --> 00:26:07,170
It consumes the notification

489
00:26:07,170 --> 00:26:11,280
and then it picks up the file
that was successfully scanned

490
00:26:11,280 --> 00:26:14,610
and it pulls that file into
the Nitro Enclave memory

491
00:26:14,610 --> 00:26:18,030
and it decrypts the file
inside the Nitro Enclave.

492
00:26:18,030 --> 00:26:19,410
Then it passes the file

493
00:26:19,410 --> 00:26:22,443
and it performs various
checks, validations,

494
00:26:23,310 --> 00:26:25,770
and then it starts
encrypting the PIA data,

495
00:26:25,770 --> 00:26:27,780
which is in each record.

496
00:26:27,780 --> 00:26:29,040
The encrypted records

497
00:26:29,040 --> 00:26:31,960
are written to Amazon
Managed Streaming for Kafka

498
00:26:33,690 --> 00:26:36,570
and the file level
metadata is also stored in

499
00:26:36,570 --> 00:26:39,513
Amazon Aurora DB for reporting purposes.

500
00:26:41,070 --> 00:26:44,100
The Kafka data return

501
00:26:44,100 --> 00:26:46,860
is consumed by various applications.

502
00:26:46,860 --> 00:26:50,760
We use Amazon Data Firehose for

503
00:26:50,760 --> 00:26:52,860
consuming the data from Kafka

504
00:26:52,860 --> 00:26:55,560
and writing into an S3 bucket.

505
00:26:55,560 --> 00:26:59,310
From there we have EMR jobs
picking up these data files

506
00:26:59,310 --> 00:27:00,540
and then aggregating

507
00:27:00,540 --> 00:27:03,540
and then writing it a separate S3 bucket.

508
00:27:03,540 --> 00:27:08,540
This S3 data gets used by our
downstream feature engineering

509
00:27:08,850 --> 00:27:13,263
and model validation and
model retraining purposes.

510
00:27:14,190 --> 00:27:16,230
We also have another consumer application

511
00:27:16,230 --> 00:27:19,890
which reads from topics
such as entitlement

512
00:27:19,890 --> 00:27:21,540
and branch code data

513
00:27:21,540 --> 00:27:22,800
and then writing that data

514
00:27:22,800 --> 00:27:25,410
into Amazon ElastiCache for Valkey.

515
00:27:25,410 --> 00:27:26,430
This Valkey data

516
00:27:26,430 --> 00:27:29,313
is used by our real time
scoring API workload.

517
00:27:30,300 --> 00:27:33,660
The same data is used is
written to Amazon Aurora DB

518
00:27:33,660 --> 00:27:35,700
for long-term storage.

519
00:27:35,700 --> 00:27:39,510
This ensures that both
our real time pipeline

520
00:27:39,510 --> 00:27:43,860
and offline pipeline use secure, validated

521
00:27:43,860 --> 00:27:46,140
and enriched data,

522
00:27:46,140 --> 00:27:48,633
share that data across the both flows.

523
00:27:51,990 --> 00:27:55,860
Next, let's dive deep
into AWS Nitro Enclaves,

524
00:27:55,860 --> 00:28:00,860
the security backbone for our
sensitive data processing.

525
00:28:05,940 --> 00:28:09,150
So the Nitro Enclaves
are the secure, hardened

526
00:28:09,150 --> 00:28:14,010
and isolated environments
running inside the EC2 instances.

527
00:28:14,010 --> 00:28:18,480
They have no external
connectivity, no SSH access

528
00:28:18,480 --> 00:28:20,490
and no persistent storage.

529
00:28:20,490 --> 00:28:23,160
Not even the root user from parent EC2

530
00:28:23,160 --> 00:28:25,110
can log into the enclave.

531
00:28:25,110 --> 00:28:28,410
Only the parent EC2 can
connect to the enclave

532
00:28:28,410 --> 00:28:32,490
using a secure local
channel via Visa protocol.

533
00:28:32,490 --> 00:28:35,220
These are ideal for
processing sensitive data

534
00:28:35,220 --> 00:28:39,150
such as bank account
numbers, credit card numbers,

535
00:28:39,150 --> 00:28:40,983
and customer data.

536
00:28:43,440 --> 00:28:45,243
Our A2A gateway application

537
00:28:45,243 --> 00:28:47,520
runs inside the Nitro Enclave,

538
00:28:47,520 --> 00:28:50,670
this is the technical
design for our application.

539
00:28:50,670 --> 00:28:52,113
During the startup,

540
00:28:53,490 --> 00:28:56,970
the application connects
to secrets manager

541
00:28:56,970 --> 00:29:00,840
and KMS proxy via proxies

542
00:29:00,840 --> 00:29:02,730
and fetches the envelope encrypted

543
00:29:02,730 --> 00:29:05,880
data encryption key into the enclave.

544
00:29:05,880 --> 00:29:09,180
Then it decrypts the encrypted key

545
00:29:09,180 --> 00:29:11,940
using KMS customer managed key

546
00:29:11,940 --> 00:29:15,360
and then it caches that
key in the enclave memory.

547
00:29:15,360 --> 00:29:18,300
When an input scoring request arrives,

548
00:29:18,300 --> 00:29:20,850
the NLB sends it to the gateway proxy

549
00:29:20,850 --> 00:29:22,890
running on the parent EC2.

550
00:29:22,890 --> 00:29:27,780
From the proxy converts the
incoming TCP protocol request

551
00:29:27,780 --> 00:29:32,040
into Visa protocol and
sends it into the enclave.

552
00:29:32,040 --> 00:29:34,380
Inside the enclave,
A2A gateway application

553
00:29:34,380 --> 00:29:37,710
receives the request,
terminates the TLS connection,

554
00:29:37,710 --> 00:29:40,410
and then it validates the payload

555
00:29:40,410 --> 00:29:45,410
and then it starts encrypting
the plain text PIA data

556
00:29:46,860 --> 00:29:50,700
using the cached deck that
was cached during the startup.

557
00:29:50,700 --> 00:29:55,350
Then it sends the,

558
00:29:55,350 --> 00:29:58,620
it converts the request from REST to GRPC

559
00:29:58,620 --> 00:30:00,210
and then it sends that request

560
00:30:00,210 --> 00:30:03,630
to downstream decision service application

561
00:30:03,630 --> 00:30:06,060
proxying via visa channel again.

562
00:30:06,060 --> 00:30:09,030
This ensures that the
sensitive data processing

563
00:30:09,030 --> 00:30:12,480
is entirely happening
inside the Nitro Enclave

564
00:30:12,480 --> 00:30:17,220
without exposing it to the
in memory exposure threats

565
00:30:17,220 --> 00:30:18,963
that Hema talked about earlier.

566
00:30:21,990 --> 00:30:24,300
These are some of the
best practices we follow

567
00:30:24,300 --> 00:30:25,890
for our application deployment

568
00:30:25,890 --> 00:30:28,500
inside the Nitro Enclaves.

569
00:30:28,500 --> 00:30:31,410
The enclave resource allocation

570
00:30:31,410 --> 00:30:34,203
is done using this allocated .YAML file.

571
00:30:35,340 --> 00:30:37,470
We declare the vCPUs

572
00:30:37,470 --> 00:30:41,040
and memory inside this YAML configuration

573
00:30:41,040 --> 00:30:44,970
and the best practice to do
this is using user data scripts.

574
00:30:44,970 --> 00:30:47,520
Because Nitro Enclaves requires

575
00:30:47,520 --> 00:30:49,770
contiguous huge pages memory,

576
00:30:49,770 --> 00:30:53,550
if the allocation or
reservation is not done

577
00:30:53,550 --> 00:30:54,690
during the start-up,

578
00:30:54,690 --> 00:30:56,490
the memory can get fragmented

579
00:30:56,490 --> 00:31:00,570
and it may not be available
for enclave to reserve.

580
00:31:00,570 --> 00:31:05,170
So in order to avoid
that, we do the allocation

581
00:31:06,030 --> 00:31:07,683
with the user data scripts.

582
00:31:08,580 --> 00:31:12,160
Next is careful tuning
of proxies is a must

583
00:31:13,140 --> 00:31:15,510
because this provides a better throughput.

584
00:31:15,510 --> 00:31:17,350
Otherwise we may not get

585
00:31:18,330 --> 00:31:21,540
the throughput that we require.

586
00:31:21,540 --> 00:31:25,060
So we use Socat proxies for converting the

587
00:31:25,980 --> 00:31:29,310
TCP to VSOC and vice versa.

588
00:31:29,310 --> 00:31:32,670
And Socat has options for
concurrent connections

589
00:31:32,670 --> 00:31:35,220
such as reuse, address and fork.

590
00:31:35,220 --> 00:31:37,140
So by carefully tuning them,

591
00:31:37,140 --> 00:31:39,513
we were able to achieve better throughput.

592
00:31:40,770 --> 00:31:42,690
Next is health check endpoints,

593
00:31:42,690 --> 00:31:45,360
should check the health of the application

594
00:31:45,360 --> 00:31:47,460
as well as proxies.

595
00:31:47,460 --> 00:31:51,030
So in order to make sure
that the instance is healthy

596
00:31:51,030 --> 00:31:54,363
before sending the traffic
to a particular instance.

597
00:31:55,770 --> 00:31:59,880
As we saw earlier, the
application inside the enclave

598
00:31:59,880 --> 00:32:02,910
fetches the encryption
keys during the startup

599
00:32:02,910 --> 00:32:05,040
and then caches them into the memory

600
00:32:05,040 --> 00:32:08,670
so that it avoids latency
in getting the keys

601
00:32:08,670 --> 00:32:10,590
for every request.

602
00:32:10,590 --> 00:32:14,280
So by using all these optimizations,
we were able to achieve

603
00:32:14,280 --> 00:32:18,270
around 150 to 200 TPS
throughput with a single M5A

604
00:32:18,270 --> 00:32:20,793
2x large EC2 instance.

605
00:32:24,660 --> 00:32:27,390
So performance is just as critical

606
00:32:27,390 --> 00:32:29,670
as security for our application.

607
00:32:29,670 --> 00:32:33,753
Next, let's take a look at
how we optimize for latency.

608
00:32:35,550 --> 00:32:40,150
We use TLS 1.3 and HTTP
2.0 throughout our stack

609
00:32:41,670 --> 00:32:46,140
for faster handshakes because
TLS 1.3 is one less round trip

610
00:32:46,140 --> 00:32:50,253
and HTTP 2.0 provides
multiplex connections.

611
00:32:51,210 --> 00:32:54,810
Both, I mean AWS load balancers
do support both of these

612
00:32:54,810 --> 00:32:56,853
and even our applications support them.

613
00:32:57,720 --> 00:33:02,410
We recommend our clients also
to use TLS 1.3 and HTTP 2.0

614
00:33:03,390 --> 00:33:05,970
because it provides
better end-to-end latency

615
00:33:05,970 --> 00:33:07,770
and throughput.

616
00:33:07,770 --> 00:33:12,770
And this is also highly secure as well.

617
00:33:14,430 --> 00:33:18,210
On the Amazon API gateway
for the ingress traffic,

618
00:33:18,210 --> 00:33:22,140
we use VPC endpoints and
for the backend integration

619
00:33:22,140 --> 00:33:26,610
with our A2A gateway
application, we use VPC link.

620
00:33:26,610 --> 00:33:30,030
So this ensures that the
traffic stays entirely on the

621
00:33:30,030 --> 00:33:31,890
Amazon backbone network,

622
00:33:31,890 --> 00:33:34,833
giving us consistent
low latency performance.

623
00:33:36,840 --> 00:33:40,634
So on the NLB for client routing policy,

624
00:33:40,634 --> 00:33:43,230
AZ affinity setting is enabled.

625
00:33:43,230 --> 00:33:47,010
This means that the client,
the traffic stays within the AZ

626
00:33:47,010 --> 00:33:50,340
where client is making the request from.

627
00:33:50,340 --> 00:33:52,383
This provides better latency as well.

628
00:33:53,280 --> 00:33:57,060
And for the load balancer,
target selection policy

629
00:33:57,060 --> 00:34:00,030
cross zone load balancing is enabled

630
00:34:00,030 --> 00:34:02,373
in order to provide the high availability.

631
00:34:03,390 --> 00:34:05,370
And the application gateway, which we,

632
00:34:05,370 --> 00:34:07,320
which is running inside the enclave,

633
00:34:07,320 --> 00:34:08,520
as we saw earlier,

634
00:34:08,520 --> 00:34:11,250
the secrets and keys are
fetched during the startup

635
00:34:11,250 --> 00:34:13,260
and cached in the memory

636
00:34:13,260 --> 00:34:17,928
and we do have background
threads refreshing these keys

637
00:34:17,928 --> 00:34:19,890
as and when needed.

638
00:34:19,890 --> 00:34:22,323
This avoids repeated fetch latencies.

639
00:34:25,260 --> 00:34:29,160
Next, let's take a look
at how we fine tune EKS

640
00:34:29,160 --> 00:34:31,503
for both performance and resiliency.

641
00:34:33,390 --> 00:34:36,570
So we use AWS load balancer controller

642
00:34:36,570 --> 00:34:39,990
to provision and manage load balancers.

643
00:34:39,990 --> 00:34:43,182
So load balancer controller provisions

644
00:34:43,182 --> 00:34:46,050
ALB for Kubernetes ingress resource

645
00:34:46,050 --> 00:34:50,250
and NLB for Kubernetes
load balancer resource.

646
00:34:50,250 --> 00:34:52,860
NLB can be provisioned
with either instance

647
00:34:52,860 --> 00:34:54,960
or IP target type.

648
00:34:54,960 --> 00:34:58,680
And so the annotation that we see here,

649
00:34:58,680 --> 00:35:01,260
AWS Load Balancer NLB target type

650
00:35:01,260 --> 00:35:06,260
is set to IP which means
the parts gets IP addresses

651
00:35:06,660 --> 00:35:10,440
directly assigned from Amazon VPC subnets,

652
00:35:10,440 --> 00:35:12,930
avoiding any extra network hops

653
00:35:12,930 --> 00:35:15,183
and providing better performance.

654
00:35:18,210 --> 00:35:20,400
Next, let's look at pod scheduling.

655
00:35:20,400 --> 00:35:21,750
There are mainly three ways

656
00:35:21,750 --> 00:35:24,300
we can control the pod scheduling:

657
00:35:24,300 --> 00:35:26,550
NodeAffinity, this addresses

658
00:35:26,550 --> 00:35:29,190
whether you want to place
a particular pod on a node

659
00:35:29,190 --> 00:35:30,900
with a specific characteristic.

660
00:35:30,900 --> 00:35:33,660
For example, a hardware
characteristic like

661
00:35:33,660 --> 00:35:36,270
enclave enablement.

662
00:35:36,270 --> 00:35:39,690
So for example, we are
now migrating our A2A

663
00:35:39,690 --> 00:35:42,960
gateway application from EC2 to EKS

664
00:35:42,960 --> 00:35:47,160
and this application pod
needs to run on the nodes

665
00:35:47,160 --> 00:35:49,560
where enclaves are enabled.

666
00:35:49,560 --> 00:35:53,013
So this is done by using the NodeAffinity.

667
00:35:54,056 --> 00:35:56,670
PodAffinity and PodAntiAffinity,

668
00:35:56,670 --> 00:35:58,320
when the placement of a pod

669
00:35:58,320 --> 00:36:00,630
depends on the location of other pods,

670
00:36:00,630 --> 00:36:05,250
PodAffinity can be used to
co-locate synergistic workloads.

671
00:36:05,250 --> 00:36:07,560
For example, if we have a web server

672
00:36:07,560 --> 00:36:10,050
and a corresponding caching pod,

673
00:36:10,050 --> 00:36:13,713
they can be co-located
together using PodAffinity.

674
00:36:14,640 --> 00:36:17,880
PodAntiAffinity, this distributes the pods

675
00:36:17,880 --> 00:36:21,720
across a failure domain,
providing high availability.

676
00:36:21,720 --> 00:36:25,380
For example, if you declare a
service with three replicas,

677
00:36:25,380 --> 00:36:29,340
you don't want all of them
to stay within the same AZ,

678
00:36:29,340 --> 00:36:33,363
so this can be controlled
by using PodAntiAffinity.

679
00:36:34,350 --> 00:36:37,050
Next is TopologySpreadConstraints.

680
00:36:37,050 --> 00:36:40,950
When you want to ensure
that the replicas from a pod

681
00:36:40,950 --> 00:36:43,920
for a single service are evenly balanced

682
00:36:43,920 --> 00:36:47,010
across the different failure domains,

683
00:36:47,010 --> 00:36:51,570
that's when we use the
topology spread constraints.

684
00:36:51,570 --> 00:36:54,933
Next we will see how we
can define some of these.

685
00:36:56,640 --> 00:37:00,420
This is a sample manifest
definition for topology spread.

686
00:37:00,420 --> 00:37:03,210
The section topology spread constraints,

687
00:37:03,210 --> 00:37:07,230
this ensures that the replicas
are evenly distributed

688
00:37:07,230 --> 00:37:09,960
across the failure domain.

689
00:37:09,960 --> 00:37:12,720
Max Q is set to one.

690
00:37:12,720 --> 00:37:14,373
This is a crucial setting,

691
00:37:15,570 --> 00:37:19,470
this dictates that the number
of pods for a given service

692
00:37:19,470 --> 00:37:23,130
in a given AZ will not
differ by more than one

693
00:37:23,130 --> 00:37:26,400
with the number of pods in other AZs.

694
00:37:26,400 --> 00:37:29,370
And the topology key is set to zone,

695
00:37:29,370 --> 00:37:31,260
which means that the failure domain

696
00:37:31,260 --> 00:37:35,283
that we talked about earlier
is at availability zone level.

697
00:37:37,800 --> 00:37:40,440
Next is topology aware routing.

698
00:37:40,440 --> 00:37:43,140
This keeps the network traffic

699
00:37:43,140 --> 00:37:46,110
within the AZ where it originated.

700
00:37:46,110 --> 00:37:49,350
This helps with the reliability
performance and cost

701
00:37:49,350 --> 00:37:52,410
because cross zone traffic is charged.

702
00:37:52,410 --> 00:37:55,923
So this is a sample definition
for topology aware routing.

703
00:37:56,760 --> 00:37:58,860
We have the service definition

704
00:37:58,860 --> 00:38:01,710
under which we have the metadata section

705
00:38:01,710 --> 00:38:06,710
and we are setting the topology
mode annotation with auto.

706
00:38:07,200 --> 00:38:09,900
This auto mode tells Kubernetes

707
00:38:09,900 --> 00:38:12,900
to keep the network
traffic within the same AZ

708
00:38:12,900 --> 00:38:14,223
where it originated.

709
00:38:15,720 --> 00:38:18,240
It also has a smart default.

710
00:38:18,240 --> 00:38:21,930
If there are no healthy parts
available within that AZ,

711
00:38:21,930 --> 00:38:24,990
it'll automatically reroute
to the available parts

712
00:38:24,990 --> 00:38:26,133
in other AZs.

713
00:38:27,090 --> 00:38:31,413
This feature works by adding
hints to endpoint slices.

714
00:38:32,310 --> 00:38:36,960
In order for the routing
algorithm to work effectively,

715
00:38:36,960 --> 00:38:40,170
it is recommended that we
have at least three replicas

716
00:38:40,170 --> 00:38:41,733
running in a given AZ.

717
00:38:42,660 --> 00:38:45,900
For example, if we have a service with the

718
00:38:45,900 --> 00:38:50,900
nine replicas defined
for within three AZs,

719
00:38:51,240 --> 00:38:53,160
this definition will make sure

720
00:38:53,160 --> 00:38:55,413
that there are three parts per each AZ.

721
00:38:58,740 --> 00:39:02,490
So combining all these
optimizations together

722
00:39:02,490 --> 00:39:06,960
and using Amazon ElastiCache
for Valkey and Memory DB

723
00:39:06,960 --> 00:39:09,900
in our realtime scoring API workload,

724
00:39:09,900 --> 00:39:14,850
we were able to achieve our
latency OLF 250 milliseconds.

725
00:39:14,850 --> 00:39:18,780
In fact, our P 99.5
latencies are much better,

726
00:39:18,780 --> 00:39:21,180
about 40% lower than that

727
00:39:21,180 --> 00:39:25,110
and we were able to
successfully test our surveys

728
00:39:25,110 --> 00:39:28,803
with the bursts of up to a thousand TPS.

729
00:39:29,820 --> 00:39:32,640
Next, let's take a look
at our multi-region

730
00:39:32,640 --> 00:39:34,560
Active-active architecture,

731
00:39:34,560 --> 00:39:37,350
which is essential for
a tier zero application

732
00:39:37,350 --> 00:39:41,973
such as VP A2A per
visa's ITDR requirements.

733
00:39:44,130 --> 00:39:48,810
So we have CloudFlare
Akamai DNS routing traffic

734
00:39:48,810 --> 00:39:50,880
to the nearest region

735
00:39:50,880 --> 00:39:54,960
and in each region, we have both compute

736
00:39:54,960 --> 00:39:58,353
and data services
deployed across three AZs.

737
00:39:59,490 --> 00:40:02,370
Load balancer is distributing the traffic

738
00:40:02,370 --> 00:40:04,470
across these three AZs.

739
00:40:04,470 --> 00:40:07,980
We have the application
layer writing to ElastiCache

740
00:40:07,980 --> 00:40:11,010
in local as well as remote region

741
00:40:11,010 --> 00:40:13,590
for our realtime duplicate checks.

742
00:40:13,590 --> 00:40:18,300
And Amazon managed
streaming for Kafka MSK data

743
00:40:18,300 --> 00:40:21,330
is replicated using MSK replicator.

744
00:40:21,330 --> 00:40:25,473
This is used by our near realtime
feature engineering flows.

745
00:40:26,310 --> 00:40:28,140
S3 cross region replication

746
00:40:28,140 --> 00:40:32,430
is enabled for our
offline flow pipe buckets

747
00:40:32,430 --> 00:40:36,550
and we use Amazon Aurora DB in
our file processing pipelines

748
00:40:37,650 --> 00:40:41,730
and Aurora DB replication
is enabled from primary to,

749
00:40:41,730 --> 00:40:43,740
primary to secondary.

750
00:40:43,740 --> 00:40:47,010
Using our AWS MSK replicator

751
00:40:47,010 --> 00:40:49,620
and S3 cross region replication

752
00:40:49,620 --> 00:40:53,040
and Aurora Global DB replication

753
00:40:53,040 --> 00:40:57,060
reduces the operational
complexity on our side

754
00:40:57,060 --> 00:41:00,420
because AWS takes care of
the replication overhead.

755
00:41:00,420 --> 00:41:03,780
So this gives us high availability,

756
00:41:03,780 --> 00:41:08,403
disaster resilience and
consistent performance globally.

757
00:41:09,300 --> 00:41:10,860
That's about as deep as we'll go

758
00:41:10,860 --> 00:41:12,780
on the technical side today.

759
00:41:12,780 --> 00:41:15,930
Next I invite Hema back
to the stage to wrap up.

760
00:41:15,930 --> 00:41:16,930
Thank you very much.

761
00:41:17,883 --> 00:41:20,883
(audience applauds)

762
00:41:27,682 --> 00:41:32,682
- Thank you Aravinda for the
detailed architecture overview.

763
00:41:34,650 --> 00:41:37,230
I would spend a few
minutes just to talk about

764
00:41:37,230 --> 00:41:42,230
the key takeaways of our
partnership with AWS team

765
00:41:42,360 --> 00:41:47,360
and what did we learn as part
of building the solution,

766
00:41:49,710 --> 00:41:51,483
Visa Protect Account to Account.

767
00:41:55,080 --> 00:41:57,480
This is for the first time actually

768
00:41:57,480 --> 00:42:02,480
we took something internally
and tried to take it to cloud.

769
00:42:02,520 --> 00:42:05,640
Visa has spent a lot of time and effort

770
00:42:05,640 --> 00:42:09,000
to build its hybrid cloud solution

771
00:42:09,000 --> 00:42:12,900
and our application was one
of the very first applications

772
00:42:12,900 --> 00:42:17,160
that decided to go and use
that hybrid cloud solution

773
00:42:17,160 --> 00:42:21,690
and host an application on
that hybrid cloud solution.

774
00:42:21,690 --> 00:42:24,717
As part of this journey, what we built

775
00:42:24,717 --> 00:42:28,740
and Aravinda did talk
about that was a solution

776
00:42:28,740 --> 00:42:32,520
that caters to Visa's
stringent operational

777
00:42:32,520 --> 00:42:34,230
and cybersecurity guidance

778
00:42:34,230 --> 00:42:36,930
is something that we
follow very religiously

779
00:42:36,930 --> 00:42:38,850
on our on-prem system,

780
00:42:38,850 --> 00:42:42,540
but to see how that is
gonna work on a hybrid cloud

781
00:42:42,540 --> 00:42:45,660
and to take that with a thousand TPS

782
00:42:45,660 --> 00:42:48,480
with 99.99 availability,

783
00:42:48,480 --> 00:42:51,450
with a fraction of second latency.

784
00:42:51,450 --> 00:42:56,450
This we consider as a
blueprint for our application

785
00:42:58,140 --> 00:43:00,930
and we were able to take
this, it's fully functional,

786
00:43:00,930 --> 00:43:04,020
our clients are using
the scoring service today

787
00:43:04,020 --> 00:43:06,090
in our UK region.

788
00:43:06,090 --> 00:43:11,090
Now, the next part of
this journey was to see

789
00:43:11,250 --> 00:43:13,710
how do we use this blueprint

790
00:43:13,710 --> 00:43:16,683
and take it to various other regions.

791
00:43:17,910 --> 00:43:20,190
As I talked about Visa
being a global company,

792
00:43:20,190 --> 00:43:23,790
a lot of our solutions are
built for our global markets

793
00:43:23,790 --> 00:43:27,090
to cater to their local requirements.

794
00:43:27,090 --> 00:43:32,010
We are using this blueprint
that we have built

795
00:43:32,010 --> 00:43:36,900
and kind of on top of that
using the IAC scripts and all

796
00:43:36,900 --> 00:43:40,500
to go faster to other
regions with slight tweaks

797
00:43:40,500 --> 00:43:42,810
that are needed for those regions.

798
00:43:42,810 --> 00:43:45,700
As an example, I will talk about

799
00:43:46,950 --> 00:43:51,270
Visa's foray into the
South American markets

800
00:43:51,270 --> 00:43:54,000
with Brazil being one of our clients

801
00:43:54,000 --> 00:43:57,930
where we are taking this
solution to Sao Paulo region

802
00:43:57,930 --> 00:44:00,750
and opening it up for our markets,

803
00:44:00,750 --> 00:44:04,740
the Visa Protect Account
to Account solution to help

804
00:44:04,740 --> 00:44:09,093
protect and prevent fraud,
account to account fraud there.

805
00:44:10,440 --> 00:44:12,900
The third important part
that I want to talk about

806
00:44:12,900 --> 00:44:17,280
is our partnership with the
AWS team, Suraj and team.

807
00:44:17,280 --> 00:44:19,650
We've worked for past two and a half years

808
00:44:19,650 --> 00:44:21,240
in trying to build this solution

809
00:44:21,240 --> 00:44:23,670
with various other partners within Visa,

810
00:44:23,670 --> 00:44:26,970
with our operations team,
with our cybersecurity team,

811
00:44:26,970 --> 00:44:28,890
and it has been a great partnership.

812
00:44:28,890 --> 00:44:32,520
We have all evolved, we have
learned with experiences

813
00:44:32,520 --> 00:44:36,510
and we have leveraged a lot
of different AWS patterns

814
00:44:36,510 --> 00:44:41,073
that has helped us scale our
services and our solution.

815
00:44:42,030 --> 00:44:46,830
With that, I would like to end
this presentation for here.

816
00:44:46,830 --> 00:44:49,920
I would like to thank
you all for coming here

817
00:44:49,920 --> 00:44:51,120
and listening to us.

818
00:44:51,120 --> 00:44:53,610
And if you guys have any questions,

819
00:44:53,610 --> 00:44:56,130
we are happy to have side
conversations on the site.

820
00:44:56,130 --> 00:44:58,110
But thank you all for your time.

821
00:44:58,110 --> 00:45:00,060
I would love for all you
to fill in the survey

822
00:45:00,060 --> 00:45:02,191
in the mobile apps, thank you.

823
00:45:02,191 --> 00:45:04,878
(audience applauds)

