1
00:00:00,900 --> 00:00:04,440
- Hello friends, my name
is Paul Meighan here

2
00:00:04,440 --> 00:00:08,490
with my friend Anna Bouchet
and we work on Amazon S3.

3
00:00:08,490 --> 00:00:10,740
Our job today is to bring
you up to speed on everything

4
00:00:10,740 --> 00:00:12,990
that the S3 team has been
working on over the course

5
00:00:12,990 --> 00:00:15,120
of the last 12 months since
we were all here at re:Invent

6
00:00:15,120 --> 00:00:16,413
together last year.

7
00:00:18,060 --> 00:00:20,400
Now I've been doing this session
for a number of years now

8
00:00:20,400 --> 00:00:23,190
and I think this is my last year doing it.

9
00:00:23,190 --> 00:00:26,520
And every year I threaten to do a demo

10
00:00:26,520 --> 00:00:30,090
and every year the marketing
team talks me out of it.

11
00:00:30,090 --> 00:00:31,740
But not this year.

12
00:00:31,740 --> 00:00:33,120
This year we have come up

13
00:00:33,120 --> 00:00:37,200
with the most complicated
demo we could think of

14
00:00:37,200 --> 00:00:40,560
and we brought in our best
engineer to help run it,

15
00:00:40,560 --> 00:00:42,240
and it's gonna go great.

16
00:00:42,240 --> 00:00:45,150
So that's the plan.

17
00:00:45,150 --> 00:00:46,590
In fact it's so complicated,

18
00:00:46,590 --> 00:00:50,430
I need a few volunteers from
the audience to participate

19
00:00:50,430 --> 00:00:51,720
and we're gonna take a video

20
00:00:51,720 --> 00:00:54,750
as I gather some names of our volunteers.

21
00:00:54,750 --> 00:00:57,210
I need two. And all you
gotta do is yell out a name

22
00:00:57,210 --> 00:00:58,530
or yell out a fake name.

23
00:00:58,530 --> 00:01:00,810
It works either way.

24
00:01:00,810 --> 00:01:03,960
So who is open to volunteering
for our demo today?

25
00:01:03,960 --> 00:01:06,960
Yes, Matt is our first volunteer.

26
00:01:06,960 --> 00:01:08,670
Another one over here.

27
00:01:08,670 --> 00:01:09,590
What?

28
00:01:09,590 --> 00:01:10,423
(indistinct)

29
00:01:10,423 --> 00:01:11,460
Indra Neil.

30
00:01:11,460 --> 00:01:14,370
All right, Matt and Indra
Neil are our two volunteers

31
00:01:14,370 --> 00:01:15,960
in our re:Invent session today.

32
00:01:15,960 --> 00:01:17,340
Anna, do we have that?

33
00:01:17,340 --> 00:01:21,240
All right, so we're
gonna cover 35 launches

34
00:01:21,240 --> 00:01:23,580
over the course of the
next 60 minutes together.

35
00:01:23,580 --> 00:01:27,450
I have six topics to take
you through really fast.

36
00:01:27,450 --> 00:01:29,850
We're gonna do security, durability,

37
00:01:29,850 --> 00:01:31,290
we're gonna talk about S3 Express,

38
00:01:31,290 --> 00:01:33,630
which is our high performance storage.

39
00:01:33,630 --> 00:01:36,750
We're gonna dive deep into
S3 objects and the S3 API

40
00:01:36,750 --> 00:01:41,052
and then round it out with
metadata, tables and vectors.

41
00:01:41,052 --> 00:01:42,390
That's the plan.

42
00:01:42,390 --> 00:01:46,080
Now every year we key this
course in as a 200 level session.

43
00:01:46,080 --> 00:01:47,250
And the reason that we do that is

44
00:01:47,250 --> 00:01:51,150
because we just cover so much
ground here in the what's new.

45
00:01:51,150 --> 00:01:52,230
We just don't have time

46
00:01:52,230 --> 00:01:54,268
to dive deep into every single feature.

47
00:01:54,268 --> 00:01:56,310
But Anna and I will stay

48
00:01:56,310 --> 00:01:58,770
after the session in the
hall for as long as it takes

49
00:01:58,770 --> 00:02:00,810
to answer any questions that you have.

50
00:02:00,810 --> 00:02:03,390
We're committed to giving
you the detail that you need

51
00:02:03,390 --> 00:02:04,710
to take some of this home with you

52
00:02:04,710 --> 00:02:06,510
and put it to work next week.

53
00:02:06,510 --> 00:02:09,360
We've also tried to build as
many links to source material

54
00:02:09,360 --> 00:02:11,430
as we can throughout the deck.

55
00:02:11,430 --> 00:02:13,170
And so you can get that once it posts

56
00:02:13,170 --> 00:02:14,730
to YouTube after the show.

57
00:02:14,730 --> 00:02:18,210
So that's the plan and
the clicker's working now.

58
00:02:18,210 --> 00:02:19,290
So let's get started.

59
00:02:19,290 --> 00:02:21,030
So we're gonna start with security.

60
00:02:21,030 --> 00:02:23,820
Security is the most
important thing we do at AWS

61
00:02:23,820 --> 00:02:24,840
and it always will be.

62
00:02:24,840 --> 00:02:26,490
So it's fitting that we're gonna start

63
00:02:26,490 --> 00:02:28,572
with the new security content this year

64
00:02:28,572 --> 00:02:30,900
at the top of the session.

65
00:02:30,900 --> 00:02:32,910
We're gonna talk about access controls,

66
00:02:32,910 --> 00:02:35,070
org level controls and encryption

67
00:02:35,070 --> 00:02:37,590
here over the course of
the next few minutes.

68
00:02:37,590 --> 00:02:40,560
Now, when you're managing access to S3

69
00:02:40,560 --> 00:02:43,380
and you just have a single
bucket and a single application

70
00:02:43,380 --> 00:02:45,450
or client, it's pretty straightforward

71
00:02:45,450 --> 00:02:47,673
once you kind of get the hang of IAM.

72
00:02:48,540 --> 00:02:50,340
But as you add more and more clients

73
00:02:50,340 --> 00:02:52,350
and more and more buckets to the mix,

74
00:02:52,350 --> 00:02:54,960
things get a little bit more complicated.

75
00:02:54,960 --> 00:02:57,870
You wanna maintain lease
privileged permissions

76
00:02:57,870 --> 00:02:59,070
to each one of these buckets.

77
00:02:59,070 --> 00:03:02,100
And so you have to tailor
sections of the policy

78
00:03:02,100 --> 00:03:04,200
for all the clients that need to come in

79
00:03:04,200 --> 00:03:05,970
and access your data.

80
00:03:05,970 --> 00:03:08,910
And since every bucket
likely has different

81
00:03:08,910 --> 00:03:12,420
clients coming in to access
different pieces of data,

82
00:03:12,420 --> 00:03:15,450
you really want to tailor
each policy to each bucket.

83
00:03:15,450 --> 00:03:17,250
And what you end up with is a whole bunch

84
00:03:17,250 --> 00:03:19,050
of different policies
that you have to juggle

85
00:03:19,050 --> 00:03:21,322
and manage across many buckets,

86
00:03:21,322 --> 00:03:24,090
which is complex and becomes even harder

87
00:03:24,090 --> 00:03:26,340
when you have to manage changes over time,

88
00:03:26,340 --> 00:03:29,010
'cause you have to now
start swapping in policies

89
00:03:29,010 --> 00:03:31,890
and management becomes
more and more difficult.

90
00:03:31,890 --> 00:03:33,510
And so to simplify this, a couple

91
00:03:33,510 --> 00:03:37,740
of weeks ago we launched
tag-based access controls for S3.

92
00:03:37,740 --> 00:03:39,510
This allows you to control permissions

93
00:03:39,510 --> 00:03:43,980
to your S3 resources using
simple policies in conjunction

94
00:03:43,980 --> 00:03:45,480
with resource tags.

95
00:03:45,480 --> 00:03:48,900
And it simplifies things by quite a lot.

96
00:03:48,900 --> 00:03:50,220
Here's how it works.

97
00:03:50,220 --> 00:03:52,140
So here's an example of the sort

98
00:03:52,140 --> 00:03:53,220
of policy fragment that

99
00:03:53,220 --> 00:03:56,340
you can now use on the client side.

100
00:03:56,340 --> 00:03:58,200
And you can see that this
policy is very simple.

101
00:03:58,200 --> 00:04:02,490
It just says give access to
this principle to any resources

102
00:04:02,490 --> 00:04:04,143
that is tagged with a blue tag.

103
00:04:05,580 --> 00:04:07,530
Very simple fragment, much simpler than

104
00:04:07,530 --> 00:04:09,540
what we saw on the previous slides.

105
00:04:09,540 --> 00:04:13,020
And then to grant access, all
I need to do is tag my buckets

106
00:04:13,020 --> 00:04:17,250
with the appropriate tag
values and access is granted.

107
00:04:17,250 --> 00:04:20,520
If I wanna make changes over
time, I can just add a tag

108
00:04:20,520 --> 00:04:23,910
to grant access and remove
a tag to remove access.

109
00:04:23,910 --> 00:04:27,344
So it's much simpler, much
easier to reason about

110
00:04:27,344 --> 00:04:31,860
than juggling policies
across my resources.

111
00:04:31,860 --> 00:04:34,830
We've also given you some
new APIs to help manage this

112
00:04:34,830 --> 00:04:37,503
that are also better
than what came before.

113
00:04:37,503 --> 00:04:42,503
We have a bucket tagging API
in the current namespace,

114
00:04:43,230 --> 00:04:46,080
but we added some new APIs
that look much more similar

115
00:04:46,080 --> 00:04:48,180
to what you're used to across AWS.

116
00:04:48,180 --> 00:04:50,310
So we have an untag resource API now

117
00:04:50,310 --> 00:04:52,710
that can remove a single tag off a bucket.

118
00:04:52,710 --> 00:04:54,270
We have a tag resource API,

119
00:04:54,270 --> 00:04:57,540
which can add a single tag onto a bucket.

120
00:04:57,540 --> 00:05:00,360
And what's different from
these new tagging APIs compared

121
00:05:00,360 --> 00:05:03,780
to what came before is
that single tag operations.

122
00:05:03,780 --> 00:05:06,360
Previously you had to
replace the entire tag set,

123
00:05:06,360 --> 00:05:08,940
even if you only want to
change one tag on the resource.

124
00:05:08,940 --> 00:05:12,510
So this is much safer and easier to use.

125
00:05:12,510 --> 00:05:14,550
Additionally, we have a list all my tags

126
00:05:14,550 --> 00:05:16,890
or list tags for resource API as well

127
00:05:16,890 --> 00:05:20,700
that spits out this handy dandy JSON list,

128
00:05:20,700 --> 00:05:21,533
showing all the tags

129
00:05:21,533 --> 00:05:23,673
that you have on a given bucket.

130
00:05:24,660 --> 00:05:26,250
So that's tag-based access controls.

131
00:05:26,250 --> 00:05:28,350
We think it simplifies things quite a lot.

132
00:05:29,280 --> 00:05:30,930
We went through general purpose

133
00:05:30,930 --> 00:05:32,370
buckets here in this example.

134
00:05:32,370 --> 00:05:36,330
But this works for all S3
bucket types, directory buckets,

135
00:05:36,330 --> 00:05:37,800
access points, table buckets,

136
00:05:37,800 --> 00:05:40,410
tables, vector buckets, vector indexes,

137
00:05:40,410 --> 00:05:43,170
which we'll talk about
here in a few minutes.

138
00:05:43,170 --> 00:05:45,600
So that's what we have on access controls.

139
00:05:45,600 --> 00:05:48,510
Now every year we come in

140
00:05:48,510 --> 00:05:51,930
and talk about these useful
features on the security side

141
00:05:51,930 --> 00:05:54,330
that we'd like y'all to turn on and use.

142
00:05:54,330 --> 00:05:56,340
We always recommend that you turn features

143
00:05:56,340 --> 00:05:58,740
like block public access
on at the account level

144
00:05:58,740 --> 00:06:00,120
so that any new bucket you create in

145
00:06:00,120 --> 00:06:01,320
that account automatically

146
00:06:01,320 --> 00:06:04,590
has public access blocked by default.

147
00:06:04,590 --> 00:06:08,280
But more and more customers
are running AWS organizations.

148
00:06:08,280 --> 00:06:10,650
And so what we've done is we've
leveled these two features

149
00:06:10,650 --> 00:06:12,450
up to the org level now.

150
00:06:12,450 --> 00:06:15,600
So now you can apply block
public access out at the org

151
00:06:15,600 --> 00:06:20,600
or OU level and this 403 context feature,

152
00:06:20,717 --> 00:06:22,590
this provides additional context

153
00:06:22,590 --> 00:06:26,010
and information about
your access denied errors.

154
00:06:26,010 --> 00:06:28,170
It used to only work
when a request started

155
00:06:28,170 --> 00:06:30,360
and ended within an AWS account.

156
00:06:30,360 --> 00:06:32,940
That's also stretched
out to an entire org now.

157
00:06:32,940 --> 00:06:34,650
So you get those improved error messages

158
00:06:34,650 --> 00:06:36,840
for any request within an org.

159
00:06:36,840 --> 00:06:39,810
So you're gonna see more
of this from us over time,

160
00:06:39,810 --> 00:06:41,430
adding more and more org-level content.

161
00:06:41,430 --> 00:06:44,370
And we're happy to get these
two org level controls out

162
00:06:44,370 --> 00:06:47,343
to you here just over the last few weeks.

163
00:06:48,510 --> 00:06:50,673
So that's our org level controls content.

164
00:06:51,630 --> 00:06:54,360
And just wanna say a few
things about encryption.

165
00:06:54,360 --> 00:06:57,750
Now, one thing that is
true about encryption

166
00:06:57,750 --> 00:07:00,150
is that it's constantly evolving.

167
00:07:00,150 --> 00:07:02,340
Here you see all the encryption types

168
00:07:02,340 --> 00:07:04,530
that we've launched over the years,

169
00:07:04,530 --> 00:07:06,177
satisfying different use cases

170
00:07:06,177 --> 00:07:07,860
and there's different
reasons why you would want

171
00:07:07,860 --> 00:07:10,350
to encrypt stuff in different ways.

172
00:07:10,350 --> 00:07:14,520
And we added another encryption
type just about a week ago,

173
00:07:14,520 --> 00:07:15,990
and that is PQ TLS,

174
00:07:15,990 --> 00:07:19,410
which provides post quantum encryption

175
00:07:19,410 --> 00:07:23,130
for TLS, for data in flight.

176
00:07:23,130 --> 00:07:25,710
Works across all S3 endpoints.

177
00:07:25,710 --> 00:07:29,550
If your client supports PQ TLS method,

178
00:07:29,550 --> 00:07:31,170
it'll automatically take that path,

179
00:07:31,170 --> 00:07:33,060
automatically encrypt in that way.

180
00:07:33,060 --> 00:07:35,880
And this, while quantum
computing isn't here yet,

181
00:07:35,880 --> 00:07:39,090
this protects against that capture now

182
00:07:39,090 --> 00:07:41,490
and decrypt later threat vector

183
00:07:41,490 --> 00:07:44,250
and just gets the quantum computing threat

184
00:07:44,250 --> 00:07:47,310
sort of out of the way moving forward.

185
00:07:47,310 --> 00:07:51,450
So PQ TLS is the latest
addition to the sort

186
00:07:51,450 --> 00:07:53,193
of S3 encryption portfolio.

187
00:07:54,240 --> 00:07:57,720
Now in the security space
over the last few years,

188
00:07:57,720 --> 00:08:01,110
one thing that we've done is
we've updated the defaults.

189
00:08:01,110 --> 00:08:03,810
We did it with block public
access, which is now default.

190
00:08:03,810 --> 00:08:05,580
We did it with access control lists,

191
00:08:05,580 --> 00:08:06,930
which are now off by default.

192
00:08:06,930 --> 00:08:09,750
We did it with encryption at rest.

193
00:08:09,750 --> 00:08:12,000
And our thinking on this is that we wanted

194
00:08:12,000 --> 00:08:14,550
to update the defaults with
best practice recommendations

195
00:08:14,550 --> 00:08:17,580
so that you start from a
place of best practices.

196
00:08:17,580 --> 00:08:20,703
It's really important for
security in particular.

197
00:08:22,200 --> 00:08:25,290
And the way that we think
about it is for those features

198
00:08:25,290 --> 00:08:28,560
that have smaller, more niche use cases,

199
00:08:28,560 --> 00:08:30,450
we'd like to have those off by default.

200
00:08:30,450 --> 00:08:33,180
They're opt-in so that you
don't have to think about 'em

201
00:08:33,180 --> 00:08:36,000
if you don't need them,
which most of you don't.

202
00:08:36,000 --> 00:08:39,450
Now, SSEC, which is an
encryption type that allows you

203
00:08:39,450 --> 00:08:42,030
to pass in an encryption key generated

204
00:08:42,030 --> 00:08:45,420
by you outside of KMS is
one of those features.

205
00:08:45,420 --> 00:08:48,420
And so a change that we've
made here in November

206
00:08:48,420 --> 00:08:51,840
is to give you the ability
to just turn this option off

207
00:08:51,840 --> 00:08:55,140
so you can update your
bucket configuration

208
00:08:55,140 --> 00:08:58,290
to not accept SEC objects regardless

209
00:08:58,290 --> 00:09:00,180
of what the client says.

210
00:09:00,180 --> 00:09:03,360
This is an optional tag
that you can flag now.

211
00:09:03,360 --> 00:09:06,390
An optional property
that you can turn on now,

212
00:09:06,390 --> 00:09:08,730
but in 2026 we'll update the default.

213
00:09:08,730 --> 00:09:13,730
So any new bucket created
will have SSEC off by default.

214
00:09:14,160 --> 00:09:18,120
So some updates to encryption.

215
00:09:18,120 --> 00:09:20,190
A big update to access controls

216
00:09:20,190 --> 00:09:21,990
with tag-based access controls

217
00:09:21,990 --> 00:09:24,540
and then some new org level controls.

218
00:09:24,540 --> 00:09:25,740
That's the update that I have for you

219
00:09:25,740 --> 00:09:28,980
on security this year from 2025.

220
00:09:28,980 --> 00:09:31,890
So that outta the way.

221
00:09:31,890 --> 00:09:34,143
Let's check in on our demo, Anna.

222
00:09:36,504 --> 00:09:38,730
- All right. Hello everyone.

223
00:09:38,730 --> 00:09:39,563
As Paul mentioned,

224
00:09:39,563 --> 00:09:41,850
we have a very complicated
demo ahead of us.

225
00:09:41,850 --> 00:09:45,060
Many things can go wrong.
Hopefully none of them will.

226
00:09:45,060 --> 00:09:47,400
And to kick it off, we took a video.

227
00:09:47,400 --> 00:09:51,030
So as Paul was talking about
the three security improvements

228
00:09:51,030 --> 00:09:53,040
I used Amazon Transcribe

229
00:09:53,040 --> 00:09:58,040
to create speech-to-text
representation of this video.

230
00:09:58,080 --> 00:10:00,780
We captured the names of
our volunteers here as well,

231
00:10:00,780 --> 00:10:02,163
Matt and Indra Neil.

232
00:10:03,600 --> 00:10:06,120
Here's what I'm going to do next.

233
00:10:06,120 --> 00:10:09,930
I also downloaded about
700 customer stories

234
00:10:09,930 --> 00:10:11,823
from AWS public website.

235
00:10:12,750 --> 00:10:15,960
I will rename all these
files to have unique names

236
00:10:15,960 --> 00:10:17,940
so that we can no longer determine

237
00:10:17,940 --> 00:10:20,790
which one has the transcript of the video.

238
00:10:20,790 --> 00:10:22,680
I will upload these files to a S3 bucket,

239
00:10:22,680 --> 00:10:24,810
which I created ahead of time.

240
00:10:24,810 --> 00:10:28,140
And then I will tag with color tag

241
00:10:28,140 --> 00:10:31,860
some of these files at random.

242
00:10:31,860 --> 00:10:35,730
I will be using shades of
purple for my color tags.

243
00:10:35,730 --> 00:10:37,350
All right.

244
00:10:37,350 --> 00:10:40,143
I have a script. I'll start with a script.

245
00:10:42,136 --> 00:10:44,760
Oh no.
- [Paul] Oh, no.

246
00:10:44,760 --> 00:10:47,400
- [Anna] Is it familiar
situation? Access denied.

247
00:10:47,400 --> 00:10:48,690
So frustrating.

248
00:10:48,690 --> 00:10:50,370
But no worries, no worries Paul.

249
00:10:50,370 --> 00:10:53,280
This is a hundred percent
staged failure to show

250
00:10:53,280 --> 00:10:56,190
how easy it is to debug
access denied issues

251
00:10:56,190 --> 00:10:58,410
with enhanced error messaging.

252
00:10:58,410 --> 00:11:00,240
So what my script is doing,

253
00:11:00,240 --> 00:11:04,443
it's trying to list my bucket
to confirm that it is empty.

254
00:11:05,670 --> 00:11:07,777
But it got access denied,

255
00:11:07,777 --> 00:11:11,790
and so from this message
I can see the actor,

256
00:11:11,790 --> 00:11:13,590
I can see the permission

257
00:11:13,590 --> 00:11:17,550
and I can see the resource
that is being accessed.

258
00:11:17,550 --> 00:11:18,900
I can also see that access

259
00:11:18,900 --> 00:11:22,080
was denied explicitly via bucket policy.

260
00:11:22,080 --> 00:11:24,870
Let's quickly hop to S3 console.

261
00:11:24,870 --> 00:11:29,640
So in my bucket policy, I
have denied for any principle,

262
00:11:29,640 --> 00:11:34,140
for least bucket action if
my principle environment tag

263
00:11:34,140 --> 00:11:37,200
does not match my bucket environment tag.

264
00:11:37,200 --> 00:11:40,770
Policies like this, they
can be very suitable

265
00:11:40,770 --> 00:11:44,730
if you want to achieve a
highly dynamic access controls

266
00:11:44,730 --> 00:11:47,880
because in policies like
this you can just manipulate

267
00:11:47,880 --> 00:11:52,380
bucket tag to make this deny go away.

268
00:11:52,380 --> 00:11:54,000
One thing I forgot, mention,

269
00:11:54,000 --> 00:11:56,850
if my principle doesn't
have environment tag,

270
00:11:56,850 --> 00:11:59,280
like all the principles
in my presentation,

271
00:11:59,280 --> 00:12:01,650
the default value of re:Invent 2025

272
00:12:01,650 --> 00:12:03,480
will be used in its place.

273
00:12:03,480 --> 00:12:05,490
So let's give it a try.

274
00:12:05,490 --> 00:12:10,320
I navigate to the properties
console, I'm adding a tag.

275
00:12:10,320 --> 00:12:14,310
It is environment tag with
the value re:Invent 2025.

276
00:12:14,310 --> 00:12:16,470
I'm saving changes and
I'm also making sure

277
00:12:16,470 --> 00:12:18,628
that bucket ABAC configuration

278
00:12:18,628 --> 00:12:23,010
is in the enabled state as
this is what enacts my tag.

279
00:12:23,010 --> 00:12:27,363
Now I can go back to my
script and restart it.

280
00:12:29,280 --> 00:12:31,083
Oh no, it is not empty.

281
00:12:32,610 --> 00:12:34,590
Let's go back to my bucket

282
00:12:34,590 --> 00:12:38,373
and let's empty it first to
make sure it is indeed empty.

283
00:12:44,670 --> 00:12:49,623
All right, going back
third time is a charm. Yes.

284
00:12:51,090 --> 00:12:55,410
Okay, it'll take us a moment
to upload this 700 object

285
00:12:55,410 --> 00:12:57,813
and we will check on
this upload in a second.

286
00:12:59,438 --> 00:13:00,420
- All right, thank you.

287
00:13:00,420 --> 00:13:04,173
That was the pick a card,
any card part of the demo.

288
00:13:05,280 --> 00:13:06,690
Now we're gonna talk about durability.

289
00:13:06,690 --> 00:13:11,130
So Esri is famously 11 nines durable.

290
00:13:11,130 --> 00:13:12,570
And my plan was to get up here

291
00:13:12,570 --> 00:13:15,540
and sort of dive deep
into how we achieve that.

292
00:13:15,540 --> 00:13:18,273
But I just don't have
time to do that today.

293
00:13:19,200 --> 00:13:20,820
So if you click on one link in the deck,

294
00:13:20,820 --> 00:13:22,500
I would click on this one.

295
00:13:22,500 --> 00:13:25,050
Links to a YouTube video with one

296
00:13:25,050 --> 00:13:26,760
of our senior engineers really diving

297
00:13:26,760 --> 00:13:29,430
into how we achieve 11
nines of durability.

298
00:13:29,430 --> 00:13:31,080
It's a really interesting topic

299
00:13:31,080 --> 00:13:33,540
and I encourage customers
to really understand it,

300
00:13:33,540 --> 00:13:36,900
'cause it just helps to build against S3.

301
00:13:36,900 --> 00:13:39,870
Just in short, there are
really three things that we do

302
00:13:39,870 --> 00:13:41,850
to get you 11 nines of durability.

303
00:13:41,850 --> 00:13:44,700
The first thing is we do
end-to-end integrity checking

304
00:13:44,700 --> 00:13:47,400
to make sure that bits
stay good as they flow

305
00:13:47,400 --> 00:13:50,880
through the system from our
front door down to storage.

306
00:13:50,880 --> 00:13:53,580
The second thing is we
always store data redundantly

307
00:13:53,580 --> 00:13:55,380
on multiple storage devices

308
00:13:55,380 --> 00:13:57,060
so that we can tolerate the failure

309
00:13:57,060 --> 00:13:59,040
of multiple devices with no impact

310
00:13:59,040 --> 00:14:00,570
to the integrity of your data.

311
00:14:00,570 --> 00:14:03,570
And the third thing that we do
is that we sweep back through

312
00:14:03,570 --> 00:14:06,930
and audit the durability of
your data over time to make sure

313
00:14:06,930 --> 00:14:10,380
that your data stays good and
is not corrupted by a bit flip

314
00:14:10,380 --> 00:14:12,150
or something like that.

315
00:14:12,150 --> 00:14:14,190
So the high level, those
are the three big things

316
00:14:14,190 --> 00:14:16,481
that are going on within S3

317
00:14:16,481 --> 00:14:18,360
and we're executing billions

318
00:14:18,360 --> 00:14:20,640
and billions of check
sums every single second

319
00:14:20,640 --> 00:14:23,370
in order to achieve these three things.

320
00:14:23,370 --> 00:14:25,920
This third one, the
durability audit is something

321
00:14:25,920 --> 00:14:27,750
that we've heard about
from customers quite a lot,

322
00:14:27,750 --> 00:14:30,840
especially customers
that run video archives

323
00:14:30,840 --> 00:14:33,270
or other sort of high-value
digital archives.

324
00:14:33,270 --> 00:14:36,960
These customers often have
very important digital assets.

325
00:14:36,960 --> 00:14:39,000
They need to go back and validate

326
00:14:39,000 --> 00:14:41,400
that they're good over time.

327
00:14:41,400 --> 00:14:43,200
And so for that use case as well

328
00:14:43,200 --> 00:14:45,060
as just a general capability,

329
00:14:45,060 --> 00:14:48,210
to give you a general capability
to validate that we have

330
00:14:48,210 --> 00:14:49,500
what you think we have,

331
00:14:49,500 --> 00:14:52,500
we launched some new
data integrity checks.

332
00:14:52,500 --> 00:14:54,450
What this feature is, is it allows you

333
00:14:54,450 --> 00:14:58,380
to efficiently run check
sums on data at rest in S3

334
00:14:58,380 --> 00:15:02,583
to validate that in fact
your data is as expected.

335
00:15:03,480 --> 00:15:06,810
You run this out of our
batch operations system.

336
00:15:06,810 --> 00:15:09,480
You just submit a job,
submit a list of objects

337
00:15:09,480 --> 00:15:10,830
to go check some.

338
00:15:10,830 --> 00:15:12,360
We'll quickly process that

339
00:15:12,360 --> 00:15:13,830
and give you back a report with

340
00:15:13,830 --> 00:15:17,220
all the checksums of the
objects in that manifest.

341
00:15:17,220 --> 00:15:19,530
Now this is far more
efficient than you can achieve

342
00:15:19,530 --> 00:15:21,570
on your own because you don't have

343
00:15:21,570 --> 00:15:24,060
to spin up any compute
to process checksums.

344
00:15:24,060 --> 00:15:25,920
You just give us a job.

345
00:15:25,920 --> 00:15:27,750
It doesn't affect the tiers

346
00:15:27,750 --> 00:15:29,550
if you're running intelligent tiering.

347
00:15:29,550 --> 00:15:33,240
So cold data will stay cold
to save you on storage costs

348
00:15:33,240 --> 00:15:35,070
and it works with glacier objects

349
00:15:35,070 --> 00:15:37,620
as well without you
having to go orchestrate

350
00:15:37,620 --> 00:15:40,620
and manage glacier restores back to S3.

351
00:15:40,620 --> 00:15:43,860
Okay, so great feature for validating

352
00:15:43,860 --> 00:15:47,553
the durability of your data within S3.

353
00:15:48,840 --> 00:15:51,690
Next thing I wanna talk
about is S3 Express.

354
00:15:51,690 --> 00:15:54,330
So the way to think about
S3 Express is it's sort

355
00:15:54,330 --> 00:15:58,770
of a stripped down lean mean version of S3

356
00:15:58,770 --> 00:16:01,050
that is just built for low latency.

357
00:16:01,050 --> 00:16:04,020
It's able to achieve
single digit millisecond

358
00:16:04,020 --> 00:16:06,360
requests very consistently.

359
00:16:06,360 --> 00:16:09,570
It's typically used for
high throughput use cases.

360
00:16:09,570 --> 00:16:13,680
Some very big training workloads
on S3 Express right now.

361
00:16:13,680 --> 00:16:16,440
But customers are coming up
with all interesting ways

362
00:16:16,440 --> 00:16:18,600
to leverage low latency S3.

363
00:16:18,600 --> 00:16:20,580
I've heard a bunch of interesting examples

364
00:16:20,580 --> 00:16:22,330
of that here at the show this year.

365
00:16:23,310 --> 00:16:26,490
We updated S3 Express in a number of ways

366
00:16:26,490 --> 00:16:27,780
over the course of the year.

367
00:16:27,780 --> 00:16:29,280
We did a price cut reducing

368
00:16:29,280 --> 00:16:32,790
the cost of S3 Express by up to 85%.

369
00:16:32,790 --> 00:16:34,230
We raised the TPS limit

370
00:16:34,230 --> 00:16:36,690
or the transaction per second limit,

371
00:16:36,690 --> 00:16:38,520
up to 2 million reads per second

372
00:16:38,520 --> 00:16:42,120
and 200,000 writes per second.

373
00:16:42,120 --> 00:16:46,200
We introduced an object rename
API in S3 Express buckets,

374
00:16:46,200 --> 00:16:49,590
so that you can anatomically
rename individual objects

375
00:16:49,590 --> 00:16:51,720
and we added access point support

376
00:16:51,720 --> 00:16:53,430
to give you more granular control

377
00:16:53,430 --> 00:16:55,920
over permissions to S3 Express.

378
00:16:55,920 --> 00:16:59,040
So a bunch of interesting
important updates

379
00:16:59,040 --> 00:17:02,040
to Express here over the
course of the last year

380
00:17:02,040 --> 00:17:04,380
and we're very excited
about the trajectory

381
00:17:04,380 --> 00:17:07,143
of this service heading into next year.

382
00:17:08,370 --> 00:17:10,350
Now the next thing that I want to cover

383
00:17:10,350 --> 00:17:12,180
for you is S3 Objects.

384
00:17:12,180 --> 00:17:15,420
So I'm gonna talk a little
bit just about the S3 API

385
00:17:15,420 --> 00:17:16,710
and some things that we're doing

386
00:17:16,710 --> 00:17:19,140
in order to help you manage lots

387
00:17:19,140 --> 00:17:22,053
and lots of objects across your buckets.

388
00:17:22,980 --> 00:17:27,090
Now last year we launched
conditional rights.

389
00:17:27,090 --> 00:17:29,100
This is sort of a long time coming

390
00:17:29,100 --> 00:17:30,723
but what conditional rights are is

391
00:17:30,723 --> 00:17:34,080
that it helps you in multi
writer scenarios to make sure

392
00:17:34,080 --> 00:17:37,860
that one writer doesn't
accidentally overwrite

393
00:17:37,860 --> 00:17:39,360
the work of another.

394
00:17:39,360 --> 00:17:41,340
And it comes in two flavors.

395
00:17:41,340 --> 00:17:44,790
You can do a put if absent
using the if absent condition.

396
00:17:44,790 --> 00:17:47,460
This is where you wanna write an object

397
00:17:47,460 --> 00:17:49,740
and you wanna make sure you
don't overwrite an object

398
00:17:49,740 --> 00:17:53,130
if it turns out there's an
object there to overwrite.

399
00:17:53,130 --> 00:17:56,400
And we also have a put if match condition

400
00:17:56,400 --> 00:17:59,160
where you pass in a checksum
with your put request

401
00:17:59,160 --> 00:18:01,811
and you will only overwrite an object

402
00:18:01,811 --> 00:18:05,370
if the object you're
about to overwrite matches

403
00:18:05,370 --> 00:18:07,170
that checksum in the put request.

404
00:18:07,170 --> 00:18:09,300
This is also called a,
referred to as a compare

405
00:18:09,300 --> 00:18:11,313
and swap operation as well.

406
00:18:12,450 --> 00:18:13,620
So we put this out last year

407
00:18:13,620 --> 00:18:17,250
and the response from
developers, which is amazing.

408
00:18:17,250 --> 00:18:21,150
So many of you took these
new APIs and built on them.

409
00:18:21,150 --> 00:18:24,030
It's one of our fastest adopted features

410
00:18:24,030 --> 00:18:26,100
and recent memory for sure.

411
00:18:26,100 --> 00:18:27,390
We're actually doing millions

412
00:18:27,390 --> 00:18:29,930
and millions of conditional puts today

413
00:18:29,930 --> 00:18:31,440
in just a short amount of time.

414
00:18:31,440 --> 00:18:32,850
And so we decided to kind

415
00:18:32,850 --> 00:18:35,850
of double down on conditional
rights and added two more.

416
00:18:35,850 --> 00:18:39,300
First we added conditional copy.

417
00:18:39,300 --> 00:18:42,420
This works the same exact
way with the same conditions

418
00:18:42,420 --> 00:18:43,860
as conditional put.

419
00:18:43,860 --> 00:18:45,750
Allows you to lock down a bucket

420
00:18:45,750 --> 00:18:48,360
to make sure that there's no way to write

421
00:18:48,360 --> 00:18:52,680
any data into your bucket
either using put or copy

422
00:18:52,680 --> 00:18:55,200
unless it has the
appropriate conditions on it.

423
00:18:55,200 --> 00:18:57,900
We also added conditional delete.

424
00:18:57,900 --> 00:19:01,470
Conditional delete works like
put if match in that you pass

425
00:19:01,470 --> 00:19:03,540
and a checksum in with a delete request

426
00:19:03,540 --> 00:19:04,860
and the delete will only go

427
00:19:04,860 --> 00:19:06,780
through if in fact you are deleting

428
00:19:06,780 --> 00:19:08,400
what you think you're deleting.

429
00:19:08,400 --> 00:19:10,350
The checksum has to match.

430
00:19:10,350 --> 00:19:12,870
This is for sure a
durability best practice,

431
00:19:12,870 --> 00:19:15,570
something that we do internally
when we're deleting data,

432
00:19:15,570 --> 00:19:19,380
sort of part of our, how we
achieve that 11 nines story.

433
00:19:19,380 --> 00:19:21,900
And it's an example of
us sort of externalizing

434
00:19:21,900 --> 00:19:23,790
the stuff that we are doing internally.

435
00:19:23,790 --> 00:19:26,340
So two new conditions that you can use

436
00:19:26,340 --> 00:19:28,953
with both copy and delete.

437
00:19:30,300 --> 00:19:33,510
Now I also wanna talk about big objects.

438
00:19:33,510 --> 00:19:35,250
So there are use cases out there,

439
00:19:35,250 --> 00:19:39,120
especially with video
and seismic and genomics

440
00:19:39,120 --> 00:19:41,520
that are generating really big objects.

441
00:19:41,520 --> 00:19:43,590
And so it was time for us to go ahead

442
00:19:43,590 --> 00:19:47,670
and lift the maximum object
size, which we did yesterday.

443
00:19:47,670 --> 00:19:50,610
So now you can run really big objects.

444
00:19:50,610 --> 00:19:53,730
Previous limit was five
terabytes, now it's 50.

445
00:19:53,730 --> 00:19:55,050
It's not really anything else

446
00:19:55,050 --> 00:19:56,160
to tell you about this feature

447
00:19:56,160 --> 00:19:59,070
other than you can run really
big objects right now, now.

448
00:19:59,070 --> 00:20:00,840
But this is a tough one
for us to implement.

449
00:20:00,840 --> 00:20:03,840
Changing in variance
in S3 is a lot of work

450
00:20:03,840 --> 00:20:06,720
and so we're happy to get
this out there, especially

451
00:20:06,720 --> 00:20:09,843
for those customers that
need those huge objects.

452
00:20:11,580 --> 00:20:13,350
Another thing that's
been true in recent years

453
00:20:13,350 --> 00:20:15,000
is that you all are managing more

454
00:20:15,000 --> 00:20:17,043
and more objects in your buckets.

455
00:20:17,910 --> 00:20:20,100
Not uncommon for
customers to have millions

456
00:20:20,100 --> 00:20:23,280
or even billions of
objects in an S3 bucket.

457
00:20:23,280 --> 00:20:26,730
We really give you two methods
for managing large groups

458
00:20:26,730 --> 00:20:29,310
of objects and those are object tags

459
00:20:29,310 --> 00:20:30,690
and batch operations,

460
00:20:30,690 --> 00:20:32,880
which we talked a little
bit about already.

461
00:20:32,880 --> 00:20:34,980
On object tags what we did this year

462
00:20:34,980 --> 00:20:37,230
is we did a price cut back in the spring,

463
00:20:37,230 --> 00:20:40,290
reduced the cost of object tags by 35%.

464
00:20:40,290 --> 00:20:43,350
So if economics were holding
you back from using tags,

465
00:20:43,350 --> 00:20:46,530
you should take another
look at the math on that.

466
00:20:46,530 --> 00:20:50,224
We also did a bunch of
updates to batch operations.

467
00:20:50,224 --> 00:20:52,560
We'll talk through those in just a second.

468
00:20:52,560 --> 00:20:53,970
But just real quick,

469
00:20:53,970 --> 00:20:56,550
for those of you who aren't
familiar with batch operations

470
00:20:56,550 --> 00:21:01,080
or BOPS as we affectionately
call it internally.

471
00:21:01,080 --> 00:21:03,510
Batch operations allows you to process

472
00:21:03,510 --> 00:21:06,180
many, many objects at scale.

473
00:21:06,180 --> 00:21:07,800
And it does this very reliably

474
00:21:07,800 --> 00:21:11,850
because we handle all of the
retries and job management

475
00:21:11,850 --> 00:21:13,740
and all the undifferentiated
work that you have

476
00:21:13,740 --> 00:21:17,760
to do in order to do anything
a billion times over.

477
00:21:17,760 --> 00:21:18,840
It's also very flexible.

478
00:21:18,840 --> 00:21:22,110
It comes with 11 sort of
prepackaged operations

479
00:21:22,110 --> 00:21:23,640
that you can execute against your data,

480
00:21:23,640 --> 00:21:26,940
like adding and removing
tags, copying data,

481
00:21:26,940 --> 00:21:30,780
adjusting object lock
settings and a bunch more.

482
00:21:30,780 --> 00:21:32,520
One of those operations is actually firing

483
00:21:32,520 --> 00:21:35,212
a lambda function, which
gives you a whole bunch

484
00:21:35,212 --> 00:21:36,675
of flexibility on top of

485
00:21:36,675 --> 00:21:39,180
those pre-canned things that you can do.

486
00:21:39,180 --> 00:21:43,653
So we made a bunch of
updates to BOPS in 2025.

487
00:21:44,640 --> 00:21:47,820
We added a no manifest
option for batch operations.

488
00:21:47,820 --> 00:21:52,020
So instead of passing in a
list of objects to go process,

489
00:21:52,020 --> 00:21:54,840
you can just point a batch
operations job at a bucket

490
00:21:54,840 --> 00:21:56,970
or a prefix and we'll
go process everything

491
00:21:56,970 --> 00:21:58,530
in that container.

492
00:21:58,530 --> 00:22:02,370
We also added, IAM roll
creation automation as well.

493
00:22:02,370 --> 00:22:04,740
So you can more easily get a job going.

494
00:22:04,740 --> 00:22:07,950
We raised the scale of each job

495
00:22:07,950 --> 00:22:12,600
so you can now process jobs
up to 20 billion objects

496
00:22:12,600 --> 00:22:15,990
and we increased the
performance of batch operations

497
00:22:15,990 --> 00:22:18,210
by up to 10 x here just this week.

498
00:22:18,210 --> 00:22:21,810
So batch operations in
2025 got some nice updates.

499
00:22:21,810 --> 00:22:24,690
It's easier to use,
more scalable and faster

500
00:22:24,690 --> 00:22:26,940
than the last time we
were all here together.

501
00:22:27,840 --> 00:22:30,480
So that is batch operations.

502
00:22:30,480 --> 00:22:34,533
And these are the updates
that we had to objects.

503
00:22:35,640 --> 00:22:38,460
So just to take a very
unnecessary side quest

504
00:22:38,460 --> 00:22:40,260
in our complicated demo,

505
00:22:40,260 --> 00:22:42,240
we're gonna hand it to
Anna to actually show

506
00:22:42,240 --> 00:22:43,540
what what BOPS looks like.

507
00:22:45,210 --> 00:22:47,280
- [Anna] All right, so we ended up

508
00:22:47,280 --> 00:22:50,220
with uploading 744 objects

509
00:22:50,220 --> 00:22:53,160
all successfully uploaded to my bucket.

510
00:22:53,160 --> 00:22:55,590
All of them renamed to some unique names

511
00:22:55,590 --> 00:22:58,590
so that we can no longer know
which one is the transcript.

512
00:22:58,590 --> 00:23:02,070
And some of these objects are
also tagged with color tag,

513
00:23:02,070 --> 00:23:04,800
which is just some random shade of purple.

514
00:23:04,800 --> 00:23:06,330
So now what if you want

515
00:23:06,330 --> 00:23:09,750
to assign a different
color tag to this object

516
00:23:09,750 --> 00:23:13,200
and most importantly, what if
you want to assign it at bulk?

517
00:23:13,200 --> 00:23:16,020
This is where a batch
operations really shine.

518
00:23:16,020 --> 00:23:19,050
In couple clicks, no manifest,

519
00:23:19,050 --> 00:23:23,250
we can create a job just
by picking our bucket,

520
00:23:23,250 --> 00:23:25,830
picking some match
criteria for the object.

521
00:23:25,830 --> 00:23:28,140
For today's demo we will
be using zero three.

522
00:23:28,140 --> 00:23:31,410
That's because it's December 3rd today.

523
00:23:31,410 --> 00:23:35,490
And in couple more clicks
we can ask Paul also,

524
00:23:35,490 --> 00:23:37,710
what color is his favorite?

525
00:23:37,710 --> 00:23:38,543
- [Paul] Black.

526
00:23:38,543 --> 00:23:40,353
- [Anna] Black, clearly.

527
00:23:43,140 --> 00:23:46,470
All right. Black for Paul.

528
00:23:46,470 --> 00:23:51,470
Now, we picked to start the
job as soon as it is ready.

529
00:23:51,480 --> 00:23:53,310
We opt out of completion report

530
00:23:53,310 --> 00:23:55,200
as we don't need it for this demo.

531
00:23:55,200 --> 00:23:58,050
And as Paul mentioned
now we can also create

532
00:23:58,050 --> 00:24:01,260
a new role that has all
the necessary permissions

533
00:24:01,260 --> 00:24:05,010
to do tag re-assignment
for all the objects.

534
00:24:05,010 --> 00:24:06,063
And that's it.

535
00:24:07,980 --> 00:24:09,300
We submit this job.

536
00:24:09,300 --> 00:24:12,780
We will check on its progress in a second.

537
00:24:12,780 --> 00:24:15,723
It will take probably
couple seconds to complete.

538
00:24:16,980 --> 00:24:19,410
And we are jumping back to Paul.

539
00:24:19,410 --> 00:24:20,343
- And thank you.

540
00:24:21,570 --> 00:24:25,380
So next I'm gonna talk a
little bit about S3 Tables.

541
00:24:25,380 --> 00:24:28,770
S3 Tables gives you fully
managed Apache Iceberg

542
00:24:28,770 --> 00:24:31,110
tables in S3.

543
00:24:31,110 --> 00:24:33,090
We launched it at re:Invent last year

544
00:24:33,090 --> 00:24:35,643
and the response from
customers was just amazing.

545
00:24:36,540 --> 00:24:39,390
We got so much great
feedback, so much interest

546
00:24:39,390 --> 00:24:41,910
and it sounds cheesy, but
the team really went home

547
00:24:41,910 --> 00:24:45,960
from re:Invent just
inspired by all the interest

548
00:24:45,960 --> 00:24:48,063
that came from customers.

549
00:24:48,930 --> 00:24:51,270
And since then y'all have created more

550
00:24:51,270 --> 00:24:53,910
than 400,000 tables in the system.

551
00:24:53,910 --> 00:24:55,710
We're very happy about that

552
00:24:55,710 --> 00:24:57,870
it's proven to be useful for you.

553
00:24:57,870 --> 00:25:00,820
And we've done a lot of
work to mature the service

554
00:25:01,680 --> 00:25:04,560
covering a lot of ground
in the first year.

555
00:25:04,560 --> 00:25:07,560
There's actually more than
I can cover on S3 Tables

556
00:25:07,560 --> 00:25:10,364
in this session that we've done this year.

557
00:25:10,364 --> 00:25:12,870
It would just be the
S3 Tables, what's new?

558
00:25:12,870 --> 00:25:14,880
And we've got a lot of
other stuff to cover.

559
00:25:14,880 --> 00:25:17,190
But I wanted to show this slide just

560
00:25:17,190 --> 00:25:19,590
in case there are folks
here in the audience

561
00:25:19,590 --> 00:25:22,530
or that are watching that
saw the launch last year,

562
00:25:22,530 --> 00:25:24,313
decided that they'd wait

563
00:25:24,313 --> 00:25:27,513
for the service to mature a bit
before taking a second look.

564
00:25:27,513 --> 00:25:29,694
I just wanna flag this for you,

565
00:25:29,694 --> 00:25:31,860
'cause we've put a lot of development

566
00:25:31,860 --> 00:25:33,240
in over the course of the last 12 months.

567
00:25:33,240 --> 00:25:34,190
It's matured a lot.

568
00:25:35,100 --> 00:25:36,690
But I am gonna kind of go through

569
00:25:36,690 --> 00:25:39,870
some of my favorites here
in the next few slides.

570
00:25:39,870 --> 00:25:41,880
Just give you a feel for the progress

571
00:25:41,880 --> 00:25:43,983
that we've made on S3 Tables.

572
00:25:45,090 --> 00:25:47,760
We introduced an Iceberg
Rest catalog endpoint

573
00:25:47,760 --> 00:25:50,370
for S3 Tables buckets.

574
00:25:50,370 --> 00:25:54,000
This allows you to communicate
directly with your tables

575
00:25:54,000 --> 00:25:57,000
and manage them using
open source Iceberg APIs.

576
00:25:57,000 --> 00:25:59,670
If you have a very simple
use case like with PI Iceberg

577
00:25:59,670 --> 00:26:02,340
where you just wanna write
directly to S3 Tables

578
00:26:02,340 --> 00:26:07,020
and skip having to go through
a third catalog system,

579
00:26:07,020 --> 00:26:09,420
you can do that by reading
and writing directly

580
00:26:09,420 --> 00:26:11,040
from that IRC endpoint.

581
00:26:11,040 --> 00:26:13,110
We raise the table limit up

582
00:26:13,110 --> 00:26:15,360
to 10,000 tables per table bucket.

583
00:26:15,360 --> 00:26:16,740
We added sort compaction,

584
00:26:16,740 --> 00:26:19,890
which rearranges the rows
in a table on compaction

585
00:26:19,890 --> 00:26:22,290
to make your queries
faster and more efficient.

586
00:26:22,290 --> 00:26:25,140
We did a compaction price
cut, reducing the price

587
00:26:25,140 --> 00:26:28,620
by 90% and we added some new options

588
00:26:28,620 --> 00:26:30,780
to the create table API

589
00:26:30,780 --> 00:26:33,450
so that you can set schema on create.

590
00:26:33,450 --> 00:26:35,768
So that's just a few
of the smaller features

591
00:26:35,768 --> 00:26:38,553
that we delivered over the
course of the last year.

592
00:26:39,660 --> 00:26:40,800
And I'm gonna take you through a few more,

593
00:26:40,800 --> 00:26:42,780
but I just wanna take one minute

594
00:26:42,780 --> 00:26:47,640
to about this concept of
tables as AWS resources.

595
00:26:47,640 --> 00:26:52,290
Now this is a characteristic
that is unique

596
00:26:52,290 --> 00:26:54,780
to S3 Tables compared to the rest

597
00:26:54,780 --> 00:26:57,210
of the ways that you can run Iceberg.

598
00:26:57,210 --> 00:27:02,210
Every table in S3 Tables is
a first class AWS resource,

599
00:27:02,550 --> 00:27:04,260
which means it has an ARN.

600
00:27:04,260 --> 00:27:06,420
And it paves the way, it
opens up a lot of options

601
00:27:06,420 --> 00:27:09,060
for us in terms of the sorts
of features that we can deliver

602
00:27:09,060 --> 00:27:12,390
to you because so many services

603
00:27:12,390 --> 00:27:15,330
and capabilities across AWS expect

604
00:27:15,330 --> 00:27:18,420
to operate on AWS resources.

605
00:27:18,420 --> 00:27:19,830
Just a few examples this year

606
00:27:19,830 --> 00:27:22,500
are table level CloudWatch and CloudTrail.

607
00:27:22,500 --> 00:27:26,250
So you can get observability
down to the table level.

608
00:27:26,250 --> 00:27:27,870
We throw events at the table level

609
00:27:27,870 --> 00:27:29,490
that tell you, CloudWatch events,

610
00:27:29,490 --> 00:27:32,018
that tell you when we compact your data.

611
00:27:32,018 --> 00:27:34,620
Cost management tooling.

612
00:27:34,620 --> 00:27:37,050
So you now have tables in the cost

613
00:27:37,050 --> 00:27:39,180
and usage report and then cost explorer

614
00:27:39,180 --> 00:27:40,350
so that you can understand

615
00:27:40,350 --> 00:27:43,440
which tables are driving your bill.

616
00:27:43,440 --> 00:27:44,640
KMS.

617
00:27:44,640 --> 00:27:48,630
We implemented table level
encryption keys at a KMS,

618
00:27:48,630 --> 00:27:51,840
so that you can assign a different KMS key

619
00:27:51,840 --> 00:27:54,980
to every single table in a table bucket.

620
00:27:54,980 --> 00:27:57,450
These are examples of features that

621
00:27:57,450 --> 00:27:59,340
would be difficult if not impossible

622
00:27:59,340 --> 00:28:02,760
to implement when tables
are logical entities

623
00:28:02,760 --> 00:28:06,481
just defined in metadata
in a file in an S3 bucket.

624
00:28:06,481 --> 00:28:09,870
A big feature that we had in mind

625
00:28:09,870 --> 00:28:11,940
when we launched S3 Tables,

626
00:28:11,940 --> 00:28:16,560
and that relies on this concept
of tables as AWS resources

627
00:28:16,560 --> 00:28:18,510
and a feature that
customers started asking us

628
00:28:18,510 --> 00:28:22,080
for immediately is table replication.

629
00:28:22,080 --> 00:28:23,160
We launched this yesterday

630
00:28:23,160 --> 00:28:25,980
and we're super excited to
get this out there to you.

631
00:28:25,980 --> 00:28:28,680
This feature provides read-only replicas

632
00:28:28,680 --> 00:28:31,953
of Iceberg tables that are in S3 tables.

633
00:28:32,820 --> 00:28:35,010
Now we've done replication
for a long time,

634
00:28:35,010 --> 00:28:38,250
we actually replicate more
than 150 petabytes a week.

635
00:28:38,250 --> 00:28:41,070
And so to be honest,

636
00:28:41,070 --> 00:28:43,230
I didn't think this was
gonna be a big deal for us

637
00:28:43,230 --> 00:28:47,220
to add table replication into the mix,

638
00:28:47,220 --> 00:28:50,280
but it is actually a different animal

639
00:28:50,280 --> 00:28:53,400
replicating tables compared
to just replicating objects.

640
00:28:53,400 --> 00:28:55,560
And I'm gonna quickly take you
through why here in a minute.

641
00:28:55,560 --> 00:28:58,290
So you understand what this feature does.

642
00:28:58,290 --> 00:29:00,780
Very high level when you
run an Iceberg query,

643
00:29:00,780 --> 00:29:02,330
there's really two parts to it.

644
00:29:03,180 --> 00:29:05,037
The query goes down and gets the metadata

645
00:29:05,037 --> 00:29:09,330
and the metadata tells the
engine what data files it needs

646
00:29:09,330 --> 00:29:12,068
to go read in order to satisfy the query.

647
00:29:12,068 --> 00:29:15,930
And so naively, if we were
just to shift all of this over

648
00:29:15,930 --> 00:29:17,400
to the replica side,

649
00:29:17,400 --> 00:29:19,800
the results would look
something like this.

650
00:29:19,800 --> 00:29:22,860
Because that metadata
is bit for bit the same

651
00:29:22,860 --> 00:29:25,950
as the source side and it
still references those original

652
00:29:25,950 --> 00:29:29,580
data files and this is
obviously not gonna work.

653
00:29:29,580 --> 00:29:32,040
And so it's not really
metadata replication

654
00:29:32,040 --> 00:29:34,020
that we're doing, it's a rewrite.

655
00:29:34,020 --> 00:29:37,710
We're rewriting the metadata
for you onto the replica side

656
00:29:37,710 --> 00:29:40,560
so that it's referencing local data files

657
00:29:40,560 --> 00:29:43,980
so that your query can actually
do what it's supposed to do.

658
00:29:43,980 --> 00:29:45,420
But we have to do more in order

659
00:29:45,420 --> 00:29:47,940
to make this a reliable service

660
00:29:47,940 --> 00:29:50,970
because our replication is asynchronous.

661
00:29:50,970 --> 00:29:53,730
Not all objects are gonna
arrive at the same time.

662
00:29:53,730 --> 00:29:56,970
And when a query comes in,
you don't want it referencing,

663
00:29:56,970 --> 00:29:58,660
you don't want the metadata
referencing an object

664
00:29:58,660 --> 00:30:00,660
that's not there yet.

665
00:30:00,660 --> 00:30:02,040
That'll just crash and burn

666
00:30:02,040 --> 00:30:05,970
and give your end users a bad experience.

667
00:30:05,970 --> 00:30:08,790
The appropriate behavior
is for that query to hit

668
00:30:08,790 --> 00:30:12,150
the most recent complete snapshot.

669
00:30:12,150 --> 00:30:15,420
So that query results can be
successfully returned back

670
00:30:15,420 --> 00:30:17,820
to your app, and then for the metadata

671
00:30:17,820 --> 00:30:21,900
to be appropriately updated
once all of the data is over.

672
00:30:21,900 --> 00:30:24,840
So that we're always giving you data

673
00:30:24,840 --> 00:30:29,640
that represents the most recent
complete data set available.

674
00:30:29,640 --> 00:30:31,740
So that's what this feature does.

675
00:30:31,740 --> 00:30:34,680
There's a catalog
consistency element to it

676
00:30:34,680 --> 00:30:36,480
and we're happy to get it out there.

677
00:30:37,590 --> 00:30:39,000
So that's sort of our big,

678
00:30:39,000 --> 00:30:41,430
one of our big launches
here at re:Invent this year.

679
00:30:41,430 --> 00:30:43,110
The other big launch that we have

680
00:30:43,110 --> 00:30:46,230
that we launched yesterday is a feature

681
00:30:46,230 --> 00:30:49,050
that personally I don't
really consider S3 Tables

682
00:30:49,050 --> 00:30:50,820
to be complete until we had it.

683
00:30:50,820 --> 00:30:52,560
It's a feature that's widely adopted

684
00:30:52,560 --> 00:30:55,170
on the general purpose side,

685
00:30:55,170 --> 00:30:58,560
a feature that customers have
saved more than $6 billion

686
00:30:58,560 --> 00:31:00,600
on storage costs with over the course

687
00:31:00,600 --> 00:31:02,730
of the last several years
since it was launched.

688
00:31:02,730 --> 00:31:04,380
And that feature is intelligence hearing,

689
00:31:04,380 --> 00:31:06,930
which is now supported with S3 Tables.

690
00:31:06,930 --> 00:31:09,300
It gets you automatic cost up optimization

691
00:31:09,300 --> 00:31:12,480
for your data lake, reduces
storage costs by up to 80%.

692
00:31:12,480 --> 00:31:14,670
And here's how that's achieved.

693
00:31:14,670 --> 00:31:16,380
So for those of you who are familiar

694
00:31:16,380 --> 00:31:17,697
with Intelligent Tiering

695
00:31:17,697 --> 00:31:19,200
and from general purpose buckets,

696
00:31:19,200 --> 00:31:21,150
this will look very familiar to you.

697
00:31:21,150 --> 00:31:24,810
It's the same exact tiers,
same exact cost reductions,

698
00:31:24,810 --> 00:31:28,860
same exact tier names as you
have with general purpose.

699
00:31:28,860 --> 00:31:32,340
The way that it works is
that we tier data down

700
00:31:32,340 --> 00:31:34,110
as it's not accessed over time.

701
00:31:34,110 --> 00:31:37,080
So if you don't access data for 30 days,

702
00:31:37,080 --> 00:31:38,970
we go down to the infrequent access tier,

703
00:31:38,970 --> 00:31:42,660
which reduces your storage cost by 40%.

704
00:31:42,660 --> 00:31:44,550
After 90 days, we reduce your storage cost

705
00:31:44,550 --> 00:31:48,270
by an additional 68% in the
archive instant access tier.

706
00:31:48,270 --> 00:31:49,410
This all happens automatically

707
00:31:49,410 --> 00:31:51,479
without you having to do anything at all.

708
00:31:51,479 --> 00:31:55,080
It's exactly the same as
with general purpose buckets.

709
00:31:55,080 --> 00:31:56,980
The intelligent tiering that you know.

710
00:31:57,900 --> 00:32:00,909
What's different is that in tables,

711
00:32:00,909 --> 00:32:04,410
our compaction and table
maintenance systems are aware

712
00:32:04,410 --> 00:32:06,030
of the underlying tiers.

713
00:32:06,030 --> 00:32:08,430
And this is a characteristic that's unique

714
00:32:08,430 --> 00:32:10,920
to S3 Tables compared
to the west of the ways

715
00:32:10,920 --> 00:32:13,530
that you can run Apache Iceberg.

716
00:32:13,530 --> 00:32:16,470
We only do compaction in
the frequent access tier.

717
00:32:16,470 --> 00:32:18,660
Our compaction systems
are aware of the tiers,

718
00:32:18,660 --> 00:32:22,710
and this prevents us from
accidentally tiering data up out

719
00:32:22,710 --> 00:32:24,180
of these cold tiers.

720
00:32:24,180 --> 00:32:27,150
And that reduces your storage costs

721
00:32:27,150 --> 00:32:28,710
and protects the cost savings

722
00:32:28,710 --> 00:32:31,260
that intelligent tiering
is delivering to you.

723
00:32:31,260 --> 00:32:34,080
Additionally, as we're compacting

724
00:32:34,080 --> 00:32:36,870
in the frequent access
tier, the reads that we need

725
00:32:36,870 --> 00:32:37,890
to do against your data

726
00:32:37,890 --> 00:32:39,810
in order to do that table maintenance

727
00:32:39,810 --> 00:32:42,570
that we're doing on your
behalf, don't count against

728
00:32:42,570 --> 00:32:45,270
that timer that I talked
about on the previous slide.

729
00:32:45,270 --> 00:32:47,370
So if we have to compact a
few times over the course

730
00:32:47,370 --> 00:32:50,040
of two weeks in order to
get you to the right sort

731
00:32:50,040 --> 00:32:52,440
of properly maintained
table, you don't have

732
00:32:52,440 --> 00:32:54,540
to wait another two
weak or another 30 days

733
00:32:54,540 --> 00:32:57,091
after the last compaction to tier down.

734
00:32:57,091 --> 00:33:00,300
You only have to wait
another initial 15 days

735
00:33:00,300 --> 00:33:01,980
or however much you have since

736
00:33:01,980 --> 00:33:05,280
the last time you read that object.

737
00:33:05,280 --> 00:33:07,557
So very happy to get intelligent tiering

738
00:33:07,557 --> 00:33:10,110
for S3 tables out there.

739
00:33:10,110 --> 00:33:15,030
And again, it works specifically
for Iceberg, which is nice.

740
00:33:16,410 --> 00:33:19,920
Now I've run through like a long list

741
00:33:19,920 --> 00:33:21,570
of feature content here

742
00:33:21,570 --> 00:33:23,280
and oftentimes when I do that,

743
00:33:23,280 --> 00:33:25,050
especially in the analytics space,

744
00:33:25,050 --> 00:33:26,910
customers ask me about

745
00:33:26,910 --> 00:33:30,810
how we're also making
things simple for them.

746
00:33:30,810 --> 00:33:33,750
And the big simplifier
that we've done this year

747
00:33:33,750 --> 00:33:36,870
for S3 Tables is to add support

748
00:33:36,870 --> 00:33:39,690
into SageMaker Unified Studio.

749
00:33:39,690 --> 00:33:43,140
And we updated Unified Studio
just a couple weeks ago

750
00:33:43,140 --> 00:33:46,263
to enable IAM-based one-click onboarding.

751
00:33:47,130 --> 00:33:50,070
And I pulled this into
the S3 deck this year

752
00:33:50,070 --> 00:33:53,520
because it really is
the easiest way to run

753
00:33:53,520 --> 00:33:57,660
and interact with data in S3 tables.

754
00:33:57,660 --> 00:33:59,910
You give a link now to jump off

755
00:33:59,910 --> 00:34:02,130
of the S3 management console

756
00:34:02,130 --> 00:34:05,760
from the detail page of
a table that you have.

757
00:34:05,760 --> 00:34:08,610
You can jump straight
into a SQL editor right

758
00:34:08,610 --> 00:34:11,790
from there with just a single click.

759
00:34:11,790 --> 00:34:14,190
Once you're in Unified Studio,
you can also navigate off

760
00:34:14,190 --> 00:34:16,170
and get access to our new
notebook that you can use

761
00:34:16,170 --> 00:34:19,740
to process data in your
tables with SQL or with Python

762
00:34:19,740 --> 00:34:22,290
or even with natural language.

763
00:34:22,290 --> 00:34:25,650
So just a very, very easy way to go browse

764
00:34:25,650 --> 00:34:30,650
and process and manipulate data in tables.

765
00:34:31,980 --> 00:34:36,120
So those are our updates on S3 tables.

766
00:34:36,120 --> 00:34:39,090
Again, very excited about the
trajectory of this service

767
00:34:39,090 --> 00:34:42,060
and very happy with with the uptake.

768
00:34:42,060 --> 00:34:47,060
And with that, wait, we'll
talk about S3 Metadata.

769
00:34:47,400 --> 00:34:48,720
S3 Metadata is another launch

770
00:34:48,720 --> 00:34:52,290
that we announced last year
in preview at re:Invent.

771
00:34:52,290 --> 00:34:55,320
And what it does is it
automatically generates metadata

772
00:34:55,320 --> 00:34:58,546
about the objects in a bucket.

773
00:34:58,546 --> 00:35:01,170
We announced preview
last year at re:Invent

774
00:35:01,170 --> 00:35:04,740
and in January we launched
what we call a journal table.

775
00:35:04,740 --> 00:35:07,817
What a journal table does
is it records records

776
00:35:07,817 --> 00:35:11,700
for all of the changes in S3 that you make

777
00:35:11,700 --> 00:35:15,600
to a given bucket automatically
down to an Iceberg table

778
00:35:15,600 --> 00:35:18,900
that you can query with standard SQL.

779
00:35:18,900 --> 00:35:23,700
So as mutations are flowing
in, puts, deletes changes

780
00:35:23,700 --> 00:35:28,700
to metadata, we're recording
those line by line in a table

781
00:35:28,710 --> 00:35:30,813
that you can go query.

782
00:35:31,680 --> 00:35:33,630
That was in January,

783
00:35:33,630 --> 00:35:36,390
and then we came back in
July with two updates.

784
00:35:36,390 --> 00:35:39,000
The first update to S3
Metadata was a price cut,

785
00:35:39,000 --> 00:35:41,220
where we reduced the price of

786
00:35:41,220 --> 00:35:45,240
that journal that we
just talked about by 33%.

787
00:35:45,240 --> 00:35:49,500
We also added this concept
called a live inventory table.

788
00:35:49,500 --> 00:35:53,070
This gives you a point in time
view of all of the contents

789
00:35:53,070 --> 00:35:57,060
of your bucket at any given point in time.

790
00:35:57,060 --> 00:36:01,200
Sort of a snapshot of the
metadata for all of your objects.

791
00:36:01,200 --> 00:36:04,020
How it works is that the journal rolls

792
00:36:04,020 --> 00:36:06,480
and it records all of the changes

793
00:36:06,480 --> 00:36:08,220
that you're making to your data.

794
00:36:08,220 --> 00:36:09,720
And every few hours we come in

795
00:36:09,720 --> 00:36:11,640
and do what we call a coalesce,

796
00:36:11,640 --> 00:36:15,300
where we generate this
inventory view of everything

797
00:36:15,300 --> 00:36:17,430
that exists in your bucket.

798
00:36:17,430 --> 00:36:19,650
And this is useful because
you can just jump onto

799
00:36:19,650 --> 00:36:22,440
a SQL prompt, write simple SQL statements

800
00:36:22,440 --> 00:36:24,750
to understand the contents of your bucket.

801
00:36:24,750 --> 00:36:26,850
For example, this is just a simple query

802
00:36:26,850 --> 00:36:29,610
that I wrote in Unified
Studio that allows me

803
00:36:29,610 --> 00:36:32,700
to quickly get after all
of the objects in my bucket

804
00:36:32,700 --> 00:36:36,180
that are greater than about
four megabytes in size.

805
00:36:36,180 --> 00:36:40,050
So it is easy to see how it's
useful to go troll through

806
00:36:40,050 --> 00:36:42,720
and find stuff out about
your data sets just

807
00:36:42,720 --> 00:36:45,600
from a SQL prompt based on tables

808
00:36:45,600 --> 00:36:47,650
that are generated for you automatically.

809
00:36:48,600 --> 00:36:52,950
So that is S3 metadata, a price cut,

810
00:36:52,950 --> 00:36:55,290
the journal and the inventory

811
00:36:55,290 --> 00:36:58,590
and now we're gonna swing back to the demo

812
00:36:58,590 --> 00:36:59,910
and check in in our uploads

813
00:36:59,910 --> 00:37:02,613
and show you the stuff in the flesh.

814
00:37:03,600 --> 00:37:06,330
- [Anna] All right, let's
get back to our job.

815
00:37:06,330 --> 00:37:09,300
It completed successfully in five seconds,

816
00:37:09,300 --> 00:37:12,120
which is fairly fast,
considering 700 objects

817
00:37:12,120 --> 00:37:14,400
that it needs to go through.

818
00:37:14,400 --> 00:37:17,970
It found 75 objects matching our criteria

819
00:37:17,970 --> 00:37:22,800
as it indeed put a
color black tag on that.

820
00:37:22,800 --> 00:37:25,320
And as we can also see the IAM role

821
00:37:25,320 --> 00:37:27,960
was automatically created for us.

822
00:37:27,960 --> 00:37:31,260
Now what I didn't mention
when I created my bucket,

823
00:37:31,260 --> 00:37:33,900
I also enabled metadata configuration.

824
00:37:33,900 --> 00:37:35,520
So now I have two tables,

825
00:37:35,520 --> 00:37:38,430
both journal table and
live inventory table

826
00:37:38,430 --> 00:37:42,660
and I can navigate to them
through S3 console as well.

827
00:37:42,660 --> 00:37:45,540
These tables store data in Iceberg format

828
00:37:45,540 --> 00:37:50,310
and we can use any Iceberg
compatible engine to query them.

829
00:37:50,310 --> 00:37:53,760
As Paul mentioned, there is
a single click experience

830
00:37:53,760 --> 00:37:57,423
to go straight to the Amazon
SageMaker Unified Studio.

831
00:37:58,410 --> 00:38:00,900
In the query editor that opens,

832
00:38:00,900 --> 00:38:04,410
we can just start exploring
our data right away.

833
00:38:04,410 --> 00:38:08,460
For example, we can count how many objects

834
00:38:08,460 --> 00:38:11,103
we created during this presentation.

835
00:38:13,200 --> 00:38:16,323
We do it by querying journal table.

836
00:38:17,670 --> 00:38:21,060
We know the answer. It should be 744.

837
00:38:21,060 --> 00:38:22,800
Yep, that's right.

838
00:38:22,800 --> 00:38:25,890
From this view, we can also
navigate to the data view

839
00:38:25,890 --> 00:38:29,820
where we can examine
the schema of our data

840
00:38:29,820 --> 00:38:31,890
and in one click we can also navigate

841
00:38:31,890 --> 00:38:33,990
to the notebook experience.

842
00:38:33,990 --> 00:38:37,431
In the notebook experience we
can use our natural language

843
00:38:37,431 --> 00:38:40,500
to start writing queries right away.

844
00:38:40,500 --> 00:38:41,510
For example...

845
00:38:46,050 --> 00:38:49,020
Let's create a distribution

846
00:38:49,020 --> 00:38:53,493
of updated keys per color tag value.

847
00:38:57,630 --> 00:39:00,420
It might take a while to
generate the proper query.

848
00:39:00,420 --> 00:39:02,903
It's a complex prompt
that we've given that.

849
00:39:02,903 --> 00:39:05,370
And with that, Paul, do you
still remember your SQL?

850
00:39:05,370 --> 00:39:06,570
- [Paul] Absolutely not.

851
00:39:08,730 --> 00:39:09,840
- [Anna] What will we do

852
00:39:09,840 --> 00:39:11,760
if gen AI doesn't generate it right?

853
00:39:11,760 --> 00:39:13,623
- [Paul] Gen AI is gonna work great.

854
00:39:16,260 --> 00:39:20,460
- [Anna] All right, we you got
our query, let's look at it.

855
00:39:20,460 --> 00:39:23,853
It indeed seems complex to
me. Let's give it a try.

856
00:39:31,440 --> 00:39:33,090
Okay. All right.

857
00:39:33,090 --> 00:39:35,130
As I mentioned, we have black

858
00:39:35,130 --> 00:39:37,950
and we have 75 objects tagged with black.

859
00:39:37,950 --> 00:39:40,170
That's the color Paul picked.

860
00:39:40,170 --> 00:39:42,630
The rest of the objects
are tagged with purple.

861
00:39:42,630 --> 00:39:46,110
This is the color I picked
ahead of this presentation.

862
00:39:46,110 --> 00:39:47,340
From this UI,

863
00:39:47,340 --> 00:39:49,200
we can also navigate to different views,

864
00:39:49,200 --> 00:39:52,950
for example, bar charts and pie charts.

865
00:39:52,950 --> 00:39:56,460
So as you explore your
data, give SageMaker a try.

866
00:39:56,460 --> 00:39:58,380
It is by far the easiest way

867
00:39:58,380 --> 00:40:01,263
to start getting insights from your data.

868
00:40:02,400 --> 00:40:06,060
- All right, so now I'm gonna talk

869
00:40:06,060 --> 00:40:08,760
a little bit about S3 Vectors.

870
00:40:08,760 --> 00:40:10,920
So I've been working on storage

871
00:40:10,920 --> 00:40:14,610
and storage accessories
for almost 20 years now.

872
00:40:14,610 --> 00:40:17,790
I'm really comfortable in the engine room.

873
00:40:17,790 --> 00:40:20,267
I gotta say though, that like
working on AI over the course

874
00:40:20,267 --> 00:40:24,150
of the last 12 to 18
months has been super fun

875
00:40:24,150 --> 00:40:25,710
just because we're all kind

876
00:40:25,710 --> 00:40:26,700
of learning about it together.

877
00:40:26,700 --> 00:40:30,185
The technology's moving
so fast and it's been fun

878
00:40:30,185 --> 00:40:32,940
and rewarding to kind of learn about it.

879
00:40:32,940 --> 00:40:36,030
I'll admit that 12 months
ago, 18 months ago,

880
00:40:36,030 --> 00:40:37,770
I would've been hard for me to explain

881
00:40:37,770 --> 00:40:40,350
what a vector is, let alone how to use it.

882
00:40:40,350 --> 00:40:42,510
And I bet there are people in this room

883
00:40:42,510 --> 00:40:43,980
who are there right now

884
00:40:43,980 --> 00:40:46,020
and don't quite understand
what a vector is.

885
00:40:46,020 --> 00:40:49,500
And so I'm gonna very quickly
explain what a vector is,

886
00:40:49,500 --> 00:40:52,050
what customers told us, sort
of why, what we were responding

887
00:40:52,050 --> 00:40:53,520
to when we launched this product,

888
00:40:53,520 --> 00:40:56,190
and then how this product
looks and how to use it.

889
00:40:56,190 --> 00:40:59,613
So that's my plan for the
next 10-ish minutes or so.

890
00:41:01,170 --> 00:41:03,630
So I have a list

891
00:41:03,630 --> 00:41:07,020
of data points in my pocket
right now, on my phone,

892
00:41:07,020 --> 00:41:09,600
which I'm sure all of you do too.

893
00:41:09,600 --> 00:41:13,290
And each one of these data
points has a machine readable

894
00:41:13,290 --> 00:41:16,650
name and a resolution or frame rate

895
00:41:16,650 --> 00:41:19,080
and number of kilobytes in storage

896
00:41:19,080 --> 00:41:21,273
and a bunch of other attributes like that.

897
00:41:22,140 --> 00:41:24,630
But if I were to describe
one of these images to you,

898
00:41:24,630 --> 00:41:26,850
I would not use those terms.

899
00:41:26,850 --> 00:41:29,760
For example, if I were
to describe this image,

900
00:41:29,760 --> 00:41:30,660
I would describe this as

901
00:41:30,660 --> 00:41:32,640
a funny looking dog sitting

902
00:41:32,640 --> 00:41:34,503
in front of chairs in an orange room.

903
00:41:35,550 --> 00:41:38,220
And what I'm communicating
to you is the meaning

904
00:41:38,220 --> 00:41:41,013
of the image and not its
physical characteristics.

905
00:41:42,180 --> 00:41:46,530
Now, computers know all about
the physical characteristics

906
00:41:46,530 --> 00:41:51,530
of our data, but they can't
directly calculate meaning.

907
00:41:52,110 --> 00:41:53,790
Only we can do that.

908
00:41:53,790 --> 00:41:56,097
And so what a vector tries to do,

909
00:41:56,097 --> 00:41:58,500
and it tries to bridge that gap

910
00:41:58,500 --> 00:42:01,016
because computers are
so good at processing

911
00:42:01,016 --> 00:42:03,240
and calculating numbers.

912
00:42:03,240 --> 00:42:06,180
What a vector is, it's simply a long list

913
00:42:06,180 --> 00:42:10,140
of numerical values
generated by a specific type

914
00:42:10,140 --> 00:42:13,500
of large language model
called an embedding model.

915
00:42:13,500 --> 00:42:17,610
And these values represent
all of the characteristics

916
00:42:17,610 --> 00:42:19,320
of that given piece of data.

917
00:42:19,320 --> 00:42:22,020
So is there a dog in the picture?

918
00:42:22,020 --> 00:42:23,580
Is there a chair in the picture?

919
00:42:23,580 --> 00:42:25,080
Is it indoors or outdoors?

920
00:42:25,080 --> 00:42:27,210
What color is the room?

921
00:42:27,210 --> 00:42:29,403
And thousands of other dimensions.

922
00:42:30,510 --> 00:42:34,500
And so we get this
mathematical approximation

923
00:42:34,500 --> 00:42:37,560
of meaning that becomes

924
00:42:37,560 --> 00:42:41,310
more and more closely resembling reality

925
00:42:41,310 --> 00:42:42,870
as the science progresses

926
00:42:42,870 --> 00:42:45,750
and the models become better and better.

927
00:42:45,750 --> 00:42:48,450
And this gives us some
interesting capabilities.

928
00:42:48,450 --> 00:42:51,270
This ability to mathematically quantify

929
00:42:51,270 --> 00:42:54,270
similarity and dissimilarity.

930
00:42:54,270 --> 00:42:55,530
Specifically what it allows us

931
00:42:55,530 --> 00:42:58,650
to do is it allows us to
go through large data sets

932
00:42:58,650 --> 00:43:03,650
that are otherwise unsorted and
arrange them into categories

933
00:43:04,290 --> 00:43:07,470
that are meaningful for humans.

934
00:43:07,470 --> 00:43:11,670
And we can do this by just
comparing the mathematical values

935
00:43:11,670 --> 00:43:12,810
that we derive from vectors

936
00:43:12,810 --> 00:43:15,780
between one piece of data and another.

937
00:43:15,780 --> 00:43:19,170
We can calculate the
distance, which represents

938
00:43:19,170 --> 00:43:21,000
the similarity or dissimilarity

939
00:43:21,000 --> 00:43:24,000
between two data points in a data set.

940
00:43:24,000 --> 00:43:26,700
We can also cluster data points together

941
00:43:26,700 --> 00:43:29,130
and these clusters represent data points

942
00:43:29,130 --> 00:43:31,953
that have a similar meaning.

943
00:43:33,090 --> 00:43:37,527
And now when we load all of
this up into a vector store,

944
00:43:37,527 --> 00:43:39,360
it becomes very interesting

945
00:43:39,360 --> 00:43:41,790
because now we can go build applications

946
00:43:41,790 --> 00:43:46,260
that very efficiently
calculate similarities

947
00:43:46,260 --> 00:43:49,860
and differences and do
groupings across thousands

948
00:43:49,860 --> 00:43:52,650
and thousands of different dimensions.

949
00:43:52,650 --> 00:43:55,560
And this is a very, very powerful concept

950
00:43:55,560 --> 00:43:57,990
and it's sort of the underlying
kind of driving force

951
00:43:57,990 --> 00:44:02,067
into when you see agents sort
of behaving very human-like.

952
00:44:02,067 --> 00:44:05,520
Now this sort of
multidimensional expression

953
00:44:05,520 --> 00:44:09,450
of relative meaning is
often expressed on slides

954
00:44:09,450 --> 00:44:11,430
and in three dimensions.

955
00:44:11,430 --> 00:44:13,290
But that's only because it's really hard

956
00:44:13,290 --> 00:44:17,130
to get 2000 dimensions
onto a PowerPoint slide.

957
00:44:17,130 --> 00:44:21,150
But this multidimensional
space that is kind of created

958
00:44:21,150 --> 00:44:24,780
by the math of many data
points that are vectorized,

959
00:44:24,780 --> 00:44:26,913
we refer to as vector space.

960
00:44:27,780 --> 00:44:31,740
And we can do interesting things
with vector space loaded up

961
00:44:31,740 --> 00:44:33,240
with a bunch of vectors.

962
00:44:33,240 --> 00:44:36,810
Specifically, if we
bring a new data point in

963
00:44:36,810 --> 00:44:38,850
and we don't know anything about it,

964
00:44:38,850 --> 00:44:43,230
we can learn a lot about
that data by determining

965
00:44:43,230 --> 00:44:45,270
which cluster it belongs into,

966
00:44:45,270 --> 00:44:47,820
like finding out what are its cohorts

967
00:44:47,820 --> 00:44:51,510
across various dimensions in vector space.

968
00:44:51,510 --> 00:44:54,720
To do that, all we need to
do is calculate a vector

969
00:44:54,720 --> 00:44:58,740
for this new piece of data
using the same embedding model

970
00:44:58,740 --> 00:45:01,230
that we use to create the vectors for all

971
00:45:01,230 --> 00:45:03,990
of the other data points
in our vector store.

972
00:45:03,990 --> 00:45:05,967
And we can then compare across

973
00:45:05,967 --> 00:45:09,360
all of those existing data points

974
00:45:09,360 --> 00:45:12,480
in order to find what
would be this new piece

975
00:45:12,480 --> 00:45:15,873
of data's nearest
neighbors and vector space.

976
00:45:17,010 --> 00:45:22,010
This ability to find and
identify similar bits of data

977
00:45:23,940 --> 00:45:26,940
in terms of meaning is
called semantic search.

978
00:45:26,940 --> 00:45:30,210
And this is the foundation of

979
00:45:30,210 --> 00:45:33,570
how all modern AI systems retrieve

980
00:45:33,570 --> 00:45:35,463
and interact with information.

981
00:45:36,510 --> 00:45:38,250
And so you can imagine over the course

982
00:45:38,250 --> 00:45:40,440
of the last 12 to 18 months,

983
00:45:40,440 --> 00:45:45,360
as many of you have been
exploring AI, running prototypes

984
00:45:45,360 --> 00:45:48,960
and taking AI-based
systems into production,

985
00:45:48,960 --> 00:45:50,370
you can imagine sort

986
00:45:50,370 --> 00:45:54,570
of the uptake in semantic search
deployment over that time.

987
00:45:54,570 --> 00:45:57,270
And when stuff like this happens

988
00:45:57,270 --> 00:46:00,093
customers come to us with
questions about storage.

989
00:46:00,960 --> 00:46:04,068
And really two big storage questions that

990
00:46:04,068 --> 00:46:07,538
we were fielding from customers

991
00:46:07,538 --> 00:46:10,800
on this new data set,
this new vector data set.

992
00:46:10,800 --> 00:46:14,640
The first is just what is
the durable, reliable storage

993
00:46:14,640 --> 00:46:16,640
that I should use for this new data set?

994
00:46:17,550 --> 00:46:19,290
What you're effectively
doing here is you're building

995
00:46:19,290 --> 00:46:22,530
a new metadata layer on
top of your data sets

996
00:46:22,530 --> 00:46:23,580
and it's gotta go somewhere.

997
00:46:23,580 --> 00:46:26,880
Your AI-driven applications depend on it

998
00:46:26,880 --> 00:46:29,493
and customers need that
reliable storage to host it.

999
00:46:30,510 --> 00:46:34,456
Now this is a storage
overhead as all metadata is,

1000
00:46:34,456 --> 00:46:37,170
and it's a small amount of overhead

1001
00:46:37,170 --> 00:46:39,270
for like image and video data,

1002
00:46:39,270 --> 00:46:43,050
but for text it can actually
be a lot of overhead.

1003
00:46:43,050 --> 00:46:45,840
The vector data for text
data sets can actually

1004
00:46:45,840 --> 00:46:48,837
be more storage than the
underlying text data.

1005
00:46:48,837 --> 00:46:50,640
And this is sort of very interesting

1006
00:46:50,640 --> 00:46:51,690
because it just goes to show just

1007
00:46:51,690 --> 00:46:55,290
how much meaning is packed
into human language.

1008
00:46:55,290 --> 00:46:57,741
So that's the first thing is
customers just were asking us,

1009
00:46:57,741 --> 00:47:00,330
what is the right sort of durable,

1010
00:47:00,330 --> 00:47:03,341
reliable storage for this new data set

1011
00:47:03,341 --> 00:47:06,060
that I need to power my applications?

1012
00:47:06,060 --> 00:47:07,860
The second set of questions really

1013
00:47:07,860 --> 00:47:09,510
around the price performance curve.

1014
00:47:09,510 --> 00:47:13,170
Now, this is a question that we work

1015
00:47:13,170 --> 00:47:16,680
with all the time for
all storage products.

1016
00:47:16,680 --> 00:47:18,679
This trade off between,

1017
00:47:18,679 --> 00:47:21,750
fast storage at a high price per gigabyte

1018
00:47:21,750 --> 00:47:24,990
versus slower storage at a
lower price per gigabyte.

1019
00:47:24,990 --> 00:47:26,100
It's why we have a range

1020
00:47:26,100 --> 00:47:30,420
of storage classes in S3 from
S3 Express on the fast side

1021
00:47:30,420 --> 00:47:33,597
to glacier deep archive
on the slower side.

1022
00:47:33,597 --> 00:47:37,860
This range of storage along
the price performance curve

1023
00:47:37,860 --> 00:47:41,050
is very helpful to match storage

1024
00:47:41,050 --> 00:47:43,740
to the appropriate
application requirements.

1025
00:47:43,740 --> 00:47:46,530
Now, most of the semantic search

1026
00:47:46,530 --> 00:47:48,570
that's been deployed over
the last several years

1027
00:47:48,570 --> 00:47:50,490
has really been for hybrid search.

1028
00:47:50,490 --> 00:47:52,170
So taking semantic search

1029
00:47:52,170 --> 00:47:54,420
and complimenting the keyword search

1030
00:47:54,420 --> 00:47:57,150
that we've been doing for a long time now.

1031
00:47:57,150 --> 00:47:59,340
And for this use case,

1032
00:47:59,340 --> 00:48:01,230
there's really a low latency requirement

1033
00:48:01,230 --> 00:48:03,210
for the underlying storage.

1034
00:48:03,210 --> 00:48:05,460
This is because a lot of
times in this use case,

1035
00:48:05,460 --> 00:48:08,850
it's a human sitting in
front of a search box.

1036
00:48:08,850 --> 00:48:10,800
And if a human is sitting between...

1037
00:48:10,800 --> 00:48:12,090
If your application is sitting

1038
00:48:12,090 --> 00:48:14,190
between a human and the buy button

1039
00:48:14,190 --> 00:48:17,220
you need to ring every
millisecond of latency out.

1040
00:48:17,220 --> 00:48:19,260
But as AI has proliferated to more

1041
00:48:19,260 --> 00:48:21,788
and more use cases, there's this universe

1042
00:48:21,788 --> 00:48:26,788
of use cases that are
happy to trade off latency

1043
00:48:27,480 --> 00:48:29,490
for a lower storage cost.

1044
00:48:29,490 --> 00:48:31,260
Just a few examples of that.

1045
00:48:31,260 --> 00:48:33,706
Retrieval, augmented generation or RAG

1046
00:48:33,706 --> 00:48:36,600
is a technique that you can use

1047
00:48:36,600 --> 00:48:40,770
to add knowledge that's
specific to your business,

1048
00:48:40,770 --> 00:48:42,330
to the knowledge that's trained

1049
00:48:42,330 --> 00:48:44,130
into the big large language models.

1050
00:48:44,130 --> 00:48:45,917
It's what allows us to go drive agents

1051
00:48:45,917 --> 00:48:50,670
that know about our own
specific data and workflows.

1052
00:48:50,670 --> 00:48:53,790
A lot of times these
workflows are asynchronous.

1053
00:48:53,790 --> 00:48:58,020
You can send an agent off
to do a multi-step process

1054
00:48:58,020 --> 00:49:01,470
and oftentimes across much,
a bunch of text as well,

1055
00:49:01,470 --> 00:49:02,910
which as we saw in the previous slide

1056
00:49:02,910 --> 00:49:04,650
can be more expensive.

1057
00:49:04,650 --> 00:49:07,890
And so in this case it's
easy to envision cases

1058
00:49:07,890 --> 00:49:10,200
where trading off a hundred milliseconds

1059
00:49:10,200 --> 00:49:13,170
or so for lower storage costs makes sense.

1060
00:49:13,170 --> 00:49:15,660
We also have a bunch of
customers in the video space

1061
00:49:15,660 --> 00:49:18,150
that wanna run semantic
search against video.

1062
00:49:18,150 --> 00:49:20,340
And the next step in workflow is

1063
00:49:20,340 --> 00:49:22,890
to go recall video data from Glacier,

1064
00:49:22,890 --> 00:49:25,050
which is a multi-hour process.

1065
00:49:25,050 --> 00:49:28,980
So for these use cases in
particular very willing

1066
00:49:28,980 --> 00:49:31,650
to trade off latency
for lower storage cost.

1067
00:49:31,650 --> 00:49:32,820
In fact, that's the whole point

1068
00:49:32,820 --> 00:49:34,650
of glacier in the first place.

1069
00:49:34,650 --> 00:49:39,030
So, basically what we found
is talking to customers

1070
00:49:39,030 --> 00:49:43,980
is that on one hand we have
this universe of applications,

1071
00:49:43,980 --> 00:49:45,570
this growing data sets that we're looking

1072
00:49:45,570 --> 00:49:49,140
for reliable storage to host them.

1073
00:49:49,140 --> 00:49:51,030
And in the other hand workload's willing

1074
00:49:51,030 --> 00:49:53,850
to trade off latency
for lower storage cost.

1075
00:49:53,850 --> 00:49:56,670
And that's why we launched S3 Vectors,

1076
00:49:56,670 --> 00:49:59,131
which is a native vector store in S3

1077
00:49:59,131 --> 00:50:04,131
that reduces the cost of
vector storage by up to 90%.

1078
00:50:04,230 --> 00:50:07,680
It gives you latency in the
a hundred millisecond range.

1079
00:50:07,680 --> 00:50:11,190
You can store up to 2
billion vectors per index up

1080
00:50:11,190 --> 00:50:14,862
to 10,000 indexes per vector bucket.

1081
00:50:14,862 --> 00:50:17,850
Now normally when I'm out here talking

1082
00:50:17,850 --> 00:50:19,200
about a big new feature like this,

1083
00:50:19,200 --> 00:50:21,960
I have a whole bunch of knobs
and dials to take you through,

1084
00:50:21,960 --> 00:50:26,430
but I just don't with this
product, it's very simple.

1085
00:50:26,430 --> 00:50:28,530
It has some APIs that you can use

1086
00:50:28,530 --> 00:50:32,010
to update and manage vector space.

1087
00:50:32,010 --> 00:50:34,920
You could almost predict
what those APIs are,

1088
00:50:34,920 --> 00:50:36,270
not even knowing what the product is.

1089
00:50:36,270 --> 00:50:37,530
Put, get, list, delete.

1090
00:50:37,530 --> 00:50:41,643
And then importantly a query
API to go query vector space.

1091
00:50:43,080 --> 00:50:44,880
What's a very important characteristic

1092
00:50:44,880 --> 00:50:49,200
of this service is that it
scales from absolute zero

1093
00:50:49,200 --> 00:50:51,420
and you only pay for what you use.

1094
00:50:51,420 --> 00:50:53,520
There's no pre-provisioning.

1095
00:50:53,520 --> 00:50:56,040
It works just like S3 in that way.

1096
00:50:56,040 --> 00:50:57,900
And what's really powerful about,

1097
00:50:57,900 --> 00:50:59,430
for this use case in particular

1098
00:50:59,430 --> 00:51:00,510
and what's been really awesome

1099
00:51:00,510 --> 00:51:02,400
for my team is this means you can

1100
00:51:02,400 --> 00:51:07,400
do tiny little AI prototypes
using this vector store

1101
00:51:07,645 --> 00:51:10,080
and it costs you very little,

1102
00:51:10,080 --> 00:51:11,130
'cause there's nothing to provision,

1103
00:51:11,130 --> 00:51:13,170
there's no purchasing decision to make.

1104
00:51:13,170 --> 00:51:14,400
But if one of those prototypes

1105
00:51:14,400 --> 00:51:17,100
or experiments takes
off, you're kind of okay

1106
00:51:17,100 --> 00:51:18,653
because the vector store scales

1107
00:51:18,653 --> 00:51:20,515
with the characteristics of S3,

1108
00:51:20,515 --> 00:51:23,340
scales elastically, has a
durability, reliability,

1109
00:51:23,340 --> 00:51:26,250
all of that stuff that you expect from us

1110
00:51:26,250 --> 00:51:29,460
when you place data on our storage.

1111
00:51:29,460 --> 00:51:30,600
So that's sort of generally what it is.

1112
00:51:30,600 --> 00:51:32,913
It's the vector bucket
with some simple APIs.

1113
00:51:34,230 --> 00:51:36,090
Really quick before I
hand it back to the demo.

1114
00:51:36,090 --> 00:51:40,710
Here's just a quick overview
on how to interact with it.

1115
00:51:40,710 --> 00:51:45,680
It's just based on a simple
70-line python example.

1116
00:51:45,680 --> 00:51:49,653
So you take an image to
insert into vector space,

1117
00:51:50,580 --> 00:51:55,170
invoke an embedding model via
bedrock to generate the vector

1118
00:51:55,170 --> 00:51:57,030
and then insert that
into the vector store.

1119
00:51:57,030 --> 00:51:59,010
It's really three simple steps

1120
00:51:59,010 --> 00:52:01,230
and as you can see, it's quite simple

1121
00:52:01,230 --> 00:52:05,430
to plan out in Python
to actually run a query.

1122
00:52:05,430 --> 00:52:06,690
It's actually very similar.

1123
00:52:06,690 --> 00:52:11,610
Same thing, start with an image,
invoke the embedding model

1124
00:52:11,610 --> 00:52:13,590
again to calculate that vector.

1125
00:52:13,590 --> 00:52:16,380
But instead of inserting
it, I'm using it as an input

1126
00:52:16,380 --> 00:52:20,820
to my query passing that,
that vector in as an input

1127
00:52:20,820 --> 00:52:21,840
and then that'll bring back

1128
00:52:21,840 --> 00:52:24,570
to me the most similar
results in vector space

1129
00:52:24,570 --> 00:52:26,610
to that image that I use to query.

1130
00:52:26,610 --> 00:52:29,040
So that's how you build
against this thing.

1131
00:52:29,040 --> 00:52:31,920
Just a quick example
of how to actually use

1132
00:52:31,920 --> 00:52:35,820
the APIs, but we actually
expect most customers

1133
00:52:35,820 --> 00:52:40,470
to take advantage of it
via other AWS services.

1134
00:52:40,470 --> 00:52:42,570
It's already integrated with OpenSearch

1135
00:52:42,570 --> 00:52:45,720
where you can basically
use it as a cold tier

1136
00:52:45,720 --> 00:52:47,610
behind your hybrid search implementation.

1137
00:52:47,610 --> 00:52:49,860
So helping to reduce the cost

1138
00:52:49,860 --> 00:52:51,420
of those hybrid search use cases

1139
00:52:51,420 --> 00:52:53,640
that we showed on the left hand side

1140
00:52:53,640 --> 00:52:55,320
of the price performance curve.

1141
00:52:55,320 --> 00:52:57,150
It also, you can click through

1142
00:52:57,150 --> 00:52:59,880
and make it an option, use
it as an option in Bedrock

1143
00:52:59,880 --> 00:53:01,410
to create a knowledge base

1144
00:53:01,410 --> 00:53:03,540
which allows you to do
RAG and standup agents

1145
00:53:03,540 --> 00:53:06,630
and all of that good
stuff with your own data.

1146
00:53:06,630 --> 00:53:09,780
This is what's been really
impactful for my team

1147
00:53:09,780 --> 00:53:12,030
and I dunno, I could
talk about it all day,

1148
00:53:12,030 --> 00:53:14,250
but we have only a few minutes
left so we'll pass that back

1149
00:53:14,250 --> 00:53:17,703
to Anna to actually show what
this looks like, the demo.

1150
00:53:19,950 --> 00:53:22,350
- [Anna] All right, so
here's the challenge.

1151
00:53:22,350 --> 00:53:25,080
We have 744 objects,

1152
00:53:25,080 --> 00:53:29,160
but only one of them has
transcript of today's session.

1153
00:53:29,160 --> 00:53:32,520
So how do we search this
corpus of data semantically?

1154
00:53:32,520 --> 00:53:33,353
To assist with that,

1155
00:53:33,353 --> 00:53:36,450
we will be using Amazon
Bedrock knowledge base

1156
00:53:36,450 --> 00:53:40,170
plus S3 Vectors and QCLI agent.

1157
00:53:40,170 --> 00:53:41,343
For the knowledge base,

1158
00:53:42,210 --> 00:53:45,060
I created it earlier
during the presentation

1159
00:53:45,060 --> 00:53:47,940
as it takes quite some time to sync it

1160
00:53:47,940 --> 00:53:51,660
with a storage with my original data.

1161
00:53:51,660 --> 00:53:55,860
And I used Amazon Nova
Multimodel embeddings model

1162
00:53:55,860 --> 00:53:59,400
to create vector
embeddings out of my data.

1163
00:53:59,400 --> 00:54:04,050
And I'm using Amazon S3
Vectors to store those vectors.

1164
00:54:04,050 --> 00:54:06,580
Now let's quickly hop

1165
00:54:07,710 --> 00:54:09,723
into QCLI setup.

1166
00:54:13,620 --> 00:54:16,590
On QCLI when I started up,

1167
00:54:16,590 --> 00:54:21,390
it also comes with Bedrock
knowledge base MCP server.

1168
00:54:21,390 --> 00:54:24,750
I installed this server
from AWS Labs GitHub

1169
00:54:24,750 --> 00:54:26,190
and it's very easy to configure.

1170
00:54:26,190 --> 00:54:27,270
You basically just need

1171
00:54:27,270 --> 00:54:30,990
to provide your AWS
credentials and region.

1172
00:54:30,990 --> 00:54:33,300
This MCP server simplifies interactions

1173
00:54:33,300 --> 00:54:37,650
between agent and the
knowledge base in Bedrock.

1174
00:54:37,650 --> 00:54:42,650
Now I also created a
context file for my agent.

1175
00:54:42,690 --> 00:54:46,140
In this context file I give
some directives to my agent.

1176
00:54:46,140 --> 00:54:49,290
For example, for every question my agent

1177
00:54:49,290 --> 00:54:52,380
should first consult Amazon
Bedrock knowledge base.

1178
00:54:52,380 --> 00:54:55,560
It should also ask for
the knowledge base id

1179
00:54:55,560 --> 00:54:57,690
and I also give some hints to my agent

1180
00:54:57,690 --> 00:55:00,033
how to query this
knowledge base effectively.

1181
00:55:08,160 --> 00:55:12,203
I need to add this
context to my agent first.

1182
00:55:16,320 --> 00:55:20,970
And after I'm done with that,
I have a question of the day.

1183
00:55:20,970 --> 00:55:24,430
What are the names of two volunteers

1184
00:55:25,500 --> 00:55:30,413
at re:Invent in re:Invent session today?

1185
00:55:37,830 --> 00:55:40,410
My agent is asking about
my knowledge base id

1186
00:55:40,410 --> 00:55:42,243
as I instructed it previously.

1187
00:55:43,170 --> 00:55:44,293
Let's copy it.

1188
00:55:48,840 --> 00:55:50,440
Oh yeah, that was the wrong one.

1189
00:55:51,300 --> 00:55:53,613
Okay, here's the knowledge base id.

1190
00:55:55,020 --> 00:55:58,890
Now the agent tells me
that that's the query.

1191
00:55:58,890 --> 00:56:00,120
It's going to run.

1192
00:56:00,120 --> 00:56:02,643
This query looks good to me, so I approve.

1193
00:56:05,850 --> 00:56:07,825
Paul, what do you think?

1194
00:56:07,825 --> 00:56:09,780
- [Paul] It has to work at this point.

1195
00:56:09,780 --> 00:56:13,833
- Yay. Congratulations
Matt and Indra Neil.

1196
00:56:15,330 --> 00:56:16,898
Thanks for volunteering.

1197
00:56:16,898 --> 00:56:20,065
(audience applauding)

1198
00:56:21,180 --> 00:56:25,020
- All right, that's what we have for you.

1199
00:56:25,020 --> 00:56:29,970
35 launches in 57 minutes.

1200
00:56:29,970 --> 00:56:33,990
We really appreciate
you taking an hour out

1201
00:56:33,990 --> 00:56:36,423
of your re:Invent to spend with us.

1202
00:56:37,470 --> 00:56:39,630
Thank you for entrusting
us with your data.

1203
00:56:39,630 --> 00:56:40,710
It's a responsibility

1204
00:56:40,710 --> 00:56:43,470
that we take very
seriously on the S3 team.

1205
00:56:43,470 --> 00:56:48,470
And please rate the
session, provide feedback.

1206
00:56:48,690 --> 00:56:50,160
We read every single comment.

1207
00:56:50,160 --> 00:56:52,080
It really helps us to determine

1208
00:56:52,080 --> 00:56:54,210
what to bring to you every single year.

1209
00:56:54,210 --> 00:56:57,150
And so encourage you all to hit the app

1210
00:56:57,150 --> 00:56:59,310
and give us a rating.

1211
00:56:59,310 --> 00:57:00,248
Thanks a lot.

1212
00:57:00,248 --> 00:57:02,190
(audience applauding)

