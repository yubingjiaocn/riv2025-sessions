# AWS re:Invent 2025 - Tranium 3 技术分享会总结

## 会议概述

本次技术分享会由AWS Annapurna Labs副总裁Colin Brace主持，重点介绍了AWS最新一代机器学习芯片Tranium 3的创新技术。会议深入探讨了2026年生成式AI的发展趋势，特别是Agentic AI（智能代理）和领域专用模型的兴起。Tranium 3正是为应对这些新趋势而设计，旨在提供高性能、低延迟、成本效益高且易于访问的AI加速器。

会议还邀请了两家合作伙伴公司Poolside和Dart分享他们在Tranium平台上的实际应用经验。Poolside的Joe Ro详细介绍了他们如何利用Tranium进行大规模推理工作，强调了推理在基础模型开发中的核心地位。会议最后展示了AWS与PyTorch基金会的深度合作，宣布Tranium 3实现了原生PyTorch支持，使开发者能够无缝迁移现有代码。

整个分享会展现了AWS在硅片创新、系统集成和软件生态方面的端到端能力，以及Tranium 3如何通过4.4倍的计算性能提升和3.9倍的内存带宽提升，为即将到来的智能代理时代提供强大的基础设施支持。

## 详细时间线

### 开场与背景介绍
- **00:00:00** - Colin Brace开场，介绍会议主题：Tranium 3的创新技术及其对客户的意义
- **00:00:30** - 会议议程预告：将讨论生成式AI行业趋势、Tranium 3如何满足需求，以及Poolside和Dart两家客户的应用案例

### 2026年AI发展趋势
- **00:01:00** - 第一大趋势：Agentic AI（智能代理）的崛起，不仅能生成响应，还能验证和执行操作
- **00:01:30** - Gartner预测：到2026年底，40%的应用将使用智能代理（目前仅5%）
- **00:02:00** - 以编码领域为例：从2022年的代码补全发展到聊天编码、Vibe编码，现在进入智能代理和代理群时代
- **00:02:30** - 第二大趋势：应用或领域专用模型的兴起，训练成本降低使定制化模型成为可能
- **00:03:00** - 举例：法律领域的LexisNexis、金融领域的Bloomberg、机器人公司都在开发专用模型

### 新趋势带来的技术需求
- **00:03:30** - 四大核心需求：高性能token生成、低延迟、易于访问且成本效益高的加速器、可扩展性
- **00:04:00** - 高性能token生成：智能代理和长推理模型需要比传统聊天应用高出数个数量级的token生成能力
- **00:04:30** - 低延迟：对于人机交互和多代理协作至关重要
- **00:05:00** - 这些需求最终驱动了Tranium 3的设计，专为智能代理工作流时代打造

### AWS硅片创新历史
- **00:05:30** - AWS Annapurna Labs在硅片创新领域已有超过10年历史
- **00:06:00** - 2013年开始研发第一款硅片Nitro卡，通过分析EC2网络和存储模式实现性能和成本优化
- **00:06:30** - Nitro现已发展到第七代，每台EC2服务器都配备Nitro卡
- **00:07:00** - 基于ARM架构的Graviton CPU，专为AWS常见工作负载优化，现已推出第五代
- **00:07:30** - 2016年末开始研发机器学习芯片，针对MXNet和ResNet模型进行优化
- **00:08:00** - 2019年推出第一款芯片Inferentia，专为当时的BERT模型和CNN设计

### Tranium 2的成功经验
- **00:08:30** - 介绍Tranium 2的规格和性能，能够在单设备上运行Llama、Qwen和GPT-2等模型
- **00:09:00** - Tranium 2服务器包含64个芯片，通过NeuronLink实现快速互连
- **00:09:30** - 采用Elastic Fabric Adapter（EFA）实现大规模扩展
- **00:10:00** - Project Rainier目标：建设数十万训练芯片的全球最大AI计算集群，规模是当时最大训练集群的5倍
- **00:10:30** - Tranium 2部署速度创纪录，数据中心容量是前代GenAI实例的3倍以上
- **00:11:00** - 部署速度提升4倍，涵盖从供应链到组装的全流程
- **00:11:30** - 成功完成Project Rainier：在一年内将印第安纳州的玉米地改造为拥有50万个Tranium 2芯片的全球最大AI计算集群

### Tranium 2性能数据
- **00:12:00** - 与Anthropic合作的性能测试结果
- **00:12:30** - 在多种工作负载下（100 token输入、100,000 token输入、视频生成），平均输出token速度提升1.4倍
- **00:13:00** - 这些性能支撑了Claude等应用的快速运行

### Tranium 3发布
- **00:13:30** - 正式介绍Tranium 3，基于Tranium 2的经验并面向Agentic AI趋势设计
- **00:14:00** - 芯片级规格：更多计算能力、更大内存、更高内存带宽，解决机器学习工作负载的瓶颈
- **00:14:30** - 服务器级创新：将多个Tranium芯片聚合为单一系统视图
- **00:15:00** - Ultra Server整体性能：计算能力提升4.4倍，内存带宽提升3.9倍（相比Tranium 2服务器）

### Tranium 3三大创新
- **00:15:30** - 第一大创新：端到端硬件创新，整合Nitro、Graviton和Tranium三大产品线
- **00:16:00** - 这种整合使AWS能够控制功耗、组装流程和供应链，加快交付速度
- **00:16:30** - 支持风冷和液冷两种部署方式，确保尽快将容量交付给客户
- **00:17:00** - 第二大创新：高性能数据类型，利用软件发现的低精度推理优势
- **00:17:30** - 在推理时使用较小数据类型可实现至少2倍的计算和内存带宽提升
- **00:18:00** - Tranium 3实现无缝的精度转换和量化，无性能损失
- **00:18:30** - GPT-2 120GB模型测试显示2倍性能提升
- **00:19:00** - 第三大创新：Neuron Switch，改进芯片间互连技术
- **00:19:30** - 相比之前的Torus拓扑结构，Neuron Switch实现更直接的一对一通信
- **00:20:00** - All-gather原语延迟降低最高6倍，All-reduce延迟降低最高2倍

### Tranium 3实际性能表现
- **00:20:30** - 重点指标：每兆瓦token数（tokens per megawatt），决定运营成本和数据中心建设速度
- **00:21:00** - Tranium 3实现每单位功耗5倍的token生成量
- **00:21:30** - 开发者视角的性能选项：在交互性和服务器吞吐量之间灵活调整
- **00:22:00** - 选项一：保持相同交互性，服务用户数提升4.5倍
- **00:22:30** - 选项二：服务相同用户数，输出token速度提升6倍以上
- **00:23:00** - Tranium 3为开发者提供了在总token吞吐量和交互性之间灵活选择的能力

### Poolside客户案例
- **00:23:30** - Poolside的Joe Ro上台分享应用经验
- **00:24:00** - Poolside介绍：从零开始训练基础模型的公司，类似OpenAI或Anthropic
- **00:24:30** - 关键洞察：在基础模型开发中，大部分计算时间花在推理上，而非预训练
- **00:25:00** - 推理用于两个主要场景：客户推理和训练工作负载中的推理（强化学习、合成数据生成、评估）
- **00:25:30** - 推理是Poolside的首要优化目标，1%的优化都会带来巨大的成本和时间节省
- **00:26:00** - 推理需求具有高度弹性：有更多推理能力就会使用更多，或者以更低成本/更快速度完成
- **00:26:30** - 两种推理工作负载：内部工作负载（注重吞吐量）和外部工作负载（注重延迟）
- **00:27:00** - 类比：个人出行需要跑车（低延迟），大规模运输需要卡车或飞机（高吞吐量）
- **00:27:30** - 现代推理工作负载日益复杂：从几年前的4K上下文窗口到现在的百万token上下文
- **00:28:00** - 强调灵活性的重要性：硬件假设很快会过时，行业发展太快
- **00:28:30** - 成本效益：推理越便宜，使用越多，会发现更多应用场景

### Poolside与AWS的合作
- **00:29:00** - Poolside与AWS Annapurna Labs合作约2.5年
- **00:29:30** - 双方在软硬件栈上进行了大量合作，包括反馈、模型设计、硬件优化
- **00:30:00** - 建议：强烈推荐与AWS团队合作以充分发挥Tranium平台潜力
- **00:30:30** - Poolside的技术实现：使用PyTorch，Neuron提供追踪编译器生成XLA图并编译为优化内核
- **00:31:00** - 每周都能看到所需修改越来越少，平台易用性持续提升
- **00:31:30** - Nki（领域特定语言）：用于Tranium平台的底层硬件优化，类似CUDA的DSL
- **00:32:00** - 通常用Nki实现最后10%的性能优化，从"足够好"的PyTorch代码迭代到极致性能
- **00:32:30** - 强调AWS Annapurna Labs团队是最佳合作伙伴
- **00:33:00** - 推理不仅是计算加速器问题，还涉及检查点加载、自动扩展、负载均衡等系统问题
- **00:33:30** - AWS在构建优秀系统方面经验丰富，总能提供解决方案

### Tranium的经济效益
- **00:34:00** - Tranium的token经济学极具吸引力
- **00:34:30** - Poolside在实践中验证了所有承诺的性能数字，包括5倍提升和每瓦性能
- **00:35:00** - 每次在新一代Tranium硬件上部署模型，都能看到承诺的性能表现
- **00:35:30** - Tranium几乎总能兑现承诺的性能数字，不是空头支票
- **00:36:00** - 对Tranium 3和软件栈的新变化充满期待，期待继续合作

### 软件开发者栈
- **00:36:30** - Colin回到台上，开始介绍开发者软件栈
- **00:37:00** - 核心理念：如果你懂Jax或PyTorch，就已经知道如何使用Tranium
- **00:37:30** - 三类目标客户：机器学习开发者、机器学习研究人员、系统优化专家
- **00:38:00** - 机器学习开发者：需要快速部署推理或训练系统，依赖vLLM或Hugging Face等框架
- **00:38:30** - 深度集成vLLM、Transformers库、Hugging Face、Ray等框架
- **00:39:00** - 与AWS服务深度集成：EKS、Amazon ECS等
- **00:39:30** - 机器学习研究人员：创建新模型、尝试新数据集，需要直接的PyTorch或框架级访问
- **00:40:00** - 重大宣布：推出PyTorch Native支持

### PyTorch原生支持
- **00:40:30** - 播放PyTorch Foundation的视频消息
- **00:41:00** - Meta的Joanna Van Grunen（PyTorch工程总监）介绍PyTorch愿景：PyTorch作为AI的开放语言
- **00:41:30** - 核心理念：PyTorch应该在任何地方运行，相同代码可在任何硬件平台上使用
- **00:42:00** - AWS Tranium实现原生PyTorch支持：现有PyTorch代码直接可用
- **00:42:30** - 默认支持eager执行，提供直观的开发者体验
- **00:43:00** - 分布式训练API无需修改，torch.compile自动优化模型
- **00:43:30** - Torch Titan等库可直接运行，支持生产规模训练
- **00:44:00** - vLLM作为PyTorch Foundation最新成员，在Tranium上得到支持

### Agentic AI的未来
- **00:44:30** - Meta的Joe Spzac（PyTorch产品总监、PyTorch Foundation董事会成员）发言
- **00:45:00** - 推理在AI基础转变中变得至关重要，特别是在预训练和后训练的强化学习中
- **00:45:30** - 现在将环境和真实世界场景纳入模型能力提升
- **00:46:00** - 智能代理系统的发展：从传统的提示-响应模式转向能够执行操作、运行代码、使用工具的代理
- **00:46:30** - 代理能够与真实环境交互并实现真实任务和结果
- **00:47:00** - 应用场景：从系统代码执行、数学计算到生成推理和训练优化内核
- **00:47:30** - 2025 PyTorch大会发布全栈解决方案：从底层内核编写到集群规模计算编排
- **00:48:00** - 介绍Monarch项目（集群规模计算）和Torch Forge项目（强化学习层）
- **00:48:30** - 现在有标准接口将任务和环境世界引入后训练和预训练流程
- **00:49:00** - 这是下一代技术栈，将推动下一波创新浪潮
- **00:49:30** - 对与AWS团队合作将这些能力带到Tranium平台感到兴奋
- **00:50:00** - Tranium在创新和提供替代计算架构方面有悠久历史，原生PyTorch支持令人振奋

### 总结
- **00:50:30** - Joanna总结：Agentic工作流代表AI的发展方向，正确的基础设施至关重要
- **00:51:00** - 这正是生态系统协作的愿景：硬件、软件和社区共同推动AI民主化
- **00:51:30** - Colin感谢PyTorch团队的合作和支持
- **00:52:00** - 会议结束，感谢观众参与