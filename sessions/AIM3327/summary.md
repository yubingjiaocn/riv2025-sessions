# AWS re:Invent 2025 - Amazon Bedrock 强化微调功能发布会总结

## 会议概述

本次会议介绍了 Amazon Bedrock 最新推出的强化微调（Reinforcement Fine-Tuning, RFT）功能，首先支持 Nova Lite 2 模型，未来将支持更多模型。会议由 Amazon Bedrock 产品负责人 Chelandra、AWS 首席数据科学家 Shreas 以及 Salesforce Agent Force 团队工程副总裁 Phil 共同主讲。

强化微调技术旨在让所有开发者都能轻松使用，无需深厚的机器学习专业知识。与传统的监督微调（SFT）相比，RFT 能够用更少的高质量标注数据集实现更好的模型性能。传统 SFT 需要大量标注数据、成本高昂且耗时，而 RFT 通过探索多个解决方案、评分并使用反馈驱动的学习循环，让模型基于奖励信号持续迭代改进。

会议展示了完整的使用流程：用户只需三步即可开始——提供数据、定义奖励函数（通过开箱即用的模板或自定义 Lambda 函数）、启动训练。训练完成后，可以通过按需推理部署微调模型，采用按 token 计费的方式。Salesforce 分享了他们使用 RFT 开发的 TaxEval 模型案例，该模型在指令遵循和任务完成方面的准确率分别达到 97% 和 95%，超越了 GPT-4 等前沿模型，同时成本不到其 10%。

## 详细时间线

### 开场介绍
[00:00:00 - 00:01:30]
- 欢迎致辞，介绍会议主题：Amazon Bedrock 上的强化微调功能，支持 Nova Lite 2 模型
- 介绍演讲嘉宾：Chelandra（Amazon Bedrock 产品负责人）、Shreas（AWS 首席数据科学家）、Phil（Salesforce Agent Force 团队工程副总裁）

### 微调基础概念
[00:01:30 - 00:03:00]
- 解释什么是微调：调整现有基础模型以适应特定用例，例如学习公司的语气、风格
- 说明为什么需要微调：基础模型虽然知识广泛，但缺乏特定领域的细节、语气和风格

### 传统监督微调（SFT）的局限性
[00:03:00 - 00:05:30]
- 介绍传统 SFT：通过输入输出示例训练模型
- SFT 的挑战：
  - 数据饥渴：需要大量高质量标注数据
  - 成本高昂且耗时
  - 模型僵化：可能记忆但不适应
  - 模型漂移：性能随时间下降
  - 不擅长偏好学习和人类偏好对齐

### 客户面临的实际挑战
[00:05:30 - 00:06:30]
- 客户反馈的问题：
  - "我们只有 100 个示例，但需要生产级质量的模型"
  - "模型在小数据集测试中表现良好，但在生产环境中出现漂移"
  - "我们需要带推理的结构化输出，但无法标注数千个高质量数据集"

### 强化微调（RFT）介绍
[00:06:30 - 00:09:00]
- 提出 RFT 的优势：从少量示例学习、自动探索数千个解决方案、使用最佳方案改进、从生产数据持续改进
- RFT 工作原理：探索多个解决方案、基于奖励函数评分、使用反馈驱动的学习循环
- 客户服务示例：展示模型如何探索不同响应（简短回复 vs 同理心回复 vs 流程化回复）并根据奖励函数选择最佳方案

### Bedrock 中的 RFT 使用流程
[00:09:00 - 00:12:00]
- 三个简单步骤：
  1. 从多个来源提供数据（本地上传、S3 文件、模型调用日志）
  2. 通过开箱即用的模板定义奖励函数，或使用 Lambda 自定义
  3. 开始训练，训练完成后使用按需推理部署
- 控制台演示：展示如何在 Bedrock 控制台创建强化微调任务

### 奖励函数选项
[00:12:00 - 00:13:30]
- 选项一：模型作为评判者（Model as Judge），提供开箱即用的模板（指令遵循、摘要、推理链、RAG 问答、忠实度等）
- 选项二：通过自定义 Lambda 函数提供可验证的奖励
- 训练指标可视化：训练奖励、验证奖励、训练和验证的 episode 长度

### 定价模型
[00:13:30 - 00:14:00]
- 按需推理：基于处理的 token 数量计费
- Nova Lite 2 的 RFT 微调模型推理价格与基础模型相同
- 训练费用：按小时计费

### 实际演示
[00:14:00 - 00:20:00]
- Shreas 演示完整流程：
  - 选择 Nova 2 Light 模型进行微调
  - 上传 JSON L 格式数据集
  - 展示两个用例：FinQA（金融问答数据集）和 Chapsa（多语言情感分析数据集）
  - 创建 Lambda 奖励函数（自定义代码或预设模板）
  - 展示模型作为评判者的选项（指令遵循、摘要、推理评估、RAG 忠实度）
  - 设置超参数：epoch 数量、学习率、每个提示的训练样本数、评估间隔
  - 查看训练指标：训练和验证奖励曲线上升，表明模型正在学习

### 推理部署演示
[00:20:00 - 00:22:00]
- 设置按需推理，无需配置预置吞吐量
- 在 Playground 中测试微调模型
- 使用金融问答示例测试，3 秒内获得完整答案
- 对比模式：将微调模型与基础模型 Nova 2.0 Light 并排比较

### Salesforce 客户案例
[00:22:00 - 00:30:00]
- Phil 介绍 Salesforce Agent Force 平台背景
- 历史背景：Deep Seek V3 论文（2024年12月）和 R1 论文（2025年1月）引入的技术
- Agent Force 360：企业 AI 系统，服务于前端客户应用和后端运营
- 使用场景：需要高准确率、低延迟、高解释力的模型
- TaxEval 模型：Salesforce 内部的 LLM 评判模型
  - 每月服务约 1300 万次查询
  - 应用于护栏、指令遵循、任务解决、引用生成、响应质量评估

### TaxEval 性能数据
[00:30:00 - 00:32:00]
- 基于 GPT-OSS 20B 基础模型微调
- 性能对比（F1 分数）：
  - GPT-4（基准）：指令遵循 88%，任务完成 83%
  - GPT-OSS 20B（基础模型）：指令遵循 64%，任务完成 79%
  - 使用 SFT：指令遵循 93%，任务完成 93%
  - 仅使用 RFT：指令遵循 95%，任务完成 79%
  - **SFT + RFT 混合**：指令遵循 97%，任务完成 95%
- 成本不到 GPT-4 的 10%，性能更优

### Agent Graph 和未来展望
[00:32:00 - 00:34:00]
- Salesforce 推出 Agent Graph：将复杂推理外部化为专业代理图
- 通过有限状态机（FSM）控制状态转换，实现确定性
- 每个子专业代理可以使用 RFT 语言模型作为推理器
- RFT 技术可能影响企业代理系统的每个组件

### 总结和 Bedrock 创新
[00:34:00 - 00:36:00]
- 使用 RFT 的三大理由：
  1. 易用性：端到端自动化，无需深厚 ML 专业知识
  2. 更好的模型性能：相比基础模型平均提升 60-70%
  3. 训练指标可见性：了解训练过程，训练完成后使用按需推理部署
- Bedrock 持续创新：
  - 提供最新最好的基础模型
  - 加倍投入模型定制（模型蒸馏、按需推理）
  - 大力投资代理 AI（Agent Core、评估和策略功能）
  - 持续推出 Bedrock Guardrails 新功能（支持编码相关用例）
- 致谢结束