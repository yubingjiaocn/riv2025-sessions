# AWS re:Invent 2025 - NVIDIA DGX Cloud 与 AI 工厂：ServiceNow 和 SLB 案例分享

## 会议概述

本次技术分享会聚焦于 NVIDIA DGX Cloud 在 AWS 上的应用实践，由 NVIDIA 主持，邀请了 ServiceNow 的首席科学家 Sathwik 和能源技术公司 SLB 的业务线总监 Jamie 分享他们的实际使用经验。会议深入探讨了 AI 工厂概念、NVIDIA-AWS 合作伙伴关系，以及两家企业如何利用 DGX Cloud 加速 AI 模型开发和部署。

会议重点介绍了 AI 发展的三个扩展定律：预训练扩展、后训练扩展和测试时扩展，强调了随着模型智能水平提升对计算资源需求的增长。NVIDIA DGX Cloud 作为完整的 AI 工厂解决方案，整合了基础设施、优化的 EKS、Run:ai GPU 编排、企业级软件和专家支持，为客户提供了最佳的 NVIDIA AI 和优化全栈平台。

## 详细时间线与关键要点

### 0:00-5:00 开场介绍与市场概况
- 主持人介绍会议议程和嘉宾背景
- 强调 AI 采用的普及性，现场大多数参与者已在开发或实施 AI
- 提出 AI 工厂概念，将其比作新的工业革命
- 介绍 AI 发展的三个扩展定律：预训练、后训练和测试时扩展

### 5:00-10:00 NVIDIA-AWS 合作伙伴关系
- 详细介绍 NVIDIA 全栈解决方案，不仅仅是 GPU 硬件
- 展示与 AWS 的深度集成：CUDA 库、AI Enterprise、Omniverse、NIM 等
- 介绍 DGX Cloud 的核心组件：GB200 基础设施、高性能网络、Lustre 存储
- 强调 Run:ai GPU 编排的重要性，帮助客户最大化 GPU 利用率

### 10:00-15:00 嘉宾介绍与业务背景
- Sathwik（ServiceNow）：领导基础模型实验室，专注于中等规模语言模型的训练
- Jamie（SLB）：数字化部门业务线总监，为能源行业提供 AI 平台和应用
- ServiceNow 专注于企业流程自动化，从传统工作流向智能代理应用转变
- SLB 在全球 120 多个国家运营，涵盖传统能源和新能源业务

### 15:00-20:00 AI 开发目标与挑战
- ServiceNow 目标：构建高效的基础模型，在单 GPU 规模上实现前沿级推理性能
- 成本控制是关键考虑因素，避免为所有用例使用最昂贵的模型
- SLB 目标：处理地震数据、地球物理数据，构建地下结构模型
- 开发领域特定的基础模型，用于地震和时间序列数据的合成响应生成

### 20:00-25:00 DGX Cloud 使用体验
- Sathwik 分享从自建硬件转向 DGX Cloud 的经验
- 关键优势：更高可靠性、硬件升级灵活性、接近零停机时间
- Run:ai 帮助实现近 100% 的集群利用率
- Jamie 强调 DGX Cloud 加速了产品上市时间，无需担心基础设施管理

### 25:00-30:00 模型开发成果展示
- ServiceNow Apriel 模型系列：5B、15B 参数规模的开源模型
- 15B 参数模型性能可媲美 600B 参数的 DeepSeek R1
- SLB 开发多模态模型，支持文本到图像、图像到文本的转换
- 两家公司都采用开源策略，增加社区认知度

### 30:00-35:00 客户部署策略与未来规划
- 讨论微调 vs RAG 的应用场景选择
- ServiceNow 主要通过代理编排和自定义策略满足客户需求
- SLB 针对不同数据类型采用不同策略：文档使用 RAG，科学数据使用微调
- 未来部署将结合云端和本地部署，满足不同客户的数据敏感性需求

### 35:00-38:24 总结与展望
- 强调 NVIDIA 和 AWS 合作的价值，提供可移植的全栈生态系统
- SLB 分享从 2D 到 3D 模型的发展历程
- 两家公司都认为目前仍处于 AI 革命的早期阶段
- 会议结束，邀请参与者进行后续交流