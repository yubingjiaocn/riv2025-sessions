# AWS re:Invent 2025 - NVIDIA DGX Cloud 技术会议总结

## 会议概述

本次AWS re:Invent技术会议重点介绍了NVIDIA DGX Cloud在AWS上的应用，以及两家企业客户ServiceNow和SLB如何利用这一平台加速AI开发。会议由NVIDIA代表主持，邀请了ServiceNow的首席科学家Satwick和SLB的业务线总监Jamie分享他们的实践经验。

会议强调了AI正在推动新一轮工业革命，AI工厂的概念正在云端实现。随着AI模型智能水平的提升，从预训练扩展到后训练，再到测试时推理扩展，对计算资源的需求也在不断增长。NVIDIA与AWS的深度合作为客户提供了从基础设施到软件的全栈优化解决方案，包括CUDA库、DGX Hub、NVIDIA AI Enterprise、Omniverse和NIM推理微服务等。

DGX Cloud是NVIDIA与AWS共同设计的企业级AI平台，整合了GB200等最新GPU、高性能网络、Lustre存储、优化的EKS、RunAI GPU编排工具以及企业级软件支持。两位嘉宾分享了他们如何利用这一平台实现业务目标：ServiceNow专注于训练高效的小型语言模型以支持企业流程自动化，而SLB则开发领域特定的基础模型用于地震数据分析和地下结构预测。

## 详细时间线与关键要点

### 开场与背景介绍 (00:00 - 05:30)

00:00 - 00:45 - 主持人开场致辞，感谢参会者参加AWS re:Invent，介绍今天的议程安排

00:45 - 01:30 - 介绍两位嘉宾：ServiceNow的Satwick和SLB（前身为斯伦贝谢）的Jamie

01:30 - 02:15 - 询问现场观众AI采用情况，大多数参会者表示已经在开发或实施AI项目

02:15 - 03:45 - 介绍AI工厂概念和AI在各行业的广泛应用，强调本次会议将讨论两个不同行业的AI用例

03:45 - 05:30 - 讲解AI的三个扩展定律：预训练扩展（从互联网学习）、后训练扩展（模型学习思考）、测试时扩展（推理前的深度思考），随着模型智能提升，计算需求也在增长

### NVIDIA与AWS合作伙伴关系 (05:30 - 08:00)

05:30 - 06:45 - 介绍NVIDIA的全栈解决方案，不仅包括GPU基础设施，还包括完整的软件层

06:45 - 08:00 - 详细说明NVIDIA与AWS的集成：CUDA库、DGX Hub、NVIDIA AI Enterprise、Omniverse、NIM推理微服务等都可在AWS Marketplace获取，并集成到各种AWS服务中

### DGX Cloud平台介绍 (08:00 - 11:00)

08:00 - 09:00 - 询问观众是否了解DGX Cloud，解释这是NVIDIA与AWS共同设计的解决方案

09:00 - 10:30 - 详细介绍DGX Cloud的技术栈：
  - 基础设施层：GB200 GPU、高性能网络、Lustre存储
  - 管理层：优化的EKS（Elastic Kubernetes Service）
  - 编排层：RunAI GPU编排工具，最大化GPU利用率
  - 软件层：企业级AI软件
  - 支持层：NVIDIA AI和云专家支持、技术客户经理、24/7企业支持

10:30 - 11:00 - 总结DGX Cloud提供了在共同设计的基础设施上运行的优化全栈NVIDIA AI平台

### 嘉宾介绍与背景 (11:00 - 15:30)

11:00 - 12:30 - Satwick自我介绍：
  - ServiceNow首席科学家，领导基础模型实验室
  - 负责模型的中期训练和后训练
  - 目标是训练与前沿模型性能相当的小型语言模型
  - ServiceNow是企业平台，托管IT、HR、法律、合规等各类数据和流程

12:30 - 15:30 - Jamie自我介绍：
  - SLB业务线总监，提供内部数据和AI平台
  - SLB是全球最大的油气服务公司之一，在120多个国家运营
  - 业务涵盖传统能源和新能源
  - 数字部门生产技术帮助客户进行规划、建模、模拟和高效运营
  - 业务高度专业化，涉及科学和工程领域

### AI开发目标 (15:30 - 22:00)

15:30 - 18:30 - Satwick阐述ServiceNow的AI目标：
  - 企业流程自动化从编码工作流演进到智能工作流，现在进入代理AI应用阶段
  - 类似Anthropic的MCP（模型上下文协议）概念，通过调用工具和函数完成任务
  - 挑战在于模型能力和成本平衡
  - 目标是构建高效的代理系统和模型，以前沿模型的性能水平但更低的成本服务客户
  - 设计定制的后训练方案，并开源大部分模型

18:30 - 22:00 - Jamie阐述SLB的AI目标：
  - 通过昂贵的方式获取大量数据（地震数据、地球物理数据）
  - 进行地理区域调查以探索地下结构
  - 钻探井眼获取岩石物理数据
  - 地质学家和地球物理学家基于这些数据构建地下模型
  - 30-40年来使用复杂的确定性物理和数学技术预测地下结构
  - 近年来采用数据驱动技术、机器学习
  - 现在使用通用基础模型，但需要从文本到文本以外的能力
  - 开发领域基础模型用于地震和时间序列数据、岩石物理数据
  - 目标是结合数据驱动建模、基于物理的建模和代理编排

### DGX Cloud如何帮助实现目标 (22:00 - 30:00)

22:00 - 25:30 - Satwick分享使用DGX Cloud的体验：
  - 之前在内部部署NVIDIA硬件，有自己的编排框架和作业管理
  - 维护超大规模集群是全职工作
  - 转向DGX Cloud和AWS因为更可靠，可以随时切换到更新的硬件
  - 集群配备Lustre存储、RunAI编排、高性能网络
  - RunAI特别重要，能够最大化集群使用率
  - 通过作业优先级管理，实现接近100%的利用率（训练、数据合成或评估）
  - 过去一年几乎零停机时间，只有几小时用于集群升级

25:30 - 30:00 - Jamie分享使用DGX Cloud的体验：
  - SLB与AWS和NVIDIA有长期合作关系
  - 之前有内部HPC基础设施用于模拟和地震处理
  - 最初使用本地A100基础设施训练模型
  - 为了加快产品上市速度，利用DGX Cloud
  - 在过去两年发布了两个主要产品（生成式AI和代理助手）
  - 开箱即用的优化堆栈让团队专注于重要工作
  - 利用AWS的弹性计算能力和优化堆栈加速领域模型开发
  - 通过支持优化，从2D模型发展到2.5D再到3D地震基础模型，同样硬件获得更高吞吐量

### 模型开发详情 (30:00 - 35:00)

30:00 - 33:00 - 主持人询问ServiceNow的Apriel模型系列及其在整体战略中的作用

33:00 - 35:00 - Satwick详细介绍Apriel模型：
  - 训练模型约2.5年，今年1月开始开源所有模型
  - 开源增加知名度，模型本身无法独立完成工作，需要代理编排、工具和数据
  - 今年发布三个模型：50亿参数模型、150亿参数推理模型、150亿参数多模态模型（处理图像和文本）
  - 目标不是与前沿实验室竞争，而是在单GPU规模上构建前沿级推理性能
  - 小模型能完成复杂任务意味着可以服务大量工作负载而不花费太多
  - 客户可以为最复杂用例保留OpenAI额度，其他用例使用ServiceNow模型
  - 展示性能对比：150亿参数的Apriel模型与DeepSeek R1（6700亿参数）和Qwen（2350亿参数）性能相当
  - 模型获得近10万次下载，社区反响热烈

### 独特挑战与解决方案 (35:00 - 42:00)

35:00 - 39:00 - Jamie分享SLB面临的独特挑战：
  - 将生成式AI技术引入现有技术组合
  - 需要物理模拟器、算法、机器学习模型等多种工具配合
  - 构建行业特定的多模态模型加速工作
  - 示例：重建数据采集或处理中