1
00:00:00,750 --> 00:00:01,773
- Everybody good?

2
00:00:02,620 --> 00:00:05,360
You all know you could
be at lunch right now,

3
00:00:05,360 --> 00:00:08,580
but you're here with
me, and I appreciate it.

4
00:00:08,580 --> 00:00:10,650
Obviously, this is a sold out session,

5
00:00:10,650 --> 00:00:11,880
and since the camera's on me,

6
00:00:11,880 --> 00:00:14,100
no one will know if I'm telling the truth.

7
00:00:14,100 --> 00:00:15,540
My name is Dutch Meyer.

8
00:00:15,540 --> 00:00:17,190
I'm a principal software engineer

9
00:00:17,190 --> 00:00:20,520
with Amazon Web Services,
Elastic Block Store, GBS.

10
00:00:20,520 --> 00:00:23,760
I'm joined here by our
principal product manager,

11
00:00:23,760 --> 00:00:26,790
Sapna Gupta, and we're
gonna talk to you today

12
00:00:26,790 --> 00:00:30,360
about how to get the most
out of your GP3 volumes.

13
00:00:30,360 --> 00:00:33,630
If you don't already
know, the GP3 volume type,

14
00:00:33,630 --> 00:00:36,210
as an operator of the service every day,

15
00:00:36,210 --> 00:00:39,480
the volume type I see the most of is GP3,

16
00:00:39,480 --> 00:00:42,300
and that's because it's designed to be

17
00:00:42,300 --> 00:00:44,850
really the default
choice or the best choice

18
00:00:44,850 --> 00:00:46,800
for the widest range of workloads.

19
00:00:46,800 --> 00:00:49,200
It's called general purpose for a reason.

20
00:00:49,200 --> 00:00:51,200
So, I'm gonna talk a little bit about

21
00:00:51,200 --> 00:00:53,550
how we think about running that service,

22
00:00:53,550 --> 00:00:55,020
how we think about building it,

23
00:00:55,020 --> 00:00:57,320
and what you can do to
get the most out of it.

24
00:00:59,220 --> 00:01:00,810
Here's our plan.

25
00:01:00,810 --> 00:01:02,910
I'm gonna start the session.

26
00:01:02,910 --> 00:01:05,160
I'm gonna talk mostly
about reads and writes

27
00:01:05,160 --> 00:01:06,390
down the I/O path.

28
00:01:06,390 --> 00:01:08,430
I'm gonna stick to storage fundamentals.

29
00:01:08,430 --> 00:01:10,200
I'm gonna talk about this problem

30
00:01:10,200 --> 00:01:12,330
of coupled versus decoupled storage,

31
00:01:12,330 --> 00:01:13,800
and I'll go into that in much more depth,

32
00:01:13,800 --> 00:01:15,600
but what I'm talking about there is

33
00:01:15,600 --> 00:01:17,910
the way that your capacity of storage

34
00:01:17,910 --> 00:01:20,820
and your performance of storage
is coupled together or not.

35
00:01:20,820 --> 00:01:22,500
I'm gonna talk about how to
monitor that performance,

36
00:01:22,500 --> 00:01:24,300
make sure you're getting what
you're supposed to be getting,

37
00:01:24,300 --> 00:01:27,030
and what to do when you start to get out

38
00:01:27,030 --> 00:01:30,840
into the extremes of performance.

39
00:01:30,840 --> 00:01:32,550
And then Sapna's gonna take over.

40
00:01:32,550 --> 00:01:34,230
Do you wanna introduce yourself?

41
00:01:34,230 --> 00:01:35,790
- Good afternoon, everyone.

42
00:01:35,790 --> 00:01:39,510
This is Sapna, Principal
Product Manager at AWS.

43
00:01:39,510 --> 00:01:41,820
I'm super excited to be here today.

44
00:01:41,820 --> 00:01:44,160
Thank you for joining us.

45
00:01:44,160 --> 00:01:47,520
No, it's lunchtime, but hope
you all enjoy this session.

46
00:01:47,520 --> 00:01:49,290
After that, you can go for the lunch.

47
00:01:49,290 --> 00:01:51,520
I'll cover the last two
topics of this session today.

48
00:01:51,520 --> 00:01:54,200
How do you evolve your volume

49
00:01:54,200 --> 00:01:57,030
to optimize for price and performance?

50
00:01:57,030 --> 00:01:59,820
I'll cover that not just for
your primary environment,

51
00:01:59,820 --> 00:02:02,730
but I will also talk about
how do you do the same thing

52
00:02:02,730 --> 00:02:04,860
for your secondary environment?

53
00:02:04,860 --> 00:02:06,630
So with that, I'll hand it over to Dutch,

54
00:02:06,630 --> 00:02:07,980
and then I'll be back.

55
00:02:07,980 --> 00:02:09,560
- Thank you so much.
- Thank you.

56
00:02:09,560 --> 00:02:14,220
- Okay, so I care a lot about storage.

57
00:02:14,220 --> 00:02:15,930
I'm really passionate about being

58
00:02:15,930 --> 00:02:18,063
a storage optimizer, a builder,

59
00:02:19,380 --> 00:02:23,490
but as a confessional, in my private life,

60
00:02:23,490 --> 00:02:26,310
I am extremely wasteful in storage.

61
00:02:26,310 --> 00:02:30,300
So I have by my computer
at my house, a rack,

62
00:02:30,300 --> 00:02:31,410
not in the server sense,

63
00:02:31,410 --> 00:02:34,140
but just like a bookshelf
full of hard drives.

64
00:02:34,140 --> 00:02:36,240
It's basically all the
hard drives I've ever had.

65
00:02:36,240 --> 00:02:39,723
A couple of them are disassembled
because I was curious,

66
00:02:40,950 --> 00:02:44,670
but I'll be an egregious
waster of space there

67
00:02:44,670 --> 00:02:46,520
in my personal life,
and if you're like me,

68
00:02:46,520 --> 00:02:50,040
and I know I am, a lot of
you are doing the same thing.

69
00:02:50,040 --> 00:02:51,510
You've got a whole bunch
of volumes lying around.

70
00:02:51,510 --> 00:02:52,680
You're not really using them,

71
00:02:52,680 --> 00:02:55,110
whether they're physical
disks or EBS volumes,

72
00:02:55,110 --> 00:02:57,320
or you're taking a system
and you're benchmarking it

73
00:02:57,320 --> 00:02:59,600
full speed against something else,

74
00:02:59,600 --> 00:03:01,680
and the point I wanna make here,

75
00:03:01,680 --> 00:03:03,840
it's really important for GP3,

76
00:03:03,840 --> 00:03:06,930
is that an optimized
system is not a system

77
00:03:06,930 --> 00:03:08,430
running as hard as possible.

78
00:03:08,430 --> 00:03:10,470
It's a system that's in balance,

79
00:03:10,470 --> 00:03:14,763
where your IOPS and your
throughput and your capacity,

80
00:03:15,840 --> 00:03:18,300
your latency certainly, and in some cases,

81
00:03:18,300 --> 00:03:20,670
even your durability is correct

82
00:03:20,670 --> 00:03:23,340
and aligned to the workload you have.

83
00:03:23,340 --> 00:03:26,220
So that's what I mean by optimized today.

84
00:03:26,220 --> 00:03:28,600
Storage is about making those hard choices

85
00:03:28,600 --> 00:03:31,320
between those different trade-offs,

86
00:03:31,320 --> 00:03:32,430
and what I'm gonna talk about

87
00:03:32,430 --> 00:03:34,680
is how we can build this system, GP3,

88
00:03:34,680 --> 00:03:36,735
which is meant to serve the vast majority

89
00:03:36,735 --> 00:03:38,970
of customer workloads, and all of them

90
00:03:38,970 --> 00:03:41,580
are valid points in that space.

91
00:03:41,580 --> 00:03:45,030
So looking at the range of possible

92
00:03:45,030 --> 00:03:47,730
performance characteristics
and sizes of GP3,

93
00:03:47,730 --> 00:03:49,320
it's quite large.

94
00:03:49,320 --> 00:03:51,120
We go from a one gig volume

95
00:03:51,120 --> 00:03:53,883
all the way up to 64 terabytes today.

96
00:03:56,480 --> 00:03:58,980
And that's important not
because it's so fast,

97
00:03:58,980 --> 00:04:02,430
it is fast, but there are
faster volumes you can get.

98
00:04:02,430 --> 00:04:04,350
What's important about this for GP3

99
00:04:04,350 --> 00:04:06,080
is that anywhere in that space

100
00:04:06,080 --> 00:04:10,440
is a valid GP3 volume with GP3 behavior,

101
00:04:10,440 --> 00:04:13,410
GP3 durability, and it's
even more complex than that

102
00:04:13,410 --> 00:04:16,740
because if you step back, you
can move in either direction.

103
00:04:16,740 --> 00:04:18,270
You can move in the capacity direction,

104
00:04:18,270 --> 00:04:20,040
make a large volume that's cold.

105
00:04:20,040 --> 00:04:21,840
That's a valid GP3 volume.

106
00:04:21,840 --> 00:04:23,310
You can make a smaller volume that's fast.

107
00:04:23,310 --> 00:04:25,260
It's a valid GP3 volume.

108
00:04:25,260 --> 00:04:27,840
If those numbers look a little bigger

109
00:04:27,840 --> 00:04:29,400
than you're used to, it's
because we bumped them

110
00:04:29,400 --> 00:04:30,420
in the last month or so.

111
00:04:30,420 --> 00:04:31,980
I was actually involved in this.

112
00:04:31,980 --> 00:04:35,310
We took the capacity up four X,

113
00:04:35,310 --> 00:04:37,230
the IOPS up five X.

114
00:04:37,230 --> 00:04:39,183
Again, all valid GP3 volumes.

115
00:04:43,140 --> 00:04:45,390
So I wanna talk about how we do that.

116
00:04:45,390 --> 00:04:46,590
It's important to understand,

117
00:04:46,590 --> 00:04:49,080
how does EBS work as a service?

118
00:04:49,080 --> 00:04:50,760
And then I'm gonna talk about

119
00:04:50,760 --> 00:04:52,290
what it means to do that scaling,

120
00:04:52,290 --> 00:04:54,000
to make a bigger GP3,

121
00:04:54,000 --> 00:04:56,800
and how you can take advantage
of it for your workloads.

122
00:04:57,660 --> 00:05:00,810
Your volume comes to us as provisioned

123
00:05:00,810 --> 00:05:02,490
with some kind of entitlement.

124
00:05:02,490 --> 00:05:04,140
It's got a set number of IOPS.

125
00:05:04,140 --> 00:05:05,040
It's got a size.

126
00:05:05,040 --> 00:05:06,490
You let us know what that is.

127
00:05:10,510 --> 00:05:13,980
What we do internally
is we take that volume

128
00:05:13,980 --> 00:05:15,030
and we shard it.

129
00:05:15,030 --> 00:05:17,130
And lots of systems shard.

130
00:05:17,130 --> 00:05:19,590
It's a core technique of storage.

131
00:05:19,590 --> 00:05:21,370
The thing I wanna point
out about this sharding

132
00:05:21,370 --> 00:05:26,030
is that it lets us take that large span

133
00:05:26,030 --> 00:05:29,070
of different GP3 characteristics

134
00:05:29,070 --> 00:05:34,070
and reduce it to a smaller
set of shard characteristics.

135
00:05:35,010 --> 00:05:36,937
So your entitlement from
a performance perspective

136
00:05:36,937 --> 00:05:38,850
is carried through to the shard

137
00:05:38,850 --> 00:05:40,680
that makes up your volume.

138
00:05:40,680 --> 00:05:42,660
But because we can vary
the number of shards,

139
00:05:42,660 --> 00:05:44,790
we can reduce the span,

140
00:05:44,790 --> 00:05:46,860
make it actually less dynamic.

141
00:05:46,860 --> 00:05:49,260
So smaller variance in our fleet

142
00:05:49,260 --> 00:05:51,410
of the performance that
those shards bring.

143
00:05:53,070 --> 00:05:55,780
What you get with that GP3 volume

144
00:05:57,450 --> 00:05:59,580
is the ability to purchase

145
00:05:59,580 --> 00:06:04,580
a decoupled performance capacity volume.

146
00:06:04,850 --> 00:06:07,850
And I am deeply envious of that ability

147
00:06:07,850 --> 00:06:10,770
because on the EBS side,
I actually can't do that.

148
00:06:10,770 --> 00:06:13,440
When I go and I buy raw commodity storage,

149
00:06:13,440 --> 00:06:15,480
you can imagine me
buying it off Amazon.com,

150
00:06:15,480 --> 00:06:18,570
I can't get exactly the right volume

151
00:06:18,570 --> 00:06:22,110
that Sapna's gonna need
for her desktop tomorrow.

152
00:06:22,110 --> 00:06:23,910
It's absolutely not what I do.

153
00:06:23,910 --> 00:06:27,300
If you look at our fleet, it's
actually fairly homogeneous.

154
00:06:27,300 --> 00:06:29,490
Now we vary by vendor.

155
00:06:29,490 --> 00:06:31,620
We vary by size and
performance a little bit

156
00:06:31,620 --> 00:06:34,380
and we do roll our hardware
revisions regularly,

157
00:06:34,380 --> 00:06:36,690
but our fleet is actually
fairly homogeneous

158
00:06:36,690 --> 00:06:38,313
from a disk drive perspective.

159
00:06:39,420 --> 00:06:41,910
So what we're doing somehow

160
00:06:41,910 --> 00:06:45,420
is we're taking a very
coupled storage fleet,

161
00:06:45,420 --> 00:06:47,763
we're turning it into decoupled storage.

162
00:06:48,780 --> 00:06:52,050
And when you, the
customer, buy that fleet,

163
00:06:52,050 --> 00:06:53,700
and let's say you're off in
the extreme of performance,

164
00:06:53,700 --> 00:06:54,960
you've got high IOPS,

165
00:06:54,960 --> 00:06:58,890
we have the freedom on our
side to use our fleet size

166
00:06:58,890 --> 00:07:00,900
to put that in the fleet
at the right place,

167
00:07:00,900 --> 00:07:04,080
the right place for
that is probably nearby

168
00:07:04,080 --> 00:07:07,923
some other customer who has
a capacity-heavy cold volume.

169
00:07:09,050 --> 00:07:12,390
It's kind of the core magic of EBS

170
00:07:12,390 --> 00:07:16,470
is that we can turn our
hardware, our physical fleet,

171
00:07:16,470 --> 00:07:20,940
a very coupled performance
and capacity storage

172
00:07:20,940 --> 00:07:24,900
into a dynamic offering that
gets a mixed performance

173
00:07:24,900 --> 00:07:26,170
and then we recombine them

174
00:07:26,170 --> 00:07:29,223
to utilize that hardware
as much as possible.

175
00:07:30,660 --> 00:07:34,770
So two immediate lessons
on your side of the stack.

176
00:07:34,770 --> 00:07:37,830
One is by specifying the
IOPS and the throughput

177
00:07:37,830 --> 00:07:40,670
that you need to be exactly what you need,

178
00:07:40,670 --> 00:07:43,320
you actually enable me to do a better job

179
00:07:43,320 --> 00:07:46,070
placing in my fleet to take
advantage of my hardware

180
00:07:46,070 --> 00:07:48,540
and it lowers costs for all of us.

181
00:07:48,540 --> 00:07:51,290
We actually just pass that
on to you for the most part.

182
00:07:52,890 --> 00:07:55,830
And then when we wanna
increase the performance

183
00:07:55,830 --> 00:07:58,020
as we did about a month ago,

184
00:07:58,020 --> 00:08:00,240
for GP3, the naive approach would be,

185
00:08:00,240 --> 00:08:01,920
oh, you can add more shards, right?

186
00:08:01,920 --> 00:08:04,020
You just like scale that out.

187
00:08:04,020 --> 00:08:06,150
You get twice as many
shards, it goes twice as fast

188
00:08:06,150 --> 00:08:07,800
'cause your IOPS are spread twice like.

189
00:08:07,800 --> 00:08:09,400
That is actually not what we do.

190
00:08:10,470 --> 00:08:13,050
Because when you do that,
it has ramifications

191
00:08:13,050 --> 00:08:15,273
to the lived experience of GP3.

192
00:08:16,230 --> 00:08:18,630
It actually makes your GP3 volume,

193
00:08:18,630 --> 00:08:23,630
the performance impacts are
not what we want for GP3.

194
00:08:23,700 --> 00:08:25,620
It makes those, it means that anywhere

195
00:08:25,620 --> 00:08:27,540
that there is a hardware event,

196
00:08:27,540 --> 00:08:29,460
something unfortunate
happens in our fleet,

197
00:08:29,460 --> 00:08:31,800
you're more likely to feel
that the more shards you have.

198
00:08:31,800 --> 00:08:33,630
So what we actually do when we're looking

199
00:08:33,630 --> 00:08:36,250
at increasing the range of GP3

200
00:08:36,250 --> 00:08:38,820
is we're making those
shards more performant

201
00:08:38,820 --> 00:08:40,170
all the way down the stack.

202
00:08:41,460 --> 00:08:44,070
Okay, I'm gonna talk about
performance in a minute.

203
00:08:44,070 --> 00:08:45,930
I wanna circle back on this coupled

204
00:08:45,930 --> 00:08:47,070
versus uncoupled performance

205
00:08:47,070 --> 00:08:49,560
because we do have a GP2 volume type.

206
00:08:49,560 --> 00:08:51,360
People use it, it's fine.

207
00:08:51,360 --> 00:08:53,910
But GP2 is giving you
that coupled performance.

208
00:08:53,910 --> 00:08:55,890
When you buy a GP2 volume,

209
00:08:55,890 --> 00:09:00,090
you're getting capacity and
performance tied together.

210
00:09:00,090 --> 00:09:04,923
It's a fine replica of
buying a physical disk.

211
00:09:06,070 --> 00:09:10,440
But it doesn't actually
have the characteristics

212
00:09:10,440 --> 00:09:11,790
that I want you to have as a customer.

213
00:09:11,790 --> 00:09:13,080
I want you to be able to

214
00:09:13,080 --> 00:09:15,453
independently scale those two axes.

215
00:09:17,190 --> 00:09:21,030
GP3 in comparison gives you
that decoupled performance.

216
00:09:21,030 --> 00:09:22,770
If you want more IOPS,
you can buy more IOPS,

217
00:09:22,770 --> 00:09:26,490
mostly independent of
the underlying capacity.

218
00:09:26,490 --> 00:09:29,223
And what you get there is a cost savings.

219
00:09:31,200 --> 00:09:32,760
So what this enables you to do

220
00:09:32,760 --> 00:09:34,680
is to build a strategy for your volumes

221
00:09:34,680 --> 00:09:36,390
depending on your workload.

222
00:09:36,390 --> 00:09:37,800
And this is the core trick

223
00:09:37,800 --> 00:09:40,200
that lets us create this volume type

224
00:09:40,200 --> 00:09:42,240
that is the most broadly applicable.

225
00:09:42,240 --> 00:09:43,590
Go ahead, take the picture.

226
00:09:46,190 --> 00:09:48,540
So for more transactional processing,

227
00:09:48,540 --> 00:09:50,130
you want more IOPS, right?

228
00:09:50,130 --> 00:09:52,170
More IOPS returning with
more latency quickly.

229
00:09:52,170 --> 00:09:54,390
Those tend to be smaller operations.

230
00:09:54,390 --> 00:09:55,590
For larger throughput,

231
00:09:55,590 --> 00:09:57,630
you're gonna have larger block sizes.

232
00:09:57,630 --> 00:09:59,010
The IOPS and throughput are connected

233
00:09:59,010 --> 00:10:00,663
to the block size that way.

234
00:10:01,860 --> 00:10:02,910
And in that entire range,

235
00:10:02,910 --> 00:10:05,490
you're getting GP3
latency and performance.

236
00:10:05,490 --> 00:10:09,270
So let's talk a little
bit about performance

237
00:10:09,270 --> 00:10:10,740
as measured by latency,

238
00:10:10,740 --> 00:10:11,573
because of course,

239
00:10:11,573 --> 00:10:13,740
performance is more than
IOPS and throughput.

240
00:10:13,740 --> 00:10:15,740
Controlling latency is really important.

241
00:10:18,120 --> 00:10:20,310
So I monitor latency extensively.

242
00:10:20,310 --> 00:10:22,950
It's the thing in EBS
that is kept constant.

243
00:10:22,950 --> 00:10:25,500
So your experience from a
latency perspective is the same,

244
00:10:25,500 --> 00:10:28,473
no matter where you are in
that range of GP3 volumes.

245
00:10:29,640 --> 00:10:31,620
GP3 experience we talk about

246
00:10:31,620 --> 00:10:34,270
as a single digit millisecond experience

247
00:10:36,390 --> 00:10:38,580
with 99% compliance.

248
00:10:38,580 --> 00:10:40,890
In comparison, there are IO2 volumes.

249
00:10:40,890 --> 00:10:42,360
Those are great if that's what you need.

250
00:10:42,360 --> 00:10:44,040
That is a very different experience.

251
00:10:44,040 --> 00:10:45,930
That's sub millisecond.

252
00:10:45,930 --> 00:10:48,810
And the biggest difference
I think of with IO2

253
00:10:48,810 --> 00:10:52,380
really is the conformity
to that experience.

254
00:10:52,380 --> 00:10:54,420
I won't put the full GP2,

255
00:10:54,420 --> 00:10:55,560
this isn't a GP2 talk,

256
00:10:55,560 --> 00:10:58,193
so I won't give you the full
performance curve there.

257
00:10:59,610 --> 00:11:04,200
But I can say that the
incidence of IO operations

258
00:11:04,200 --> 00:11:06,570
that take more than 800 microseconds

259
00:11:06,570 --> 00:11:09,990
is 10x lower on IO2 than GP3.

260
00:11:09,990 --> 00:11:12,750
So if someone needs
that really fast latency

261
00:11:12,750 --> 00:11:15,420
and they need that really
strict compliance to latency,

262
00:11:15,420 --> 00:11:16,770
that's the IO2 experience.

263
00:11:16,770 --> 00:11:18,570
It's just different from the GP experience

264
00:11:18,570 --> 00:11:20,190
regardless of how many IOPS

265
00:11:20,190 --> 00:11:21,790
or how much throughput you have.

266
00:11:23,830 --> 00:11:25,350
Okay, so we've talked about

267
00:11:25,350 --> 00:11:27,210
coupled versus uncoupled storage.

268
00:11:27,210 --> 00:11:28,800
Talked just a little bit about latency

269
00:11:28,800 --> 00:11:30,570
and what the product is.

270
00:11:30,570 --> 00:11:33,480
I wanna give you one of my best tools.

271
00:11:33,480 --> 00:11:35,310
I use this every single day.

272
00:11:35,310 --> 00:11:36,660
This is not about EBS.

273
00:11:36,660 --> 00:11:38,880
This is just storage fundamentals.

274
00:11:38,880 --> 00:11:42,300
But there's a relationship
between your latency

275
00:11:42,300 --> 00:11:43,890
and your IOPS.

276
00:11:43,890 --> 00:11:45,930
It's connected by queue depth.

277
00:11:45,930 --> 00:11:49,020
This is just basic queuing theory, okay?

278
00:11:49,020 --> 00:11:53,190
Your latency is your queue
depth divided by your IOPS.

279
00:11:53,190 --> 00:11:54,183
It's physics.

280
00:11:55,810 --> 00:11:58,980
And there's a linear
relationship between the two.

281
00:11:58,980 --> 00:12:03,720
So let's say, for example,
you've got a GP3 volume.

282
00:12:03,720 --> 00:12:05,400
You've got some workloads running on it.

283
00:12:05,400 --> 00:12:08,520
And at noon, you figure
everyone's gonna be at lunch.

284
00:12:08,520 --> 00:12:11,370
So you set your cron jobs
or your scheduled tasks

285
00:12:11,370 --> 00:12:12,540
to all fire at noon.

286
00:12:12,540 --> 00:12:13,373
They all fire up.

287
00:12:13,373 --> 00:12:15,120
They launch a bunch of
IOPS against the system.

288
00:12:15,120 --> 00:12:15,963
What happens?

289
00:12:18,510 --> 00:12:20,730
All those operations
increase the queue depth.

290
00:12:20,730 --> 00:12:22,800
You get more queue of
operations coming in.

291
00:12:22,800 --> 00:12:24,330
The result is you get more performance.

292
00:12:24,330 --> 00:12:25,163
Your IOPS goes up.

293
00:12:25,163 --> 00:12:26,460
Your latency's gonna hold constant.

294
00:12:26,460 --> 00:12:28,650
Your IOPS is gonna increase right up

295
00:12:28,650 --> 00:12:30,420
until you hit your IOPS entitlement.

296
00:12:30,420 --> 00:12:32,250
And when you have enough
scheduled jobs firing

297
00:12:32,250 --> 00:12:35,640
all at the same time to hit that cap,

298
00:12:35,640 --> 00:12:37,940
then your latency's gonna
come up in response.

299
00:12:39,480 --> 00:12:41,310
Anytime I'm looking at a system
and trying to figure out,

300
00:12:41,310 --> 00:12:42,390
well, what is this system doing

301
00:12:42,390 --> 00:12:43,830
from a storage perspective?

302
00:12:43,830 --> 00:12:45,930
I've got that equation
in the back of my head.

303
00:12:45,930 --> 00:12:48,030
It will make you seem really
smart in front of your peers

304
00:12:48,030 --> 00:12:49,913
'cause you can just throw it out there.

305
00:12:51,480 --> 00:12:54,690
By the way, if you jitter your cron jobs

306
00:12:54,690 --> 00:12:55,920
to not start on the hour,

307
00:12:55,920 --> 00:12:59,730
then I won't see a huge spike
in IOPS every single hour.

308
00:12:59,730 --> 00:13:01,440
That's my problem, just saying.

309
00:13:01,440 --> 00:13:02,740
But you could be a friend.

310
00:13:04,530 --> 00:13:06,300
Okay, this slide is the same thing.

311
00:13:06,300 --> 00:13:09,300
I'm just gonna reiterate
the same thing again.

312
00:13:09,300 --> 00:13:11,580
The queue depth, the IOPS, the latency

313
00:13:11,580 --> 00:13:12,510
are all connected together.

314
00:13:12,510 --> 00:13:15,030
You can bring your performance up

315
00:13:15,030 --> 00:13:18,660
by increasing your queues
up to your IOPS entitlement,

316
00:13:18,660 --> 00:13:20,100
and then you're gonna
start to feel latency.

317
00:13:20,100 --> 00:13:22,740
But what this means is that
you actually have control

318
00:13:22,740 --> 00:13:23,970
of your latency.

319
00:13:23,970 --> 00:13:27,150
Even though the latency of the GP3 product

320
00:13:27,150 --> 00:13:29,250
is the thing that I hold constant,

321
00:13:29,250 --> 00:13:32,400
you actually have a lot of
control in both directions.

322
00:13:32,400 --> 00:13:34,770
If you drive it too hard from
a queue depth perspective,

323
00:13:34,770 --> 00:13:36,780
you will see the latency come up.

324
00:13:36,780 --> 00:13:38,460
Your instance will start throttling.

325
00:13:38,460 --> 00:13:40,950
We actually have, it was October,

326
00:13:40,950 --> 00:13:44,790
late October release, there
are CloudWatch metrics now

327
00:13:44,790 --> 00:13:46,170
that will tell you when this is happening.

328
00:13:46,170 --> 00:13:47,760
So you can diagnose this yourself.

329
00:13:47,760 --> 00:13:49,230
There's also new CloudWatch metrics

330
00:13:49,230 --> 00:13:51,660
to manage throughput and latency.

331
00:13:51,660 --> 00:13:53,160
So we have that monitoring built in.

332
00:13:53,160 --> 00:13:54,560
You can look at it directly.

333
00:13:56,110 --> 00:14:00,030
The actual performance of the queue depth

334
00:14:00,030 --> 00:14:01,650
versus the IOPS and the latency

335
00:14:01,650 --> 00:14:04,920
is gonna depend on your
workload and your block size.

336
00:14:04,920 --> 00:14:09,750
As a general rule, you should expect

337
00:14:09,750 --> 00:14:13,210
when you're in the 16K
range for block size,

338
00:14:13,210 --> 00:14:15,480
one QD per 1,000 IOPS.

339
00:14:15,480 --> 00:14:17,010
And if you reverse the math on that,

340
00:14:17,010 --> 00:14:19,170
it comes to one millisecond latency.

341
00:14:19,170 --> 00:14:22,740
It's all connected
exactly by that equation.

342
00:14:22,740 --> 00:14:25,233
So that is the theory.

343
00:14:26,100 --> 00:14:27,750
That is the basic theory of storage.

344
00:14:27,750 --> 00:14:29,580
It is absolutely true.

345
00:14:29,580 --> 00:14:31,683
But I think we know in practice,

346
00:14:33,900 --> 00:14:36,510
the average doesn't tell the whole story.

347
00:14:36,510 --> 00:14:39,450
In practice, we have to monitor

348
00:14:39,450 --> 00:14:42,633
across the range of
performance characteristics.

349
00:14:44,040 --> 00:14:45,810
So again, how do we monitor?

350
00:14:45,810 --> 00:14:47,583
We've got CloudWatch metrics.

351
00:14:48,450 --> 00:14:50,250
We've got NVMe CLI.

352
00:14:50,250 --> 00:14:53,520
There's some new commands here
I'll talk about in a second.

353
00:14:53,520 --> 00:14:57,420
The best tool that I have
to monitor performance

354
00:14:57,420 --> 00:14:59,190
is actually a latency histogram.

355
00:14:59,190 --> 00:15:01,080
And again, this is not EBS specific.

356
00:15:01,080 --> 00:15:03,120
This is just fundamental storage.

357
00:15:03,120 --> 00:15:04,830
So I'm gonna show you what a
latency histogram looks like

358
00:15:04,830 --> 00:15:05,790
if you haven't seen it.

359
00:15:05,790 --> 00:15:07,390
I'll talk you through the slide.

360
00:15:12,180 --> 00:15:13,950
This is an actual latency histogram.

361
00:15:13,950 --> 00:15:15,410
It is not internal EBS.

362
00:15:15,410 --> 00:15:18,930
It is something I pulled from
my personal cloud desktop

363
00:15:18,930 --> 00:15:21,660
on one weekend day.

364
00:15:21,660 --> 00:15:23,220
Let's look at what my system is doing

365
00:15:23,220 --> 00:15:24,720
for the purposes of this deck.

366
00:15:26,340 --> 00:15:28,230
You'll see sort of a pulse wave.

367
00:15:28,230 --> 00:15:32,013
On the x-axis, that is your
latency in microseconds.

368
00:15:33,540 --> 00:15:37,320
So 1,024 microseconds,
that's about a millisecond.

369
00:15:37,320 --> 00:15:38,943
That's where you get the peak.

370
00:15:39,810 --> 00:15:42,600
On the y-axis, that's the
number of I/O operations

371
00:15:42,600 --> 00:15:44,433
that fall into that latency bucket.

372
00:15:46,470 --> 00:15:48,270
This is nine times out of 10 the best way

373
00:15:48,270 --> 00:15:50,370
to look at your latency of your system.

374
00:15:50,370 --> 00:15:53,040
So when I am dealing
with a latency histogram,

375
00:15:53,040 --> 00:15:54,810
the first thing I look at
is, well, where's my peak?

376
00:15:54,810 --> 00:15:56,730
That's sort of the typical performance.

377
00:15:56,730 --> 00:15:57,930
In this case, it just happens.

378
00:15:57,930 --> 00:15:59,130
It's around a millisecond.

379
00:15:59,130 --> 00:16:01,650
It's about what I expect
from a GP3 volume.

380
00:16:01,650 --> 00:16:03,270
I kinda was hoping I
would get something weird

381
00:16:03,270 --> 00:16:04,800
in my graph, but I didn't.

382
00:16:04,800 --> 00:16:05,840
It's a normal behaving system.

383
00:16:05,840 --> 00:16:08,850
I also look at how that pulse is skewed.

384
00:16:08,850 --> 00:16:11,640
If it's totally normal, I'm
getting an even distribution

385
00:16:11,640 --> 00:16:13,320
of I/Os that are slower and faster.

386
00:16:13,320 --> 00:16:16,260
In this case, I'm skewed
left a little bit.

387
00:16:16,260 --> 00:16:18,240
So the system is generally
under a millisecond.

388
00:16:18,240 --> 00:16:21,390
Again, it's pretty good
performance for a GP3 volume,

389
00:16:21,390 --> 00:16:23,220
especially one that's not
under a lot of stress.

390
00:16:23,220 --> 00:16:24,620
Queue depth isn't that deep.

391
00:16:27,330 --> 00:16:28,890
I think about how wide this curve is.

392
00:16:28,890 --> 00:16:31,383
That's the variability in performance.

393
00:16:32,700 --> 00:16:35,490
So here, there's some
variance in performance.

394
00:16:35,490 --> 00:16:36,630
I'm getting four milliseconds.

395
00:16:36,630 --> 00:16:37,710
I'm getting one millisecond.

396
00:16:37,710 --> 00:16:39,030
I'm getting things that are completing

397
00:16:39,030 --> 00:16:40,470
in a quarter of a millisecond.

398
00:16:40,470 --> 00:16:41,433
It's pretty fast.

399
00:16:42,540 --> 00:16:45,320
If this were GP, sorry, if
this were an I/O two volume

400
00:16:45,320 --> 00:16:47,550
instead of a GP3 volume,
you can sort of predict

401
00:16:47,550 --> 00:16:48,383
what you'd see.

402
00:16:48,383 --> 00:16:50,280
You'd see that whole
pulse skewed to the left

403
00:16:50,280 --> 00:16:53,370
because it's faster, and you
would see the width narrow

404
00:16:53,370 --> 00:16:55,500
because it's a more compliant system.

405
00:16:55,500 --> 00:16:57,663
That's just a different lived experience.

406
00:16:59,670 --> 00:17:01,830
Now, when you have a graph like this,

407
00:17:01,830 --> 00:17:03,210
it's great to track it over time.

408
00:17:03,210 --> 00:17:05,130
You know about what your system's doing.

409
00:17:05,130 --> 00:17:07,320
You can track how that's
changing over time.

410
00:17:07,320 --> 00:17:09,960
When you start to have
a performance problem,

411
00:17:09,960 --> 00:17:12,240
what that will show up as is a new mode,

412
00:17:12,240 --> 00:17:16,470
a new bump, usually towards the side

413
00:17:16,470 --> 00:17:17,490
that we care about there, that right,

414
00:17:17,490 --> 00:17:20,040
that I guess would be
left side for you, right?

415
00:17:20,040 --> 00:17:23,670
In the high latency band,
when you see a bump there,

416
00:17:23,670 --> 00:17:26,343
that's a bunch of I/Os that
are delayed by something.

417
00:17:29,430 --> 00:17:31,290
So I wanna talk a little bit about

418
00:17:31,290 --> 00:17:34,620
what causes those latency
outliers that we see

419
00:17:34,620 --> 00:17:36,633
and how we handle them.

420
00:17:37,560 --> 00:17:39,480
Before I do that, I do
wanna point out this.

421
00:17:39,480 --> 00:17:42,300
So this is actually EBS NVMe output.

422
00:17:42,300 --> 00:17:45,930
The tool that you can
use to grab it is there.

423
00:17:45,930 --> 00:17:48,440
That's the tool I use to grab this.

424
00:17:48,440 --> 00:17:53,370
We also have CloudWatch metrics

425
00:17:53,370 --> 00:17:56,970
with this histogram now
built in on Nitro instances,

426
00:17:56,970 --> 00:17:59,730
but you can also use
basically any tool to monitor.

427
00:17:59,730 --> 00:18:01,980
We use IOstat extensively.

428
00:18:01,980 --> 00:18:03,900
Any tool that monitors performance

429
00:18:03,900 --> 00:18:06,720
you can use to build
this kind of histogram.

430
00:18:06,720 --> 00:18:09,210
So let's talk about what
causes these outliers.

431
00:18:09,210 --> 00:18:12,690
I have to go in to how SSDs are built.

432
00:18:12,690 --> 00:18:15,450
I'm gonna do it on a very shallow level.

433
00:18:15,450 --> 00:18:17,070
I can talk for an hour about this.

434
00:18:17,070 --> 00:18:18,780
If you wanna learn more about SSDs

435
00:18:18,780 --> 00:18:19,830
and programmer race cycles,

436
00:18:19,830 --> 00:18:22,590
I would love to chat with
you outside this session.

437
00:18:22,590 --> 00:18:23,940
But essentially your SSD,

438
00:18:23,940 --> 00:18:27,030
whether it's your phone or
your laptop or EBS storage,

439
00:18:27,030 --> 00:18:29,430
it's a physical device.

440
00:18:29,430 --> 00:18:30,320
It's a real disk.

441
00:18:30,320 --> 00:18:34,200
It's made up of blocks and
those blocks have cells.

442
00:18:34,200 --> 00:18:36,810
Inside the cells, there
are little MOSFET gates

443
00:18:36,810 --> 00:18:38,640
that physically trap electrons.

444
00:18:38,640 --> 00:18:39,753
They're super cool.

445
00:18:41,310 --> 00:18:43,620
But because they're a physical device,

446
00:18:43,620 --> 00:18:46,680
they're subject to the cruel
whims of the physical world,

447
00:18:46,680 --> 00:18:48,120
which means things break.

448
00:18:48,120 --> 00:18:49,260
Those little gates can break.

449
00:18:49,260 --> 00:18:50,910
And when they break,

450
00:18:50,910 --> 00:18:53,250
there's a bunch of failure
modes for a physical disk.

451
00:18:53,250 --> 00:18:54,780
But one of them is that it can find

452
00:18:54,780 --> 00:18:57,903
that it's unable to
reprogram one of those gates.

453
00:18:58,920 --> 00:18:59,753
And when it does,

454
00:18:59,753 --> 00:19:01,350
when it finds that,

455
00:19:01,350 --> 00:19:02,610
it'll retry a few times,

456
00:19:02,610 --> 00:19:05,130
but then it'll declare,
"Oh, I've got a dead cell."

457
00:19:05,130 --> 00:19:06,750
And what the disk has
to do with a dead cell

458
00:19:06,750 --> 00:19:09,060
is it's gonna have to move
information out of that block,

459
00:19:09,060 --> 00:19:10,740
mark the block as dead,

460
00:19:10,740 --> 00:19:12,840
and go onto a new block.

461
00:19:12,840 --> 00:19:15,870
Now, if I pick a random
server in our fleet,

462
00:19:15,870 --> 00:19:18,270
the odds are it doesn't have
any of these dead blocks.

463
00:19:18,270 --> 00:19:20,700
But if I picked a couple
servers, I would see a few.

464
00:19:20,700 --> 00:19:23,190
It's normal, it's healthy, it's fine.

465
00:19:23,190 --> 00:19:24,270
But when that happens,

466
00:19:24,270 --> 00:19:27,090
when the disk has to
take on the extra work

467
00:19:27,090 --> 00:19:29,310
of moving the data from one block

468
00:19:29,310 --> 00:19:31,290
that's been damaged to another,

469
00:19:31,290 --> 00:19:33,030
we will see latency events

470
00:19:33,030 --> 00:19:35,520
on the order of 10 to 100 microseconds.

471
00:19:35,520 --> 00:19:36,870
They can last a little while

472
00:19:36,870 --> 00:19:39,723
as the disk works through that workload.

473
00:19:41,280 --> 00:19:42,570
It's unavoidable.

474
00:19:42,570 --> 00:19:44,640
This is just a fact of
living with storage.

475
00:19:44,640 --> 00:19:45,930
You have those outliers.

476
00:19:45,930 --> 00:19:47,400
And if I had one or two of those,

477
00:19:47,400 --> 00:19:50,400
those could be consistent
with a GP3 experience.

478
00:19:50,400 --> 00:19:51,660
But as a general rule,

479
00:19:51,660 --> 00:19:54,240
I don't wanna just pass
those upward to the customer

480
00:19:54,240 --> 00:19:58,830
because GP3 as a product
has its own experience.

481
00:19:58,830 --> 00:20:00,690
You would be very unsatisfied with me

482
00:20:00,690 --> 00:20:03,240
if I said, "Well, I just failed, so."

483
00:20:03,240 --> 00:20:06,303
My job is to present you
GP3, not physical disks.

484
00:20:08,010 --> 00:20:09,440
So what we do is we monitor

485
00:20:09,440 --> 00:20:12,060
via those same latency histograms,

486
00:20:12,060 --> 00:20:13,860
those same throughput metrics.

487
00:20:13,860 --> 00:20:15,660
We monitor this very actively.

488
00:20:15,660 --> 00:20:17,610
And we have thresholds that tell us

489
00:20:17,610 --> 00:20:20,460
when I've got a drive that is behaving

490
00:20:20,460 --> 00:20:22,510
outside of the normal operating envelope.

491
00:20:23,960 --> 00:20:27,180
What I do is basically
the same as the disk

492
00:20:27,180 --> 00:20:29,010
or an analogous behavior to the disk.

493
00:20:29,010 --> 00:20:30,840
When I see this misbehavior,

494
00:20:30,840 --> 00:20:33,870
I route your data off to another server.

495
00:20:33,870 --> 00:20:35,760
I take that server in for repair,

496
00:20:35,760 --> 00:20:38,210
or sometimes it just needs
a rest like all of us.

497
00:20:40,360 --> 00:20:42,480
Effectively, by using that same technique

498
00:20:42,480 --> 00:20:46,470
that the disk uses, I can
build a higher performance,

499
00:20:46,470 --> 00:20:48,870
more stable performance product

500
00:20:48,870 --> 00:20:51,093
out of less performant
underlying hardware.

501
00:20:52,590 --> 00:20:54,450
And that's a tool that works
all the way up the stack,

502
00:20:54,450 --> 00:20:57,123
even on services built on top of EBS.

503
00:20:59,620 --> 00:21:03,570
Now, to get it, that's a
little high level, I admit.

504
00:21:03,570 --> 00:21:04,980
To get into the details,

505
00:21:04,980 --> 00:21:06,360
we have to face another one of those

506
00:21:06,360 --> 00:21:09,810
really hard choices around storage.

507
00:21:09,810 --> 00:21:12,330
There are no, there's no one answer here.

508
00:21:12,330 --> 00:21:14,730
It's as varied as there
are different workloads.

509
00:21:16,500 --> 00:21:20,460
So let's just thought experiment.

510
00:21:20,460 --> 00:21:23,670
You get a request that
goes to some EBS server.

511
00:21:23,670 --> 00:21:24,990
The EBS server writes it down.

512
00:21:24,990 --> 00:21:26,540
Let's say it's a write request.

513
00:21:27,780 --> 00:21:30,780
And then EBS is replicated,
that shouldn't be a surprise,

514
00:21:30,780 --> 00:21:33,600
for reliability, durability purposes.

515
00:21:33,600 --> 00:21:34,530
We reach out to the peer,

516
00:21:34,530 --> 00:21:37,320
we say we'd like you to write this data.

517
00:21:37,320 --> 00:21:38,823
And I don't hear back.

518
00:21:39,660 --> 00:21:42,810
I don't hear back for 500 microseconds,

519
00:21:42,810 --> 00:21:45,510
which isn't that much time,
it's like really short.

520
00:21:45,510 --> 00:21:49,140
But for a computer system,
it's actually a long time.

521
00:21:49,140 --> 00:21:50,040
So what do you do?

522
00:21:51,700 --> 00:21:54,690
And most of your day one
EBS engineers will say,

523
00:21:54,690 --> 00:21:57,150
well, you could retry, you could time out,

524
00:21:57,150 --> 00:21:58,780
and you could try again.

525
00:21:58,780 --> 00:22:01,143
And that's not necessarily
the wrong answer.

526
00:22:02,160 --> 00:22:03,753
At EBS, it's the wrong answer.

527
00:22:04,710 --> 00:22:07,320
Because we are a very
latency sensitive product.

528
00:22:07,320 --> 00:22:10,200
We wanna hold that latency
bar as tightly as we can.

529
00:22:10,200 --> 00:22:11,580
Retry is usually the wrong answer.

530
00:22:11,580 --> 00:22:14,610
Usually when a host is
telling you that it's slow,

531
00:22:14,610 --> 00:22:16,680
it's because it's gonna be slow.

532
00:22:16,680 --> 00:22:20,370
And the right answer is
to move past that host.

533
00:22:20,370 --> 00:22:22,370
Find a new friend, write the data there.

534
00:22:23,760 --> 00:22:25,320
Think about read requests too.

535
00:22:25,320 --> 00:22:27,840
One of the tools, so retry is valid.

536
00:22:27,840 --> 00:22:30,990
It's valid, I shy away from it,

537
00:22:30,990 --> 00:22:31,920
from a performance perspective,

538
00:22:31,920 --> 00:22:33,690
'cause I care a lot about latency.

539
00:22:33,690 --> 00:22:36,180
I would rather use the
throughput of my system

540
00:22:36,180 --> 00:22:38,823
to move the data away
from a misbehaving peer.

541
00:22:40,050 --> 00:22:43,740
One technique we use a lot
of is read and write hedging.

542
00:22:43,740 --> 00:22:46,050
So imagine a different scenario

543
00:22:46,050 --> 00:22:47,790
where the customer issues a read request.

544
00:22:47,790 --> 00:22:48,900
They want their data.

545
00:22:48,900 --> 00:22:50,970
I send that read down to disk.

546
00:22:50,970 --> 00:22:52,620
The disk is slow.

547
00:22:52,620 --> 00:22:55,170
Takes a few hundred
microseconds to come back.

548
00:22:55,170 --> 00:22:57,120
It's longer than I expect.

549
00:22:57,120 --> 00:22:59,820
I can proactively issue
another read in parallel

550
00:22:59,820 --> 00:23:01,260
to the peer that has the data.

551
00:23:01,260 --> 00:23:04,260
Say, I don't know if
my disk is coming back.

552
00:23:04,260 --> 00:23:05,700
It seems slightly delayed,

553
00:23:05,700 --> 00:23:07,590
but I'm just gonna go grab
the data somewhere else.

554
00:23:07,590 --> 00:23:09,990
And I'll take whichever
answer comes back first.

555
00:23:11,100 --> 00:23:13,740
So read and write hedging
are highly valuable.

556
00:23:13,740 --> 00:23:18,240
I can't tell you what the right
answer for your system is,

557
00:23:18,240 --> 00:23:21,330
except to say that these latency outliers

558
00:23:21,330 --> 00:23:24,510
are a fact of life in all
storage systems at any level.

559
00:23:24,510 --> 00:23:25,530
Now, we could build,

560
00:23:25,530 --> 00:23:28,530
I could build you the system
where this doesn't happen.

561
00:23:28,530 --> 00:23:29,700
It's essentially I/O 2.

562
00:23:29,700 --> 00:23:31,140
It's more expensive, right?

563
00:23:31,140 --> 00:23:34,590
But a well-optimized system
that suits most workloads,

564
00:23:34,590 --> 00:23:37,383
which is GP3, is gonna
have these outliers.

565
00:23:38,840 --> 00:23:40,800
What you can do,

566
00:23:40,800 --> 00:23:44,580
there's a system called AWS
Fault Injection Service.

567
00:23:44,580 --> 00:23:48,243
And this year, we added EBS
Latency Event Injection.

568
00:23:49,200 --> 00:23:52,200
So you can fire up Fault Injection Service

569
00:23:52,200 --> 00:23:53,700
on your own system.

570
00:23:53,700 --> 00:23:54,990
Actually, Sapna's gonna talk about

571
00:23:54,990 --> 00:23:58,020
how to get a test environment
out of a production system

572
00:23:58,020 --> 00:24:00,210
where this is a great use case

573
00:24:00,210 --> 00:24:02,640
for firing up some fault injection

574
00:24:02,640 --> 00:24:05,583
to see how your system behaves
to those latency outliers.

575
00:24:08,190 --> 00:24:10,440
Most systems will just tolerate them,

576
00:24:10,440 --> 00:24:12,060
especially in the write path.

577
00:24:12,060 --> 00:24:13,200
You issue a write.

578
00:24:13,200 --> 00:24:14,250
The write comes back later.

579
00:24:14,250 --> 00:24:15,720
A little bit of latency is fine.

580
00:24:15,720 --> 00:24:16,770
Sometimes when you're in

581
00:24:16,770 --> 00:24:18,750
a more transactional database mode,

582
00:24:18,750 --> 00:24:20,800
that write is the commit block

583
00:24:20,800 --> 00:24:22,680
with a bunch of I/O behind it,

584
00:24:22,680 --> 00:24:24,930
and you get what's called
head-of-line blocking.

585
00:24:24,930 --> 00:24:26,670
That's a big performance issue.

586
00:24:26,670 --> 00:24:29,880
So understanding how your system behaves

587
00:24:29,880 --> 00:24:31,300
to those kind of latency outliers

588
00:24:31,300 --> 00:24:33,700
is really critical in determining

589
00:24:33,700 --> 00:24:36,090
how sensitive you are to
latency and what you do

590
00:24:36,090 --> 00:24:37,660
if you issue some kind of request hedging

591
00:24:37,660 --> 00:24:39,140
or giving up on a peer

592
00:24:39,140 --> 00:24:41,583
when that happens on your storage system.

593
00:24:42,900 --> 00:24:45,123
So let me wrap all this up.

594
00:24:46,000 --> 00:24:50,253
We talked about coupled
versus uncoupled scaling,

595
00:24:52,110 --> 00:24:54,720
building your system to
move in multiple dimensions

596
00:24:54,720 --> 00:24:56,280
and optimize each of them,

597
00:24:56,280 --> 00:24:59,700
so optimizing in the dimension
of IOPS or throughput,

598
00:24:59,700 --> 00:25:02,370
optimizing in the dimension of capacity.

599
00:25:02,370 --> 00:25:05,190
We talked about using queue
depth to control latency

600
00:25:05,190 --> 00:25:07,083
and manipulate that performance,

601
00:25:07,950 --> 00:25:09,600
either bringing the performance up

602
00:25:09,600 --> 00:25:11,740
if the system is underfed
with a small queue

603
00:25:11,740 --> 00:25:13,320
or bringing it down.

604
00:25:13,320 --> 00:25:15,670
Shorter queues give you
a faster response time.

605
00:25:16,540 --> 00:25:20,310
We talked about monitoring
latency using a histogram,

606
00:25:20,310 --> 00:25:23,310
some of the new CloudWatch
metrics we have,

607
00:25:23,310 --> 00:25:27,183
and NVMe CLI, which is another
tool on the Nitro instances.

608
00:25:28,260 --> 00:25:31,053
And we talked about using
the latency response,

609
00:25:32,400 --> 00:25:34,740
the response of your system
to high latency, rather,

610
00:25:34,740 --> 00:25:35,820
choosing that really carefully

611
00:25:35,820 --> 00:25:38,103
and testing it with
fault injection service.

612
00:25:38,980 --> 00:25:43,980
How your system responds to
those different behaviors

613
00:25:44,190 --> 00:25:46,230
is what defines the lived experience.

614
00:25:46,230 --> 00:25:48,090
I keep saying lived experience, right?

615
00:25:48,090 --> 00:25:50,220
It's that lived experience that gets you

616
00:25:50,220 --> 00:25:53,820
the GP3 volume type or
the IO2 volume type.

617
00:25:53,820 --> 00:25:55,940
That's how we build that product

618
00:25:55,940 --> 00:25:58,683
separate from the underlying medium.

619
00:25:59,640 --> 00:26:01,860
And that's also how, when
we're dealing with these,

620
00:26:01,860 --> 00:26:03,750
we have big surges in our workloads, too.

621
00:26:03,750 --> 00:26:07,180
We had Prime Day just
recently hit new peaks for EBS

622
00:26:07,180 --> 00:26:10,170
and through it all, we
can maintain our SLAs

623
00:26:10,170 --> 00:26:13,260
of EBS performance.

624
00:26:13,260 --> 00:26:18,120
So this has all been workloads
in a very static sense,

625
00:26:18,120 --> 00:26:19,410
fixed workloads.

626
00:26:19,410 --> 00:26:22,650
We're gonna talk next about
how workloads change over time.

627
00:26:22,650 --> 00:26:24,063
I'm gonna hand off to Sapna.

628
00:26:27,870 --> 00:26:29,490
- Thank you, Doug.
- Thanks.

629
00:26:30,480 --> 00:26:33,060
- I love his passion for block storage

630
00:26:33,060 --> 00:26:36,580
and the way he explains
some of the fundamentals

631
00:26:36,580 --> 00:26:39,690
of block storage is pretty exciting.

632
00:26:39,690 --> 00:26:43,500
All right, let's dive deep into

633
00:26:43,500 --> 00:26:45,933
how do you evolve your volume?

634
00:26:48,210 --> 00:26:50,460
I'm gonna take a step back first.

635
00:26:50,460 --> 00:26:54,630
Some of you already know
what is the right performance

636
00:26:54,630 --> 00:26:57,540
for you for your specific use case,

637
00:26:57,540 --> 00:26:59,550
but some of you may actually not know

638
00:26:59,550 --> 00:27:02,470
or may not be sure about what is the right

639
00:27:04,760 --> 00:27:06,990
performance place for you.

640
00:27:06,990 --> 00:27:09,030
And that is where there are two options.

641
00:27:09,030 --> 00:27:12,390
One, you can start with
higher performing volume

642
00:27:12,390 --> 00:27:14,010
and then you can scale it down

643
00:27:14,010 --> 00:27:16,980
as you learn more about
your volume and the use case

644
00:27:16,980 --> 00:27:18,460
and the performance that you need

645
00:27:18,460 --> 00:27:22,140
or you can start with
a tighter performance.

646
00:27:22,140 --> 00:27:25,290
For example, GP3 baseline performance,

647
00:27:25,290 --> 00:27:27,450
you can start there
and you can scale it up

648
00:27:27,450 --> 00:27:30,640
as your volume demand grows.

649
00:27:30,640 --> 00:27:34,770
Dutch talked about how can
you measure the performance,

650
00:27:34,770 --> 00:27:36,900
went into some of the details of latency.

651
00:27:36,900 --> 00:27:39,450
So all of those data points
can help you learn more

652
00:27:39,450 --> 00:27:41,880
about what is the right IOPS throughput

653
00:27:41,880 --> 00:27:44,253
that you need for your specific use case.

654
00:27:46,530 --> 00:27:48,300
GP3 is pretty impressive,

655
00:27:48,300 --> 00:27:52,920
especially with our recent
launch of higher limits.

656
00:27:52,920 --> 00:27:56,430
It gives you a pretty broad spectrum.

657
00:27:56,430 --> 00:27:59,280
For example, data size,
you can start pretty small,

658
00:27:59,280 --> 00:28:03,063
one TB and then you can
go up to 64 terabyte.

659
00:28:04,350 --> 00:28:09,040
On-demand power, now you
can go up to 80,000 IOPS

660
00:28:09,040 --> 00:28:13,320
and if you need that only
for a short amount of time,

661
00:28:13,320 --> 00:28:17,460
for example, just for a
few days or a few months,

662
00:28:17,460 --> 00:28:18,750
you can leave it till then

663
00:28:18,750 --> 00:28:21,630
and then you can scale back
down when you don't need it.

664
00:28:21,630 --> 00:28:24,210
That way you're not
paying for the performance

665
00:28:24,210 --> 00:28:26,310
that you're not utilizing.

666
00:28:26,310 --> 00:28:29,180
Striping is a technique that
some of our customers use

667
00:28:29,180 --> 00:28:34,180
to get the right data size
and also the performance.

668
00:28:34,650 --> 00:28:38,040
But with GP3 larger and higher limits,

669
00:28:38,040 --> 00:28:40,080
you don't have to worry
about the complexity

670
00:28:40,080 --> 00:28:41,790
behind striping.

671
00:28:41,790 --> 00:28:44,110
You can reduce and minimize that overhead

672
00:28:45,020 --> 00:28:49,383
by just increasing independently
your IOPS and throughput.

673
00:28:51,460 --> 00:28:55,020
So let's get into what is the mechanism

674
00:28:55,020 --> 00:28:58,680
for you to evolve or modify your volume.

675
00:28:58,680 --> 00:29:01,830
EV is the way to go.

676
00:29:01,830 --> 00:29:06,360
It basically allows you to
dynamically increase your size,

677
00:29:06,360 --> 00:29:09,540
your IOPS and throughput independently.

678
00:29:09,540 --> 00:29:11,910
There are three actions
that you can change,

679
00:29:11,910 --> 00:29:13,650
that you can take.

680
00:29:13,650 --> 00:29:16,050
First one, change volume.

681
00:29:16,050 --> 00:29:19,440
What if you just want to
migrate from GP2 to GP3

682
00:29:19,440 --> 00:29:22,050
or GP3 to IO2?

683
00:29:22,050 --> 00:29:24,280
You can do that using EV.

684
00:29:24,280 --> 00:29:28,053
You can also tune your performance
within the GP3 spectrum.

685
00:29:28,940 --> 00:29:33,600
You can move from 16,000
IOPS to 80,000 for example.

686
00:29:33,600 --> 00:29:36,390
Size, I want you to be a
little cautious about it

687
00:29:36,390 --> 00:29:38,280
because size you can only scale up,

688
00:29:38,280 --> 00:29:40,050
you cannot come back down.

689
00:29:40,050 --> 00:29:45,050
So if you really need
higher size of your volume,

690
00:29:45,090 --> 00:29:48,183
please keep that in mind that
you cannot come back down.

691
00:29:51,120 --> 00:29:52,710
So what are the best practices?

692
00:29:52,710 --> 00:29:54,990
What are the different
steps that you need to take

693
00:29:54,990 --> 00:29:58,650
to actually request for a
modification of your volume?

694
00:29:58,650 --> 00:30:02,760
One snapshot, for some of
you who may not be aware

695
00:30:02,760 --> 00:30:03,960
of what snapshot means,

696
00:30:03,960 --> 00:30:07,500
is a point in time copy of your volume.

697
00:30:07,500 --> 00:30:10,050
And you can restore your volume

698
00:30:10,050 --> 00:30:11,613
from that specific snapshot.

699
00:30:13,090 --> 00:30:15,510
Once you take the snapshot,

700
00:30:15,510 --> 00:30:17,730
there are different ways of requesting

701
00:30:17,730 --> 00:30:19,620
the modification of your volume

702
00:30:19,620 --> 00:30:22,260
and you can ask for GP2 to GP3

703
00:30:22,260 --> 00:30:25,290
or within GP3 you can
ask for different limits.

704
00:30:25,290 --> 00:30:28,080
After that, you can monitor the progress

705
00:30:28,080 --> 00:30:29,250
of your modification.

706
00:30:29,250 --> 00:30:31,233
I'll show how in the next slide.

707
00:30:32,810 --> 00:30:36,300
Number four, I want you
to be cautious about,

708
00:30:36,300 --> 00:30:39,900
sometimes EC2 instances don't recognize,

709
00:30:39,900 --> 00:30:41,640
especially the size change,

710
00:30:41,640 --> 00:30:43,500
don't recognize your size change.

711
00:30:43,500 --> 00:30:47,910
And for example, if you are
using 100 gigabyte of volume

712
00:30:47,910 --> 00:30:50,630
and then you increase the size to 200,

713
00:30:50,630 --> 00:30:53,580
if EC2 does not know about that,

714
00:30:53,580 --> 00:30:54,810
you're paying for 200,

715
00:30:54,810 --> 00:30:58,110
but actually only utilizing 100 gigabyte.

716
00:30:58,110 --> 00:31:01,370
So extending file system helps to sync

717
00:31:01,370 --> 00:31:04,863
the physical disk space
with the EC2 instance.

718
00:31:07,350 --> 00:31:10,200
Here are a few methods using which

719
00:31:10,200 --> 00:31:12,573
you can request for modification.

720
00:31:13,770 --> 00:31:18,180
One is AWS CLI, PowerShell script.

721
00:31:18,180 --> 00:31:19,590
You can just go to your console

722
00:31:19,590 --> 00:31:21,360
and also monitor your status.

723
00:31:21,360 --> 00:31:22,830
You can see the volume state,

724
00:31:22,830 --> 00:31:24,090
your current volume state,

725
00:31:24,090 --> 00:31:27,270
and then the modification state.

726
00:31:27,270 --> 00:31:30,870
Optimizing basically
means we are transitioning

727
00:31:30,870 --> 00:31:33,213
to your new limits that
you have requested.

728
00:31:37,380 --> 00:31:40,830
Okay, so let's dive deep
into what is happening

729
00:31:40,830 --> 00:31:44,040
behind the scene when you're
requesting for a modification.

730
00:31:44,040 --> 00:31:47,430
Raise your hand if you're curious about

731
00:31:47,430 --> 00:31:48,740
what happens behind the scene

732
00:31:48,740 --> 00:31:52,650
when you're requesting, for example,

733
00:31:52,650 --> 00:31:55,120
higher IOPS or throughput.

734
00:31:55,120 --> 00:31:59,430
Okay, yeah, it's pretty
exciting to understand.

735
00:31:59,430 --> 00:32:01,830
And Dutch explained some of this already.

736
00:32:01,830 --> 00:32:04,203
I'm just gonna pick a specific scenario.

737
00:32:05,070 --> 00:32:07,233
We are running at a massive scale.

738
00:32:08,070 --> 00:32:10,380
We get hundreds of thousands of requests

739
00:32:10,380 --> 00:32:12,090
to modify the volume,

740
00:32:12,090 --> 00:32:15,340
and that is where we have to
check every single request

741
00:32:15,340 --> 00:32:19,140
to make sure where your
volume was initially placed,

742
00:32:19,140 --> 00:32:21,210
let's say server A, in this case,

743
00:32:21,210 --> 00:32:25,173
we do have the right capacity
to bump up your performance.

744
00:32:27,060 --> 00:32:29,370
And if not, then in that case,

745
00:32:29,370 --> 00:32:33,000
we replace your volume to the server,

746
00:32:33,000 --> 00:32:34,533
in this case, server B,

747
00:32:35,610 --> 00:32:38,310
so that you get the
performance that you need,

748
00:32:38,310 --> 00:32:40,503
you're able to get the size that you need.

749
00:32:41,490 --> 00:32:43,710
And this is pretty seamless, by the way,

750
00:32:43,710 --> 00:32:45,640
from customer point of view.

751
00:32:45,640 --> 00:32:48,000
I'm just sharing so that you understand

752
00:32:48,000 --> 00:32:49,713
what happens behind the scene.

753
00:32:54,390 --> 00:32:57,950
Okay, so let's talk about what
are the different boundaries

754
00:32:57,950 --> 00:33:01,560
and challenges associated with EV.

755
00:33:01,560 --> 00:33:05,270
Although we give you a wide range of GP3,

756
00:33:05,270 --> 00:33:08,910
in terms of size, in terms
of IOPS and throughput,

757
00:33:08,910 --> 00:33:10,590
there are certain limitations.

758
00:33:10,590 --> 00:33:12,453
I mentioned some of them already.

759
00:33:13,560 --> 00:33:15,360
One-way sizing.

760
00:33:15,360 --> 00:33:17,890
When you're requesting
to go from 100 gigabyte

761
00:33:17,890 --> 00:33:20,310
to 200 gigabyte, for example,

762
00:33:20,310 --> 00:33:23,310
you cannot come back to 100 gigabyte.

763
00:33:23,310 --> 00:33:24,510
There might be some workarounds,

764
00:33:24,510 --> 00:33:26,700
but directly, you cannot do so.

765
00:33:26,700 --> 00:33:28,713
So just be cognizant of that.

766
00:33:29,610 --> 00:33:31,310
The six-hour timer.

767
00:33:31,310 --> 00:33:35,160
Between two requests that you're making

768
00:33:35,160 --> 00:33:39,270
to modify your volume,
there's a six-hour wait time.

769
00:33:39,270 --> 00:33:44,100
And this is to make sure
your volume integrity

770
00:33:44,100 --> 00:33:48,630
consistent in performance,
all of that is intact.

771
00:33:48,630 --> 00:33:51,933
So we have kept this boundary
to make sure of that.

772
00:33:52,800 --> 00:33:56,820
The last one is more along the
lines of what I said before.

773
00:33:56,820 --> 00:34:00,300
Some of the older EC2
instances may not recognize,

774
00:34:00,300 --> 00:34:03,990
for example, the size that
you have requested for,

775
00:34:03,990 --> 00:34:06,330
and that is where you might need to reboot

776
00:34:06,330 --> 00:34:10,260
or run OS command to make
sure your EC2 instance

777
00:34:10,260 --> 00:34:13,353
is also recognizing the change
that you have requested.

778
00:34:16,620 --> 00:34:20,370
I love sharing customer success stories.

779
00:34:20,370 --> 00:34:24,030
We've talked about how can
you evolve your volume,

780
00:34:24,030 --> 00:34:26,010
what is the mechanism to do so,

781
00:34:26,010 --> 00:34:28,860
and now let's see one of the example

782
00:34:28,860 --> 00:34:31,500
where one of our customer,
VMware Carbon Black,

783
00:34:31,500 --> 00:34:34,880
they were actually able to
optimize their cost performance,

784
00:34:34,880 --> 00:34:36,573
leveraging GP3.

785
00:34:42,950 --> 00:34:47,520
Before I get into some of
the details of the scenario,

786
00:34:47,520 --> 00:34:50,193
just wanted to quickly
call out what they do,

787
00:34:51,290 --> 00:34:53,910
what's their scale, so
that you can connect

788
00:34:53,910 --> 00:34:56,640
with this success story and
may apply a similar principle

789
00:34:56,640 --> 00:34:58,593
and strategy to optimize the cost.

790
00:35:00,990 --> 00:35:05,880
VMware Carbon Black is a leading
industry in cybersecurity.

791
00:35:05,880 --> 00:35:09,090
They're focusing on endpoint detection,

792
00:35:09,090 --> 00:35:10,863
next-generation antivirus,

793
00:35:11,970 --> 00:35:15,843
they're handling millions
of requests every second.

794
00:35:17,490 --> 00:35:20,640
And to make sure their
customer reputation,

795
00:35:20,640 --> 00:35:23,670
they're giving the best
application experience

796
00:35:23,670 --> 00:35:26,850
to their customer,
latency is super important

797
00:35:26,850 --> 00:35:29,070
for this specific use case.

798
00:35:29,070 --> 00:35:30,840
They need high-speed volume,

799
00:35:30,840 --> 00:35:35,313
and that is where they are using EBS.

800
00:35:37,010 --> 00:35:40,050
Here is a pretty high-level architecture.

801
00:35:40,050 --> 00:35:43,200
They're using multiple services of AWS.

802
00:35:43,200 --> 00:35:44,820
I'm not going to get into the detail

803
00:35:44,820 --> 00:35:47,280
of every single service.

804
00:35:47,280 --> 00:35:51,430
My focus and this session focus is EBS.

805
00:35:51,430 --> 00:35:56,190
They're using EBS for high-speed volume.

806
00:35:56,190 --> 00:35:59,150
They're processing millions
of requests and records

807
00:35:59,150 --> 00:36:02,370
and sorting and compressing
within the volume

808
00:36:02,370 --> 00:36:04,470
using EKS cluster.

809
00:36:04,470 --> 00:36:09,170
And for this to work out,
they need to read and write

810
00:36:09,170 --> 00:36:13,260
the data pretty often,
and that is where IOPS

811
00:36:13,260 --> 00:36:15,310
and their performance is super important.

812
00:36:19,320 --> 00:36:21,840
So what was the challenge,
and how did they optimize

813
00:36:21,840 --> 00:36:24,000
and mitigate that challenge?

814
00:36:24,000 --> 00:36:28,260
Initially, they were using GP2,

815
00:36:28,260 --> 00:36:32,130
and like Dutch mentioned, GP2 performance

816
00:36:32,130 --> 00:36:33,660
and size is coupled.

817
00:36:33,660 --> 00:36:35,820
They just wanted 3,000 IOPS,

818
00:36:35,820 --> 00:36:37,890
and for them to get to 3,000 IOPS,

819
00:36:37,890 --> 00:36:39,620
they were stuck with two options.

820
00:36:39,620 --> 00:36:44,620
One is either they pay for
extra 960 gigabyte of storage

821
00:36:45,810 --> 00:36:48,240
and get the 3,000 IOPS,

822
00:36:48,240 --> 00:36:51,420
or they could use bursting performance,

823
00:36:51,420 --> 00:36:55,260
but that wouldn't guarantee
the consistent performance

824
00:36:55,260 --> 00:36:56,760
that they needed all the time.

825
00:36:57,630 --> 00:36:59,280
They went for the second option,

826
00:36:59,280 --> 00:37:01,863
and that is where they were struggling,

827
00:37:02,970 --> 00:37:06,420
getting throttled, and
because of throttling,

828
00:37:06,420 --> 00:37:09,480
they were also seeing that
their cluster scaled up

829
00:37:09,480 --> 00:37:12,630
unnecessary EC2 nodes.

830
00:37:12,630 --> 00:37:15,660
So let's see how did they
resolve this problem.

831
00:37:15,660 --> 00:37:17,820
I'm assuming it's pretty obvious by now.

832
00:37:17,820 --> 00:37:19,863
We've talked about GP3 quite a lot.

833
00:37:20,820 --> 00:37:25,080
They moved to GP3, and
they were able to get

834
00:37:25,080 --> 00:37:30,080
guaranteed 3,000 IOPS without
paying for unused storage.

835
00:37:31,770 --> 00:37:34,650
And with that migration,
they were directly able

836
00:37:34,650 --> 00:37:38,220
to save 20% cost in the storage.

837
00:37:38,220 --> 00:37:41,520
They were able to reduce
the number of EC2 instances

838
00:37:41,520 --> 00:37:43,053
by 50 count.

839
00:37:44,220 --> 00:37:48,300
Altogether, at least they were
able to save 25K per month,

840
00:37:48,300 --> 00:37:50,100
and now that number is pretty large.

841
00:37:53,490 --> 00:37:56,400
Here's the anecdote
directly from our customer.

842
00:37:56,400 --> 00:37:57,630
They also shared this graph.

843
00:37:57,630 --> 00:38:01,690
As you can see, the number of EC2 instance

844
00:38:02,610 --> 00:38:06,360
decreased by 50 count
after they migrated to GP3,

845
00:38:06,360 --> 00:38:09,453
and got the consistent
performance of 3,000 IOPS.

846
00:38:13,620 --> 00:38:18,620
This anecdote summarizes
that the migration

847
00:38:18,960 --> 00:38:22,590
really helped them, and
they were able to save

848
00:38:22,590 --> 00:38:25,470
several thousands of dollar per month.

849
00:38:25,470 --> 00:38:28,320
So this is one story
where one of our customer

850
00:38:28,320 --> 00:38:31,110
moved from GP2 to GP3,
and they were able to

851
00:38:31,110 --> 00:38:33,273
optimize price and performance.

852
00:38:34,290 --> 00:38:36,810
You can also do that within GP3 spectrum

853
00:38:36,810 --> 00:38:38,960
with the higher limits
that we offer today.

854
00:38:43,020 --> 00:38:45,720
Okay, so far, what we have talked about

855
00:38:45,720 --> 00:38:48,990
is in the context of primary environment.

856
00:38:48,990 --> 00:38:52,077
How do you improve, or how do you optimize

857
00:38:52,077 --> 00:38:54,900
the price and performance using GP3

858
00:38:54,900 --> 00:38:56,490
for your primary environment?

859
00:38:56,490 --> 00:38:59,160
But what about secondary environment

860
00:38:59,160 --> 00:39:01,203
that you might be using for dev testing?

861
00:39:02,420 --> 00:39:07,420
How do you think about
optimizing price and performance

862
00:39:07,500 --> 00:39:08,973
for those use cases?

863
00:39:12,280 --> 00:39:14,193
I'll share one scenario.

864
00:39:17,280 --> 00:39:22,280
Previously, when we did not
offer higher limits of GP3,

865
00:39:24,270 --> 00:39:26,850
our customers were stuck with IO2

866
00:39:26,850 --> 00:39:31,413
when they needed higher
IOPS, 16,000 for example.

867
00:39:32,340 --> 00:39:35,400
And in case of secondary environment,

868
00:39:35,400 --> 00:39:38,460
unless latency and
durability is super critical,

869
00:39:38,460 --> 00:39:42,140
you can actually now move from IO2 to GP3

870
00:39:42,140 --> 00:39:45,033
and pay much less for your dev testing.

871
00:39:47,640 --> 00:39:49,860
And that way, you're able to optimize cost

872
00:39:49,860 --> 00:39:51,870
and not paying premium for IO2

873
00:39:51,870 --> 00:39:56,163
when you're just experimenting
with your use cases.

874
00:39:57,270 --> 00:39:58,770
This is one scenario.

875
00:39:58,770 --> 00:40:01,080
Within, again, within
GP3 spectrum as well,

876
00:40:01,080 --> 00:40:03,330
you can do a lot more,
strategize how you want

877
00:40:03,330 --> 00:40:04,980
to optimize your dev environment.

878
00:40:06,660 --> 00:40:08,920
Now, similar to primary environment,

879
00:40:08,920 --> 00:40:11,670
I wanted to touch base on,

880
00:40:11,670 --> 00:40:13,620
how do you even make
a copy of your volume?

881
00:40:13,620 --> 00:40:15,670
How do you create a secondary environment

882
00:40:17,960 --> 00:40:20,430
without interrupting your primary

883
00:40:20,430 --> 00:40:21,873
production live environment?

884
00:40:22,740 --> 00:40:25,170
There are two methods to do so.

885
00:40:25,170 --> 00:40:27,660
One is snapshot.

886
00:40:27,660 --> 00:40:32,660
Snapshot, again, it's a point
in time copy of your volume.

887
00:40:32,730 --> 00:40:34,980
You take a snapshot and
then you can hydrate

888
00:40:34,980 --> 00:40:37,230
or restore your snapshot,

889
00:40:37,230 --> 00:40:39,753
your volume from the
snapshot that you have taken.

890
00:40:41,400 --> 00:40:44,547
Since we are talking about the cost

891
00:40:44,547 --> 00:40:46,080
and the price and performance,

892
00:40:46,080 --> 00:40:51,030
I wanted to educate you
on how EBS snapshot works

893
00:40:51,030 --> 00:40:54,900
and how behind the scene
we are trying to optimize

894
00:40:54,900 --> 00:40:56,580
that price and performance for you

895
00:40:56,580 --> 00:40:59,163
by avoiding duplicated data.

896
00:41:02,730 --> 00:41:07,560
State one, for example, you
have 10 gigabyte of data,

897
00:41:07,560 --> 00:41:09,960
10 gigabyte of volume.

898
00:41:09,960 --> 00:41:11,940
If there is no snapshot existing,

899
00:41:11,940 --> 00:41:14,223
we will first take the full snapshot.

900
00:41:15,060 --> 00:41:16,350
And state two,

901
00:41:16,350 --> 00:41:18,600
let's say you made a
change of four gigabyte.

902
00:41:19,740 --> 00:41:23,250
In that state, we are not going
to take the full snapshot.

903
00:41:23,250 --> 00:41:25,560
Again, we are only going
to take the snapshot

904
00:41:25,560 --> 00:41:30,270
of four gigabyte and
refer to the snapshot A

905
00:41:30,270 --> 00:41:33,270
for six gigabyte of data
that you have not touched.

906
00:41:33,270 --> 00:41:34,680
That way, we are not duplicating,

907
00:41:34,680 --> 00:41:38,523
you are not paying multiple
times for the full snapshot.

908
00:41:39,690 --> 00:41:42,060
State three, let's say now you have added

909
00:41:42,060 --> 00:41:45,480
two gigabyte of data to your volume.

910
00:41:45,480 --> 00:41:47,700
Now in state three, we
are only going to take

911
00:41:47,700 --> 00:41:49,230
the snapshot of two gigabyte,

912
00:41:49,230 --> 00:41:51,900
refer to snapshot B for four,

913
00:41:51,900 --> 00:41:54,540
and then refer to snapshot A for six.

914
00:41:54,540 --> 00:41:59,540
So you're only paying for
the incremental backup here.

915
00:41:59,560 --> 00:42:03,780
And that is what I wanted you to take away

916
00:42:03,780 --> 00:42:08,193
to make sure that we are
optimizing the cost for you here.

917
00:42:10,540 --> 00:42:12,840
This works even if you have hydrated,

918
00:42:12,840 --> 00:42:16,470
even if you have restored
your volume from the snapshot.

919
00:42:16,470 --> 00:42:18,660
In this scenario, for example,

920
00:42:18,660 --> 00:42:21,330
if you have a 10 gigabyte of volume

921
00:42:21,330 --> 00:42:25,230
and you have taken the
snapshot and you have hydrated,

922
00:42:25,230 --> 00:42:28,680
you have restored your
volume, let's say volume two,

923
00:42:28,680 --> 00:42:31,720
and then you have made some
changes of four gigabyte.

924
00:42:31,720 --> 00:42:34,530
In that scenario, we are only taking

925
00:42:34,530 --> 00:42:36,780
the snapshot for four gigabyte,

926
00:42:36,780 --> 00:42:38,910
and we are maintaining your lineage

927
00:42:38,910 --> 00:42:42,810
and referring to snapshot
A for the 10 gigabyte

928
00:42:42,810 --> 00:42:45,303
of data that you have not touched.

929
00:42:49,860 --> 00:42:52,560
All right, so some of
you might be thinking,

930
00:42:52,560 --> 00:42:54,900
okay, snapshot, we take a snapshot

931
00:42:54,900 --> 00:42:56,820
and then hydrate or restore.

932
00:42:56,820 --> 00:42:58,440
There are two steps involved here.

933
00:42:58,440 --> 00:43:02,090
What if you want to
optimize the performance

934
00:43:02,090 --> 00:43:05,130
of restoring, of hydrating your volume?

935
00:43:05,130 --> 00:43:06,990
Sometimes you may want immediately

936
00:43:06,990 --> 00:43:09,540
because you have critical use cases.

937
00:43:09,540 --> 00:43:12,990
So we offer three options here.

938
00:43:12,990 --> 00:43:14,250
Depending on your use case,

939
00:43:14,250 --> 00:43:19,050
how critical your application
is, you can pick and choose.

940
00:43:19,050 --> 00:43:21,690
For you, if you care more about cost,

941
00:43:21,690 --> 00:43:24,330
you can go with the standard restore.

942
00:43:24,330 --> 00:43:26,630
It's a free tier, you
don't have to pay extra.

943
00:43:27,720 --> 00:43:31,293
But if you need predictable performance,

944
00:43:32,490 --> 00:43:35,990
how soon you can recover the volume,

945
00:43:35,990 --> 00:43:39,630
or if you need to see
your volume immediately,

946
00:43:39,630 --> 00:43:42,600
then we have two option, provisioned rate.

947
00:43:42,600 --> 00:43:44,520
You can pick the speed at

948
00:43:44,520 --> 00:43:46,800
which your volume is provisioning.

949
00:43:46,800 --> 00:43:50,640
And then also FSR, you get
instant access to your volume.

950
00:43:50,640 --> 00:43:52,260
Those two options you have to pay,

951
00:43:52,260 --> 00:43:53,760
and that is where you have to think about

952
00:43:53,760 --> 00:43:56,583
the strategy of optimizing
cost and performance.

953
00:44:00,870 --> 00:44:03,900
All right, especially
for the dev test use case

954
00:44:03,900 --> 00:44:08,210
where you just need to
create a copy of your volume,

955
00:44:08,210 --> 00:44:10,293
there are certain limitations,

956
00:44:11,280 --> 00:44:14,343
especially if you need an
instant copy of your volume.

957
00:44:15,410 --> 00:44:20,070
Like I mentioned before,
snapshot have at least two steps.

958
00:44:20,070 --> 00:44:21,570
You need to first create the snapshot,

959
00:44:21,570 --> 00:44:23,610
and then you need to restore your volume

960
00:44:23,610 --> 00:44:25,920
from that specific snapshot.

961
00:44:25,920 --> 00:44:28,380
Your copy may not be instantly available

962
00:44:28,380 --> 00:44:30,753
depending on which option you're picking.

963
00:44:31,650 --> 00:44:33,333
Restore is definitely required.

964
00:44:40,590 --> 00:44:44,530
So this is where we have
recently launched clones.

965
00:44:44,530 --> 00:44:47,390
If you're looking to instantly
just copy your volume

966
00:44:47,390 --> 00:44:50,610
and want to just start with the testing,

967
00:44:50,610 --> 00:44:52,917
one click of a button, you can do so.

968
00:44:52,917 --> 00:44:57,917
One API, instant point-in-time
copy, no restore required.

969
00:45:01,380 --> 00:45:05,313
So how does this clone works?

970
00:45:07,140 --> 00:45:08,823
We have EC2 Instant.

971
00:45:10,110 --> 00:45:12,420
There's also a volume attached to it.

972
00:45:12,420 --> 00:45:14,523
Let's say in this scenario, IO2.

973
00:45:15,720 --> 00:45:17,130
And if you want to,
like I mentioned before,

974
00:45:17,130 --> 00:45:20,010
if you want to optimize the cost here,

975
00:45:20,010 --> 00:45:23,910
you can actually choose GP3

976
00:45:23,910 --> 00:45:26,550
when you're copying your volume.

977
00:45:26,550 --> 00:45:29,520
And that way you're not paying
for the premium IO2 volume.

978
00:45:29,520 --> 00:45:32,160
Instead, you're just paying for the GP3,

979
00:45:32,160 --> 00:45:33,260
which is much cheaper.

980
00:45:34,820 --> 00:45:37,140
On the console, there's one option.

981
00:45:37,140 --> 00:45:38,100
You can click off a button.

982
00:45:38,100 --> 00:45:40,154
You can just create copy.

983
00:45:40,154 --> 00:45:42,360
(audience applauds)

984
00:45:42,360 --> 00:45:47,360
All right, we have reached
towards the end of our session.

985
00:45:47,400 --> 00:45:49,270
I'm just gonna quickly summarize

986
00:45:50,220 --> 00:45:52,270
the last two topics that we talked about.

987
00:45:53,130 --> 00:45:56,520
GP3 gives you a pretty large range

988
00:45:56,520 --> 00:46:00,270
of size, IOPS, and throughput.

989
00:46:00,270 --> 00:46:05,270
With that and the EV, you
can evolve your volume.

990
00:46:05,660 --> 00:46:09,740
You can migrate seamlessly
from GP2 to GP3.

991
00:46:09,740 --> 00:46:13,230
You can fine-tune, right-size your volume

992
00:46:13,230 --> 00:46:15,900
depending on your use case
and what you've learned

993
00:46:15,900 --> 00:46:20,013
about your data by monitoring performance.

994
00:46:20,940 --> 00:46:22,620
You can modernize your architecture.

995
00:46:22,620 --> 00:46:27,620
Now we offer up to 64 terabyte
of size limit with GP3.

996
00:46:28,860 --> 00:46:32,910
And the great thing about EV and GP3

997
00:46:32,910 --> 00:46:35,190
is we are doing this in place.

998
00:46:35,190 --> 00:46:37,740
You don't have to worry
about stopping your instance.

999
00:46:37,740 --> 00:46:41,080
You don't have to worry
about detaching your volume.

1000
00:46:41,080 --> 00:46:43,290
All of that is happening behind the scene.

1001
00:46:43,290 --> 00:46:46,320
For you, the live and
production data volume

1002
00:46:46,320 --> 00:46:48,153
is not interrupted.

1003
00:46:54,030 --> 00:46:58,743
We have about 11 sessions
at this re:Invent lined up.

1004
00:47:00,120 --> 00:47:02,040
We're done with four of them,

1005
00:47:02,040 --> 00:47:05,310
so I would highly encourage
you to look at other sessions

1006
00:47:05,310 --> 00:47:09,300
if you want to learn more
about EBS and EBS snapshots.

1007
00:47:09,300 --> 00:47:12,270
Snapshots in particular,
if you want to learn more,

1008
00:47:12,270 --> 00:47:15,120
we have STG325.

1009
00:47:15,120 --> 00:47:18,360
We have got STG326.

1010
00:47:18,360 --> 00:47:23,280
I'm co-presenting STG406 on Wednesday

1011
00:47:23,280 --> 00:47:26,330
if you're interested in
having a hands-on experience

1012
00:47:26,330 --> 00:47:29,220
and learn more about EBS snapshots,

1013
00:47:29,220 --> 00:47:31,173
you can come join us there.

1014
00:47:33,810 --> 00:47:38,640
With that, thanks a lot
for staying back in here.

1015
00:47:38,640 --> 00:47:41,430
We would love to hear your feedback.

1016
00:47:41,430 --> 00:47:44,760
This is how we evolve our content

1017
00:47:44,760 --> 00:47:46,770
and make sure it's relevant.

1018
00:47:46,770 --> 00:47:49,950
So please take the survey

1019
00:47:49,950 --> 00:47:52,370
and let us know how you like the session

1020
00:47:52,370 --> 00:47:55,290
and enjoy the rest of your re:Invent.

1021
00:47:55,290 --> 00:47:56,123
Thank you.

