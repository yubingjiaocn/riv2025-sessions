# AWS re:Invent 2025 会议总结：安全团队如何拥抱生成式AI

## 会议概述

本次会议由Amazon安全团队的杰出工程师Eric Branwine主讲，他在Amazon工作了18年，其中15年在安全团队。会议的核心主题是安全团队如何在生成式AI浪潮中保持相关性并有效支持业务发展。

Eric首先坦承，当生成式AI出现时，安全团队并没有一夜之间变成AI专家或机器学习科学家。但他通过个人经历和团队实践，展示了安全团队无需攀登"喜马拉雅山"般的技术高峰，只需要投资并开始实践，就能从生成式AI中获得真实价值。他强调这不是区块链式的炒作，而是像电子商务一样具有变革性的技术革命。

会议的关键信息是：生成式AI的准入门槛极低，无需阅读手册、无需特殊培训，任何人都可以通过自然语言开始使用。安全团队应该立即开始使用生成式AI，既要交付价值，也要学习其优势和局限性。通过具体案例，Eric展示了如何用最小的投入获得巨大的回报，以及如何通过"闭环思维"来应对模型的非确定性和幻觉问题。

## 详细时间线与关键要点

### 开场与背景介绍 (00:00 - 03:30)
- **00:00** - Eric Branwine自我介绍，Amazon安全团队杰出工程师，18年Amazon经验，其中15年在安全团队
- **00:30** - 指出生成式AI"颠覆了一切"，但安全团队并未一夜之间变成AI专家
- **01:00** - 会议主题：安全团队如何保持相关性并有效支持业务
- **01:30** - 生成式AI定义：能够基于提示词和训练数据生成前所未见内容的模型
- **02:00** - 2017年"Attention is All You Need"论文引入Transformer概念，开启生成式AI热潮
- **02:30** - 三年前ChatGPT发布，成为"杀手级应用"
- **03:00** - 强调三年时间在技术领域很短，仍处于生成式AI早期阶段

### 技术演进与炒作周期 (03:30 - 08:00)
- **03:30** - 回顾早期生成式AI的局限：手指数量错误、图像中文字混乱、上下文窗口小
- **04:00** - 现在的模型可以容纳整个代码库或大量科学论文
- **04:30** - 介绍Gartner炒作周期曲线：技术创新触发、期望膨胀峰值、幻灭低谷、启蒙爬升、生产力高原
- **05:30** - 区块链案例：Long Island Iced Tea公司改名为Long Blockchain，股价一夜翻三倍
- **06:00** - Eric承认自己没有理解区块链，选择观望是正确的决定
- **06:30** - 展示1999年Barron's杂志封面"Amazon.bomb"，当时不看好电子商务
- **07:00** - 提出关键问题：生成式AI是区块链还是Amazon？
- **07:30** - 指出进入生成式AI领域的门槛看似很高：基础模型训练成本上亿美元、AI研究人员薪酬包高达2.5亿美元

### 个人实践案例 (08:00 - 15:00)
- **08:00** - 强调实际上不需要攀登"喜马拉雅山"，而是"阿巴拉契亚山脉"（较小的山）
- **08:30** - 案例一：书店图书分类 - Eric的妻子在独立书店工作，发现成人图画小说被错放在儿童区
- **09:00** - 传统解决方案需要手动搜索或编写自动化脚本，工作量大
- **09:30** - 妻子直接将库存数据粘贴到Claude，要求识别不适合儿童的书籍
- **10:00** - 仅用5分钟完成任务，包括3-4次重新提示
- **10:30** - 关键洞察：提示词"严重欠指定"（underspecified），但模型自动填补空白
- **11:00** - 案例二：CNC路由器G代码生成 - Eric的爱好是制作东西，拥有CNC路由器
- **11:30** - G代码是1960年代开发的机器人"汇编语言"，编写繁琐
- **12:00** - 展示完整提示词，非常简短，没有指定机器类型或详细参数
- **12:30** - Claude生成的代码完美运行，每次修改只需在提示词中添加一句话重新生成
- **13:00** - 强调重新生成比修改Python代码更快
- **13:30** - 关键点：无需阅读手册、无语法错误、专注于想要的切割模式
- **14:00** - 确信没有人想到用开发助手工具来运行CNC机器
- **14:30** - 两个案例的共同点：戏剧性的投资回报率、"试试看"的低成本

### 安全团队实践案例 (15:00 - 22:00)
- **15:00** - 成本结构翻转：构建原型的成本远低于召开规划会议
- **15:30** - 案例三：事件响应自动化 - 每个安全团队都需要做事件响应
- **16:00** - 事件响应任务：追踪网络入侵的入口点和所有被触及的系统
- **16:30** - 理想情况：所有请求有唯一标识符、时钟同步、日志格式统一
- **17:00** - 现实情况：测试系统、收购系统不符合标准，时钟不同步，日志格式各异
- **17:30** - 最糟糕情况：跨越夏令时变更进行事件响应
- **18:00** - Hackathon实践 - 安全团队定期举办黑客马拉松，专注学习和突破边界
- **18:30** - 两名工程师用48小时开发了CloudHound原型
- **19:00** - CloudHound在7分钟内完成训练演习，远快于人类
- **19:30** - 成本：当时91美元（约每小时8美元），18个月后成本更低
- **20:00** - 关键发现：CloudHound在测试中表现优于初级工程师
- **20:30** - 两天从零到超越初级水平，令人震惊
- **21:00** - CloudHound现已成为生产服务，表现与最优秀工程师相当
- **21:30** - 黑客马拉松的最大价值：团队的兴奋感和思想碰撞

### 核心观点与建议 (22:00 - 30:00)
- **22:00** - Eric确信这次技术革命与以往不同，具有真实价值
- **22:30** - Amazon在生成式AI上的巨大投资：Nova基础模型、AWS服务、Rufus购物助手、Kiro
- **23:00** - 但安全团队受益的方式不是这些大投资，而是像CloudHound这样的应用
- **23:30** - 可访问性极高：无准入门槛、无需阅读、无需学习、直接用自然语言提问
- **24:00** - 变化速度快是好事：上周不行的事情今天可能就行了
- **24:30** - 生成式AI已准备好用于生产环境
- **25:00** - Eric承认自己不理解模型如何工作，也不是ML科学家，但这不妨碍使用
- **25:30** - 曾被信息洪流吓退，但意识到不需要做所有事情，只需专注一个工具、一个问题
- **26:00** - 从未见过如此容易上手的技术：文本框、无语法错误、无需手册
- **26:30** - 妻子和自己都在几分钟内从零到解决问题
- **27:00** - 原型代码几乎从不进入生产 - 但CloudHound的生产服务从原型提示词开始构建
- **27:30** - 核心建议：每个安全团队现在都应该使用生成式AI，既交付价值又学习优缺点

### 开发实践与工具使用 (30:00 - 35:00)
- **30:00** - 安全是构建者组织，使用Kiro关注最新的生成式AI软件开发
- **30:30** - 实际案例：团队估计10工程师天（2周）的项目，8小时（1天）完成
- **31:00** - 这对成功至关重要：服务团队和业务都在加速，安全团队必须跟上
- **31:30** - 服务团队还在摸索如何用生成式AI构建，整个行业都在学习
- **32:00** - 如果不使用相同工具、不了解其优缺点，安全组织将失效
- **32:30** - 一天完成两周项目的吸引力无法忽视
- **33:00** - 会议不会深入软件开发流程，re:Invent有大量相关演讲
- **33:30** - 重点讨论如何让安全团队"登山"

### 非确定性与幻觉问题 (35:00 - 42:00)
- **35:00** - 最大担忧：非确定性和幻觉
- **35:30** - 非确定性冒犯了计算机科学家：同一程序应该得到相同答案
- **36:00** - 幻觉问题：模型会编造答案，并将真假信息自信地混合在一起
- **36:30** - 但实际上一直在处理非确定性组件：人类就是非确定性的
- **37:00** - 人类有数千年与人类打交道的经验，擅长读取非语言线索和判断信心
- **37:30** - 模型缺乏这些线索，需要通过互动、构建、学习来建立直觉
- **38:00** - 一直知道LLM不是人，但友好的聊天界面可能让人误以为在与人交流
- **38:30** - 这是计算机，但又像人，但又是计算机，但又像人
- **39:00** - 实际上是"token预测引擎"，没有人在那里
- **39:30** - 绕过护栏的各种方法：忽略之前的指令、祖母讲故事技巧、用诗歌格式提示
- **40:00** - 这些方法对人类不起作用，但对模型有效
- **40:30** - 它们不是确定性计算机，不是人类，是第三种东西、新东西
- **41:00** - Eric发现自己不断忘记这一点，双向都会忘记

### 闭环思维与验证策略 (42:00 - 结束)
- **42:00** - 核心思维：模型擅长提出候选解决方案
- **42:30** - 问题越简单，正确答案的可能性越高
- **43:00** - 实际案例：让Claude找工业风格吊扇，5个结果中2个不存在（404链接）
- **43:30** - 这只是增强型网络搜索，模型仍然产生幻觉
- **44:00** - 关键认识：找到答案成本高，但验证答案成本低且通常是确定性的
- **44:30** - 2023年律师使用ChatGPT案例：提交了虚假判例
- **45:00** - 2023年是"生成式AI年代的永恒之前"，人类几乎没有LLM经验
- **45:30** - 令人失望的是，此后出现了一系列类似故事
- **46:00** - 结论：人类可以训练模型，但人类自己不可训练
- **46:30** - 同情律师：利用新工具值得赞赏，任务确实具有挑战性
- **47:00** - 如果将答案视为候选答案而非最终答案，检查后会有完全不同的结果
- **47:30** - 做研究耗时，但点击链接浏览案例快速简单
- **48:00** - 最重要的事情：人类会给出信心线索，模型会自信地给出错误信息
- **48:30** - 闭环工作：任何时候看生成式AI相关内容，都要思考循环在哪里、如何闭环
- **49:00** - 让模型提供引用链接
- **49:30** - 人类需要点击链接阅读案例来闭环
- **50:00** - 进一步优化：让模型做更多工作，自动验证引用有效性
- **50:30** - 使用"LLM作为评判者"（LLM as judge）的概念
- **51:00** - 让模型检查自己的工作，虽然成本更高但无需人工监督