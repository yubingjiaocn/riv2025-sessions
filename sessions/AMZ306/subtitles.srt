1
00:00:02,220 --> 00:00:06,420
- Welcome everyone to Session AMZ306,

2
00:00:06,420 --> 00:00:08,580
Elevate Streaming Quality with AI:

3
00:00:08,580 --> 00:00:11,400
Prime Video's Innovative Approach.

4
00:00:11,400 --> 00:00:15,723
So before we start, how many
of you watch Prime Video?

5
00:00:17,670 --> 00:00:18,503
It's awesome.

6
00:00:18,503 --> 00:00:21,367
So you might have seen the
latest shows like "Fallout"

7
00:00:21,367 --> 00:00:25,470
"Ted Lasso" or like watched
"NBA" or "TNF" on there.

8
00:00:25,470 --> 00:00:28,440
So this session will be really
interesting for you to learn

9
00:00:28,440 --> 00:00:32,130
how Prime Video uses
AI, particularly GenAI,

10
00:00:32,130 --> 00:00:34,200
to improve their artwork quality,

11
00:00:34,200 --> 00:00:36,390
to be able to improve
the streaming quality

12
00:00:36,390 --> 00:00:39,270
so that you can watch
uninterrupted movies,

13
00:00:39,270 --> 00:00:41,970
uninterrupted sports on
your favorite channel

14
00:00:41,970 --> 00:00:42,843
of Prime Video.

15
00:00:44,490 --> 00:00:46,980
So to start off with, I'm Tulip Gupta,

16
00:00:46,980 --> 00:00:50,490
I'm a principal solutions
architect with AWS

17
00:00:50,490 --> 00:00:54,450
and I support the strategic
accounts under AWS,

18
00:00:54,450 --> 00:00:56,700
primarily the media
attainments under Amazon

19
00:00:56,700 --> 00:00:59,790
and Prime Video is one of my customers.

20
00:00:59,790 --> 00:01:02,820
And I have with me Brian
and Mona from Prime Video.

21
00:01:02,820 --> 00:01:04,830
Brian is a principal SD at Prime Video

22
00:01:04,830 --> 00:01:07,560
and Mona a senior manager
of data engineering.

23
00:01:07,560 --> 00:01:09,930
And they are going to
introduce themselves later on

24
00:01:09,930 --> 00:01:12,063
when they present their use cases.

25
00:01:13,230 --> 00:01:15,780
So we are going to start
up with an introduction,

26
00:01:15,780 --> 00:01:17,730
go through all of the use cases

27
00:01:17,730 --> 00:01:18,960
that Brian is gonna talk about

28
00:01:18,960 --> 00:01:21,780
the artwork quality moderation
that he uses GenAI with.

29
00:01:21,780 --> 00:01:25,440
And then Mona is gonna elaborate
on the streaming quality,

30
00:01:25,440 --> 00:01:28,830
how they improve that using GenAI agents.

31
00:01:28,830 --> 00:01:31,890
And we are gonna talk about
the challenges that they faced

32
00:01:31,890 --> 00:01:33,930
in their individual journeys

33
00:01:33,930 --> 00:01:37,740
as well as the solution and journey on AWS

34
00:01:37,740 --> 00:01:39,570
and also like talk about the demonstration

35
00:01:39,570 --> 00:01:41,013
and benefits at the end.

36
00:01:43,320 --> 00:01:46,440
So when we talk about Prime Video,

37
00:01:46,440 --> 00:01:47,580
Prime Video is global,

38
00:01:47,580 --> 00:01:50,940
Prime Video has millions
of titles in its catalog

39
00:01:50,940 --> 00:01:53,970
that it can stream on hundreds of devices.

40
00:01:53,970 --> 00:01:57,450
They support over 30 different languages,

41
00:01:57,450 --> 00:02:00,960
operate in more than 240 plus
countries and territories

42
00:02:00,960 --> 00:02:05,370
and support over 200 million
Prime members worldwide.

43
00:02:05,370 --> 00:02:07,680
So nearly anywhere around the world

44
00:02:07,680 --> 00:02:09,390
you can log in onto Prime Video

45
00:02:09,390 --> 00:02:11,760
and enjoy your favorite content.

46
00:02:11,760 --> 00:02:13,950
So let's say you're
traveling to Europe or India

47
00:02:13,950 --> 00:02:15,450
or anywhere like you'll be able to log in

48
00:02:15,450 --> 00:02:17,400
and watch your favorite show out there.

49
00:02:19,290 --> 00:02:24,060
Like basically like there's
315 million data points

50
00:02:24,060 --> 00:02:26,580
that Prime Video has a
global ad supported reach

51
00:02:26,580 --> 00:02:30,573
for over like 300 million monthly viewers.

52
00:02:31,650 --> 00:02:35,370
And this is how Prime Video
has grown linearly over time.

53
00:02:35,370 --> 00:02:38,400
So they started off like in
2016 with "The Late Show"

54
00:02:38,400 --> 00:02:40,440
and then as you can see as they have moved

55
00:02:40,440 --> 00:02:43,470
from like 2018 to 2020 to 2022,

56
00:02:43,470 --> 00:02:47,070
they have increased
the number of streaming

57
00:02:47,070 --> 00:02:47,970
that they have done,

58
00:02:47,970 --> 00:02:50,550
number of shows that they have produced.

59
00:02:50,550 --> 00:02:52,710
So it's a lot that's going on.

60
00:02:52,710 --> 00:02:56,260
And so as they're scaling they
also want to be able to scale

61
00:02:57,818 --> 00:03:00,690
how they are able to like use
it for the streaming quality

62
00:03:00,690 --> 00:03:02,250
and how they're able to like reach it out

63
00:03:02,250 --> 00:03:04,350
to like millions of customers out there.

64
00:03:04,350 --> 00:03:05,640
So it's a huge accomplishment

65
00:03:05,640 --> 00:03:07,890
and as they continue to
grow bigger and bigger,

66
00:03:07,890 --> 00:03:10,410
scaling efficiently does
impact their ability

67
00:03:10,410 --> 00:03:14,433
to deliver these events
over our "Swim Team"

68
00:03:15,900 --> 00:03:17,130
and then lining up their biggest

69
00:03:17,130 --> 00:03:19,650
or slate of critical movies ever.

70
00:03:19,650 --> 00:03:21,360
And with Prime Video customers,

71
00:03:21,360 --> 00:03:23,250
they can customize
their viewing experience

72
00:03:23,250 --> 00:03:26,250
and find the favorite movies,
series and live events

73
00:03:26,250 --> 00:03:29,310
including Amazon MGM
Studios produced series

74
00:03:29,310 --> 00:03:30,960
and the movies "Fall Out."

75
00:03:30,960 --> 00:03:34,080
And so in 2025 they also added, you know,

76
00:03:34,080 --> 00:03:37,890
like the Fox 1 and Peacock
Premium Plus as well.

77
00:03:37,890 --> 00:03:41,703
So it's been kind of a journey
as they've grown over time.

78
00:03:43,048 --> 00:03:44,610
And so a little trivia out here.

79
00:03:44,610 --> 00:03:47,430
So in 2025, what was the peak audience

80
00:03:47,430 --> 00:03:49,470
for the Commanders versus
Packers Thursday night

81
00:03:49,470 --> 00:03:51,303
football game on Prime Video?

82
00:03:52,312 --> 00:03:54,262
Can anyoneanswer the question out here?

83
00:03:56,700 --> 00:03:57,603
Any guesses?

84
00:03:58,740 --> 00:04:02,015
18.
- 20.

85
00:04:02,015 --> 00:04:06,217
- 18, so you can see
like how massive it was

86
00:04:06,217 --> 00:04:10,890
like with like 18 million people
logging in at the same time

87
00:04:10,890 --> 00:04:13,110
and watching the TNF game.

88
00:04:13,110 --> 00:04:15,960
And so to operate at that
scale they also want to be able

89
00:04:15,960 --> 00:04:17,573
to scale out the
infrastructure primarily on AWS

90
00:04:17,573 --> 00:04:19,833
to able to meet that demands.

91
00:04:21,390 --> 00:04:26,190
And so to kind of like talk
about what the use cases were,

92
00:04:26,190 --> 00:04:28,080
you know, so they wanted
to be able to improve

93
00:04:28,080 --> 00:04:30,090
and moderate the streaming quality

94
00:04:30,090 --> 00:04:31,470
that Mona is gonna touch about

95
00:04:31,470 --> 00:04:33,210
and then like also to be able

96
00:04:33,210 --> 00:04:35,040
to moderate the artwork quality

97
00:04:35,040 --> 00:04:36,840
that Brian's gonna talk about at scale.

98
00:04:36,840 --> 00:04:38,340
Like when you talk about
the scale that we looked at

99
00:04:38,340 --> 00:04:40,170
like and it goes up linearly.

100
00:04:40,170 --> 00:04:41,490
To operate at this scale,

101
00:04:41,490 --> 00:04:45,300
they depended on
solutions like GenAI tools

102
00:04:45,300 --> 00:04:46,980
for evaluation and performance

103
00:04:46,980 --> 00:04:49,320
and that's how they were
able to scale it out.

104
00:04:49,320 --> 00:04:52,980
So like for example, for
like for Brian's use case

105
00:04:52,980 --> 00:04:54,690
they get content from like, you know,

106
00:04:54,690 --> 00:04:56,940
Peacock, from like Stars, you know,

107
00:04:56,940 --> 00:05:00,687
PBS for NFL and MLB and NBA

108
00:05:00,687 --> 00:05:04,950
and they are able to moderate
it by using GenAI tools.

109
00:05:04,950 --> 00:05:08,010
And so if folks are aware
with some of the GenAI tools

110
00:05:08,010 --> 00:05:09,330
out here, I'm also gonna cover

111
00:05:09,330 --> 00:05:11,580
some of the GenAI tools
in the coming slides.

112
00:05:12,990 --> 00:05:14,580
So when we talk about GenAI tools,

113
00:05:14,580 --> 00:05:18,000
you might have heard about
AI agents particularly.

114
00:05:18,000 --> 00:05:19,860
So what are AI agents?

115
00:05:19,860 --> 00:05:22,410
And it's a tectonic shift of how we build,

116
00:05:22,410 --> 00:05:24,330
deploy, and interact.

117
00:05:24,330 --> 00:05:26,610
And so for an AWS we have

118
00:05:26,610 --> 00:05:29,340
what we call our agents which is like,

119
00:05:29,340 --> 00:05:32,040
and it's kind of a GenAI
agents that come out there.

120
00:05:32,040 --> 00:05:33,120
These are like autonomous

121
00:05:33,120 --> 00:05:35,940
or semi-autonomous software
systems that can reason,

122
00:05:35,940 --> 00:05:38,430
plan, and act to accomplish goals

123
00:05:38,430 --> 00:05:40,800
in digital or physical environments.

124
00:05:40,800 --> 00:05:42,690
And so these, you know...

125
00:05:42,690 --> 00:05:45,780
We had the foundation models
which can do one task.

126
00:05:45,780 --> 00:05:47,190
But when we talk about agents,

127
00:05:47,190 --> 00:05:49,350
they're able to do a task independently,

128
00:05:49,350 --> 00:05:51,330
they're able to call the foundation model,

129
00:05:51,330 --> 00:05:53,010
they're able to access database,

130
00:05:53,010 --> 00:05:55,391
they're able to go and call tools.

131
00:05:55,391 --> 00:05:58,650
And they're able to perform
that task on their own.

132
00:05:58,650 --> 00:06:01,650
And so these can leverage
AI to reason and plan

133
00:06:01,650 --> 00:06:06,650
and accomplish the plan,
the task on their own.

134
00:06:08,490 --> 00:06:10,492
And so that makes sense
to what we think about

135
00:06:10,492 --> 00:06:11,325
the evolution of agentic AI.

136
00:06:12,510 --> 00:06:14,880
And so we move from left to right,

137
00:06:14,880 --> 00:06:17,880
there was more human oversight
when we first started

138
00:06:17,880 --> 00:06:19,470
with help of like generative AI assistance

139
00:06:19,470 --> 00:06:20,580
and you might have heard about it

140
00:06:20,580 --> 00:06:22,350
and two years ago when it came out,

141
00:06:22,350 --> 00:06:24,930
they follow a set of rules,
they automate repetitive tasks.

142
00:06:24,930 --> 00:06:26,460
And as we move towards the right

143
00:06:26,460 --> 00:06:27,693
then we have the GenAI agents

144
00:06:27,693 --> 00:06:29,820
that we touched about
in the previous slide

145
00:06:29,820 --> 00:06:33,300
and they are able to do a
singular task very well.

146
00:06:33,300 --> 00:06:35,850
And then we moved onto the
right is agentic AI system,

147
00:06:35,850 --> 00:06:38,340
which are like fully
autonomous multi-agents

148
00:06:38,340 --> 00:06:41,640
and they can orchestrate
between them to accomplish

149
00:06:41,640 --> 00:06:43,020
a set of tasks

150
00:06:43,020 --> 00:06:45,930
and so they can mimic
human logic and reasoning.

151
00:06:45,930 --> 00:06:47,730
And so as we move from left to right,

152
00:06:47,730 --> 00:06:49,530
there's less and less human oversight

153
00:06:49,530 --> 00:06:51,393
and there's increasing autonomy.

154
00:06:53,190 --> 00:06:56,610
And so when we talk about
AWS agentic AI portfolio

155
00:06:56,610 --> 00:06:58,860
and for some of you it might be familiar

156
00:06:58,860 --> 00:07:01,140
with something called SageMaker.

157
00:07:01,140 --> 00:07:04,140
So SageMaker essentially it
gives you the compute power,

158
00:07:04,140 --> 00:07:05,850
it gives you the model
where you can customize

159
00:07:05,850 --> 00:07:07,830
your own model, you can build, you train

160
00:07:07,830 --> 00:07:09,180
and you host your own models

161
00:07:09,180 --> 00:07:11,370
and we also provide compute stuff

162
00:07:11,370 --> 00:07:13,530
like Trainium and Inferentia.

163
00:07:13,530 --> 00:07:15,900
Then we have the middle layer

164
00:07:15,900 --> 00:07:19,440
which is AI and agent development
software and services.

165
00:07:19,440 --> 00:07:21,990
And in this we have Amazon Bedrock.

166
00:07:21,990 --> 00:07:25,590
How many of you're familiar
with Amazon Bedrock out here?

167
00:07:25,590 --> 00:07:27,660
Cool, so for folks who are not,

168
00:07:27,660 --> 00:07:29,610
this is the managed service.

169
00:07:29,610 --> 00:07:32,760
Basically it gives you a
flexible comprehensive service

170
00:07:32,760 --> 00:07:35,190
for GenAI application
and agent development.

171
00:07:35,190 --> 00:07:37,890
It offers access to
like foundation models,

172
00:07:37,890 --> 00:07:39,480
you can customers models

173
00:07:39,480 --> 00:07:41,430
and you can application with your data

174
00:07:41,430 --> 00:07:42,990
and apply safety guardrails.

175
00:07:42,990 --> 00:07:44,820
So let's say you're building an assistant

176
00:07:44,820 --> 00:07:46,380
that provides you move recommendation

177
00:07:46,380 --> 00:07:48,540
and you say like, "Hey
you don't ask me questions

178
00:07:48,540 --> 00:07:50,580
about politics," they
shouldn't be able to do that.

179
00:07:50,580 --> 00:07:52,410
That's what we mean by guardrails.

180
00:07:52,410 --> 00:07:54,930
And also like it provides you AgentCore

181
00:07:54,930 --> 00:07:57,180
which is one of the new service we added

182
00:07:57,180 --> 00:08:00,750
with essentially like helps you
scale, deploy your services,

183
00:08:00,750 --> 00:08:02,310
operate at scale on AWS.

184
00:08:02,310 --> 00:08:03,750
So if you build an application

185
00:08:03,750 --> 00:08:05,400
and you want to scale
it out to like millions

186
00:08:05,400 --> 00:08:09,090
and hundreds of users, AgentCore
is what we will be using.

187
00:08:09,090 --> 00:08:11,880
And this is just briefly
touching about the services.

188
00:08:11,880 --> 00:08:14,250
And then we have SDK for agents .

189
00:08:14,250 --> 00:08:16,860
Like some of the one with NOVA Act

190
00:08:16,860 --> 00:08:20,310
which is designed to like take
action within the browser.

191
00:08:20,310 --> 00:08:22,140
And then we have Strands Agents,

192
00:08:22,140 --> 00:08:25,380
which Mona and Brian are
gonna talk about it in deep

193
00:08:25,380 --> 00:08:26,880
when they talk about their use cases

194
00:08:26,880 --> 00:08:28,980
and how they have been leveraging Strands.

195
00:08:28,980 --> 00:08:33,420
As the Strands Agents is
another open source SDK

196
00:08:33,420 --> 00:08:34,680
that AWS has developed

197
00:08:34,680 --> 00:08:38,100
and it basically is a
lightweight source SDK,

198
00:08:38,100 --> 00:08:40,980
which helps you develop
agents very quickly

199
00:08:40,980 --> 00:08:43,110
and be able to leverage

200
00:08:43,110 --> 00:08:47,313
and call multiple tools to be
able to accomplish that task.

201
00:08:48,750 --> 00:08:51,090
And on the top we have applications.

202
00:08:51,090 --> 00:08:52,860
So these, as we moved on from broad term,

203
00:08:52,860 --> 00:08:53,820
like obviously it gets,

204
00:08:53,820 --> 00:08:57,060
you know, there it gets
more and more managed,

205
00:08:57,060 --> 00:08:59,160
and you don't have to manage
your own infrastructure

206
00:08:59,160 --> 00:09:00,200
like for Bedrock.

207
00:09:00,200 --> 00:09:01,260
you do not have to manage
your infrastructure

208
00:09:01,260 --> 00:09:04,230
and the applications you get
like a whole applications

209
00:09:04,230 --> 00:09:06,090
out there that you can
like play around with.

210
00:09:06,090 --> 00:09:09,840
So for example, like Kiro, it
acts like a coding assistant.

211
00:09:09,840 --> 00:09:10,860
If you wanna develop an app.

212
00:09:10,860 --> 00:09:12,930
Like hey I wanna Streamlit app

213
00:09:12,930 --> 00:09:14,700
to act like a travel agent, it'll be able

214
00:09:14,700 --> 00:09:17,340
to quickly design it and
build it out for you.

215
00:09:17,340 --> 00:09:19,140
Then we have Amazon Q Developer

216
00:09:19,140 --> 00:09:21,150
for accelerating software development.

217
00:09:21,150 --> 00:09:24,540
Q Business for your data
and to answer questions.

218
00:09:24,540 --> 00:09:28,074
Transform to accelerate
enterprise modernization of dotnet

219
00:09:28,074 --> 00:09:30,510
and mainframe and VMware Clouds

220
00:09:30,510 --> 00:09:34,200
and then Amazon Connect which
is to speed customer service

221
00:09:34,200 --> 00:09:35,550
to delight customers.

222
00:09:35,550 --> 00:09:36,840
And then lastly, Marketplace,

223
00:09:36,840 --> 00:09:39,630
we can go and access a
lot of agents out there.

224
00:09:39,630 --> 00:09:42,180
And just a quick brief overview for you

225
00:09:42,180 --> 00:09:44,313
to like see all the portfolio out there.

226
00:09:45,600 --> 00:09:47,130
And that brings us Strands Agents.

227
00:09:47,130 --> 00:09:49,980
So we briefly covered
what Strands Agents is

228
00:09:49,980 --> 00:09:50,880
in the previous slide.

229
00:09:50,880 --> 00:09:53,490
So again, like I said, open source SDK.

230
00:09:53,490 --> 00:09:56,910
So in the past like LLMs were a, you know,

231
00:09:56,910 --> 00:10:00,840
we basically would
provide agents a template

232
00:10:00,840 --> 00:10:02,640
of how to do stuff.

233
00:10:02,640 --> 00:10:05,280
And right now LLMs are
getting smarter and smarter

234
00:10:05,280 --> 00:10:07,530
and Strands Agents provides
a lightweight model

235
00:10:07,530 --> 00:10:09,300
where it's more intuitive development

236
00:10:09,300 --> 00:10:11,550
and it can figure it out to call the LLM

237
00:10:11,550 --> 00:10:13,170
and the tools on its own

238
00:10:13,170 --> 00:10:16,380
and it can get started in
minutes instead of hours.

239
00:10:16,380 --> 00:10:18,360
And it provides you robust capabilities

240
00:10:18,360 --> 00:10:20,880
like native tools, MCP servers,

241
00:10:20,880 --> 00:10:23,100
and you can also like extend the support

242
00:10:23,100 --> 00:10:26,670
for custom mobile providers,
custom tools and MCP.

243
00:10:26,670 --> 00:10:30,150
So it helps you like with rapid
development and prototyping

244
00:10:30,150 --> 00:10:34,530
and that's what the key thing
that Mona's and Brain's team

245
00:10:34,530 --> 00:10:36,360
leverage to be able to experiment quickly,

246
00:10:36,360 --> 00:10:38,010
to be able to use new services,

247
00:10:38,010 --> 00:10:40,710
and be able to like iterate
on what they're developing

248
00:10:40,710 --> 00:10:43,830
to develop that robust evaluation loop.

249
00:10:43,830 --> 00:10:46,080
And with that I'll hand it over to Brian

250
00:10:46,080 --> 00:10:49,413
to talk about his use case
and the artwork quality.

251
00:10:54,150 --> 00:10:57,993
- All right, thanks,
Mona or thanks, Tulip.

252
00:10:59,040 --> 00:11:00,720
So my name is Brian Breck

253
00:11:00,720 --> 00:11:02,370
and I'm a principal engineer

254
00:11:02,370 --> 00:11:05,970
with the partner experience
team within Prime Video.

255
00:11:05,970 --> 00:11:10,320
We work with our content
creators like major studios

256
00:11:10,320 --> 00:11:13,350
and independent filmmakers
to ingest their content

257
00:11:13,350 --> 00:11:16,350
and prepare it for streaming customers.

258
00:11:16,350 --> 00:11:19,470
That content includes not
only the streamable assets

259
00:11:19,470 --> 00:11:23,730
but also artwork, metadata, trailers

260
00:11:23,730 --> 00:11:27,393
and bonus material as a
part of that data set.

261
00:11:29,100 --> 00:11:33,450
So just to get us started,
quick question for everybody,

262
00:11:33,450 --> 00:11:35,130
how many of you, raise of hands,

263
00:11:35,130 --> 00:11:37,233
stream with more than one device?

264
00:11:38,070 --> 00:11:41,010
Probably most of you, it
looks like most of you.

265
00:11:41,010 --> 00:11:46,010
So I stream with my
phone, my laptop, my TV,

266
00:11:46,290 --> 00:11:48,990
my tablet and that creates
a lot of complexity

267
00:11:48,990 --> 00:11:50,460
at Prime Video.

268
00:11:50,460 --> 00:11:55,460
We have to support a
number of form factors,

269
00:11:56,070 --> 00:11:59,010
backgrounds on top of 30 plus languages

270
00:11:59,010 --> 00:12:01,083
and over 200 territories.

271
00:12:03,060 --> 00:12:05,340
Now what we're gonna
be talking about today

272
00:12:05,340 --> 00:12:07,740
is artwork quality.

273
00:12:07,740 --> 00:12:12,740
We use artwork to
represent movies, TV shows,

274
00:12:12,900 --> 00:12:15,753
channels, carousels.

275
00:12:17,190 --> 00:12:20,070
And that artwork can show
up in the streaming site

276
00:12:20,070 --> 00:12:24,390
as well as in marketing
material and advertising.

277
00:12:24,390 --> 00:12:27,160
Now the artwork is
provided by our partners

278
00:12:28,080 --> 00:12:29,490
and the artwork may be beautiful

279
00:12:29,490 --> 00:12:32,283
but it may not meet the
needs of Prime Video.

280
00:12:33,300 --> 00:12:37,980
So for example, we need to be able

281
00:12:37,980 --> 00:12:42,240
to crop artwork depending
on what form factor

282
00:12:42,240 --> 00:12:44,370
we're going to be showing it.

283
00:12:44,370 --> 00:12:47,320
We also may need to overlay a logo

284
00:12:49,599 --> 00:12:51,810
or an action button.

285
00:12:51,810 --> 00:12:55,200
And so we need a safe zone on
the perimeter of the artwork

286
00:12:55,200 --> 00:13:00,000
so that we can work
within those parameters.

287
00:13:00,000 --> 00:13:04,200
So I've highlighted the
safe zone in purple.

288
00:13:04,200 --> 00:13:07,050
In the first example
we can see on the left

289
00:13:07,050 --> 00:13:11,160
there's plenty of text
on the side of "Lioness"

290
00:13:11,160 --> 00:13:13,800
or plenty of space on
the side of "Lioness"

291
00:13:13,800 --> 00:13:16,680
and on the right only the shoulder

292
00:13:16,680 --> 00:13:18,960
of the last actor is being cut off.

293
00:13:18,960 --> 00:13:22,353
So this is a perfectly
acceptable safe zone.

294
00:13:23,250 --> 00:13:26,880
Now in the second example we can see that

295
00:13:26,880 --> 00:13:30,780
a head is being cut off
for one of the characters

296
00:13:30,780 --> 00:13:33,870
and there's some text
that's completely obscured.

297
00:13:33,870 --> 00:13:37,710
So we could not use this for Prime Video.

298
00:13:37,710 --> 00:13:40,890
Now this is just one type of
defect that we're looking for.

299
00:13:40,890 --> 00:13:44,070
We're looking for other
things like mature content,

300
00:13:44,070 --> 00:13:48,180
pixelation, issues with localization,

301
00:13:48,180 --> 00:13:53,093
as well as accessibility for
things like colorblindness.

302
00:13:57,150 --> 00:14:00,390
So we have a number of
challenges in this space.

303
00:14:00,390 --> 00:14:02,760
One of the biggest is
that a lot of this work

304
00:14:02,760 --> 00:14:05,220
is traditionally done manually.

305
00:14:05,220 --> 00:14:09,060
So our partners provide
us a piece of artwork,

306
00:14:09,060 --> 00:14:12,900
it needs to get into a
manual evaluator's queue,

307
00:14:12,900 --> 00:14:15,960
they need to provide their feedback

308
00:14:15,960 --> 00:14:18,360
and then that feedback needs
to get back to the partner

309
00:14:18,360 --> 00:14:21,030
and there may be multiple
iterations of this.

310
00:14:21,030 --> 00:14:24,450
So the entire process
can take multiple days.

311
00:14:24,450 --> 00:14:27,660
Now our solution to
this generally has been

312
00:14:27,660 --> 00:14:30,840
to create an ML model that we train

313
00:14:30,840 --> 00:14:34,020
with artwork examples

314
00:14:34,020 --> 00:14:39,020
and then we use those
examples to find defects.

315
00:14:39,330 --> 00:14:42,360
But that's a time consuming process

316
00:14:42,360 --> 00:14:45,630
and we have over 30 defects
that we're looking for today

317
00:14:45,630 --> 00:14:49,203
in our artwork and that
number is only growing.

318
00:14:50,100 --> 00:14:53,550
Now another problem that we have is data.

319
00:14:53,550 --> 00:14:58,170
We have been collecting data
from our manual evaluators

320
00:14:58,170 --> 00:15:01,890
on acceptable and not acceptable content.

321
00:15:01,890 --> 00:15:04,980
However, they don't always
follow the same SOP,

322
00:15:04,980 --> 00:15:06,660
standard operating procedure.

323
00:15:06,660 --> 00:15:10,830
And so what may pass one
evaluator may not pass another

324
00:15:10,830 --> 00:15:15,830
and that data can leak
into our our data sets

325
00:15:15,840 --> 00:15:18,840
and make it more difficult to use.

326
00:15:18,840 --> 00:15:22,020
And then once we have a solution,

327
00:15:22,020 --> 00:15:26,340
the evaluation can take a
while, we can take several days

328
00:15:26,340 --> 00:15:29,640
to run it over a data set

329
00:15:29,640 --> 00:15:32,350
and then figure out what
changes need to be made

330
00:15:33,420 --> 00:15:37,053
for the next iteration so that
we can improve the system.

331
00:15:39,300 --> 00:15:41,730
Now when we're talking about evaluation,

332
00:15:41,730 --> 00:15:45,330
what we generally do is we
take a few thousand images

333
00:15:45,330 --> 00:15:49,500
and we have a human
annotator that goes through

334
00:15:49,500 --> 00:15:52,380
and provides what the
correct result should be

335
00:15:52,380 --> 00:15:55,740
and that's what we see with
the ground truth column.

336
00:15:55,740 --> 00:15:58,050
Then we run our automated solution

337
00:15:58,050 --> 00:16:03,050
and then we look for
discrepancies in the results.

338
00:16:03,240 --> 00:16:07,800
So in this example in the
second image we can see

339
00:16:07,800 --> 00:16:10,650
that ground truth says that
this should have passed,

340
00:16:10,650 --> 00:16:14,130
but the model that we were
using is going to fail

341
00:16:14,130 --> 00:16:15,393
that piece of artwork.

342
00:16:16,290 --> 00:16:19,290
Maybe it thought that Ryan
Reynolds hair was too close

343
00:16:19,290 --> 00:16:23,400
to the edge and so now we
need to provide more artwork

344
00:16:23,400 --> 00:16:26,495
as training data to fall for this problem

345
00:16:26,495 --> 00:16:30,330
or create some additional
instruction in our algorithm

346
00:16:30,330 --> 00:16:31,953
to allow this to pass.

347
00:16:32,970 --> 00:16:35,490
But we're only looking
at four images here.

348
00:16:35,490 --> 00:16:38,010
Imagine if we're looking
at thousands of images

349
00:16:38,010 --> 00:16:42,810
with results in S3 buckets
that we've gotta pour over,

350
00:16:42,810 --> 00:16:46,263
it can just take a lot of
time to evaluate the results.

351
00:16:49,680 --> 00:16:53,850
Now one of the things
that we noticed is that

352
00:16:53,850 --> 00:16:58,500
with LLMs and their
multimodal counterparts

353
00:16:58,500 --> 00:17:01,890
we can detect certain defects

354
00:17:01,890 --> 00:17:04,203
with those foundational models.

355
00:17:05,250 --> 00:17:08,250
We'd seen anecdotes of it
being used in other places

356
00:17:08,250 --> 00:17:11,250
and so we wanted to try
it out for our use case.

357
00:17:11,250 --> 00:17:14,610
So what we ended up doing is using Q CLI,

358
00:17:14,610 --> 00:17:16,810
which is now folded into Kiro

359
00:17:17,850 --> 00:17:20,550
to generate a few algorithms for us

360
00:17:20,550 --> 00:17:23,520
and use a few different
foundational models

361
00:17:23,520 --> 00:17:26,010
to try out some results.

362
00:17:26,010 --> 00:17:28,260
We wanted to move quickly,
we wanted to make sure

363
00:17:28,260 --> 00:17:30,243
that this was going to work for us.

364
00:17:31,950 --> 00:17:36,030
When we ran the results,
we anecdotally saw

365
00:17:36,030 --> 00:17:38,040
that it was promising.

366
00:17:38,040 --> 00:17:39,840
So then we wanted to go

367
00:17:39,840 --> 00:17:41,790
and perform one of the evaluations

368
00:17:41,790 --> 00:17:43,293
that we just took a look at.

369
00:17:44,460 --> 00:17:47,550
We saw that our precision
wasn't high enough

370
00:17:47,550 --> 00:17:49,410
and we knew that we could improve it,

371
00:17:49,410 --> 00:17:52,530
but we also knew that it was
going to take a few iterations.

372
00:17:52,530 --> 00:17:56,583
So we wanted to move a lot
faster than we had in the past.

373
00:17:59,550 --> 00:18:01,620
So what we ended up doing is creating

374
00:18:01,620 --> 00:18:06,333
an evaluation framework that
we use for defect detection.

375
00:18:07,830 --> 00:18:11,880
We take as input data
sets with ground truth

376
00:18:11,880 --> 00:18:14,250
as well as some initial configuration

377
00:18:14,250 --> 00:18:18,840
and then as output we get the results

378
00:18:18,840 --> 00:18:22,083
as well as feedback on
how it could be improved.

379
00:18:23,070 --> 00:18:25,710
Instead of diving into S3 buckets,

380
00:18:25,710 --> 00:18:28,470
we can see views of the artwork.

381
00:18:28,470 --> 00:18:30,750
So for example with "Uncharted"

382
00:18:30,750 --> 00:18:33,360
we can see a mobile view versus a web view

383
00:18:33,360 --> 00:18:36,603
and how that's going to look
in the different use cases.

384
00:18:38,280 --> 00:18:41,550
And then we get some
benchmarking statistics

385
00:18:41,550 --> 00:18:45,870
based off of how compared to ground truth

386
00:18:45,870 --> 00:18:49,590
as well as how it
compared to previous runs.

387
00:18:49,590 --> 00:18:51,990
And then we also have
the ability to dive in

388
00:18:51,990 --> 00:18:54,090
and take a look at some of the issues

389
00:18:54,090 --> 00:18:56,223
at the individual artwork level.

390
00:19:00,690 --> 00:19:05,070
So here we're taking a look
at the backend architecture

391
00:19:05,070 --> 00:19:08,490
for our evaluation system.

392
00:19:08,490 --> 00:19:12,690
It is a system that is
orchestrated by Strands

393
00:19:12,690 --> 00:19:17,370
with individual agents that
perform the defect detection.

394
00:19:17,370 --> 00:19:21,990
They perform the evaluation of the results

395
00:19:21,990 --> 00:19:24,810
and then also agents for

396
00:19:24,810 --> 00:19:26,823
suggesting improvements to the process.

397
00:19:27,750 --> 00:19:32,750
So the way that it works
is that a user will submit

398
00:19:32,820 --> 00:19:35,280
an initial request through our CloudFront

399
00:19:35,280 --> 00:19:37,680
and load balancer instances

400
00:19:37,680 --> 00:19:39,513
and hit our API.

401
00:19:40,380 --> 00:19:45,380
That API will store the
data in our config table

402
00:19:45,630 --> 00:19:48,270
and that will include an initial prompt

403
00:19:48,270 --> 00:19:50,130
used for defect detection.

404
00:19:50,130 --> 00:19:53,040
It will include a link to a data set

405
00:19:53,040 --> 00:19:55,980
along with the ground truth data.

406
00:19:55,980 --> 00:19:59,220
And then it will also include
some initial configuration

407
00:19:59,220 --> 00:20:02,040
such as which model to use

408
00:20:02,040 --> 00:20:06,840
and some things like
temperature to determine

409
00:20:06,840 --> 00:20:08,343
how to use that model.

410
00:20:12,360 --> 00:20:16,110
Next the orchestrator will
pick up that configuration

411
00:20:16,110 --> 00:20:18,810
and delegate each piece of artwork

412
00:20:18,810 --> 00:20:23,040
to our eval subject agent

413
00:20:23,040 --> 00:20:26,400
that will actually perform
the defect detection.

414
00:20:26,400 --> 00:20:29,310
And once we've gotten through all of those

415
00:20:29,310 --> 00:20:30,728
pieces of artwork, we write the results

416
00:20:30,728 --> 00:20:33,663
to our S3 results bucket.

417
00:20:36,690 --> 00:20:40,260
Once the results are written
we use our results calculator

418
00:20:40,260 --> 00:20:44,140
to generate the statistics that
we saw in the previous slide

419
00:20:45,000 --> 00:20:49,833
and we also use that data to
determine some next steps.

420
00:20:53,190 --> 00:20:57,420
Finally, we will send

421
00:20:57,420 --> 00:21:02,040
that data on both the results
from the defect detection

422
00:21:02,040 --> 00:21:05,700
as well as the results
that were calculated

423
00:21:05,700 --> 00:21:08,070
and send that to our prompt improver.

424
00:21:08,070 --> 00:21:09,600
The prompt improver agent

425
00:21:09,600 --> 00:21:11,520
will take a look at all of that data

426
00:21:11,520 --> 00:21:15,900
and make determination on
what should be done next.

427
00:21:15,900 --> 00:21:18,810
That could be making
changes to the prompt,

428
00:21:18,810 --> 00:21:22,590
it could be suggesting
different models to use

429
00:21:22,590 --> 00:21:25,473
or different configurations
for using those models.

430
00:21:27,180 --> 00:21:30,843
Now once we've gotten
through that process,

431
00:21:31,860 --> 00:21:35,430
that data is written
back to our config table

432
00:21:35,430 --> 00:21:37,653
and then can be used for the next run.

433
00:21:41,010 --> 00:21:43,260
I'll take a step back real quick.

434
00:21:43,260 --> 00:21:47,493
So Strands is doing a few things for us.

435
00:21:48,660 --> 00:21:52,113
At first it's simplifying
our interaction with LLMs.

436
00:21:53,280 --> 00:21:57,640
Second, it is allowing us to
easily create relationships

437
00:21:58,604 --> 00:22:00,180
with between agents.

438
00:22:00,180 --> 00:22:02,970
And it also provides
some out-of-the-box tools

439
00:22:02,970 --> 00:22:05,070
that we've been able to take advantage of.

440
00:22:07,230 --> 00:22:09,120
So when we're talking about tools,

441
00:22:09,120 --> 00:22:12,990
there's a few built in tools
that we use on a regular basis.

442
00:22:12,990 --> 00:22:16,350
One would be the image
reader which allows us

443
00:22:16,350 --> 00:22:19,410
to prepare the artwork

444
00:22:19,410 --> 00:22:23,130
for the LLM when it's
initially being called

445
00:22:23,130 --> 00:22:25,110
or the LMM.

446
00:22:25,110 --> 00:22:27,240
We use file read and write

447
00:22:27,240 --> 00:22:31,740
so that we can do intermediate
manipulation of the images

448
00:22:31,740 --> 00:22:34,590
as a part of certain processes.

449
00:22:34,590 --> 00:22:37,890
And then also agents as tools
to create those relationships

450
00:22:37,890 --> 00:22:41,373
between the agents and be
able to call them explicitly.

451
00:22:42,420 --> 00:22:45,720
On top of that we've also
created some custom tools.

452
00:22:45,720 --> 00:22:50,220
So like the safe zone
example that we talked about,

453
00:22:50,220 --> 00:22:53,160
we have a crop image custom tool.

454
00:22:53,160 --> 00:22:57,030
We have also a transparency
check tool for readability

455
00:22:57,030 --> 00:23:00,330
and accessibility and
that the set of tools

456
00:23:00,330 --> 00:23:02,520
that we have available to us now

457
00:23:02,520 --> 00:23:04,023
has grown significantly.

458
00:23:07,500 --> 00:23:11,520
Now unlike the safe zone,

459
00:23:11,520 --> 00:23:15,420
defect detection process,
not everything can be

460
00:23:15,420 --> 00:23:18,960
a pass/fail or can we use that information

461
00:23:18,960 --> 00:23:22,620
as pass/fail in order
to improve the system.

462
00:23:22,620 --> 00:23:25,680
Sometimes we need qualitative results

463
00:23:25,680 --> 00:23:29,490
as well as the quantitative
results in order to decide

464
00:23:29,490 --> 00:23:31,830
what our next steps are.

465
00:23:31,830 --> 00:23:35,340
So what we do is we create a judge

466
00:23:35,340 --> 00:23:39,810
that takes a look at the
evaluation performance

467
00:23:39,810 --> 00:23:44,810
and provides some additional
context for why things failed,

468
00:23:45,090 --> 00:23:49,893
what could be done better on
that individual artwork basis.

469
00:23:51,330 --> 00:23:56,070
And so what we do is we provide
some initial configuration

470
00:23:56,070 --> 00:24:00,150
to our DynamoDB table for the judge.

471
00:24:00,150 --> 00:24:04,350
The judge configurator
agent reads that information

472
00:24:04,350 --> 00:24:07,830
and prepares it for the judge itself.

473
00:24:07,830 --> 00:24:10,890
And then the judge will then take

474
00:24:10,890 --> 00:24:15,207
a look at the results of the evaluation

475
00:24:16,080 --> 00:24:20,010
and provide additional
context that can then be used

476
00:24:20,010 --> 00:24:22,323
as a part of the prompt improver step.

477
00:24:23,520 --> 00:24:26,010
Now we could use a judge

478
00:24:26,010 --> 00:24:29,850
for all of our defects,

479
00:24:29,850 --> 00:24:31,770
but it's expensive

480
00:24:31,770 --> 00:24:35,460
and so we only add that configuration

481
00:24:35,460 --> 00:24:39,060
and step when it's absolutely necessary.

482
00:24:39,060 --> 00:24:41,220
But it has been critical to get

483
00:24:41,220 --> 00:24:44,793
some of our defect detection
mechanisms in place.

484
00:24:46,380 --> 00:24:49,920
Now overall Strands has greatly simplified

485
00:24:49,920 --> 00:24:52,260
a lot of what we're trying to do.

486
00:24:52,260 --> 00:24:56,970
It has made it so that we don't
have to move images around,

487
00:24:56,970 --> 00:24:59,490
all of our interfaces are text based,

488
00:24:59,490 --> 00:25:04,490
we can access the images centrally.

489
00:25:05,400 --> 00:25:07,770
And we're also able to use the system

490
00:25:07,770 --> 00:25:11,160
to run regression tests.

491
00:25:11,160 --> 00:25:14,670
So if we want to change the model

492
00:25:14,670 --> 00:25:18,570
or if we want to change the configuration

493
00:25:18,570 --> 00:25:20,610
or a prompt, we can validate

494
00:25:20,610 --> 00:25:22,310
that we haven't made things worse.

495
00:25:24,630 --> 00:25:28,410
This has been so successful
that we've also started using it

496
00:25:28,410 --> 00:25:30,540
for things like text.

497
00:25:30,540 --> 00:25:33,390
So like I mentioned, we receive metadata

498
00:25:33,390 --> 00:25:36,750
and so we have to
validate say the synopsis.

499
00:25:36,750 --> 00:25:39,870
And so we have a bunch of
defects that we look for

500
00:25:39,870 --> 00:25:42,693
in a synopsis that is running
through the same process.

501
00:25:46,110 --> 00:25:47,850
So like I was saying before,

502
00:25:47,850 --> 00:25:50,250
we generally use about 2000 images

503
00:25:50,250 --> 00:25:54,030
and we have humans run through them

504
00:25:54,030 --> 00:25:56,733
and provide the ground truth result.

505
00:25:58,380 --> 00:26:00,780
When we initially started this process,

506
00:26:00,780 --> 00:26:03,900
we were running into some

507
00:26:03,900 --> 00:26:06,180
local maximums with precision.

508
00:26:06,180 --> 00:26:09,030
We just weren't hitting
the values that we thought

509
00:26:09,030 --> 00:26:11,070
that we should be able to reach.

510
00:26:11,070 --> 00:26:12,690
So we were running into situations

511
00:26:12,690 --> 00:26:17,100
where when we would fix one false positive

512
00:26:17,100 --> 00:26:21,210
or false negative we
would cause another one.

513
00:26:21,210 --> 00:26:22,830
And what we realized is that

514
00:26:22,830 --> 00:26:24,870
as we started digging into the data

515
00:26:24,870 --> 00:26:29,870
that the manual evaluators were
using inconsistent criteria.

516
00:26:31,500 --> 00:26:33,120
Some of them would pass something

517
00:26:33,120 --> 00:26:35,463
that others would fail and vice versa.

518
00:26:36,630 --> 00:26:40,320
And so as a part of
this process we ended up

519
00:26:40,320 --> 00:26:42,750
creating a standard operating procedure

520
00:26:42,750 --> 00:26:44,700
for the manual evaluators

521
00:26:44,700 --> 00:26:49,350
that would also be shared
with the automated system

522
00:26:49,350 --> 00:26:51,960
and so we could have consistent results.

523
00:26:51,960 --> 00:26:53,190
And those consistent results

524
00:26:53,190 --> 00:26:56,010
led to a better ground truth data set.

525
00:26:56,010 --> 00:26:59,130
And that better ground
truth data set allowed us

526
00:26:59,130 --> 00:27:01,710
to run that loop that I was showing

527
00:27:01,710 --> 00:27:05,430
where we could run an evaluation data set,

528
00:27:05,430 --> 00:27:08,340
we could look for ways to improve it

529
00:27:08,340 --> 00:27:12,030
and then run another evaluation set.

530
00:27:12,030 --> 00:27:15,360
Basically creating a auto tuning mechanism

531
00:27:15,360 --> 00:27:17,640
where it was completely
hands off the wheel

532
00:27:17,640 --> 00:27:21,723
and we could just watch
the system improve itself.

533
00:27:24,810 --> 00:27:28,290
And so that actually simplified
our runtime solution.

534
00:27:28,290 --> 00:27:33,290
So we take the configuration
that we were generating

535
00:27:33,900 --> 00:27:36,450
in our evaluation phase

536
00:27:36,450 --> 00:27:40,950
and we load that into
an app config instance.

537
00:27:40,950 --> 00:27:45,180
Then we allow our partners
to upload their artwork

538
00:27:45,180 --> 00:27:48,600
through our portal, goes
through API gateway,

539
00:27:48,600 --> 00:27:52,440
and then is delegated to modules

540
00:27:52,440 --> 00:27:55,353
each representing a particular defect.

541
00:27:56,220 --> 00:28:01,050
So we can run defect detection in parallel

542
00:28:01,050 --> 00:28:05,280
and for each defect detection
mechanism we are reading

543
00:28:05,280 --> 00:28:09,840
the configuration from the app config

544
00:28:09,840 --> 00:28:14,760
in combination with providing
the artwork to Bedrock.

545
00:28:14,760 --> 00:28:19,710
And we're able to generate
those results almost real time.

546
00:28:19,710 --> 00:28:22,770
So where it would take
several days for the partners

547
00:28:22,770 --> 00:28:26,070
to get the results back,
we're actually providing that

548
00:28:26,070 --> 00:28:28,170
within a minute.

549
00:28:28,170 --> 00:28:30,360
So when it comes...

550
00:28:30,360 --> 00:28:34,260
So the solution isn't perfect

551
00:28:34,260 --> 00:28:37,770
and so when the partner gets a result

552
00:28:37,770 --> 00:28:39,840
that they don't agree with,

553
00:28:39,840 --> 00:28:44,250
they are allowed to override that result.

554
00:28:44,250 --> 00:28:47,610
So what that means is we recommend

555
00:28:47,610 --> 00:28:50,610
that they make some particular update

556
00:28:50,610 --> 00:28:52,830
and they think the artwork's fine.

557
00:28:52,830 --> 00:28:56,970
So it goes into a manual evaluator's queue

558
00:28:56,970 --> 00:28:59,043
and we'll use the old process.

559
00:29:00,120 --> 00:29:02,940
But the great thing about
this solution is that

560
00:29:02,940 --> 00:29:06,930
where we were reviewing
100% of the artwork

561
00:29:06,930 --> 00:29:11,220
that was provided to us, now
we're only reviewing say 10,

562
00:29:11,220 --> 00:29:13,080
12% of the artwork.

563
00:29:13,080 --> 00:29:17,430
And that has been a huge time saver

564
00:29:17,430 --> 00:29:19,893
for our manual evaluators.

565
00:29:22,350 --> 00:29:25,860
So through this process,

566
00:29:25,860 --> 00:29:28,620
starting with the Q CLI solution,

567
00:29:28,620 --> 00:29:31,080
we were only at about 37% precision.

568
00:29:31,080 --> 00:29:33,801
We were able to get that number to 78%

569
00:29:33,801 --> 00:29:36,810
for the safe zone.

570
00:29:36,810 --> 00:29:41,040
We were able to reduce false
positives and negatives by 70%

571
00:29:41,040 --> 00:29:46,040
and we were also able to
reduce the amount of time

572
00:29:46,560 --> 00:29:50,400
it takes to get results from several days

573
00:29:50,400 --> 00:29:53,583
to less than an hour for
certain circumstances.

574
00:29:55,800 --> 00:29:57,600
But most importantly
we were able to reduce

575
00:29:57,600 --> 00:30:00,023
that manual effort by 88%.

576
00:30:02,220 --> 00:30:04,270
So we learned a few things along the way.

577
00:30:05,220 --> 00:30:08,850
First was don't try to
do too much at once.

578
00:30:08,850 --> 00:30:12,000
The context windows have
been growing recently

579
00:30:12,000 --> 00:30:15,420
and we initially tried to
take advantage of that.

580
00:30:15,420 --> 00:30:17,880
We tried to run defect detection

581
00:30:17,880 --> 00:30:21,210
for multiple types of issues
at once and found that

582
00:30:21,210 --> 00:30:22,890
that was just not the way to go.

583
00:30:22,890 --> 00:30:26,340
So what we ended up doing
was we broke the problem down

584
00:30:26,340 --> 00:30:29,790
into the individual defect types,

585
00:30:29,790 --> 00:30:32,760
ranked them by how often they occurred

586
00:30:32,760 --> 00:30:37,760
and how much effort it was to
manually perform the detection

587
00:30:38,040 --> 00:30:40,233
and tackle them one at a time.

588
00:30:41,520 --> 00:30:42,930
The next thing that we learned

589
00:30:42,930 --> 00:30:45,330
is that we can really use generative AI

590
00:30:45,330 --> 00:30:46,920
throughout the lifecycle.

591
00:30:46,920 --> 00:30:51,480
So we started off with our
initial proof of concept

592
00:30:51,480 --> 00:30:53,850
being generated by Q CLI.

593
00:30:53,850 --> 00:30:57,720
Then we used generative AI

594
00:30:57,720 --> 00:31:01,270
to create our system design

595
00:31:02,820 --> 00:31:06,090
for development and then for evaluation

596
00:31:06,090 --> 00:31:09,183
and a lot of our monitoring.

597
00:31:10,050 --> 00:31:13,560
So we estimate that we used generative AI

598
00:31:13,560 --> 00:31:17,670
for about 85% of what I showed

599
00:31:17,670 --> 00:31:22,547
for both the evaluation framework
and the production system.

600
00:31:22,547 --> 00:31:24,630
And so it was a huge time saver,

601
00:31:24,630 --> 00:31:26,613
saved months of engineering work.

602
00:31:27,660 --> 00:31:29,970
We also found that LLMs are effective

603
00:31:29,970 --> 00:31:32,130
at proving their own prompts.

604
00:31:32,130 --> 00:31:36,210
So we used Claude to
take a look at prompts

605
00:31:36,210 --> 00:31:38,310
that we were providing to Claude

606
00:31:38,310 --> 00:31:41,130
and it was effective at telling us

607
00:31:41,130 --> 00:31:44,880
where we could improve
things that were specific

608
00:31:44,880 --> 00:31:46,233
to that particular model.

609
00:31:47,580 --> 00:31:51,510
We also found it very helpful to establish

610
00:31:51,510 --> 00:31:53,340
that robust evaluation loop.

611
00:31:53,340 --> 00:31:56,253
So being able to just iterate quickly,

612
00:31:57,150 --> 00:31:59,340
even when the changes that
we were making were manual,

613
00:31:59,340 --> 00:32:02,940
we could just kick the process
off again, see how it worked

614
00:32:02,940 --> 00:32:07,110
and not spend so much time
in the investigation phase

615
00:32:07,110 --> 00:32:09,900
was critical for our success.

616
00:32:09,900 --> 00:32:14,640
And last we learned that
manual evaluation is hard

617
00:32:14,640 --> 00:32:18,090
and error prone and so it was worth it

618
00:32:18,090 --> 00:32:22,740
to take the time to generate
a high quality data set

619
00:32:22,740 --> 00:32:25,170
so that we can make sure

620
00:32:25,170 --> 00:32:27,483
that our automated
processes are successful.

621
00:32:30,300 --> 00:32:34,770
So we started off with the safe zone.

622
00:32:34,770 --> 00:32:38,373
We've since moved to
logo and text placement,

623
00:32:39,210 --> 00:32:41,580
offensive and mature content.

624
00:32:41,580 --> 00:32:44,820
We're taking a look at text legibility,

625
00:32:44,820 --> 00:32:47,190
localization and accessibility.

626
00:32:47,190 --> 00:32:50,193
All of these things are
either in production,

627
00:32:51,120 --> 00:32:55,150
being able to be used by our
partners or they are able

628
00:32:56,220 --> 00:32:58,470
or they will be there
by the end of the year.

629
00:32:59,880 --> 00:33:04,880
So that's the presentation.

630
00:33:04,920 --> 00:33:06,810
It was great to be able
to share that with you

631
00:33:06,810 --> 00:33:09,120
and I'm gonna hand it back to Tulip

632
00:33:09,120 --> 00:33:10,413
who is going to continue.

633
00:33:15,090 --> 00:33:16,140
- Thank you, Brian.

634
00:33:16,140 --> 00:33:17,820
And so you heard from Brian like

635
00:33:17,820 --> 00:33:20,157
how they were able to use Agent TKI,

636
00:33:20,157 --> 00:33:23,340
how they were able to use
Strands Agents in their framework

637
00:33:23,340 --> 00:33:26,550
to be able to moderate
their artwork quality

638
00:33:26,550 --> 00:33:28,440
and be able to place
it at the right place.

639
00:33:28,440 --> 00:33:31,860
And so that, you know, I
want to talk about agent TKI

640
00:33:31,860 --> 00:33:34,080
a little bit more before
I hand it over to Mona

641
00:33:34,080 --> 00:33:37,593
to talk about her story about
Strands Agents and GenAI.

642
00:33:38,610 --> 00:33:41,430
So when we think about the two flavors,

643
00:33:41,430 --> 00:33:43,170
one, you know, is basically

644
00:33:43,170 --> 00:33:44,790
accelerating software development

645
00:33:44,790 --> 00:33:48,000
where we talked about the
applications like Kiro

646
00:33:48,000 --> 00:33:51,240
and Q Developer, helping
you to be the code assistant

647
00:33:51,240 --> 00:33:53,220
and helping you code and develop apps.

648
00:33:53,220 --> 00:33:55,920
And the second persona is like reimagining

649
00:33:55,920 --> 00:33:58,410
business workflows with custom agents.

650
00:33:58,410 --> 00:34:01,470
And this is where Mona's use
case becomes really important.

651
00:34:01,470 --> 00:34:05,040
So they were able to use
a few agents out there

652
00:34:05,040 --> 00:34:07,410
that were able to orchestrate between them

653
00:34:07,410 --> 00:34:09,240
and create that custom workflow

654
00:34:09,240 --> 00:34:11,730
to be able to accomplish
that goal that they need

655
00:34:11,730 --> 00:34:14,193
which was improving the streaming quality.

656
00:34:15,480 --> 00:34:17,880
And so this is the one
that we are gonna focus on

657
00:34:17,880 --> 00:34:22,290
in the next use case that
Mona is gonna talk about.

658
00:34:22,290 --> 00:34:25,410
And so briefly when we talk about

659
00:34:25,410 --> 00:34:27,453
enterprise agentic AI applications,

660
00:34:28,380 --> 00:34:31,200
we have our LLM in the
center, that's the brain.

661
00:34:31,200 --> 00:34:34,440
That basically reasons,
that basically understand

662
00:34:34,440 --> 00:34:37,680
and gives you the output
but it needs help.

663
00:34:37,680 --> 00:34:38,760
Like when we talk about LLMs,

664
00:34:38,760 --> 00:34:40,620
it probably doesn't have
the latest information.

665
00:34:40,620 --> 00:34:42,930
So if you go and ask
any of the LLMs out here

666
00:34:42,930 --> 00:34:44,250
like what's the weather in New York,

667
00:34:44,250 --> 00:34:45,840
it wouldn't be able to answer it

668
00:34:45,840 --> 00:34:47,910
because it doesn't know what date it is,

669
00:34:47,910 --> 00:34:50,490
it doesn't know what the weather is.

670
00:34:50,490 --> 00:34:53,070
So we want to give it some data points

671
00:34:53,070 --> 00:34:56,280
and so the tools out
there help it call maybe

672
00:34:56,280 --> 00:34:58,110
like a weather API or the current time,

673
00:34:58,110 --> 00:35:00,390
some of the database can maybe shows you

674
00:35:00,390 --> 00:35:02,760
like where New York is, things like that.

675
00:35:02,760 --> 00:35:06,630
The information it needs to
be able to correctly reason

676
00:35:06,630 --> 00:35:08,070
and give you that information.

677
00:35:08,070 --> 00:35:09,930
It also probably needs memory to be able

678
00:35:09,930 --> 00:35:12,480
to understand the current
information, current context

679
00:35:12,480 --> 00:35:15,240
and conversation and take that action

680
00:35:15,240 --> 00:35:16,680
and give you the correct prompt,

681
00:35:16,680 --> 00:35:17,850
give you the correct answer like

682
00:35:17,850 --> 00:35:20,490
oh it's 56 degree Fahrenheit in New York.

683
00:35:20,490 --> 00:35:22,560
I'm not sure if it's 56
degree Fahrenheit right now,

684
00:35:22,560 --> 00:35:24,633
but I'm just like saying it.

685
00:35:24,633 --> 00:35:27,690
And also like another
important thing is observation

686
00:35:27,690 --> 00:35:30,240
and guardrails where we want to ensure

687
00:35:30,240 --> 00:35:32,070
that if you are asking it
for weather information,

688
00:35:32,070 --> 00:35:34,380
it doesn't answer me like about politics,

689
00:35:34,380 --> 00:35:35,213
answering me about like who

690
00:35:35,213 --> 00:35:37,680
the current President of United States is.

691
00:35:37,680 --> 00:35:40,230
So we want to be able to
restrict it to be able

692
00:35:40,230 --> 00:35:42,240
to answer only for the prompt

693
00:35:42,240 --> 00:35:44,343
or the context that we are asking it for.

694
00:35:46,140 --> 00:35:48,900
And so with Amazon Bedrock
you're able to do all of that.

695
00:35:48,900 --> 00:35:52,440
It provides you the
ability to access models,

696
00:35:52,440 --> 00:35:55,170
it provides you ability
to call tools with MCP,

697
00:35:55,170 --> 00:35:58,200
which is model context
protocol, that's been introduced

698
00:35:58,200 --> 00:36:01,050
by Anthropic and A2A
as well, agent to agent

699
00:36:01,050 --> 00:36:03,300
like how your multiple
agents can orchestrate.

700
00:36:03,300 --> 00:36:04,803
Provides you frameworks
like Strands Agents

701
00:36:04,803 --> 00:36:05,940
that we talked about,

702
00:36:05,940 --> 00:36:08,700
CrewAI and LangGraph to be able to build

703
00:36:08,700 --> 00:36:10,290
that agentic AI framework,

704
00:36:10,290 --> 00:36:14,550
agent code to deploy your
agents at a scale on AWS

705
00:36:14,550 --> 00:36:17,130
with your runtime gateway,
memory and observability.

706
00:36:17,130 --> 00:36:20,130
And obviously it provides
you the ability to customize

707
00:36:20,130 --> 00:36:22,140
and fine tune your models.

708
00:36:22,140 --> 00:36:25,020
And with that I'll hand it over to Mona

709
00:36:25,020 --> 00:36:27,660
to talk about her agentic AI use case,

710
00:36:27,660 --> 00:36:29,583
how they improve streaming quality.

711
00:36:33,720 --> 00:36:35,610
- All right, thanks, Tulip.

712
00:36:35,610 --> 00:36:36,900
So quick show of hands,

713
00:36:36,900 --> 00:36:39,810
how many of you have been
on call trying to use

714
00:36:39,810 --> 00:36:43,203
a whole bunch of data to detect
localized mitigate issues?

715
00:36:44,340 --> 00:36:46,080
Alright, I see a few hands here.

716
00:36:46,080 --> 00:36:48,300
Well I'm very excited to
talk about our journey

717
00:36:48,300 --> 00:36:50,040
of building an agentic workflow

718
00:36:50,040 --> 00:36:52,170
to detect localize root cause

719
00:36:52,170 --> 00:36:54,420
and mitigate streaming quality issues.

720
00:36:54,420 --> 00:36:57,393
I'm Mona and I'm a senior
manager at Amazon Prime Video.

721
00:36:59,280 --> 00:37:01,950
Prime Video is a global platform.

722
00:37:01,950 --> 00:37:05,310
We stream high scale live
events, video on demand

723
00:37:05,310 --> 00:37:08,070
and linear content for our customers.

724
00:37:08,070 --> 00:37:10,080
Ensuring our customers are able

725
00:37:10,080 --> 00:37:12,030
to watch their favorite content,

726
00:37:12,030 --> 00:37:15,390
whether it is the Patriot
scoring a touchdown, go Pats,

727
00:37:15,390 --> 00:37:18,090
or you know, Barcelona scoring a goal.

728
00:37:18,090 --> 00:37:20,580
We want to be able to
obsess over our customers

729
00:37:20,580 --> 00:37:22,280
being able to take in that moment.

730
00:37:23,370 --> 00:37:26,160
Sometimes what can
happen is while streaming

731
00:37:26,160 --> 00:37:29,160
to millions of customers,
even an interruption

732
00:37:29,160 --> 00:37:32,040
for a brief moment can mean that thousands

733
00:37:32,040 --> 00:37:34,350
or millions of customers are interrupted

734
00:37:34,350 --> 00:37:36,570
from their viewing experience.

735
00:37:36,570 --> 00:37:38,400
Traditional operational approaches

736
00:37:38,400 --> 00:37:40,890
such as manual monitoring of metrics

737
00:37:40,890 --> 00:37:44,130
or alternatively reactive root causing

738
00:37:44,130 --> 00:37:45,933
just do not cut it at our scale.

739
00:37:46,770 --> 00:37:48,900
We asked ourselves the question,

740
00:37:48,900 --> 00:37:52,320
what must we fundamentally build a system

741
00:37:52,320 --> 00:37:54,780
that's not just monitoring these metrics

742
00:37:54,780 --> 00:37:57,300
but actively understanding these metrics,

743
00:37:57,300 --> 00:37:59,070
learning from these metrics

744
00:37:59,070 --> 00:38:02,580
and able to autonomously take
action towards these metrics.

745
00:38:02,580 --> 00:38:05,610
And that's exactly what
we sought out to build.

746
00:38:05,610 --> 00:38:08,550
Now a few challenges that
we had kind of kept in mind

747
00:38:08,550 --> 00:38:10,620
and used as the guiding principle

748
00:38:10,620 --> 00:38:12,600
while working through these systems.

749
00:38:12,600 --> 00:38:16,560
First off, we wanted our system
to be able to have access

750
00:38:16,560 --> 00:38:19,350
to a multimodal sort of set of data.

751
00:38:19,350 --> 00:38:22,263
So this can include
things like time series,

752
00:38:23,250 --> 00:38:26,730
infrastructure logs, player logs, graphs,

753
00:38:26,730 --> 00:38:28,290
so on and so forth.

754
00:38:28,290 --> 00:38:31,230
We also wanted the system
to not just have access

755
00:38:31,230 --> 00:38:34,080
to this data but to actually
understand this data

756
00:38:34,080 --> 00:38:36,570
and almost build an intuition behind it.

757
00:38:36,570 --> 00:38:38,820
And finally we wanted to be able

758
00:38:38,820 --> 00:38:40,860
to have the system accessible

759
00:38:40,860 --> 00:38:43,530
to pretty much anyone in
the engineering teams.

760
00:38:43,530 --> 00:38:45,720
So this did not require special expertise

761
00:38:45,720 --> 00:38:48,670
or domain expertise when it
came to understanding our data.

762
00:38:49,665 --> 00:38:51,930
With that in mind, we went ahead

763
00:38:51,930 --> 00:38:55,050
and built an AI agentic system

764
00:38:55,050 --> 00:38:58,590
that sort of put together multiple agents

765
00:38:58,590 --> 00:39:00,780
that were orchestrated using Strand

766
00:39:00,780 --> 00:39:03,870
to be able to accomplish these tasks.

767
00:39:03,870 --> 00:39:07,380
One of the qualities of the
system is that it can reason

768
00:39:07,380 --> 00:39:08,910
through complex tasks,

769
00:39:08,910 --> 00:39:11,880
break them down into multiple simple tasks

770
00:39:11,880 --> 00:39:13,980
and then chain them together in a sequence

771
00:39:13,980 --> 00:39:15,513
to accomplish the set task.

772
00:39:16,500 --> 00:39:18,900
We also made sure that the system

773
00:39:18,900 --> 00:39:20,250
was not just a one and done

774
00:39:20,250 --> 00:39:23,430
but was constantly learning
from all of the data around it,

775
00:39:23,430 --> 00:39:26,910
keeping the most current
operational snapshot of the data

776
00:39:26,910 --> 00:39:29,520
and also be able to sort of actively learn

777
00:39:29,520 --> 00:39:32,253
from any past mistakes or
feedback that it received.

778
00:39:34,710 --> 00:39:38,400
So with that, let's take
an overall 30,000 feet view

779
00:39:38,400 --> 00:39:41,130
of what the architecture looks like.

780
00:39:41,130 --> 00:39:43,770
This system was built as an AWS native

781
00:39:43,770 --> 00:39:46,260
as well as an AI native system.

782
00:39:46,260 --> 00:39:48,540
It sort of orchestrated with Strands,

783
00:39:48,540 --> 00:39:51,810
uses AWS Lambda for
things like authentication

784
00:39:51,810 --> 00:39:53,460
as well as things like orchestrating

785
00:39:53,460 --> 00:39:55,650
across the different agents.

786
00:39:55,650 --> 00:39:59,460
As well as uses Athena both
for querying as a data store

787
00:39:59,460 --> 00:40:02,460
as well as DynamoDB for some
of the global state management.

788
00:40:03,480 --> 00:40:06,360
This system is the
foundational backend system

789
00:40:06,360 --> 00:40:08,340
that can be used for a multitude

790
00:40:08,340 --> 00:40:10,410
of different frontend interfaces.

791
00:40:10,410 --> 00:40:13,740
So it can be used for example
as a chat bot interface

792
00:40:13,740 --> 00:40:15,840
where somebody can put in
a natural language question

793
00:40:15,840 --> 00:40:17,400
and be able to get an answer.

794
00:40:17,400 --> 00:40:21,000
It can also be used to sort
of autonomously be triggered

795
00:40:21,000 --> 00:40:24,300
by a different system that
maybe has detected an issue.

796
00:40:24,300 --> 00:40:27,630
So really a bunch of different,
you know, sort of use cases

797
00:40:27,630 --> 00:40:30,303
that can be facilitated with
the same backend system.

798
00:40:31,590 --> 00:40:35,880
So diving into the components
of the system, as I said,

799
00:40:35,880 --> 00:40:37,830
this is a multi-agent system.

800
00:40:37,830 --> 00:40:40,980
We have a bunch of different
agents as well as sub-agents

801
00:40:40,980 --> 00:40:42,870
kind of working together to be able

802
00:40:42,870 --> 00:40:44,730
to accomplish the task.

803
00:40:44,730 --> 00:40:48,363
First off, we start by looking
at the request handler.

804
00:40:49,672 --> 00:40:54,672
The request handler is sort of
the front gate of the system.

805
00:40:54,690 --> 00:40:57,690
What the request handler does
is once it receives a request,

806
00:40:57,690 --> 00:41:00,360
it'll first go ahead and
authenticate the request,

807
00:41:00,360 --> 00:41:04,170
then it'll validate the
request and then it'll go ahead

808
00:41:04,170 --> 00:41:07,620
and start to decompose this
request into simpler tasks.

809
00:41:07,620 --> 00:41:10,380
So for example, if the question asked was

810
00:41:10,380 --> 00:41:13,650
what was the rebuffing rate
on iPhone devices in Germany

811
00:41:13,650 --> 00:41:15,090
over the past week?

812
00:41:15,090 --> 00:41:17,580
The request handler will
sort of break this down

813
00:41:17,580 --> 00:41:21,180
to understand, okay, this
ask involves a metric ask,

814
00:41:21,180 --> 00:41:23,340
it involves a trend analysis ask,

815
00:41:23,340 --> 00:41:26,190
and then it also involves an
ask for a specific cohort.

816
00:41:26,190 --> 00:41:27,990
Cohort being devices,

817
00:41:27,990 --> 00:41:30,870
geographic information and time periods.

818
00:41:30,870 --> 00:41:33,570
The request handler also
has a guardrail agent

819
00:41:33,570 --> 00:41:35,550
as you heard from Tulip previously,

820
00:41:35,550 --> 00:41:37,920
that's sort of able to validate

821
00:41:37,920 --> 00:41:40,140
and make sure that the
request is compliant

822
00:41:40,140 --> 00:41:42,393
with what the system
is supposed to support.

823
00:41:43,800 --> 00:41:46,770
Now that we have kind of talked
about the request handler,

824
00:41:46,770 --> 00:41:49,920
let's look at what happens after that.

825
00:41:49,920 --> 00:41:53,310
Suppose the request handler
is the routing agent.

826
00:41:53,310 --> 00:41:55,350
You can think of the routing agent

827
00:41:55,350 --> 00:41:57,720
as an intelligent orchestrator

828
00:41:57,720 --> 00:42:01,230
or sort of a traffic controller
that based on the request

829
00:42:01,230 --> 00:42:04,770
that it got from the handler
sort of tries to understand

830
00:42:04,770 --> 00:42:07,320
what are the different capabilities
that need to be invoked

831
00:42:07,320 --> 00:42:09,750
to be able to serve as this request.

832
00:42:09,750 --> 00:42:12,060
So the routing agent kind of understands

833
00:42:12,060 --> 00:42:13,860
what those capabilities are

834
00:42:13,860 --> 00:42:16,260
and sort of passes that along as a signal

835
00:42:16,260 --> 00:42:19,500
to invoke other sub-agents, agents, tools,

836
00:42:19,500 --> 00:42:21,360
data sources and so on.

837
00:42:21,360 --> 00:42:23,760
So it can be really thought
as the brain of the operation

838
00:42:23,760 --> 00:42:26,340
once it gets that decomposed request.

839
00:42:26,340 --> 00:42:29,400
The routing agent also uses
the chain of thought process

840
00:42:29,400 --> 00:42:31,830
in terms of breaking down a complex task,

841
00:42:31,830 --> 00:42:34,140
kind of reasoning through
it like a human would

842
00:42:34,140 --> 00:42:36,450
and then finally understanding
what are the capabilities

843
00:42:36,450 --> 00:42:37,473
that it requires.

844
00:42:39,060 --> 00:42:40,320
From the routing agent,

845
00:42:40,320 --> 00:42:43,320
we then have the integrator subagent.

846
00:42:43,320 --> 00:42:46,110
The integrator subagent can be thought of

847
00:42:46,110 --> 00:42:48,153
as some sort of a traffic controller.

848
00:42:49,020 --> 00:42:51,420
It's sort of, you know,
once it has got the request

849
00:42:51,420 --> 00:42:55,530
from the routing agent, it
knows what specific tools

850
00:42:55,530 --> 00:42:57,933
and data sources it needs to connect to.

851
00:42:58,800 --> 00:43:02,250
This integrator subagent
sort of works through MCP,

852
00:43:02,250 --> 00:43:04,140
which is model context protocol,

853
00:43:04,140 --> 00:43:07,170
and is able to talk through
a host of different tools

854
00:43:07,170 --> 00:43:08,940
as well as data sources

855
00:43:08,940 --> 00:43:10,080
and is able to work

856
00:43:10,080 --> 00:43:13,350
through things like different
access patterns, APIs,

857
00:43:13,350 --> 00:43:16,050
access formats, so on and so forth.

858
00:43:16,050 --> 00:43:17,850
It's also able to kind of combine

859
00:43:17,850 --> 00:43:19,260
a bunch of these data together

860
00:43:19,260 --> 00:43:22,650
by knowing the right joint
conditions and so on.

861
00:43:22,650 --> 00:43:24,450
The other thing about
the integrator subagent

862
00:43:24,450 --> 00:43:27,330
is that it also sort of
serves as a data quality check

863
00:43:27,330 --> 00:43:29,490
and makes sure that it's
only the right kind of data

864
00:43:29,490 --> 00:43:30,720
and the right quality of data

865
00:43:30,720 --> 00:43:32,270
that is accessed by the system.

866
00:43:34,140 --> 00:43:36,480
Post the integrator subagent,

867
00:43:36,480 --> 00:43:39,840
we then have what is
the analysis subagent.

868
00:43:39,840 --> 00:43:42,480
The analysis subagent
can really be thought of

869
00:43:42,480 --> 00:43:44,640
as a data scientist in a box.

870
00:43:44,640 --> 00:43:48,870
The analysis subagent is
primarily based out on Bedrock

871
00:43:48,870 --> 00:43:52,140
and it has a whole bunch of
both large language models

872
00:43:52,140 --> 00:43:54,870
as well as small language
models that it can access

873
00:43:54,870 --> 00:43:58,230
in order to be able to
service a specific request.

874
00:43:58,230 --> 00:43:59,400
You can really think of this

875
00:43:59,400 --> 00:44:01,800
as sitting in an ensemble
of different models

876
00:44:01,800 --> 00:44:04,770
and sort of leveraging the
right models per the use case

877
00:44:04,770 --> 00:44:06,663
and per the capability that's needed.

878
00:44:09,060 --> 00:44:11,730
Now once we have talked
about the analysis subagent,

879
00:44:11,730 --> 00:44:15,030
the next thing that we have
is the reasoning agent.

880
00:44:15,030 --> 00:44:18,000
So the reasoning agent sort
of takes all of this input

881
00:44:18,000 --> 00:44:20,940
that it has received from
sort of the prior agents

882
00:44:20,940 --> 00:44:22,080
and sub-agents

883
00:44:22,080 --> 00:44:25,650
and what it does is it kind
of uses business context

884
00:44:25,650 --> 00:44:28,860
that it has access to
to be able to determine

885
00:44:28,860 --> 00:44:31,860
if the sort of analysis
that it has been provided

886
00:44:31,860 --> 00:44:33,390
is sort of, you know, pertinent

887
00:44:33,390 --> 00:44:35,670
and is sort of relevant with
all of the business context

888
00:44:35,670 --> 00:44:36,780
that it has.

889
00:44:36,780 --> 00:44:39,720
So this can be thought of
really as you know, sort of,

890
00:44:39,720 --> 00:44:42,090
it uses an iterative approach

891
00:44:42,090 --> 00:44:44,400
and uses LLMs as a judge to be able

892
00:44:44,400 --> 00:44:47,760
to use an independent LLM to
kind of validate the responses

893
00:44:47,760 --> 00:44:48,593
that it has received

894
00:44:48,593 --> 00:44:51,780
from the previous analysis
agent for example.

895
00:44:51,780 --> 00:44:54,900
And using that business
context is able to tell,

896
00:44:54,900 --> 00:44:57,420
okay, is this really the sort
of, you know, expected answer

897
00:44:57,420 --> 00:45:00,000
that I would have or would
it be something else?

898
00:45:00,000 --> 00:45:02,130
The reasoning agent also has the ability

899
00:45:02,130 --> 00:45:05,040
to sort of have an iterative
loop to kind of go back

900
00:45:05,040 --> 00:45:07,380
and request a different capability

901
00:45:07,380 --> 00:45:09,510
or invoke a different data source

902
00:45:09,510 --> 00:45:12,603
based on what it might have
gotten from the LLM as a judge.

903
00:45:14,580 --> 00:45:16,020
From the reasoning agent,

904
00:45:16,020 --> 00:45:18,540
we then have the response handler.

905
00:45:18,540 --> 00:45:20,880
So the response handler kind of takes

906
00:45:20,880 --> 00:45:24,780
all of the different input
that it is received from the,

907
00:45:24,780 --> 00:45:27,090
you know, routing agent,
the reasoning agent

908
00:45:27,090 --> 00:45:28,920
as well as if at all you
had to run, you know,

909
00:45:28,920 --> 00:45:30,840
multiple iterations of the loop

910
00:45:30,840 --> 00:45:33,840
that we have just talked
about and really packages

911
00:45:33,840 --> 00:45:36,210
all of this information together

912
00:45:36,210 --> 00:45:38,760
into the expected output format.

913
00:45:38,760 --> 00:45:40,950
So this could be things like the response

914
00:45:40,950 --> 00:45:42,480
to a natural language question,

915
00:45:42,480 --> 00:45:45,090
it could be generation of a complex SQL

916
00:45:45,090 --> 00:45:48,450
or alternatively it could
also be an autonomous action

917
00:45:48,450 --> 00:45:49,710
or a mitigation lever

918
00:45:49,710 --> 00:45:52,563
that can be pulled in the
response to a certain trigger.

919
00:45:53,610 --> 00:45:55,860
The response handler also interacts

920
00:45:55,860 --> 00:45:58,860
with the guardrail agent,
again to sort of make sure

921
00:45:58,860 --> 00:46:01,860
that the response is compliant
with sort of, you know,

922
00:46:01,860 --> 00:46:05,580
the required sort of data sharing

923
00:46:05,580 --> 00:46:08,253
and you know, other
such sort of activities.

924
00:46:09,360 --> 00:46:13,770
Separately from that, the
response handler can also sort of,

925
00:46:13,770 --> 00:46:16,230
you know, goes ahead and
logs all of these decisions

926
00:46:16,230 --> 00:46:18,090
that it has made similar to what

927
00:46:18,090 --> 00:46:19,000
all of the other agents have.

928
00:46:19,000 --> 00:46:22,260
So it kind of takes all of these
logs which can then be used

929
00:46:22,260 --> 00:46:24,960
for sort of, you know, reflective analysis

930
00:46:24,960 --> 00:46:27,543
in terms of improving some
of the decision-making.

931
00:46:30,000 --> 00:46:33,240
So taking a step back and
kind of looking at the system,

932
00:46:33,240 --> 00:46:35,490
you know, overall, what
we talked about, you know,

933
00:46:35,490 --> 00:46:38,730
first off was the request
handler that takes in a request,

934
00:46:38,730 --> 00:46:41,490
sort of validates it,
make sure it's the right

935
00:46:41,490 --> 00:46:43,020
sort of format,

936
00:46:43,020 --> 00:46:46,470
decomposes it into the
sort of simpler tasks,

937
00:46:46,470 --> 00:46:49,500
moves that along to the routing
agent that then sort of,

938
00:46:49,500 --> 00:46:51,000
you know, knows which capabilities

939
00:46:51,000 --> 00:46:52,680
it's going to need to invoke.

940
00:46:52,680 --> 00:46:54,660
From there, it goes to
the integrator agent

941
00:46:54,660 --> 00:46:56,907
which can orchestrate a
bunch of different tools

942
00:46:56,907 --> 00:47:00,150
and data sources followed
by the analysis agent,

943
00:47:00,150 --> 00:47:02,310
which is really the
data scientist in a box,

944
00:47:02,310 --> 00:47:05,010
ensemble of LLMs that can be used.

945
00:47:05,010 --> 00:47:06,660
Finally the reasoning agent

946
00:47:06,660 --> 00:47:08,490
that kind of uses business context

947
00:47:08,490 --> 00:47:11,820
along with what's given from
the analysis agent to make sure

948
00:47:11,820 --> 00:47:15,510
that it is sort of an
acceptable and reliable answer.

949
00:47:15,510 --> 00:47:18,390
Along with that it can
also trigger reiterations

950
00:47:18,390 --> 00:47:20,580
of the loop, invoking other capabilities

951
00:47:20,580 --> 00:47:22,860
and invoking other data sources as needed.

952
00:47:22,860 --> 00:47:25,710
And finally the response handler
that packages all of this

953
00:47:25,710 --> 00:47:29,160
and makes it available either
as some sort of an output

954
00:47:29,160 --> 00:47:32,970
or a mitigation lever
or an autonomous action.

955
00:47:32,970 --> 00:47:34,590
As you can see we also have

956
00:47:34,590 --> 00:47:36,240
both an online evaluation system,

957
00:47:36,240 --> 00:47:38,520
so similar to the LLM as
a judge that I mentioned,

958
00:47:38,520 --> 00:47:40,800
as well as an offline evaluation system

959
00:47:40,800 --> 00:47:43,500
that kind of takes all of
these logs of decision-making

960
00:47:43,500 --> 00:47:45,510
and can be used sort of iteratively

961
00:47:45,510 --> 00:47:48,060
in terms of understanding
and improving the system.

962
00:47:49,890 --> 00:47:51,630
Alright, so now that
we have kind of talked

963
00:47:51,630 --> 00:47:53,970
through overall components of the system

964
00:47:53,970 --> 00:47:57,060
and how it works in real
time to be able to sort of,

965
00:47:57,060 --> 00:47:59,070
you know, help detect, localize,

966
00:47:59,070 --> 00:48:01,110
as well as root cause issues.

967
00:48:01,110 --> 00:48:03,750
Taking a step into some of
the lessons that we learned

968
00:48:03,750 --> 00:48:07,110
through the process of
developing the system.

969
00:48:07,110 --> 00:48:11,070
First off, you know, data quality
always beats data quantity

970
00:48:11,070 --> 00:48:12,960
when you're trying to
develop such a system

971
00:48:12,960 --> 00:48:15,300
it can be sort of, you know, there can be

972
00:48:15,300 --> 00:48:16,950
a whole lot of things that
you might want to add.

973
00:48:16,950 --> 00:48:20,580
Things like infrastructure
logs, metrics, past incidents,

974
00:48:20,580 --> 00:48:22,950
you know, tickets, so on and so forth.

975
00:48:22,950 --> 00:48:25,230
But you really want to
make the most efficient use

976
00:48:25,230 --> 00:48:26,340
of your context window

977
00:48:26,340 --> 00:48:29,160
and make sure that you're
giving the right set of data

978
00:48:29,160 --> 00:48:31,830
and data that will actually
get you to the right outcomes.

979
00:48:31,830 --> 00:48:33,870
So being judicious in terms of the data

980
00:48:33,870 --> 00:48:36,060
that you sort of have
as part of the system

981
00:48:36,060 --> 00:48:38,100
is especially important.

982
00:48:38,100 --> 00:48:41,100
The next thing is building
feedback loops early and often,

983
00:48:41,100 --> 00:48:43,590
this really helps in the
efficiency of development

984
00:48:43,590 --> 00:48:45,900
as well as the efficiency
of getting your system

985
00:48:45,900 --> 00:48:47,280
to the levels of accuracy

986
00:48:47,280 --> 00:48:50,073
and levels of reliability
that you're looking for.

987
00:48:50,970 --> 00:48:53,190
The other one is planning
for failure modes.

988
00:48:53,190 --> 00:48:54,720
You know, systems do fail,

989
00:48:54,720 --> 00:48:56,370
we want to have an autonomous system

990
00:48:56,370 --> 00:48:58,410
but there are going to
be times when you might

991
00:48:58,410 --> 00:49:00,270
want to have sort of human evaluation

992
00:49:00,270 --> 00:49:02,160
or when you might want to have times

993
00:49:02,160 --> 00:49:04,200
that the system is just,
you know, sort of exposed

994
00:49:04,200 --> 00:49:05,580
to a brand new situation.

995
00:49:05,580 --> 00:49:08,580
So you do want to have safe
ways that the system can fail

996
00:49:08,580 --> 00:49:09,990
and trigger the right sort of, you know,

997
00:49:09,990 --> 00:49:11,610
human involvement as needed.

998
00:49:11,610 --> 00:49:16,530
And finally continuing to use
AI to amplify human expertise,

999
00:49:16,530 --> 00:49:19,410
whether that is understanding
the business context better,

1000
00:49:19,410 --> 00:49:21,750
data better, so on and so forth.

1001
00:49:21,750 --> 00:49:25,410
You always want to have AI
amplifying human expertise

1002
00:49:25,410 --> 00:49:28,803
throughout the SDLC software
development lifecycle process.

1003
00:49:30,180 --> 00:49:33,480
In terms of what's next for our system,

1004
00:49:33,480 --> 00:49:36,270
we want to continue to build
more mitigation levels,

1005
00:49:36,270 --> 00:49:38,580
sort of have more and
more autonomous actions

1006
00:49:38,580 --> 00:49:41,250
that the system is able
to perform by itself.

1007
00:49:41,250 --> 00:49:42,810
Continuing to use AI

1008
00:49:42,810 --> 00:49:45,150
through the software development cycle

1009
00:49:45,150 --> 00:49:47,400
in terms of accelerating our development

1010
00:49:47,400 --> 00:49:49,500
in terms of having quick prototyping,

1011
00:49:49,500 --> 00:49:51,540
using things like SageMaker,

1012
00:49:51,540 --> 00:49:53,910
as well as Strands for quick orchestration

1013
00:49:53,910 --> 00:49:55,770
and quick proof of concepts,

1014
00:49:55,770 --> 00:49:58,050
as well as using AI in the deployment,

1015
00:49:58,050 --> 00:50:01,170
as well as the overall
maintenance of the system.

1016
00:50:01,170 --> 00:50:03,750
And finally having more
and more safety mechanisms

1017
00:50:03,750 --> 00:50:06,030
so that you continue to not only build

1018
00:50:06,030 --> 00:50:08,130
but also keep trust in your system

1019
00:50:08,130 --> 00:50:09,450
and sort of have it leverage

1020
00:50:09,450 --> 00:50:11,790
more and more autonomous actions.

1021
00:50:11,790 --> 00:50:14,613
So with that I will hand it over to Tulip.

1022
00:50:19,350 --> 00:50:20,520
- Thank you, Mona.

1023
00:50:20,520 --> 00:50:23,310
So let's see how many of
you were paying attention.

1024
00:50:23,310 --> 00:50:26,310
What was the peak audience
during the commanders

1025
00:50:26,310 --> 00:50:28,350
and the Packers game in 2025?

1026
00:50:28,350 --> 00:50:29,403
Does anyone remember?

1027
00:50:31,075 --> 00:50:32,520
- 18 million.
- That's right, yes.

1028
00:50:32,520 --> 00:50:35,640
So you heard from Mona and Brian

1029
00:50:35,640 --> 00:50:39,300
about how they were able to
use GenAI in their solutions

1030
00:50:39,300 --> 00:50:40,890
in their use cases respectively,

1031
00:50:40,890 --> 00:50:42,570
and how they were able to,

1032
00:50:42,570 --> 00:50:44,490
if you remember that agentic AI evolution

1033
00:50:44,490 --> 00:50:47,010
from like going from a
more human oversight,

1034
00:50:47,010 --> 00:50:48,060
less human oversight

1035
00:50:48,060 --> 00:50:50,667
and be able to automate
a lot of those tasks.

1036
00:50:50,667 --> 00:50:54,120
And so both of them wanted to
be able to, for the use cases

1037
00:50:54,120 --> 00:50:56,820
where their main thing was
to deliver premium video

1038
00:50:56,820 --> 00:50:59,040
and artwork to scale to
millions of customers

1039
00:50:59,040 --> 00:51:02,400
across thousands of different devices.

1040
00:51:02,400 --> 00:51:04,830
And obviously when
you're trying to do that,

1041
00:51:04,830 --> 00:51:06,390
there's millions of customers globally

1042
00:51:06,390 --> 00:51:08,850
and you're streaming content 24/7.

1043
00:51:08,850 --> 00:51:11,010
And you want to be able to...

1044
00:51:11,010 --> 00:51:13,860
And it can impact thousands
or even millions of viewers

1045
00:51:13,860 --> 00:51:15,840
instantly if you're not
doing the right job.

1046
00:51:15,840 --> 00:51:18,240
So they wanted to ensure
like none of their customers

1047
00:51:18,240 --> 00:51:20,070
of Prime Videos are impacted

1048
00:51:20,070 --> 00:51:21,300
while they're trying to experiment,

1049
00:51:21,300 --> 00:51:24,210
while they're trying to like
see how it is being delivered.

1050
00:51:24,210 --> 00:51:28,170
And so that's where automation
using AI helped them.

1051
00:51:28,170 --> 00:51:31,530
Like they were able to
do it more precisely

1052
00:51:31,530 --> 00:51:32,790
with increased productivity

1053
00:51:32,790 --> 00:51:34,830
because there was less human oversight.

1054
00:51:34,830 --> 00:51:37,620
They were able to make
those agents do the work

1055
00:51:37,620 --> 00:51:40,980
and be able to experiment
it faster and faster

1056
00:51:40,980 --> 00:51:43,230
and be able to get the
output that they need.

1057
00:51:44,940 --> 00:51:48,840
And so their first key
takeaway is more automation,

1058
00:51:48,840 --> 00:51:50,760
the reduced evaluation time.

1059
00:51:50,760 --> 00:51:53,490
And you heard from Brian,
it took them days before

1060
00:51:53,490 --> 00:51:56,760
and they were able to like
reduce it to 15 minutes

1061
00:51:56,760 --> 00:51:59,670
and maintain performance
without human intervention.

1062
00:51:59,670 --> 00:52:01,170
And so like going from again,

1063
00:52:01,170 --> 00:52:03,120
that slide from like
going from right to left

1064
00:52:03,120 --> 00:52:06,003
with more human oversight
to the less human oversight.

1065
00:52:06,870 --> 00:52:08,070
They were able to scale smart.

1066
00:52:08,070 --> 00:52:11,610
So what they did is break
out problem into small chunks

1067
00:52:11,610 --> 00:52:12,750
and then scale out.

1068
00:52:12,750 --> 00:52:15,210
So if you have a bigger use
case like they wanna achieve,

1069
00:52:15,210 --> 00:52:18,300
you break it out into smaller
chunks, iterate on it,

1070
00:52:18,300 --> 00:52:20,790
work on it, AI agents,

1071
00:52:20,790 --> 00:52:23,820
and then build a multitude of agents

1072
00:52:23,820 --> 00:52:25,440
to do the orchestrated across

1073
00:52:25,440 --> 00:52:28,713
and then be able to accomplish
the goal that you need.

1074
00:52:29,610 --> 00:52:31,620
And have a robust evolution loop.

1075
00:52:31,620 --> 00:52:34,440
And so whatever you build
and whatever you wanna do,

1076
00:52:34,440 --> 00:52:36,240
even from Monas and Brian's use cases,

1077
00:52:36,240 --> 00:52:39,030
they were able to like
enable rapid iteration

1078
00:52:39,030 --> 00:52:40,980
and continuous improvement
through all of their process

1079
00:52:40,980 --> 00:52:42,900
because they had a robust evolution loop.

1080
00:52:42,900 --> 00:52:43,880
Whatever they were
building, they were able

1081
00:52:43,880 --> 00:52:46,020
to like validate the agent's outcome,

1082
00:52:46,020 --> 00:52:47,370
see if it's the right one,

1083
00:52:47,370 --> 00:52:52,020
and then basically improve
their agents as they went on

1084
00:52:52,020 --> 00:52:54,093
with their experimentation.

1085
00:52:55,680 --> 00:52:59,640
So I'll leave with this
like quote from Andy Jassy

1086
00:52:59,640 --> 00:53:02,280
who recently said, "What
makes this agentic future

1087
00:53:02,280 --> 00:53:04,890
so compelling for Amazon,
is that these agents

1088
00:53:04,890 --> 00:53:06,660
are going to change the scope and speed

1089
00:53:06,660 --> 00:53:08,940
at which we can innovate for customers."

1090
00:53:08,940 --> 00:53:10,753
And that's true, we are working on it.

1091
00:53:10,753 --> 00:53:14,640
That's the agentic future
that we look at at Amazon

1092
00:53:14,640 --> 00:53:17,883
and that's the scope that we
look at when we talk about it.

1093
00:53:20,160 --> 00:53:22,770
So if you are here,
right around the corner,

1094
00:53:22,770 --> 00:53:25,020
there's the One Amazon Lane
as well where you can see

1095
00:53:25,020 --> 00:53:26,790
some of the other use cases

1096
00:53:26,790 --> 00:53:28,680
that we have out there from Prime Video.

1097
00:53:28,680 --> 00:53:33,210
So we have the Extra
Recap, we have Rapid Recap

1098
00:53:33,210 --> 00:53:36,120
and a NASCAR Burn Bar which
kind of like shows you

1099
00:53:36,120 --> 00:53:37,920
some of the sport
innovations that we have done

1100
00:53:37,920 --> 00:53:41,580
as well as like extra recap,
if you watch Prime Video,

1101
00:53:41,580 --> 00:53:43,620
when you watch a movie or
you watch a TV episode,

1102
00:53:43,620 --> 00:53:47,460
it allows you to basically
kind of like show you

1103
00:53:47,460 --> 00:53:49,710
what the scene is about
and you can go and recap

1104
00:53:49,710 --> 00:53:51,030
and summarizes it.

1105
00:53:51,030 --> 00:53:53,430
So that's out there in the
demo at the One Amazon Lane.

1106
00:53:53,430 --> 00:53:55,350
On top of that other demos as well,

1107
00:53:55,350 --> 00:53:56,760
like the Zoox robotaxi.

1108
00:53:56,760 --> 00:53:58,530
So if you're out here,
I would highly recommend

1109
00:53:58,530 --> 00:54:00,330
just passing by it and check out

1110
00:54:00,330 --> 00:54:02,220
from some of the cool demos from Amazon,

1111
00:54:02,220 --> 00:54:04,420
including some of the
ones from Prime Video.

1112
00:54:05,550 --> 00:54:08,250
And with that I'll end the session

1113
00:54:08,250 --> 00:54:12,300
and if you have enjoyed the session,

1114
00:54:12,300 --> 00:54:13,740
you know, please complete
the session survey

1115
00:54:13,740 --> 00:54:14,940
in the mobile app.

1116
00:54:14,940 --> 00:54:16,560
We look forward to your feedback

1117
00:54:16,560 --> 00:54:18,210
and we obviously work off your feedback

1118
00:54:18,210 --> 00:54:19,740
to improve our sessions as we go.

1119
00:54:19,740 --> 00:54:22,090
So thank you so much for
attending the session.

