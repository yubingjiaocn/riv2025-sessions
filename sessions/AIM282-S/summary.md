# AWS re:Invent 2025 分会场总结：Capital One 的企业级 AI/ML 平台实践

## 会议概述

本次分会场由 Capital One 企业 AI/ML 平台与工程负责人 Avijit Bose 主讲，深入分享了 Capital One 在企业级 AI 基础设施建设方面长达十年的转型历程。作为一家拥有 30 年历史的财富 100 强金融科技公司，Capital One 展示了如何在高度监管的行业环境中，以"良好管理的速度"推进 AI 创新。

演讲的核心聚焦于两大战略支柱：定制化 AI 模型和企业级 AI 平台。Bose 强调，Capital One 不仅仅是使用现成的 AI 工具，而是构建了与自身基础设施深度融合的定制化模型和平台能力。公司在 AI 人才方面连续三年在 Evident AI Index 中排名第一，并跻身全球生成式 AI 和智能体专利前十名企业，与 Google、Microsoft、Nvidia 等科技巨头并列。

演讲详细阐述了 Capital One 如何构建世界级的 HPC 训练集群和高性能推理平台，通过结合 AWS 原生服务（如 EKS、SageMaker、Bedrock）、开源项目和自研能力，打造了一个灵活且可扩展的 AI 平台。特别值得关注的是，公司成功将多智能体应用 Chat Concierge 的响应时间从 40 秒优化到 1 秒，并通过自研的 MACAO 多智能体框架实现了企业级智能体的规模化部署。最后，演讲以智能体编码工具（如 Claude Code）的部署案例，展示了如何在零信任架构下安全地将 AI 工具推广到 14,000 名开发者手中。

## 详细时间线与关键要点

### 开场与背景介绍
[00:00 - 02:30]
- Avijit Bose 自我介绍，担任 Capital One 企业 AI/ML 平台与工程负责人
- 介绍议程：AI 在 Capital One 的应用、十年转型历程、定制化 AI 战略、AI 平台建设、智能体编码工具案例研究、人才战略

### Capital One 公司概况与 AI 成就
[02:30 - 05:00]
- Capital One 是 30 年历史的创始人领导型财富 100 强公司，美国最大的直销银行
- 创始人 Rich Fairbank 亲自参与 AI 战略制定
- 连续三年在 Evident AI Index 人才排名中位列第一（评估全球 50-60 家银行的 AI 成熟度）
- 跻身全球生成式 AI 和智能体专利前十名企业，与 Google、Microsoft、Nvidia、Samsung 并列

### AI 转型的基础能力
[05:00 - 07:30]
- 公司基因：数据驱动决策、现代技术栈、内部数据科学与工程人才、全面云化
- 2020 年关闭最后一个数据中心，全面迁移至 AWS
- 数据生态系统转型：数据优势即 AI 优势
- 企业平台 DNA：从数据到 AI 到云，构建全公司统一使用的平台而非定制化应用开发
- 卓越的风险管理能力

### 十年 AI 转型历程
[07:30 - 10:00]
- 时间线回顾：从开源技术采用到全面云化宣言，2020 年退出最后一个数据中心
- 推出 Capital One Software，将内部能力产品化服务外部客户
- 多租户理念的形成：从服务提供商视角重新审视平台设计
- 2024 年末开始构建首个智能体应用
- 2025 年 1 月推出首个面向客户的多智能体应用 Chat Concierge（银行业首例）

### 核心挑战：速度与管理的平衡
[10:00 - 12:30]
- 在监管行业中扩展 AI 面临的两大需求：创新速度 vs. 良好管理
- 每天都有新模型、新算法、新工具出现，需要快速引入
- 零容忍的网络安全和风险失败标准
- "良好管理的速度"定义：不仅 AI 技术快速发展，风险、网络安全、治理、审计等控制流程也必须同步提速

### AI 创新的八大基石
[12:30 - 14:00]
1. 专有数据、云和无服务器 GPU 计算基础设施
2. 现代技术栈
3. 企业平台
4. 内部人才
5. 风险治理
6. 模型定制化：从零构建 LLM，针对专业任务定制，需要基础设施和人才支持

### 企业 AI 平台战略
[14:00 - 17:00]
- 过去四年持续构建 AI 平台，经历多次重新设计
- 核心原则：基于 AWS 最佳服务构建（EKS、ECS、SageMaker、Bedrock、OpenSearch、EFS、S3、FSX for Lustre）
- 结合开源项目和自研能力
- 三方能力组合：AWS 服务 + 开源 + 自研专有能力
- 关键洞察：设计能够轻松整合三方能力的控制平面，避免单一供应商依赖

### HPC 训练集群建设
[17:00 - 22:00]
- 基础三要素：计算、网络、存储
- 需要在 AWS 基础上构建大量额外能力以达到前沿水平
- **基础设施层**（AWS 原生）：高速网络、调度与队列、高性能文件系统、GPU 节点恢复
- **软件栈**（开源 + 自研）：训练流水线核心，支持数百甚至数千 GPU 的分布式训练
- **用户体验层**（自研）：完全自主构建

三阶段建设历程：
- **阶段一**：AWS 提供和配置基础设施
- **阶段二**：专注训练流水线，建立 CUDA 内核知识，掌握 GPU 训练代码调试
- **阶段三**：优化停机时间，实现快速检查点和重启，整个过程约一年

### 训练基础设施架构
[22:00 - 24:30]
- 支持多样化用户群体：从 AI 研究员到数据科学家到应用开发者
- 维护多个集群，针对不同任务定制（强化学习、后训练、预训练、专用 Transformer）
- 内置多租户支持，可为特定应用或部门建立独立集群以满足数据隔离要求
- 数据层集成：Snowflake（大规模部署）、S3、数据湖，数据通过 FSX 输送到 GPU

### 模型托管与推理优化
[24:30 - 28:00]
- 训练成本固定，但推理成本持续增长
- 推理平台不优化会导致巨大开支，无法实现良好的单位经济效益
- 许多生成式 AI 应用在 POC 阶段失败的原因：未考虑如何扩展到数百万用户和每分钟数百万 token

三大优化重点：
1. 规模化的单位成本：每 token 或每查询成本
2. 用户体验：延迟和吞吐量（Chat Concierge 从 40 秒优化到约 1 秒）
3. 可靠性：大规模 GPU 推理集群的正常运行时间、依赖管理、GPU 利用率

### 托管服务 vs. 自托管的选择
[28:00 - 31:00]
- **托管提供商优势**：快速启动、按 token 付费、适合可变使用模式
- **托管提供商劣势**：数据隐私问题、需要审计、规模扩大后成本高昂
- **自托管优势**：完全控制、优化单位经济效益
- **自托管劣势**：需要 DevOps 团队、AI 工程师、GPU 配置、固定成本
- **Capital One 方案**：两者结合，设计控制平面同时支持第三方和自托管平台，根据用例灵活选择

### 推理优化实践
[31:00 - 33:00]
- **模型级优化**：定制化模型
- **基础设施栈优化**：最大化 GPU 利用率、装箱（bin packing）、多租户共享 GPU（需解决数据隔离和查询隔离）
- 成果：相比一年前实现数量级的成本降低
- 目标基准：达到与 OpenAI、Anthropic、GCP、Bedrock 等外部提供商相当的竞争力

### Chat Concierge 多智能体应用案例
[33:00 - 38:00]
- **背景**：2024 年开始为汽车业务构建首个消费者级用例
- **应用场景**：经销商网站上的聊天机器人，帮助客户查找车辆库存
- **技术栈**：
  - 多智能体框架 MACAO
  - TensorRT-LLM 用于模型托管
  - 微调的 Llama 模型
  - 自定义护栏
- **部署规模**：已在多个经销商网站上线，计划扩展到数百至数千家经销商

### MACAO 多智能体框架架构
[38:00 - 41:00]
- **理解智能体（Understanding Agent）**：与客户交互理解意图（例如："蓝色 BMW"→ 品牌 BMW + 颜色蓝色）
- **规划智能体（Planner Agent）**：基于理解结果进行规划
- **评估智能体（Evaluator Agent）**：访问沙箱环境和 API，使用合成数据进行模拟验证答案正确性
- **解释智能体（Explainer Agent）**：将评估结果转换为自然语言回复客户
- **多轮多次交互**：规划智能体可回到理解智能体请求更多信息
- **支持能力**：记忆、知识库、工具和 API（如经销商库存实时查询、预约安排）
- **通用性**：面向目标的规划-评估-解释框架具有通用性，正在推广到公司其他用例

### 智能体平台生态系统建设
[41:00 - 43:30]
- 从定制化 Chat Concierge 到通用智能体生态系统和平台
- 验证想法并看到良好使用情况后，开始构建通用化方案
- **平台架构层次**（自下而上）：
  - GenAI 基础服务（已有）
  - GenAI 核心服务（已有）
  - 智能体运行时（新建）
  - 智能体编排器（新建）
  - 智能体开发工具包（新建）
  - 智能体市场（新建）
  - 应用生命周期管理（新建）
- **目标**：公司任何人都能创建智能体，使用 SDK 开发，平台自动管理智能体
- **理念**：企业平台而非定制化能力，统一平台规模化部署智能体 AI

### 智能体编码工具案例：Claude Code 部署
[43:30 - 48:00]
- **背景**：市场上出现 Cursor、Windsurf、Claude Code、Copilot 等工具
- **挑战**：在监管企业环境中引入 LLM 存在固有风险，但工具发展迅速不能等待
- **解决方案**：为 Claude Code 创建零信任 AI 环境架构模式

技术架构：
- **Claude Code**：CLI 工具，安装在开发者笔记本上
- **访问方式**：通过 AWS Bedrock 访问 Claude（而非直接访问 Anthropic）
- **选择 Bedrock 的原因**：
  - 账户级权限管理
  - 增强安全性
  - 数据不用于训练的保证
  - 快速上市时间
  - 规模化能力
  - 访问 Claude Sonnet、Opus 等模型

自研 AI 网关：
- **Token 速率限制**：防止单个离线任务消耗大量 token（Claude Code 可批量处理多个代码库）
- **可观测性**：内置检测控制，监控流量
- **沙箱环境**：智能体在开发者桌面运行时访问文件需要适当隔离
- **配置管理**：企业管理设置 + 开发者自定义设置的组合
- **部署速度**：数周内完成部署，推广到 14,000 名开发者

### 未来展望与总结
[48:00 - 50:00]
- **垂直整合端到端栈**：为应对新技术、新模型、新架构做好准备
- **混合方案优势**：有意识地选择开源 + AWS + 第三方解决方案的组合
- **控制平面设计**：允许灵活整合不同来源能力的混合平台
- **完全基础设施控制**：不依赖特定供应商，可在每个层级调优
- **定制化优势**：从定制模型到定制硬件、定制流水线、定制推理栈、定制智能体层
- **风险与网络安全**：通过自动化控制和专用环境实现"良好管理的速度"
- **核心竞争力**：在高度监管行业中以创新速度安全部署 AI 的能力

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


会议核心价值： 本次分享为金融等监管行业的企业提供了一个完整的 AI 平台建设蓝图，展示了如何在不牺牲安全性和合规性的前提下，快速采用最新 AI 技术并实现规模化部署。Capital One 的经验证明，通过合理的架构设计和平台化思维，企业可以在"良好管理的速度"下实现 AI 转型。