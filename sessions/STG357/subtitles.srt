1
00:00:00,270 --> 00:00:02,040
- All right, a very warm welcome.

2
00:00:02,040 --> 00:00:04,290
Thank you for being here.

3
00:00:04,290 --> 00:00:06,660
I want to start our discussion today

4
00:00:06,660 --> 00:00:08,280
by sharing some scenarios

5
00:00:08,280 --> 00:00:10,383
that are becoming increasingly common.

6
00:00:11,340 --> 00:00:13,380
Let's say it's 8:00 a.m.

7
00:00:13,380 --> 00:00:16,530
or 8:30 a.m. on a beautiful
Wednesday morning,

8
00:00:16,530 --> 00:00:19,620
and Alice, your lead data
scientist, is looking

9
00:00:19,620 --> 00:00:21,630
for labeled and processed images

10
00:00:21,630 --> 00:00:24,750
amongst 23 million objects in her bucket.

11
00:00:24,750 --> 00:00:28,530
Meanwhile, your security
team is trying to get a list

12
00:00:28,530 --> 00:00:31,410
of all the objects that
have sensitive information

13
00:00:31,410 --> 00:00:34,470
because your audit, because
an audit request came in

14
00:00:34,470 --> 00:00:36,450
and now they're listing all of the objects

15
00:00:36,450 --> 00:00:38,970
and trying to figure out which objects

16
00:00:38,970 --> 00:00:41,940
have sensitive content one by one.

17
00:00:41,940 --> 00:00:43,920
Does that sound familiar?

18
00:00:43,920 --> 00:00:46,380
This is not a data problem.

19
00:00:46,380 --> 00:00:48,663
This is a data discovery problem.

20
00:00:49,620 --> 00:00:53,220
Hi, I'm Roohi Sood, and I'm a
Product Manager at Amazon S3.

21
00:00:53,220 --> 00:00:55,290
I'm joined today by Claire,

22
00:00:55,290 --> 00:00:57,713
our Software Development Manager,

23
00:00:57,713 --> 00:00:59,250
who leads the brilliant engineering team

24
00:00:59,250 --> 00:01:00,540
that has brought the vision

25
00:01:00,540 --> 00:01:03,660
of accelerated object
discovery to reality.

26
00:01:03,660 --> 00:01:07,020
Together, Claire and I have
just one goal for today,

27
00:01:07,020 --> 00:01:08,820
to walk you through a roadmap

28
00:01:08,820 --> 00:01:12,090
of how you can overcome these
data discovery challenges

29
00:01:12,090 --> 00:01:14,250
in your organizations forever.

30
00:01:14,250 --> 00:01:17,550
Your data scientists will get
what they need within minutes.

31
00:01:17,550 --> 00:01:19,920
Your security teams will answer questions

32
00:01:19,920 --> 00:01:21,423
with simple queries.

33
00:01:23,040 --> 00:01:25,500
And here's our journey for today.

34
00:01:25,500 --> 00:01:26,940
We'll start by diving deep

35
00:01:26,940 --> 00:01:28,530
into the data discovery challenges

36
00:01:28,530 --> 00:01:30,450
that are costing organizations millions

37
00:01:30,450 --> 00:01:33,120
in lost opportunities.

38
00:01:33,120 --> 00:01:34,530
Then Claire and I will show you

39
00:01:34,530 --> 00:01:37,830
exactly how S3 Metadata
overcomes these challenges.

40
00:01:37,830 --> 00:01:41,490
We will walk you through
different use cases with demos,

41
00:01:41,490 --> 00:01:43,560
not theory, real examples

42
00:01:43,560 --> 00:01:46,620
that you can replicate
in your own environments.

43
00:01:46,620 --> 00:01:50,160
Everything that we show you
today is available right now.

44
00:01:50,160 --> 00:01:53,730
You can literally enable this
before you leave Las Vegas.

45
00:01:53,730 --> 00:01:56,700
And finally, we'll wrap up
with three key takeaways

46
00:01:56,700 --> 00:01:59,073
that you can take back
to your organizations.

47
00:02:00,360 --> 00:02:01,860
So let's start by understanding

48
00:02:01,860 --> 00:02:04,440
these data discovery challenges.

49
00:02:04,440 --> 00:02:06,240
We call this a challenge,

50
00:02:06,240 --> 00:02:08,730
but truly these are opportunities,

51
00:02:08,730 --> 00:02:10,380
one of the biggest opportunities,

52
00:02:10,380 --> 00:02:12,723
and in a moment you'll understand why.

53
00:02:13,620 --> 00:02:15,900
It's no surprise to anyone in this room

54
00:02:15,900 --> 00:02:17,850
that data is growing faster than ever.

55
00:02:17,850 --> 00:02:21,630
It's growing at a rate that was
unimaginable five years ago.

56
00:02:21,630 --> 00:02:24,240
Every app you use, every car you drive,

57
00:02:24,240 --> 00:02:28,080
sensors in the buildings,
every security camera,

58
00:02:28,080 --> 00:02:31,320
they're all dumping data into storage.

59
00:02:31,320 --> 00:02:33,090
And to put that into perspective,

60
00:02:33,090 --> 00:02:37,200
Amazon S3 now stores over
500 trillion objects.

61
00:02:37,200 --> 00:02:39,543
That's half a quadrillion objects.

62
00:02:40,500 --> 00:02:42,510
And enter the GenAI revolution,

63
00:02:42,510 --> 00:02:45,930
which has fundamentally changed
the value equation for data,

64
00:02:45,930 --> 00:02:48,060
suddenly, all of that unstructured data

65
00:02:48,060 --> 00:02:51,150
that's sitting in your S3
buckets, all of the videos,

66
00:02:51,150 --> 00:02:54,780
the images, the logs, that's
not just storage anymore,

67
00:02:54,780 --> 00:02:57,600
that's potential training
data worth millions.

68
00:02:57,600 --> 00:03:01,290
But here's the challenge,
whether you are training models

69
00:03:01,290 --> 00:03:02,820
or fine tuning them

70
00:03:02,820 --> 00:03:06,690
or building rag use cases using
knowledge spaces, you need

71
00:03:06,690 --> 00:03:09,420
to be able to identify, categorize,

72
00:03:09,420 --> 00:03:11,700
and access your data sets quickly.

73
00:03:11,700 --> 00:03:13,440
If your teams are spending hours

74
00:03:13,440 --> 00:03:16,620
and days scouring the
data for the right videos

75
00:03:16,620 --> 00:03:19,980
or identifying documents
which have the right,

76
00:03:19,980 --> 00:03:23,490
which don't have sensitive
content, you're already behind.

77
00:03:23,490 --> 00:03:27,210
And so this is why we call this
a data discovery opportunity

78
00:03:27,210 --> 00:03:29,790
because whoever solves this
data discovery challenge

79
00:03:29,790 --> 00:03:32,010
for their organizations first is going

80
00:03:32,010 --> 00:03:35,400
to have an insurmountable AI advantage.

81
00:03:35,400 --> 00:03:38,910
And this brings us to our
fundamental question for today.

82
00:03:38,910 --> 00:03:43,593
How do I find or access
actionable data sets at scale?

83
00:03:44,670 --> 00:03:48,330
The answer, as many of you
already know, is metadata.

84
00:03:48,330 --> 00:03:50,430
It is the DNA of your data.

85
00:03:50,430 --> 00:03:54,270
It tells you what you
have, where it is stored,

86
00:03:54,270 --> 00:03:56,700
sometimes what it potentially contains,

87
00:03:56,700 --> 00:04:00,150
but this is where most
organizations get stuck.

88
00:04:00,150 --> 00:04:03,270
Traditional metadata solutions
typically live outside

89
00:04:03,270 --> 00:04:04,710
of your storage solutions.

90
00:04:04,710 --> 00:04:08,070
So they're already complex

91
00:04:08,070 --> 00:04:10,230
and then they have sync issues.

92
00:04:10,230 --> 00:04:13,500
Second, they're incredibly
difficult to build, operate,

93
00:04:13,500 --> 00:04:15,060
and maintain at scale.

94
00:04:15,060 --> 00:04:19,920
We have seen organizations
spending weeks, sometimes months

95
00:04:19,920 --> 00:04:23,310
to just get the fundamental
metadata solutions right.

96
00:04:23,310 --> 00:04:25,830
And finally, but the most important one,

97
00:04:25,830 --> 00:04:29,130
your metadata is only
useful if it is current.

98
00:04:29,130 --> 00:04:32,340
In fact, still metadata
is worse than no metadata

99
00:04:32,340 --> 00:04:33,173
because it's going

100
00:04:33,173 --> 00:04:36,030
to potentially lead you
down the wrong path.

101
00:04:36,030 --> 00:04:39,060
And this is why we asked
ourselves this question,

102
00:04:39,060 --> 00:04:43,350
what if metadata worked
just like S3, simple,

103
00:04:43,350 --> 00:04:45,003
reliable, and scalable?

104
00:04:46,290 --> 00:04:49,530
And this is why we created S3 Metadata.

105
00:04:49,530 --> 00:04:52,320
But before I tell you
what S3 Metadata does,

106
00:04:52,320 --> 00:04:55,110
I'm actually going to tell
you what it doesn't do.

107
00:04:55,110 --> 00:04:58,590
It doesn't require you to
change how you work with S3

108
00:04:58,590 --> 00:05:01,890
and it doesn't require you
to do any complex setups.

109
00:05:01,890 --> 00:05:06,090
S3 Metadata is metadata
done right, automatic,

110
00:05:06,090 --> 00:05:08,463
comprehensive, and always current.

111
00:05:10,170 --> 00:05:12,390
Simply put, S3 Metadata provides

112
00:05:12,390 --> 00:05:16,080
automatic metadata extraction
from your objects in S3

113
00:05:16,080 --> 00:05:18,840
that you can query with
simple SQL statements.

114
00:05:18,840 --> 00:05:21,120
That means every time you add an object

115
00:05:21,120 --> 00:05:22,650
or you delete an object,

116
00:05:22,650 --> 00:05:24,933
we automatically update your metadata.

117
00:05:26,130 --> 00:05:29,970
Let's dig into why this is
exciting for customers today.

118
00:05:29,970 --> 00:05:34,290
First, it captures both
system and custom metadata.

119
00:05:34,290 --> 00:05:36,540
System metadata is object size,

120
00:05:36,540 --> 00:05:38,670
checksum types, encryption types.

121
00:05:38,670 --> 00:05:40,535
We're gonna dive into both

122
00:05:40,535 --> 00:05:43,560
of these a little bit more
in our presentation today.

123
00:05:43,560 --> 00:05:46,260
Second, it's built on
the Apache Iceberg format

124
00:05:46,260 --> 00:05:48,870
and stored in S3 table
buckets, which means

125
00:05:48,870 --> 00:05:51,930
that it's using proven
open source standards,

126
00:05:51,930 --> 00:05:55,110
which you can use with any
compatible query engine

127
00:05:55,110 --> 00:05:57,360
either now or in the future.

128
00:05:57,360 --> 00:05:59,730
And third, it's completely automatic.

129
00:05:59,730 --> 00:06:02,250
The moment you upload, update

130
00:06:02,250 --> 00:06:05,820
or delete an object, your
metadata tables update as well.

131
00:06:05,820 --> 00:06:08,073
There is no manual syncing required.

132
00:06:14,010 --> 00:06:15,960
So how does it really work?

133
00:06:15,960 --> 00:06:18,000
It's actually very simple.

134
00:06:18,000 --> 00:06:21,390
All you have to do is on
your general purpose bucket,

135
00:06:21,390 --> 00:06:24,210
you will add a simple
configuration that says,

136
00:06:24,210 --> 00:06:28,620
that tells us you want metadata
for this particular bucket.

137
00:06:28,620 --> 00:06:30,780
Once we see this configuration,

138
00:06:30,780 --> 00:06:35,780
we will set up a managed
AWS S3 table bucket,

139
00:06:36,180 --> 00:06:37,710
and within this table bucket,

140
00:06:37,710 --> 00:06:40,350
we start populating your metadata tables.

141
00:06:40,350 --> 00:06:44,640
These are called journal tables
and live inventory tables.

142
00:06:44,640 --> 00:06:45,780
Now, you might be wondering,

143
00:06:45,780 --> 00:06:47,490
what are these tables exactly?

144
00:06:47,490 --> 00:06:49,140
What do they contain?

145
00:06:49,140 --> 00:06:50,940
I'm going to invite Claire on stage

146
00:06:50,940 --> 00:06:52,770
to help us understand the difference

147
00:06:52,770 --> 00:06:56,523
between these metadata tables
and their different use cases.

148
00:06:59,610 --> 00:07:00,443
- Thank you.

149
00:07:05,550 --> 00:07:07,380
All right, thanks, Roohi.

150
00:07:07,380 --> 00:07:11,880
So as Roohi mentioned,
when you enable S3 Metadata

151
00:07:11,880 --> 00:07:14,760
for your bucket, we create two tables.

152
00:07:14,760 --> 00:07:17,040
The first is the journal table.

153
00:07:17,040 --> 00:07:18,450
You can think of the journal table

154
00:07:18,450 --> 00:07:20,850
as an audit log for your bucket.

155
00:07:20,850 --> 00:07:22,830
Every put, every delete,

156
00:07:22,830 --> 00:07:26,133
every modification is
captured as its own row.

157
00:07:26,970 --> 00:07:29,640
This isn't just metadata,
it's a time machine

158
00:07:29,640 --> 00:07:31,680
for your source bucket.

159
00:07:31,680 --> 00:07:34,500
You can see exactly what
happened, when it happened

160
00:07:34,500 --> 00:07:36,480
and who made it happen.

161
00:07:36,480 --> 00:07:39,330
Because the journal table
refreshes within minutes,

162
00:07:39,330 --> 00:07:42,090
you're always working
with current information.

163
00:07:42,090 --> 00:07:45,630
And this year, to make
cost management simple,

164
00:07:45,630 --> 00:07:48,783
we introduced automatic
expiration of old records.

165
00:07:49,830 --> 00:07:53,193
The second table is the
live inventory table.

166
00:07:54,330 --> 00:07:57,390
The live inventory table
gives you a detailed view

167
00:07:57,390 --> 00:07:59,310
of what's in your bucket.

168
00:07:59,310 --> 00:08:01,260
You can think of it as a snapshot

169
00:08:01,260 --> 00:08:03,870
of your entire data estate.

170
00:08:03,870 --> 00:08:06,930
There's one row per object version.

171
00:08:06,930 --> 00:08:08,620
It refreshes every hour

172
00:08:09,810 --> 00:08:12,993
and it answers questions
about the state of your data.

173
00:08:13,830 --> 00:08:17,010
This is perfect for
analytics and reporting.

174
00:08:17,010 --> 00:08:19,950
You no longer have to issue
expensive list requests

175
00:08:19,950 --> 00:08:21,630
and paginate through them,

176
00:08:21,630 --> 00:08:23,010
and you no longer have to build

177
00:08:23,010 --> 00:08:25,230
those complicated custom solutions

178
00:08:25,230 --> 00:08:27,990
that Roohi was mentioning earlier.

179
00:08:27,990 --> 00:08:30,333
It's all here and ready for you to query.

180
00:08:32,190 --> 00:08:34,440
Let's take a look at this in action.

181
00:08:34,440 --> 00:08:38,100
So you have a set here of users

182
00:08:38,100 --> 00:08:40,740
who are all operating on a shared data set

183
00:08:40,740 --> 00:08:44,370
and they are adding, updating

184
00:08:44,370 --> 00:08:46,080
and deleting objects.

185
00:08:46,080 --> 00:08:48,720
Now, what we can see is
because S3 Metadata is enabled,

186
00:08:48,720 --> 00:08:50,670
as they're making these actions,

187
00:08:50,670 --> 00:08:54,540
new rows appear in the journal
table capturing those actions

188
00:08:54,540 --> 00:08:56,073
and the associated metadata.

189
00:09:01,935 --> 00:09:02,768
Now, for these changes

190
00:09:02,768 --> 00:09:05,580
to be reflected in your inventory table,

191
00:09:05,580 --> 00:09:08,580
we run a job roughly
once an hour that's going

192
00:09:08,580 --> 00:09:11,580
to read the new rows recorded
in your journal table

193
00:09:11,580 --> 00:09:13,980
and apply them to your inventory table,

194
00:09:13,980 --> 00:09:16,623
generating a new snapshot for your bucket.

195
00:09:17,490 --> 00:09:21,150
So here we see the metadata
flowing from the journal table

196
00:09:21,150 --> 00:09:22,800
into your inventory table,

197
00:09:22,800 --> 00:09:25,143
creating that snapshot for you to query.

198
00:09:28,500 --> 00:09:30,450
Let's take a look inside.

199
00:09:30,450 --> 00:09:32,850
The schema is incredibly rich.

200
00:09:32,850 --> 00:09:34,950
In total, there are 21 types

201
00:09:34,950 --> 00:09:38,130
of metadata recorded in the tables.

202
00:09:38,130 --> 00:09:41,520
The journal table
contains request metadata.

203
00:09:41,520 --> 00:09:44,370
That's things such as record type,

204
00:09:44,370 --> 00:09:49,370
request record timestamp,
requester, source IP, and system

205
00:09:49,560 --> 00:09:52,680
and custom metadata are
captured in both tables.

206
00:09:52,680 --> 00:09:56,460
System metadata would be
things like bucket, key,

207
00:09:56,460 --> 00:09:59,280
object size, object storage class,

208
00:09:59,280 --> 00:10:02,340
but also information
about encryption status,

209
00:10:02,340 --> 00:10:04,920
whether the object was uploaded
using multi-part upload

210
00:10:04,920 --> 00:10:07,623
and even the encryption
algorithm that was used.

211
00:10:08,580 --> 00:10:11,940
Custom metadata is metadata
defined by you in the form

212
00:10:11,940 --> 00:10:14,133
of user metadata or object tags.

213
00:10:16,110 --> 00:10:18,540
Okay, so where do these tables live,

214
00:10:18,540 --> 00:10:20,400
and how do you query them?

215
00:10:20,400 --> 00:10:21,630
There are three things you need

216
00:10:21,630 --> 00:10:24,090
to know about your metadata tables.

217
00:10:24,090 --> 00:10:27,570
The first is that they
are Apache Iceberg format.

218
00:10:27,570 --> 00:10:29,910
The second is that they are S3 Tables,

219
00:10:29,910 --> 00:10:32,040
which live in S3 table buckets.

220
00:10:32,040 --> 00:10:35,850
And the third is that
they're managed by AWS.

221
00:10:35,850 --> 00:10:38,493
Let's break down what each of these mean.

222
00:10:41,010 --> 00:10:44,040
So Apache Iceberg is an open table format

223
00:10:44,040 --> 00:10:47,823
that supports SQL-like
queries over parquet data.

224
00:10:49,077 --> 00:10:50,790
It's very popular.

225
00:10:50,790 --> 00:10:53,160
In fact, it's one of the
fastest growing types

226
00:10:53,160 --> 00:10:54,573
of data in S3.

227
00:10:55,770 --> 00:10:59,550
One reason why customers
like it is it allows them,

228
00:10:59,550 --> 00:11:03,540
because it's just, Iceberg
itself is just data at rest,

229
00:11:03,540 --> 00:11:07,140
it allows customers to choose
which query engine they want

230
00:11:07,140 --> 00:11:10,050
to use when interacting with their data.

231
00:11:10,050 --> 00:11:11,730
And in fact, you may have different users

232
00:11:11,730 --> 00:11:14,310
using different query engines
to access the same tables

233
00:11:14,310 --> 00:11:18,270
at the same time depending
on their individual needs.

234
00:11:18,270 --> 00:11:20,070
Apache Iceberg is also designed

235
00:11:20,070 --> 00:11:22,260
to support analytics at scales.

236
00:11:22,260 --> 00:11:25,710
It supports queries on
tables with petabytes of data

237
00:11:25,710 --> 00:11:27,570
and billions of files.

238
00:11:27,570 --> 00:11:32,310
It also supports functionality
such as time travel

239
00:11:32,310 --> 00:11:34,383
and schema evolution capabilities.

240
00:11:37,350 --> 00:11:41,013
S3 Tables is our managed
solution for Apache Iceberg.

241
00:11:41,850 --> 00:11:44,070
There's few reasons why S3 Tables

242
00:11:44,070 --> 00:11:46,170
is the right place for your metadata.

243
00:11:46,170 --> 00:11:48,420
The first is performance.

244
00:11:48,420 --> 00:11:52,433
S3 Tables offer 10 times
the transactions per second

245
00:11:52,433 --> 00:11:56,190
compared to Apache Iceberg tables stored

246
00:11:56,190 --> 00:11:58,320
in general purpose buckets.

247
00:11:58,320 --> 00:12:01,500
The other reason is S3
Tables' deep integration

248
00:12:01,500 --> 00:12:05,100
with AWS analytics solution services.

249
00:12:05,100 --> 00:12:06,300
In particular, I want

250
00:12:06,300 --> 00:12:08,100
to highlight the seamless integration

251
00:12:08,100 --> 00:12:09,990
with AWS Lake Formation,

252
00:12:09,990 --> 00:12:12,510
giving you fine grain access control

253
00:12:12,510 --> 00:12:14,553
down to the row and column level.

254
00:12:15,690 --> 00:12:18,180
Now, this allows you to share metadata

255
00:12:18,180 --> 00:12:21,120
with several different
teams while making sure

256
00:12:21,120 --> 00:12:25,230
that sensitive information
remains protected.

257
00:12:25,230 --> 00:12:26,760
Okay, and then the third thing I mentioned

258
00:12:26,760 --> 00:12:29,610
was that the tables were managed by AWS.

259
00:12:29,610 --> 00:12:31,920
What this means is that only S3 can write

260
00:12:31,920 --> 00:12:33,840
to your metadata tables,

261
00:12:33,840 --> 00:12:36,000
but you still contain full access

262
00:12:36,000 --> 00:12:37,920
over who can read what data.

263
00:12:37,920 --> 00:12:41,610
Now, the reason we limit
the write access to S3

264
00:12:41,610 --> 00:12:42,990
is so that you know

265
00:12:42,990 --> 00:12:44,460
that what is in your tables

266
00:12:44,460 --> 00:12:47,210
is an accurate representation
of what's in your bucket.

267
00:12:48,090 --> 00:12:51,000
The second thing that managed by AWS means

268
00:12:51,000 --> 00:12:52,320
is that we configure

269
00:12:52,320 --> 00:12:54,900
and control the maintenance
operations on your tables.

270
00:12:54,900 --> 00:12:57,630
So for example, we set the
target compaction size.

271
00:12:57,630 --> 00:13:01,200
We also set configure how
frequent compaction should run.

272
00:13:01,200 --> 00:13:04,440
We set up and configure
the snapshot management

273
00:13:04,440 --> 00:13:07,860
and the configurations for
unreferenced file cleanup.

274
00:13:07,860 --> 00:13:11,130
And we do that because
that way you don't have to,

275
00:13:11,130 --> 00:13:14,490
but we also do that so we can
optimize these configurations

276
00:13:14,490 --> 00:13:18,540
based off the size and structure
of your metadata tables.

277
00:13:18,540 --> 00:13:22,980
Now, just yesterday, we
announced three new types

278
00:13:22,980 --> 00:13:24,273
of managed tables.

279
00:13:25,200 --> 00:13:30,060
You can now enable integrations
to create managed S3 Tables

280
00:13:30,060 --> 00:13:32,193
for AWS CloudWatch logs,

281
00:13:37,020 --> 00:13:39,970
SageMaker Unified Studio Asset Metadata

282
00:13:40,950 --> 00:13:42,633
and S3 Lens data.

283
00:13:43,590 --> 00:13:45,450
This is incredibly powerful.

284
00:13:45,450 --> 00:13:48,750
We now have metadata from
several different AWS services

285
00:13:48,750 --> 00:13:51,630
all in one place, and
since they all stored

286
00:13:51,630 --> 00:13:54,453
in the same format, you
can query them together.

287
00:13:57,390 --> 00:13:59,760
So how do you query your managed tables

288
00:13:59,760 --> 00:14:02,130
as well as your S3 Metadata tables?

289
00:14:02,130 --> 00:14:03,750
There are several different options.

290
00:14:03,750 --> 00:14:06,595
The first is using AWS Analytics Services.

291
00:14:06,595 --> 00:14:10,020
S3 integrates deeply with these services.

292
00:14:10,020 --> 00:14:14,700
So you can use Athena Redshift,
SageMaker Unified Studio

293
00:14:14,700 --> 00:14:17,130
pretty easily right out of the box.

294
00:14:17,130 --> 00:14:19,620
Second is since S3 Tables supports

295
00:14:19,620 --> 00:14:21,900
Iceberg Rest catalog endpoint,

296
00:14:21,900 --> 00:14:24,690
you can take advantage
of the open ecosystem

297
00:14:24,690 --> 00:14:28,200
of engines that support
Iceberg, whether that's DuckDB,

298
00:14:28,200 --> 00:14:31,293
Apache Spark, Flink,
or Trino to name a few.

299
00:14:32,400 --> 00:14:36,990
And finally, this year we
launched MCP for S3 Tables,

300
00:14:36,990 --> 00:14:38,880
making it possible for you to chat

301
00:14:38,880 --> 00:14:40,863
with your data using natural language.

302
00:14:43,020 --> 00:14:45,480
What else can you do with your metadata?

303
00:14:45,480 --> 00:14:47,640
On the screen, we have a few examples

304
00:14:47,640 --> 00:14:50,130
of how you can leverage Amazon QuickSight

305
00:14:50,130 --> 00:14:52,800
to visualize your metadata.

306
00:14:52,800 --> 00:14:55,050
This is an easy and effective way to track

307
00:14:55,050 --> 00:14:57,750
and summarize different usage
patterns in your bucket.

308
00:15:00,060 --> 00:15:01,860
Okay, enough theory.

309
00:15:01,860 --> 00:15:03,633
Let's talk about some use cases.

310
00:15:05,970 --> 00:15:09,270
To start, let's take a second to step back

311
00:15:09,270 --> 00:15:10,320
to that slide showing

312
00:15:11,216 --> 00:15:13,110
what's in your metadata tables.

313
00:15:13,110 --> 00:15:15,840
If you recall, there are
three types of metadata,

314
00:15:15,840 --> 00:15:19,353
request metadata, custom
metadata, and system metadata.

315
00:15:20,280 --> 00:15:22,710
We're gonna take a look
at each of these to see

316
00:15:22,710 --> 00:15:25,380
how you can use them
to understand your data

317
00:15:25,380 --> 00:15:26,913
and take action on your data.

318
00:15:28,320 --> 00:15:31,800
So request metadata lives
in the journal table

319
00:15:31,800 --> 00:15:33,030
and it's what you use

320
00:15:33,030 --> 00:15:35,583
to inspect what's changing on your bucket.

321
00:15:37,770 --> 00:15:39,570
So here we have another set of users

322
00:15:39,570 --> 00:15:42,630
who are again operating
on a shared dataset.

323
00:15:42,630 --> 00:15:46,920
Now, you as the owner of the
dataset, want to understand

324
00:15:46,920 --> 00:15:49,770
what's changed within the past day.

325
00:15:49,770 --> 00:15:52,080
You can do that by writing
a SQL query that's going

326
00:15:52,080 --> 00:15:56,190
to group the results based off
of source IP and requester.

327
00:15:56,190 --> 00:15:58,110
Source IP is going to
tell you the source IP

328
00:15:58,110 --> 00:16:00,600
of the request, and
requester is gonna tell you

329
00:16:00,600 --> 00:16:02,370
the AWS account.

330
00:16:02,370 --> 00:16:04,680
For request coming from S3 Lifecycle

331
00:16:04,680 --> 00:16:06,420
or other AWS services,

332
00:16:06,420 --> 00:16:09,600
you're gonna see the AWS's
Services Service Principle

333
00:16:09,600 --> 00:16:10,983
instead of the account ID.

334
00:16:13,680 --> 00:16:17,430
Now, say you want to zero in
on a particular operation,

335
00:16:17,430 --> 00:16:20,380
maybe you actually want to
understand who is deleting data.

336
00:16:21,330 --> 00:16:23,040
You can do that in a couple different ways

337
00:16:23,040 --> 00:16:25,560
depending on whether or not
you have versioning enabled

338
00:16:25,560 --> 00:16:26,910
on your bucket.

339
00:16:26,910 --> 00:16:29,040
So for an un-version bucket, you're going

340
00:16:29,040 --> 00:16:32,070
to add a filter on record type,

341
00:16:32,070 --> 00:16:33,810
and for a version bucket, you're going

342
00:16:33,810 --> 00:16:37,620
to add an additional
filter on delete markers.

343
00:16:37,620 --> 00:16:39,930
Delete markers allow you to distinguish

344
00:16:39,930 --> 00:16:41,520
between permanent deletes

345
00:16:41,520 --> 00:16:43,440
and the delete markers that are added

346
00:16:43,440 --> 00:16:46,473
to version bucket when a
delete request is made.

347
00:16:47,670 --> 00:16:50,937
So now you've understand
who has been deleting data,

348
00:16:50,937 --> 00:16:53,370
you probably wanna know what
data was actually deleted,

349
00:16:53,370 --> 00:16:55,890
and you just do that by
updating the select statement

350
00:16:55,890 --> 00:16:57,633
to have bucket key and version ID.

351
00:16:58,964 --> 00:17:03,540
Now, while you can recover
from permanent deletes

352
00:17:03,540 --> 00:17:07,140
on un-version buckets, if
you have versioning enabled,

353
00:17:07,140 --> 00:17:09,510
you can actually use
the output of this query

354
00:17:09,510 --> 00:17:12,060
to roll back the delete requests made

355
00:17:12,060 --> 00:17:14,624
by removing the delete markers.

356
00:17:14,624 --> 00:17:19,080
In fact, last week we published
a tool that allows you

357
00:17:19,080 --> 00:17:22,380
to do this at scale using S3 Metadata.

358
00:17:22,380 --> 00:17:24,930
This tool doesn't just roll back deletes,

359
00:17:24,930 --> 00:17:27,120
it can actually revert
all the changes made

360
00:17:27,120 --> 00:17:29,823
as long as you have versioning
enabled on your bucket.

361
00:17:30,660 --> 00:17:34,920
It does this by querying
S3 Metadata to understand

362
00:17:34,920 --> 00:17:37,800
what versions existed at a particular time

363
00:17:37,800 --> 00:17:42,000
and then it uses S3 batch
operations to revert those,

364
00:17:42,000 --> 00:17:44,910
to the revert your bucket
back to the state that it was

365
00:17:44,910 --> 00:17:49,080
by removing delete markers
and copying objects in place.

366
00:17:49,080 --> 00:17:51,540
I highly recommend you all
take a look at the tool

367
00:17:51,540 --> 00:17:53,283
and maybe even try it out.

368
00:17:55,740 --> 00:17:57,810
Okay, so that was request metadata.

369
00:17:57,810 --> 00:18:00,247
Now I'm gonna move on to system metadata.

370
00:18:00,247 --> 00:18:02,130
So system metadata lives
in both your journal

371
00:18:02,130 --> 00:18:03,780
and your inventory table,

372
00:18:03,780 --> 00:18:06,090
and it exposes the information you need

373
00:18:06,090 --> 00:18:07,833
to understand your data landscape.

374
00:18:09,690 --> 00:18:13,410
For example, up on the
screen, we have a part

375
00:18:13,410 --> 00:18:16,350
of an inventory table for a legacy bucket.

376
00:18:16,350 --> 00:18:19,170
What we know about this bucket
is that it contains millions

377
00:18:19,170 --> 00:18:21,000
of objects that have been uploaded

378
00:18:21,000 --> 00:18:23,940
by different departments
over several years.

379
00:18:23,940 --> 00:18:27,060
Now, last year, your compliance team

380
00:18:27,060 --> 00:18:29,730
updated the policy requiring all objects

381
00:18:29,730 --> 00:18:31,860
be encrypted using SSE-KMS.

382
00:18:31,860 --> 00:18:34,170
Does this sound familiar to anyone?

383
00:18:34,170 --> 00:18:35,430
Well, now a year later you've gone

384
00:18:35,430 --> 00:18:37,080
and you've updated all of your application

385
00:18:37,080 --> 00:18:39,030
to use SSE-KMS,

386
00:18:39,030 --> 00:18:41,010
but you're not sure about
those objects that were created

387
00:18:41,010 --> 00:18:43,350
before the policy was in place.

388
00:18:43,350 --> 00:18:45,810
How are you going to find
the remaining objects

389
00:18:45,810 --> 00:18:47,940
that are still using plain text?

390
00:18:47,940 --> 00:18:50,100
As you can see from the
screen, it can be like looking

391
00:18:50,100 --> 00:18:52,200
for a needle in a haystack.

392
00:18:52,200 --> 00:18:54,090
Enter S3 Metadata.

393
00:18:54,090 --> 00:18:56,490
With S3 Metadata, this
challenge is reduced

394
00:18:56,490 --> 00:18:59,043
to a single pretty simple SQL query.

395
00:19:00,930 --> 00:19:03,720
And what's even more exciting
is once again, you don't have

396
00:19:03,720 --> 00:19:06,540
to stop here, you can take
action to encrypt these objects

397
00:19:06,540 --> 00:19:10,200
by passing your output into a
batch operation, running copy

398
00:19:10,200 --> 00:19:12,000
and place on these objects

399
00:19:12,000 --> 00:19:14,880
and specifying your desired
encryption type as part

400
00:19:14,880 --> 00:19:16,083
of the copy request.

401
00:19:18,690 --> 00:19:20,460
There's one more use case
that I wanna talk about

402
00:19:20,460 --> 00:19:24,150
for system metadata that
actually combine system metadata

403
00:19:24,150 --> 00:19:25,950
and request metadata.

404
00:19:25,950 --> 00:19:29,250
So in this example, you're
looking at who is uploading

405
00:19:29,250 --> 00:19:32,730
how much data to which storage classes,

406
00:19:32,730 --> 00:19:35,520
and you can do that with
the query on the screen.

407
00:19:35,520 --> 00:19:37,560
Now, what's compelling about this use case

408
00:19:37,560 --> 00:19:40,050
is that traditionally your access logs

409
00:19:40,050 --> 00:19:41,850
and your storage data are probably stored

410
00:19:41,850 --> 00:19:45,180
in two very different places
with different access controls

411
00:19:45,180 --> 00:19:46,920
and probably in a different format.

412
00:19:46,920 --> 00:19:49,560
And it would take actually
a significant amount

413
00:19:49,560 --> 00:19:52,320
of developer effort to go
and get access to these

414
00:19:52,320 --> 00:19:54,030
and combine the data into a single place

415
00:19:54,030 --> 00:19:57,750
that you can now look at to
analyze this data together.

416
00:19:57,750 --> 00:19:59,970
For S3 Metadata, it's all there.

417
00:19:59,970 --> 00:20:01,473
One place ready to query.

418
00:20:03,120 --> 00:20:05,280
Okay, so finally, the
third type of metadata,

419
00:20:05,280 --> 00:20:07,620
custom metadata, which I think is one

420
00:20:07,620 --> 00:20:09,630
of the most exciting parts.

421
00:20:09,630 --> 00:20:13,230
It's what allows you to
enhance your metadata

422
00:20:13,230 --> 00:20:15,510
with your own business context.

423
00:20:15,510 --> 00:20:17,160
There are a few ways to do this.

424
00:20:17,160 --> 00:20:19,173
So the first is to use user metadata.

425
00:20:20,220 --> 00:20:22,770
User metadata is visible
in both the journal

426
00:20:22,770 --> 00:20:24,033
and the inventory table.

427
00:20:24,870 --> 00:20:26,910
To attach user metadata to an object,

428
00:20:26,910 --> 00:20:30,660
you provide it as key value
pairs as part of the put request

429
00:20:30,660 --> 00:20:32,250
to create an object.

430
00:20:32,250 --> 00:20:34,650
And because user metadata
is specified as part

431
00:20:34,650 --> 00:20:37,170
of the put request, it is immutable.

432
00:20:37,170 --> 00:20:39,540
This makes it ideal
for storing information

433
00:20:39,540 --> 00:20:41,370
that shouldn't change
throughout the course

434
00:20:41,370 --> 00:20:43,410
of the object's lifecycle.

435
00:20:43,410 --> 00:20:45,480
For example, something like the providence

436
00:20:45,480 --> 00:20:48,390
or source of an object should not change

437
00:20:48,390 --> 00:20:51,150
and therefore we see
customers storing this type

438
00:20:51,150 --> 00:20:52,923
of information as user metadata.

439
00:20:54,330 --> 00:20:56,913
The second option is to use object tags.

440
00:20:58,290 --> 00:21:00,630
Object tags, you can attach object tags

441
00:21:00,630 --> 00:21:04,470
to your object using object tagging APIs,

442
00:21:04,470 --> 00:21:08,400
and once they're attached to the object,

443
00:21:08,400 --> 00:21:10,620
they are automatically become visible

444
00:21:10,620 --> 00:21:13,230
in your journal and inventory table.

445
00:21:13,230 --> 00:21:15,240
Now, one other thing that is important

446
00:21:15,240 --> 00:21:16,770
to understand about user metadata

447
00:21:16,770 --> 00:21:19,710
and object tags is once
they're attached to the object,

448
00:21:19,710 --> 00:21:21,000
they live with the object.

449
00:21:21,000 --> 00:21:24,630
So if the object is copied
or replicated, the metadata,

450
00:21:24,630 --> 00:21:27,360
this custom metadata is gonna go with it.

451
00:21:27,360 --> 00:21:30,030
And similarly, if your object
is deleted, we will clean up

452
00:21:30,030 --> 00:21:31,173
that metadata for you.

453
00:21:32,070 --> 00:21:34,260
Now, finally, there's a third option,

454
00:21:34,260 --> 00:21:36,930
and that is to store your custom metadata

455
00:21:36,930 --> 00:21:39,450
in your own self-managed S3 table

456
00:21:39,450 --> 00:21:41,670
within an S3 table bucket.

457
00:21:41,670 --> 00:21:44,460
We often see customers
taking this approach

458
00:21:44,460 --> 00:21:47,820
when their metadata exceeds
the size limits for tags

459
00:21:47,820 --> 00:21:49,020
and user-defined metadata.

460
00:21:49,020 --> 00:21:50,760
This might be to store thumbnails

461
00:21:50,760 --> 00:21:54,960
or summarization of text
documents or video files.

462
00:21:54,960 --> 00:21:58,740
Self-managed S3 Tables are an
incredibly flexible option,

463
00:21:58,740 --> 00:22:00,360
and since they are compatible

464
00:22:00,360 --> 00:22:02,490
with the same query engine tools,

465
00:22:02,490 --> 00:22:05,700
you can easily combine them
with your managed metadata.

466
00:22:05,700 --> 00:22:09,360
Now, a trick off with
using self-managed tables

467
00:22:09,360 --> 00:22:11,520
is that updates from your buckets

468
00:22:11,520 --> 00:22:14,940
aren't propagated into
your self-managed tables.

469
00:22:14,940 --> 00:22:17,430
So you may have to join
your self-managed table

470
00:22:17,430 --> 00:22:20,400
with your managed S3 Metadata tables

471
00:22:20,400 --> 00:22:24,693
in order to understand if your
metadata is still current.

472
00:22:26,640 --> 00:22:30,450
Let's take a look at a couple
use cases for custom metadata.

473
00:22:30,450 --> 00:22:32,550
Something we hear from
customers all the time

474
00:22:32,550 --> 00:22:35,490
is that with the explosion
of synthetic data,

475
00:22:35,490 --> 00:22:37,770
they need a way to separate AI generated

476
00:22:37,770 --> 00:22:40,233
versus non-AI generated metadata.

477
00:22:41,460 --> 00:22:44,546
They also wanna track the
lineage of how a piece

478
00:22:44,546 --> 00:22:47,550
of AI data was created.

479
00:22:47,550 --> 00:22:50,640
So as part of the NOVA
launch at Reinvent last year,

480
00:22:50,640 --> 00:22:53,430
Bedrock began annotating videos

481
00:22:53,430 --> 00:22:57,510
and images that are uploaded
to S3 using user metadata.

482
00:22:57,510 --> 00:23:00,960
The annotations indicate that
the object came from Bedrock

483
00:23:00,960 --> 00:23:03,690
and the model that was
used to create them.

484
00:23:03,690 --> 00:23:06,360
Now, if you have S3 Metadata
enabled, the problem

485
00:23:06,360 --> 00:23:10,140
of separating AI versus
non-AI generated information

486
00:23:10,140 --> 00:23:12,093
is reduced to a single SQL query.

487
00:23:14,310 --> 00:23:17,611
Now, in the last example,
we talked about making sense

488
00:23:17,611 --> 00:23:20,160
of data generated by AI,

489
00:23:20,160 --> 00:23:22,560
but another source of growing data in S3

490
00:23:22,560 --> 00:23:25,680
is from the exponential
increase of sensors

491
00:23:25,680 --> 00:23:28,860
and monitors that we
see in our daily lives.

492
00:23:28,860 --> 00:23:32,700
Some examples would be security
camera, vehicle sensors,

493
00:23:32,700 --> 00:23:34,530
sensors on planes, ships,

494
00:23:34,530 --> 00:23:37,740
and also information
from scientific studies

495
00:23:37,740 --> 00:23:41,730
such as topography, data
geospatial or lunar imagery

496
00:23:41,730 --> 00:23:44,850
or even DNA sequences
from genomic studies.

497
00:23:44,850 --> 00:23:47,400
Now, common in all of these use cases

498
00:23:47,400 --> 00:23:51,750
is that the data itself lacks
the contextual information

499
00:23:51,750 --> 00:23:53,493
that's required to analyze it.

500
00:23:54,341 --> 00:23:55,500
And this is particularly true

501
00:23:55,500 --> 00:23:57,400
when it's first uploaded to the cloud.

502
00:23:58,560 --> 00:24:01,020
For example, if you think of a sensor data

503
00:24:01,020 --> 00:24:03,060
that you're uploading, you need to record

504
00:24:03,060 --> 00:24:06,030
additional information such as the time

505
00:24:06,030 --> 00:24:10,530
the recording might have been
made where the sensor existed

506
00:24:10,530 --> 00:24:13,890
or other sensor configurations.

507
00:24:13,890 --> 00:24:15,480
Now, fortunately, you can add all

508
00:24:15,480 --> 00:24:18,270
of this contextual metadata to your object

509
00:24:18,270 --> 00:24:22,140
and to your data through
the use of object tags.

510
00:24:22,140 --> 00:24:25,980
And then once the object
tags attached to your data,

511
00:24:25,980 --> 00:24:28,380
it flows through into your metadata tables

512
00:24:28,380 --> 00:24:30,180
so you can find your data by querying

513
00:24:30,180 --> 00:24:33,300
on that same contextual
information you attach

514
00:24:33,300 --> 00:24:34,353
to your object.

515
00:24:37,170 --> 00:24:38,003
All right.

516
00:24:38,003 --> 00:24:40,473
I think it's time to see
some of this in action.

517
00:24:49,230 --> 00:24:50,580
- All right.

518
00:24:50,580 --> 00:24:52,320
We're very excited about the demo today.

519
00:24:52,320 --> 00:24:54,930
Like I said, everything that we're going

520
00:24:54,930 --> 00:24:58,320
to show you today is available
and you can literally on

521
00:24:58,320 --> 00:25:00,231
before you leave today.

522
00:25:00,231 --> 00:25:02,070
There are three parts to the demo today.

523
00:25:02,070 --> 00:25:03,750
We're gonna start first by learning

524
00:25:03,750 --> 00:25:06,960
how do you set up metadata
configuration on a bucket?

525
00:25:06,960 --> 00:25:08,160
Then we will talk

526
00:25:08,160 --> 00:25:11,460
about how you can query
those metadata tables

527
00:25:11,460 --> 00:25:13,050
with different analytics engines.

528
00:25:13,050 --> 00:25:14,400
And finally we'll see

529
00:25:14,400 --> 00:25:16,680
how you can take storage
management actions

530
00:25:16,680 --> 00:25:19,770
based on the outputs of
your metadata queries.

531
00:25:19,770 --> 00:25:21,450
So let's get rolling.

532
00:25:21,450 --> 00:25:23,430
Let's work on the first part,

533
00:25:23,430 --> 00:25:27,843
which is setting metadata on
your general purpose buckets.

534
00:25:30,300 --> 00:25:31,133
Okay.

535
00:25:31,133 --> 00:25:32,580
So this is the easiest part.

536
00:25:32,580 --> 00:25:35,940
You can duly set this up in
under a minute on your buckets.

537
00:25:35,940 --> 00:25:38,490
All you have to do is figure
out the general purpose bucket

538
00:25:38,490 --> 00:25:40,170
where you want the metadata.

539
00:25:40,170 --> 00:25:41,430
In this case, we're gonna be working

540
00:25:41,430 --> 00:25:43,020
on the star watcher bucket.

541
00:25:43,020 --> 00:25:45,000
The two tables that we
talked about, journal

542
00:25:45,000 --> 00:25:47,460
and live inventory
tables, you enable them.

543
00:25:47,460 --> 00:25:49,770
You can provide the encryption types,

544
00:25:49,770 --> 00:25:52,530
and the record expiration allows you

545
00:25:52,530 --> 00:25:55,380
to expire the records
after a certain duration.

546
00:25:55,380 --> 00:25:57,870
So let's say if you want
records older than 365 days

547
00:25:57,870 --> 00:26:00,450
to be expired, you can do that.

548
00:26:00,450 --> 00:26:02,520
Live inventory table is enabled.

549
00:26:02,520 --> 00:26:05,550
You can again choose the
encryption type for the tables

550
00:26:05,550 --> 00:26:07,803
and create your metadata configurations.

551
00:26:09,000 --> 00:26:10,710
A few things that I
wanna mention over here

552
00:26:10,710 --> 00:26:13,080
is once you create your
configuration, the first thing

553
00:26:13,080 --> 00:26:15,210
that happens is that
your table status goes

554
00:26:15,210 --> 00:26:17,340
into a creating stage.

555
00:26:17,340 --> 00:26:21,540
Within a few seconds, the
journal table will become active.

556
00:26:21,540 --> 00:26:24,030
Journal tables are forward
looking, they capture your puts

557
00:26:24,030 --> 00:26:25,830
and deletes forward
looking, puts and deletes.

558
00:26:25,830 --> 00:26:28,740
So it's active as soon as
you set up the configuration.

559
00:26:28,740 --> 00:26:30,630
The live inventory table first goes

560
00:26:30,630 --> 00:26:33,240
into a back filling stage
because the live inventory table

561
00:26:33,240 --> 00:26:34,950
has everything about your buckets.

562
00:26:34,950 --> 00:26:37,530
All of the existing objects are captured.

563
00:26:37,530 --> 00:26:39,720
So it's first going to go
into the backfill stage.

564
00:26:39,720 --> 00:26:42,330
This is where we get the
metadata of all of your objects

565
00:26:42,330 --> 00:26:45,150
and get your ready and prepped up.

566
00:26:45,150 --> 00:26:47,730
Another thing I wanna
call out is you can see

567
00:26:47,730 --> 00:26:50,520
the table bucket here is AWS S3.

568
00:26:50,520 --> 00:26:53,850
This is the table bucket where
all of your metadata tables,

569
00:26:53,850 --> 00:26:56,850
all your journal live
inventory tables for the region

570
00:26:56,850 --> 00:26:58,530
and for the account will be hosted.

571
00:26:58,530 --> 00:27:01,140
So within this single
bucket you can see all

572
00:27:01,140 --> 00:27:03,540
of your inventory and journal tables.

573
00:27:03,540 --> 00:27:06,030
The namespace again might look familiar.

574
00:27:06,030 --> 00:27:09,150
This is the, this corresponds
very closely with the name

575
00:27:09,150 --> 00:27:12,210
of the bucket on which we
set up the metadata table.

576
00:27:12,210 --> 00:27:16,053
It's typically, it typically
has a prefix of B underscore.

577
00:27:17,280 --> 00:27:19,080
All right, so that was
all about setting up

578
00:27:19,080 --> 00:27:20,583
the metadata configuration.

579
00:27:21,780 --> 00:27:26,640
Next, we are going to go to
the second part of the demo,

580
00:27:26,640 --> 00:27:30,840
which is how do we query
these metadata tables?

581
00:27:30,840 --> 00:27:33,060
Now that we have learned
to set up these tables,

582
00:27:33,060 --> 00:27:36,960
let's explore how we can
read them and query them.

583
00:27:36,960 --> 00:27:39,360
So here I have both my journal

584
00:27:39,360 --> 00:27:42,120
and live inventory table are active.

585
00:27:42,120 --> 00:27:45,570
We're gonna start by
querying the journal table.

586
00:27:45,570 --> 00:27:48,630
Today I'm going to use both
SageMaker Unified Studio

587
00:27:48,630 --> 00:27:51,060
and Athena to query these tables.

588
00:27:51,060 --> 00:27:53,460
We'll start by querying the journal table

589
00:27:53,460 --> 00:27:56,100
and I'm using SageMaker Unified Studio.

590
00:27:56,100 --> 00:27:58,500
The integration between
SageMaker Unified Studio

591
00:27:58,500 --> 00:28:00,330
and S3 Table buckets is really simple.

592
00:28:00,330 --> 00:28:01,803
It's a one click integration.

593
00:28:03,300 --> 00:28:06,060
The first thing that we're
gonna see over here is I'd like

594
00:28:06,060 --> 00:28:09,183
to show you the entire topology
that we just talked about.

595
00:28:10,530 --> 00:28:12,480
So we're gonna go look up our bucket.

596
00:28:12,480 --> 00:28:15,990
As you can see, the AWS
S3 Table bucket shows up,

597
00:28:15,990 --> 00:28:18,930
your namespace, the which
is be underscore the name

598
00:28:18,930 --> 00:28:21,540
of your bucket, and the two
tables, which is journal

599
00:28:21,540 --> 00:28:24,540
and live inventory table
are showing up here.

600
00:28:24,540 --> 00:28:26,790
The next thing that I
would like to show you,

601
00:28:26,790 --> 00:28:29,010
this is the schema that
we just walked through

602
00:28:29,010 --> 00:28:31,380
when we talked about journal
and live inventory tables.

603
00:28:31,380 --> 00:28:33,330
Because this is journal, you're gonna see

604
00:28:33,330 --> 00:28:37,350
the three special, the special
fields about the requester,

605
00:28:37,350 --> 00:28:40,260
the request ID, the source IP address.

606
00:28:40,260 --> 00:28:41,340
The scenario that I'm going

607
00:28:41,340 --> 00:28:44,070
to test on the journal tables
today is show me everything

608
00:28:44,070 --> 00:28:46,950
that was deleted in the past seven days

609
00:28:46,950 --> 00:28:48,600
in a specific prefix.

610
00:28:48,600 --> 00:28:51,660
You can filter by a lot of
additional filters here,

611
00:28:51,660 --> 00:28:54,210
but I'd like to figure out
someone deleted objects

612
00:28:54,210 --> 00:28:56,400
in my specific prefix.

613
00:28:56,400 --> 00:28:57,750
And there we go.

614
00:28:57,750 --> 00:29:00,540
Within a couple of seconds,
I have a list of everything

615
00:29:00,540 --> 00:29:01,830
that was deleted

616
00:29:01,830 --> 00:29:05,400
and the requester who deleted
these objects in my prefix.

617
00:29:05,400 --> 00:29:07,560
This is why journal
tables are very powerful

618
00:29:07,560 --> 00:29:10,230
for investigative queries,
for audit analysis,

619
00:29:10,230 --> 00:29:12,270
for any time you're trying to track

620
00:29:12,270 --> 00:29:14,403
what happened in my bucket and who did it.

621
00:29:16,020 --> 00:29:17,460
The next part

622
00:29:17,460 --> 00:29:19,637
of this demo is querying
the live inventory tables.

623
00:29:19,637 --> 00:29:21,510
We are using Athena over here

624
00:29:21,510 --> 00:29:23,463
to query the live inventory tables.

625
00:29:24,570 --> 00:29:26,670
Live inventory tables are useful

626
00:29:26,670 --> 00:29:28,320
for storage landscape analysis.

627
00:29:28,320 --> 00:29:30,900
When you want to look
across your entire bucket

628
00:29:30,900 --> 00:29:32,520
and see what's going on.

629
00:29:32,520 --> 00:29:34,470
Here, I'd like to see everything

630
00:29:34,470 --> 00:29:36,480
that has specific kinds of tags

631
00:29:36,480 --> 00:29:38,280
and is in a particular storage class.

632
00:29:38,280 --> 00:29:40,050
You can add additional types

633
00:29:40,050 --> 00:29:43,290
of filters on encryption
on checks and types.

634
00:29:43,290 --> 00:29:45,750
Here I'm trying to get a list

635
00:29:45,750 --> 00:29:48,659
of all objects which meet the
object tags of a specific type

636
00:29:48,659 --> 00:29:52,250
of weather and a training category.

637
00:29:52,250 --> 00:29:55,410
And again, within a few
seconds I have a list

638
00:29:55,410 --> 00:29:58,170
of everything without me
having to list the objects,

639
00:29:58,170 --> 00:29:59,790
do the get object tagging.

640
00:29:59,790 --> 00:30:00,780
I'm able to get a list

641
00:30:00,780 --> 00:30:04,830
of everything that met the
specific criteria for the tags.

642
00:30:04,830 --> 00:30:07,230
Another interesting thing
that I want to point out here

643
00:30:07,230 --> 00:30:10,920
is that all of these objects
are in glacier storage classes,

644
00:30:10,920 --> 00:30:12,360
but the metadata is queryable.

645
00:30:12,360 --> 00:30:15,360
This is another powerful
aspect for S3 Metadata.

646
00:30:15,360 --> 00:30:17,662
You can get metadata across
all of your objects, across all

647
00:30:17,662 --> 00:30:21,390
of your storage classes and
it is instantly queryable.

648
00:30:21,390 --> 00:30:23,190
We are not restoring
anything at this point

649
00:30:23,190 --> 00:30:24,723
to get to this metadata.

650
00:30:26,130 --> 00:30:27,570
All right, so we have done two parts

651
00:30:27,570 --> 00:30:28,620
of our demo at this point.

652
00:30:28,620 --> 00:30:30,870
We learned how to set
up the configuration.

653
00:30:30,870 --> 00:30:33,750
We explored how can we
query both the journal

654
00:30:33,750 --> 00:30:35,550
and the live inventory tables.

655
00:30:35,550 --> 00:30:37,260
The third part of our demo today

656
00:30:37,260 --> 00:30:41,970
is taking storage management
actions based on the outputs

657
00:30:41,970 --> 00:30:43,953
of our metadata tables.

658
00:30:45,150 --> 00:30:49,470
So to do this, I am going
to take another scenario.

659
00:30:49,470 --> 00:30:51,780
You remember Alice, our data
scientist who was trying

660
00:30:51,780 --> 00:30:54,390
to look up data for training her models?

661
00:30:54,390 --> 00:30:56,100
She's actually training

662
00:30:56,100 --> 00:30:59,640
a parking assist specifically
for rainy weather.

663
00:30:59,640 --> 00:31:03,510
So what I want to do over
here is all of my raw data

664
00:31:03,510 --> 00:31:05,160
is stored in Glacier,

665
00:31:05,160 --> 00:31:07,530
and I want to restore all of that data

666
00:31:07,530 --> 00:31:09,423
so it's ready for her to use.

667
00:31:10,260 --> 00:31:11,610
Let's go over this quickly.

668
00:31:14,370 --> 00:31:16,290
So you've seen this query before.

669
00:31:16,290 --> 00:31:19,050
We just ran it on our inventory table.

670
00:31:19,050 --> 00:31:20,100
We got the list

671
00:31:20,100 --> 00:31:24,210
of everything which met
these specific tag criteria

672
00:31:24,210 --> 00:31:25,890
in Glacier storage class.

673
00:31:25,890 --> 00:31:28,560
I've limited it down to
just the bucket and the key

674
00:31:28,560 --> 00:31:30,390
because that is what I need

675
00:31:30,390 --> 00:31:32,793
to pass it on to a batch operations job.

676
00:31:33,990 --> 00:31:37,260
The easiest way for me to
pass the output of this query

677
00:31:37,260 --> 00:31:40,830
is to go to the bucket
where this output is stored.

678
00:31:40,830 --> 00:31:43,950
So I'm going to the actual S3 bucket

679
00:31:43,950 --> 00:31:45,810
where my query results are stored

680
00:31:45,810 --> 00:31:48,240
and that is what I'm going to pass

681
00:31:48,240 --> 00:31:50,220
to the batch operations job.

682
00:31:50,220 --> 00:31:53,670
So my results are stored in
this training query prefix.

683
00:31:53,670 --> 00:31:55,740
I'm going to go pick up the result.

684
00:31:55,740 --> 00:31:57,240
I have to do one small thing

685
00:31:57,240 --> 00:31:58,770
before I can use this result,

686
00:31:58,770 --> 00:32:00,510
which is convert it into a format

687
00:32:00,510 --> 00:32:02,700
in which batch operations job can read it.

688
00:32:02,700 --> 00:32:04,020
I can do it right here.

689
00:32:04,020 --> 00:32:06,570
I'm just going to
comma-separate the two values,

690
00:32:06,570 --> 00:32:08,790
which is bucket and key right here

691
00:32:08,790 --> 00:32:11,940
and store it as a manifest file.

692
00:32:11,940 --> 00:32:13,440
So that's what we are doing here.

693
00:32:13,440 --> 00:32:16,110
We're taking the output
from our metadata tables,

694
00:32:16,110 --> 00:32:18,510
converting it into a comma-separated list,

695
00:32:18,510 --> 00:32:21,030
storing it in this manifest file.

696
00:32:21,030 --> 00:32:23,940
I'm going to go fetch the
URI for that manifest file.

697
00:32:23,940 --> 00:32:26,010
And this is basically
the most important step.

698
00:32:26,010 --> 00:32:27,870
You got your data, you got it prepped

699
00:32:27,870 --> 00:32:30,690
and it's ready for your batch processing.

700
00:32:30,690 --> 00:32:34,140
I'm going to take the URI of
this manifest file, pass it

701
00:32:34,140 --> 00:32:35,853
to the batch operations job.

702
00:32:36,690 --> 00:32:39,360
That's essentially the largest step

703
00:32:39,360 --> 00:32:40,860
that you have with this.

704
00:32:40,860 --> 00:32:44,130
Next, what we need to do, a
couple of more configurations.

705
00:32:44,130 --> 00:32:46,560
Most importantly, you need
to choose the operation

706
00:32:46,560 --> 00:32:48,330
that you want the batch operations job

707
00:32:48,330 --> 00:32:49,410
to perform here.

708
00:32:49,410 --> 00:32:52,560
We're restoring objects from
Glacier, getting them ready.

709
00:32:52,560 --> 00:32:53,580
So I chose Restore.

710
00:32:53,580 --> 00:32:56,100
I want them to be available for five days,

711
00:32:56,100 --> 00:32:58,260
and I'd like to store,

712
00:32:58,260 --> 00:33:01,230
and we're going to run
this batch operations job

713
00:33:01,230 --> 00:33:02,220
at a priority.

714
00:33:02,220 --> 00:33:04,743
So I'm selecting run when it's ready.

715
00:33:06,000 --> 00:33:08,130
I typically like to have
my completion reports

716
00:33:08,130 --> 00:33:10,260
in the same place as my manifest file.

717
00:33:10,260 --> 00:33:11,490
So I'm quickly going to go

718
00:33:11,490 --> 00:33:15,423
and fetch the URI of where
my completion reports sit.

719
00:33:16,980 --> 00:33:19,230
So here's the URI for
my completion reports.

720
00:33:19,230 --> 00:33:22,020
I'm going to patch pass this
to the batch operations job

721
00:33:22,020 --> 00:33:23,130
and that's basically it.

722
00:33:23,130 --> 00:33:25,630
You choose the role that
you want batch operations

723
00:33:26,543 --> 00:33:29,730
to use when it's running
your restore operation

724
00:33:29,730 --> 00:33:32,730
and you have set up a batch operations job

725
00:33:32,730 --> 00:33:35,283
using the output of your queries.

726
00:33:37,140 --> 00:33:38,610
Couple of things that you wanna look at

727
00:33:38,610 --> 00:33:39,930
as you're setting up the job

728
00:33:39,930 --> 00:33:42,450
is when you have the job set up,

729
00:33:42,450 --> 00:33:44,760
you can see the whole manifest,
you can see the number

730
00:33:44,760 --> 00:33:46,740
of objects that it is going to restore.

731
00:33:46,740 --> 00:33:48,256
So let's go look

732
00:33:48,256 --> 00:33:50,220
at the batch operations job
here that we just set up.

733
00:33:50,220 --> 00:33:53,880
It's showing that there 88
objects in my manifest list,

734
00:33:53,880 --> 00:33:55,530
they're all going to be restored.

735
00:33:55,530 --> 00:33:56,820
The restore is the operation.

736
00:33:56,820 --> 00:33:59,850
It's going to be restored for five days.

737
00:33:59,850 --> 00:34:02,220
So this is the power of us being able

738
00:34:02,220 --> 00:34:05,520
to use S3 Metadata to run
batch processing jobs.

739
00:34:05,520 --> 00:34:08,340
You're no longer spending
hours time to prep your data,

740
00:34:08,340 --> 00:34:11,430
you're just taking the output
of a query, processing it,

741
00:34:11,430 --> 00:34:14,380
getting it ready for a batch
operations job and passing it.

742
00:34:15,270 --> 00:34:19,260
And finally, for the last part
of our demo today, I'm going

743
00:34:19,260 --> 00:34:22,170
to talk about querying
with natural language.

744
00:34:22,170 --> 00:34:25,020
So you can ask, Claire had
mentioned there are three ways

745
00:34:25,020 --> 00:34:27,630
you can query your metadata tables.

746
00:34:27,630 --> 00:34:28,890
We're gonna try querying them

747
00:34:28,890 --> 00:34:33,003
with natural language using
Kiro CLI and MCP for S3 Tables.

748
00:34:34,800 --> 00:34:39,800
So let me start this by
first initializing Kiro.

749
00:34:41,820 --> 00:34:45,480
I already have MCP for S3
Tables set up with Kiro.

750
00:34:45,480 --> 00:34:47,790
So you can see that as
soon as I initialize it,

751
00:34:47,790 --> 00:34:51,933
it will start loading up the
S3 Tables for MCP server.

752
00:34:52,870 --> 00:34:54,300
And this is what actually enables us

753
00:34:54,300 --> 00:34:57,180
to query metadata tables
using natural language,

754
00:34:57,180 --> 00:34:59,280
the MCP for S3 Tables.

755
00:34:59,280 --> 00:35:02,502
Now, instead of manually
creating the SQL queries,

756
00:35:02,502 --> 00:35:03,540
I'm basically passing
it a prompt, telling it

757
00:35:03,540 --> 00:35:06,840
to analyze my storage, go look
at all of the object counts,

758
00:35:06,840 --> 00:35:09,513
the storage class
distribution, the prefixes.

759
00:35:10,380 --> 00:35:12,750
And now look as Kiro interprets my request

760
00:35:12,750 --> 00:35:15,960
and automatically executes the analysis.

761
00:35:15,960 --> 00:35:17,040
Behind the scenes,

762
00:35:17,040 --> 00:35:19,860
it's basically connecting
to S3 Metadata tables.

763
00:35:19,860 --> 00:35:23,220
It's going to ask me
permissions to run a few tools,

764
00:35:23,220 --> 00:35:24,840
specifically the query database.

765
00:35:24,840 --> 00:35:26,290
That's where, that's the tool

766
00:35:27,150 --> 00:35:29,350
it's going to go and
use to query my tables.

767
00:35:30,360 --> 00:35:33,840
And now it's ready to run the SQL queries.

768
00:35:33,840 --> 00:35:36,630
It's connecting to the
S3 Tables at this point,

769
00:35:36,630 --> 00:35:39,330
optimizing the SQL
queries that it's running.

770
00:35:39,330 --> 00:35:41,070
I also prompted Kiro

771
00:35:41,070 --> 00:35:43,350
to tell me why it's
running a certain query.

772
00:35:43,350 --> 00:35:46,080
So you can see first started
by looking at the count,

773
00:35:46,080 --> 00:35:48,060
the size, the distribution.

774
00:35:48,060 --> 00:35:51,900
Once it has that information,
it's moving on to prefixes

775
00:35:51,900 --> 00:35:53,790
and finding the largest prefixes.

776
00:35:53,790 --> 00:35:56,370
This is all happening without me running

777
00:35:56,370 --> 00:35:59,550
a single SQL query or telling
it like what SQL do I want.

778
00:35:59,550 --> 00:36:02,670
All I did was tell it exactly
what I wanted to look at.

779
00:36:02,670 --> 00:36:06,030
So it's correcting, it's
looking up prefixes,

780
00:36:06,030 --> 00:36:08,280
correcting the prefix analysis,

781
00:36:08,280 --> 00:36:10,983
analyzing the storage
distribution per prefix.

782
00:36:12,599 --> 00:36:16,170
And finally, I'm expecting
it to provide me all

783
00:36:16,170 --> 00:36:19,500
of this in a summary, which
it is doing at this point.

784
00:36:19,500 --> 00:36:22,530
A clean executive summary
with actionable outputs.

785
00:36:22,530 --> 00:36:24,660
You can see the story,
total number of objects.

786
00:36:24,660 --> 00:36:26,970
You can see the storage
class distribution,

787
00:36:26,970 --> 00:36:28,631
the prefix distribution.

788
00:36:28,631 --> 00:36:31,020
It's also making some
recommendations on some

789
00:36:31,020 --> 00:36:33,090
of the actions that I can take.

790
00:36:33,090 --> 00:36:37,680
So this is the power of
combining your S3 Metadata Tables

791
00:36:37,680 --> 00:36:41,610
with with Kiro, with MCP for S3 Tables.

792
00:36:41,610 --> 00:36:42,600
You're able to talk

793
00:36:42,600 --> 00:36:46,620
to your metadata tables,
your business teams can talk

794
00:36:46,620 --> 00:36:47,760
to your metadata tables.

795
00:36:47,760 --> 00:36:49,863
You do not have to be a SQL expert.

796
00:36:51,750 --> 00:36:54,870
All right, all right, so that's
metadata in action today.

797
00:36:54,870 --> 00:36:56,790
The next question that
you're probably asking

798
00:36:56,790 --> 00:36:58,860
is, which, where can I use it?

799
00:36:58,860 --> 00:37:00,360
Which regions are we available in?

800
00:37:00,360 --> 00:37:03,030
We're available in 28 regions today,

801
00:37:03,030 --> 00:37:06,420
22 of which were launched
in the past week.

802
00:37:06,420 --> 00:37:09,610
We are continuing to expand
our regional footprint

803
00:37:10,470 --> 00:37:11,823
and are very excited.

804
00:37:13,530 --> 00:37:15,120
We've covered a lot today.

805
00:37:15,120 --> 00:37:18,870
We've gone about, we
talked about the challenges

806
00:37:18,870 --> 00:37:21,420
and the opportunities with data discovery.

807
00:37:21,420 --> 00:37:25,170
We've talked about how S3
Metadata solves these challenges.

808
00:37:25,170 --> 00:37:27,300
We've seen a demo in action.

809
00:37:27,300 --> 00:37:29,580
Now, I wanna talk about the real impact.

810
00:37:29,580 --> 00:37:32,610
I wanna show, share with you two customer,

811
00:37:32,610 --> 00:37:34,140
real world customer scenarios

812
00:37:34,140 --> 00:37:38,490
where customers deployed metadata
and saw immediate results.

813
00:37:38,490 --> 00:37:41,190
The first one that I want
to share is the story

814
00:37:41,190 --> 00:37:44,850
of a medical imaging customer
that generates 3D models

815
00:37:44,850 --> 00:37:47,280
by, from CT scans, processing thousands

816
00:37:47,280 --> 00:37:49,710
of objects and files every hour.

817
00:37:49,710 --> 00:37:52,560
Now, previously, this customer used Events

818
00:37:52,560 --> 00:37:55,170
and they processed each file with Lambda.

819
00:37:55,170 --> 00:37:57,240
This whole workflow was very fragile.

820
00:37:57,240 --> 00:37:58,530
It would time out.

821
00:37:58,530 --> 00:38:00,600
There was a lot of complexity involved.

822
00:38:00,600 --> 00:38:03,000
When we introduced S3 Metadata Tables,

823
00:38:03,000 --> 00:38:05,190
this customer completely refreshed

824
00:38:05,190 --> 00:38:08,130
their job processing
pipeline with S3 Metadata.

825
00:38:08,130 --> 00:38:11,310
They now use S3 Metadata journal tables.

826
00:38:11,310 --> 00:38:14,430
They query, they get the query
outputs every 15 minutes,

827
00:38:14,430 --> 00:38:17,010
and they use an open source
orchestration platform.

828
00:38:17,010 --> 00:38:19,680
So they feed the output
of the, every 15 minutes,

829
00:38:19,680 --> 00:38:21,270
they feed the output of the journal table

830
00:38:21,270 --> 00:38:23,430
to the open source orchestration platform,

831
00:38:23,430 --> 00:38:26,670
and that helps them batch
process their new objects.

832
00:38:26,670 --> 00:38:30,000
They're now processing
thousands of files every hour,

833
00:38:30,000 --> 00:38:31,350
which has dramatically reduced

834
00:38:31,350 --> 00:38:33,300
their overall processing time.

835
00:38:33,300 --> 00:38:35,400
This is really an impactful way

836
00:38:35,400 --> 00:38:37,983
to meaningfully speed up your workflows.

837
00:38:38,970 --> 00:38:40,620
The second story that I wanna share

838
00:38:40,620 --> 00:38:43,800
is of a digital content
company that has millions

839
00:38:43,800 --> 00:38:46,380
of articles from thousands of publishers.

840
00:38:46,380 --> 00:38:48,720
Their challenge, they're moving everything

841
00:38:48,720 --> 00:38:51,600
from on-premise to cloud
and they need to keep track

842
00:38:51,600 --> 00:38:53,040
of these millions of files.

843
00:38:53,040 --> 00:38:55,023
Imagine their different systems.

844
00:38:55,987 --> 00:38:58,230
They have got old servers
databases, there's S3.

845
00:38:58,230 --> 00:38:59,820
Without S3 Metadata,

846
00:38:59,820 --> 00:39:02,340
this would have been a manual nightmare.

847
00:39:02,340 --> 00:39:05,810
However, with S3 Metadata
inventory tables...

848
00:39:06,840 --> 00:39:07,673
Sorry.

849
00:39:07,673 --> 00:39:09,660
This with S3 Metadata inventory tables,

850
00:39:09,660 --> 00:39:11,310
they're using simple SQL queries

851
00:39:11,310 --> 00:39:16,140
to find their data based on
database IDs and migration tags.

852
00:39:16,140 --> 00:39:17,280
Now they're able

853
00:39:17,280 --> 00:39:19,920
to answer simple
questions like which files

854
00:39:19,920 --> 00:39:23,340
were successfully updated,
which files are duplicates,

855
00:39:23,340 --> 00:39:25,050
which ones we can just delete.

856
00:39:25,050 --> 00:39:26,790
They're just able to
answer these questions

857
00:39:26,790 --> 00:39:28,770
with simple SQL queries.

858
00:39:28,770 --> 00:39:30,300
They're now moving petabytes

859
00:39:30,300 --> 00:39:34,200
of data confidently full
visibility into every file

860
00:39:34,200 --> 00:39:37,653
and there's no manual tracking,
no guesswork involved.

861
00:39:40,230 --> 00:39:42,840
Finally, to conclude our session for today

862
00:39:42,840 --> 00:39:46,830
with these impactful stories,
I also want to leave you

863
00:39:46,830 --> 00:39:48,210
with three key takeaways

864
00:39:48,210 --> 00:39:50,520
on how S3 Metadata will help

865
00:39:50,520 --> 00:39:53,400
your organizations with data discovery.

866
00:39:53,400 --> 00:39:57,450
First, it always provides
you with current metadata.

867
00:39:57,450 --> 00:39:58,830
So you can stop searching

868
00:39:58,830 --> 00:40:01,410
and just start querying your
tables for what you need.

869
00:40:01,410 --> 00:40:03,120
Your data scientists, your security teams,

870
00:40:03,120 --> 00:40:04,487
your engineers

871
00:40:04,487 --> 00:40:06,713
can basically look up
information within minutes.

872
00:40:07,590 --> 00:40:09,720
Second, you can, with these,

873
00:40:09,720 --> 00:40:11,940
the two Iceberg-compatible
tables, the journal

874
00:40:11,940 --> 00:40:13,590
and the live inventory tables,

875
00:40:13,590 --> 00:40:15,810
you can now build smart workflows

876
00:40:15,810 --> 00:40:18,690
and convert all of these
storage insights into actions.

877
00:40:18,690 --> 00:40:23,220
You can, we saw how you can
build batch workflows at scale.

878
00:40:23,220 --> 00:40:26,490
And finally, you can
future-proof your data lake.

879
00:40:26,490 --> 00:40:30,090
S3 Metadata provides the
foundation for your AI agents

880
00:40:30,090 --> 00:40:33,393
that can now interact
intelligently with your storage.

881
00:40:34,620 --> 00:40:36,510
And here are some resources

882
00:40:36,510 --> 00:40:39,810
that will help you get started
on your metadata journey.

883
00:40:39,810 --> 00:40:42,120
We're very grateful to
have you all here today.

884
00:40:42,120 --> 00:40:44,280
Thank you for spending the time with us,

885
00:40:44,280 --> 00:40:47,043
and good luck with the rest
of your Reinvent session.

886
00:40:47,043 --> 00:40:47,973
Thank you.

