1
00:00:01,198 --> 00:00:03,960
(audience murmuring)

2
00:00:03,960 --> 00:00:05,760
- So, my name is Anup Nair.

3
00:00:05,760 --> 00:00:09,003
I am the CTO for Mphasis AI.

4
00:00:09,900 --> 00:00:11,050
And this is Bharat.

5
00:00:11,050 --> 00:00:14,193
- I'm Bharat, senior
partner in Mphasis.ai.

6
00:00:15,600 --> 00:00:19,030
- So, we are gonna talk
about legacy modernization

7
00:00:19,030 --> 00:00:23,820
and Mphasis framework,
agentic AI framework

8
00:00:23,820 --> 00:00:25,140
for legacy modernization.

9
00:00:25,140 --> 00:00:27,210
That's what we are gonna talk about.

10
00:00:27,210 --> 00:00:32,210
We have some exciting demos
as well as part of this thing.

11
00:00:32,400 --> 00:00:35,250
So, I'm hoping you'll like it.

12
00:00:35,250 --> 00:00:37,470
And if you have any questions,

13
00:00:37,470 --> 00:00:40,440
please feel free to get us offline.

14
00:00:40,440 --> 00:00:41,940
I don't think they
allow us questions here,

15
00:00:41,940 --> 00:00:43,593
but yeah, why not, right?

16
00:00:44,550 --> 00:00:47,460
So, to start off with,

17
00:00:47,460 --> 00:00:49,920
let me give a little
background here, right?

18
00:00:49,920 --> 00:00:52,023
Let's discuss the problem statement first.

19
00:00:55,170 --> 00:01:00,170
I've, in the last 25 years,
every CIO I have met,

20
00:01:00,180 --> 00:01:05,180
this is the problem: "I
can't innovate fast enough

21
00:01:05,400 --> 00:01:08,727
because it's too risky to
touch any legacy platform."

22
00:01:10,230 --> 00:01:15,150
Are you guys on the same
page when it comes to this?

23
00:01:15,150 --> 00:01:18,510
In fact, one of the CIOs actually told me

24
00:01:18,510 --> 00:01:22,570
that I have 49 core systems that are built

25
00:01:24,040 --> 00:01:29,040
on COBOL mainframes, and
if I touch one of them,

26
00:01:29,220 --> 00:01:31,773
I will touch all the remaining 48 as well.

27
00:01:33,450 --> 00:01:35,703
So, this is the problem
we decided to solve.

28
00:01:36,850 --> 00:01:41,850
Mphasis.ai has built
many different agentic AI

29
00:01:41,940 --> 00:01:44,400
solutions to solve these problems.

30
00:01:44,400 --> 00:01:48,210
So, Mphasis has an experience building

31
00:01:48,210 --> 00:01:52,380
and modernizing legacy
application for many years.

32
00:01:52,380 --> 00:01:54,420
So, we are gonna talk about this,

33
00:01:54,420 --> 00:01:56,340
but more importantly,

34
00:01:56,340 --> 00:01:58,980
I think I wanna double click
on the problem a little bit

35
00:01:58,980 --> 00:02:02,940
more to really understand
the genesis of our solution.

36
00:02:02,940 --> 00:02:05,670
Really describe the solution itself.

37
00:02:05,670 --> 00:02:08,790
So, you know, the problem
with legacy system

38
00:02:08,790 --> 00:02:13,440
is that enterprises, you know,

39
00:02:13,440 --> 00:02:16,710
are anchored on these systems for years.

40
00:02:16,710 --> 00:02:20,733
And, these systems have code,

41
00:02:20,733 --> 00:02:24,753
business logic embedded
inside code for years, right?

42
00:02:25,740 --> 00:02:29,943
And, every time you want to do something,

43
00:02:30,930 --> 00:02:33,540
you need engineers, right?

44
00:02:33,540 --> 00:02:35,590
You add business logic, this is in COBOL,

45
00:02:36,540 --> 00:02:41,130
this is in Java, or it could
be Natural or Natural Adabas

46
00:02:41,130 --> 00:02:44,370
or it could be Assembler
for that matter, right?

47
00:02:44,370 --> 00:02:46,080
It could be all of those kind of system.

48
00:02:46,080 --> 00:02:49,293
But you need specialist
engineers to handle this.

49
00:02:50,160 --> 00:02:52,590
These engineers are not easily available.

50
00:02:52,590 --> 00:02:54,450
They're not in the market, they're out

51
00:02:54,450 --> 00:02:56,970
of the workforce at this point in time.

52
00:02:56,970 --> 00:03:00,720
Therefore, you start
reinventing the wheel,

53
00:03:00,720 --> 00:03:03,360
keep reinventing the
wheel on an ongoing basis,

54
00:03:03,360 --> 00:03:05,710
and you start creating
a lot of technical debt.

55
00:03:06,660 --> 00:03:09,960
Now, any change is too costly

56
00:03:09,960 --> 00:03:12,180
because you're doing it
at 10 different places

57
00:03:12,180 --> 00:03:14,760
and you're connecting,
creating layers and layers

58
00:03:14,760 --> 00:03:16,740
and layers to solve the problem

59
00:03:16,740 --> 00:03:19,620
because you don't know
what's actually going on.

60
00:03:19,620 --> 00:03:22,350
So, now you think about this

61
00:03:22,350 --> 00:03:27,090
and you decide to put AI on
top of it, agentic AI, right?

62
00:03:27,090 --> 00:03:29,280
You're driving innovation.
You need to put agents, right?

63
00:03:29,280 --> 00:03:32,040
Whether it is agents around claims

64
00:03:32,040 --> 00:03:33,540
or agents around underwriting,

65
00:03:33,540 --> 00:03:36,780
or whichever agent, you wanna
put agents on top of this.

66
00:03:36,780 --> 00:03:38,250
Think how it is gonna be,

67
00:03:38,250 --> 00:03:40,110
because all of these legacy systems

68
00:03:40,110 --> 00:03:44,970
are so deeply monolithic, it
is extremely hard for any agent

69
00:03:44,970 --> 00:03:46,110
to give you any productivity.

70
00:03:46,110 --> 00:03:49,650
So, all your agentic AI objectives

71
00:03:49,650 --> 00:03:52,620
and goals goes for a toss
just because you have this.

72
00:03:52,620 --> 00:03:55,260
This is the reason why modernization.

73
00:03:55,260 --> 00:03:58,320
This is the reason why
any innovation is so slow,

74
00:03:58,320 --> 00:04:00,670
because you have to touch
everything out there.

75
00:04:01,800 --> 00:04:05,730
So, at Mphasis, we thought about this

76
00:04:05,730 --> 00:04:09,260
and we said, okay, you
know, it is really not just

77
00:04:09,260 --> 00:04:11,370
about technical modernization.

78
00:04:11,370 --> 00:04:14,610
It's not just modernizing COBOL to Java

79
00:04:14,610 --> 00:04:19,610
or Natural to Java or
C++ to something new.

80
00:04:20,940 --> 00:04:22,416
That is not the point.

81
00:04:22,416 --> 00:04:24,960
It's about taking the intelligence

82
00:04:24,960 --> 00:04:29,960
out of the legacy systems and
converting that into data.

83
00:04:31,860 --> 00:04:35,700
That should be the goal, as
opposed to just modernizing

84
00:04:35,700 --> 00:04:37,800
code from A to B.

85
00:04:37,800 --> 00:04:39,950
That is exactly the
approach we have taken.

86
00:04:41,220 --> 00:04:42,390
So, let me talk a little bit more

87
00:04:42,390 --> 00:04:44,100
on how we are about to solve this thing.

88
00:04:44,100 --> 00:04:47,520
But for that, how do
you typically modernize?

89
00:04:47,520 --> 00:04:49,220
Are you aligned to this, everyone?

90
00:04:50,070 --> 00:04:53,250
First you'll relearn
from your legacy systems,

91
00:04:53,250 --> 00:04:54,693
then you reimagine,

92
00:04:55,890 --> 00:04:59,007
then you re-architect, and then you recode

93
00:04:59,007 --> 00:05:00,870
and then you start running it.

94
00:05:00,870 --> 00:05:03,030
This is how you do, you take COBOL

95
00:05:03,030 --> 00:05:05,100
and you convert it into Java

96
00:05:05,100 --> 00:05:06,700
and you start managing it again.

97
00:05:07,740 --> 00:05:10,440
In about three years' time,
that becomes legacy again.

98
00:05:11,490 --> 00:05:14,250
This is how you typically do it, right?

99
00:05:14,250 --> 00:05:18,020
So, what we have done is
slightly different approach here.

100
00:05:18,020 --> 00:05:23,020
We've built agent AI agents,

101
00:05:23,130 --> 00:05:25,440
AI agents that are autonomous

102
00:05:25,440 --> 00:05:28,560
and semi-autonomous
with human in the loop.

103
00:05:28,560 --> 00:05:30,300
Because anybody who says

104
00:05:30,300 --> 00:05:33,720
that everything is 100%
autonomous is joking.

105
00:05:33,720 --> 00:05:35,850
It's not. Okay?

106
00:05:35,850 --> 00:05:38,100
So, we took documents

107
00:05:38,100 --> 00:05:41,073
and code, we built an
agent called NeoZeta.

108
00:05:42,090 --> 00:05:45,540
And, this agent actually reads everything,

109
00:05:45,540 --> 00:05:50,540
converts everything into
human-understandable knowledge.

110
00:05:51,540 --> 00:05:52,770
How do you do that?

111
00:05:52,770 --> 00:05:55,903
We use domain knowledge,
Mphasis domain knowledge

112
00:05:55,903 --> 00:05:57,450
to really make that happen

113
00:05:57,450 --> 00:05:59,460
because we have encoded them.

114
00:05:59,460 --> 00:06:02,610
We create a knowledge graph.

115
00:06:02,610 --> 00:06:05,520
And then we thought, okay,
now we have a knowledge graph.

116
00:06:05,520 --> 00:06:10,110
How do you take this
and create a new system?

117
00:06:10,110 --> 00:06:13,050
What do you do? You generate
user stories out of it.

118
00:06:13,050 --> 00:06:17,230
So, we created another
agent, we call it NeoSaba.

119
00:06:18,630 --> 00:06:21,210
This agent takes everything
that you've learned,

120
00:06:21,210 --> 00:06:23,490
converts into user stories

121
00:06:23,490 --> 00:06:27,180
and focuses on governance,
focuses on compliance,

122
00:06:27,180 --> 00:06:30,303
focuses on processes, business processes.

123
00:06:31,170 --> 00:06:34,113
Saba stands for semi-autonomous
business analyst.

124
00:06:35,160 --> 00:06:39,750
We take that and we allow a
business analyst to work on it.

125
00:06:39,750 --> 00:06:42,780
We then convert whatever
we create out of this,

126
00:06:42,780 --> 00:06:44,640
we put it back

127
00:06:44,640 --> 00:06:48,033
into the knowledge graph.

128
00:06:49,140 --> 00:06:50,373
Now, what's next step?

129
00:06:51,270 --> 00:06:56,130
You re-architect. We've
created another agent.

130
00:06:56,130 --> 00:06:58,680
we called it NeoRaina.

131
00:06:58,680 --> 00:07:03,300
This is a agent that will
take everything you've learned

132
00:07:03,300 --> 00:07:06,303
and helps you define the
target state architecture.

133
00:07:07,140 --> 00:07:11,250
This target state
architecture is customizable

134
00:07:11,250 --> 00:07:13,740
for you, for the client.

135
00:07:13,740 --> 00:07:16,620
And, everything you
create out of this aligns

136
00:07:16,620 --> 00:07:18,240
to enterprise standards.

137
00:07:18,240 --> 00:07:19,650
The reason why it's customizable

138
00:07:19,650 --> 00:07:22,380
is because you need to
make sure that it aligns

139
00:07:22,380 --> 00:07:23,580
to enterprise standards.

140
00:07:24,870 --> 00:07:25,703
Okay?

141
00:07:25,703 --> 00:07:27,120
So, once you create that,

142
00:07:27,120 --> 00:07:29,193
then all the architecture is done.

143
00:07:30,030 --> 00:07:32,040
You take that and you rewrite.

144
00:07:32,040 --> 00:07:34,800
We've created an agent called NeoCrux,

145
00:07:34,800 --> 00:07:37,860
which takes everything,
allows you to prompt,

146
00:07:37,860 --> 00:07:40,260
it uses whatever coding agent

147
00:07:40,260 --> 00:07:43,260
is available in your enterprise

148
00:07:43,260 --> 00:07:45,753
and starts writing code out of it.

149
00:07:47,400 --> 00:07:52,080
Continuation to ops as
well, right? Why not?

150
00:07:52,080 --> 00:07:54,630
You've built everything,
you put into production,

151
00:07:54,630 --> 00:07:56,520
you've tested it through Crux and all.

152
00:07:56,520 --> 00:07:57,990
Now you're putting into production,

153
00:07:57,990 --> 00:07:59,463
you run operations as well.

154
00:08:00,450 --> 00:08:03,090
But, all through this we've
built a connective tissue.

155
00:08:03,090 --> 00:08:06,090
We've extracted the intelligence
out of the whole thing,

156
00:08:06,090 --> 00:08:09,390
and created an enterprise knowledge graph.

157
00:08:09,390 --> 00:08:12,510
That enterprise knowledge
graph is not just a database.

158
00:08:12,510 --> 00:08:14,040
Knowledge graph's a database,

159
00:08:14,040 --> 00:08:15,810
but this is not just a database.

160
00:08:15,810 --> 00:08:19,770
It has meaning, it has enterprise meaning

161
00:08:19,770 --> 00:08:22,290
because it is connected to your domain.

162
00:08:22,290 --> 00:08:23,880
We call it Ontosphere.

163
00:08:23,880 --> 00:08:26,250
And then we used a layer on top of it

164
00:08:26,250 --> 00:08:30,120
to orchestrate the whole
thing so that we can get it

165
00:08:30,120 --> 00:08:31,833
as autonomous as possible.

166
00:08:33,090 --> 00:08:36,783
So, this is Mphasis'
approach to modernization.

167
00:08:37,680 --> 00:08:40,620
Now, you will see some
of them are autonomous,

168
00:08:40,620 --> 00:08:43,620
some of them are semi-autonomous.

169
00:08:43,620 --> 00:08:48,420
As AI progresses, as
context abilities grow,

170
00:08:48,420 --> 00:08:50,340
this will become more and more autonomous.

171
00:08:50,340 --> 00:08:52,080
And, essentially you create a live

172
00:08:52,080 --> 00:08:55,050
intelligence of the enterprise.

173
00:08:55,050 --> 00:08:58,620
So, what you are going
to see today as a demo,

174
00:08:58,620 --> 00:09:00,167
you're gonna see NeoZeta

175
00:09:01,886 --> 00:09:06,180
extracting intelligence out of code.

176
00:09:06,180 --> 00:09:10,470
You're gonna see NeoSaba
allowing the business user

177
00:09:10,470 --> 00:09:12,390
to create user stories.

178
00:09:12,390 --> 00:09:16,080
You're gonna see NeoRaina,
you are gonna see NewCrux

179
00:09:16,080 --> 00:09:17,880
and you're not gonna see AI ops

180
00:09:17,880 --> 00:09:19,830
because we didn't have enough time.

181
00:09:19,830 --> 00:09:22,480
But you're gonna see a
glimpse of Ontosphere as well.

182
00:09:23,340 --> 00:09:28,340
So, you're gonna see
five things, Zeta, Saba,

183
00:09:28,830 --> 00:09:32,253
Raina, Crux and Ontosphere.

184
00:09:35,100 --> 00:09:35,933
Okay?

185
00:09:35,933 --> 00:09:39,090
So, I'm gonna switch to a demo

186
00:09:39,090 --> 00:09:43,263
and let Bharat walk you
through a whole suit of things.

187
00:09:44,850 --> 00:09:45,683
Bharat.

188
00:09:45,683 --> 00:09:48,630
- So, let me start with first
talking about Ontosphere

189
00:09:48,630 --> 00:09:50,130
because this is where the heart

190
00:09:50,130 --> 00:09:52,170
of the information lies, right?

191
00:09:52,170 --> 00:09:54,960
Every knowledge in the
system, whether it is code

192
00:09:54,960 --> 00:09:57,630
or document, is gonna be brought

193
00:09:57,630 --> 00:09:59,760
into as data on the on sphere.

194
00:09:59,760 --> 00:10:02,950
Now, we are using a couple
of domain ontologies

195
00:10:07,530 --> 00:10:09,750
which have been built
up, which is, you know,

196
00:10:09,750 --> 00:10:13,170
on the financial industry,
insurance and other industries.

197
00:10:13,170 --> 00:10:15,330
We have also kind of
really put in a couple

198
00:10:15,330 --> 00:10:18,810
of engineering ontologies,
which is through our agents.

199
00:10:18,810 --> 00:10:19,643
Okay?

200
00:10:19,643 --> 00:10:21,420
This is the basis on which a lot

201
00:10:21,420 --> 00:10:23,940
of modeling will be done into the system.

202
00:10:23,940 --> 00:10:27,060
Let me go to the first
agent. So, this is NeoZeta.

203
00:10:27,060 --> 00:10:31,440
As you see, NeoZeta,
I've selected a program,

204
00:10:31,440 --> 00:10:33,480
I've selected a capability,

205
00:10:33,480 --> 00:10:35,850
and we took a very complex
capability in terms

206
00:10:35,850 --> 00:10:38,460
of the post-trade processing because this

207
00:10:38,460 --> 00:10:42,270
is where you have a process
which stops at 4:00.

208
00:10:42,270 --> 00:10:45,480
You have maybe millions of
trades which have to be processed

209
00:10:45,480 --> 00:10:48,900
and files have to be sent
to the federal authorities.

210
00:10:48,900 --> 00:10:52,320
So, we took this problem,
we said that the only way

211
00:10:52,320 --> 00:10:55,290
that we will be able to
solve this is pick up a lot

212
00:10:55,290 --> 00:10:57,420
of intelligence from
each of these processes,

213
00:10:57,420 --> 00:10:59,493
which have been coded in legacy COBOL.

214
00:11:00,360 --> 00:11:03,090
This is an example of COBOL
that we are showing you,

215
00:11:03,090 --> 00:11:07,200
and then reverse engineer it
and post that help to kind of

216
00:11:07,200 --> 00:11:09,780
really modernize it using our agents

217
00:11:09,780 --> 00:11:12,240
onto a Flink based architecture.

218
00:11:12,240 --> 00:11:15,330
And I'll show you a glimpse
of how did we execute it

219
00:11:15,330 --> 00:11:17,130
on A CPU, as well as on A GPU,

220
00:11:17,130 --> 00:11:19,230
how the performance varies.

221
00:11:19,230 --> 00:11:22,020
So, continuing with our demo,

222
00:11:22,020 --> 00:11:26,190
I've selected the capability.

223
00:11:26,190 --> 00:11:28,470
I have capability to upload my files,

224
00:11:28,470 --> 00:11:31,980
which may be documents, which
may be different assets.

225
00:11:31,980 --> 00:11:34,620
I've uploaded COBOL copy books,

226
00:11:34,620 --> 00:11:37,023
and I've also uploaded, as you see,

227
00:11:37,950 --> 00:11:39,540
a domain model out here.

228
00:11:39,540 --> 00:11:42,120
So, these are the programs that you see,

229
00:11:42,120 --> 00:11:44,880
and this is the domain
model that I've uploaded.

230
00:11:44,880 --> 00:11:46,860
Using the relearn agent,

231
00:11:46,860 --> 00:11:49,710
I'm actually relearning
each and every program

232
00:11:49,710 --> 00:11:51,540
or an entire capability.

233
00:11:51,540 --> 00:11:54,930
So, as you see, I've actually
selected a program which

234
00:11:54,930 --> 00:11:57,780
is Quantiles, along with copy books,

235
00:11:57,780 --> 00:12:00,513
and I'm doing a reverse
engineering of it, right?

236
00:12:02,270 --> 00:12:05,610
So, what it does is it has capabilities

237
00:12:05,610 --> 00:12:07,443
to generate the data dictionary.

238
00:12:08,280 --> 00:12:10,950
Now, we haven't fed any information,

239
00:12:10,950 --> 00:12:13,200
but you see that there's a
good amount of information

240
00:12:13,200 --> 00:12:15,960
that it has brought out from the system,

241
00:12:15,960 --> 00:12:17,220
and it's starting to map it

242
00:12:17,220 --> 00:12:20,160
with the actual functional attributes.

243
00:12:20,160 --> 00:12:23,670
And I'm also showing you the
document that it produces.

244
00:12:23,670 --> 00:12:27,810
The document has a certain
format, it produces summary.

245
00:12:27,810 --> 00:12:30,120
Then, for each and every business rule,

246
00:12:30,120 --> 00:12:32,820
it'll start providing me
a lot of data elements,

247
00:12:32,820 --> 00:12:34,173
as you would see, right?

248
00:12:35,100 --> 00:12:37,200
It'll gimme the logic,
it'll gimme the input,

249
00:12:37,200 --> 00:12:40,290
it'll gimme the data
effect, it'll give me a lot

250
00:12:40,290 --> 00:12:43,440
of other processing elements
that I would need as part

251
00:12:43,440 --> 00:12:45,210
of relearning a certain code.

252
00:12:45,210 --> 00:12:48,660
Now, the most important
piece is how do you verify

253
00:12:48,660 --> 00:12:51,450
that the system has
done it very accurately.

254
00:12:51,450 --> 00:12:54,810
So, we've introduced,
what is an LLM as a judge,

255
00:12:54,810 --> 00:12:57,420
and it gives me a confidence
score for every rule

256
00:12:57,420 --> 00:12:59,820
that has been reverse engineered.

257
00:12:59,820 --> 00:13:03,840
As you see, it tells
me that one of the rule

258
00:13:03,840 --> 00:13:06,900
has got an average coverage, which is 85%.

259
00:13:06,900 --> 00:13:09,240
For us, 85% is average.

260
00:13:09,240 --> 00:13:11,250
Anything about 95% is something

261
00:13:11,250 --> 00:13:14,130
which we kind of counted as a better one.

262
00:13:14,130 --> 00:13:18,321
But it tells me there is a human

263
00:13:18,321 --> 00:13:21,240
in the loop interface over here.

264
00:13:21,240 --> 00:13:24,690
I can go and make, you
know, corrections over here.

265
00:13:24,690 --> 00:13:26,010
The first 100,000 lines

266
00:13:26,010 --> 00:13:28,020
of code typically need such corrections.

267
00:13:28,020 --> 00:13:30,630
But then for the next
million lines of code,

268
00:13:30,630 --> 00:13:31,710
it's an automated way

269
00:13:31,710 --> 00:13:35,670
where you get 95% accuracy
from the relearn agent.

270
00:13:35,670 --> 00:13:39,810
So, we've produced data
dictionary, we've produced

271
00:13:39,810 --> 00:13:42,450
business rules, and you've
seen the business rules

272
00:13:42,450 --> 00:13:43,740
verification as well.

273
00:13:43,740 --> 00:13:46,950
The third part is to convert
everything onto the Ontosphere,

274
00:13:46,950 --> 00:13:48,690
which is the knowledge graph

275
00:13:48,690 --> 00:13:51,723
and which is what, let me
demonstrate that to you.

276
00:13:55,350 --> 00:13:57,030
It's just showing you
some of the attributes

277
00:13:57,030 --> 00:13:58,353
from LLM as a judge.

278
00:14:00,270 --> 00:14:03,390
So, I'll go to the
knowledge graph right now.

279
00:14:03,390 --> 00:14:05,790
Actually, before I go
to the knowledge graph,

280
00:14:05,790 --> 00:14:09,030
it has produced information
also for the entire capability.

281
00:14:09,030 --> 00:14:12,300
So, you have a model to
take program by program

282
00:14:12,300 --> 00:14:14,610
or for the entire capability.

283
00:14:14,610 --> 00:14:18,000
Typically, if you have a
big job, which is running

284
00:14:18,000 --> 00:14:20,370
and multiple programs in
it, multiple processes,

285
00:14:20,370 --> 00:14:23,340
you would like to see such a view in terms

286
00:14:23,340 --> 00:14:25,443
of bringing the entire intelligence out.

287
00:14:26,700 --> 00:14:28,980
And finally, the third step in converting

288
00:14:28,980 --> 00:14:30,000
it to a knowledge graph.

289
00:14:30,000 --> 00:14:33,210
So, if you look at it, I've picked up one

290
00:14:33,210 --> 00:14:36,120
of the programs which
are reverse engineered,

291
00:14:36,120 --> 00:14:38,610
and it is showing me which
are the domain models

292
00:14:38,610 --> 00:14:40,290
it has been associated with it.

293
00:14:40,290 --> 00:14:44,700
I can double click on
functions, on the attributes,

294
00:14:44,700 --> 00:14:46,290
on the information.

295
00:14:46,290 --> 00:14:49,110
And, this is something that SME can use it

296
00:14:49,110 --> 00:14:52,020
for a verification purposes, right?

297
00:14:52,020 --> 00:14:55,440
So, essentially everything

298
00:14:55,440 --> 00:14:58,290
that you saw earlier was the first agent.

299
00:14:58,290 --> 00:15:01,320
This is where the information
comes to the second agent.

300
00:15:01,320 --> 00:15:05,160
All information is put up as
part of the business process.

301
00:15:05,160 --> 00:15:07,530
This was exactly the graph
that you saw earlier.

302
00:15:07,530 --> 00:15:12,530
It's coming now in a
business as a workflow item.

303
00:15:12,900 --> 00:15:16,440
And, I can leverage this, look at it

304
00:15:16,440 --> 00:15:18,870
and start reimagining my application,

305
00:15:18,870 --> 00:15:21,960
which is in the top section
out that you would see.

306
00:15:21,960 --> 00:15:24,330
I can even pick up business rule-wise,

307
00:15:24,330 --> 00:15:27,810
combine multiple business
rules and create a new rule.

308
00:15:27,810 --> 00:15:30,300
So, we've given those
capabilities out there.

309
00:15:30,300 --> 00:15:33,813
On the top, what you're seeing
is essentially nothing but,

310
00:15:35,070 --> 00:15:38,730
and overall agile remodeling
of the application.

311
00:15:38,730 --> 00:15:40,410
So, I'm completely re-imagining it.

312
00:15:40,410 --> 00:15:43,410
It's not a lift and
shift model that we have.

313
00:15:43,410 --> 00:15:47,110
I'm helping the BSA define

314
00:15:48,240 --> 00:15:51,390
the epics, features, user stories.

315
00:15:51,390 --> 00:15:53,310
And for every user story,

316
00:15:53,310 --> 00:15:56,010
there is an assessment done on
the quality of the user story

317
00:15:56,010 --> 00:15:57,960
through the invest score as well

318
00:15:57,960 --> 00:15:59,730
as you have a prompt mechanism

319
00:15:59,730 --> 00:16:03,690
by which you can generate
acceptance criterias,

320
00:16:03,690 --> 00:16:05,520
generate a lot more information.

321
00:16:05,520 --> 00:16:08,400
So, you see the Gherkin output over here.

322
00:16:08,400 --> 00:16:10,680
We actually create BDD
at this point in time.

323
00:16:10,680 --> 00:16:13,680
So, quality essentially starts
for us at this point in time.

324
00:16:17,040 --> 00:16:20,490
This is the prompt where
I can generate information

325
00:16:20,490 --> 00:16:22,020
in the form of...

326
00:16:22,020 --> 00:16:25,230
If it doesn't meet my invest criteria,

327
00:16:25,230 --> 00:16:28,530
I can further break it down
into many different rules.

328
00:16:28,530 --> 00:16:31,380
So, I'm going to the third
agent in the interest of time.

329
00:16:32,670 --> 00:16:34,140
From a third agent perspective,

330
00:16:34,140 --> 00:16:38,670
now, my important part is to
re-architect or redesign it.

331
00:16:38,670 --> 00:16:43,670
So, over here, what is needed
is three things, right?

332
00:16:43,950 --> 00:16:45,480
Actually, two things.

333
00:16:45,480 --> 00:16:47,280
One is a playbook about the steps

334
00:16:47,280 --> 00:16:49,590
which we have to do in terms
of reverse engineering.

335
00:16:49,590 --> 00:16:52,593
And, second is the
standards that I will use.

336
00:16:53,790 --> 00:16:55,470
We've given both as an input

337
00:16:55,470 --> 00:16:57,060
and I'll just show you how do we do this.

338
00:16:57,060 --> 00:16:59,820
But it's started to
give me all information,

339
00:16:59,820 --> 00:17:04,820
whether it's a logical model
or it's a physical model

340
00:17:05,670 --> 00:17:07,260
or it's a sequence diagram

341
00:17:07,260 --> 00:17:10,470
or it's an observability pattern
that has to be leveraged.

342
00:17:10,470 --> 00:17:12,240
Everything which has been fed

343
00:17:12,240 --> 00:17:15,810
as a standard is being
brought up by the agent,

344
00:17:15,810 --> 00:17:19,923
which is Raina the third agent
that Anup spoke about, right?

345
00:17:22,980 --> 00:17:26,940
So, the job of this particular agent

346
00:17:26,940 --> 00:17:29,220
is to build the entire context

347
00:17:29,220 --> 00:17:30,900
and make a prompt ready

348
00:17:30,900 --> 00:17:34,200
for my next agent from a
development perspective.

349
00:17:34,200 --> 00:17:37,140
So, if you look at it,
I'm generating right now,

350
00:17:37,140 --> 00:17:38,880
I've fed the information that I need

351
00:17:38,880 --> 00:17:42,570
to generate it using a
Java Flink model, okay?

352
00:17:42,570 --> 00:17:45,723
I've just shown the step how
fast you can actually do it.

353
00:17:48,760 --> 00:17:50,223
In the interest of time.

354
00:17:52,200 --> 00:17:56,380
So, I'm trying to create a new design

355
00:17:57,270 --> 00:18:00,927
and I'm giving to it the playbook as well

356
00:18:05,190 --> 00:18:06,870
as the standards which have to be used.

357
00:18:06,870 --> 00:18:09,180
I'm using data engineering architecture.

358
00:18:09,180 --> 00:18:10,950
I'm giving the playbook
for these standards,

359
00:18:10,950 --> 00:18:12,750
which is the Flink one.

360
00:18:12,750 --> 00:18:16,140
And, I'm generating the entire context.

361
00:18:16,140 --> 00:18:19,410
So, all information
pretty much gets generated

362
00:18:19,410 --> 00:18:21,240
from that perspective.

363
00:18:21,240 --> 00:18:23,430
Now, I'm going to my fourth agent,

364
00:18:23,430 --> 00:18:25,380
which is the code generation agent.

365
00:18:25,380 --> 00:18:28,500
Now, you know, this general,

366
00:18:28,500 --> 00:18:30,300
this gets the entire information

367
00:18:30,300 --> 00:18:32,490
and generates code from a Java Flink

368
00:18:32,490 --> 00:18:34,260
architecture perspective.

369
00:18:34,260 --> 00:18:38,160
What you essentially see is an example

370
00:18:38,160 --> 00:18:41,670
of how this ran on the GPU
as well as it ran on the CPU.

371
00:18:41,670 --> 00:18:44,910
So, there's a big distinct
difference in terms of the timing

372
00:18:44,910 --> 00:18:46,800
that you would see if you run.

373
00:18:46,800 --> 00:18:49,200
We've modeled it for
multiple sets of traits.

374
00:18:49,200 --> 00:18:52,620
So, example, if I go from 20,000
trait to a 100,000 traits,

375
00:18:52,620 --> 00:18:54,780
what will be the cost
from a CPU perspective

376
00:18:54,780 --> 00:18:56,490
versus the GPU perspective?

377
00:18:56,490 --> 00:18:59,940
And we help them to make a call from a job

378
00:18:59,940 --> 00:19:01,173
by job perspective.

379
00:19:04,050 --> 00:19:06,000
So, that was one I had to show.

380
00:19:06,000 --> 00:19:08,433
Let's go back to the deck.

381
00:19:09,330 --> 00:19:12,400
So, guys, this is what we have seen so far

382
00:19:13,290 --> 00:19:15,810
from a results perspective.

383
00:19:15,810 --> 00:19:20,810
We've achieved this as a
typical modernization program

384
00:19:21,090 --> 00:19:23,880
with about 50 million lines of code takes

385
00:19:23,880 --> 00:19:25,563
around seven years.

386
00:19:26,940 --> 00:19:31,320
Okay? We've achieved
this kind of progress.

387
00:19:31,320 --> 00:19:35,200
That infinity at the
end, guess what it is.

388
00:19:35,200 --> 00:19:40,200
It is the fact that your entire
intelligence moves into data

389
00:19:40,620 --> 00:19:42,570
and lives with you forever.

390
00:19:42,570 --> 00:19:46,110
You'll never have a legacy,
right? That's the thing.

391
00:19:46,110 --> 00:19:49,230
And this is kind of,
we kind of put together

392
00:19:49,230 --> 00:19:51,120
how are we different from anybody else?

393
00:19:51,120 --> 00:19:54,450
There are a lot of people
you'll get who move A to B,

394
00:19:54,450 --> 00:19:57,570
but very few who will extract intelligence

395
00:19:57,570 --> 00:19:59,430
and give you a state where you'll never

396
00:19:59,430 --> 00:20:00,530
have a legacy anymore.

397
00:20:02,520 --> 00:20:04,530
So, that's what we wanted to cover today-

398
00:20:04,530 --> 00:20:07,920
- And, our agents have
actually run on DABstar,

399
00:20:07,920 --> 00:20:11,130
so, you know, they scale
up pretty high on that.

400
00:20:11,130 --> 00:20:12,690
So, we've constantly measuring it,

401
00:20:12,690 --> 00:20:14,613
monitoring it from that perspective.

402
00:20:15,600 --> 00:20:16,433
Thank you guys.
- Thank you, guys.

403
00:20:16,433 --> 00:20:17,730
- That's all we had.

404
00:20:17,730 --> 00:20:19,410
The time is up. Thank you.

405
00:20:19,410 --> 00:20:20,410
- Thank you so much.

