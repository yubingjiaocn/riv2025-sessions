# AWS re:Invent 2025 - NVIDIA与AWS AI代理设计会议总结

## 会议概述

本次会议重点介绍了NVIDIA与AWS在生成式AI代理设计方面的合作成果。演讲者详细阐述了如何利用NVIDIA的技术栈和AWS的云服务来构建、优化和部署企业级AI代理系统。会议强调了AI代理不是单一的整体系统,而是由多个高度专注的智能模型组成的生态系统,这些模型通过编排器协调工作。

演讲涵盖了从2023年1月大语言模型(LLM)普及开始的生成式AI发展历程,经历了RAG(检索增强生成)、参数高效微调,最终演进到当前的AI代理阶段。NVIDIA提供了完整的工具链,包括Nemo神经模块、Nemo代理工具包、NVIDIA推理微服务(NIMs)和蓝图(Blueprints),帮助开发者快速将AI代理从概念验证推向生产环境。

会议特别强调了在扩展AI代理系统时面临的挑战,包括数据治理、性能优化、安全认证等问题,并展示了如何通过模块化设计和最佳实践来避免这些陷阱。NVIDIA与AWS的深度集成使得开发者可以在AWS的各种计算服务上轻松部署优化的AI代理解决方案。

## 详细时间线

00:00:00 - 会议开场与合作介绍
- 欢迎参会者,介绍NVIDIA与AWS的强大合作关系
- 强调双方在AI代理设计领域的长期协作

00:00:30 - 生成式AI发展历程
- 2023年1月:生成式AI开始被广泛采用,用户开始使用自然语言与高智能科学模型交互
- RAG技术的引入,提高通用模型的有效性
- 参数高效微调(PEFT)的出现
- AI代理成为自然演进的下一步

00:01:15 - AI代理的本质
- 代理是多个超聚焦智能设计模型子集的编排系统
- 强调代理应被视为庞大的生态系统而非单一整体
- 代理会产生指数级增长的token数量

00:01:45 - NVIDIA术语解释
- Nemo:神经模块,十多年前就开始模块化机器学习流程
- Nemo代理工具包:专门用于代理设计的Nemo模块
- NVIDIA推理微服务(NIMs):优化的容器,包含前沿模型或Hugging Face模型
- 蓝图(Blueprints):Helm图表,用于在Kubernetes上部署多个容器

00:02:30 - 生产部署的重要性
- 强调加速计算和规模化的重要性
- 警告在扩展过程中问题会被放大
- 目标是帮助开发者顺利进入生产环境

00:03:00 - AI代理的组成要素
- 工具使用和计算机使用
- 内存管理(代理使用时内存会快速耗尽)
- 多代理交互形成生态系统
- 推理过程会创建大量token

00:03:45 - 生产环境的复杂性
- 遗留代码和现有流程的整合
- 异构数据源的处理
- 可靠性能的实现难度
- 需要采用防御性编程和数据科学方法

00:04:30 - 避免扩展陷阱
- 扩展过程中问题会加深和扩大
- 数据治理、性能分析和安全性的挑战
- 人类和计算机都需要快速认证
- 需要在进入生产前考虑所有问题

00:05:00 - 简化聊天机器人架构展示
- 展示机器学习流程的复杂性
- 从PC扩展到生产时,每个组件会扩展10倍
- 问题会被放大10倍

00:05:45 - Nemo蓝图的优势
- 大部分免费,可在GitHub上使用
- 开发者许可证只需登录NGC容器注册表
- 白色框代表不同的Nemo处理流程

00:06:15 - 技术栈架构
- 底层:GPU和加速计算
- 加速库层:免费提供,无需学习新技能
- Nemo代理工具包:模块化的代理流程加速
- 顶层:不同类型的蓝图(Kubernetes Helm图表设计)
- 支持即插即用和与其他框架的集成

00:07:00 - 科学方法的重要性
- 强调机器学习和AI仍然是与计算机交互的科学过程
- 需要人类创造性地识别和修复问题
- 计算机无法自行识别错误

00:07:30 - 数据飞轮概念
- 数据管理:策划、定制、评估
- 护栏设置:防止幻觉和数据漂移
- 创建NIM容器:获得可靠处理的时间快照
- 容器易于更新和即插即用

00:08:30 - 代理设计的包含内容
- 框架无关:支持LangChain、Semantic Kernel等
- YAML配置:易于修改,可通过Python交互
- 安全性和评估的重要性
- 代理生态系统连接器:MCP、自定义插件等

00:09:15 - 性能指标展示
- 代码行数减少57%:节省数周开发时间
- 更高吞吐量:更快的token化处理
- 响应时间提升:医疗虚拟助手案例显示近2倍速度提升

00:10:00 - Nemo Curator性能
- GPU数据处理比CPU快16倍
- 加速数据清理和准备过程
- 大规模数据集的去重处理
- 支持Python API,随处开发

00:11:00 - Nemo定制化的重要性
- 在专有数据上定制模型至关重要
- 通用模型无法在超特定场景下表现良好
- 代理旨在减少通用性,实现超特定化

00:11:45 - 定制化架构
- 参数高效微调(PEFT)或低秩适应(LoRA)
- 在通用模型上创建超特定化子集
- 只需输入"LoRA"即可进行微调

00:12:30 - 迭代流程
- 强调整个过程是迭代的
- 不再是一次性运行就完成的时代
- 持续交互和优化的重要性

00:13:00 - NVIDIA与AWS服务集成
- NVIDIA框架和SDK已集成到AWS服务中
- 例如OpenSearch的最新集成
- 可在AWS堆栈中广泛使用

00:13:30 - Nemo代理工具包总结
- 模块化设计,开箱即用
- 无需担心问题修复位置
- 专注于实现流程而非编写底层代码

00:14:00 - 框架和部署选项
- 支持多种框架
- 可在AWS计算堆栈的任何位置部署
- 推荐使用EKS进行规模化部署
- SageMaker适合开发和设计
- 在AWS Marketplace上提供服务,包括商业和IC Marketplace

00:15:00 - NVIDIA展台介绍(1022号)
- 大规模环境重建演示
- 计算机视觉和VLM展示
- 边缘到云的AI代理愿景
- 实际工程师现场解答问题

00:16:00 - 资源链接
- AWS Marketplace:查看NVIDIA产品列表
- AWS-NVIDIA技术博客:实现教程(如在SageMaker上使用Nemo微调)
- build.nvidia.com:生成式AI游乐场,包含数百个NIM和蓝图

00:17:00 - API密钥和实现简便性
- 使用OpenAI协议调用生成式AI模型
- 只需更改一行代码指定模型
- 第二行粘贴API密钥
- 微调只需添加"training type: LoRA"
- 总共三行代码即可完成

00:18:00 - 会议结束
- 感谢参会者
- 鼓励参观展台和探索资源