# AWS re:Invent 2025 Fireworks AI 技术分享会总结

## 会议概述 (Session Overview)

本次技术分享会由 Fireworks AI 公司代表主讲，重点介绍了该公司在 AI 代理（Agents）领域的技术平台和解决方案。演讲者强调 2025 年是"代理年"，并预测未来十年都将是代理技术的黄金时期。Fireworks AI 作为开源模型推理和定制化引擎，帮助众多企业在生产环境中构建各类 AI 代理，包括编程代理、文档处理代理、销售营销代理、招聘代理和客服代理等。

演讲者详细阐述了构建生产级 AI 代理面临的技术挑战，包括模型选择、延迟优化、质量保证、成本控制和基础设施复杂性等问题。Fireworks AI 通过其专有的 FireOptimizer 技术和全栈优化方案，为客户提供一站式 AI 平台服务，实现了在 AWS 基础设施上的高性能部署。

## 详细时间线与关键要点 (Detailed Timeline)

### 0:00-3:00 开场与行业背景
- 强调 2025 年是 AI 代理元年，未来十年将持续发展
- 介绍 Fireworks AI 服务的多样化代理应用场景
- 涵盖零售、保险、金融、教育、生命科学、安全等各行业

### 3:00-6:00 构建代理的技术挑战
- 模型选择难题：开源 vs 闭源，小模型（8B）vs 大模型（万亿参数）
- 性能要求：搜索场景需要 300 毫秒以下的 LLM 响应时间
- 成本控制：闭源提供商成本快速上升的问题
- 基础设施复杂性：EKS/ECS 集群部署和多 GPU 节点管理

### 6:00-9:00 Fireworks 平台架构介绍
- 开源推理和定制化引擎，支持 DeepSeek、Kimi、Llama、Qwen 等模型
- 兼容 OpenAI SDK，仅需两行代码即可迁移
- FireOptimizer 定制化引擎：工作负载优化和模型微调
- 全球多区域虚拟云基础设施，基于 AWS、Nvidia 和 AMD 合作

### 9:00-12:00 技术深度解析
- 工作负载优化：84,000 个参数空间的自动化配置
- 全栈优化：从 CUDA 内核到模型层的完整优化
- FireAttention 自定义 CUDA 内核
- 推测解码技术：70% 以上接受率，实现 100 毫秒以下延迟

### 12:00-15:00 微调技术与案例
- 监督微调：意图分类、情感分析、产品目录清理
- 强化学习微调（RFT）：通过评估函数实现模型持续改进
- Genspark 合作案例：质量提升 20%，同时降低延迟和成本
- 数据飞轮机制：快速迭代和模型更新

### 15:00-17:00 AWS 集成与客户成功案例
- 基于 AWS EC2、ECS、EKS 的完整部署方案
- 多种部署选项：SaaS 平台到完全气隙环境
- 大型杂货配送平台：日处理 200-300 万查询，延迟 300 毫秒以下
- Notion AI：4 倍延迟降低，服务 1 亿+ 用户
- DoorDash：VLM 模型速度提升 3 倍，成本降低 10%