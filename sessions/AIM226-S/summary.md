# AWS re:Invent 2025 分会场总结：前沿 AI 模型的高性能推理

## 会议概述

本次分会场由来自 Base 10 的 Philip 主讲，主题聚焦于前沿 AI 模型的高性能推理工程。Base 10 是一家推理服务提供商，在 AWS 市场上提供开源、微调和定制模型的推理服务，其基础设施专为生产环境而设计。

演讲深入探讨了推理工程（Inference Engineering）的核心概念，强调这是一个在生产环境中运行 AI 模型的系统化工程实践。Philip 指出，随着 Hugging Face 上开源模型数量从几年前的数万个激增至超过 200 万个，且这些模型在质量上已达到前沿水平（如 DeepSeek R1、Qwen K2 等思维模型），推理优化变得至关重要。演讲通过多个实际客户案例（如 Open Evidence、Zed、Superhuman 等）展示了如何通过运行时优化和基础设施管理的双重策略，实现生产级别的高性能推理服务。

推理工程的三大核心原则包括：优化需要明确的约束条件、规模化能够解锁更多性能优化技术、系统需要保持动态调整能力以适应实时流量变化。Philip 强调，成功的推理服务不仅需要在单个 GPU 层面实现极致性能，还需要具备跨区域、跨集群的水平扩展能力，并提供 99.99% 以上的可靠性保证。

## 详细时间线与关键要点

### 开场与背景介绍
[00:00 - 01:30] - 演讲者 Philip 介绍自己来自 Base 10，将讨论前沿 AI 模型的高性能推理，涵盖推理工程、开源基础模型的崛起、推理栈组件、运行时性能优化、基础设施以及生产环境应用

[01:30 - 02:45] - Base 10 公司介绍：专注于推理服务，在 AWS 市场提供开源、微调和定制模型服务，基础设施专为生产环境构建

[02:45 - 04:00] - 推理的三个核心维度：运行时层面的性能优化（最大化 GPU 利用率、提高吞吐量、降低延迟）、基础设施层面的水平扩展能力（支持 2、10、100、1000 个副本跨集群跨区域部署）、优秀的开发者体验和专家级技术支持

### 开源模型的崛起
[04:00 - 05:30] - 开源模型现状：Hugging Face 上已有超过 200 万个开源模型，相比四五年前的数万个大幅增长；不仅数量增加，质量也达到前沿水平

[05:30 - 07:00] - 开源模型质量突破：Qwen K2 思维模型、DeepSeek R1（1月发布）等开源模型已跨越与闭源模型的差距，实现前沿智能水平

[07:00 - 08:15] - 多模态开源模型：语音领域（自动语音识别、文本转语音、说话人分离）、图像生成（Flux、Stable Diffusion）、视频生成与处理、嵌入模型（支持文本和多模态数据）

### 推理工程的核心原则
[08:15 - 10:00] - 推理工程定义：在生产环境中运行 AI 模型的过程，遵循三大原则：1) 优化需要约束条件（需要明确优化目标）；2) 规模化解锁更多性能技术（如大规模并行、解耦等需要足够流量支撑）；3) 保持动态性（系统需实时调整以适应流量变化，而非静态配置）

[10:00 - 12:00] - NFL 类比：用 NFL 球员的例子说明推理优化的平衡艺术——NFL 球员不是最大（相扑选手）、最快（短跑运动员）或最强（举重冠军），但他们在多个维度上达到了适合比赛的最佳平衡；推理优化同理，关键是找到适合特定应用需求的能力组合，而非单纯追求某个基准测试的第一名

[12:00 - 13:30] - 客户案例 - Open Evidence：为医生提供最新医疗信息的聊天应用，是一家大规模成功的 AI 医疗初创公司；其 CTO 表示每周通过 Base 10 处理数十亿次定制和微调 LLM 调用，为美国几乎所有主要医疗机构的医疗服务提供者提供高风险医疗信息服务

### 推理栈：运行时层
[13:30 - 14:30] - 推理栈的两大组成部分：运行时组件（确保单个 GPU 性能最优）和基础设施组件（实现跨多个副本的扩展）

[14:30 - 16:30] - 运行时优化技术概述：推理运行时本质上是应用研究，需要将学术论文（如 NeurIPS 等会议）中的创新想法应用到生产环境

[16:30 - 19:00] - 量化技术（Quantization）：从 16 位浮点数降至 8 位或 4 位，以访问更强大的张量核心并更好地利用内存带宽；Base 10 在 Nvidia 展台分享了关于 NVFP4（Hopper/Blackwell 的新微缩放数据格式）的应用；量化策略需精细化，不是简单地将整个模型量化，而是选择性地量化权重、激活值、KV 缓存（可量化为 FP8），同时保持注意力机制不变；甚至可以只量化中间层，保留输入输出层

[19:00 - 21:30] - 推测解码（Speculative Decoding）：通过不同算法生成草稿令牌，在内存受限的解码阶段提高每秒令牌数；每次模型前向传播可生成多个令牌；主要算法包括 Eagle 3（使用专门训练的模型，从目标模型的隐藏状态生成草稿令牌）和前瞻解码/N-gram 推测（特别适用于代码补全等词汇受限场景）

[21:30 - 23:30] - 缓存技术：KV 缓存感知路由确保高缓存命中率；客户案例 - Zed IDE：通过 Base 10 推理栈（包括大量 KV 缓存重用）实现端到端代码补全速度提升 2 倍，P90 延迟降低 45%，系统吞吐量提高 3.5 倍

[23:30 - 25:30] - 并行技术：随着混合专家（Mixture of Experts）成为大规模语言模型的主流架构，需要仔细平衡张量并行和专家并行，在延迟和吞吐量之间做出正确权衡；对于视频生成等模态（即使是 DGX B200 系统也面临挑战），需要上下文并行技术将注意力计算分散到 8 个 GPU 上，高效利用所有计算资源

[25:30 - 27:00] - 解耦技术（Disaggregation）：将预填充（prefill）和解码（decode）分离到独立扩展的工作节点上；允许采用不同的内核策略和运行时，针对计算密集型预填充或内存带宽受限解码进行专门优化；Dynamo 也支持此技术

### 推理栈：基础设施层
[27:00 - 28:00] - 基础设施的重要性：即使实现了世界级的模型性能技术，没有匹配的基础设施也无法满足生产推理服务需求

[28:00 - 31:00] - 自动扩展（Autoscaling）问题：许多公司（特别是从大型训练集群起步的公司）拥有固定的 GPU 容量；固定容量的问题在于流量波动——商业应用在工作时间流量高，夜间和周末低；消费应用可能因 TikTok 病毒式传播一夜获得百万用户，其他时候则较平静；静态容量无法匹配流量：流量低时浪费支出，流量高时即使有所有优化也会吞吐量不足、无法满足 SLA

[31:00 - 33:30] - 多集群容量管理（Multi-cluster Capacity Management）：Base 10 基于流量做自动扩展决策；与许多仅依赖单区域容量的推理服务不同（如只在 US East 1），多集群容量管理可跨多个区域、多个独立集群调配计算资源；通过单一全局控制平面将其视为统一资源，例如 10 个副本可以在一个实例上调度 8 个，另一个上调度 2 个；优势包括跨区域主动-主动可靠性、访问更多容量和弹性、为全球分布式应用提供更接近终端用户的地理位置

[33:30 - 35:30] - 端到端延迟的重要性：无论模型服务器多快，如果网络延迟高（如从新加坡发送请求到加州再返回）或队列深度高（如只有 10 个 GPU 但有 20 个 GPU 的流量），端到端推理时间都会受影响；因此需要同时进行运行时和基础设施优化以确保出色的端到端延迟

[35:30 - 36:30] - 客户案例 - Latent：一家制药搜索公司，表示 Base 10 通过多集群策略和自动扩展能力，在实现高可靠性推理方面为他们节省了大量压力和开发时间

### 多模态推理
[36:30 - 38:00] - 超越大语言模型：虽然 LLM 和 AI 理应同义，但还有许多其他开源模型模态，包括图像生成、视频生成、嵌入模型、文本转语音、语音转文本以及各种新兴模态

[38:00 - 40:30] - 嵌入推理（Embedding Inference）：嵌入模型将文本作为输入，输出编码文本语义含义的向量；也有多模态嵌入模型可处理图像或视频；架构上与大语言模型非常相似——当前前沿嵌入模型（如 Embedding Gemma、Qwen Embed）都是基于开源 LLM 构建的；因此可以使用相同的运行时，只需构建周围系统

[40:30 - 43:00] - 嵌入推理架构：在模型服务器前端处理请求，使用多工作节点分词器处理通常数百或数千个单独句子或输入（这些输入可能批量打包在单个嵌入模型推理请求中），放入批处理管理器排队，利用运行时提供的逐令牌动态批处理（continuous batching）机制；通过这种方式可在全新模态中获得同样高质量的推理服务

[43:00 - 44:00] - 多模态支持：Base 10 已为嵌入、语音转文本、文本转语音、图像生成、视频生成等六大主要模型模态实现了这种架构

[44:00 - 45:30] - 客户案例 - Superhuman：最近被 Grammarly 收购并重新品牌化为 Superhuman 的电子邮件应用；原 Superhuman 的 CTO Loïck 表示，通过 Base 10 嵌入推理，他们在支持应用关键功能的大量微调嵌入模型上将 P95 延迟降低了 80%

[45:30 - 47:00] - P90/P95/P99 延迟的重要性：Philip 强调最自豪的是这些高百分位延迟的改善，因为它们展示了运行时和基础设施双重优化的影响——不仅要快，还要可靠地快

### 总结与关键要点
[47:00 - 49:30] - 生产推理的四大要素：
  1. 世界级性能：在首令牌时间（TTFT）、每秒令牌数（TPS）或其他对终端用户重要的指标上实现最先进的性能
  2. 高可靠性基础设施：提供 99.99% 或更高的可靠性，以支持医疗等关键任务领域的应用
  3. 跨区域扩展能力：能够跨区域甚至跨不同 VPC 扩展 GPU 容量，以应对 AI 应用的快速增长（客户经常在一年内实现多倍复合增长）
  4. 全模型支持：不仅支持一个模型或一种模态，而是支持 Hugging Face 上 200 万个开源模型中的任何一个、任何微调模型、任何定制模型，以满足企业在生产中提供差异化价值的需求

[49:30 - 50:30] - 结束语：Philip 感谢听众，邀请大家访问 1632 号展位（Reddus 展位后方），那里有"人工智能 T 恤"、团队成员演示，欢迎任何关于演讲内容的问题

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


核心技术要点总结：
- 量化、推测解码、缓存、并行、解耦是五大主要运行时优化技术
- 自动扩展和多集群容量管理是基础设施层的关键能力
- 成功案例涵盖医疗（Open Evidence）、开发工具（Zed）、生产力应用（Superhuman）、制药（Latent）等多个领域
- 推理工程强调运行时性能和基础设施可靠性的协同优化