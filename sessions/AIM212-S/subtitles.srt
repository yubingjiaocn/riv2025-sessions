1
00:00:00,630 --> 00:00:01,636
- Hey, everybody.

2
00:00:01,636 --> 00:00:02,469
(audience chattering)

3
00:00:02,469 --> 00:00:04,440
Thanks for joining in.

4
00:00:04,440 --> 00:00:06,150
What we're gonna talk about
today is probably something

5
00:00:06,150 --> 00:00:08,910
that is on the minds of
many people around here.

6
00:00:08,910 --> 00:00:12,660
We're gonna talk about LLMs
and observability together,

7
00:00:12,660 --> 00:00:14,730
which is probably a very interesting topic

8
00:00:14,730 --> 00:00:17,040
for a lot of you guys moving into Bedrock,

9
00:00:17,040 --> 00:00:19,683
using OpenAI, using
Anthropic, using whatever,

10
00:00:20,550 --> 00:00:22,983
and we're gonna focus
on how we do it right.

11
00:00:23,850 --> 00:00:26,730
So let's start with why LLM
observability even matters.

12
00:00:26,730 --> 00:00:30,810
I think that probably 99% of
you guys use LLM to some form,

13
00:00:30,810 --> 00:00:32,460
but why does observability

14
00:00:32,460 --> 00:00:34,950
and kind of monitoring
them, why does it matter?

15
00:00:34,950 --> 00:00:37,170
We started, you know, just
maybe a couple years ago,

16
00:00:37,170 --> 00:00:40,470
we like simple flows of an agent

17
00:00:40,470 --> 00:00:42,510
or a model that I query, get a response,

18
00:00:42,510 --> 00:00:44,160
everything was simple,

19
00:00:44,160 --> 00:00:46,650
and now we have a ton
of complication, right?

20
00:00:46,650 --> 00:00:49,620
Multi-turn agents, multiple
steps with, you know,

21
00:00:49,620 --> 00:00:52,530
different functionalities,
kind of using different agent,

22
00:00:52,530 --> 00:00:55,170
different models to get
into a final outcome.

23
00:00:55,170 --> 00:00:56,700
We have retrieval pipelines, right?

24
00:00:56,700 --> 00:01:00,330
Like Rug that enrich that embed,

25
00:01:00,330 --> 00:01:01,680
that create a data pipeline

26
00:01:01,680 --> 00:01:03,870
up to the actual model at the end.

27
00:01:03,870 --> 00:01:06,720
Any silent failure on any of these

28
00:01:06,720 --> 00:01:08,460
will screw up the entire process, right?

29
00:01:08,460 --> 00:01:09,875
We have tool augmentation.

30
00:01:09,875 --> 00:01:14,310
Our workflows now use scripts and APIs

31
00:01:14,310 --> 00:01:17,700
and different stuff that
basically kind of comprehends

32
00:01:17,700 --> 00:01:19,980
the entire workflow,
what we're trying to do.

33
00:01:19,980 --> 00:01:22,170
Something goes wrong,
something is high latency,

34
00:01:22,170 --> 00:01:25,320
and the entire workflow
study doesn't work, right?

35
00:01:25,320 --> 00:01:27,570
Now, you wanna observe a
lot of different things

36
00:01:27,570 --> 00:01:29,010
that you're already thinking about

37
00:01:29,010 --> 00:01:30,510
if you're using LLMs today, right?

38
00:01:30,510 --> 00:01:33,630
The most common things we hear
about is token usage, right?

39
00:01:33,630 --> 00:01:35,880
All of these pricing models
are connected somehow

40
00:01:35,880 --> 00:01:39,420
to token usage, to kind
of on demand capacity

41
00:01:39,420 --> 00:01:43,530
that you're querying, one
million tokens of contacts,

42
00:01:43,530 --> 00:01:44,490
it just keeps growing, right?

43
00:01:44,490 --> 00:01:45,330
You wanna monitor that.

44
00:01:45,330 --> 00:01:47,520
You wanna know how much
your users are using

45
00:01:47,520 --> 00:01:48,810
and you have no control of that

46
00:01:48,810 --> 00:01:51,120
because eventually your
developers are the ones

47
00:01:51,120 --> 00:01:52,710
that making a choice of how many tokens

48
00:01:52,710 --> 00:01:54,540
is being inputted and outputted, right?

49
00:01:54,540 --> 00:01:55,773
Response latency.

50
00:01:56,670 --> 00:01:58,500
AI has gotten into production

51
00:01:58,500 --> 00:02:01,110
quicker than we would imagine, right?

52
00:02:01,110 --> 00:02:04,170
The value was so high that we
just rushed into production.

53
00:02:04,170 --> 00:02:06,930
It eventually, your APIs
depends on it today, right?

54
00:02:06,930 --> 00:02:08,847
If you're serving a customer-facing API

55
00:02:08,847 --> 00:02:12,990
and using AI in the middle,
any latency there can impact

56
00:02:12,990 --> 00:02:14,730
hundreds of percent of
your latency suddenly

57
00:02:14,730 --> 00:02:16,470
just because you wanted that value.

58
00:02:16,470 --> 00:02:18,030
Same for error rates, right?

59
00:02:18,030 --> 00:02:20,430
If a model suddenly replies with an error,

60
00:02:20,430 --> 00:02:22,410
which happens due to structure issues

61
00:02:22,410 --> 00:02:24,360
or to anything that can happen,

62
00:02:24,360 --> 00:02:25,410
and you don't know about it,

63
00:02:25,410 --> 00:02:27,570
your API can suddenly go wrong.

64
00:02:27,570 --> 00:02:29,910
Before it was all a
box you can control on,

65
00:02:29,910 --> 00:02:32,610
now it's getting more complex.

66
00:02:32,610 --> 00:02:35,280
And the fourth thing, and
maybe the most sensitive one

67
00:02:35,280 --> 00:02:36,990
is LLM responses, right?

68
00:02:36,990 --> 00:02:38,400
How do I detect hallucinations?

69
00:02:38,400 --> 00:02:41,520
How do I know that the
response is even correct?

70
00:02:41,520 --> 00:02:44,100
Any of you that tried
to query an LLM 50 times

71
00:02:44,100 --> 00:02:46,740
for the same thing got 50
different results, right?

72
00:02:46,740 --> 00:02:48,724
And eventually we want to figure out

73
00:02:48,724 --> 00:02:52,110
how to kind of monitor and
figure out what's going on.

74
00:02:52,110 --> 00:02:55,920
Now, the challenges are that the speed

75
00:02:55,920 --> 00:02:57,630
in which your stack is changing

76
00:02:57,630 --> 00:03:00,210
has never been as fast as now, right?

77
00:03:00,210 --> 00:03:04,650
We've implemented AI into
production, skipping security,

78
00:03:04,650 --> 00:03:08,010
skipping privacy, skipping
anything in your companies

79
00:03:08,010 --> 00:03:11,070
to get value into production
and into your customers, right?

80
00:03:11,070 --> 00:03:13,620
And anything that kind of prohibited

81
00:03:13,620 --> 00:03:15,870
that value was put aside,

82
00:03:15,870 --> 00:03:18,150
but right now you're
introducing a new tool

83
00:03:18,150 --> 00:03:21,510
every month, right, from Cloud Code

84
00:03:21,510 --> 00:03:25,110
to anything that is creating
even code into your production

85
00:03:25,110 --> 00:03:28,100
to actual AI used in your
workflows to get value

86
00:03:28,100 --> 00:03:29,790
to your customers, right?

87
00:03:29,790 --> 00:03:32,790
The rating with this is
changing is higher than the rate

88
00:03:32,790 --> 00:03:35,550
that you can observe,
instrument your code,

89
00:03:35,550 --> 00:03:37,320
instrument your application to even track

90
00:03:37,320 --> 00:03:38,790
and monitor what you're doing.

91
00:03:38,790 --> 00:03:42,240
I'm sure that 50% of the
companies here are not even sure

92
00:03:42,240 --> 00:03:44,190
what models you're using
right now in production

93
00:03:44,190 --> 00:03:46,560
just because of the
pace things are changing

94
00:03:46,560 --> 00:03:47,910
and things are examined.

95
00:03:47,910 --> 00:03:50,340
The second thing is the
sensitivity of data.

96
00:03:50,340 --> 00:03:53,760
AI eventually creates a
very interesting situation

97
00:03:53,760 --> 00:03:57,300
where user input can be
incredibly sensitive, right?

98
00:03:57,300 --> 00:04:00,240
It can be not even something
that depends on you.

99
00:04:00,240 --> 00:04:02,910
What happens if your
application exposes a chatbot

100
00:04:02,910 --> 00:04:04,350
to your end user?

101
00:04:04,350 --> 00:04:07,020
They can input a very
sensitive information

102
00:04:07,020 --> 00:04:09,510
about their customers, about themselves,

103
00:04:09,510 --> 00:04:11,670
that you might never
wanted in your application,

104
00:04:11,670 --> 00:04:14,310
but now you can control and
now you're responsible for.

105
00:04:14,310 --> 00:04:16,440
So who gets to upload what?

106
00:04:16,440 --> 00:04:17,580
Where the data goes?

107
00:04:17,580 --> 00:04:18,413
Who stores it?

108
00:04:18,413 --> 00:04:20,340
Which third party am I relying on?

109
00:04:20,340 --> 00:04:21,720
That's a big part of the challenges

110
00:04:21,720 --> 00:04:24,050
around LLM observability.

111
00:04:24,050 --> 00:04:26,520
Groundcover does things very different

112
00:04:26,520 --> 00:04:28,080
around observability in general,

113
00:04:28,080 --> 00:04:30,780
but it has a lot to do with how

114
00:04:30,780 --> 00:04:33,360
people should think about LLM.

115
00:04:33,360 --> 00:04:35,520
Most of the people, most of you all

116
00:04:35,520 --> 00:04:38,070
is currently using Bedrock, for example,

117
00:04:38,070 --> 00:04:40,050
with AWS, has similar concerns.

118
00:04:40,050 --> 00:04:42,330
The first thing that
groundcover does differently

119
00:04:42,330 --> 00:04:44,820
is that we use a
bring-your-own-cloud architecture.

120
00:04:44,820 --> 00:04:47,460
Basically, instead of
being a simple SaaS model

121
00:04:47,460 --> 00:04:51,450
where we store the data of
all the monitoring telemetry

122
00:04:51,450 --> 00:04:53,460
that you send out and present it to you,

123
00:04:53,460 --> 00:04:56,250
we deploy and manage an
observability backend

124
00:04:56,250 --> 00:04:59,220
on your cloud premises
in your AWS account.

125
00:04:59,220 --> 00:05:02,250
It's fully managed, it's fully hands-off

126
00:05:02,250 --> 00:05:03,180
like a SUS experience,

127
00:05:03,180 --> 00:05:06,180
but you get the value of
data privacy, data residency,

128
00:05:06,180 --> 00:05:08,370
and full control of what's being stored

129
00:05:08,370 --> 00:05:10,440
without us processing it, storing it,

130
00:05:10,440 --> 00:05:12,810
or removing it from your environment.

131
00:05:12,810 --> 00:05:15,210
We also use eBPF, which we're
gonna talk about in a second,

132
00:05:15,210 --> 00:05:17,220
which is a kernel-level visibility

133
00:05:17,220 --> 00:05:19,260
to instrument your application

134
00:05:19,260 --> 00:05:21,240
at the speed you write them in, right?

135
00:05:21,240 --> 00:05:23,880
If you introduce a Bedrock
application tomorrow,

136
00:05:23,880 --> 00:05:25,380
it's instantly instrumented

137
00:05:25,380 --> 00:05:27,600
with eBPF without you doing anything.

138
00:05:27,600 --> 00:05:29,670
And again, the pace in
which things are moving

139
00:05:29,670 --> 00:05:31,320
is higher than what you can predict.

140
00:05:31,320 --> 00:05:34,560
Those two combined, the
data collection with eBPF

141
00:05:34,560 --> 00:05:37,170
and bring your own cloud to
store it cost effectively

142
00:05:37,170 --> 00:05:40,800
and private is what allows us
also to build on experiences

143
00:05:40,800 --> 00:05:43,320
which are high granularity,
which we'll show in a second,

144
00:05:43,320 --> 00:05:46,110
of how LLMS observability
can actually look like.

145
00:05:46,110 --> 00:05:47,073
So what is eBPF?

146
00:05:47,073 --> 00:05:50,070
eBPF is basically a
kernel-level capability,

147
00:05:50,070 --> 00:05:54,090
kind of a superpower that
allows us to look into what APIs

148
00:05:54,090 --> 00:05:56,070
and microservices are doing

149
00:05:56,070 --> 00:05:58,110
without you instrumenting code.

150
00:05:58,110 --> 00:06:00,870
With LLMs and AI, it gets
even more important, right?

151
00:06:00,870 --> 00:06:03,960
You can literally cover
your production now,

152
00:06:03,960 --> 00:06:07,230
any AI involved without
modifying your applications,

153
00:06:07,230 --> 00:06:08,910
without even redeploying production,

154
00:06:08,910 --> 00:06:12,990
without adding an SDK into
your code, which again,

155
00:06:12,990 --> 00:06:14,700
you wanna get to value of very quickly,

156
00:06:14,700 --> 00:06:16,260
you don't wanna do it, right?

157
00:06:16,260 --> 00:06:18,540
Second, we get kernel-level visibility.

158
00:06:18,540 --> 00:06:21,300
We can actually capture
the full API requests

159
00:06:21,300 --> 00:06:23,010
and responses, the actual payload,

160
00:06:23,010 --> 00:06:25,050
so we can see the prompts,
we can see the responses,

161
00:06:25,050 --> 00:06:27,000
we can see what's actually
being transmitted,

162
00:06:27,000 --> 00:06:28,470
so you get the visibility

163
00:06:28,470 --> 00:06:30,840
to figure out what's happening,

164
00:06:30,840 --> 00:06:32,970
and we can translate
that to realtime insights

165
00:06:32,970 --> 00:06:35,670
because suddenly you have
access to token usage,

166
00:06:35,670 --> 00:06:39,060
to reasoning path, to
LLM calls and error rates

167
00:06:39,060 --> 00:06:41,610
to actually correlate
them to logs, metrics,

168
00:06:41,610 --> 00:06:43,920
traces collected outside
of your environment,

169
00:06:43,920 --> 00:06:47,490
outside of that LLM application.

170
00:06:47,490 --> 00:06:49,500
And the bring-your-own-cloud piece

171
00:06:49,500 --> 00:06:51,780
is basically what allows us to do it

172
00:06:51,780 --> 00:06:53,280
so securely and privately.

173
00:06:53,280 --> 00:06:56,310
Basically what it means is
that we've found the balance

174
00:06:56,310 --> 00:07:00,630
between a managed service
and an on-prem deployment.

175
00:07:00,630 --> 00:07:03,060
We separate the control
plane and the data plane.

176
00:07:03,060 --> 00:07:04,860
So the data plane that
you can see on the left

177
00:07:04,860 --> 00:07:06,990
is basically being hosted in a sub account

178
00:07:06,990 --> 00:07:08,910
in your AWS environment,

179
00:07:08,910 --> 00:07:11,580
but the control plane that
manages the infrastructure

180
00:07:11,580 --> 00:07:14,130
scales it, updates it, provides the SLA

181
00:07:14,130 --> 00:07:16,260
that you deserve from
an observability company

182
00:07:16,260 --> 00:07:18,570
is being run by groundcover on our side.

183
00:07:18,570 --> 00:07:20,160
We have no access to your data.

184
00:07:20,160 --> 00:07:22,170
We don't store it, we don't process it.

185
00:07:22,170 --> 00:07:24,540
You don't have to rely on us for security

186
00:07:24,540 --> 00:07:27,300
and privacy of your GDPR sensitive

187
00:07:27,300 --> 00:07:29,883
and very kind of business critical data.

188
00:07:31,470 --> 00:07:33,420
That allows us to do a
few interesting things.

189
00:07:33,420 --> 00:07:36,540
First is protect LLM pipelines
from data exposure, right?

190
00:07:36,540 --> 00:07:39,690
You can now capture very sensitive data

191
00:07:39,690 --> 00:07:42,600
on what your users are doing with your AI

192
00:07:42,600 --> 00:07:43,950
without worrying that this data

193
00:07:43,950 --> 00:07:45,570
will be tracked or shipped anywhere.

194
00:07:45,570 --> 00:07:47,460
All this data has been
stored in your premises

195
00:07:47,460 --> 00:07:50,190
with groundcover, and there's
no third party storage,

196
00:07:50,190 --> 00:07:52,890
no outbound communication,
no egress of this data

197
00:07:52,890 --> 00:07:55,380
to anywhere that you need
to be concerned about.

198
00:07:55,380 --> 00:07:57,300
The second is that you
can actually use this

199
00:07:57,300 --> 00:08:00,150
to improve performance and manage spend,

200
00:08:00,150 --> 00:08:02,280
which is, again, with all the value of AI,

201
00:08:02,280 --> 00:08:03,600
this is becoming a critical issue

202
00:08:03,600 --> 00:08:05,730
that everybody wants to
take a look at, right?

203
00:08:05,730 --> 00:08:07,950
You can look at token
usage, latency, throughputs,

204
00:08:07,950 --> 00:08:09,690
we're gonna see example of error rates

205
00:08:09,690 --> 00:08:11,280
and things that we're gonna dive deep

206
00:08:11,280 --> 00:08:12,930
into the demo in a second.

207
00:08:12,930 --> 00:08:14,730
And the third and most important thing,

208
00:08:14,730 --> 00:08:18,180
you can actually monitor LLM
responses with the context

209
00:08:18,180 --> 00:08:20,760
that allows you to do
something with it, right?

210
00:08:20,760 --> 00:08:22,650
Maybe I wanna look at response payload.

211
00:08:22,650 --> 00:08:24,900
I wanna see which tools the LLM used

212
00:08:24,900 --> 00:08:27,360
or chose to use in the actual application.

213
00:08:27,360 --> 00:08:30,510
I wanna see a history of
path that the LLM chose

214
00:08:30,510 --> 00:08:33,810
to eventually do stuff,
and I wanna figure out

215
00:08:33,810 --> 00:08:37,260
what variance is my user's
getting from my deployed AI,

216
00:08:37,260 --> 00:08:39,570
what's actually working and what's not.

217
00:08:39,570 --> 00:08:41,250
All of this can be deployed in minutes.

218
00:08:41,250 --> 00:08:42,720
Install the sensor.

219
00:08:42,720 --> 00:08:43,890
It has an auto discovery.

220
00:08:43,890 --> 00:08:45,000
You don't have to tell it

221
00:08:45,000 --> 00:08:47,820
that you're just instrumented
Bedrock into your application.

222
00:08:47,820 --> 00:08:50,684
It will auto detect it and
suddenly we'll see Bedrock APIs

223
00:08:50,684 --> 00:08:54,180
going out of your microservices
and they will be monitored

224
00:08:54,180 --> 00:08:57,150
and correlated to all the other
insights that you collect,

225
00:08:57,150 --> 00:08:59,700
like log management,
infrastructure monitoring,

226
00:08:59,700 --> 00:09:01,523
and all the things you're used to seeing.

227
00:09:03,240 --> 00:09:04,860
Jumping into a quick demo,

228
00:09:04,860 --> 00:09:07,260
just to kinda show you
how that might look like.

229
00:09:08,880 --> 00:09:09,813
Sorry about that.

230
00:09:13,140 --> 00:09:15,600
So with groundcover,
basically the first thing

231
00:09:15,600 --> 00:09:17,940
that you're gonna see is
all of your deployments

232
00:09:17,940 --> 00:09:19,680
and what's running inside the cluster.

233
00:09:19,680 --> 00:09:21,450
You can also see a dependency map

234
00:09:21,450 --> 00:09:22,830
of who's communicating to who.

235
00:09:22,830 --> 00:09:25,590
In this case specifically,
I can say filter Bedrock

236
00:09:25,590 --> 00:09:28,230
and see shop-service, which
is a regular microservice

237
00:09:28,230 --> 00:09:29,850
running on my Kubernetes cluster,

238
00:09:29,850 --> 00:09:31,950
communicating with the Bedrock service

239
00:09:31,950 --> 00:09:35,280
through Bedrock traces that
I could actually observe.

240
00:09:35,280 --> 00:09:38,220
The second thing that we
can look at is actually see

241
00:09:38,220 --> 00:09:42,180
our API calls, in this
case, focus on Bedrock,

242
00:09:42,180 --> 00:09:44,190
I can see which models
I'm using in general,

243
00:09:44,190 --> 00:09:46,710
what's the latency, what's the throughput,

244
00:09:46,710 --> 00:09:48,000
how many of these being used.

245
00:09:48,000 --> 00:09:50,850
Just to emphasize, anything
that you're seeing here,

246
00:09:50,850 --> 00:09:52,290
you didn't do anything to get.

247
00:09:52,290 --> 00:09:53,880
The eBPF sensor, just collects it

248
00:09:53,880 --> 00:09:56,880
once you deploy a demon
set on your EKS cluster

249
00:09:56,880 --> 00:09:58,050
and you're good to go.

250
00:09:58,050 --> 00:10:01,770
AI information and context is
being collected immediately.

251
00:10:01,770 --> 00:10:04,410
Let's look at a cool example.

252
00:10:04,410 --> 00:10:05,970
This is the tracer screen groundcover.

253
00:10:05,970 --> 00:10:08,250
Basically the eBPF sensor
will collect any trace

254
00:10:08,250 --> 00:10:10,020
that flows through the system.

255
00:10:10,020 --> 00:10:12,420
We can focus, for example,

256
00:10:12,420 --> 00:10:15,090
on anything that is Bedrock-related.

257
00:10:15,090 --> 00:10:17,460
I wanna see, you know, what's
flowing through the cluster

258
00:10:17,460 --> 00:10:19,770
and I can immediately see flows

259
00:10:19,770 --> 00:10:22,110
that are even wider than Bedrock.

260
00:10:22,110 --> 00:10:24,630
In this specific example,
I can see a service map

261
00:10:24,630 --> 00:10:27,930
of a client communicating
to another a service,

262
00:10:27,930 --> 00:10:29,820
and then I can see Bedrock being executed

263
00:10:29,820 --> 00:10:31,230
in the middle at some point, right,

264
00:10:31,230 --> 00:10:33,810
where AI needs to be involved.

265
00:10:33,810 --> 00:10:35,070
The cool thing about it is like,

266
00:10:35,070 --> 00:10:38,010
I can actually see the full eBPF context

267
00:10:38,010 --> 00:10:38,843
of what's happening.

268
00:10:38,843 --> 00:10:40,320
I can see the request model that you use,

269
00:10:40,320 --> 00:10:42,930
the response model, the
tokens that are being used.

270
00:10:42,930 --> 00:10:44,160
I can see the chat, right?

271
00:10:44,160 --> 00:10:46,800
I can see the prompt and the
response from the server.

272
00:10:46,800 --> 00:10:49,800
I can see the raw request
in case I wanna figure out

273
00:10:49,800 --> 00:10:51,750
exactly what was transmitted,

274
00:10:51,750 --> 00:10:54,420
and I can even see correlation to things

275
00:10:54,420 --> 00:10:56,250
that you care about, like clogs, right?

276
00:10:56,250 --> 00:10:58,980
In this case, shop-service was telling me

277
00:10:58,980 --> 00:11:02,400
that it's about to create an AI greeting.

278
00:11:02,400 --> 00:11:06,480
In this case, it created
some kind of greeting

279
00:11:06,480 --> 00:11:09,540
with the AI service and I can
correlate it with the logs

280
00:11:09,540 --> 00:11:11,880
and even with the
infrastructure monitoring tool,

281
00:11:11,880 --> 00:11:15,030
so how much is that AI being called from

282
00:11:15,030 --> 00:11:15,900
in front of Bedrock?

283
00:11:15,900 --> 00:11:17,430
What is the actual latency?

284
00:11:17,430 --> 00:11:18,810
What is the CPO of the container?

285
00:11:18,810 --> 00:11:20,730
Anything that I wanna take a look at,

286
00:11:20,730 --> 00:11:22,830
and all of this was correlated at the edge

287
00:11:22,830 --> 00:11:26,077
by the eBPF sensor without
you thinking about it,

288
00:11:26,077 --> 00:11:28,530
instrumenting, notifying the sensor

289
00:11:28,530 --> 00:11:29,363
what to take a look at, right,

290
00:11:29,363 --> 00:11:31,833
'cause eventually you're
removing really quick.

291
00:11:32,940 --> 00:11:35,100
The other thing we can
look at is errors, right?

292
00:11:35,100 --> 00:11:36,450
In this case, this is an example

293
00:11:36,450 --> 00:11:39,720
of a Bedrock Anthropic model that failed

294
00:11:39,720 --> 00:11:40,857
and I can see the actual prompt

295
00:11:40,857 --> 00:11:42,990
and the response so that in this case,

296
00:11:42,990 --> 00:11:44,940
in case of kind of IAM problem

297
00:11:44,940 --> 00:11:47,760
that I couldn't eventually
use the relevant model

298
00:11:47,760 --> 00:11:50,550
or kind of approach the
relevant Bedrock service,

299
00:11:50,550 --> 00:11:54,180
troubleshooting that without
observability would require

300
00:11:54,180 --> 00:11:56,760
someone from the DevOps to
figure out what's going on.

301
00:11:56,760 --> 00:11:59,100
In this case, a developer
that is experimenting

302
00:11:59,100 --> 00:12:00,480
and trying to deploy this to production

303
00:12:00,480 --> 00:12:02,550
can now see what's going on.

304
00:12:02,550 --> 00:12:04,260
Another interesting example

305
00:12:04,260 --> 00:12:06,210
that we can take a look at

306
00:12:06,210 --> 00:12:08,610
is basically trying to explore this data

307
00:12:08,610 --> 00:12:10,680
in ways that can help you

308
00:12:10,680 --> 00:12:13,170
build business context on top of that.

309
00:12:13,170 --> 00:12:15,060
We can take a look at our traces.

310
00:12:15,060 --> 00:12:19,410
In this case, for example,
take a look at P90 latency

311
00:12:19,410 --> 00:12:21,960
of any model that we use.

312
00:12:21,960 --> 00:12:24,990
So in this case, I can
actually take a look

313
00:12:24,990 --> 00:12:27,060
over a specific period of time.

314
00:12:27,060 --> 00:12:30,420
I can look at which models
are the high-latency models

315
00:12:30,420 --> 00:12:32,030
that I wanna take take a look at, right?

316
00:12:32,030 --> 00:12:35,040
In this case, Anthropic is
one of the high-latency ones.

317
00:12:35,040 --> 00:12:38,370
There's a few faster ones that
are maybe operating better.

318
00:12:38,370 --> 00:12:40,380
Now I can correlate that with token usage.

319
00:12:40,380 --> 00:12:41,970
I can correlate that with value,

320
00:12:41,970 --> 00:12:44,250
with specific APIs that
trigger these models

321
00:12:44,250 --> 00:12:45,930
and I can make decisions, right?

322
00:12:45,930 --> 00:12:48,630
I can also do the same for token usage.

323
00:12:48,630 --> 00:12:52,290
I can suddenly take a
look at any token usage

324
00:12:52,290 --> 00:12:55,717
that I have inside the platform and say,

325
00:12:55,717 --> 00:12:57,134
"You know what, I wanna figure out

326
00:12:57,134 --> 00:12:59,640
what is the actual usage of tokens

327
00:12:59,640 --> 00:13:01,230
across by different models."

328
00:13:01,230 --> 00:13:02,340
And I'll make a decision

329
00:13:02,340 --> 00:13:04,920
about should I really use this model?

330
00:13:04,920 --> 00:13:08,070
Is this equivalent to the
quota I was expecting?

331
00:13:08,070 --> 00:13:09,210
What is my team doing?

332
00:13:09,210 --> 00:13:10,320
What are they building?

333
00:13:10,320 --> 00:13:12,420
So I can figure out what's going on.

334
00:13:12,420 --> 00:13:14,640
We can also do more sophisticated stuff

335
00:13:14,640 --> 00:13:18,270
that will eventually allow
us to dive into areas

336
00:13:18,270 --> 00:13:20,640
where we care about for actual SLA

337
00:13:20,640 --> 00:13:22,260
in front of our customers.

338
00:13:22,260 --> 00:13:24,813
Say that, you know, I'm using Bedrock,

339
00:13:25,920 --> 00:13:29,160
but I also wanna figure out anything

340
00:13:29,160 --> 00:13:32,880
that is of duration
above one second, right,

341
00:13:32,880 --> 00:13:36,180
because that's the SLA that
I promise my customers.

342
00:13:36,180 --> 00:13:39,540
Anything that, any API
that crosses one second

343
00:13:39,540 --> 00:13:43,320
kind of affects my SLA that
I guarantee to my customers.

344
00:13:43,320 --> 00:13:46,530
First of all, I can see the
actual traces that causes this.

345
00:13:46,530 --> 00:13:48,810
I can actually take a
look at that example,

346
00:13:48,810 --> 00:13:52,230
in this case, a 1.4 second
response from Bedrock,

347
00:13:52,230 --> 00:13:54,120
and I can see, okay, what's going on?

348
00:13:54,120 --> 00:13:55,200
What's the actual flow?

349
00:13:55,200 --> 00:13:58,230
But also what's the prompt
that triggered that?

350
00:13:58,230 --> 00:13:59,280
What's the context?

351
00:13:59,280 --> 00:14:01,080
Why is it taking so long?

352
00:14:01,080 --> 00:14:03,450
The second thing that
I can do is drill down

353
00:14:03,450 --> 00:14:05,010
on this specific filter and figure out

354
00:14:05,010 --> 00:14:07,350
if there's anything I should care about.

355
00:14:07,350 --> 00:14:09,630
Is there any model that
is taking care of that?

356
00:14:09,630 --> 00:14:13,800
Is there any specific token
usage that might be triggering

357
00:14:13,800 --> 00:14:15,000
that specific latency?

358
00:14:15,000 --> 00:14:17,460
Am I seeing that high latency only on like

359
00:14:17,460 --> 00:14:19,800
very big prompts, very big context,

360
00:14:19,800 --> 00:14:21,900
or very specific workflows?

361
00:14:21,900 --> 00:14:23,610
That is very valuable.

362
00:14:23,610 --> 00:14:25,560
Just getting into this
data and correlating

363
00:14:25,560 --> 00:14:28,410
and figuring out what's
going on, that's really hard.

364
00:14:28,410 --> 00:14:32,310
I can also take a look at the
relevant workloads, right?

365
00:14:32,310 --> 00:14:34,650
Who eventually is the service

366
00:14:34,650 --> 00:14:36,426
that is responsible for any of these?

367
00:14:36,426 --> 00:14:39,300
What's creating these queries

368
00:14:39,300 --> 00:14:40,923
inside my specific environment?

369
00:14:41,910 --> 00:14:45,210
The second thing I could
do is set alerts, right?

370
00:14:45,210 --> 00:14:48,930
Now that you have all this
data and you care about SLA

371
00:14:48,930 --> 00:14:51,450
and you know that if this
specific SLA is breached,

372
00:14:51,450 --> 00:14:55,680
you wanna know why not just
create a monitor of that,

373
00:14:55,680 --> 00:14:58,500
that you can now use to alert your teams

374
00:14:58,500 --> 00:15:00,180
and alert yourself that
something is going on,

375
00:15:00,180 --> 00:15:02,280
so in this specific case,

376
00:15:02,280 --> 00:15:07,260
if there's an LLM SLA
breach, I might wanna know.

377
00:15:07,260 --> 00:15:08,970
Every time that it crosses one second,

378
00:15:08,970 --> 00:15:12,090
I wanna get notified and look how easy

379
00:15:12,090 --> 00:15:13,770
that was without you doing anything

380
00:15:13,770 --> 00:15:16,170
because the traces were
there, the context were there,

381
00:15:16,170 --> 00:15:18,750
and now you could just need
to define the business value

382
00:15:18,750 --> 00:15:20,310
of what you care about.

383
00:15:20,310 --> 00:15:23,970
Anything that revolves around
that can now be created

384
00:15:23,970 --> 00:15:24,990
into dashboards, right?

385
00:15:24,990 --> 00:15:26,940
The things that are repetitive

386
00:15:26,940 --> 00:15:29,220
that you should care about
for a longer period of time,

387
00:15:29,220 --> 00:15:31,680
that you want to even
share with your customers,

388
00:15:31,680 --> 00:15:34,320
that you want people to be
aware of when you're using AI,

389
00:15:34,320 --> 00:15:37,320
which models and vendors am I using?

390
00:15:37,320 --> 00:15:38,580
What's the latency?

391
00:15:38,580 --> 00:15:41,310
How is token usage behaving over time?

392
00:15:41,310 --> 00:15:42,436
Am I crossing my quota?

393
00:15:42,436 --> 00:15:43,980
Am I in a good state?

394
00:15:43,980 --> 00:15:46,920
How is latency behaving
over time and who to blame?

395
00:15:46,920 --> 00:15:48,390
Which models am I using?

396
00:15:48,390 --> 00:15:49,710
Have I introduced a new model?

397
00:15:49,710 --> 00:15:51,150
Is something kind of moving around?

398
00:15:51,150 --> 00:15:54,870
Is it related to a version
that I wanna focus on?

399
00:15:54,870 --> 00:15:57,150
Anything that we've seen so far

400
00:15:57,150 --> 00:15:59,490
can basically be baked into monitors,

401
00:15:59,490 --> 00:16:02,670
dashboards, and things that
can be repetitively used

402
00:16:02,670 --> 00:16:04,953
to dive into what you're looking at.

403
00:16:07,200 --> 00:16:10,410
Baking that into the
context of your production,

404
00:16:10,410 --> 00:16:11,400
your dependency map,

405
00:16:11,400 --> 00:16:14,320
who's communicating with who
is what eventually can drive

406
00:16:15,180 --> 00:16:18,270
the information, the value
that you need to your business.

407
00:16:18,270 --> 00:16:21,753
Going back a bit to kind
of the start where we were,

408
00:16:24,570 --> 00:16:26,850
all of this is basically achieved

409
00:16:26,850 --> 00:16:31,230
with a very innovative
approach to how we monitor AI.

410
00:16:31,230 --> 00:16:34,320
If up until now, you
had to instrument an SDK

411
00:16:34,320 --> 00:16:37,650
into your code, pick and
choose what you wanna monitor.

412
00:16:37,650 --> 00:16:40,230
I've instrumented the new AI
applications into my code.

413
00:16:40,230 --> 00:16:42,480
I now need to instrument
the functions using it

414
00:16:42,480 --> 00:16:46,530
and notify my SDK, my open telemetry SDK,

415
00:16:46,530 --> 00:16:48,480
or whatever I'm using,
I need to notify that,

416
00:16:48,480 --> 00:16:50,040
that this is important for me

417
00:16:50,040 --> 00:16:52,080
while my developers are running forward,

418
00:16:52,080 --> 00:16:53,970
now I can deploy an eBPF sensor

419
00:16:53,970 --> 00:16:55,410
and get the value immediately.

420
00:16:55,410 --> 00:16:58,080
If before I had to redeploy production

421
00:16:58,080 --> 00:17:00,090
or test things that staging and dev,

422
00:17:00,090 --> 00:17:02,100
now production is running

423
00:17:02,100 --> 00:17:05,190
and it's covered in 30
seconds from deployment,

424
00:17:05,190 --> 00:17:07,380
and also I can store logs, metrics,

425
00:17:07,380 --> 00:17:09,180
and traces in the same environment

426
00:17:09,180 --> 00:17:10,890
under my bring your own cloud,

427
00:17:10,890 --> 00:17:12,510
not worry about data residency,

428
00:17:12,510 --> 00:17:15,750
not worry about privacy, not
worry about data security,

429
00:17:15,750 --> 00:17:19,290
and not worry about
creating a new sub processor

430
00:17:19,290 --> 00:17:23,133
that might concern my customers
and get all this value.

431
00:17:24,120 --> 00:17:25,140
Thank you for listening.

432
00:17:25,140 --> 00:17:28,410
If there's any follow-up questions,

433
00:17:28,410 --> 00:17:30,120
first of all, you can find us

434
00:17:30,120 --> 00:17:32,850
in the yellow shirt
booth right over there.

435
00:17:32,850 --> 00:17:34,140
Can't miss us.

436
00:17:34,140 --> 00:17:36,150
I'll be right over here for any questions

437
00:17:36,150 --> 00:17:38,130
or follow-ups that come up.

438
00:17:38,130 --> 00:17:40,200
We have a playground
that you can check out

439
00:17:40,200 --> 00:17:43,500
at play.groundcover.com where
you can see all this demo

440
00:17:43,500 --> 00:17:45,270
and play with it yourselves.

441
00:17:45,270 --> 00:17:48,390
I'm sure that it will kind
of give way to new questions,

442
00:17:48,390 --> 00:17:50,400
so feel free to try it out.

443
00:17:50,400 --> 00:17:52,320
We have a free tier and we're over there

444
00:17:52,320 --> 00:17:54,420
and just happy to answer any questions.

445
00:17:54,420 --> 00:17:58,193
Thank you for the time.
(audience applauding)

