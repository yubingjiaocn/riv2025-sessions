# AWS re:Invent 2025 会议总结：构建容错和消息传递系统

## 会议概述

本次会议是 AWS re:Invent 2025 第一天的技术分享，由两位高级解决方案架构师 Parnub Basup 和 Tom Romano 主讲。会议聚焦于如何使用 AWS 服务构建具有高可用性和容错能力的消息传递系统。两位讲师都是无服务器技术领域社区的成员，Parnub 同时也是数据分析技术社区的成员，这意味着本次分享的架构模式不仅适用于消息传递系统，也同样适用于数据管道场景。

会议采用了理论结合实践的方式，前半部分由 Parnub 介绍架构模式和设计考虑因素，后半部分由 Tom 进行现场编码演示。讲师通过一个咖啡订单处理系统作为示例应用，展示了如何在不大幅修改主应用代码的情况下，实现多种容错模式。会议强调的核心理念是：通过合理的架构设计，开发者可以避免在深夜或周末被系统故障叫醒，让系统能够优雅地处理各种异常情况。

会议涵盖了 AWS 原生消息服务（如 SQS、SNS、Kinesis、EventBridge）和托管开源服务（如 Amazon MQ 和 MSK）。通过现场调查发现，大多数与会者更倾向于使用 AWS 原生服务，因此演示主要基于这些服务展开。讲师介绍了五种关键的容错模式：死信队列（DLQ）、重试与退避、断路器、Saga 编排和冗余模式，并详细演示了前两种模式的实现。

## 详细时间线与关键要点

### 00:00 - 开场与背景介绍
- 讲师幽默地提到 re:Invent 第一天的三大发现：拉斯维加斯不只有赌场、运动鞋是最好的朋友、时差是真实存在的
- 通过举手调查了解听众背景：多数人正在使用或计划使用 AWS 消息服务
- 询问有多少人曾因消息系统故障在非工作时间被叫醒处理问题

### 02:30 - 讲师自我介绍
- Parnub Basup：高级解决方案架构师，帮助公共部门客户采用 AWS，开发者背景
- Tom Romano：高级解决方案架构师，与 Parnub 合作 5 年，两人都是无服务器技术社区成员
- Parnub 同时是数据分析技术社区成员，强调这些模式在数据工程中同样适用

### 04:00 - 会议议程
- AWS 消息服务简介和模式概述
- 设计消息系统时的弹性考虑因素
- 架构模式详解
- 现场编码演示（300 级别，无生成式 AI 辅助）
- Q&A 环节

### 05:30 - AWS 消息服务概览
- **AWS 原生服务**：SQS（标准队列和 FIFO 队列）、SNS（发布/订阅）、Kinesis（流式处理）、EventBridge（企业服务总线）
- **托管开源服务**：Amazon MQ（支持 RabbitMQ 和 ActiveMQ）、Amazon MSK（托管 Kafka）
- 现场调查显示更多人使用 AWS 原生服务而非托管开源服务

### 08:00 - 消息系统的故障点分析
- **生产者故障**：可能停止发送消息或发送格式错误的消息
- **消息服务故障**：可能因持久性错误或瞬态错误而宕机
- **消费者故障**：可能因网络问题或部署失败而无法处理消息
- 强调需要通过架构模式来应对这些故障场景

### 10:00 - 模式一：死信队列（DLQ）
- DLQ 是一个标准队列，用于存储无法正常处理的错误消息
- 讲师不喜欢"死信"这个名称，认为应该叫"错误消息队列"，因为这些消息并非真正"死亡"
- 配置简单：定义 DLQ 并设置最大接收次数（max receive count）
- 消息保留期设置为 14 天，给团队足够时间发现和修复问题
- 可以手动或系统化地拦截、修正并重新发布这些消息
- 现场调查显示这是采用率最高的模式

### 13:30 - 模式二：重试与指数退避
- 用于处理瞬态错误（如临时网络故障、消息过载导致的服务配额限制）
- 通过在重试之间增加延迟时间，给消息服务或消费者恢复到健康状态的时间
- 某些 AWS 服务（如 SNS、Lambda）已内置此功能
- 可以根据需要覆盖默认配置

### 16:00 - 模式三：断路器模式
- 用于处理持久性错误（非临时性、不可自动恢复的错误）
- 类比电路系统：正常时电路闭合，出现持久错误时电路打开
- 使用 Step Functions 或 Lambda 将消息暂存到 SQS
- 通过 EventBridge 定期进行健康检查
- 当消费者恢复健康后，将消息重新驱动回原始消费者
- 现场调查显示采用率较低

### 18:30 - 模式四：Saga 编排模式
- 用于分布式事务场景，需要多个消费者都成功提交才算完整事务
- 使用 Step Functions 作为编排器，协调多个消费者之间的事务
- 如果某个消费者不可用，可以执行补偿事务，回滚已提交的操作
- 将消息存储在数据库或其他队列中，等待重试
- 名称来源于"史诗故事"（saga），每个消费者就像故事中的章节

### 21:00 - 模式五：冗余模式
- 用于极少见的基础设施不可用情况
- Amazon MQ 支持主备对配置，部署在两个可用区，通过 EFS 连接
- 对于更高的弹性需求，可以配置跨多区域、多可用区的活动队列网络
- 实现零停机时间

### 23:00 - 现场编码演示开始
- Tom 展示咖啡订单处理系统的演示应用
- 架构：前端 → API Gateway → SQS → Lambda 消费者 → 外部支付系统 + DynamoDB
- 演示了正常工作流程：提交订单、查看订单历史

### 26:00 - 故障注入演示
- Tom 注入故障，模拟外部支付系统宕机
- 提交多个咖啡订单后，订单历史中没有显示新订单
- 通过 CloudWatch 观察到问题：
  - 只提交了 3 个订单，但每 10 秒产生 6 条消息
  - SQS 队列深度保持在 3，消息不断被重复处理
  - Lambda 调用次数激增，每次调用持续 30 秒（超时时间）
  - 大量计算资源浪费，但没有任何进展

### 30:00 - 实现 DLQ 解决方案
- Tom 使用 Kiro（AWS 新推出的 IDE）进行现场编码
- 创建 DLQ：标准 SQS 队列，消息保留期设置为 1,209,600 秒（14 天）
- 配置重驱策略（redrive policy）：
  - 指定 DLQ 的 ARN
  - 设置最大接收次数（max receive count）为 2-3 次
- 授予 Lambda 函数访问 DLQ 的权限
- 将 DLQ 的名称和 URL 作为环境变量传递给 Lambda 函数
- **关键点**：无需修改主应用代码，只需配置 SQS 即可

### 35:00 - 实现重驱（Redrive）功能
- 创建新的 Lambda 函数用于重驱消息
- **金丝雀检查**：在重驱之前，先发送测试订单（金额为 0）验证支付系统是否恢复
- 使用 SQS 的 start_message_move_task API：
  - source_queue_url：DLQ 的 URL
  - destination_queue_url：原始订单队列的 URL
  - max_number_of_messages_per_second：控制重驱速率（演示中设置为 1 条/秒）
- SQS 会自动将消息从 DLQ 移动到目标队列，无需手动处理

### 40:00 - 金丝雀健康检查代码展示
- 发送金额为 0 的测试订单到支付系统
- 支付系统配置为忽略此类测试消息
- 可以测量延迟、响应时间等指标
- 确保系统健康后再执行重驱操作

### 42:00 - DLQ 效果演示
- 部署 DLQ 配置后，系统行为立即改善：
  - 消息数量快速下降，不再无限循环
  - Lambda 调用得到控制，不再出现"Lambda 风暴"
  - 虽然仍有少量 30 秒超时的执行，但不再持续发生
- **成本优化**：在故障情况下，系统能够优雅降级，避免资源失控

### 45:00 - 重驱演示
- 修复支付系统（移除故障注入）
- 执行重驱函数
- 以每秒 1 条消息的速率，订单开始从 DLQ 重新处理
- 订单历史中逐渐显示之前失败的订单
- 演示了完整的故障恢复流程

### 48:00 - Q&A 环节
- **问题 1**：消息在队列之间移动时，是否有 TTL 防止无限循环？
  - 回答：通过金丝雀健康检查确保目标系统可用后才重驱，避免消息在队列间反复弹跳
 
- **问题 2**：如果消息本身有问题导致持续失败怎么办？
  - 回答：建议使用多个 DLQ 策略：一个用于需要人工干预的消息（如格式错误），另一个用于等待系统恢复的消息
 
- **问题 3**：控制台是否支持手动修改 DLQ 中的消息并重驱？
  - 回答：控制台可以查看消息但不能直接修改，需要使用 AWS CLI 或自定义工具来实现消息修改和重驱
 
- **问题 4**：重驱时消息的属性（如年龄）是保持原值还是重置？
  - 回答：会话在此处被截断，但这是关于消息元数据在重驱过程中如何处理的重要问题

### 总结
本次会议通过理论讲解和现场编码相结合的方式，展示了如何构建生产级别的容错消息系统。重点强调了通过 AWS 托管服务的内置功能，可以在最小化代码修改的前提下实现复杂的容错模式，帮助开发者构建更可靠、更易维护的分布式系统。