1
00:00:00,828 --> 00:00:01,661
- [Cody] This is DAT 444.

2
00:00:02,900 --> 00:00:04,260
We like the number four.

3
00:00:04,260 --> 00:00:05,940
Letter D is the fourth
letter of the alphabet.

4
00:00:05,940 --> 00:00:08,400
We're data geeks so we geek out over this,

5
00:00:08,400 --> 00:00:11,850
but this is deep dive into
DocumentDB and it's innovation.

6
00:00:11,850 --> 00:00:15,120
So what we're gonna do
here today is talk about

7
00:00:15,120 --> 00:00:17,460
all the big feature releases
that we've announced

8
00:00:17,460 --> 00:00:20,400
for DocumentDB in 2025.

9
00:00:20,400 --> 00:00:22,290
DocumentDB is a purpose-built database

10
00:00:22,290 --> 00:00:24,810
for managing your document
database workloads

11
00:00:24,810 --> 00:00:26,580
at enterprise level scale.

12
00:00:26,580 --> 00:00:29,910
This past year we've released
a lot of really cool features.

13
00:00:29,910 --> 00:00:33,060
Serverless, our new compression ratio.

14
00:00:33,060 --> 00:00:36,570
We've had improvements to
things like global failover,

15
00:00:36,570 --> 00:00:38,880
global switchover,

16
00:00:38,880 --> 00:00:41,430
and we're gonna tell you
about these features.

17
00:00:41,430 --> 00:00:42,720
Get kind of deep in 'em.

18
00:00:42,720 --> 00:00:44,550
Again, it's a 400 level course

19
00:00:44,550 --> 00:00:47,730
and show you how you can
improve your workloads using

20
00:00:47,730 --> 00:00:49,290
a lot of these new features on DocumenDB.

21
00:00:49,290 --> 00:00:51,180
I guess we haven't really
introduced ourselves.

22
00:00:51,180 --> 00:00:53,220
- No, I guess we haven't.
- I'm, I'm Cody Allen.

23
00:00:53,220 --> 00:00:56,010
I'm a solution architect with the service.

24
00:00:56,010 --> 00:00:57,420
With me is Vin.

25
00:00:57,420 --> 00:00:58,563
- Hey everybody, my name is Vin.

26
00:00:58,563 --> 00:01:01,980
I am one of the product managers
with the DocumentDB team

27
00:01:01,980 --> 00:01:04,170
and I'm super excited to share,

28
00:01:04,170 --> 00:01:06,900
you know, what we've launched this year.

29
00:01:06,900 --> 00:01:08,310
- Awesome, so we're gonna start off,

30
00:01:08,310 --> 00:01:10,470
we're gonna talk about the architecture.

31
00:01:10,470 --> 00:01:11,520
Before I start,

32
00:01:11,520 --> 00:01:13,560
who's never been to re:Invent before?

33
00:01:13,560 --> 00:01:15,480
This is your first year here?

34
00:01:15,480 --> 00:01:16,313
Dude, this is awesome.

35
00:01:16,313 --> 00:01:18,810
We had a session earlier and
like 80% of the attendees

36
00:01:18,810 --> 00:01:20,100
were brand new, so that's awesome.

37
00:01:20,100 --> 00:01:21,300
I'm glad that you're here.

38
00:01:21,300 --> 00:01:24,300
Anybody been here for like five times,

39
00:01:24,300 --> 00:01:26,520
six times, seven times?

40
00:01:26,520 --> 00:01:28,590
Man, that's awesome.

41
00:01:28,590 --> 00:01:30,780
I don't know how many years
re:Invent's been going on,

42
00:01:30,780 --> 00:01:32,780
but five, six years in a row is awesome.

43
00:01:34,290 --> 00:01:36,090
The reason I ask that is because

44
00:01:36,090 --> 00:01:38,430
we have names and numbers for sessions.

45
00:01:38,430 --> 00:01:40,410
This one's DAT 444.

46
00:01:40,410 --> 00:01:43,650
DAT means it's a database session 444.

47
00:01:43,650 --> 00:01:44,700
Pick any one of them.

48
00:01:44,700 --> 00:01:46,080
That just means it's an expert level.

49
00:01:46,080 --> 00:01:48,120
So an expert level session means that

50
00:01:48,120 --> 00:01:50,400
we kind of expect you to know a little bit

51
00:01:50,400 --> 00:01:51,390
about DocumentDB.

52
00:01:51,390 --> 00:01:52,770
You've probably been hands-on with it.

53
00:01:52,770 --> 00:01:55,440
You're using it in a
production environment.

54
00:01:55,440 --> 00:01:59,053
What that means is we're gonna
skip over the basic stuff.

55
00:01:59,053 --> 00:01:59,886
We're gonna go straight

56
00:01:59,886 --> 00:02:01,140
into the kind of expert level materials,

57
00:02:01,140 --> 00:02:02,790
but we are gonna start
off with the architecture.

58
00:02:02,790 --> 00:02:04,740
I want to set a foundation for us

59
00:02:04,740 --> 00:02:07,470
because DocumentDB was purpose built

60
00:02:07,470 --> 00:02:10,170
and one of the key features
of it is its architecture.

61
00:02:10,170 --> 00:02:12,133
So we're gonna spend some time on that

62
00:02:12,133 --> 00:02:12,990
and we're gonna get into the fun stuff.

63
00:02:12,990 --> 00:02:15,420
We're gonna go over those things
like enhanced compression,

64
00:02:15,420 --> 00:02:17,310
Serverless, Graviton4.

65
00:02:17,310 --> 00:02:18,840
We were so excited about Graviton4,

66
00:02:18,840 --> 00:02:21,090
we skipped Graviton3.

67
00:02:21,090 --> 00:02:23,280
We came out with DocumentDB 8.0.

68
00:02:23,280 --> 00:02:25,020
Again, we just skipped six and seven

69
00:02:25,020 --> 00:02:26,340
and went straight to eight.

70
00:02:26,340 --> 00:02:28,260
And we're gonna talk
about New Query Planner,

71
00:02:28,260 --> 00:02:29,580
one of the new features that you get

72
00:02:29,580 --> 00:02:31,250
with a DocumentDB 8.0.

73
00:02:32,250 --> 00:02:36,180
But like I said, 400 level session,

74
00:02:36,180 --> 00:02:39,150
we're gonna skip over the marketing slide.

75
00:02:39,150 --> 00:02:41,070
We're not gonna spend
time on the fluffy stuff

76
00:02:41,070 --> 00:02:42,360
that you can read on our webpage.

77
00:02:42,360 --> 00:02:43,530
We don't have a need for it.

78
00:02:43,530 --> 00:02:45,510
You know what you're top...

79
00:02:45,510 --> 00:02:46,343
Oh, sorry.

80
00:02:47,640 --> 00:02:49,320
Sorry, that was not supposed to be there.

81
00:02:49,320 --> 00:02:50,580
Sorry.

82
00:02:50,580 --> 00:02:53,043
So we're gonna skip over
the marketing slide.

83
00:02:53,910 --> 00:02:55,830
Sometimes you'll see
like the NASCAR slides,

84
00:02:55,830 --> 00:02:56,857
you know, some, they'll say,

85
00:02:56,857 --> 00:02:58,860
"Oh, look at Aurora and look
at all of its customers.

86
00:02:58,860 --> 00:03:00,270
We have to justify ourselves."

87
00:03:00,270 --> 00:03:02,137
And they'll segment out
their customers by saying,

88
00:03:02,137 --> 00:03:03,450
"Oh look, we have customers here

89
00:03:03,450 --> 00:03:04,590
in the marketing department."

90
00:03:04,590 --> 00:03:06,360
Like we need to just...

91
00:03:06,360 --> 00:03:08,640
Oh, son of a...

92
00:03:08,640 --> 00:03:10,440
So ignoring that,

93
00:03:10,440 --> 00:03:12,780
we're not gonna talk
about that kind of stuff.

94
00:03:12,780 --> 00:03:14,490
Help me baby Jesus.

95
00:03:14,490 --> 00:03:15,420
There we go, cool.

96
00:03:15,420 --> 00:03:16,950
Architecture, let's jump right into it.

97
00:03:16,950 --> 00:03:20,160
Now DocumentDB is a purpose-built database

98
00:03:20,160 --> 00:03:21,990
and the main feature here

99
00:03:21,990 --> 00:03:24,390
is a separation of compute and storage.

100
00:03:24,390 --> 00:03:25,470
The reason we did this

101
00:03:25,470 --> 00:03:27,270
is so that you could scale your workloads

102
00:03:27,270 --> 00:03:29,490
based on the resources that you need,

103
00:03:29,490 --> 00:03:32,160
either that compute
layer, that storage layer,

104
00:03:32,160 --> 00:03:33,630
and that storage layer
is actually broken up

105
00:03:33,630 --> 00:03:34,950
into two different pieces.

106
00:03:34,950 --> 00:03:37,290
We have the distributed storage
volume that holds your data

107
00:03:37,290 --> 00:03:39,690
and we have your backups that go to S3.

108
00:03:39,690 --> 00:03:43,350
We'll dive deep into that
here in a little bit.

109
00:03:43,350 --> 00:03:45,930
Your data is automatically
replicated six times

110
00:03:45,930 --> 00:03:47,490
across three availability zones.

111
00:03:47,490 --> 00:03:50,160
That means you're in multiple
physical data centers

112
00:03:50,160 --> 00:03:53,490
and that gives you that
enterprise grade durability.

113
00:03:53,490 --> 00:03:56,970
What's really cool about this durability

114
00:03:56,970 --> 00:03:58,860
is you get six copies of it

115
00:03:58,860 --> 00:04:01,170
even if you have a
single compute instance.

116
00:04:01,170 --> 00:04:04,890
It's not dependent on how many
compute instances you have.

117
00:04:04,890 --> 00:04:05,910
You can even run headless.

118
00:04:05,910 --> 00:04:09,210
You can run with no DocumentDB
instances whatsoever.

119
00:04:09,210 --> 00:04:10,410
And so six copies of your data.

120
00:04:10,410 --> 00:04:12,150
Now can't interact with it

121
00:04:12,150 --> 00:04:13,530
'cause you do have to
have a compute instance

122
00:04:13,530 --> 00:04:14,910
to actually query your data,

123
00:04:14,910 --> 00:04:16,010
but you could do that.

124
00:04:17,190 --> 00:04:20,100
Because this data durability

125
00:04:20,100 --> 00:04:21,960
is handled at that storage layer.

126
00:04:21,960 --> 00:04:23,160
Again, you don't have to worry

127
00:04:23,160 --> 00:04:25,110
about that compute layer at all.

128
00:04:25,110 --> 00:04:29,070
We scale up to 128
terabytes of data for you.

129
00:04:29,070 --> 00:04:30,090
You don't have to worry about it.

130
00:04:30,090 --> 00:04:32,190
And that means that you're not gonna hit

131
00:04:32,190 --> 00:04:34,590
any kind of compute
layer size restrictions.

132
00:04:34,590 --> 00:04:36,150
In some database systems

133
00:04:36,150 --> 00:04:39,483
because the compute and
storage is co-located together,

134
00:04:40,350 --> 00:04:43,050
if you hit a certain amount
of storage that you're using

135
00:04:43,050 --> 00:04:44,640
in order to get more storage,

136
00:04:44,640 --> 00:04:45,473
you have to scale up,

137
00:04:45,473 --> 00:04:47,147
you have to add more vCPU to it.

138
00:04:47,147 --> 00:04:48,690
You have to add more RAM.

139
00:04:48,690 --> 00:04:50,100
DocumentDB, you scale independently.

140
00:04:50,100 --> 00:04:51,180
You don't have to do this.

141
00:04:51,180 --> 00:04:54,360
This also eliminates the need
to do things like sharding

142
00:04:54,360 --> 00:04:55,950
where you have to split your data

143
00:04:55,950 --> 00:04:59,190
up into certain segments
on different clusters.

144
00:04:59,190 --> 00:05:03,420
Now all of this is gonna reduce
that operational overhead.

145
00:05:03,420 --> 00:05:06,300
It's gonna reduce the risk
of having data hotspots.

146
00:05:06,300 --> 00:05:10,500
And because your data is replicated...

147
00:05:10,500 --> 00:05:12,630
Has six copies across
three availability zones

148
00:05:12,630 --> 00:05:13,920
no matter how many instances you have,

149
00:05:13,920 --> 00:05:15,900
no matter what size you have,

150
00:05:15,900 --> 00:05:17,370
it means that it's gonna grow with you,

151
00:05:17,370 --> 00:05:19,200
it's gonna grow with your workload.

152
00:05:19,200 --> 00:05:22,230
Document database has no SQL...

153
00:05:22,230 --> 00:05:25,710
Most legacy systems were
built on relational databases.

154
00:05:25,710 --> 00:05:27,086
Microsoft SQL Server, Oracle.

155
00:05:27,086 --> 00:05:30,540
Any relational DBAs,
database folks in here?

156
00:05:30,540 --> 00:05:31,650
My people, I love you.

157
00:05:31,650 --> 00:05:34,260
You're gonna love what we
show a little bit later.

158
00:05:34,260 --> 00:05:36,270
Document databases are
still kind of greenfield,

159
00:05:36,270 --> 00:05:37,103
they're still net new.

160
00:05:37,103 --> 00:05:38,760
You do see some migrations to it,

161
00:05:38,760 --> 00:05:41,070
but it's really a developer's database.

162
00:05:41,070 --> 00:05:42,660
So you're building from scratch

163
00:05:42,660 --> 00:05:44,190
and when you do that,

164
00:05:44,190 --> 00:05:46,560
you don't really care about durability

165
00:05:46,560 --> 00:05:47,610
or availability, right?

166
00:05:47,610 --> 00:05:50,790
You just wanna develop something
to see if it's gonna work,

167
00:05:50,790 --> 00:05:52,170
and that quickly grows, right?

168
00:05:52,170 --> 00:05:53,220
As you have success for it,

169
00:05:53,220 --> 00:05:54,053
it's gonna build up.

170
00:05:54,053 --> 00:05:57,390
We all know about the server
under the developer's desk

171
00:05:57,390 --> 00:05:58,590
that's running a production system

172
00:05:58,590 --> 00:06:00,570
because they developed
it, it looked great,

173
00:06:00,570 --> 00:06:01,635
they sold it to leadership

174
00:06:01,635 --> 00:06:04,165
and now it's sitting
under their desk still.

175
00:06:04,165 --> 00:06:05,280
That's...

176
00:06:05,280 --> 00:06:07,230
It just kind of hides it in the shadow.

177
00:06:07,230 --> 00:06:08,063
With DocumentDB,

178
00:06:08,063 --> 00:06:10,230
you get this no matter what size instance

179
00:06:10,230 --> 00:06:11,580
you have as you scale up.

180
00:06:11,580 --> 00:06:13,980
So you're gonna have that
enterprise level durability

181
00:06:13,980 --> 00:06:17,580
and availability and security
from start to finish.

182
00:06:17,580 --> 00:06:21,840
Now the data in DocumentDB
is log structured.

183
00:06:21,840 --> 00:06:23,337
Most traditional databases,

184
00:06:23,337 --> 00:06:25,830
you use a block storage device.

185
00:06:25,830 --> 00:06:28,320
This is designed around
database interaction.

186
00:06:28,320 --> 00:06:30,990
So that means that it doesn't
have to concern itself

187
00:06:30,990 --> 00:06:33,840
with that underlying
storage layer whatsoever.

188
00:06:33,840 --> 00:06:37,140
Data is gonna continuously stream to S3.

189
00:06:37,140 --> 00:06:38,340
Most database systems,

190
00:06:38,340 --> 00:06:39,872
you have to do a...

191
00:06:39,872 --> 00:06:41,100
You know, I'm gonna take this
back to our SQL server days.

192
00:06:41,100 --> 00:06:42,300
You have to do your full backup

193
00:06:42,300 --> 00:06:46,230
and then you have to do
your incremental backups.

194
00:06:46,230 --> 00:06:47,700
That all runs in the compute layer, right?

195
00:06:47,700 --> 00:06:49,650
We have to use compute layer resources

196
00:06:49,650 --> 00:06:51,330
in order to do those backups.

197
00:06:51,330 --> 00:06:53,580
But because of the log
structure of DocumentDB

198
00:06:53,580 --> 00:06:54,960
and because we stream this

199
00:06:54,960 --> 00:06:57,270
from the distributed storage volume,

200
00:06:57,270 --> 00:06:58,800
that happens outside of the critical path.

201
00:06:58,800 --> 00:07:01,080
That means that your
applications are not affected

202
00:07:01,080 --> 00:07:03,090
by any of these backups whatsoever.

203
00:07:03,090 --> 00:07:05,690
It all happens seamlessly
for you behind the scenes.

204
00:07:06,600 --> 00:07:09,630
These blocks of data that we store in,

205
00:07:09,630 --> 00:07:10,710
these are called...

206
00:07:10,710 --> 00:07:12,120
These are about 10 gigs in size

207
00:07:12,120 --> 00:07:14,400
and this is called a protection group.

208
00:07:14,400 --> 00:07:16,770
These are the things that
get replicated across 10

209
00:07:16,770 --> 00:07:19,170
or across six different AZs.

210
00:07:19,170 --> 00:07:22,230
This is why your storage
grows in 10 gigabyte chunks,

211
00:07:22,230 --> 00:07:24,120
and this is why the minimum you charge

212
00:07:24,120 --> 00:07:25,290
for storage is 10 gigabytes

213
00:07:25,290 --> 00:07:27,690
because that's the smallest block of data

214
00:07:27,690 --> 00:07:29,300
that we have in DocumentDB.

215
00:07:30,180 --> 00:07:32,460
Now this storage device...

216
00:07:32,460 --> 00:07:36,150
The storage layer is designed
around handling failures.

217
00:07:36,150 --> 00:07:38,730
It can lose an entire availability zone

218
00:07:38,730 --> 00:07:41,690
and a block of data in another AZ

219
00:07:41,690 --> 00:07:44,970
and still be accessible.

220
00:07:44,970 --> 00:07:46,050
We have a core model,

221
00:07:46,050 --> 00:07:48,300
that means that in order
to have a successful write,

222
00:07:48,300 --> 00:07:50,070
we have waited for an acknowledgement

223
00:07:50,070 --> 00:07:52,620
of four out of those six blocks.

224
00:07:52,620 --> 00:07:53,550
And when we...

225
00:07:53,550 --> 00:07:55,410
So basically your
application sends a write,

226
00:07:55,410 --> 00:07:56,430
we write to four blocks.

227
00:07:56,430 --> 00:07:58,260
As soon as four blocks
have acknowledged it,

228
00:07:58,260 --> 00:08:00,030
we tell the application you're good to go.

229
00:08:00,030 --> 00:08:01,800
We have the data and at the same time,

230
00:08:01,800 --> 00:08:03,420
we write those last two blocks to give you

231
00:08:03,420 --> 00:08:05,250
those six copies of data.

232
00:08:05,250 --> 00:08:06,840
Now for reads,

233
00:08:06,840 --> 00:08:09,540
we're just looking for
three outta the six blocks,

234
00:08:09,540 --> 00:08:10,560
which is what you see here.

235
00:08:10,560 --> 00:08:13,530
And honestly, for recovery
purposes for reading,

236
00:08:13,530 --> 00:08:15,840
we're really just looking for
one healthy node to come back

237
00:08:15,840 --> 00:08:18,030
and give us that data back.

238
00:08:18,030 --> 00:08:19,770
Now in this situation,

239
00:08:19,770 --> 00:08:21,210
we have that worst case scenario,

240
00:08:21,210 --> 00:08:22,140
that failure situation.

241
00:08:22,140 --> 00:08:22,973
And guess what?

242
00:08:22,973 --> 00:08:24,690
The database is still available

243
00:08:24,690 --> 00:08:26,223
because of this core model.

244
00:08:27,090 --> 00:08:30,240
The storage volume has a
self-healing architecture.

245
00:08:30,240 --> 00:08:32,850
That means that if a block is down,

246
00:08:32,850 --> 00:08:34,860
it's failed or it's
gone under maintenance,

247
00:08:34,860 --> 00:08:36,310
whatever, it's not available,

248
00:08:37,230 --> 00:08:39,240
the storage nodes actually
use a peer-to-peer

249
00:08:39,240 --> 00:08:42,450
gossip protocol where they're
able to talk to each other

250
00:08:42,450 --> 00:08:44,940
and rebalance that data among themselves

251
00:08:44,940 --> 00:08:48,000
while it's getting those
other data blocks back.

252
00:08:48,000 --> 00:08:50,940
Other cool thing about this is
traditional database systems.

253
00:08:50,940 --> 00:08:53,010
If you have 10 terabytes of data

254
00:08:53,010 --> 00:08:54,483
and that node goes down,

255
00:08:55,350 --> 00:08:57,300
you're recovering 10 terabytes of data.

256
00:08:57,300 --> 00:08:58,830
That can take a minute or two, right?

257
00:08:58,830 --> 00:09:01,710
That might take a second to get that back.

258
00:09:01,710 --> 00:09:03,210
Our storage blocks are 10 gigs,

259
00:09:03,210 --> 00:09:06,150
so the meantime to
recovery is much faster.

260
00:09:06,150 --> 00:09:09,750
It takes a lot less time to
restore that 10 gigs of data,

261
00:09:09,750 --> 00:09:12,480
all of this without having to do anything.

262
00:09:12,480 --> 00:09:15,330
This is all handled
for you in the backend.

263
00:09:15,330 --> 00:09:18,900
Last thing I wanna talk about
is the checkpoint processes.

264
00:09:18,900 --> 00:09:21,630
Databases use a regular
checkpointing process

265
00:09:21,630 --> 00:09:23,010
and nearly all of them,

266
00:09:23,010 --> 00:09:24,570
it's a single threaded process.

267
00:09:24,570 --> 00:09:28,080
The writer writes block
after block after block.

268
00:09:28,080 --> 00:09:29,310
With DocumentDB,

269
00:09:29,310 --> 00:09:30,930
with this attributed storage model,

270
00:09:30,930 --> 00:09:34,530
we can actually make these in parallel

271
00:09:34,530 --> 00:09:36,630
and we can also recover in parallel

272
00:09:36,630 --> 00:09:38,220
and that makes that recovery process

273
00:09:38,220 --> 00:09:41,250
a whole lot faster than most
traditional database systems.

274
00:09:41,250 --> 00:09:43,500
Now you would think that, again,

275
00:09:43,500 --> 00:09:45,270
like traditional database systems that

276
00:09:45,270 --> 00:09:46,980
readers would have to go all the way

277
00:09:46,980 --> 00:09:48,540
back to that distributed storage volume

278
00:09:48,540 --> 00:09:51,300
to get new data as data's updated

279
00:09:51,300 --> 00:09:52,860
and that would make reading slower.

280
00:09:52,860 --> 00:09:55,680
But again, with DocumentDB,

281
00:09:55,680 --> 00:09:58,350
what we've designed is we send updates

282
00:09:58,350 --> 00:09:59,940
to distributed storage volume

283
00:09:59,940 --> 00:10:01,110
and then we send 'em directly

284
00:10:01,110 --> 00:10:04,200
to the reader instances
via write ahead logs.

285
00:10:04,200 --> 00:10:06,660
So readers will look at those changes

286
00:10:06,660 --> 00:10:08,250
and if they have 'em in their storage,

287
00:10:08,250 --> 00:10:09,180
they apply 'em.

288
00:10:09,180 --> 00:10:10,590
If not, they ignore 'em.

289
00:10:10,590 --> 00:10:11,820
In other words,

290
00:10:11,820 --> 00:10:13,260
if I have a document

291
00:10:13,260 --> 00:10:15,310
that says Cody's favorite color is purple

292
00:10:16,320 --> 00:10:17,640
and I do an update,

293
00:10:17,640 --> 00:10:19,290
well, that update goes
to the writer instance.

294
00:10:19,290 --> 00:10:20,347
The writer gets it and says,

295
00:10:20,347 --> 00:10:22,560
"Okay, Cody's favorite color is now red.

296
00:10:22,560 --> 00:10:24,600
I'm gonna write that in storage volume."

297
00:10:24,600 --> 00:10:26,400
The writer's then gonna
tell the secondary,

298
00:10:26,400 --> 00:10:27,233
he's gonna say,

299
00:10:27,233 --> 00:10:29,220
"Hey, I have an update to Cody's document.

300
00:10:29,220 --> 00:10:31,560
If you have that memory, update it."

301
00:10:31,560 --> 00:10:33,787
The reader instance is
gonna look at it and says,

302
00:10:33,787 --> 00:10:34,740
"Yes, I have that.

303
00:10:34,740 --> 00:10:37,320
I'm gonna update my information
that I have in my cache."

304
00:10:37,320 --> 00:10:38,880
If it doesn't just throws it away,

305
00:10:38,880 --> 00:10:40,260
it ignores it, right?

306
00:10:40,260 --> 00:10:43,020
That's gonna lower your
replication lag as well.

307
00:10:43,020 --> 00:10:45,690
But it also shows you that
we don't dirty the cache

308
00:10:45,690 --> 00:10:47,340
of reader instances from the primaries.

309
00:10:47,340 --> 00:10:50,313
We keep these caches completely
separate from each other.

310
00:10:51,270 --> 00:10:53,940
So now that we have a background
done in our architecture,

311
00:10:53,940 --> 00:10:55,260
let's talk about these features.

312
00:10:55,260 --> 00:10:56,490
And the first one I wanna talk about

313
00:10:56,490 --> 00:10:58,053
is this enhanced compression.

314
00:10:58,980 --> 00:11:00,360
Two years ago,

315
00:11:00,360 --> 00:11:01,290
I think it was two years ago,

316
00:11:01,290 --> 00:11:04,560
we released LZ4
compression with DocumentDB

317
00:11:04,560 --> 00:11:06,420
and this made a pretty significant impact

318
00:11:06,420 --> 00:11:08,610
for your storage volume

319
00:11:08,610 --> 00:11:10,050
'cause we didn't have any
compression before that.

320
00:11:10,050 --> 00:11:14,070
This was the first kind of
compression that we released out.

321
00:11:14,070 --> 00:11:17,610
This year we've released
Zstandard dictionary compression

322
00:11:17,610 --> 00:11:20,310
and this is a very
significant step forward

323
00:11:20,310 --> 00:11:23,790
with the storage savings
you can see with DocumentDB.

324
00:11:23,790 --> 00:11:25,620
Now with LZ4,

325
00:11:25,620 --> 00:11:28,200
the biggest gains were
with large documents.

326
00:11:28,200 --> 00:11:29,310
The larger the object is,

327
00:11:29,310 --> 00:11:30,750
the better it compresses.

328
00:11:30,750 --> 00:11:32,040
But with Zstandard,

329
00:11:32,040 --> 00:11:36,720
we're seeing significant gains
even in smaller documents.

330
00:11:36,720 --> 00:11:40,470
LZ4 has to analyze each
document independently.

331
00:11:40,470 --> 00:11:42,330
It doesn't look at a
broad range of documents.

332
00:11:42,330 --> 00:11:45,180
It looks at each one
that it's coming across.

333
00:11:45,180 --> 00:11:49,530
LZ4 or with Zstandard,

334
00:11:49,530 --> 00:11:52,530
we're pre analyzing common field patterns

335
00:11:52,530 --> 00:11:54,660
and this is a dictionary of these.

336
00:11:54,660 --> 00:11:57,480
So we're seeing highly
efficient compression

337
00:11:57,480 --> 00:11:59,850
when you have repeated fill
names, repeated schemas,

338
00:11:59,850 --> 00:12:04,170
and this all translates
to significant storage,

339
00:12:04,170 --> 00:12:05,310
cost decreases,

340
00:12:05,310 --> 00:12:06,630
a couple other performance increases

341
00:12:06,630 --> 00:12:08,850
that we'll talk about in a second.

342
00:12:08,850 --> 00:12:10,230
All right, that's the marketing slide.

343
00:12:10,230 --> 00:12:11,063
What does this mean?

344
00:12:11,063 --> 00:12:11,970
What does this look like?

345
00:12:11,970 --> 00:12:15,690
So here we have a sample schema, right?

346
00:12:15,690 --> 00:12:16,890
Dictionary compression

347
00:12:16,890 --> 00:12:19,410
is going to replace
frequently occurring values

348
00:12:19,410 --> 00:12:21,630
with references to a dictionary.

349
00:12:21,630 --> 00:12:23,490
So in this example,

350
00:12:23,490 --> 00:12:26,850
we have these very
verbose field names like

351
00:12:26,850 --> 00:12:28,450
customer_identification_number

352
00:12:29,308 --> 00:12:30,480
and product_description_text, right?

353
00:12:30,480 --> 00:12:32,400
These were really long.

354
00:12:32,400 --> 00:12:33,750
I'm gonna pause here for a second

355
00:12:33,750 --> 00:12:34,770
because I wanna take a segue

356
00:12:34,770 --> 00:12:36,900
before we go down the compression route.

357
00:12:36,900 --> 00:12:38,400
Does anybody see an issue with...

358
00:12:38,400 --> 00:12:39,930
Well, maybe issue is not the right word.

359
00:12:39,930 --> 00:12:41,850
Does anybody see an anti-pattern here?

360
00:12:41,850 --> 00:12:43,530
Or maybe something that we can improve

361
00:12:43,530 --> 00:12:45,390
even without compression?

362
00:12:45,390 --> 00:12:46,740
What could we do with this?

363
00:12:48,180 --> 00:12:49,770
JSON is great.

364
00:12:49,770 --> 00:12:51,270
JSON data model is fantastic

365
00:12:51,270 --> 00:12:53,040
because it's very easy to parse.

366
00:12:53,040 --> 00:12:54,300
It's human readable.

367
00:12:54,300 --> 00:12:57,240
We can all look at this and
determine what it's doing,

368
00:12:57,240 --> 00:12:59,070
what the data is, right?

369
00:12:59,070 --> 00:12:59,970
It's very helpful.

370
00:13:00,810 --> 00:13:01,770
But you have to remember

371
00:13:01,770 --> 00:13:04,170
that we're taking up a lot of extra space

372
00:13:04,170 --> 00:13:05,640
with these verbose names, right?

373
00:13:05,640 --> 00:13:07,800
Every single character
that we have in here

374
00:13:07,800 --> 00:13:10,200
is more bytes that we're
adding to our document.

375
00:13:10,200 --> 00:13:13,860
Simplifying field names
can save us a ton of space.

376
00:13:13,860 --> 00:13:16,680
So for example, if we shorten these,

377
00:13:16,680 --> 00:13:19,380
we made customer_identification_number,
cust_id.

378
00:13:19,380 --> 00:13:22,110
We're saving 23 bytes per document

379
00:13:22,110 --> 00:13:24,090
just in shortening that field name.

380
00:13:24,090 --> 00:13:25,170
Product_description_text,

381
00:13:25,170 --> 00:13:26,520
we shortened that to four bytes, right?

382
00:13:26,520 --> 00:13:29,760
We just saved 20 bytes
per document, right?

383
00:13:29,760 --> 00:13:32,460
Imagine you had a
relatively small collection

384
00:13:32,460 --> 00:13:34,920
of about 10 million of these documents.

385
00:13:34,920 --> 00:13:37,080
We do the same thing for every
single one of those fields.

386
00:13:37,080 --> 00:13:37,913
Guess what?

387
00:13:37,913 --> 00:13:39,870
We just saved 60% of our storage

388
00:13:39,870 --> 00:13:42,750
on that document by using
shorter field names.

389
00:13:42,750 --> 00:13:44,730
We haven't even brought
compression into this, right?

390
00:13:44,730 --> 00:13:47,520
We're just saying shorten
those field names.

391
00:13:47,520 --> 00:13:49,080
Something to keep in mind, right?

392
00:13:49,080 --> 00:13:50,340
If you go back and look at your documents,

393
00:13:50,340 --> 00:13:51,173
think about this,

394
00:13:51,173 --> 00:13:52,006
look at your field names.

395
00:13:52,006 --> 00:13:53,910
They don't have to be verbose.

396
00:13:53,910 --> 00:13:55,260
It's good that they're human readable,

397
00:13:55,260 --> 00:13:57,990
but you and I are not reading at the speed

398
00:13:57,990 --> 00:14:00,120
and frequency that most of our workloads

399
00:14:00,120 --> 00:14:01,350
are running through.

400
00:14:01,350 --> 00:14:02,850
All right, so that being said,

401
00:14:05,400 --> 00:14:07,860
this is showing you what that looks like.

402
00:14:07,860 --> 00:14:09,720
So the dictionary compressions
are gonna go through

403
00:14:09,720 --> 00:14:13,080
and it's gonna create a mapping
of those common field names

404
00:14:13,080 --> 00:14:14,640
and it's gonna replace them.

405
00:14:14,640 --> 00:14:19,640
Now the data that's stored in
DocumentDB when you use this,

406
00:14:20,040 --> 00:14:21,390
is storing the references.

407
00:14:21,390 --> 00:14:23,730
It's not storing the full field,

408
00:14:23,730 --> 00:14:25,830
it's storing just the references to that.

409
00:14:25,830 --> 00:14:27,420
Essentially it's building this very

410
00:14:27,420 --> 00:14:29,190
sophisticated dictionary

411
00:14:29,190 --> 00:14:31,350
by analyzing this large sample set.

412
00:14:31,350 --> 00:14:33,450
You have to have at least a 1000 documents

413
00:14:33,450 --> 00:14:34,650
before it kicks in.

414
00:14:34,650 --> 00:14:37,110
So as soon as you turn it on,

415
00:14:37,110 --> 00:14:38,850
until you get to that 1000 documents,

416
00:14:38,850 --> 00:14:40,020
you're not gonna have that dictionary

417
00:14:40,020 --> 00:14:42,120
and then it'll create it.

418
00:14:42,120 --> 00:14:44,040
It's going to...

419
00:14:44,040 --> 00:14:45,450
You know, with...

420
00:14:45,450 --> 00:14:46,680
When you have millions of documents,

421
00:14:46,680 --> 00:14:49,860
it's gonna see that customer
identification number occurs

422
00:14:49,860 --> 00:14:51,120
in predictable positions,

423
00:14:51,120 --> 00:14:52,410
in predictable context,

424
00:14:52,410 --> 00:14:55,410
and it's gonna create a more
efficient coating standard

425
00:14:55,410 --> 00:14:56,283
because of that.

426
00:14:57,480 --> 00:14:59,100
We did some tests.

427
00:14:59,100 --> 00:15:00,120
We'll start off with red,

428
00:15:00,120 --> 00:15:01,080
because red is negative,

429
00:15:01,080 --> 00:15:01,950
red means bad, right?

430
00:15:01,950 --> 00:15:06,240
What we see in red is the CPU usage

431
00:15:06,240 --> 00:15:09,693
or CPU utilization when we
turn on Z standard compression,

432
00:15:11,220 --> 00:15:13,084
it went up by about 17%.

433
00:15:13,084 --> 00:15:14,850
It went up from 59% to 69%.

434
00:15:14,850 --> 00:15:17,730
Well, clearly compression
isn't free, right?

435
00:15:17,730 --> 00:15:22,320
That CPU represents that
computational cost of compressing

436
00:15:22,320 --> 00:15:24,450
and decompressing those documents.

437
00:15:24,450 --> 00:15:27,300
Next, the obvious impact
here is the blue line, right?

438
00:15:27,300 --> 00:15:29,400
Even versus LZ4,

439
00:15:29,400 --> 00:15:31,620
the amount of storage
that we're keeping on disk

440
00:15:31,620 --> 00:15:33,990
with this compression
is significantly lower.

441
00:15:33,990 --> 00:15:37,740
We dropped from about three
40 gigs down to 125 gigs.

442
00:15:37,740 --> 00:15:39,540
And then finally what we see in red,

443
00:15:39,540 --> 00:15:40,830
purple, pink, I don't know,

444
00:15:40,830 --> 00:15:42,180
it's kind of a weird color.

445
00:15:42,180 --> 00:15:44,520
The same color as your
headphones, I think.

446
00:15:44,520 --> 00:15:46,590
We saw that write throughput went down,

447
00:15:46,590 --> 00:15:48,990
from 142 megabits to 85.

448
00:15:48,990 --> 00:15:52,230
But what's cool here is it went down

449
00:15:52,230 --> 00:15:53,820
and our operations increased.

450
00:15:53,820 --> 00:15:56,160
So we were able to increase from 92,000

451
00:15:56,160 --> 00:15:59,790
inserts per second to
143,000 inserts per second

452
00:15:59,790 --> 00:16:01,200
while lowering this throughput.

453
00:16:01,200 --> 00:16:04,323
So you're getting
improvements across the board.

454
00:16:05,250 --> 00:16:06,960
We also wanted to test how this worked

455
00:16:06,960 --> 00:16:08,400
with different schemas, right?

456
00:16:08,400 --> 00:16:11,970
I told you that LZ4 is
great with big documents.

457
00:16:11,970 --> 00:16:13,770
Zstandard is great in small documents.

458
00:16:13,770 --> 00:16:16,770
So we started off with
the GitHub user database.

459
00:16:16,770 --> 00:16:20,400
This is a publicly available data set

460
00:16:20,400 --> 00:16:23,130
and documents here are
about one kilobytes in size

461
00:16:23,130 --> 00:16:25,860
and when we use Zstandard over LZ4,

462
00:16:25,860 --> 00:16:29,550
we saw a greater than 3x
improvement in compression ratio.

463
00:16:29,550 --> 00:16:31,290
We also created our own retail schema

464
00:16:31,290 --> 00:16:32,790
of about two kilobyte documents

465
00:16:32,790 --> 00:16:36,270
and we saw about a 2x increase
in that compression ratio.

466
00:16:36,270 --> 00:16:37,380
At the very end,

467
00:16:37,380 --> 00:16:39,150
we have this large metadata,

468
00:16:39,150 --> 00:16:42,150
13 megabyte documents and
we saw a compression ratio

469
00:16:42,150 --> 00:16:47,150
go from about 18 to one
with LZ4 to 147 to one.

470
00:16:47,280 --> 00:16:48,113
Imagine that,

471
00:16:48,113 --> 00:16:51,270
your 13 megabyte document just
went down to 90 kilobytes.

472
00:16:51,270 --> 00:16:53,700
That's a very substantial decrease.

473
00:16:53,700 --> 00:16:55,560
What's cool about that very last graph,

474
00:16:55,560 --> 00:16:56,940
that last example,

475
00:16:56,940 --> 00:16:58,020
that's not a test.

476
00:16:58,020 --> 00:16:59,910
That is a production
customer that did that.

477
00:16:59,910 --> 00:17:00,840
When we released that,

478
00:17:00,840 --> 00:17:02,760
they applied this to their workload

479
00:17:02,760 --> 00:17:05,790
and they were able to
see an 8x increase...

480
00:17:05,790 --> 00:17:08,343
Not from uncompressed versus LZ4.

481
00:17:10,050 --> 00:17:12,688
Now at enterprise scale,

482
00:17:12,688 --> 00:17:14,520
we'll talk about the CPU again.

483
00:17:14,520 --> 00:17:15,540
the negative part, right,

484
00:17:15,540 --> 00:17:16,470
I wanna focus on that.

485
00:17:16,470 --> 00:17:18,120
But at enterprise scale,

486
00:17:18,120 --> 00:17:21,450
storage cost often exceed the CPU cost

487
00:17:21,450 --> 00:17:24,120
of slightly slower compression.

488
00:17:24,120 --> 00:17:24,953
So for example,

489
00:17:24,953 --> 00:17:27,900
a 40% compression ratio can translate into

490
00:17:27,900 --> 00:17:30,810
pretty substantial cost reductions

491
00:17:30,810 --> 00:17:33,480
for what you're paying for storage.

492
00:17:33,480 --> 00:17:36,180
Beyond direct storage cost,

493
00:17:36,180 --> 00:17:38,190
compression reduces your I/O operation,

494
00:17:38,190 --> 00:17:40,110
it lowers your backup footprint

495
00:17:40,110 --> 00:17:43,290
and it even decreases your
network bandwidth usage.

496
00:17:43,290 --> 00:17:46,380
Compressed documents require
fewer I/O operations.

497
00:17:46,380 --> 00:17:49,260
This is because more logical data

498
00:17:49,260 --> 00:17:52,653
fits within each physical I/O operation.

499
00:17:53,640 --> 00:17:55,710
Compressed document gives you better

500
00:17:55,710 --> 00:17:56,790
cache utilization, right?

501
00:17:56,790 --> 00:17:58,830
Because you can fit more of
these documents in cache.

502
00:17:58,830 --> 00:18:01,890
We keep documents compressed
on storage over the wire

503
00:18:01,890 --> 00:18:03,870
and in your cache on
those compute instances.

504
00:18:03,870 --> 00:18:07,890
So what that means is that might decrease

505
00:18:07,890 --> 00:18:10,080
the need for your compute instance size.

506
00:18:10,080 --> 00:18:12,240
So that gives you additional cost savings

507
00:18:12,240 --> 00:18:13,800
through cost optimizations.

508
00:18:13,800 --> 00:18:15,930
Here we're just seeing
what the cost difference is

509
00:18:15,930 --> 00:18:17,190
because that's the physical thing.

510
00:18:17,190 --> 00:18:18,510
Now everything else that
we're talking about,

511
00:18:18,510 --> 00:18:20,310
your cache, your compute size,

512
00:18:20,310 --> 00:18:21,810
that depends on your workload.

513
00:18:23,430 --> 00:18:27,810
Here I wanna kind of stretch
your minds a little bit.

514
00:18:27,810 --> 00:18:31,890
I wanna think about this as a
multidimensional improvement.

515
00:18:31,890 --> 00:18:35,460
A couple years ago we
released I/O optimized storage

516
00:18:35,460 --> 00:18:37,710
for our customers that have very busy,

517
00:18:37,710 --> 00:18:39,630
very heavy I/O heavy workloads

518
00:18:39,630 --> 00:18:43,410
and this was really marketed
as a cost improvement, right?

519
00:18:43,410 --> 00:18:46,680
It allowed you to stop paying for I/O

520
00:18:46,680 --> 00:18:48,600
and basically get it for free.

521
00:18:48,600 --> 00:18:50,790
But the trade off is you're
paying more for storage,

522
00:18:50,790 --> 00:18:52,350
but behind the scenes there's actually

523
00:18:52,350 --> 00:18:54,030
a lot of performance increases, right?

524
00:18:54,030 --> 00:18:57,510
Throughput increases and
latency decreases for that.

525
00:18:57,510 --> 00:18:59,040
We won't go into that too much here,

526
00:18:59,040 --> 00:19:02,040
but you get cost benefits
and performance benefits.

527
00:19:02,040 --> 00:19:02,873
Now the trade off,

528
00:19:02,873 --> 00:19:04,170
what is a trade off here?

529
00:19:04,170 --> 00:19:06,270
You don't pay for I/O anymore,

530
00:19:06,270 --> 00:19:08,700
but you do pay three
times more for storage.

531
00:19:08,700 --> 00:19:11,400
So you're paying, as of right now,

532
00:19:11,400 --> 00:19:13,920
U.S. East-1, you're paying
10 cents per gigabyte

533
00:19:13,920 --> 00:19:15,390
per month with standard.

534
00:19:15,390 --> 00:19:17,520
With I/O optimize, you're paying 30 cents.

535
00:19:17,520 --> 00:19:19,023
So three times more.

536
00:19:20,400 --> 00:19:21,363
In this example,

537
00:19:22,350 --> 00:19:26,250
with LZ4, we were at 1.7
terabytes compressed,

538
00:19:26,250 --> 00:19:29,850
but with Zstandard we went
down to 500 gigabytes.

539
00:19:29,850 --> 00:19:32,040
What that means is that
our inflection point

540
00:19:32,040 --> 00:19:35,340
for where I/O optimize can make sense

541
00:19:35,340 --> 00:19:36,780
goes down significantly.

542
00:19:36,780 --> 00:19:39,420
So our break even point for standard

543
00:19:39,420 --> 00:19:42,210
was about 1.7 billion
operations per month,

544
00:19:42,210 --> 00:19:44,433
about 660 operations per second.

545
00:19:45,510 --> 00:19:46,980
Once we use Zstandard,

546
00:19:46,980 --> 00:19:49,140
that inflection point lowered

547
00:19:49,140 --> 00:19:51,930
to about 500 million operations per month

548
00:19:51,930 --> 00:19:54,420
or about 193 operations per second.

549
00:19:54,420 --> 00:19:57,690
So you don't have to have
this massive enterprise scale

550
00:19:57,690 --> 00:20:00,060
to start recognizing these cost savings

551
00:20:00,060 --> 00:20:02,340
when you start combining these.

552
00:20:02,340 --> 00:20:04,800
Zstandard allows you to combine

553
00:20:04,800 --> 00:20:06,147
with other DocumentDB features

554
00:20:06,147 --> 00:20:08,040
and really multiply your savings

555
00:20:08,040 --> 00:20:10,353
and possibly even
multiply your performance.

556
00:20:12,060 --> 00:20:12,893
Where do you use it?

557
00:20:12,893 --> 00:20:13,800
Where does this work best?

558
00:20:13,800 --> 00:20:16,110
Well, dictionary
compression is really good

559
00:20:16,110 --> 00:20:17,820
when you have repetitive field names

560
00:20:17,820 --> 00:20:19,680
and repetitive structured elements.

561
00:20:19,680 --> 00:20:23,130
Collections that have deeply nested fields

562
00:20:23,130 --> 00:20:25,410
with consistent schemas,
standardized schemas.

563
00:20:25,410 --> 00:20:26,520
They're gonna get the
best outta this, right?

564
00:20:26,520 --> 00:20:28,620
Because they have field names

565
00:20:28,620 --> 00:20:29,910
that are in a predictable spot

566
00:20:29,910 --> 00:20:30,990
and they can make the...

567
00:20:30,990 --> 00:20:32,400
The dictionary can help that.

568
00:20:32,400 --> 00:20:34,020
However, the other side of this,

569
00:20:34,020 --> 00:20:37,800
if you have a lot of documents
that have variable schemas,

570
00:20:37,800 --> 00:20:40,860
very sparse schemas or
highly variable structures,

571
00:20:40,860 --> 00:20:43,590
well, you're only gonna
see modest improvements.

572
00:20:43,590 --> 00:20:44,700
You're still gonna see improvement,

573
00:20:44,700 --> 00:20:48,270
but not nearly as much as if
you had standardized schema.

574
00:20:48,270 --> 00:20:51,600
Compression is usually
better with larger documents.

575
00:20:51,600 --> 00:20:52,433
That's it.

576
00:20:52,433 --> 00:20:53,760
400 level session expert,

577
00:20:53,760 --> 00:20:54,593
that's what you learn.

578
00:20:54,593 --> 00:20:56,190
Compression works with big things.

579
00:20:56,190 --> 00:20:57,450
That's it, thank you, goodnight.

580
00:20:57,450 --> 00:20:58,710
No, I'm kidding.

581
00:20:58,710 --> 00:21:01,680
Compression works great
on big objects but,

582
00:21:01,680 --> 00:21:03,570
and that's where LZ4 focus.

583
00:21:03,570 --> 00:21:05,310
Zstandard works with those big documents.

584
00:21:05,310 --> 00:21:07,170
We saw it with a 13 megabyte documents,

585
00:21:07,170 --> 00:21:09,270
but also in the smaller documents.

586
00:21:09,270 --> 00:21:10,440
Read heavy workloads

587
00:21:10,440 --> 00:21:12,960
generally are gonna
benefit more from Zstandard

588
00:21:12,960 --> 00:21:16,230
than write heavy just because
there's less I/O bandwidth

589
00:21:16,230 --> 00:21:19,920
and we can process those a
lot more efficiently in cache.

590
00:21:19,920 --> 00:21:21,990
Things like high frequency point queries

591
00:21:21,990 --> 00:21:23,520
are gonna have an improved latency.

592
00:21:23,520 --> 00:21:25,320
But on the other side,

593
00:21:25,320 --> 00:21:27,600
if you're doing large range queries

594
00:21:27,600 --> 00:21:29,160
that are returning multiple documents,

595
00:21:29,160 --> 00:21:31,110
you might see a performance decrease

596
00:21:31,110 --> 00:21:33,300
and that's because of
that overhead, right?

597
00:21:33,300 --> 00:21:36,750
That compression and decompression
overhead we saw earlier.

598
00:21:36,750 --> 00:21:37,860
For write operations,

599
00:21:37,860 --> 00:21:39,990
simple inserts benefit from this

600
00:21:39,990 --> 00:21:43,380
again because of those
lower I/O operations.

601
00:21:43,380 --> 00:21:46,260
However, if you are doing any
kind of complex operations

602
00:21:46,260 --> 00:21:48,270
where you're doing
multiple writes at once,

603
00:21:48,270 --> 00:21:50,130
there's a lot more compression
that has to happen.

604
00:21:50,130 --> 00:21:51,723
Higher CPU usage.

605
00:21:52,980 --> 00:21:55,280
Oh, and we have pretty
pictures to go with it.

606
00:21:57,060 --> 00:22:00,510
All right, next Graviton4 instances.

607
00:22:00,510 --> 00:22:04,770
Is anybody using Graviton4
instance in DocumentDB yet?

608
00:22:04,770 --> 00:22:05,670
You started to raise your hand.

609
00:22:05,670 --> 00:22:06,503
You want to.

610
00:22:06,503 --> 00:22:07,600
You were like right there.

611
00:22:08,460 --> 00:22:10,770
Okay, not with DocDB, gotcha.

612
00:22:10,770 --> 00:22:12,510
So Graviton...

613
00:22:12,510 --> 00:22:13,620
Stepping away from DocumentDB,

614
00:22:13,620 --> 00:22:15,360
Graviton is unique to AWS, right?

615
00:22:15,360 --> 00:22:16,470
It was really designed

616
00:22:16,470 --> 00:22:19,420
to give you the best price
performance for cloud workloads.

617
00:22:20,700 --> 00:22:23,340
Graviton instances are really good at CPU

618
00:22:23,340 --> 00:22:24,173
intensive workloads.

619
00:22:24,173 --> 00:22:27,600
They're really good for
memory intensive workloads,

620
00:22:27,600 --> 00:22:28,710
like databases.

621
00:22:28,710 --> 00:22:29,880
So it really makes sense.

622
00:22:29,880 --> 00:22:33,060
When we released Graviton2
instances in DocumentDB,

623
00:22:33,060 --> 00:22:36,450
we saw a 30% price performance increase.

624
00:22:36,450 --> 00:22:39,060
Each Graviton instance that
we released from two to three

625
00:22:39,060 --> 00:22:42,240
and now to four brings pretty
significant performance gains

626
00:22:42,240 --> 00:22:46,200
and this fourth generation is
seeing 30% performance gains

627
00:22:46,200 --> 00:22:47,641
over Graviton3,

628
00:22:47,641 --> 00:22:50,100
not Graviton2 but Graviton3.

629
00:22:50,100 --> 00:22:52,050
So similar to when we released Graviton2,

630
00:22:52,050 --> 00:22:54,300
we're seeing pretty
significant performance impact

631
00:22:54,300 --> 00:22:57,720
to workloads and cost
benefits from using these.

632
00:22:57,720 --> 00:22:59,550
Well, what does that look like?

633
00:22:59,550 --> 00:23:02,220
So we looked at it from an in-memory

634
00:23:02,220 --> 00:23:04,470
versus out of memory workload to see

635
00:23:04,470 --> 00:23:06,450
what kind of performance
impact we could have.

636
00:23:06,450 --> 00:23:07,800
Clearly in-memory workloads

637
00:23:07,800 --> 00:23:09,993
are gonna benefit significantly from this.

638
00:23:11,400 --> 00:23:14,010
We're seeing over a 100% price performance

639
00:23:14,010 --> 00:23:18,180
increase in read workloads
and update workloads, 136%.

640
00:23:18,180 --> 00:23:20,490
Even for those workloads
that are out of memory

641
00:23:20,490 --> 00:23:21,600
where things don't fit in cache,

642
00:23:21,600 --> 00:23:23,580
you're still seeing a significant increase

643
00:23:23,580 --> 00:23:24,963
in your performance there.

644
00:23:27,030 --> 00:23:29,253
Now there is a price difference.

645
00:23:30,120 --> 00:23:31,950
We started off with R5 instances...

646
00:23:31,950 --> 00:23:33,090
We actually started off with R4.

647
00:23:33,090 --> 00:23:33,923
We'll ignore...

648
00:23:33,923 --> 00:23:34,756
So ignore those.

649
00:23:34,756 --> 00:23:35,910
We started off with R5,

650
00:23:35,910 --> 00:23:37,830
we released Graviton2, r6g,

651
00:23:37,830 --> 00:23:40,170
and we lowered the cost by 5%.

652
00:23:40,170 --> 00:23:41,910
So what was cool is

653
00:23:41,910 --> 00:23:46,710
even if I did nothing except
change my R5 instances to r6g,

654
00:23:46,710 --> 00:23:48,330
I saved money in my performance increase,

655
00:23:48,330 --> 00:23:49,650
it was dead simple.

656
00:23:49,650 --> 00:23:51,000
It made sense.

657
00:23:51,000 --> 00:23:52,710
With r8g,

658
00:23:52,710 --> 00:23:54,180
you're seeing a 5% increase.

659
00:23:54,180 --> 00:23:56,010
So essentially if you're
going from R5 to R8,

660
00:23:56,010 --> 00:23:56,843
there's no difference.

661
00:23:56,843 --> 00:23:57,676
It's staying the same.

662
00:23:57,676 --> 00:23:59,520
It went down, it went up.

663
00:23:59,520 --> 00:24:01,230
Despite the cost increase though,

664
00:24:01,230 --> 00:24:04,454
we can see a performance
increase that far exceeds

665
00:24:04,454 --> 00:24:07,560
that price premium that
you're paying for these.

666
00:24:07,560 --> 00:24:09,960
Performance gains are consistent

667
00:24:09,960 --> 00:24:11,820
across all instant sizes,

668
00:24:11,820 --> 00:24:16,710
whether it's extra large
up to 16x, extra large.

669
00:24:16,710 --> 00:24:18,750
But what you're seeing here
is this performance benefit

670
00:24:18,750 --> 00:24:20,403
by switching over to these.

671
00:24:21,810 --> 00:24:22,985
Next, Serverless.

672
00:24:22,985 --> 00:24:25,260
So no r8g users.

673
00:24:25,260 --> 00:24:27,510
Anybody using DocumentDB
Serverless currently

674
00:24:27,510 --> 00:24:29,190
or tried it out?

675
00:24:29,190 --> 00:24:31,260
Oh man, y'all have so much
to do when you get back.

676
00:24:31,260 --> 00:24:32,730
You need to try out Serverless.

677
00:24:32,730 --> 00:24:34,020
But to talk about this,

678
00:24:34,020 --> 00:24:35,160
I'm gonna hand things over to Vin

679
00:24:35,160 --> 00:24:37,320
and he's gonna walk us through that.

680
00:24:37,320 --> 00:24:38,973
- All right, thanks, Cody.

681
00:24:39,900 --> 00:24:40,733
Really awesome.

682
00:24:40,733 --> 00:24:42,110
Actually some of those things that

683
00:24:43,380 --> 00:24:46,560
Cody shared just earlier
about the performance benefits

684
00:24:46,560 --> 00:24:47,970
of r8g, we actually...

685
00:24:47,970 --> 00:24:48,803
When we launched it,

686
00:24:48,803 --> 00:24:50,970
we actually already had a
few customers come to me.

687
00:24:50,970 --> 00:24:53,130
I actually met some of
'em earlier today about

688
00:24:53,130 --> 00:24:54,960
how all they had to do was switch over

689
00:24:54,960 --> 00:24:56,910
from an r6g to r8g.

690
00:24:56,910 --> 00:24:58,650
There was no functional changes

691
00:24:58,650 --> 00:25:00,000
and they just got better price.

692
00:25:00,000 --> 00:25:01,890
In fact, some of them
were able to actually

693
00:25:01,890 --> 00:25:03,030
scale down their instance.

694
00:25:03,030 --> 00:25:04,650
So there's a cost...

695
00:25:04,650 --> 00:25:07,650
You know, indirect cost
savings there as well.

696
00:25:07,650 --> 00:25:09,900
All right, so you know for those

697
00:25:09,900 --> 00:25:10,920
who have missed the first few minutes,

698
00:25:10,920 --> 00:25:12,750
I just saw a few folks coming in later.

699
00:25:12,750 --> 00:25:13,583
My name is Vin,

700
00:25:13,583 --> 00:25:16,230
I'm one of the product
managers with DocumentDB.

701
00:25:16,230 --> 00:25:17,730
Again, and we're gonna
dive right into Serverless

702
00:25:17,730 --> 00:25:19,680
where I'm gonna provide a short overview

703
00:25:19,680 --> 00:25:22,170
and we're going to dive into
some of the inner workings

704
00:25:22,170 --> 00:25:25,173
of how we made Serverless
work for DocumentDB.

705
00:25:26,040 --> 00:25:30,060
So the core concept of Serverless
is that the system scales

706
00:25:30,060 --> 00:25:32,580
with your workload requirements, right?

707
00:25:32,580 --> 00:25:34,920
So as compared to a traditional system

708
00:25:34,920 --> 00:25:37,230
where you could either,
A, under provision,

709
00:25:37,230 --> 00:25:39,780
which causes some challenges
if you have a peak.

710
00:25:39,780 --> 00:25:42,630
So there might cause some
application challenges there

711
00:25:42,630 --> 00:25:45,780
if it goes over your system requirements.

712
00:25:45,780 --> 00:25:46,613
It would cause some challenges.

713
00:25:46,613 --> 00:25:48,270
Or you could overprovision

714
00:25:48,270 --> 00:25:50,910
again, this is gonna cause
some other types of challenges

715
00:25:50,910 --> 00:25:54,630
in terms of being inefficient
use of your resources.

716
00:25:54,630 --> 00:25:57,570
Now in between this you
could manually alter

717
00:25:57,570 --> 00:25:59,130
and change the way...

718
00:25:59,130 --> 00:26:00,930
Change your resources as you go.

719
00:26:00,930 --> 00:26:04,413
However, that is an extra step for you.

720
00:26:05,400 --> 00:26:07,890
The core idea is that
Serverless really scales

721
00:26:07,890 --> 00:26:09,480
with your workload needs

722
00:26:09,480 --> 00:26:13,503
and this is exactly what
Serverless in DocumentDB does.

723
00:26:15,060 --> 00:26:18,090
So in August we launched
DocumentDB Serverless.

724
00:26:18,090 --> 00:26:19,640
So if you haven't tried
it, I don't blame you,

725
00:26:19,640 --> 00:26:21,870
it was just right before the holidays

726
00:26:21,870 --> 00:26:23,340
and it was at the end of summer.

727
00:26:23,340 --> 00:26:25,740
Maybe y'all were coming
back from your vacation.

728
00:26:27,000 --> 00:26:29,670
So the idea is that if
you have larger workloads,

729
00:26:29,670 --> 00:26:32,490
the instance will scale
up to meet the needs

730
00:26:32,490 --> 00:26:34,590
of your instance...

731
00:26:34,590 --> 00:26:35,790
Meet the needs of your workload.

732
00:26:35,790 --> 00:26:38,550
And it could scale anywhere
from as less than one CPU

733
00:26:38,550 --> 00:26:40,260
with two gigs of memory

734
00:26:40,260 --> 00:26:43,680
all the way to 64 CPUs
with 512 gigs of memory.

735
00:26:43,680 --> 00:26:44,880
So anywhere in between,

736
00:26:44,880 --> 00:26:46,470
in fairly granular form.

737
00:26:46,470 --> 00:26:49,020
You're not making those
jumps like you do with a,

738
00:26:49,020 --> 00:26:51,630
say r6g large, which has two CPUs

739
00:26:51,630 --> 00:26:53,310
to an XL which has four CPUs,

740
00:26:53,310 --> 00:26:55,563
which is a more discreet jump.

741
00:26:56,400 --> 00:26:58,230
You can easily switch between Serverless

742
00:26:58,230 --> 00:26:59,370
and provision instances.

743
00:26:59,370 --> 00:27:02,010
So you can see here I
have a serverless instance

744
00:27:02,010 --> 00:27:02,843
in the middle there,

745
00:27:02,843 --> 00:27:04,800
it's sitting beside two other instances,

746
00:27:04,800 --> 00:27:06,000
two other replicas,

747
00:27:06,000 --> 00:27:08,400
which are different provision instances.

748
00:27:08,400 --> 00:27:09,990
What this really enables you to do

749
00:27:09,990 --> 00:27:11,700
is to adopt Serverless

750
00:27:11,700 --> 00:27:13,710
without having to do any data movement

751
00:27:13,710 --> 00:27:15,540
because of that benefit of separation

752
00:27:15,540 --> 00:27:17,250
of compute and storage.

753
00:27:17,250 --> 00:27:19,770
So whether you have a
100 terabytes of data

754
00:27:19,770 --> 00:27:21,360
or 10 gigabytes of data,

755
00:27:21,360 --> 00:27:23,130
the amount of time to switch from

756
00:27:23,130 --> 00:27:27,000
between provisioned and serverless
is pretty much constant.

757
00:27:27,000 --> 00:27:27,900
And last but not least,

758
00:27:27,900 --> 00:27:31,620
we introduced a new unit
to measure database...

759
00:27:31,620 --> 00:27:32,640
Serverless resources

760
00:27:32,640 --> 00:27:35,553
and it's gonna be called
DocumentDB Capacity Units.

761
00:27:37,200 --> 00:27:38,880
So let's talk a little
bit about that as well

762
00:27:38,880 --> 00:27:41,253
because there's some
cost implications here.

763
00:27:42,750 --> 00:27:44,460
Database Capacity Units,

764
00:27:44,460 --> 00:27:46,050
DCU for short has...

765
00:27:46,050 --> 00:27:49,740
One DCU has approximately
two gigabytes of memory,

766
00:27:49,740 --> 00:27:51,690
some CPU and network resources.

767
00:27:51,690 --> 00:27:53,250
And this comes in a package,

768
00:27:53,250 --> 00:27:55,350
that is, we don't scale your CPU,

769
00:27:55,350 --> 00:27:56,910
your memory independently.

770
00:27:56,910 --> 00:27:58,180
This comes as a package

771
00:27:59,280 --> 00:28:00,570
and on the right side you can see

772
00:28:00,570 --> 00:28:02,940
that there's a max and minimum DCU.

773
00:28:02,940 --> 00:28:04,440
When you are provisioning your cluster,

774
00:28:04,440 --> 00:28:07,680
what you could do is specify
the number of max DCUs

775
00:28:07,680 --> 00:28:10,500
or minimum DCUs to help
you control your cost

776
00:28:10,500 --> 00:28:12,990
or provide performance guarantees.

777
00:28:12,990 --> 00:28:15,660
And our system does scale in increments

778
00:28:15,660 --> 00:28:17,910
as small as half a DCU.

779
00:28:17,910 --> 00:28:20,370
So you're not scaling
between, I was saying...

780
00:28:20,370 --> 00:28:22,536
In discreet steps of say,

781
00:28:22,536 --> 00:28:26,005
you know, two vCPUs all
the way to four CPUs.

782
00:28:26,005 --> 00:28:28,660
This does more incremental scaling

783
00:28:30,180 --> 00:28:32,190
and we scale on many dimensions.

784
00:28:32,190 --> 00:28:34,410
There's an internal
algorithm where we scale on

785
00:28:34,410 --> 00:28:37,860
CPU utilization, memory pressure,

786
00:28:37,860 --> 00:28:39,120
network throughput.

787
00:28:39,120 --> 00:28:41,160
All these aspects will
contribute to scaling

788
00:28:41,160 --> 00:28:43,443
and adding more DCUs to your system.

789
00:28:45,030 --> 00:28:45,863
And the last point,

790
00:28:45,863 --> 00:28:47,890
this is actually a very important point

791
00:28:48,780 --> 00:28:49,740
and I'll tell you why later.

792
00:28:49,740 --> 00:28:51,660
So just keep a note of
this last point here,

793
00:28:51,660 --> 00:28:53,820
later in this slide I'll
be referencing this.

794
00:28:53,820 --> 00:28:55,680
And also an interesting aspect

795
00:28:55,680 --> 00:28:57,570
is that we don't scale linear.

796
00:28:57,570 --> 00:28:59,820
That is if you have a larger database,

797
00:28:59,820 --> 00:29:01,860
it will scale faster
than that of a small one.

798
00:29:01,860 --> 00:29:05,670
So let's say you have a 20 DCU instance,

799
00:29:05,670 --> 00:29:07,200
it's gonna be able to add 10 DCUs

800
00:29:07,200 --> 00:29:09,390
faster than that of a instance

801
00:29:09,390 --> 00:29:11,553
that has only one DCU as a starting point.

802
00:29:13,170 --> 00:29:15,300
All right, let's get to
dive a little bit deeper

803
00:29:15,300 --> 00:29:18,450
on actually some of the
implementation details.

804
00:29:18,450 --> 00:29:22,530
Actually one of the key aspects
how we made Serverless work

805
00:29:22,530 --> 00:29:23,790
for DocumentDB

806
00:29:23,790 --> 00:29:25,860
and that is actually
one thing specifically

807
00:29:25,860 --> 00:29:29,640
I wanna talk about is
buffer cache resizing.

808
00:29:29,640 --> 00:29:31,320
Actually for a while now,

809
00:29:31,320 --> 00:29:33,540
you could actually already today

810
00:29:33,540 --> 00:29:35,700
dynamically change your resources

811
00:29:35,700 --> 00:29:36,780
applied to a virtual system.

812
00:29:36,780 --> 00:29:38,760
So if you're working with Cgroups in Linux

813
00:29:38,760 --> 00:29:41,790
or containers which is built
on the concept of Cgroups,

814
00:29:41,790 --> 00:29:44,940
you could actually change the setting

815
00:29:44,940 --> 00:29:48,090
of how many CPUs and how much
memory is being applied to

816
00:29:48,090 --> 00:29:50,070
your container in Kubernetes.

817
00:29:50,070 --> 00:29:52,410
However, the challenge with a database

818
00:29:52,410 --> 00:29:54,000
is if you just start...

819
00:29:54,000 --> 00:29:54,937
If you just decide to,

820
00:29:54,937 --> 00:29:57,360
"Hey look, I'm gonna just
reduce the amount of memory,"

821
00:29:57,360 --> 00:29:58,590
this could cause a lot of thrashing.

822
00:29:58,590 --> 00:30:01,260
That is, you may be
evicting the wrong pages

823
00:30:01,260 --> 00:30:03,240
and that's gonna cause a
lot of performance issues

824
00:30:03,240 --> 00:30:05,103
and it's gonna drive up I/Os as well.

825
00:30:06,390 --> 00:30:08,138
So let's talk a little bit about,

826
00:30:08,138 --> 00:30:08,971
you know, this diagram.

827
00:30:08,971 --> 00:30:12,810
So here in the bars you see
from the pink to the red,

828
00:30:12,810 --> 00:30:16,110
these represent pages
inside our buffer pool.

829
00:30:16,110 --> 00:30:17,400
The darker the red,

830
00:30:17,400 --> 00:30:19,770
the more frequently it is accessed

831
00:30:19,770 --> 00:30:21,600
and one of the things that we follow

832
00:30:21,600 --> 00:30:22,860
that we have internal algorithm of

833
00:30:22,860 --> 00:30:24,840
how we sort the pages is

834
00:30:24,840 --> 00:30:27,390
a combination of both least recently used

835
00:30:27,390 --> 00:30:29,640
and least frequently used.

836
00:30:29,640 --> 00:30:31,980
So let's say I do a lot of frequent reads

837
00:30:31,980 --> 00:30:34,200
from my buffer pool, my application,

838
00:30:34,200 --> 00:30:35,850
those are in the deep red.

839
00:30:35,850 --> 00:30:38,450
And let's say I need to go
to disc to get some data.

840
00:30:39,420 --> 00:30:42,090
So here I'm gonna go
fetch some data from disc

841
00:30:42,090 --> 00:30:44,130
and notice how the way we sort it

842
00:30:44,130 --> 00:30:45,750
is not in the very front,

843
00:30:45,750 --> 00:30:46,740
that is, it may...

844
00:30:46,740 --> 00:30:50,370
Just because we accessed it recently,

845
00:30:50,370 --> 00:30:52,590
doesn't mean we access it very frequently.

846
00:30:52,590 --> 00:30:55,710
So later on when we do
any resizing activities,

847
00:30:55,710 --> 00:30:58,230
we may just truncate those pages

848
00:30:58,230 --> 00:31:00,840
before we do more frequent ones.

849
00:31:00,840 --> 00:31:01,790
More frequent ones.

850
00:31:03,030 --> 00:31:04,290
All right, let's take a
look at how we shrink.

851
00:31:04,290 --> 00:31:05,520
And it's pretty simple.

852
00:31:05,520 --> 00:31:08,272
So for pages that haven't
been used for some time

853
00:31:08,272 --> 00:31:09,105
and it is...

854
00:31:09,105 --> 00:31:12,600
So based on a combination
of LRU and and LFU,

855
00:31:12,600 --> 00:31:14,250
we're just gonna truncate
and drop those pages

856
00:31:14,250 --> 00:31:17,640
from our buffer pool and
this is gonna enable us

857
00:31:17,640 --> 00:31:19,200
to shrink our buffer pool

858
00:31:19,200 --> 00:31:21,450
and resize our buffer pool.

859
00:31:21,450 --> 00:31:23,730
So this is what you see when
Serverless scales back down

860
00:31:23,730 --> 00:31:25,560
when you're not using that many resources.

861
00:31:25,560 --> 00:31:27,120
This is how it happens.

862
00:31:27,120 --> 00:31:28,590
And again, I really wanna stress that

863
00:31:28,590 --> 00:31:31,560
this is a little bit of
magic here because, again,

864
00:31:31,560 --> 00:31:34,770
if we evict the wrong
pages very frequently,

865
00:31:34,770 --> 00:31:38,610
it's gonna drive up I/Os and
this strategy really ensures

866
00:31:38,610 --> 00:31:40,080
that we're not doing some of those things

867
00:31:40,080 --> 00:31:42,213
that cost performance and cost issues.

868
00:31:43,350 --> 00:31:45,240
Let's go into an example here.

869
00:31:45,240 --> 00:31:47,580
I wanna show you two workloads.

870
00:31:47,580 --> 00:31:50,220
One, there's the workload
against a provisioned instance

871
00:31:50,220 --> 00:31:53,250
and the other workload is
against a serverless instance.

872
00:31:53,250 --> 00:31:56,130
The view you are seeing
here is the view we have...

873
00:31:56,130 --> 00:31:58,170
A tool called performance insights.

874
00:31:58,170 --> 00:32:01,380
It measures how much
load is on your instance

875
00:32:01,380 --> 00:32:03,660
and that dotted line you see in the middle

876
00:32:03,660 --> 00:32:07,650
is how many resources measured
its vCPUs on your instance,

877
00:32:07,650 --> 00:32:08,670
your system.

878
00:32:08,670 --> 00:32:10,950
And the reason why this vCPU is...

879
00:32:10,950 --> 00:32:14,100
Why we choose vCPU is because, one,

880
00:32:14,100 --> 00:32:14,933
if you have...

881
00:32:14,933 --> 00:32:16,350
This represents approximately

882
00:32:16,350 --> 00:32:19,170
how many processes do you
go around at any given time.

883
00:32:19,170 --> 00:32:21,120
So you can see in this diagram that

884
00:32:21,120 --> 00:32:22,860
most of the time we have about three

885
00:32:22,860 --> 00:32:24,270
or four average active sessions.

886
00:32:24,270 --> 00:32:27,060
That is, there are three or four queries

887
00:32:27,060 --> 00:32:29,580
that want to be run at
any given time on average.

888
00:32:29,580 --> 00:32:33,660
However, we only have two vCPUs,

889
00:32:33,660 --> 00:32:36,210
even if we add more load to our database,

890
00:32:36,210 --> 00:32:37,710
it's static.

891
00:32:37,710 --> 00:32:39,960
It's not any adding any more...

892
00:32:39,960 --> 00:32:41,640
It's not changing the amount of resources.

893
00:32:41,640 --> 00:32:43,950
And what this means is
that I may be experiencing

894
00:32:43,950 --> 00:32:45,150
additional query latency

895
00:32:45,150 --> 00:32:47,490
even if my indexes are in place,

896
00:32:47,490 --> 00:32:49,140
even if my date...

897
00:32:49,140 --> 00:32:50,970
My queries are optimized,

898
00:32:50,970 --> 00:32:53,760
my database is under provisioned.

899
00:32:53,760 --> 00:32:55,830
Let's take a look at it from another view.

900
00:32:55,830 --> 00:32:57,510
So we could slice this by weights

901
00:32:57,510 --> 00:32:59,460
and what this tells me is

902
00:32:59,460 --> 00:33:02,850
what is my process spending
a lot of its time doing?

903
00:33:02,850 --> 00:33:05,130
And here notice how there's a lot of red,

904
00:33:05,130 --> 00:33:07,590
depending on the color may
be brown from the back.

905
00:33:07,590 --> 00:33:09,030
This is I/Os.

906
00:33:09,030 --> 00:33:11,370
And what this in indicates to me is that

907
00:33:11,370 --> 00:33:13,530
not only am I trying to
do three or four things

908
00:33:13,530 --> 00:33:14,970
at any given time

909
00:33:14,970 --> 00:33:17,370
and I only have two vCPUs,

910
00:33:17,370 --> 00:33:19,470
I'm actually spending a
lot of time going to disc,

911
00:33:19,470 --> 00:33:21,480
spending a lot of time going to I/O.

912
00:33:21,480 --> 00:33:22,807
To me this indicates that,

913
00:33:22,807 --> 00:33:25,110
"Hey, maybe I don't have enough memory,

914
00:33:25,110 --> 00:33:27,900
I don't have enough memory for
the working set of my query."

915
00:33:27,900 --> 00:33:30,600
So I'm actually being very inefficient

916
00:33:30,600 --> 00:33:33,780
and it's gonna increase the
query latency of my workload.

917
00:33:33,780 --> 00:33:35,490
Now let's take a look at the same workload

918
00:33:35,490 --> 00:33:36,933
on a serverless instance.

919
00:33:38,040 --> 00:33:41,070
So on the bottom there, on the bar graphs,

920
00:33:41,070 --> 00:33:43,290
you see that it's about
three, that's how many...

921
00:33:43,290 --> 00:33:45,510
Again, three average active sessions.

922
00:33:45,510 --> 00:33:48,750
But you notice that the bar already at 16.

923
00:33:48,750 --> 00:33:50,790
So what this tells me is, "Hey, look,

924
00:33:50,790 --> 00:33:53,730
the system needed more resources

925
00:33:53,730 --> 00:33:55,500
perhaps probably for memory, right?"

926
00:33:55,500 --> 00:33:56,333
So the working,

927
00:33:56,333 --> 00:33:59,010
so even though I'm only
running three processes

928
00:33:59,010 --> 00:34:00,240
at any given time,

929
00:34:00,240 --> 00:34:04,890
I needed 16 vCPU or
DCUs worth of resources

930
00:34:04,890 --> 00:34:06,420
to run this query.

931
00:34:06,420 --> 00:34:07,770
Now later on to the right,

932
00:34:07,770 --> 00:34:11,040
what I did was I just
added more artificial load

933
00:34:11,040 --> 00:34:14,172
just for demonstration
purposes to show you that

934
00:34:14,172 --> 00:34:16,620
that vCPU line which represents
the number of resources

935
00:34:16,620 --> 00:34:18,690
on my instance is not static,

936
00:34:18,690 --> 00:34:22,683
it does scale as I'm adding
more load to our database.

937
00:34:23,760 --> 00:34:25,710
Now I wanna show you from another view,

938
00:34:25,710 --> 00:34:27,120
this is the view based on, "Wait,

939
00:34:27,120 --> 00:34:29,610
what are my CPUs doing?"

940
00:34:29,610 --> 00:34:31,200
Notice here that it's mostly green

941
00:34:31,200 --> 00:34:34,440
and it's actually not spending
a lot of time doing I/Os

942
00:34:34,440 --> 00:34:36,067
and this signals to me that,

943
00:34:36,067 --> 00:34:37,620
"Hey, this is actually quite efficient."

944
00:34:37,620 --> 00:34:40,590
I'm not spending a lot of time
going to disc, getting data,

945
00:34:40,590 --> 00:34:42,420
bringing it back and processing it.

946
00:34:42,420 --> 00:34:44,880
I'm actually spending a lot of time in CPU

947
00:34:44,880 --> 00:34:47,400
which means I'm just processing this data.

948
00:34:47,400 --> 00:34:48,663
So quite efficient there.

949
00:34:49,740 --> 00:34:51,450
So if you haven't tried out Serverless,

950
00:34:51,450 --> 00:34:52,283
highly recommend it.

951
00:34:52,283 --> 00:34:55,013
It's really easy to switch
back and forth from provisioned

952
00:34:56,040 --> 00:34:57,450
as well.

953
00:34:57,450 --> 00:35:00,750
All right, switching
gears to DocumentDB 8.0

954
00:35:01,700 --> 00:35:02,700
and the New Query Planner,

955
00:35:02,700 --> 00:35:06,360
which is core of DocumentDB 8.0.

956
00:35:06,360 --> 00:35:08,750
Who here heard that we
launched DocumentDB 8.0?

957
00:35:11,820 --> 00:35:12,653
Very few folks.

958
00:35:12,653 --> 00:35:13,486
Well, I wouldn't blame you

959
00:35:13,486 --> 00:35:15,540
'cause we actually just
launched this three weeks ago,

960
00:35:15,540 --> 00:35:16,680
pre re:Invent,

961
00:35:16,680 --> 00:35:20,250
probably during
Thanksgiving for this thing,

962
00:35:20,250 --> 00:35:21,810
before this event.

963
00:35:21,810 --> 00:35:22,643
So if you missed it,

964
00:35:22,643 --> 00:35:26,490
this is a very high level
recap of what's included.

965
00:35:26,490 --> 00:35:28,740
A big focus this year

966
00:35:28,740 --> 00:35:31,410
in terms of our latest launch
is really around performance

967
00:35:31,410 --> 00:35:32,670
and I think Cody already covered

968
00:35:32,670 --> 00:35:34,680
some of those aspects with compression.

969
00:35:34,680 --> 00:35:36,810
But the New Query Planner is also

970
00:35:36,810 --> 00:35:40,230
a very key part of the DocumentDB 8.0,

971
00:35:40,230 --> 00:35:42,690
which I'm gonna go into shortly in a bit.

972
00:35:42,690 --> 00:35:45,690
The last performance piece
is around vector indexes.

973
00:35:45,690 --> 00:35:48,330
So you know, to future proof
you a little bit about,

974
00:35:48,330 --> 00:35:50,250
you know some of those apps
that you are gonna build

975
00:35:50,250 --> 00:35:51,750
on DocumentDB,

976
00:35:51,750 --> 00:35:54,300
we introduced parallel index
build for vector indexes,

977
00:35:54,300 --> 00:35:58,170
which provides up to 30
times faster index builds

978
00:35:58,170 --> 00:35:59,313
for vector indexes.

979
00:36:00,570 --> 00:36:03,960
The second piece of it is
MongoDB API compatibility.

980
00:36:03,960 --> 00:36:05,280
We heard from a lot of our customers

981
00:36:05,280 --> 00:36:10,280
they want to bring their MongoDB
7, 6 and 8.0 applications.

982
00:36:10,470 --> 00:36:12,690
They have the drivers that
work with those versions.

983
00:36:12,690 --> 00:36:15,870
You can now bring those
workloads to DocumentDB as well.

984
00:36:15,870 --> 00:36:19,290
We've also introduced a
handful of new MongoDB APIs

985
00:36:19,290 --> 00:36:22,020
and functions such as
Views and Collations,

986
00:36:22,020 --> 00:36:23,580
that's included as well.

987
00:36:23,580 --> 00:36:25,890
So if you are waiting on
some of these features

988
00:36:25,890 --> 00:36:27,903
to adopt DocumentDB, we have them now.

989
00:36:29,430 --> 00:36:31,380
Let's go into the New Query Planner,

990
00:36:31,380 --> 00:36:35,010
which we're gonna spend the
last bit of this session on.

991
00:36:35,010 --> 00:36:36,180
So starting 8.0,

992
00:36:36,180 --> 00:36:38,820
we launched a New Query Planner

993
00:36:38,820 --> 00:36:41,910
and this is gonna be a large
collection of improvements.

994
00:36:41,910 --> 00:36:43,470
However, for the remainder of the time,

995
00:36:43,470 --> 00:36:45,660
I'm only gonna show two examples

996
00:36:45,660 --> 00:36:46,563
that highlight some of the things

997
00:36:46,563 --> 00:36:48,423
that the New Query Planner can do.

998
00:36:49,860 --> 00:36:54,860
First is improvements
around choosing an index.

999
00:36:55,140 --> 00:37:00,140
So the New Query Planner
has expanded index support,

1000
00:37:00,210 --> 00:37:01,830
delivering performance improvements

1001
00:37:01,830 --> 00:37:04,140
for certain operations.

1002
00:37:04,140 --> 00:37:06,240
And this includes, for this example,

1003
00:37:06,240 --> 00:37:08,100
if you have negation operations,

1004
00:37:08,100 --> 00:37:11,850
we will now choose a
index over a collection.

1005
00:37:11,850 --> 00:37:13,800
So let's walk through
a very simple example.

1006
00:37:13,800 --> 00:37:15,090
I have a sample document here,

1007
00:37:15,090 --> 00:37:16,380
very simple one,

1008
00:37:16,380 --> 00:37:19,740
I'm gonna create an index on
it and then I wanna do a find.

1009
00:37:19,740 --> 00:37:23,280
But notice how that find
has a $nin operation

1010
00:37:23,280 --> 00:37:25,320
or not in operation.

1011
00:37:25,320 --> 00:37:27,390
Now with Query Planner 1,

1012
00:37:27,390 --> 00:37:29,880
now you could actually
rewrite your query to do this,

1013
00:37:29,880 --> 00:37:31,380
to take advantage of the index.

1014
00:37:31,380 --> 00:37:34,200
But the default behavior was to use...

1015
00:37:34,200 --> 00:37:36,030
The winning plan was to
use a collection scan.

1016
00:37:36,030 --> 00:37:38,040
You can see here it says COLLSCAN.

1017
00:37:38,040 --> 00:37:39,270
And however,

1018
00:37:39,270 --> 00:37:42,870
with the new query planner we
default to the winning stage

1019
00:37:42,870 --> 00:37:46,110
of using an index scan as noted there.

1020
00:37:46,110 --> 00:37:47,490
The second thing I'd like to call out

1021
00:37:47,490 --> 00:37:49,800
and point out to you is we also...

1022
00:37:49,800 --> 00:37:51,540
When you do explain plan,

1023
00:37:51,540 --> 00:37:55,950
we also included the planner
version as part of the output.

1024
00:37:55,950 --> 00:37:58,380
So if you are doing a migration

1025
00:37:58,380 --> 00:38:00,750
and you want to use the first...

1026
00:38:00,750 --> 00:38:02,010
You know, the old planner version

1027
00:38:02,010 --> 00:38:03,090
as you're switching over,

1028
00:38:03,090 --> 00:38:05,820
we allow you to switch
between planner versions

1029
00:38:05,820 --> 00:38:08,643
but it's highly recommended
that you use the latest one.

1030
00:38:10,200 --> 00:38:11,490
So that was a very simple example

1031
00:38:11,490 --> 00:38:12,720
where we changed a single stage

1032
00:38:12,720 --> 00:38:15,240
and we swapped it from
collection to scan to index scan.

1033
00:38:15,240 --> 00:38:17,220
But let's take a look at
a more complex example

1034
00:38:17,220 --> 00:38:20,130
with aggregation pipelines.

1035
00:38:20,130 --> 00:38:22,650
Not only do we change certain stages,

1036
00:38:22,650 --> 00:38:24,030
we actually have the ability to

1037
00:38:24,030 --> 00:38:27,870
take a look at your query as a
whole and find optimizations.

1038
00:38:27,870 --> 00:38:31,950
So here there's two stages
of my aggregation pipeline.

1039
00:38:31,950 --> 00:38:32,783
One of 'em is a lookup,

1040
00:38:32,783 --> 00:38:34,410
if you're familiar with SQL,

1041
00:38:34,410 --> 00:38:36,390
this is equivalent to a JOIN.

1042
00:38:36,390 --> 00:38:40,120
And unwind is pretty much an
operation that flattens out

1043
00:38:41,040 --> 00:38:44,790
any document that has an array in it.

1044
00:38:44,790 --> 00:38:46,260
So by doing...

1045
00:38:46,260 --> 00:38:48,240
First, what this query is gonna go do

1046
00:38:48,240 --> 00:38:50,160
is first do a JOIN effectively

1047
00:38:50,160 --> 00:38:53,430
and then flatten out all the arrays.

1048
00:38:53,430 --> 00:38:54,840
But with the New Query Planner,

1049
00:38:54,840 --> 00:38:55,890
we actually simplify this

1050
00:38:55,890 --> 00:38:57,690
and do a single step
and I'll show you that

1051
00:38:57,690 --> 00:38:59,430
through an example in a bit.

1052
00:38:59,430 --> 00:39:02,370
Now for those who are
unfamiliar with unwind,

1053
00:39:02,370 --> 00:39:04,260
I wanna show just a very simple example

1054
00:39:04,260 --> 00:39:06,570
of what it kinda looks like with emojis.

1055
00:39:06,570 --> 00:39:08,670
If your document looks something like this

1056
00:39:09,630 --> 00:39:13,380
and you do an unwind on the array field,

1057
00:39:13,380 --> 00:39:14,970
you get three documents.

1058
00:39:14,970 --> 00:39:19,970
So you do an unwind on
that chef emoji there,

1059
00:39:20,250 --> 00:39:21,700
you get three instead of one.

1060
00:39:23,520 --> 00:39:25,170
All right, so let's go through an example.

1061
00:39:25,170 --> 00:39:26,730
So let's say I have two collections,

1062
00:39:26,730 --> 00:39:30,210
one of them is my orders collection.

1063
00:39:30,210 --> 00:39:32,130
Here I have the customer name

1064
00:39:32,130 --> 00:39:34,710
and the product IDs of which they ordered.

1065
00:39:34,710 --> 00:39:36,000
And then I have a second collection

1066
00:39:36,000 --> 00:39:37,950
which has all the product details.

1067
00:39:37,950 --> 00:39:40,200
So laptop, the price, phone,

1068
00:39:40,200 --> 00:39:41,763
the price and so on.

1069
00:39:42,870 --> 00:39:46,410
Now let's say I want a set of documents

1070
00:39:46,410 --> 00:39:49,890
that has all the orders for
each product detail, right?

1071
00:39:49,890 --> 00:39:51,120
So you can see here effectively

1072
00:39:51,120 --> 00:39:55,920
what I'm expanding on is
the orders collection.

1073
00:39:55,920 --> 00:39:57,210
I want to blow that up

1074
00:39:57,210 --> 00:40:00,903
and I want to get a detail
of every single product,

1075
00:40:01,980 --> 00:40:04,140
that is being bought in the orders.

1076
00:40:04,140 --> 00:40:05,520
The way I would do this,

1077
00:40:05,520 --> 00:40:07,410
the exact same query I shown before.

1078
00:40:07,410 --> 00:40:09,660
First I'm gonna join these two collections

1079
00:40:09,660 --> 00:40:11,610
and then I'm gonna do an unwind on it,

1080
00:40:11,610 --> 00:40:13,923
which flattens out each
one of those arrays.

1081
00:40:16,020 --> 00:40:17,760
So with Planner Version 1,

1082
00:40:17,760 --> 00:40:19,170
and I know there's a lot going on here,

1083
00:40:19,170 --> 00:40:20,250
while I'm showing you on the left,

1084
00:40:20,250 --> 00:40:21,810
and this is for reference purposes,

1085
00:40:21,810 --> 00:40:23,790
this is the output of the explain plan,

1086
00:40:23,790 --> 00:40:27,003
but I have the tree view
equivalent on the very right.

1087
00:40:28,260 --> 00:40:29,250
When I am...

1088
00:40:29,250 --> 00:40:33,960
One thing to note is that
this nested loop lookup stage,

1089
00:40:33,960 --> 00:40:36,150
what I'm actually doing
is I'm actually joining

1090
00:40:36,150 --> 00:40:37,650
these two collections

1091
00:40:37,650 --> 00:40:39,000
and then I'm gonna store it in memory

1092
00:40:39,000 --> 00:40:40,890
so they can unwind it later.

1093
00:40:40,890 --> 00:40:44,070
Now this ends up to look
something like this.

1094
00:40:44,070 --> 00:40:45,420
Where I have order,

1095
00:40:45,420 --> 00:40:46,320
order number one,

1096
00:40:46,320 --> 00:40:48,480
and then I have two products
under product details,

1097
00:40:48,480 --> 00:40:49,620
order number two,

1098
00:40:49,620 --> 00:40:51,753
and I have multiple product details.

1099
00:40:52,590 --> 00:40:56,670
You can imagine that if my
array had 100s of entries

1100
00:40:56,670 --> 00:40:59,130
and I have lots of different
products to look under,

1101
00:40:59,130 --> 00:41:02,520
this object can be very, very big.

1102
00:41:02,520 --> 00:41:04,950
And the fact that we're
just storing it in memory

1103
00:41:04,950 --> 00:41:06,180
means that this intermediate step

1104
00:41:06,180 --> 00:41:07,893
could be very, very expensive.

1105
00:41:08,940 --> 00:41:10,980
Whereas Planner Version 3,

1106
00:41:10,980 --> 00:41:12,660
one thing that we do is that

1107
00:41:12,660 --> 00:41:14,640
we do not create this temporary object

1108
00:41:14,640 --> 00:41:16,440
and store it into memory.

1109
00:41:16,440 --> 00:41:19,320
Instead, if we take a look
at both of these collections,

1110
00:41:19,320 --> 00:41:21,900
one thing that you'll note is that

1111
00:41:21,900 --> 00:41:24,630
the final stages produces the documents

1112
00:41:24,630 --> 00:41:27,487
and how the query planner
does that is it notices that,

1113
00:41:27,487 --> 00:41:29,430
"Hey look, I'm trying to do an unwind

1114
00:41:29,430 --> 00:41:31,320
after a lookup stage."

1115
00:41:31,320 --> 00:41:34,020
So instead of building
that temporary structure

1116
00:41:34,020 --> 00:41:35,760
and then just to unwind it later,

1117
00:41:35,760 --> 00:41:38,760
as I'm doing the lookup for
each individual product ID,

1118
00:41:38,760 --> 00:41:40,560
why don't I just do the lookup

1119
00:41:40,560 --> 00:41:42,660
against the products collection

1120
00:41:42,660 --> 00:41:44,910
and stream those results as the final set.

1121
00:41:44,910 --> 00:41:46,410
And that's exactly what we do.

1122
00:41:47,580 --> 00:41:49,590
So if you compare the tree structure

1123
00:41:49,590 --> 00:41:51,540
of both these query plans,

1124
00:41:51,540 --> 00:41:53,400
one thing I like to point out is

1125
00:41:53,400 --> 00:41:56,910
this nested loop lookup
returned eight documents.

1126
00:41:56,910 --> 00:41:59,040
I know obviously I'm working with a very

1127
00:41:59,040 --> 00:42:00,060
small collection here,

1128
00:42:00,060 --> 00:42:04,080
but if you have a lot of documents,

1129
00:42:04,080 --> 00:42:05,980
you could think of this as almost like

1130
00:42:07,080 --> 00:42:09,630
O-N squared to notation of runtime.

1131
00:42:09,630 --> 00:42:13,263
So this stage can use up a lot of memory.

1132
00:42:14,910 --> 00:42:15,743
So these...

1133
00:42:15,743 --> 00:42:18,660
Again, these are just two
examples of the New Query Planner.

1134
00:42:18,660 --> 00:42:23,190
There's actually like tens of
10s and 100s of optimizations

1135
00:42:23,190 --> 00:42:25,110
that we're planning to go do

1136
00:42:25,110 --> 00:42:26,670
that you could check
on our developer docs.

1137
00:42:26,670 --> 00:42:28,980
These are just two to
show case what we're doing

1138
00:42:28,980 --> 00:42:30,693
with our New Query Planner.

1139
00:42:32,310 --> 00:42:33,270
All right, one more thing.

1140
00:42:33,270 --> 00:42:34,740
I know this wasn't on the agenda slide,

1141
00:42:34,740 --> 00:42:37,110
but I really wanted to show this slide

1142
00:42:37,110 --> 00:42:38,310
and I just really wanna highlight

1143
00:42:38,310 --> 00:42:40,530
that not everything we do in DocumentDB

1144
00:42:40,530 --> 00:42:42,150
is an explicit feature.

1145
00:42:42,150 --> 00:42:43,710
There are a lot of things that we do

1146
00:42:43,710 --> 00:42:45,360
behind the scenes just
to make things better.

1147
00:42:45,360 --> 00:42:47,250
And I wanna show you
something that's actually

1148
00:42:47,250 --> 00:42:49,170
what our engineers see.

1149
00:42:49,170 --> 00:42:51,450
It's a very chaotic graph,

1150
00:42:51,450 --> 00:42:54,510
but this actually represents
all the time someone...

1151
00:42:54,510 --> 00:42:56,700
Our customers do when
they do a switchover.

1152
00:42:56,700 --> 00:42:58,050
So if you're not familiar with switchover,

1153
00:42:58,050 --> 00:42:59,310
this is when you're running

1154
00:42:59,310 --> 00:43:00,840
a cluster across two different regions

1155
00:43:00,840 --> 00:43:02,890
and you wanna promote one over the other.

1156
00:43:04,620 --> 00:43:08,100
Between August 4th and
August 11th on this graph,

1157
00:43:08,100 --> 00:43:11,190
before we actually
introduced a optimization,

1158
00:43:11,190 --> 00:43:16,190
the P90 time to do a completed
switchover was 300 seconds.

1159
00:43:16,470 --> 00:43:20,580
We actually rolled out a
change starting August 11th

1160
00:43:20,580 --> 00:43:23,910
and we finished our
deployment where we simplified

1161
00:43:23,910 --> 00:43:26,130
the workflow,

1162
00:43:26,130 --> 00:43:28,500
the promotion workflow behind the scenes.

1163
00:43:28,500 --> 00:43:30,210
And we actually found that our P99

1164
00:43:30,210 --> 00:43:32,580
can go as low as 30 seconds.

1165
00:43:32,580 --> 00:43:34,306
Our P90 is low as 30 seconds.

1166
00:43:34,306 --> 00:43:37,320
So this is a 90% increase over switchover.

1167
00:43:37,320 --> 00:43:38,370
So this is just...

1168
00:43:38,370 --> 00:43:39,420
There are a lot of things that we do

1169
00:43:39,420 --> 00:43:41,550
as a part of our service...

1170
00:43:41,550 --> 00:43:44,880
Part of our service that
just makes things better.

1171
00:43:44,880 --> 00:43:47,730
So this is something I really
want to show you all there.

1172
00:43:48,990 --> 00:43:50,160
Okay, summary recap.

1173
00:43:50,160 --> 00:43:53,043
There's a lot of things
that we launched this year.

1174
00:43:53,970 --> 00:43:55,800
You know this, this recording...

1175
00:43:55,800 --> 00:43:57,600
I think this, this recording

1176
00:43:57,600 --> 00:43:58,980
will be available on YouTube later,

1177
00:43:58,980 --> 00:44:00,750
hence it's being recorded in the back.

1178
00:44:00,750 --> 00:44:01,590
So if you missed anything,

1179
00:44:01,590 --> 00:44:03,510
you can always come back.

1180
00:44:03,510 --> 00:44:04,980
You can always check out our what's new

1181
00:44:04,980 --> 00:44:06,450
or our release notes pages,

1182
00:44:06,450 --> 00:44:08,040
which has every single thing

1183
00:44:08,040 --> 00:44:09,940
that we've launched this year as well.

1184
00:44:11,340 --> 00:44:12,360
One thing I like to call out,

1185
00:44:12,360 --> 00:44:14,310
you may have seen some
folks wearing this hoodie.

1186
00:44:14,310 --> 00:44:17,763
I think the expo is just about
to open tonight or tomorrow.

1187
00:44:18,990 --> 00:44:20,340
If you go to the database booth,

1188
00:44:20,340 --> 00:44:21,570
remember that thing I talked about

1189
00:44:21,570 --> 00:44:24,420
that was really important
about serverless databases?

1190
00:44:24,420 --> 00:44:26,880
So one thing that you can go to them is

1191
00:44:26,880 --> 00:44:28,260
if you go to this booth,

1192
00:44:28,260 --> 00:44:31,410
go to them and tell them one
interesting thing that learned

1193
00:44:31,410 --> 00:44:34,470
and that is, as a reminder
if you forgot about it,

1194
00:44:34,470 --> 00:44:36,600
is that with serverless instances,

1195
00:44:36,600 --> 00:44:39,810
large instances scale faster
than that of smaller ones.

1196
00:44:39,810 --> 00:44:40,773
So remember that.

1197
00:44:41,790 --> 00:44:44,460
I think that the booth is
just opening up right now

1198
00:44:44,460 --> 00:44:47,610
and we are one of the ones that...

1199
00:44:47,610 --> 00:44:49,140
One of the sessions that just follow

1200
00:44:49,140 --> 00:44:50,490
right before the booth opening.

1201
00:44:50,490 --> 00:44:54,630
So hopefully you all are
actually getting the secret there

1202
00:44:54,630 --> 00:44:56,190
before others do,

1203
00:44:56,190 --> 00:44:58,410
given that the session is on a Monday.

1204
00:44:58,410 --> 00:44:59,280
And with that said,

1205
00:44:59,280 --> 00:45:00,870
thank you so much for attending a session.

1206
00:45:00,870 --> 00:45:03,420
I know it's Monday, 5:30.

1207
00:45:03,420 --> 00:45:05,520
Again, but the last ask
I have of all of you,

1208
00:45:05,520 --> 00:45:08,310
and this enables Cody and I to come back

1209
00:45:08,310 --> 00:45:10,950
year after year is really
complete the session survey

1210
00:45:10,950 --> 00:45:12,570
in the mobile app.

1211
00:45:12,570 --> 00:45:16,424
So thank you so much and
have a great re:Invent.

1212
00:45:16,424 --> 00:45:19,385
(audience applauds)

