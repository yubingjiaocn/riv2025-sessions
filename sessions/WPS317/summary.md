# AWS re:Invent 会议总结：高监管环境中的安全 AI 代理与交付效率

## 会议概述

本次会议由 AWS 巴西解决方案架构师 Amanda Kinto 主持，重点探讨了如何在高度监管的环境中安全地部署和运营 AI 代理及生成式 AI 工作负载。会议邀请了两家客户代表——巴西 Sicoob 金融合作社的 Edson Lisboa 和荷兰 Holland Casinos 的 Andreas，分享了他们在各自高度监管行业中实施生成式 AI 的实践经验。

会议强调了在金融、公共部门等受严格监管的行业中，合规性、安全性、治理和风险管理是部署 AI 解决方案的核心支柱。全球已有超过 1,000 项 AI 相关法规覆盖 69 个国家，包括欧盟的 AI 法案、巴西的 LGPD 数据保护法等。尽管各国法规不同，但都围绕着合规与治理、法律与隐私、控制与风险管理这四大核心要素展开。

AWS 提供了分层的解决方案架构来应对这些挑战：底层是生成式 AI 应用，中间层包括各国的监管要求、AWS 负责任 AI 原则、ISO 等合规标准，顶层则是 NIST AI 600 框架、OWASP LLM 安全指南以及 AWS Well-Architected Framework 的生成式 AI 最佳实践。会议展示了两种主要的技术实现路径：基于 Amazon EKS 的 GPU 集群部署开源大语言模型，以及基于 Amazon Bedrock 的托管式 AI 代理服务。

## 详细时间线与关键要点

### 00:00 - 开场介绍
- Amanda Kinto 介绍自己是 AWS 巴西解决方案架构师，专注于公共财政领域已有六年
- 介绍会议主题：高监管环境中的安全 AI 代理与交付效率
- 介绍两位客户嘉宾：Sicoob 的 Edson 和 Holland Casinos 的 Andreas

### 01:30 - 会议议程概览
- 讨论高监管环境及如何在其上运行 AI 工作负载
- 介绍如何使用 Amazon EKS 运行 AI 代理和 AI 工作负载
- Sicoob 案例分享：在高监管环境中运行生成式 AI 工作负载
- Amazon Bedrock 介绍及如何帮助交付符合监管要求的 AI 代理
- Holland Casinos 案例分享：使用 Bedrock Agent 运行工作负载

### 03:00 - 高监管环境的挑战
- 部署模型不仅是技术问题，需要遵守大量法律、标准和规范
- 欧盟 AI 法案：基于风险的框架，禁止某些 AI 应用，对高风险系统有严格要求
- 巴西：结合新兴 AI 立法与现有数据保护法（LGPD），联邦层面要求公共部门数据留在境内
- 金融和公共财政等特定行业受中央银行等机构的额外监管
- 全球已有超过 1,000 项 AI 法规，覆盖 69 个国家

### 05:30 - 监管的共同支柱
- 尽管各国法规不同,但存在四个共同支柱：
  1. 合规与治理（Compliance and Governance）
  2. 法律与隐私（Legal and Privacy）
  3. 控制（Controls）
  4. 风险管理（Risk Management）
- 这些支柱构成了全球通用的基础框架

### 07:00 - 分层监管模式
- 底层：生成式 AI 应用
- 监管层：各国特定法规（欧洲、加拿大、智利、中国等各不相同）
- 负责任 AI 层：AWS 负责任 AI 原则，包括数据保护、客户安全保障、安全和提示工程
- 合规与标准层：ISO 标准，包括专门针对生成式 AI 工作负载的 ISO
- 技术框架层：
  - NIST AI 600 框架（美国 AI 风险管理框架）
  - OWASP LLM AI 安全模式和 Top 10 漏洞
  - AWS Well-Architected Framework 的生成式 AI 层

### 10:00 - 监管不应阻碍 AI 采用
- 监管实际上需要 AI 的采用
- 可以使用 AI 来理解和遵守监管要求
- AI 可以加速高监管环境中的工作负载
- 在巴西政府、金融和医疗领域都能看到这种协同效应

### 11:00 - AWS 生成式 AI 三层架构
- 顶层：Amazon Q 系列应用
- 中间层：Amazon Bedrock（生成式 AI 工作负载套件，通过 API 使用不同模型）
- 底层：基础设施层
  - Trainium 和 Inferentia 芯片（AWS 专为机器学习开发）
  - SageMaker（完整的机器学习解决方案）
  - GPU 工作负载（多种 GPU 类型可用）

### 13:00 - 模型选择策略
- 首先明确用例：确定要解决的问题是否适合使用生成式 AI
- 选择合适的模型：市场上有多种模型可选，包括 Amazon Nova
- 建议使用 2-3 个模型进行测试
- 不要"爱上"某个特定模型，应根据用例选择最佳模型
- 模型会不断更新，应保持灵活性

### 15:00 - 模型定制方法
1. 基础模型 + RAG/提示工程：使用检索增强生成或提示工程将数据整合到模型中
2. 微调（Fine-tuning）：使用特定行业数据微调模型，需要大量数据，适合金融、医疗、公共部门等特定行业
3. 从头训练：训练自己的模型，需要大量数据和 GPU 资源，是最重的任务

### 17:00 - 推理部署选项
在 AWS 上有三种推理部署方式：
1. Amazon Bedrock：托管服务
2. SageMaker：可部署特定模型并提供推理端点
3. Amazon EKS：使用 Kubernetes 部署，适合已有 Kubernetes 经验的团队

### 18:30 - Amazon EKS 的优势
- 如果团队已经使用 Kubernetes 运行生产工作负载，这是理想选择
- 如果团队不熟悉 Kubernetes，学习曲线较陡峭
- 提供灾难恢复、成本可见性、部署自动化等生产级能力
- 可以在 EKS 上部署推理端点和生成式 AI 工作负载，而不是直接在 EC2 上部署

### 20:00 - 选择 EKS 进行推理的原因
- **开源生态系统**：Kubernetes 的开源社区在过去 10 多年中积累了丰富的资源，生成式 AI 工作负载也不例外
- 一年半前还没有 GPU 集群部署解决方案，现在已有成熟的开源项目
- **可移植性**：可在 AWS、其他云提供商或本地基础设施上运行
- **灾难恢复、可扩展性、成本预测**等其他优势

### 22:00 - Sicoob 案例介绍开始
Edson Lisboa 介绍 Sicoob：
- 巴西最大的合作金融系统之一
- 覆盖全国近 2,500 个巴西城市
- 超过 400 个城市仅有 Sicoob 提供金融服务
- 拥有 300 多家信用合作社、14 个中央合作社、4,700 个网点
- 超过 60,000 名员工，服务 900 多万客户
- 98% 以上的交易通过移动应用和网上银行等数字渠道完成

### 24:00 - Sicoob 的监管环境与治理
- 作为巴西金融机构，受中央银行严格监管
- 治理不仅是合规复选框,更是真正的竞争差异化因素
- 建立了集中化治理结构,明确问责制、风险分类和持续监控

### 25:00 - Sicoob 的四大不可妥协支柱
1. 安全性：作为金融机构,信用合作社和会员数据的安全是首要任务
2. 可扩展性：考虑到当前规模和未来增长目标,需要支持大规模部署的环境
3. 高效成本管理：采用按需付费策略运行大型模型,采用开源 LLM
4. 多模型支持：为不同任务使用合适的模型,不相信单一模型适用所有场景

### 26:30 - Sicoob 使用的开源模型
在 AWS 上使用 Amazon EKS 运行以下开源模型：
- **Llama**（Meta）
- **Mistral**
- **DeepSeek R1**（中国模型）
- **Granite**（IBM）
- 有时在同一用例中并行使用多个模型
- 根据速度或准确性要求选择特定模型

### 28:00 - Sicoob 的技术架构详解
Amanda 详细介绍架构：
- **核心**：Amazon EKS 集群
- **计算**：在 EKS 中部署带 GPU 的 EC2 实例
- **最佳实践**：
  - 使用专门为生成式 AI/GPU 工作负载准备的 AMI（包含 Nvidia 驱动等）
  - 从 Amazon S3 下载模型,而不是每次启动 Pod 时都下载
  - 使用特定的开源工具优化部署

### 30:00 - 关键开源工具
1. Karpenter：
   - AWS 开发的开源解决方案
   - 根据需求自动扩展带 GPU 的 Kubernetes 节点
   - 避免固定部署特定 GPU 实例的 node group
   - 实现真正的按需付费

2. KEDA（Kubernetes Event-Driven Autoscaling）：
   - 根据请求自动扩展 Pod
   - 无请求时不运行 Pod,不部署 GPU 实例,不产生费用
   - 有请求时才部署,适合非事务性工作负载
   - 部署需要几秒钟,但对某些用例可接受

### 33:00 - 模型管理工具
3. Ollama：
   - 开源解决方案,用于管理集群中的模型
   - 提供用户界面,无需命令行操作
   - AWS 文档中有部署指南

4. vLLM：
   - Sicoob 最近采用的新工具,替代了之前的 Llama
   - 比之前的解决方案更快、计算优化更好
   - 是新兴的开源工具,社区持续改进
   - 在 Kubernetes 上运行 LLM 模型更加经济高效

### 35:00 - 完整架构组件
- **Amazon ECR**：存储容器镜像
- **AWS Load Balancer**：通过 Ingress 暴露模型端点
- **Amazon S3**：存储模型文件
- 主要基础设施和架构逻辑在 EKS 集群内部
- Karpenter 可以配置多个可用区、多种 GPU 类型,自动选择最佳/最便宜/可用的选项
- 可以使用 Kubernetes 预算控制实例终止方式,确保新实例部署后再终止旧实例

### 37:00 - Sicoob 的业务成果
Edson 分享实际用例和业务结果：

1. SISBE Code AI（内部智能开发助手）：
   - 基于开源 LLM 自主开发,未购买第三方解决方案
   - 集成到开发者的 IDE 中
   - 近 1,500 名开发人员使用
   - 好处：自动化编码任务、代码建议、加速新开发人员入职、节省交付时间、提高解决方案质量、全周期开发支持

2. DevOps 自动化：
   - 约 8 个数字机器人自动化手动任务
   - 不仅是简单任务,还包括复杂任务
   - 使用生成式 AI 集成 AI 代理
   - 节省了约 40 万人工小时

3. 投资顾问：
   - 使用生成式 AI 的专业投资支持
   - 根据会员档案、最佳市场实践和信用合作社提供的投资产品创建投资建议

4. SISBE AI（核心银行智能助手）：
   - 多种功能：
     - 与文档交互（包括法律文件和合同）
     - 使用自然语言获取答案
     - 智能搜索
     - 创建分析
     - 自动生成决策报告

### 41:00 - Amazon Bedrock 介绍
Amanda 转向 Amazon Bedrock：
- 专注于构建者和开发人员的工具
- 使用构建块交付生成式 AI 解决方案
- 2023 年开始时只有一些模型和基于 API 的端点
- 现在是完整的解决方案,包括：模型、护栏、安全、提示工程、提示缓存等

### 42:30 - Amazon Bedrock 的合规性
- 符合多种法规和标准
- 多个 ISO 认证
- SOC 1、2、3 认证
- HIPAA 合规
- **重点**：ISO 42001 - 专门针对生成式 AI 工作负载的 ISO 标准
- AWS 的 Amazon Bedrock 是首个获得此 ISO 认证的云服务提供商
- 对于高监管环境非常重要

### 44:00 - 数据隐私与安全（会议记录在此处截断）
Amanda 提到客户最常问的问题是关于数据隐私和安全...

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


注：由于提供的字幕文本在讨论 Amazon Bedrock 数据隐私部分时突然截断,完整的会议内容可能还包括：
- Amazon Bedrock 的数据保护机制
- Holland Casinos 使用 Bedrock Agents 的案例分享
- 更多关于护栏和安全最佳实践的讨论
- 问答环节