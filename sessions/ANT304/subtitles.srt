1
00:00:00,600 --> 00:00:02,910
- Everyone, thank you
for coming, good morning.

2
00:00:02,910 --> 00:00:04,500
You probably guessed, I'm Taz,

3
00:00:04,500 --> 00:00:07,253
tech leader for AWS data analytics.

4
00:00:07,253 --> 00:00:08,670
- And I'm Shikha Verma.

5
00:00:08,670 --> 00:00:13,290
I'm the head of product for
data and analytics at AWS,

6
00:00:13,290 --> 00:00:15,210
very excited to have you all here.

7
00:00:15,210 --> 00:00:16,710
Let's get started, lots of content.

8
00:00:16,710 --> 00:00:18,052
- Right, everyone ready?

9
00:00:18,052 --> 00:00:19,110
- [Audience Member 1] Yes.

10
00:00:19,110 --> 00:00:21,480
- I know it's a little early
in the morning at least for me.

11
00:00:21,480 --> 00:00:23,180
Let's wake up.

12
00:00:23,180 --> 00:00:24,570
(Taz giggles)
(Shikha giggles)

13
00:00:24,570 --> 00:00:26,790
Okay, so, in terms of the agenda,

14
00:00:26,790 --> 00:00:29,250
what we have for you today is first,

15
00:00:29,250 --> 00:00:32,640
we will be level-setting what
exactly data foundation means.

16
00:00:32,640 --> 00:00:35,597
Because you ask the person next to you,

17
00:00:35,597 --> 00:00:37,860
you'll get a different
definition depending

18
00:00:37,860 --> 00:00:40,260
on the day, on the time of the day.

19
00:00:40,260 --> 00:00:42,030
So, we'll level-set what that means.

20
00:00:42,030 --> 00:00:44,340
We'll in fact take it to the next level,

21
00:00:44,340 --> 00:00:48,090
which is data analytics,
sorry, data foundation,

22
00:00:48,090 --> 00:00:49,770
what it means for data analytics

23
00:00:49,770 --> 00:00:53,580
and what it means for
agentic AI applications.

24
00:00:53,580 --> 00:00:55,500
We'll talk about how
you can bridge the gap

25
00:00:55,500 --> 00:00:59,600
between data and AI using
the AI-ready data foundation

26
00:00:59,600 --> 00:01:00,433
on AWS.

27
00:01:02,160 --> 00:01:06,900
Examples of this will
include AWS MCP servers,

28
00:01:06,900 --> 00:01:09,450
followed by an in-depth look

29
00:01:09,450 --> 00:01:14,160
of how you can use AWS data
analytics to manage context.

30
00:01:14,160 --> 00:01:16,860
We'll also talk about
event-driven architectures

31
00:01:16,860 --> 00:01:20,340
where your application
may have multiple agents

32
00:01:20,340 --> 00:01:22,860
and then, you will bring this all together

33
00:01:22,860 --> 00:01:25,620
with the most critical
part for data and AI,

34
00:01:25,620 --> 00:01:28,920
which is how do you make
your data ready for AI?

35
00:01:28,920 --> 00:01:31,830
We'll talk about data governance,

36
00:01:31,830 --> 00:01:34,050
we'll talk about metadata management

37
00:01:34,050 --> 00:01:36,153
and that's essentially data readiness.

38
00:01:37,110 --> 00:01:38,963
So, Shikha, shall we begin?

39
00:01:40,680 --> 00:01:43,050
- That all sounds good theoretically,

40
00:01:43,050 --> 00:01:45,570
but I think we need to root it in example

41
00:01:45,570 --> 00:01:46,890
where we can show everybody

42
00:01:46,890 --> 00:01:49,020
how we can actually do this live.

43
00:01:49,020 --> 00:01:51,150
Don't you guys think, yeah?

44
00:01:51,150 --> 00:01:54,360
So, we should use a demo to
demonstrate what we have.

45
00:01:54,360 --> 00:01:55,320
Can we do that, Taz?

46
00:01:55,320 --> 00:01:57,540
Can we use our own data for the AI agents?

47
00:01:57,540 --> 00:01:58,770
- Your wish is my command.

48
00:01:58,770 --> 00:02:00,480
I have a demo for you.

49
00:02:00,480 --> 00:02:02,880
We are a travel agent company.

50
00:02:02,880 --> 00:02:04,380
That's the premise of the demo.

51
00:02:04,380 --> 00:02:06,270
We'll use that to showcase

52
00:02:06,270 --> 00:02:09,090
how you can leverage the
data foundation on AWS

53
00:02:09,090 --> 00:02:11,130
to build an agentic AI application

54
00:02:11,130 --> 00:02:12,480
and get the most out of it.

55
00:02:13,504 --> 00:02:14,337
All right.

56
00:02:14,337 --> 00:02:15,170
- That sounds good, guys?

57
00:02:16,230 --> 00:02:17,063
- Thank you.

58
00:02:17,063 --> 00:02:17,910
- Let's do it.

59
00:02:17,910 --> 00:02:19,320
- So, let's begin by level-setting

60
00:02:19,320 --> 00:02:20,770
on what is a data foundation.

61
00:02:22,020 --> 00:02:24,750
On AWS, this is essentially
for data analytics,

62
00:02:24,750 --> 00:02:28,140
this is what it means when we
say data foundation, right?

63
00:02:28,140 --> 00:02:32,580
This is having access to data tools

64
00:02:32,580 --> 00:02:34,380
that allow you to deliver

65
00:02:34,380 --> 00:02:37,380
on basic constructs of data analytics,

66
00:02:37,380 --> 00:02:39,300
like data storage, data processing,

67
00:02:39,300 --> 00:02:42,300
data integration, data governance.

68
00:02:42,300 --> 00:02:45,510
And all of this with the price performance

69
00:02:45,510 --> 00:02:48,870
and the ability to scale
as and when you need it.

70
00:02:48,870 --> 00:02:52,080
This is at its very core is what we mean

71
00:02:52,080 --> 00:02:54,303
by data foundation for data analytics.

72
00:02:56,010 --> 00:02:58,830
And these foundations,
constructs basically,

73
00:02:58,830 --> 00:03:01,740
they come together to give
you a typical data pipeline

74
00:03:01,740 --> 00:03:03,093
that you see over here

75
00:03:03,093 --> 00:03:05,880
with the different
components coming together

76
00:03:05,880 --> 00:03:09,393
to give you an output that is
important to your business.

77
00:03:10,680 --> 00:03:12,603
But what does this mean for AI?

78
00:03:13,710 --> 00:03:15,030
What we are trying to say,

79
00:03:15,030 --> 00:03:19,023
is when you are working on
building AI applications, right,

80
00:03:20,283 --> 00:03:21,750
it doesn't actually have to mean

81
00:03:21,750 --> 00:03:23,400
that you have to look for something new,

82
00:03:23,400 --> 00:03:26,220
or build something new in
terms of data foundation.

83
00:03:26,220 --> 00:03:29,220
You should be able to leverage
what you already have.

84
00:03:29,220 --> 00:03:30,960
And that's essentially the crux

85
00:03:30,960 --> 00:03:33,330
of this particular section over here.

86
00:03:33,330 --> 00:03:36,570
You should be able to evolve and adapt

87
00:03:36,570 --> 00:03:38,910
to build your agentic AI applications.

88
00:03:38,910 --> 00:03:40,590
Because data

89
00:03:40,590 --> 00:03:44,760
and data engineering are
key AI differentiators.

90
00:03:44,760 --> 00:03:48,780
Even though, that might
be a backend aspect

91
00:03:48,780 --> 00:03:50,370
of agentic AI applications,

92
00:03:50,370 --> 00:03:52,230
it is still critical.

93
00:03:52,230 --> 00:03:54,963
And here's that story in two pictures.

94
00:03:54,963 --> 00:03:56,880
Shikha, what do you think?

95
00:03:56,880 --> 00:03:58,290
- What do we think, guys?

96
00:03:58,290 --> 00:03:59,490
Sound okay?

97
00:03:59,490 --> 00:04:01,680
Like do we think data is
a differentiator for AI?

98
00:04:01,680 --> 00:04:03,570
Just show of hands.

99
00:04:03,570 --> 00:04:04,403
Yeah?

100
00:04:04,403 --> 00:04:05,490
Pretty much everybody.

101
00:04:05,490 --> 00:04:06,840
But do you guys ever say

102
00:04:06,840 --> 00:04:10,593
that data engineering is
a differentiator for AI?

103
00:04:12,120 --> 00:04:13,800
Anybody says that?

104
00:04:13,800 --> 00:04:14,880
- I mean, we believe you

105
00:04:14,880 --> 00:04:17,575
when you say data is important for AI.

106
00:04:17,575 --> 00:04:18,408
(Shikha giggles)

107
00:04:18,408 --> 00:04:20,490
But let's be honest,
when we talk about AI,

108
00:04:20,490 --> 00:04:24,120
is data processing, data
storage, data governance,

109
00:04:24,120 --> 00:04:25,633
is that top of mind?

110
00:04:27,008 --> 00:04:27,841
- [Audience Member 2] Yes.

111
00:04:27,841 --> 00:04:29,250
- Yeah, top of mind?

112
00:04:29,250 --> 00:04:30,810
- Nice, we have a great crowd here.

113
00:04:30,810 --> 00:04:33,150
And that is essentially
the point we are trying

114
00:04:33,150 --> 00:04:34,320
to make over here,

115
00:04:34,320 --> 00:04:38,070
which is data is critical for
a successful AI application

116
00:04:38,070 --> 00:04:40,173
and needs a strong data foundation.

117
00:04:41,597 --> 00:04:43,770
And it should be made up of data tools

118
00:04:43,770 --> 00:04:47,430
that allow you to extend the capabilities

119
00:04:47,430 --> 00:04:49,083
to build the AI application.

120
00:04:51,254 --> 00:04:54,060
All right, Shikha, shall we
proceed with the next section?

121
00:04:54,060 --> 00:04:56,130
- Yeah, I think you have
a good story to tell here

122
00:04:56,130 --> 00:04:59,280
and I can see that our
audience is engaged.

123
00:04:59,280 --> 00:05:01,920
So, folks, I'm gonna leave
you in Professor Taz's hands

124
00:05:01,920 --> 00:05:03,150
for a bit, right?

125
00:05:03,150 --> 00:05:04,890
He's gonna show us how this is done

126
00:05:04,890 --> 00:05:08,220
and he'll show us some real
data with real AI agents.

127
00:05:08,220 --> 00:05:09,660
And then I'll come back on stage

128
00:05:09,660 --> 00:05:10,920
to talk about a few other things.

129
00:05:10,920 --> 00:05:11,753
Go for it, Taz.

130
00:05:11,753 --> 00:05:12,586
- Thank you, Shikha.

131
00:05:12,586 --> 00:05:14,223
Okay, so, really quick,

132
00:05:15,660 --> 00:05:18,210
an AI-ready data foundation on an AWS,

133
00:05:18,210 --> 00:05:21,420
at least for businesses,
what does that mean?

134
00:05:21,420 --> 00:05:24,360
Essentially, what we are
saying is it's a foundation

135
00:05:24,360 --> 00:05:26,610
that lets you fuel AI innovations,

136
00:05:26,610 --> 00:05:28,860
speeds up efficient delivery

137
00:05:28,860 --> 00:05:33,860
with little to no impact to
your bottom line as a business.

138
00:05:33,900 --> 00:05:37,470
That is a data foundation
that businesses get from AWS.

139
00:05:37,470 --> 00:05:38,460
But we want to focus

140
00:05:38,460 --> 00:05:41,240
on what do developers and
builders get out of this.

141
00:05:41,240 --> 00:05:43,110
And that is what we will be covering

142
00:05:43,110 --> 00:05:44,433
for the most of this talk.

143
00:05:45,510 --> 00:05:48,570
And this image here is
actually a testament

144
00:05:48,570 --> 00:05:52,020
to the AI-ready data foundation on AWS.

145
00:05:52,020 --> 00:05:56,313
Really quick, any guesses
as to what this image is?

146
00:05:58,163 --> 00:05:59,910
- [Audience Member 3] Project Rainier.

147
00:05:59,910 --> 00:06:00,990
- Thank you.

148
00:06:00,990 --> 00:06:05,100
So, yeah, this is our Project
Rainier Cluster Compute

149
00:06:05,100 --> 00:06:06,540
that was recently launched,

150
00:06:06,540 --> 00:06:09,060
one of the world's largest
AI compute clusters.

151
00:06:09,060 --> 00:06:11,730
It uses a lot of core components

152
00:06:11,730 --> 00:06:14,303
of the AI-ready data
foundation that we have on AWS.

153
00:06:16,560 --> 00:06:19,440
So, for the most part of this talk,

154
00:06:19,440 --> 00:06:22,710
we want to focus on how
can data personalities,

155
00:06:22,710 --> 00:06:24,450
like data engineers,

156
00:06:24,450 --> 00:06:28,500
data analysts, data scientists
get the most benefits

157
00:06:28,500 --> 00:06:30,213
from an AI-ready data foundation.

158
00:06:31,440 --> 00:06:34,410
So, we are recommending
two approaches essentially.

159
00:06:34,410 --> 00:06:36,060
The first is quite fundamental.

160
00:06:36,060 --> 00:06:37,950
It's actually understanding

161
00:06:37,950 --> 00:06:40,860
how your data foundation constructs,

162
00:06:40,860 --> 00:06:44,460
such as storage, processing, ingestion,

163
00:06:44,460 --> 00:06:48,420
are influenced when you
build an AI application.

164
00:06:48,420 --> 00:06:51,000
And the second is more,

165
00:06:51,000 --> 00:06:52,500
it's a more prescriptive approach

166
00:06:52,500 --> 00:06:55,650
about deploying the AI-ready capabilities

167
00:06:55,650 --> 00:06:58,320
that the data foundation has

168
00:06:58,320 --> 00:07:00,513
to accelerate your AI development.

169
00:07:01,350 --> 00:07:03,630
The first approach is best understood

170
00:07:03,630 --> 00:07:05,703
by looking at the evolution of AI.

171
00:07:06,570 --> 00:07:08,040
Now, this has rapidly progressed

172
00:07:08,040 --> 00:07:10,440
from something simple like assistance

173
00:07:10,440 --> 00:07:13,230
to more complex autonomous systems.

174
00:07:13,230 --> 00:07:15,270
We started with GenAI assistance

175
00:07:15,270 --> 00:07:19,560
that follow predefined rules
to automate repetitive tasks.

176
00:07:19,560 --> 00:07:23,700
Then, quickly, moved on to AI
agents that are goal-oriented,

177
00:07:23,700 --> 00:07:27,240
capable of handling a
broader range of tasks

178
00:07:27,240 --> 00:07:29,730
and adopting the changing environments.

179
00:07:29,730 --> 00:07:32,400
And then, today, where we
are at, is we are moving

180
00:07:32,400 --> 00:07:36,030
towards fully autonomous
multi-agent systems

181
00:07:36,030 --> 00:07:39,240
that can make complex decisions.

182
00:07:39,240 --> 00:07:42,240
The evolution of AI has a direct impact

183
00:07:42,240 --> 00:07:45,570
on your typical data pipeline.

184
00:07:45,570 --> 00:07:48,870
They also evolve when used in the context

185
00:07:48,870 --> 00:07:50,643
of building AI applications.

186
00:07:52,710 --> 00:07:55,740
The very first thing or one
of the very first things

187
00:07:55,740 --> 00:08:00,740
that AI introduces is a need
for additional data sources.

188
00:08:01,380 --> 00:08:04,980
And these are primarily in
the form of unstructured data.

189
00:08:04,980 --> 00:08:07,800
Now, unstructured data
is not a new construct.

190
00:08:07,800 --> 00:08:09,780
However, it is new in the sense

191
00:08:09,780 --> 00:08:13,680
that it doesn't conform
to a preset data model,

192
00:08:13,680 --> 00:08:15,960
it doesn't have a predefined schema.

193
00:08:15,960 --> 00:08:20,340
And this is new territory
for most data personalities.

194
00:08:20,340 --> 00:08:23,610
Functions like metadata
management, data governance

195
00:08:23,610 --> 00:08:27,480
for unstructured data
takes a whole new meaning

196
00:08:27,480 --> 00:08:28,650
and has a big influence

197
00:08:28,650 --> 00:08:32,340
on things like data
processing, data integration

198
00:08:32,340 --> 00:08:33,513
and data storage.

199
00:08:35,850 --> 00:08:36,990
The next construct

200
00:08:36,990 --> 00:08:40,110
that gets influenced is
around data processing, right?

201
00:08:40,110 --> 00:08:43,530
This is an area where
you would start working

202
00:08:43,530 --> 00:08:47,400
on aspects of training
or validating your data

203
00:08:47,400 --> 00:08:50,430
to move data from data
warehouses or data lakes

204
00:08:50,430 --> 00:08:52,590
to improve the model accuracy,

205
00:08:52,590 --> 00:08:54,240
or you could be running inference

206
00:08:54,240 --> 00:08:57,450
for your continued pre-training approach

207
00:08:57,450 --> 00:08:59,760
for near real-world scenarios,

208
00:08:59,760 --> 00:09:03,150
or managing vector data
for a RAG-based application

209
00:09:03,150 --> 00:09:05,640
to provide real-time context.

210
00:09:05,640 --> 00:09:09,090
And all this data processing
requires advanced forms

211
00:09:09,090 --> 00:09:11,343
of data integrations and data storage.

212
00:09:12,930 --> 00:09:17,040
Next, some AI applications
also incorporate techniques,

213
00:09:17,040 --> 00:09:21,900
such as including human feedback
to optimize the ML models.

214
00:09:21,900 --> 00:09:26,310
These applications need to
capture user input in real time

215
00:09:26,310 --> 00:09:29,220
for a highly personalized user experience.

216
00:09:29,220 --> 00:09:30,840
Capturing this information

217
00:09:30,840 --> 00:09:34,650
in the right kind of storage
with efficient pipelines helps

218
00:09:34,650 --> 00:09:36,903
with latency and accuracy.

219
00:09:38,490 --> 00:09:42,840
And then finally, because an
AI application opens itself up

220
00:09:42,840 --> 00:09:47,760
to a much wider variety
of data sources and users,

221
00:09:47,760 --> 00:09:50,160
data governance becomes a function

222
00:09:50,160 --> 00:09:52,740
of the entire end-to-end pipeline.

223
00:09:52,740 --> 00:09:53,880
Behind the scenes,

224
00:09:53,880 --> 00:09:57,540
governance functions such as
data privacy, data quality,

225
00:09:57,540 --> 00:10:00,060
data cataloging are required

226
00:10:00,060 --> 00:10:02,550
to deliver a comprehensive data governance

227
00:10:02,550 --> 00:10:03,813
for an AI application.

228
00:10:06,240 --> 00:10:07,650
Okay, so, that was the first part

229
00:10:07,650 --> 00:10:11,100
of how data personalities
can make the most, right?

230
00:10:11,100 --> 00:10:15,090
Have a macro-vision

231
00:10:15,090 --> 00:10:17,190
of what it means to the data pipeline.

232
00:10:17,190 --> 00:10:19,800
The next aspect is a little
bit more prescriptive approach,

233
00:10:19,800 --> 00:10:21,780
like taking a deeper look

234
00:10:21,780 --> 00:10:26,040
into how to make the most of
the AI-ready data foundation.

235
00:10:26,040 --> 00:10:29,190
And that is about leveraging
the enhanced capabilities

236
00:10:29,190 --> 00:10:30,570
of that foundation

237
00:10:30,570 --> 00:10:34,380
to drive AI developer
and end user experience.

238
00:10:34,380 --> 00:10:36,360
And we thought it best to cover

239
00:10:36,360 --> 00:10:40,350
with an example agentic AI
travel reservation application,

240
00:10:40,350 --> 00:10:41,750
just like I promised Shikha.

241
00:10:44,700 --> 00:10:45,840
All right,

242
00:10:45,840 --> 00:10:50,370
this is an airlines reservation
agentic AI demo application

243
00:10:50,370 --> 00:10:51,320
that we have built.

244
00:10:52,530 --> 00:10:55,530
I asked the agent to show
me all available flights

245
00:10:55,530 --> 00:11:00,063
from JFK, New York to
Los Angeles, California.

246
00:11:02,190 --> 00:11:07,190
The agent starts thinking
about it and goes to work.

247
00:11:08,190 --> 00:11:10,110
For the purpose of this demo,

248
00:11:10,110 --> 00:11:14,100
you will see, we are also
printing diagnostic information.

249
00:11:14,100 --> 00:11:18,090
So, that tells us what
the agent is calling,

250
00:11:18,090 --> 00:11:20,343
what it's doing and for what purpose.

251
00:11:21,840 --> 00:11:25,110
So, we give it a few seconds
for the agent to do its job.

252
00:11:25,110 --> 00:11:28,440
And then, we can see very soon

253
00:11:28,440 --> 00:11:32,850
that the agent will return
several flight options for me.

254
00:11:32,850 --> 00:11:35,790
And those options will include things,

255
00:11:35,790 --> 00:11:40,470
like flight schedules
for different airlines,

256
00:11:40,470 --> 00:11:44,793
it will show me the ticket
pricing again by each airline.

257
00:11:45,630 --> 00:11:48,840
In this case, it was talking
to a third-party travel agency.

258
00:11:48,840 --> 00:11:50,610
It'll show me user reviews

259
00:11:50,610 --> 00:11:54,360
from my colleagues in my organization

260
00:11:54,360 --> 00:11:58,380
that have flown the same
airline in the recent past.

261
00:11:58,380 --> 00:12:01,740
So, like that gives me a
well-rounded, well-informed way

262
00:12:01,740 --> 00:12:04,980
of choosing the flight that
I want to use over here.

263
00:12:04,980 --> 00:12:05,850
It's also telling me

264
00:12:05,850 --> 00:12:08,580
that based on my company travel policies,

265
00:12:08,580 --> 00:12:10,440
there are certain cases

266
00:12:10,440 --> 00:12:14,970
where I might need an exception
to book that flight, right?

267
00:12:14,970 --> 00:12:18,210
I mean, we at Amazon, have that policy.

268
00:12:18,210 --> 00:12:20,490
I'm hoping most organizations
have something similar

269
00:12:20,490 --> 00:12:23,910
that they can relate to,
is when making a choice

270
00:12:23,910 --> 00:12:26,670
that doesn't align with a company policy,

271
00:12:26,670 --> 00:12:28,920
you have to work through
an exception process.

272
00:12:28,920 --> 00:12:30,810
So, that's what's happening over here.

273
00:12:30,810 --> 00:12:31,643
And then, finally,

274
00:12:31,643 --> 00:12:34,200
the agent is also making recommendations

275
00:12:34,200 --> 00:12:36,840
that are actually policy-compliant

276
00:12:36,840 --> 00:12:40,110
and factors in my travel preferences.

277
00:12:40,110 --> 00:12:41,970
So, this is the first
part of the demo, right,

278
00:12:41,970 --> 00:12:43,420
getting all that information.

279
00:12:45,780 --> 00:12:48,180
Now, we are calling this scenario one,

280
00:12:48,180 --> 00:12:49,313
because there is a scenario two.

281
00:12:49,313 --> 00:12:52,800
This is where I feel I
don't want to get in trouble

282
00:12:52,800 --> 00:12:53,730
with my manager, right?

283
00:12:53,730 --> 00:12:56,160
I mean, I don't need to get an exception.

284
00:12:56,160 --> 00:12:57,270
And I go ahead

285
00:12:57,270 --> 00:13:02,270
and I ask the agent to book
the American Airlines flight.

286
00:13:03,630 --> 00:13:04,710
I know that is compliant,

287
00:13:04,710 --> 00:13:07,680
looking at the pricing aspect
that it's showed me earlier,

288
00:13:07,680 --> 00:13:09,660
it works with the company policy.

289
00:13:09,660 --> 00:13:13,380
And timing-wise, let's say for
now, it works for me as well.

290
00:13:13,380 --> 00:13:16,263
Again, the agent's thinking
starts going to work.

291
00:13:17,130 --> 00:13:19,320
Similar to previous part,

292
00:13:19,320 --> 00:13:21,240
we are printing some
diagnostic information

293
00:13:21,240 --> 00:13:22,863
that we will come back to later.

294
00:13:24,450 --> 00:13:26,730
The agent confirms the reservation.

295
00:13:26,730 --> 00:13:31,590
It uses my travel preferences
such as my meal type,

296
00:13:31,590 --> 00:13:33,120
the kind of seats I prefer,

297
00:13:33,120 --> 00:13:34,680
whether a window seat or an aisle seat,

298
00:13:34,680 --> 00:13:36,090
what kind of person I am

299
00:13:36,090 --> 00:13:38,250
and performs a few backend operations,

300
00:13:38,250 --> 00:13:40,893
like emailing me the e-ticket and stuff.

301
00:13:41,910 --> 00:13:44,250
Okay, so, that was a demo.

302
00:13:44,250 --> 00:13:47,490
And for the purpose of
the rest of this talk,

303
00:13:47,490 --> 00:13:50,430
we will use this reference architecture

304
00:13:50,430 --> 00:13:52,620
of the airlines demo

305
00:13:52,620 --> 00:13:55,803
to help us work through what's
happening behind the scenes.

306
00:13:56,640 --> 00:14:00,060
The user prompts,
interacts with the agent,

307
00:14:00,060 --> 00:14:04,320
which in turn then works with
a list of tools or API calls

308
00:14:04,320 --> 00:14:06,990
that integrate with
different types of sources,

309
00:14:06,990 --> 00:14:09,060
like the third-party travel agency

310
00:14:09,060 --> 00:14:10,980
that makes flight data available,

311
00:14:10,980 --> 00:14:15,690
the user's internal corporate
data warehouse and data lake,

312
00:14:15,690 --> 00:14:17,970
which is on Amazon Redshift and S3

313
00:14:17,970 --> 00:14:20,103
and a booking action with a third-party.

314
00:14:20,970 --> 00:14:23,790
There are a few operational
transactions happening

315
00:14:23,790 --> 00:14:24,990
behind the scenes

316
00:14:24,990 --> 00:14:27,933
that we will cover when we
talk about multiple agents.

317
00:14:28,920 --> 00:14:30,810
So, just for context,

318
00:14:30,810 --> 00:14:32,160
this is a demo

319
00:14:32,160 --> 00:14:35,913
that we whitecoded using
Kiro and Strands Agents.

320
00:14:36,900 --> 00:14:39,687
People, you're familiar with
Kiro and Strands Agents?

321
00:14:39,687 --> 00:14:40,520
- [Audience Member 4] Yes.

322
00:14:40,520 --> 00:14:41,353
- Nice.

323
00:14:41,353 --> 00:14:44,920
For those, who aren't, Kiro is AWS' AI IDE

324
00:14:46,080 --> 00:14:48,513
that turns prompts into
specs and then into code.

325
00:14:48,513 --> 00:14:50,310
It's actually pretty cool.

326
00:14:50,310 --> 00:14:52,740
And then, Strands Agents
is an open-source SDK

327
00:14:52,740 --> 00:14:57,060
that takes a model-driven
approach to building AI agents.

328
00:14:57,060 --> 00:15:00,060
Strands supports all LLMs.

329
00:15:00,060 --> 00:15:03,120
I suspect most of us
have our favorite model.

330
00:15:03,120 --> 00:15:04,680
Mine is Claude Sonnet

331
00:15:04,680 --> 00:15:06,653
and that is what we are using over here.

332
00:15:10,200 --> 00:15:11,033
There you go.

333
00:15:12,240 --> 00:15:14,730
All right, before we jump

334
00:15:14,730 --> 00:15:16,290
into the details of the application,

335
00:15:16,290 --> 00:15:18,840
I want to take a slight detour over here

336
00:15:18,840 --> 00:15:22,560
to position what is generally
called an agentic loop,

337
00:15:22,560 --> 00:15:24,213
or a ReAct loop.

338
00:15:25,170 --> 00:15:26,850
In terms of the transaction workflow

339
00:15:26,850 --> 00:15:31,800
between the agent and the
tool and the model or the LLM,

340
00:15:31,800 --> 00:15:34,630
one may think that the
flow is more or less linear

341
00:15:35,700 --> 00:15:40,023
as in the user prompt invokes the agent,

342
00:15:41,400 --> 00:15:44,460
which in turn, executes the tool API

343
00:15:44,460 --> 00:15:47,760
and then, returns the results to the agent

344
00:15:47,760 --> 00:15:51,123
and then, returns the final
response to the prompt.

345
00:15:52,320 --> 00:15:53,973
Well, not really.

346
00:15:55,695 --> 00:15:59,307
At the center of an agentic
AI application is a loop.

347
00:16:00,600 --> 00:16:03,270
When the user instructs the system

348
00:16:03,270 --> 00:16:06,360
to complete a task or a transaction,

349
00:16:06,360 --> 00:16:09,780
the workflow enters an event loop

350
00:16:09,780 --> 00:16:14,700
where it iterates until it
considers the task completed,

351
00:16:14,700 --> 00:16:16,860
or the question answered.

352
00:16:16,860 --> 00:16:21,570
This design pattern is typically
called the Reason plus Act,

353
00:16:21,570 --> 00:16:23,010
or the ReAct loop

354
00:16:23,010 --> 00:16:24,840
and is the most popular design pattern

355
00:16:24,840 --> 00:16:27,390
for agentic AI applications.

356
00:16:27,390 --> 00:16:29,190
This is how we are illustrating it.

357
00:16:29,190 --> 00:16:33,000
This basic architecture allows
an agentic AI application

358
00:16:33,000 --> 00:16:35,400
to accomplish tasks over the course

359
00:16:35,400 --> 00:16:38,250
of multiple event loop iterations.

360
00:16:38,250 --> 00:16:40,770
Now, while on one hand,

361
00:16:40,770 --> 00:16:44,310
this event loop means a more meaningful

362
00:16:44,310 --> 00:16:46,950
and relevant response to the user,

363
00:16:46,950 --> 00:16:48,930
however, on the other hand,

364
00:16:48,930 --> 00:16:53,430
it also means a slower
and less efficient agent.

365
00:16:53,430 --> 00:16:56,400
It also means an unpredictable number

366
00:16:56,400 --> 00:17:00,990
of model invocations consuming
an uncontrolled amount

367
00:17:00,990 --> 00:17:03,840
of input and output tokens

368
00:17:03,840 --> 00:17:06,750
that impact both cost and performance.

369
00:17:06,750 --> 00:17:10,710
A good data foundation can help
with minimizing that impact,

370
00:17:10,710 --> 00:17:14,373
starting with the AWS data
and analytics MCP servers.

371
00:17:17,670 --> 00:17:21,180
Coming back to the reference
architecture of our agent demo,

372
00:17:21,180 --> 00:17:23,400
let's look at the tools that we are using.

373
00:17:23,400 --> 00:17:27,540
These are either hardcoded
API calls or function calls

374
00:17:27,540 --> 00:17:31,140
that you can pass as tools
to the agent to invoke.

375
00:17:31,140 --> 00:17:31,973
What this means,

376
00:17:31,973 --> 00:17:36,420
is that the agent's capabilities
are fixed at design time.

377
00:17:36,420 --> 00:17:39,360
It doesn't really have
a choice or the ability

378
00:17:39,360 --> 00:17:43,140
to reason or plan which
tool to use to call,

379
00:17:43,140 --> 00:17:44,610
depending on the task.

380
00:17:44,610 --> 00:17:46,770
You are limiting its capabilities

381
00:17:46,770 --> 00:17:49,410
and all the queries that
it is using also need

382
00:17:49,410 --> 00:17:52,203
to be predefined for the usage over here.

383
00:17:53,190 --> 00:17:56,310
This is where MCP servers bring value.

384
00:17:56,310 --> 00:17:58,230
You let the model figure out

385
00:17:58,230 --> 00:18:00,870
what it needs through natural language.

386
00:18:00,870 --> 00:18:02,760
It also replaces the heavy-lifting

387
00:18:02,760 --> 00:18:06,630
of managing multiple API SDKs and formats,

388
00:18:06,630 --> 00:18:10,233
which I can imagine is a
production nightmare to most.

389
00:18:11,160 --> 00:18:14,610
MCPs enable true agent autonomy,

390
00:18:14,610 --> 00:18:18,870
the ability to iteratively
call tools, get results

391
00:18:18,870 --> 00:18:22,143
and decide the next steps in a ReAct loop.

392
00:18:25,950 --> 00:18:30,090
Just a little bit of
overview on MCP servers.

393
00:18:30,090 --> 00:18:32,250
They don't really replace APIs.

394
00:18:32,250 --> 00:18:35,580
They add a conversational
layer to the LLM.

395
00:18:35,580 --> 00:18:37,710
They act as a universal language,

396
00:18:37,710 --> 00:18:40,770
eliminating the need for
custom integration layers

397
00:18:40,770 --> 00:18:43,773
between different AI models and tools.

398
00:18:44,700 --> 00:18:48,780
Too much structured data with
traditional API calls tends

399
00:18:48,780 --> 00:18:50,223
to confuse the agent.

400
00:18:51,180 --> 00:18:52,500
I do want to point out over here

401
00:18:52,500 --> 00:18:56,940
that MCP servers are not
always a one-size-fit-all.

402
00:18:56,940 --> 00:18:58,950
There are operations
that are better served

403
00:18:58,950 --> 00:19:00,960
through traditional API calls.

404
00:19:00,960 --> 00:19:04,200
For example, the
MCP-driven agents struggle

405
00:19:04,200 --> 00:19:08,460
with large-scale data
processing with structured data

406
00:19:08,460 --> 00:19:10,260
and complex data transformations even,

407
00:19:10,260 --> 00:19:13,230
such as the get flight details function

408
00:19:13,230 --> 00:19:14,970
that you saw earlier over there.

409
00:19:14,970 --> 00:19:16,650
We did not convert that into MCP,

410
00:19:16,650 --> 00:19:18,510
we didn't have an MCP server for it.

411
00:19:18,510 --> 00:19:22,200
But regardless, a direct API
call serves a better purpose

412
00:19:22,200 --> 00:19:24,240
for those kinds of transactions.

413
00:19:24,240 --> 00:19:25,470
Similarly, operations

414
00:19:25,470 --> 00:19:30,470
that require deterministic
and time-sensitive outcomes,

415
00:19:30,570 --> 00:19:34,140
like the booking a reservation,
right, that function call,

416
00:19:34,140 --> 00:19:37,020
that is also a better fit
for a direct API call,

417
00:19:37,020 --> 00:19:39,600
instead of an MCP server call.

418
00:19:39,600 --> 00:19:41,940
It's very easy for these operations,

419
00:19:41,940 --> 00:19:43,560
the kinds that we just discussed

420
00:19:43,560 --> 00:19:46,410
to exceed the LLMs' context windows

421
00:19:46,410 --> 00:19:48,633
and drive up costs and latency.

422
00:19:49,620 --> 00:19:54,620
In our demo, we used the Redshift
and S3 Tables MCP servers.

423
00:19:55,380 --> 00:19:57,840
The Redshift MCP server acts as a bridge

424
00:19:57,840 --> 00:20:01,470
between your Redshift
clients sitting on the agent

425
00:20:01,470 --> 00:20:03,753
and your Amazon Redshift infrastructure.

426
00:20:04,830 --> 00:20:07,980
It does natural language transformations.

427
00:20:07,980 --> 00:20:09,960
And among other things,

428
00:20:09,960 --> 00:20:12,420
it automatically discovers
your Redshift clusters,

429
00:20:12,420 --> 00:20:14,310
whether it's provisioned or serverless,

430
00:20:14,310 --> 00:20:16,770
can work across multiple clusters,

431
00:20:16,770 --> 00:20:19,980
browse databases, schemas,
tables and columns

432
00:20:19,980 --> 00:20:21,690
through natural language

433
00:20:21,690 --> 00:20:24,840
and execute SQL queries in read-only mode,

434
00:20:24,840 --> 00:20:27,990
which provides you with
built-in safety protections.

435
00:20:27,990 --> 00:20:31,410
Likewise, with the Amazon
S3 Tables MCP server,

436
00:20:31,410 --> 00:20:34,200
you can interact with tabular datasets,

437
00:20:34,200 --> 00:20:36,210
using natural language.

438
00:20:36,210 --> 00:20:39,030
You can list and create S3 Table buckets,

439
00:20:39,030 --> 00:20:41,430
namespace and tables.

440
00:20:41,430 --> 00:20:44,880
You can also query and
commit data to an S3 table

441
00:20:44,880 --> 00:20:46,173
through this MCP server.

442
00:20:48,120 --> 00:20:51,150
The AWS MCP server repository is a suite

443
00:20:51,150 --> 00:20:53,550
of specialized MCP servers.

444
00:20:53,550 --> 00:20:54,720
And we invite you

445
00:20:54,720 --> 00:20:58,200
to check out all the publicly
available MCP servers

446
00:20:58,200 --> 00:21:01,470
that can help you make your
agentic AI applications,

447
00:21:01,470 --> 00:21:03,993
more autonomous and less expensive.

448
00:21:06,510 --> 00:21:10,350
Moving on from MCP servers
to context management,

449
00:21:10,350 --> 00:21:13,890
another area where data personalities,

450
00:21:13,890 --> 00:21:15,570
like data engineers, data analysts,

451
00:21:15,570 --> 00:21:19,290
data scientists can leverage
the AI-ready data foundation

452
00:21:19,290 --> 00:21:20,123
on AWS.

453
00:21:21,600 --> 00:21:24,060
We touched a little bit on context

454
00:21:24,060 --> 00:21:27,240
when we discussed the ReAct agentic loop.

455
00:21:27,240 --> 00:21:32,240
A simple way to think about
context here is to think of it

456
00:21:32,490 --> 00:21:36,780
as the working memory of
the model, of the LLM,

457
00:21:36,780 --> 00:21:40,530
the memory that lets it interpret, reason

458
00:21:40,530 --> 00:21:43,863
and then take action for
the best outcome possible.

459
00:21:45,960 --> 00:21:48,780
Let's look at that in a
little bit more detail.

460
00:21:48,780 --> 00:21:52,410
So, while AI agents get
sophisticated reasoning capabilities

461
00:21:52,410 --> 00:21:54,180
from models and LLMs,

462
00:21:54,180 --> 00:21:56,550
they face two key limitations.

463
00:21:56,550 --> 00:21:59,160
One, LLMs are stateless

464
00:21:59,160 --> 00:22:02,130
and unable to remember past interactions,

465
00:22:02,130 --> 00:22:03,870
or maintain context.

466
00:22:03,870 --> 00:22:08,280
And two, the finite or
limited context window

467
00:22:08,280 --> 00:22:10,230
that LLMs have,

468
00:22:10,230 --> 00:22:13,200
restricts the amount of
data that they can hold

469
00:22:13,200 --> 00:22:16,203
and use it to process at each step.

470
00:22:18,480 --> 00:22:19,980
Now, the kind of data

471
00:22:19,980 --> 00:22:23,100
that the model typically keeps

472
00:22:23,100 --> 00:22:26,970
within the context window
involves three core areas.

473
00:22:26,970 --> 00:22:29,760
First is the instruction set, right,

474
00:22:29,760 --> 00:22:32,670
instructions such as the
user and system prompts,

475
00:22:32,670 --> 00:22:36,330
the state of the conversation
session that you're having,

476
00:22:36,330 --> 00:22:40,110
conversation history, learned
lessons, user profiles

477
00:22:40,110 --> 00:22:40,950
and so on.

478
00:22:40,950 --> 00:22:42,780
So, that's the kind of
data and information

479
00:22:42,780 --> 00:22:46,920
that sits in that instruction set.

480
00:22:46,920 --> 00:22:49,860
The second area is the knowledge base.

481
00:22:49,860 --> 00:22:53,010
So, knowledge bases are,

482
00:22:53,010 --> 00:22:55,560
again, think of it like
centralized repositories

483
00:22:55,560 --> 00:22:58,950
of structured and unstructured data.

484
00:22:58,950 --> 00:23:02,733
This is information that serves
as grounding data for AI.

485
00:23:03,662 --> 00:23:04,650
They can include things,

486
00:23:04,650 --> 00:23:09,240
like documents, FAQs,
product specifications,

487
00:23:09,240 --> 00:23:11,610
best practices from your organization.

488
00:23:11,610 --> 00:23:12,600
In our demo,

489
00:23:12,600 --> 00:23:16,050
we are using the company
travel policy documents

490
00:23:16,050 --> 00:23:16,950
as the knowledge base

491
00:23:16,950 --> 00:23:18,330
and we look at it

492
00:23:18,330 --> 00:23:21,060
in a little bit more deep
in this, very quickly.

493
00:23:21,060 --> 00:23:23,730
This is where the agent can tell me,

494
00:23:23,730 --> 00:23:27,330
which tickets are within
the company travel policy

495
00:23:27,330 --> 00:23:29,310
and which are not.

496
00:23:29,310 --> 00:23:30,660
And then we have data coming

497
00:23:30,660 --> 00:23:34,350
from the tools or the MCP
servers that we just saw.

498
00:23:34,350 --> 00:23:39,000
In this demo, we bring the
airline reviews from S3 Tables.

499
00:23:39,000 --> 00:23:41,700
These are reviews by my colleagues

500
00:23:41,700 --> 00:23:44,460
that were posted into that knowledge base

501
00:23:44,460 --> 00:23:46,620
that we use make decisions.

502
00:23:46,620 --> 00:23:48,000
So, the model needs to work

503
00:23:48,000 --> 00:23:51,120
with a lot of current
and fresh information

504
00:23:51,120 --> 00:23:53,940
within its limited context window

505
00:23:53,940 --> 00:23:57,660
to be able to advise me on
the right course of action.

506
00:23:57,660 --> 00:23:59,670
This is where context management,

507
00:23:59,670 --> 00:24:02,343
or context engineering comes into play.

508
00:24:04,920 --> 00:24:09,480
This working memory as
defined by a context window,

509
00:24:09,480 --> 00:24:11,910
is a length of the maximum amount

510
00:24:11,910 --> 00:24:16,560
of input data it can hold
or consider or remember

511
00:24:16,560 --> 00:24:17,733
at any point in time.

512
00:24:18,780 --> 00:24:21,030
And here's the kicker for that.

513
00:24:21,030 --> 00:24:25,383
Context windows are finite
and tokens are expensive.

514
00:24:26,880 --> 00:24:31,080
We saw earlier how MCP
servers are designed

515
00:24:31,080 --> 00:24:32,790
to be more context-efficient

516
00:24:32,790 --> 00:24:35,850
and less verbose than traditional APIs.

517
00:24:35,850 --> 00:24:38,400
So, next, we will look at how best

518
00:24:38,400 --> 00:24:42,510
to manage the limited and
expensive context window

519
00:24:42,510 --> 00:24:46,323
with the help of agentic
memory and vector stores.

520
00:24:52,170 --> 00:24:55,950
All right, so, let's
quickly refresh our memory

521
00:24:55,950 --> 00:24:59,580
from the parts of the demo
where the agent is retrieving

522
00:24:59,580 --> 00:25:02,310
and memorizing several data points,

523
00:25:02,310 --> 00:25:05,250
including flight availability information,

524
00:25:05,250 --> 00:25:08,580
my booking reservation
and my travel preferences.

525
00:25:08,580 --> 00:25:13,080
So, this is the agentic memory
in play, in action over here.

526
00:25:13,080 --> 00:25:15,630
Here, for this function,
what we are doing,

527
00:25:15,630 --> 00:25:19,170
is we're using the
Amazon Bedrock AgentCore

528
00:25:19,170 --> 00:25:21,183
to serve the purpose of agentic memory.

529
00:25:22,800 --> 00:25:24,810
AgentCore, some of you might know this,

530
00:25:24,810 --> 00:25:27,060
it's a modular service

531
00:25:27,060 --> 00:25:29,220
that provides you
everything you need to go

532
00:25:29,220 --> 00:25:33,900
with agents, up to production,
including observability.

533
00:25:33,900 --> 00:25:34,733
In our example,

534
00:25:34,733 --> 00:25:38,220
we are using it to deliver
personalized experiences

535
00:25:38,220 --> 00:25:42,780
with persistent memory stored
in this particular service.

536
00:25:42,780 --> 00:25:46,800
It supports most of the popular
frameworks and protocols,

537
00:25:46,800 --> 00:25:48,450
like the Strands Agents

538
00:25:48,450 --> 00:25:51,570
and the MCP servers that we saw earlier

539
00:25:51,570 --> 00:25:53,120
that we are using for our demo.

540
00:25:55,890 --> 00:25:58,050
And then, depending on the nature

541
00:25:58,050 --> 00:26:01,290
of your agentic application
and the business case,

542
00:26:01,290 --> 00:26:05,730
if you're a shop that
needs an over-the-shelf,

543
00:26:05,730 --> 00:26:07,110
or off-the-shelf kind of solution,

544
00:26:07,110 --> 00:26:10,170
like Bedrock AgentCore is
a good choice over there,

545
00:26:10,170 --> 00:26:11,820
or if you're an organization

546
00:26:11,820 --> 00:26:13,710
that needs to build something custom,

547
00:26:13,710 --> 00:26:16,650
something more specific to your needs,

548
00:26:16,650 --> 00:26:18,900
AWS has several other options

549
00:26:18,900 --> 00:26:22,260
that agents can use to
build their agentic memory.

550
00:26:22,260 --> 00:26:25,470
These include things like
ElastiCache for Valkey

551
00:26:25,470 --> 00:26:28,860
and in-memory, high-performance,
key-value datastore.

552
00:26:28,860 --> 00:26:32,850
We also have DynamoDB or
distributed NoSQL database,

553
00:26:32,850 --> 00:26:33,900
Neptune Analytics

554
00:26:33,900 --> 00:26:37,440
for knowledge, graph-based
dynamic relationships

555
00:26:37,440 --> 00:26:39,210
and several others.

556
00:26:39,210 --> 00:26:42,090
So, the agent can retain knowledge

557
00:26:42,090 --> 00:26:44,160
to learn and problem-solve.

558
00:26:44,160 --> 00:26:46,320
It can deliver contextual intelligence

559
00:26:46,320 --> 00:26:48,120
through pattern recognition

560
00:26:48,120 --> 00:26:49,770
and personalized interactions

561
00:26:49,770 --> 00:26:53,913
by remembering preferences
and past conversations.

562
00:26:55,500 --> 00:26:56,460
The next important piece

563
00:26:56,460 --> 00:26:59,850
of context management is
knowledge bases, right?

564
00:26:59,850 --> 00:27:04,850
This is where unstructured
data comes into the front.

565
00:27:05,550 --> 00:27:06,570
In our demo,

566
00:27:06,570 --> 00:27:09,480
we are using the company's
travel policy documents,

567
00:27:09,480 --> 00:27:13,980
such as PDF, HTML files scraped through

568
00:27:13,980 --> 00:27:17,250
to build a knowledge base on Amazon S3.

569
00:27:17,250 --> 00:27:21,330
And then, we are using Amazon
OpenSearch as a vector store

570
00:27:21,330 --> 00:27:23,310
to do a semantic search

571
00:27:23,310 --> 00:27:25,803
to apply the correct
policy to the booking.

572
00:27:28,590 --> 00:27:30,390
Again, like we did previously,

573
00:27:30,390 --> 00:27:33,120
let's refresh our memory
on the information

574
00:27:33,120 --> 00:27:37,170
that our agent is using
from the knowledge base

575
00:27:37,170 --> 00:27:39,783
to apply the relevant travel policy.

576
00:27:41,820 --> 00:27:45,690
We see a pricing policy
around lower and upper bounds

577
00:27:45,690 --> 00:27:48,240
that was retrieved from
the vector database

578
00:27:48,240 --> 00:27:50,403
to add to the context window.

579
00:27:51,300 --> 00:27:54,030
In this case, our agent is
using the knowledge bases

580
00:27:54,030 --> 00:27:57,393
for Amazon Bedrock service
to perform this function.

581
00:27:58,830 --> 00:28:01,440
Now, the way this is
working behind the scenes,

582
00:28:01,440 --> 00:28:06,440
this service provides the
end-to-end RAG workflow for us.

583
00:28:06,870 --> 00:28:09,960
You simply specify the
location of your data,

584
00:28:09,960 --> 00:28:11,703
which, in this case, is S3.

585
00:28:12,570 --> 00:28:14,070
It selects an embedding model

586
00:28:14,070 --> 00:28:16,890
to convert that data
into vector embeddings.

587
00:28:16,890 --> 00:28:20,700
And then have the service
point to a vector store,

588
00:28:20,700 --> 00:28:22,980
which, in our case, is Amazon OpenSearch

589
00:28:22,980 --> 00:28:25,173
to create that vector index.

590
00:28:26,130 --> 00:28:28,290
Our agent fetches that context

591
00:28:28,290 --> 00:28:31,710
and adds it to the context
window in the model,

592
00:28:31,710 --> 00:28:33,690
so, for the model to be able to reason

593
00:28:33,690 --> 00:28:36,813
and use that information in
the most appropriate way.

594
00:28:40,950 --> 00:28:44,700
A little bit about vector
engine for Amazon OpenSearch,

595
00:28:44,700 --> 00:28:46,200
it's a search service,

596
00:28:46,200 --> 00:28:49,750
delivers highly relevant
and accurate responses

597
00:28:51,210 --> 00:28:54,930
and provides the vector indexes of scales

598
00:28:54,930 --> 00:28:57,210
that are production-ready.

599
00:28:57,210 --> 00:28:59,190
And at this "re:Invent",

600
00:28:59,190 --> 00:29:01,530
we are introducing the
general availability

601
00:29:01,530 --> 00:29:05,580
of GPU acceleration for the
OpenSearch vector engine.

602
00:29:05,580 --> 00:29:09,240
By offloading vector search
computations to GPUs,

603
00:29:09,240 --> 00:29:12,060
OpenSearch can achieve up to 10 times

604
00:29:12,060 --> 00:29:14,970
on speed and compute costs,

605
00:29:14,970 --> 00:29:15,803
which is great

606
00:29:15,803 --> 00:29:19,380
for applications like RAG,
Retrieval-Augmented Generation

607
00:29:19,380 --> 00:29:21,633
and recommendation systems.

608
00:29:24,090 --> 00:29:28,500
And similarly, just like
AWS for agentic memory,

609
00:29:28,500 --> 00:29:32,490
depending on your use case
and your business needs,

610
00:29:32,490 --> 00:29:34,230
AWS provides additional choices

611
00:29:34,230 --> 00:29:37,473
for deploying a vector store
for your AI application,

612
00:29:38,850 --> 00:29:40,860
including the general availability

613
00:29:40,860 --> 00:29:45,180
of Amazon S3 Vectors
announced at this "re:Invent"

614
00:29:45,180 --> 00:29:48,690
for cost-effective,
large-scale vector processing,

615
00:29:48,690 --> 00:29:49,923
or vector storage.

616
00:29:52,290 --> 00:29:55,860
Okay, besides personalized responses,

617
00:29:55,860 --> 00:29:57,900
another area of context management,

618
00:29:57,900 --> 00:30:00,720
is to have a timely response.

619
00:30:00,720 --> 00:30:04,710
A travel reservation agentic
AI application is expected

620
00:30:04,710 --> 00:30:06,960
to maintain the latest information

621
00:30:06,960 --> 00:30:09,120
of the user's travel preferences

622
00:30:09,120 --> 00:30:11,160
as they can change over time.

623
00:30:11,160 --> 00:30:15,330
This is where Change Data
Capture, CDC events combined

624
00:30:15,330 --> 00:30:18,030
with streaming ingestion, storage

625
00:30:18,030 --> 00:30:22,500
and near real-time processing
provide current information

626
00:30:22,500 --> 00:30:24,693
to the AI application's knowledge base.

627
00:30:25,530 --> 00:30:29,130
Also, depending on the use
case and business needs,

628
00:30:29,130 --> 00:30:33,660
AWS has several streaming and
CDC options to choose from,

629
00:30:33,660 --> 00:30:37,770
including Amazon Kinesis
Data Streams, AWS Glue,

630
00:30:37,770 --> 00:30:41,163
Amazon Managed Apache Flink
for real-time aggregations.

631
00:30:43,680 --> 00:30:45,180
Moving on to the next aspect

632
00:30:45,180 --> 00:30:49,950
of leveraging your AI-ready
data foundation on AWS,

633
00:30:49,950 --> 00:30:51,990
let's talk about
event-driven architectures

634
00:30:51,990 --> 00:30:54,900
in scenarios where you
have multiple agents

635
00:30:54,900 --> 00:30:57,693
that are providing an agentic AI function.

636
00:30:58,620 --> 00:31:00,330
Multiple agents is where an agent needs

637
00:31:00,330 --> 00:31:03,030
to gather information
from multiple sources,

638
00:31:03,030 --> 00:31:06,090
including other agents, tools and systems.

639
00:31:06,090 --> 00:31:09,000
And this is a very likely
scenario in production

640
00:31:09,000 --> 00:31:10,800
when you build an agentic AI application

641
00:31:10,800 --> 00:31:13,443
where you end up with multiple AI agents.

642
00:31:14,340 --> 00:31:17,190
Multiple agents, a good
way to think about those,

643
00:31:17,190 --> 00:31:21,120
is like a loosely coupled
asynchronous architecture

644
00:31:21,120 --> 00:31:24,990
that is very similar to a
microservices-based application

645
00:31:24,990 --> 00:31:28,680
where multiple agents work independently,

646
00:31:28,680 --> 00:31:32,280
but at the same time, in
collaboration with each other,

647
00:31:32,280 --> 00:31:34,320
moving data and events,

648
00:31:34,320 --> 00:31:38,013
making sure unavailability of
one does not affect the other.

649
00:31:39,180 --> 00:31:40,240
This is scenario two

650
00:31:41,700 --> 00:31:45,150
where let's say I want to book a flight,

651
00:31:45,150 --> 00:31:47,910
which actually violates
the policy exceptions.

652
00:31:47,910 --> 00:31:49,710
So, this will demonstrate

653
00:31:49,710 --> 00:31:53,370
how multiple agents
comes into play, right?

654
00:31:53,370 --> 00:31:55,440
So, as I said, scenario two of the demo

655
00:31:55,440 --> 00:31:58,980
where I am having to book a ticket

656
00:31:58,980 --> 00:32:01,680
that is outside of my company policy,

657
00:32:01,680 --> 00:32:04,683
but works better for my
availability schedule.

658
00:32:05,940 --> 00:32:09,120
As expected, it comes back to tell me

659
00:32:09,120 --> 00:32:13,290
that this particular flight
will need my manager approval.

660
00:32:13,290 --> 00:32:16,290
And as part of backend
operation, what I'm doing,

661
00:32:16,290 --> 00:32:19,330
I request and get the
approval from my manager

662
00:32:20,730 --> 00:32:22,863
and then we'll wait for the agent,

663
00:32:22,863 --> 00:32:24,870
it's doing a little bit of
a hard-thinking over here,

664
00:32:24,870 --> 00:32:28,800
it's a policy exception,
so, there it goes.

665
00:32:28,800 --> 00:32:30,360
So, now, the agent is telling me

666
00:32:30,360 --> 00:32:31,950
that I need an approval from my manager.

667
00:32:31,950 --> 00:32:33,690
Because this choice

668
00:32:33,690 --> 00:32:37,530
of flight is violating my company policy.

669
00:32:37,530 --> 00:32:40,770
Like I mentioned, I get the
approval from my manager

670
00:32:40,770 --> 00:32:42,390
through a backend operation.

671
00:32:42,390 --> 00:32:45,897
And then I tell the agent,
"Yes, I have the approval."

672
00:32:46,890 --> 00:32:51,450
The agent takes that information
and does some validation

673
00:32:51,450 --> 00:32:55,350
and then goes ahead and
books the reservation for me.

674
00:32:55,350 --> 00:32:57,750
As you can see, it's performing
the similar operations

675
00:32:57,750 --> 00:32:58,770
that we saw earlier,

676
00:32:58,770 --> 00:33:01,410
which used components
of the knowledge base,

677
00:33:01,410 --> 00:33:04,660
the agentic memory, my user preferences

678
00:33:05,820 --> 00:33:10,050
and also does a few backend operations.

679
00:33:10,050 --> 00:33:13,260
Let's take a look at those
backend operations, right?

680
00:33:13,260 --> 00:33:16,770
These backend operations are
basically additional agents

681
00:33:16,770 --> 00:33:19,380
that are collaborating
with our main agent.

682
00:33:19,380 --> 00:33:22,920
There's a confirmation
email notification agent,

683
00:33:22,920 --> 00:33:25,500
an agent that triggers a rule

684
00:33:25,500 --> 00:33:29,430
if a company policy on
expenses is satisfied or not

685
00:33:29,430 --> 00:33:31,410
and an agent that logs all transactions

686
00:33:31,410 --> 00:33:33,990
to a backend database.

687
00:33:33,990 --> 00:33:37,380
Now, our main agent could actually be made

688
00:33:37,380 --> 00:33:40,740
to handle all these tasks in one.

689
00:33:40,740 --> 00:33:44,130
But if you recall the ReAct loop, right,

690
00:33:44,130 --> 00:33:47,160
agents can also get easily confused

691
00:33:47,160 --> 00:33:50,850
and you're also working with
the limited context window

692
00:33:50,850 --> 00:33:52,203
and the agentic memory.

693
00:33:53,130 --> 00:33:56,490
So, on the other hand,
having multiple agents,

694
00:33:56,490 --> 00:33:57,900
how does that help?

695
00:33:57,900 --> 00:34:01,980
Multiple agents performing
a unit or a single task,

696
00:34:01,980 --> 00:34:03,630
are easier to build

697
00:34:03,630 --> 00:34:05,400
and you can build in resiliency

698
00:34:05,400 --> 00:34:08,160
with an event-driven architecture, right?

699
00:34:08,160 --> 00:34:11,913
This is something that looks
like this over here for us.

700
00:34:13,320 --> 00:34:15,690
Using a service like
Amazon Managed Streaming

701
00:34:15,690 --> 00:34:19,590
for Apache Kafka or Amazon MSK,

702
00:34:19,590 --> 00:34:22,380
which is a fully managed
Apache Kafka setup,

703
00:34:22,380 --> 00:34:25,470
you can loosely couple all the agents,

704
00:34:25,470 --> 00:34:29,370
while ensuring real-time
exchange for low latency

705
00:34:29,370 --> 00:34:30,810
with built-in scalability

706
00:34:30,810 --> 00:34:34,560
where there is a need to add
more agents, for example,

707
00:34:34,560 --> 00:34:39,540
and manage persistent events
with Multi-AZ deployments

708
00:34:39,540 --> 00:34:41,043
and automated recovery.

709
00:34:44,490 --> 00:34:46,020
An event-driven multiple architecture,

710
00:34:46,020 --> 00:34:49,590
not only decouples and
scales the agent task,

711
00:34:49,590 --> 00:34:53,250
it also provides a distributed
processing platform

712
00:34:53,250 --> 00:34:55,350
for the agentic AI application

713
00:34:55,350 --> 00:34:59,490
with durable messages
and delivery guarantees

714
00:34:59,490 --> 00:35:02,583
as provided with the MSK service.

715
00:35:03,990 --> 00:35:06,690
A little bit about the Amazon MSK service.

716
00:35:06,690 --> 00:35:09,840
It can process high-throughput events,

717
00:35:09,840 --> 00:35:13,500
enabling agents to respond
to various patterns

718
00:35:13,500 --> 00:35:16,050
and real-time scenario changes.

719
00:35:16,050 --> 00:35:19,830
Asynchronous communication
in Amazon MSK facilitated

720
00:35:19,830 --> 00:35:22,230
by event-driven architectures
provides benefits,

721
00:35:22,230 --> 00:35:25,320
such as decoupling, independent scaling

722
00:35:25,320 --> 00:35:28,413
and improving fall tolerance
through replication.

723
00:35:29,280 --> 00:35:32,550
And now, I would like to
invite Shikha back to us

724
00:35:32,550 --> 00:35:35,760
to tell us how data readiness can be made,

725
00:35:35,760 --> 00:35:39,099
can be worked upon through
the AI-ready data foundation.

726
00:35:39,099 --> 00:35:39,932
Shikha?

727
00:35:46,800 --> 00:35:48,540
- Thank you, Taz.

728
00:35:48,540 --> 00:35:49,373
How are we doing?

729
00:35:49,373 --> 00:35:52,470
That was a lot to consume, right?

730
00:35:52,470 --> 00:35:53,303
How are we doing?

731
00:35:53,303 --> 00:35:54,690
Good, good?

732
00:35:54,690 --> 00:35:56,175
All right, I think Taz did

733
00:35:56,175 --> 00:35:59,220
what he told me that he was gonna do.

734
00:35:59,220 --> 00:36:01,650
He was like, "Okay, I will use
our data to create agents."

735
00:36:01,650 --> 00:36:04,050
And he created multiple agents.

736
00:36:04,050 --> 00:36:05,640
As we were getting ready for the session,

737
00:36:05,640 --> 00:36:08,910
we purposefully wanted
to do more of a show,

738
00:36:08,910 --> 00:36:09,743
rather than tell.

739
00:36:09,743 --> 00:36:11,190
So, I hope it's working for you.

740
00:36:11,190 --> 00:36:13,650
Quick raise of hands if
it's working for you.

741
00:36:13,650 --> 00:36:14,483
Yeah?

742
00:36:14,483 --> 00:36:15,780
Okay, cool, cool, cool.

743
00:36:15,780 --> 00:36:20,403
So, who believes in this?

744
00:36:22,470 --> 00:36:24,900
Does this make sense to you guys, yeah?

745
00:36:24,900 --> 00:36:29,400
So, just like how humans
need maps, AI needs metadata.

746
00:36:29,400 --> 00:36:31,740
So, how does this fit
into our overall picture

747
00:36:31,740 --> 00:36:33,570
of what we have seen so far?

748
00:36:33,570 --> 00:36:38,370
So, we saw Taz use a lot of
data and built-in context.

749
00:36:38,370 --> 00:36:40,290
We talked about context management

750
00:36:40,290 --> 00:36:43,500
and how context comes in
through knowledge bases

751
00:36:43,500 --> 00:36:45,270
and through session logs

752
00:36:45,270 --> 00:36:48,363
and through data that is coming
through tools or databases.

753
00:36:50,280 --> 00:36:51,840
Is metadata new, guys,

754
00:36:51,840 --> 00:36:54,720
like how many of you
have dealt with metadata

755
00:36:54,720 --> 00:36:57,450
for like decades now, right?

756
00:36:57,450 --> 00:36:59,550
It's not new, but it is cool now.

757
00:36:59,550 --> 00:37:00,570
So, I think,

758
00:37:00,570 --> 00:37:03,750
metadata is definitely
having a moment right now.

759
00:37:03,750 --> 00:37:05,190
And I think, it's great.

760
00:37:05,190 --> 00:37:06,413
Because all of the context

761
00:37:06,413 --> 00:37:09,510
that we are talking about
is essentially metadata

762
00:37:09,510 --> 00:37:12,000
and data that is sitting
in unstructured files,

763
00:37:12,000 --> 00:37:14,550
like Taz pointed out the policy file.

764
00:37:14,550 --> 00:37:16,500
We are gonna go through more

765
00:37:16,500 --> 00:37:19,950
of the traditional data side of readiness,

766
00:37:19,950 --> 00:37:24,000
which is data sitting in your
data warehouses, data lakes.

767
00:37:24,000 --> 00:37:25,350
And how do you get that ready

768
00:37:25,350 --> 00:37:27,030
for consumption by our AI agents?

769
00:37:27,030 --> 00:37:29,790
Because that is where all of
our data is sitting right now,

770
00:37:29,790 --> 00:37:30,623
right?

771
00:37:30,623 --> 00:37:32,670
So, when Taz and I started
thinking about our session,

772
00:37:32,670 --> 00:37:33,503
we were like,

773
00:37:33,503 --> 00:37:35,700
"Well, all of our data is
like literally all over.

774
00:37:35,700 --> 00:37:38,010
And it's not even in AWS,
some of it is outside.

775
00:37:38,010 --> 00:37:40,020
How do we bring it all together?"

776
00:37:40,020 --> 00:37:42,660
So, if everybody accepts this,

777
00:37:42,660 --> 00:37:43,800
then, I think we are gonna go

778
00:37:43,800 --> 00:37:46,650
into the next side of the story.

779
00:37:46,650 --> 00:37:49,530
So, we have our data sources, right?

780
00:37:49,530 --> 00:37:52,620
Traditionally, we've had
data warehouses, data lakes,

781
00:37:52,620 --> 00:37:54,510
then came the lakehouses,

782
00:37:54,510 --> 00:37:56,940
then all these different data sources

783
00:37:56,940 --> 00:37:58,920
where data is just sprawling.

784
00:37:58,920 --> 00:38:02,250
All of that comes together
through the architecture

785
00:38:02,250 --> 00:38:03,900
that Taz is pointing out

786
00:38:03,900 --> 00:38:07,170
with generating more and
more metadata, right?

787
00:38:07,170 --> 00:38:08,580
So, we generate metadata

788
00:38:08,580 --> 00:38:12,120
as we enrich each of these
datasets with more information.

789
00:38:12,120 --> 00:38:15,540
We generate metadata on data
quality and data lineage

790
00:38:15,540 --> 00:38:18,300
as we get this all data moved into a form

791
00:38:18,300 --> 00:38:21,330
where our agents can use
it and humans can use it.

792
00:38:21,330 --> 00:38:23,190
So, metadata is definitely having a moment

793
00:38:23,190 --> 00:38:26,430
and I would highly encourage
everybody to think of this

794
00:38:26,430 --> 00:38:29,670
as a key foundation that
you have to invest in

795
00:38:29,670 --> 00:38:31,773
for your AI initiatives.

796
00:38:34,200 --> 00:38:36,720
So, what does really
metadata help us do, right?

797
00:38:36,720 --> 00:38:39,450
So, I don't think this
story will be very new,

798
00:38:39,450 --> 00:38:40,920
but again, I'm gonna emphasize

799
00:38:40,920 --> 00:38:45,920
that it's what we used to
do for humans is a must now

800
00:38:46,290 --> 00:38:47,580
for AI agents.

801
00:38:47,580 --> 00:38:50,610
Because AI agents are not
gonna call their friend, right?

802
00:38:50,610 --> 00:38:54,690
Like we want autonomous
agents working on our data

803
00:38:54,690 --> 00:38:56,670
and on the context that
we are providing them.

804
00:38:56,670 --> 00:38:58,680
So, it becomes more and more useful for us

805
00:38:58,680 --> 00:39:00,870
to bake metadata into our systems.

806
00:39:00,870 --> 00:39:03,990
So, let me just talk about
this cycle for a second.

807
00:39:03,990 --> 00:39:05,730
So, in most companies

808
00:39:05,730 --> 00:39:07,890
and I'm assuming that this
will sound familiar to you,

809
00:39:07,890 --> 00:39:09,270
there's data producers,

810
00:39:09,270 --> 00:39:12,030
people who create the data or own the data

811
00:39:12,030 --> 00:39:13,230
and there's data consumers,

812
00:39:13,230 --> 00:39:17,730
which now is getting filled
up a lot with not just humans,

813
00:39:17,730 --> 00:39:19,650
but also AI, right?

814
00:39:19,650 --> 00:39:21,616
Does this sound familiar to you, guys?

815
00:39:21,616 --> 00:39:22,449
- [Audience Member 5] Yes.
- Yeah?

816
00:39:22,449 --> 00:39:25,350
It's very much the story
that we've been living with.

817
00:39:25,350 --> 00:39:28,260
But then it's now vital
for us to have the context,

818
00:39:28,260 --> 00:39:30,870
because AI cannot work
without the context, right?

819
00:39:30,870 --> 00:39:32,010
So, we gotta make sure

820
00:39:32,010 --> 00:39:35,130
that we are building the
right technology in the middle

821
00:39:35,130 --> 00:39:37,410
to bring these things together, right,

822
00:39:37,410 --> 00:39:40,860
to add the right context
in terms of metadata,

823
00:39:40,860 --> 00:39:43,110
to have the right frameworks
in terms of governance,

824
00:39:43,110 --> 00:39:46,590
so that humans and AI
can use the same systems

825
00:39:46,590 --> 00:39:48,060
that we have been using

826
00:39:48,060 --> 00:39:50,370
by adding the right components to it.

827
00:39:50,370 --> 00:39:52,020
Does this make sense?

828
00:39:52,020 --> 00:39:53,970
Quick show of hands, okay, cool.

829
00:39:53,970 --> 00:39:58,080
All right, so, let's go ahead.

830
00:39:58,080 --> 00:40:00,483
How many of you have
heard of Amazon SageMaker?

831
00:40:01,800 --> 00:40:02,633
Yeah?

832
00:40:02,633 --> 00:40:06,450
So, we invested a ton in this platform

833
00:40:06,450 --> 00:40:07,710
in the last few years.

834
00:40:07,710 --> 00:40:10,950
And for those of you
familiar with SageMaker AI,

835
00:40:10,950 --> 00:40:13,230
which used to be the traditional
machine-learning side

836
00:40:13,230 --> 00:40:14,160
of our offering,

837
00:40:14,160 --> 00:40:18,210
we really expanded that
brand and portfolio

838
00:40:18,210 --> 00:40:20,727
to include all of our data analytics

839
00:40:20,727 --> 00:40:22,260
and AI capabilities into it.

840
00:40:22,260 --> 00:40:24,000
So, that is what Amazon SageMaker is.

841
00:40:24,000 --> 00:40:27,210
It's really the center for
all your data analytics AI.

842
00:40:27,210 --> 00:40:29,820
Because we saw this thing coming.

843
00:40:29,820 --> 00:40:31,380
We're like, "Okay, we are gonna need

844
00:40:31,380 --> 00:40:32,880
to bring it all together, right?

845
00:40:32,880 --> 00:40:36,090
Because we can't have these
agents running a havoc

846
00:40:36,090 --> 00:40:37,590
through your ecosystem."

847
00:40:37,590 --> 00:40:39,840
So, what Amazon SageMaker is built on,

848
00:40:39,840 --> 00:40:43,380
is a foundation of the lakehouse,

849
00:40:43,380 --> 00:40:46,080
which is if you're Iceberg-compliant,

850
00:40:46,080 --> 00:40:48,450
you're gonna, this is gonna
be music to your ears,

851
00:40:48,450 --> 00:40:50,790
because anything that is
Iceberg-compliant can come

852
00:40:50,790 --> 00:40:52,050
into this ecosystem

853
00:40:52,050 --> 00:40:55,770
and can bubbled up through
SageMaker's capabilities

854
00:40:55,770 --> 00:40:59,130
of data and AI governance as
well as the Unified Studio,

855
00:40:59,130 --> 00:41:01,110
which brings all of these
capabilities together.

856
00:41:01,110 --> 00:41:02,790
So, next set, couple of demos

857
00:41:02,790 --> 00:41:04,590
that Taz and I gonna show you,

858
00:41:04,590 --> 00:41:08,160
are really using these capabilities

859
00:41:08,160 --> 00:41:10,920
to build the context and the metadata

860
00:41:10,920 --> 00:41:13,920
for your users and your AI first.

861
00:41:13,920 --> 00:41:15,630
And then use that context

862
00:41:15,630 --> 00:41:17,460
to create an application quickly, right?

863
00:41:17,460 --> 00:41:20,040
So, we are going a little
bit behind the scenes here

864
00:41:20,040 --> 00:41:23,910
where Taz really showed you
almost the front-end application

865
00:41:23,910 --> 00:41:25,230
of the agent

866
00:41:25,230 --> 00:41:28,800
and the customer using it
built-in to book a flight.

867
00:41:28,800 --> 00:41:30,510
Now, we are gonna go a
little bit in the backend

868
00:41:30,510 --> 00:41:32,403
and show you just how this gets done.

869
00:41:35,460 --> 00:41:39,180
All right, so, at the center
of that SageMaker picture,

870
00:41:39,180 --> 00:41:42,480
you saw a data and AI governance, right?

871
00:41:42,480 --> 00:41:43,980
So, what does that really mean?

872
00:41:43,980 --> 00:41:48,980
This is the center for where
all of your data, your models,

873
00:41:49,080 --> 00:41:51,870
your agents come together

874
00:41:51,870 --> 00:41:54,240
for availability across your
entire organization, right?

875
00:41:54,240 --> 00:41:59,240
So, I'm assuming, most people
here are data engineers,

876
00:41:59,520 --> 00:42:02,973
software engineers, data platform owners,

877
00:42:03,960 --> 00:42:07,232
that sort of demographic.

878
00:42:07,232 --> 00:42:08,340
Is that right?

879
00:42:08,340 --> 00:42:09,900
Can I, yeah?

880
00:42:09,900 --> 00:42:11,010
Okay, cool.

881
00:42:11,010 --> 00:42:12,760
So, you guys deal with this, right?

882
00:42:13,680 --> 00:42:16,320
So, what SageMaker
really allows you to do,

883
00:42:16,320 --> 00:42:18,300
is bring all of that together.

884
00:42:18,300 --> 00:42:19,800
Then you build trust in it.

885
00:42:19,800 --> 00:42:22,170
So, we talked about data
quality and data lineage.

886
00:42:22,170 --> 00:42:24,660
Again, words that are not really new,

887
00:42:24,660 --> 00:42:27,030
but are of immense
importance in this new world.

888
00:42:27,030 --> 00:42:28,500
Because we gotta get this right,

889
00:42:28,500 --> 00:42:31,101
so that the context is all built-in

890
00:42:31,101 --> 00:42:33,843
into our data and metadata.

891
00:42:34,680 --> 00:42:37,170
And then, applying the right guardrails

892
00:42:37,170 --> 00:42:39,930
around who can use this data?

893
00:42:39,930 --> 00:42:44,930
You don't want your notify
agent using your policy data,

894
00:42:45,390 --> 00:42:48,510
you don't want your, like
it's the whole picture,

895
00:42:48,510 --> 00:42:49,380
like in humans,

896
00:42:49,380 --> 00:42:51,930
you can call and make the
right controls happen.

897
00:42:51,930 --> 00:42:52,763
But in agents,

898
00:42:52,763 --> 00:42:55,440
it's all the more important
to set the right guardrails

899
00:42:55,440 --> 00:42:58,353
and the right workflows
in the system itself.

900
00:42:59,280 --> 00:43:02,460
All of that, of course, is
very transparent and auditable.

901
00:43:02,460 --> 00:43:05,160
So, all of the metadata that
gets generated through this,

902
00:43:05,160 --> 00:43:07,290
for example, at the asset level goes back

903
00:43:07,290 --> 00:43:09,000
into S3 Tables, right?

904
00:43:09,000 --> 00:43:10,110
So, what does that mean?

905
00:43:10,110 --> 00:43:12,780
That your agents can go read
that data directly, right?

906
00:43:12,780 --> 00:43:14,580
So, if I put all the metadata

907
00:43:14,580 --> 00:43:17,910
that I'm creating through
SageMaker back into S3 Tables,

908
00:43:17,910 --> 00:43:19,200
you can read that data,

909
00:43:19,200 --> 00:43:21,630
your agents can read that data
and infer information from it

910
00:43:21,630 --> 00:43:24,093
without having to go
through the whole stack.

911
00:43:26,070 --> 00:43:26,903
All right,

912
00:43:26,903 --> 00:43:29,580
so two key important things,
again, takeaways, right?

913
00:43:29,580 --> 00:43:33,300
So, for data and AI
initiatives in general, right,

914
00:43:33,300 --> 00:43:35,310
like you want to apply the same framework

915
00:43:35,310 --> 00:43:38,250
across your data, models
and all kinds of assets.

916
00:43:38,250 --> 00:43:40,560
And you wanna invest in metadata,

917
00:43:40,560 --> 00:43:43,443
as well as a comprehensive
data governance around it.

918
00:43:45,240 --> 00:43:47,700
All right, let me show
you some stuff in action.

919
00:43:47,700 --> 00:43:49,650
So, we talked a lot about metadata.

920
00:43:49,650 --> 00:43:53,820
Metadata is not easily
available for the things

921
00:43:53,820 --> 00:43:56,430
that we've had for
many, many years, right?

922
00:43:56,430 --> 00:43:58,500
Not every system that we own,

923
00:43:58,500 --> 00:44:02,490
has useful business context
attached to it, it doesn't.

924
00:44:02,490 --> 00:44:04,800
The tables are hard to tell.

925
00:44:04,800 --> 00:44:07,830
For those of you who are in the SAP land,

926
00:44:07,830 --> 00:44:10,530
like the table name doesn't tell you much,

927
00:44:10,530 --> 00:44:11,760
it's things like that, right?

928
00:44:11,760 --> 00:44:15,120
So, what we need to do is
really add the right metadata

929
00:44:15,120 --> 00:44:18,720
to all of our tables as well as columns.

930
00:44:18,720 --> 00:44:21,450
But we can't expect humans
to be creating that, right?

931
00:44:21,450 --> 00:44:24,240
So, what I wanna show you
here is for one of the tables

932
00:44:24,240 --> 00:44:25,840
that Taz showed you in the demo,

933
00:44:27,510 --> 00:44:30,660
we go and create metadata
automatically through AI.

934
00:44:30,660 --> 00:44:32,490
So, again, with SageMaker,

935
00:44:32,490 --> 00:44:34,890
what we have tried to do is
we are building the system

936
00:44:34,890 --> 00:44:36,900
for you to use for your AI initiatives.

937
00:44:36,900 --> 00:44:39,570
But we have also built
enough AI into the system

938
00:44:39,570 --> 00:44:43,020
to make the system more productive, right?

939
00:44:43,020 --> 00:44:44,190
So, in this particular case,

940
00:44:44,190 --> 00:44:46,710
I'm picking the customer_reviews table.

941
00:44:46,710 --> 00:44:47,543
You can see

942
00:44:47,543 --> 00:44:52,050
that there is a quick generate
description button up here.

943
00:44:52,050 --> 00:44:54,360
It's showing me the
typical technical metadata

944
00:44:54,360 --> 00:44:56,040
that is readily available usually

945
00:44:56,040 --> 00:44:57,540
and I can add to it.

946
00:44:57,540 --> 00:45:01,050
I can see the schema of
the table that I have,

947
00:45:01,050 --> 00:45:03,450
it shows some data quality,
we'll get to that in a minute.

948
00:45:03,450 --> 00:45:07,260
But I want some more useful
business context added to it,

949
00:45:07,260 --> 00:45:11,307
so that I can feed it to my
humans as well as AI, right?

950
00:45:11,307 --> 00:45:14,054
So, it's gonna think a
minute, it's doing it live.

951
00:45:14,054 --> 00:45:16,950
We didn't wanna shorten
this stuff for you,

952
00:45:16,950 --> 00:45:19,290
so that you know that
it does take a minute.

953
00:45:19,290 --> 00:45:21,300
But as you can see while it's loading,

954
00:45:21,300 --> 00:45:22,560
like on the rest of the screen,

955
00:45:22,560 --> 00:45:25,230
you have glossary terms, metadata forms,

956
00:45:25,230 --> 00:45:27,390
which you can add additional ones.

957
00:45:27,390 --> 00:45:30,330
And there you go, the
metadata is generated.

958
00:45:30,330 --> 00:45:33,630
So, here, I have an option to
edit the metadata if I want.

959
00:45:33,630 --> 00:45:35,460
But I've seen pretty good success,

960
00:45:35,460 --> 00:45:36,990
so I'm just gonna accept it.

961
00:45:36,990 --> 00:45:38,430
And you can see that in the summary,

962
00:45:38,430 --> 00:45:40,350
it's giving some very helpful pointers.

963
00:45:40,350 --> 00:45:42,690
So, what does this data have?

964
00:45:42,690 --> 00:45:45,300
What can it be used for, right?

965
00:45:45,300 --> 00:45:48,030
Things that we would be
like wanting somebody

966
00:45:48,030 --> 00:45:49,200
to put in the data,

967
00:45:49,200 --> 00:45:51,990
so that we can query this information,

968
00:45:51,990 --> 00:45:54,450
we can use this information
when you're trying to use it.

969
00:45:54,450 --> 00:45:57,750
And it just generates all of
that in automatically, right?

970
00:45:57,750 --> 00:45:59,400
That's the power of LLMs

971
00:45:59,400 --> 00:46:01,830
that we are getting the
benefit of right now.

972
00:46:01,830 --> 00:46:03,900
It also went to the column level

973
00:46:03,900 --> 00:46:05,640
and generated descriptions there too.

974
00:46:05,640 --> 00:46:08,880
So, earlier, if you remember
when we came to the schema,

975
00:46:08,880 --> 00:46:10,260
it was largely empty.

976
00:46:10,260 --> 00:46:11,100
But now, you can see

977
00:46:11,100 --> 00:46:14,302
that there's even column-level
descriptions added.

978
00:46:14,302 --> 00:46:17,610
And I am accepting all of the
descriptions here as well,

979
00:46:17,610 --> 00:46:20,310
because I think they're
pretty recent for my use here.

980
00:46:21,390 --> 00:46:23,880
The other things you see on this screen,

981
00:46:23,880 --> 00:46:26,940
are there is a data quality scores

982
00:46:26,940 --> 00:46:29,100
that are available as well, right?

983
00:46:29,100 --> 00:46:31,770
So, this got pulled in
by looking at the rules

984
00:46:31,770 --> 00:46:35,160
that I had set for this
table back in Glue,

985
00:46:35,160 --> 00:46:36,540
which is how I brought it, right?

986
00:46:36,540 --> 00:46:38,850
So, those table, that is pulled in.

987
00:46:38,850 --> 00:46:40,410
I have a clear understanding here

988
00:46:40,410 --> 00:46:43,950
that "Okay, this data is only 60% good,

989
00:46:43,950 --> 00:46:45,390
you'd really wanna use it?"

990
00:46:45,390 --> 00:46:48,390
But you wanna at least show
that to your users upfront,

991
00:46:48,390 --> 00:46:49,950
so that they know what
they're getting into, right,

992
00:46:49,950 --> 00:46:51,780
and they're not being misled.

993
00:46:51,780 --> 00:46:53,373
Now, the lineage story here,

994
00:46:53,373 --> 00:46:54,206
(Shikha laughs)

995
00:46:54,206 --> 00:46:56,250
Taz and I were laughing that
it just shows one table,

996
00:46:56,250 --> 00:46:58,560
because we didn't combine it
with a bunch of other things.

997
00:46:58,560 --> 00:47:00,360
But if you bring a bunch
of things together,

998
00:47:00,360 --> 00:47:01,920
it shows you a beautiful graph

999
00:47:01,920 --> 00:47:04,140
of how this data was constructed.

1000
00:47:04,140 --> 00:47:06,900
Anyways, now that I have
put all that information,

1001
00:47:06,900 --> 00:47:10,440
I publish that asset back
into SageMaker Catalog.

1002
00:47:10,440 --> 00:47:12,240
So, now, it becomes readily available

1003
00:47:12,240 --> 00:47:15,240
for another user to come in,
who's coming in right now

1004
00:47:15,240 --> 00:47:19,050
and they search for customer
reviews and that table appears.

1005
00:47:19,050 --> 00:47:23,010
So, now, another user from
another side of the world,

1006
00:47:23,010 --> 00:47:25,920
or another agent can come
in and search your catalog

1007
00:47:25,920 --> 00:47:28,620
for using this information
by some keywords.

1008
00:47:28,620 --> 00:47:30,690
And if I like it, you can subscribe to it,

1009
00:47:30,690 --> 00:47:32,910
which means that you're
getting the actual grants

1010
00:47:32,910 --> 00:47:35,640
to use this data for your systems,

1011
00:47:35,640 --> 00:47:38,040
which is again, like
something that is audited,

1012
00:47:38,040 --> 00:47:41,400
you can query it, you know
who's using what data.

1013
00:47:41,400 --> 00:47:42,900
So, again, pretty useful.

1014
00:47:42,900 --> 00:47:44,790
Really quick demo on the behind the scenes

1015
00:47:44,790 --> 00:47:47,193
of how this data got ready for Taz to use.

1016
00:47:48,090 --> 00:47:49,200
So, in this area,

1017
00:47:49,200 --> 00:47:52,500
we are really introducing
a lot of capabilities.

1018
00:47:52,500 --> 00:47:55,740
Some of you may be familiar but maybe new.

1019
00:47:55,740 --> 00:47:57,930
So, I'm just gonna
quickly go through that.

1020
00:47:57,930 --> 00:48:00,750
But metadata, in general, is
getting a lot of investment

1021
00:48:00,750 --> 00:48:02,010
across AWS.

1022
00:48:02,010 --> 00:48:04,050
You'll see that we are
adding metadata capabilities

1023
00:48:04,050 --> 00:48:06,180
at the storage layer at S3.

1024
00:48:06,180 --> 00:48:07,770
I'm gonna call out some
of the key capabilities

1025
00:48:07,770 --> 00:48:09,900
that we have added to SageMaker Catalog,

1026
00:48:09,900 --> 00:48:11,790
which is really at column level.

1027
00:48:11,790 --> 00:48:14,220
You can have enhanced descriptions,

1028
00:48:14,220 --> 00:48:15,510
you can have metadata forms,

1029
00:48:15,510 --> 00:48:17,460
gives you so much more control

1030
00:48:17,460 --> 00:48:20,760
over what kind of context do
you want added to your data,

1031
00:48:20,760 --> 00:48:21,780
so that your agents

1032
00:48:21,780 --> 00:48:25,560
and your humans have all the
information available, right?

1033
00:48:25,560 --> 00:48:27,180
You can also enforce that, right?

1034
00:48:27,180 --> 00:48:29,550
So, you can now say
that "For this use case,

1035
00:48:29,550 --> 00:48:31,770
this particular table or
column is not usable."

1036
00:48:31,770 --> 00:48:33,330
That also gives you that feature

1037
00:48:33,330 --> 00:48:35,220
with the enforcement of metadata forms,

1038
00:48:35,220 --> 00:48:36,053
as well as glossaries.

1039
00:48:36,053 --> 00:48:39,090
Because you can spell that
out a lot more clearly

1040
00:48:39,090 --> 00:48:41,040
when the context is added to your data.

1041
00:48:43,110 --> 00:48:45,090
Also, again, it's really hard

1042
00:48:45,090 --> 00:48:46,650
to create these things
from scratch, right?

1043
00:48:46,650 --> 00:48:49,500
So, we are continuing to
invest in our automation

1044
00:48:49,500 --> 00:48:52,440
where we gave the automated
business subscriptions first

1045
00:48:52,440 --> 00:48:54,930
and now, we are also
giving the capabilities

1046
00:48:54,930 --> 00:48:57,210
to add business glossaries automatically.

1047
00:48:57,210 --> 00:48:59,640
They'll give you suggestions,
you can accept them,

1048
00:48:59,640 --> 00:49:02,460
or you can associate them
with a pre-existing glossary.

1049
00:49:02,460 --> 00:49:04,680
It just become super easy.

1050
00:49:04,680 --> 00:49:06,390
And then, like I mentioned earlier,

1051
00:49:06,390 --> 00:49:09,420
we are putting that all
back into S3, right?

1052
00:49:09,420 --> 00:49:12,090
So, then, it's available
at even your storage layer,

1053
00:49:12,090 --> 00:49:13,620
the metadata itself is available

1054
00:49:13,620 --> 00:49:15,843
for you to query and use with your agents.

1055
00:49:17,520 --> 00:49:20,343
How many folks here have
data outside of AWS?

1056
00:49:22,050 --> 00:49:23,160
Most of you, right?

1057
00:49:23,160 --> 00:49:24,489
Yeah, even I do.

1058
00:49:24,489 --> 00:49:25,410
(Shikha giggles)

1059
00:49:25,410 --> 00:49:28,440
So, very excited for this
particular capability,

1060
00:49:28,440 --> 00:49:30,270
which is if you are, again,

1061
00:49:30,270 --> 00:49:33,330
if you subscribe to
Iceberg, this is great.

1062
00:49:33,330 --> 00:49:35,133
We are investing a ton in Iceberg.

1063
00:49:36,150 --> 00:49:37,290
This is catalog federation

1064
00:49:37,290 --> 00:49:39,090
to external Apache Iceberg Catalogs.

1065
00:49:39,090 --> 00:49:42,210
So, for example, if you use
Databricks or Snowflake,

1066
00:49:42,210 --> 00:49:44,520
you can bring in, read those tables

1067
00:49:44,520 --> 00:49:47,370
from these third-party solutions

1068
00:49:47,370 --> 00:49:49,980
and you can bring them
into Glue Data Catalog,

1069
00:49:49,980 --> 00:49:53,460
so that it becomes available
in the same SageMaker ecosystem

1070
00:49:53,460 --> 00:49:54,960
that I showed you earlier, right?

1071
00:49:54,960 --> 00:49:58,980
So, you can, for example, bring
in a table from Databricks,

1072
00:49:58,980 --> 00:50:00,780
you can catalog it,

1073
00:50:00,780 --> 00:50:03,660
you can add the automated
business descriptions to it,

1074
00:50:03,660 --> 00:50:06,300
you can do all of that
from within SageMaker

1075
00:50:06,300 --> 00:50:08,250
if you're leveraging this capability.

1076
00:50:08,250 --> 00:50:09,510
Super excited about this one!

1077
00:50:09,510 --> 00:50:12,183
This has been a huge ask
from most of our customers!

1078
00:50:14,220 --> 00:50:15,360
All right.

1079
00:50:15,360 --> 00:50:17,280
Now that we have all
of that data together,

1080
00:50:17,280 --> 00:50:22,280
so, now, our data engineer,
data scientist, Taz wants

1081
00:50:22,350 --> 00:50:24,690
to build something on
all of that data, right?

1082
00:50:24,690 --> 00:50:26,010
So, how do you do that quickly

1083
00:50:26,010 --> 00:50:29,070
without having to now build
like the traditional pipelines,

1084
00:50:29,070 --> 00:50:33,540
or go to five different
tools to do your things?

1085
00:50:33,540 --> 00:50:35,970
So, if you're familiar with
SageMaker Unified Studio,

1086
00:50:35,970 --> 00:50:38,430
we are adding a ton here.

1087
00:50:38,430 --> 00:50:40,800
We have actually in the
last couple of weeks.

1088
00:50:40,800 --> 00:50:43,590
So, one is one-click onboarding
of existing datasets.

1089
00:50:43,590 --> 00:50:44,423
What this means,

1090
00:50:44,423 --> 00:50:48,510
is if your data is part
of Glue Data Catalog,

1091
00:50:48,510 --> 00:50:50,850
or you just did the external federation

1092
00:50:50,850 --> 00:50:52,530
from Databricks or Snowflake,

1093
00:50:52,530 --> 00:50:53,790
with a single click of that,

1094
00:50:53,790 --> 00:50:56,970
that data can be made available
in SageMaker Unified Studio.

1095
00:50:56,970 --> 00:50:58,320
What that means there,

1096
00:50:58,320 --> 00:51:00,390
is there is also a net new notebook there

1097
00:51:00,390 --> 00:51:02,010
that we have added, which is pretty cool.

1098
00:51:02,010 --> 00:51:04,230
I would encourage you guys to try out.

1099
00:51:04,230 --> 00:51:07,290
Taz is gonna come show
that to us in a minute.

1100
00:51:07,290 --> 00:51:08,760
But this is a polyglot notebook.

1101
00:51:08,760 --> 00:51:12,570
So, you can have cells with
SQL, cells with Python,

1102
00:51:12,570 --> 00:51:14,040
they can correlate

1103
00:51:14,040 --> 00:51:17,070
and it has a built-in data agent in it,

1104
00:51:17,070 --> 00:51:21,090
which, again, Taz used to help
create the little application

1105
00:51:21,090 --> 00:51:22,830
that we did, which we'll just show you,

1106
00:51:22,830 --> 00:51:24,450
which is really, really cool, guys,

1107
00:51:24,450 --> 00:51:28,980
like it can create code of
all types automatically.

1108
00:51:28,980 --> 00:51:30,330
We'll show you in a second.

1109
00:51:30,330 --> 00:51:33,000
It can also correct
coding errors within it.

1110
00:51:33,000 --> 00:51:34,860
So, if you guys are playing with,

1111
00:51:34,860 --> 00:51:36,090
how many folks here are familiar

1112
00:51:36,090 --> 00:51:38,280
with like Cursor or Windsurf or Lovable,

1113
00:51:38,280 --> 00:51:39,150
or that breed of things?

1114
00:51:39,150 --> 00:51:40,830
Yeah, so, I think you'll love it

1115
00:51:40,830 --> 00:51:42,840
if you dabbled in that space.

1116
00:51:42,840 --> 00:51:45,630
Because there is a lot of
good stuff here that you can,

1117
00:51:45,630 --> 00:51:46,863
it saves a ton of time.

1118
00:51:47,760 --> 00:51:49,650
I was playing with an
application last week

1119
00:51:49,650 --> 00:51:52,650
and literally, like, it
created the code for me.

1120
00:51:52,650 --> 00:51:54,205
And then, I'm like
"What the heck is this?"

1121
00:51:54,205 --> 00:51:55,038
(Shikha giggles)

1122
00:51:55,038 --> 00:51:55,871
And then, there was an error

1123
00:51:55,871 --> 00:51:58,140
and I did the click fix with AI,

1124
00:51:58,140 --> 00:51:59,130
it fixed the code for me.

1125
00:51:59,130 --> 00:52:00,750
We are actually gonna show you that live

1126
00:52:00,750 --> 00:52:02,220
in a second as well.

1127
00:52:02,220 --> 00:52:06,420
So, Taz, do you wanna come back
and show us how this works?

1128
00:52:06,420 --> 00:52:07,253
- Absolutely!

1129
00:52:08,100 --> 00:52:10,861
Well, if you haven't realized
so far, I'm the demo guy.

1130
00:52:10,861 --> 00:52:12,450
(Shikha laughs)

1131
00:52:12,450 --> 00:52:15,760
Okay, I know we threw,
we showed a lot of demos

1132
00:52:16,740 --> 00:52:18,360
and this is the last one we promise.

1133
00:52:18,360 --> 00:52:19,890
But this is the coolest one.

1134
00:52:19,890 --> 00:52:21,990
And you can see how excited
I'm, right, with this.

1135
00:52:21,990 --> 00:52:24,780
So, you earlier saw Shikha talk

1136
00:52:24,780 --> 00:52:26,850
about data agent, data notebook.

1137
00:52:26,850 --> 00:52:28,890
And what I want to show here,

1138
00:52:28,890 --> 00:52:31,860
is the customer user review data.

1139
00:52:31,860 --> 00:52:36,390
How easy it is to do a
subjective analysis of that data.

1140
00:52:36,390 --> 00:52:37,740
Anyone here tried ever

1141
00:52:37,740 --> 00:52:40,560
to do a sentiment analysis of a dataset

1142
00:52:40,560 --> 00:52:43,980
that is human-dependent?

1143
00:52:43,980 --> 00:52:45,420
I mean, you can imagine the number

1144
00:52:45,420 --> 00:52:48,030
of machine-learning libraries
that you have to use

1145
00:52:48,030 --> 00:52:52,680
to get the code right and
actually get the right results.

1146
00:52:52,680 --> 00:52:57,570
To me, I can imagine it can
take me about two weeks of time.

1147
00:52:57,570 --> 00:53:00,330
This demo took less than 10 minutes.

1148
00:53:00,330 --> 00:53:04,290
Let me stop talking about
that, let me start the demo.

1149
00:53:04,290 --> 00:53:06,663
It's a little long, but pay attention.

1150
00:53:07,877 --> 00:53:10,099
I think I can promise you
that you'll enjoy this.

1151
00:53:10,099 --> 00:53:11,490
All right, so, we are starting

1152
00:53:11,490 --> 00:53:16,490
with the launching the
IAM-based SageMaker interface

1153
00:53:19,020 --> 00:53:20,640
and quickly navigating

1154
00:53:20,640 --> 00:53:23,430
to our customer review dataset, right?

1155
00:53:23,430 --> 00:53:27,840
And then, we create a data notebook just

1156
00:53:27,840 --> 00:53:29,190
by the click of a button,

1157
00:53:29,190 --> 00:53:32,890
pre-configured with a
cell that is telling me

1158
00:53:33,780 --> 00:53:34,830
what my dataset is

1159
00:53:34,830 --> 00:53:37,020
and a simple SQL query,

1160
00:53:37,020 --> 00:53:39,570
so I can make sure I have
the right permissions

1161
00:53:39,570 --> 00:53:40,770
to my dataset

1162
00:53:40,770 --> 00:53:44,043
and I'm looking at the right
data in the right context.

1163
00:53:45,300 --> 00:53:47,490
I go ahead and run that query

1164
00:53:47,490 --> 00:53:50,370
and confirm I have the right dataset.

1165
00:53:50,370 --> 00:53:53,880
So, keep in mind, this is
the customer user review data

1166
00:53:53,880 --> 00:53:56,703
that we are using in our agentic AI demo.

1167
00:53:57,720 --> 00:54:01,410
On the right side of the data
notebook is the data agent

1168
00:54:01,410 --> 00:54:03,310
that Shikha was talking about earlier.

1169
00:54:04,320 --> 00:54:06,450
I want to test this data agent, right?

1170
00:54:06,450 --> 00:54:07,830
I want to put it through a grind

1171
00:54:07,830 --> 00:54:09,030
to see how good it is, right?

1172
00:54:09,030 --> 00:54:11,330
And this is what we are
going to do over here.

1173
00:54:12,540 --> 00:54:15,397
I start with giving it
something very simple by saying,

1174
00:54:15,397 --> 00:54:19,260
"Give me the count of unique
flights with reviews," right?

1175
00:54:19,260 --> 00:54:21,420
I also intentionally use bad grammar

1176
00:54:21,420 --> 00:54:23,760
to see is it picking up my intent,

1177
00:54:23,760 --> 00:54:25,661
what I'm really trying to find?

1178
00:54:25,661 --> 00:54:28,260
(audience member coughing)

1179
00:54:28,260 --> 00:54:30,480
It goes ahead, spins up a cell,

1180
00:54:30,480 --> 00:54:33,300
it decides to use SQL on its own.

1181
00:54:33,300 --> 00:54:35,040
And I run that query

1182
00:54:35,040 --> 00:54:37,440
and yes, that is essentially
what I had in mind,

1183
00:54:37,440 --> 00:54:38,760
what I was looking for.

1184
00:54:38,760 --> 00:54:40,290
It gave me that data.

1185
00:54:40,290 --> 00:54:42,783
Okay, so, that was simple, right?

1186
00:54:43,680 --> 00:54:46,860
I want to test it with
something more challenging.

1187
00:54:46,860 --> 00:54:48,480
So, I come up with something over here.

1188
00:54:48,480 --> 00:54:51,420
I give the agent a complex task

1189
00:54:51,420 --> 00:54:56,160
to do a subjective analysis
of the customer reviews

1190
00:54:56,160 --> 00:54:59,340
by asking it to do a sentiment analysis

1191
00:54:59,340 --> 00:55:01,020
of the data that I have.

1192
00:55:01,020 --> 00:55:02,677
So, this is where I want to see,

1193
00:55:02,677 --> 00:55:05,760
"Do I really have confidence
in my user reviews," right?

1194
00:55:05,760 --> 00:55:07,770
How's the distribution of
positive versus negative?

1195
00:55:07,770 --> 00:55:09,330
What does that look like?

1196
00:55:09,330 --> 00:55:13,897
So, I go ahead, you can see,
I'm asking it very nicely.

1197
00:55:13,897 --> 00:55:15,450
"Can you do a sentiment analysis

1198
00:55:15,450 --> 00:55:17,007
for the reviews of my data?"

1199
00:55:17,007 --> 00:55:19,060
The agent goes into thinking mode

1200
00:55:22,200 --> 00:55:26,133
and then comes back with an
execution plan over here.

1201
00:55:29,100 --> 00:55:29,933
Waiting on it.

1202
00:55:30,840 --> 00:55:31,800
There you go.

1203
00:55:31,800 --> 00:55:35,400
So, what it's doing is
it automatically decided

1204
00:55:35,400 --> 00:55:38,790
to switch from SQL to
Python for this task.

1205
00:55:38,790 --> 00:55:40,470
I don't have to tell it anything.

1206
00:55:40,470 --> 00:55:44,100
It determines the Python
libraries that are needed

1207
00:55:44,100 --> 00:55:47,403
and available for this task
in my notebook environment.

1208
00:55:49,740 --> 00:55:52,290
For example, it plans to use,

1209
00:55:52,290 --> 00:55:54,450
and I know it's a little hard to see,

1210
00:55:54,450 --> 00:55:55,740
but take my word for it,

1211
00:55:55,740 --> 00:55:58,740
it's trying to use the TextBlob library

1212
00:55:58,740 --> 00:56:00,750
to process textual data

1213
00:56:00,750 --> 00:56:03,960
and the Transformers library
that it found in my environment

1214
00:56:03,960 --> 00:56:07,560
for natural language processing
using pre-trained models

1215
00:56:07,560 --> 00:56:10,620
for that sentiment analysis
that I was asking about it.

1216
00:56:10,620 --> 00:56:14,880
And in this case, it decided
to use the DistilBERT model,

1217
00:56:14,880 --> 00:56:17,460
that is ideal for a sentiment analysis

1218
00:56:17,460 --> 00:56:19,233
in notebook-style environments.

1219
00:56:23,670 --> 00:56:24,503
All right?

1220
00:56:24,503 --> 00:56:26,970
And then, the agent tries
to show off actually

1221
00:56:26,970 --> 00:56:29,940
by also proposing a visualization summary

1222
00:56:29,940 --> 00:56:31,620
of the sentiment analysis,

1223
00:56:31,620 --> 00:56:34,620
followed by a breakdown of the flight

1224
00:56:34,620 --> 00:56:36,170
with the most positive reviews.

1225
00:56:37,650 --> 00:56:39,630
We run the first cell over here.

1226
00:56:39,630 --> 00:56:42,840
And as you can see, it is now
working on the dependencies,

1227
00:56:42,840 --> 00:56:44,910
it is thinking through it.

1228
00:56:44,910 --> 00:56:47,070
Let's see what it comes back with.

1229
00:56:47,070 --> 00:56:50,790
All right, it came back
with an error over here.

1230
00:56:50,790 --> 00:56:52,620
This error is essentially
what it's telling us,

1231
00:56:52,620 --> 00:56:55,260
the DataFrame that was passed to this cell

1232
00:56:55,260 --> 00:56:58,563
from the previous SQL query is not right.

1233
00:57:00,120 --> 00:57:02,073
Being who I am, a little bit lazy,

1234
00:57:03,210 --> 00:57:06,750
I know what's wrong, I'm just
don't wanna fix it myself.

1235
00:57:06,750 --> 00:57:11,553
I use the fix with AI feature
that we have in this notebook.

1236
00:57:13,530 --> 00:57:14,363
There you go.

1237
00:57:15,870 --> 00:57:17,133
- [Shikha Verma] And folks,
as it is fixing with the AI,

1238
00:57:17,133 --> 00:57:19,290
I know we are coming up at time.

1239
00:57:19,290 --> 00:57:23,190
So, we tried something
different here for this session.

1240
00:57:23,190 --> 00:57:25,110
We did more show than tell.

1241
00:57:25,110 --> 00:57:26,130
If this worked for you,

1242
00:57:26,130 --> 00:57:28,530
please do give us feedback
in the session survey.

1243
00:57:28,530 --> 00:57:30,870
'Cause we'd like to do more of
this if this worked for you.

1244
00:57:30,870 --> 00:57:32,717
So, while that is fixing with AI,

1245
00:57:32,717 --> 00:57:34,830
here's a human announcement for you.

1246
00:57:34,830 --> 00:57:36,360
- Right, it did its job, Shikha.

1247
00:57:36,360 --> 00:57:37,260
Thank you for that.

1248
00:57:37,260 --> 00:57:40,500
And all right, it fixed it,
it made the code change.

1249
00:57:40,500 --> 00:57:41,857
And now, it is telling me,

1250
00:57:41,857 --> 00:57:43,320
"Okay, I've made that change.

1251
00:57:43,320 --> 00:57:44,640
I think this will work.

1252
00:57:44,640 --> 00:57:47,220
If you like it, accept and run."

1253
00:57:47,220 --> 00:57:48,303
I do that.

1254
00:57:52,470 --> 00:57:56,400
Okay, as in the real world,
another error, right?

1255
00:57:56,400 --> 00:57:59,910
And this time, it's a
little bit more challenging,

1256
00:57:59,910 --> 00:58:01,500
what that error is.

1257
00:58:01,500 --> 00:58:03,960
It's talking about a Keras library

1258
00:58:03,960 --> 00:58:07,890
that is not compatible with
what it's trying to do.

1259
00:58:07,890 --> 00:58:10,867
And the Python code is telling me,

1260
00:58:10,867 --> 00:58:14,637
"Go ahead and install this
to make it compatible."

1261
00:58:16,440 --> 00:58:18,240
Again, okay, now I want

1262
00:58:18,240 --> 00:58:20,790
to test the fix with AI a little bit more.

1263
00:58:20,790 --> 00:58:23,640
I tell it, "Why don't you try to fix it?"

1264
00:58:23,640 --> 00:58:26,130
So, this is what the agent does,
the data agent does, right?

1265
00:58:26,130 --> 00:58:27,390
It looks at the error

1266
00:58:27,390 --> 00:58:29,910
and it knows that there's
a compatibility issue,

1267
00:58:29,910 --> 00:58:32,490
but it takes a different
direction in fixing that error.

1268
00:58:32,490 --> 00:58:34,950
So, instead of fixing
the compatibility issue,

1269
00:58:34,950 --> 00:58:39,540
it actually pivots to a
different library like PyTorch,

1270
00:58:39,540 --> 00:58:42,300
because it knows that will work for sure.

1271
00:58:42,300 --> 00:58:44,490
Because that's the response
that it is giving me.

1272
00:58:44,490 --> 00:58:47,430
You can see it's made the
code change line number eight.

1273
00:58:47,430 --> 00:58:49,980
And again, like I said, the
agent was actually showing off.

1274
00:58:49,980 --> 00:58:51,580
It made the change in the comment as well,

1275
00:58:51,580 --> 00:58:54,810
that "I'm changing this
library to PyTorch."

1276
00:58:54,810 --> 00:58:57,333
All right, I accept it, I run it.

1277
00:59:01,500 --> 00:59:03,750
It's running the code,
it's running the cell.

1278
00:59:06,240 --> 00:59:08,760
All right, I got one output that it cannot

1279
00:59:08,760 --> 00:59:12,180
and I got the results of
what it was trying to do.

1280
00:59:12,180 --> 00:59:13,890
Okay, so, that was the first cell

1281
00:59:13,890 --> 00:59:17,520
where it was doing the
analysis for the sentiment.

1282
00:59:17,520 --> 00:59:22,520
Next, it's going to do a graphing
of the results over here.

1283
00:59:24,030 --> 00:59:26,340
Again, it generated the
Python code on its own.

1284
00:59:26,340 --> 00:59:29,520
It's using Matplot libraries by itself.

1285
00:59:29,520 --> 00:59:31,290
I simply go ahead, run it.

1286
00:59:31,290 --> 00:59:33,510
This one runs without any issues.

1287
00:59:33,510 --> 00:59:35,130
I got my data.

1288
00:59:35,130 --> 00:59:37,860
I go from data engineer,
to data scientists,

1289
00:59:37,860 --> 00:59:41,727
to a data analyst, all in one over here.

1290
00:59:41,727 --> 00:59:42,560
All right?

1291
00:59:42,560 --> 00:59:44,010
And then the last query,

1292
00:59:44,010 --> 00:59:47,040
which is essentially
giving me the highest count

1293
00:59:47,040 --> 00:59:50,430
of the best airline with
the most positive reviews.

1294
00:59:50,430 --> 00:59:51,510
In the interest of time,

1295
00:59:51,510 --> 00:59:53,580
I'm just gonna move on to the next slide,

1296
00:59:53,580 --> 00:59:55,680
which is the second-last slide over here.

1297
00:59:55,680 --> 00:59:58,230
Key takeaways, not gonna read this slide.

1298
00:59:58,230 --> 01:00:00,360
We talked a lot about this over here.

1299
01:00:00,360 --> 01:00:03,090
Key idea is you don't really
have to build anything new

1300
01:00:03,090 --> 01:00:04,797
in terms of data foundation.

1301
01:00:04,797 --> 01:00:06,180
AWS has what it takes for you

1302
01:00:06,180 --> 01:00:08,313
to build your agentic AI applications.

1303
01:00:11,160 --> 01:00:12,240
Leverage the critical path

1304
01:00:12,240 --> 01:00:15,390
between data and AI through
purpose-built capabilities,

1305
01:00:15,390 --> 01:00:17,700
some of which we saw over here today,

1306
01:00:17,700 --> 01:00:20,070
MCP servers, context management,

1307
01:00:20,070 --> 01:00:22,590
vector stores and data governance.

1308
01:00:22,590 --> 01:00:24,990
And with that, folks,

1309
01:00:24,990 --> 01:00:26,460
if you have additional needs,

1310
01:00:26,460 --> 01:00:28,893
we do have programs to
help you get started.

1311
01:00:31,740 --> 01:00:32,780
Thank you for-

1312
01:00:32,780 --> 01:00:33,613
- Thank you.

1313
01:00:33,613 --> 01:00:34,650
- [Imtiaz Sayed] Your
patience staying with us.

1314
01:00:34,650 --> 01:00:38,093
I appreciate that.
(audience applauding)

