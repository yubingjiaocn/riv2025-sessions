1
00:00:00,070 --> 00:00:02,100
- Yeah, just to kick this off,

2
00:00:02,100 --> 00:00:05,130
today we're gonna be
looking at log analytics

3
00:00:05,130 --> 00:00:08,520
and log management through
the power of Dynatrace

4
00:00:08,520 --> 00:00:10,410
and the power of Davis AI.

5
00:00:10,410 --> 00:00:13,920
So looking at how we can do log
analytics at petabyte scale,

6
00:00:13,920 --> 00:00:17,220
ingest, and precise root cause analysis.

7
00:00:17,220 --> 00:00:19,800
And we've got a really interesting story

8
00:00:19,800 --> 00:00:22,590
to sort of back this up as
well from from Alex here,

9
00:00:22,590 --> 00:00:26,580
from Storio group, and also
of course with the partnership

10
00:00:26,580 --> 00:00:29,310
that we have here with AWS, and again,

11
00:00:29,310 --> 00:00:32,130
how of course Dynatrace, AWS,

12
00:00:32,130 --> 00:00:34,860
and of course our customers
that we have together,

13
00:00:34,860 --> 00:00:36,600
work hand in hand.

14
00:00:36,600 --> 00:00:39,480
So I am Jon Griffiths.

15
00:00:39,480 --> 00:00:41,850
I'm the field CTO at Dynatrace,

16
00:00:41,850 --> 00:00:44,913
covering primarily Europe,
Middle East, and Africa.

17
00:00:45,750 --> 00:00:47,070
And...

18
00:00:47,070 --> 00:00:48,960
- My name is Alex Hibbitt.

19
00:00:48,960 --> 00:00:51,060
I'm a engineering director.

20
00:00:51,060 --> 00:00:54,720
I'm responsible for the customer
platform of Storio group.

21
00:00:54,720 --> 00:00:56,130
- And I'm Frank Schwarzenau.

22
00:00:56,130 --> 00:00:58,740
I'm the global lead for
the observability use case

23
00:00:58,740 --> 00:01:00,483
for AWS Partner Specialists.

24
00:01:02,460 --> 00:01:05,280
And yeah, let you start now.

25
00:01:05,280 --> 00:01:06,863
- No worries.
- Back in a second.

26
00:01:08,010 --> 00:01:08,843
- So

27
00:01:10,004 --> 00:01:11,730
as we sort of started on,

28
00:01:11,730 --> 00:01:14,220
we'll look at first Dynatrace and AWS

29
00:01:14,220 --> 00:01:15,540
and that partnership together,

30
00:01:15,540 --> 00:01:18,690
and how we sort of enable
customers to innovate,

31
00:01:18,690 --> 00:01:21,210
move faster, and of course reduce

32
00:01:21,210 --> 00:01:24,183
their mean time to resolution
and resolve an instance.

33
00:01:25,170 --> 00:01:27,000
We'll then move on to Alex

34
00:01:27,000 --> 00:01:29,130
with the interesting story
around Storio group's journey

35
00:01:29,130 --> 00:01:30,120
with log analytics,

36
00:01:30,120 --> 00:01:32,613
and how they've made
massive changes around that.

37
00:01:33,690 --> 00:01:35,580
And then finally, touching on

38
00:01:35,580 --> 00:01:37,860
actually modern log management,

39
00:01:37,860 --> 00:01:39,780
the sort of things that
you can learn from that,

40
00:01:39,780 --> 00:01:41,220
how we can look at, you know,

41
00:01:41,220 --> 00:01:43,440
significant cost savings
a lot of the time,

42
00:01:43,440 --> 00:01:48,390
the sort of 60 to 70%
reduction in log spend

43
00:01:48,390 --> 00:01:51,483
while still getting even more
significant value out of that.

44
00:01:53,580 --> 00:01:56,130
So I'll start with Dynatrace.

45
00:01:56,130 --> 00:01:58,050
How many of you here, let's say,

46
00:01:58,050 --> 00:02:00,370
are a customer of Dynatrace?

47
00:02:00,370 --> 00:02:01,890
Okay, reasonable amount.

48
00:02:01,890 --> 00:02:04,767
How many of you here work for
a competitor of Dynatrace?

49
00:02:05,786 --> 00:02:06,619
No?

50
00:02:06,619 --> 00:02:08,370
Ah, there's a couple.

51
00:02:08,370 --> 00:02:10,230
Security? No.

52
00:02:10,230 --> 00:02:11,063
Okay.

53
00:02:11,940 --> 00:02:13,980
Now, as a whole, basically,

54
00:02:13,980 --> 00:02:16,710
Dynatrace, for those
that aren't fully aware,

55
00:02:16,710 --> 00:02:21,710
focuses on observability and
security with AI at its core.

56
00:02:22,830 --> 00:02:25,890
Now, what that ends up
meaning is that all data,

57
00:02:25,890 --> 00:02:27,570
all signals, whether that's the logs,

58
00:02:27,570 --> 00:02:29,610
whether that's metrics, traces,

59
00:02:29,610 --> 00:02:32,130
whether that's from your
infrastructure, your applications,

60
00:02:32,130 --> 00:02:35,730
the real users, your AI agents or LLMs,

61
00:02:35,730 --> 00:02:36,570
whatever it may be,

62
00:02:36,570 --> 00:02:40,440
getting all of that
deep insight in context.

63
00:02:40,440 --> 00:02:41,640
And the context is key,

64
00:02:41,640 --> 00:02:45,213
because whenever you look at
things like automation or AI,

65
00:02:46,650 --> 00:02:48,720
good-quality data is is called to that.

66
00:02:48,720 --> 00:02:51,570
You know, rubbish data, rubbish output;

67
00:02:51,570 --> 00:02:53,670
good-quality data obviously allows you

68
00:02:53,670 --> 00:02:56,553
to have far better outputs
associated with that.

69
00:02:57,450 --> 00:03:00,300
Now, again, when you
start to make decisions

70
00:03:00,300 --> 00:03:02,400
and data-driven decisions,

71
00:03:02,400 --> 00:03:04,380
again, data, context,

72
00:03:04,380 --> 00:03:06,840
and especially the domain
context that comes into that

73
00:03:06,840 --> 00:03:08,970
can be very crucial to this.

74
00:03:08,970 --> 00:03:13,860
So really, Dynatrace is
providing that deep insight

75
00:03:13,860 --> 00:03:16,260
in the context of your
domain, in the context

76
00:03:16,260 --> 00:03:18,900
of wherever it's grabbing
that information from,

77
00:03:18,900 --> 00:03:21,360
so that, as you can see here,

78
00:03:21,360 --> 00:03:23,640
we like to say that we'll help
you understand your business

79
00:03:23,640 --> 00:03:24,873
like never before.

80
00:03:26,640 --> 00:03:31,050
Now, again, obviously a
few quick slides first,

81
00:03:31,050 --> 00:03:34,650
but we are obviously
leading in this market.

82
00:03:34,650 --> 00:03:37,200
As you can see here, this is
the Gartner Magic Quadrant,

83
00:03:37,200 --> 00:03:39,300
where the most recent one

84
00:03:39,300 --> 00:03:42,033
we've been in the highest
ability to execute,

85
00:03:42,900 --> 00:03:44,040
and ranked number one

86
00:03:44,040 --> 00:03:46,983
in four of the six critical capabilities.

87
00:03:48,660 --> 00:03:50,370
And finally, on there, as you can see,

88
00:03:50,370 --> 00:03:52,530
we are the only vendor to have been named

89
00:03:52,530 --> 00:03:54,870
in every single Gartner Magic Quadrant

90
00:03:54,870 --> 00:03:57,630
for APM and observability
since its inception.

91
00:03:57,630 --> 00:03:59,880
So we've been in that leader
quadrant in every single one.

92
00:03:59,880 --> 00:04:03,570
So that's 15 years of staying
in that leader quadrant.

93
00:04:03,570 --> 00:04:06,750
Again, innovation and
continuously striving forwards

94
00:04:06,750 --> 00:04:08,433
is obviously key to that.

95
00:04:10,410 --> 00:04:12,704
And of course it's not just
the analyst side of things.

96
00:04:12,704 --> 00:04:15,210
You know, we have customers
from every single industry.

97
00:04:15,210 --> 00:04:16,620
You know, whether that's,
as you can see here,

98
00:04:16,620 --> 00:04:21,300
things like aviation,
banking, retail, utilities,

99
00:04:21,300 --> 00:04:23,520
and all across the board.

100
00:04:23,520 --> 00:04:25,980
We have customers in every single industry

101
00:04:25,980 --> 00:04:28,560
and across every single continent.

102
00:04:28,560 --> 00:04:31,575
So again, really driving
that platform faster.

103
00:04:31,575 --> 00:04:34,500
Obviously we are taking the
insights from our customers,

104
00:04:34,500 --> 00:04:36,510
expanding obviously on our awareness

105
00:04:36,510 --> 00:04:39,180
and ensuring that we are
innovating to keep up

106
00:04:39,180 --> 00:04:42,573
and match what our
customers actually require.

107
00:04:44,580 --> 00:04:45,413
Oh, Frank.

108
00:04:46,410 --> 00:04:47,243
- That's my cue.

109
00:04:48,600 --> 00:04:50,640
So as I said before,

110
00:04:50,640 --> 00:04:53,130
my job at AWS is essentially just focusing

111
00:04:53,130 --> 00:04:55,920
on the AWS observability use case

112
00:04:55,920 --> 00:04:59,340
and working with both
ISVs and SI partners.

113
00:04:59,340 --> 00:05:03,660
And we have about 120,000
partners in our network.

114
00:05:03,660 --> 00:05:06,060
And of those 120,000 partners,

115
00:05:06,060 --> 00:05:08,520
we have about 200 in our
cloud operations competency,

116
00:05:08,520 --> 00:05:10,470
which includes observability.

117
00:05:10,470 --> 00:05:14,280
And there's actually just eight
ISV observability partners

118
00:05:14,280 --> 00:05:17,250
that are specialists or
deemed specialists from AWS

119
00:05:17,250 --> 00:05:18,990
and got the stamp of approval

120
00:05:18,990 --> 00:05:20,460
to be that observability partner.

121
00:05:20,460 --> 00:05:25,170
So the 10 years of
partnership is a great number,

122
00:05:25,170 --> 00:05:27,150
but it's more important to focus

123
00:05:27,150 --> 00:05:29,997
on how much Dynatrace
is leaning in with AWS

124
00:05:29,997 --> 00:05:31,530
and with the customers.

125
00:05:31,530 --> 00:05:33,930
And that leaning in is
of course represented

126
00:05:33,930 --> 00:05:37,050
by all of these competencies
that are achieved

127
00:05:37,050 --> 00:05:38,130
in the bottom here,

128
00:05:38,130 --> 00:05:41,190
which essentially are the
stamp of approval from AWS,

129
00:05:41,190 --> 00:05:43,560
where Dynatrace jumps
through a lot of hoops

130
00:05:43,560 --> 00:05:45,750
that we put out there in order

131
00:05:45,750 --> 00:05:47,767
for you as a customer
to be able to say, like,

132
00:05:47,767 --> 00:05:50,010
"Wow, Dynatrace, they really
know what they're doing

133
00:05:50,010 --> 00:05:51,747
around all of these use cases."

134
00:05:52,590 --> 00:05:54,300
But one exciting part of course

135
00:05:54,300 --> 00:05:58,770
is how much Dynatrace is
leaning in on the AI use case

136
00:05:58,770 --> 00:06:01,290
and is launching on, you know,

137
00:06:01,290 --> 00:06:04,530
on a monthly basis something
like all of the integrations

138
00:06:04,530 --> 00:06:07,110
with Bedrock AgentCore, for instance,

139
00:06:07,110 --> 00:06:09,180
for end-to-end observability
for your agents.

140
00:06:09,180 --> 00:06:12,640
Or even Kiro integrations for AI-assisted

141
00:06:14,220 --> 00:06:15,870
troubleshooting through your IDE.

142
00:06:17,190 --> 00:06:18,510
But also what's really important,

143
00:06:18,510 --> 00:06:20,790
and I think what I always come across

144
00:06:20,790 --> 00:06:25,320
is there's very few customers
that complain about Dynatrace,

145
00:06:25,320 --> 00:06:26,700
which is really nice to see,

146
00:06:26,700 --> 00:06:29,220
because I think, from my perspective,

147
00:06:29,220 --> 00:06:30,990
and I see a lot of different
partner engagements

148
00:06:30,990 --> 00:06:32,400
and a lot of different
customer engagements

149
00:06:32,400 --> 00:06:33,660
without partners,

150
00:06:33,660 --> 00:06:35,580
Dynatrace is really kind
of moving the needle

151
00:06:35,580 --> 00:06:38,100
and really kind of
defining of what it means

152
00:06:38,100 --> 00:06:39,300
to be customer-obsessed,

153
00:06:39,300 --> 00:06:42,030
what it means to be aligned with AWS,

154
00:06:42,030 --> 00:06:43,920
and provide that value throughout,

155
00:06:43,920 --> 00:06:46,170
not just make migration easy,

156
00:06:46,170 --> 00:06:48,540
but then also how does
the customer get value

157
00:06:48,540 --> 00:06:51,330
out of using both AWS in the combination

158
00:06:51,330 --> 00:06:52,743
with Dynatrace afterwards.

159
00:06:53,580 --> 00:06:55,920
One of the cool things is
also the native integrations.

160
00:06:55,920 --> 00:06:57,870
This is probably way over
a hundred at this point

161
00:06:57,870 --> 00:06:59,700
because the slide is a few weeks old,

162
00:06:59,700 --> 00:07:04,410
but we see Dynatrace really leaning in

163
00:07:04,410 --> 00:07:06,510
on launching services together with us.

164
00:07:06,510 --> 00:07:07,470
And being a launch partner,

165
00:07:07,470 --> 00:07:09,840
we beat our customers for stuff

166
00:07:09,840 --> 00:07:12,513
that you don't even know
it's around the corner yet.

167
00:07:13,410 --> 00:07:15,460
But really kind of being tied at the hip.

168
00:07:16,650 --> 00:07:19,980
And of course you were Global
Partner of the Year last year.

169
00:07:19,980 --> 00:07:21,690
No, this year, 2025.

170
00:07:21,690 --> 00:07:24,090
You were Technology Partner
of the Year in EMEA last year.

171
00:07:24,090 --> 00:07:27,570
So I think there's a big recognition

172
00:07:27,570 --> 00:07:31,263
from both the AWS environment
as well as from our customers.

173
00:07:33,540 --> 00:07:35,190
Dynatrace leans in,

174
00:07:35,190 --> 00:07:37,710
and I can talk about
each of these pillars.

175
00:07:37,710 --> 00:07:39,450
So I hope everybody's
familiar with these pillars

176
00:07:39,450 --> 00:07:41,670
from the Well-Architected Framework.

177
00:07:41,670 --> 00:07:46,110
But of course you have,
you know, resilience,

178
00:07:46,110 --> 00:07:48,900
you have resource optimization,
you have cost optimization,

179
00:07:48,900 --> 00:07:50,670
you have performance optimization,

180
00:07:50,670 --> 00:07:52,170
and SLA management and so on.

181
00:07:52,170 --> 00:07:54,630
But the more important
fact about this slide

182
00:07:54,630 --> 00:07:57,900
is that Dynatrace thinks about
the impact to the customer,

183
00:07:57,900 --> 00:08:00,360
very much aligned with
how AWS thinks about

184
00:08:00,360 --> 00:08:01,920
how to have impact with the customer,

185
00:08:01,920 --> 00:08:05,070
so that it's gonna be easy

186
00:08:05,070 --> 00:08:07,440
for all of us to work together

187
00:08:07,440 --> 00:08:09,930
and talk around the same
kind of core concepts

188
00:08:09,930 --> 00:08:12,990
and core guiding principles.

189
00:08:12,990 --> 00:08:15,150
I talked about AI before

190
00:08:15,150 --> 00:08:17,850
and how much Dynatrace is
leaning into these integrations.

191
00:08:17,850 --> 00:08:21,780
This number here of 500
implementations (chuckles)

192
00:08:21,780 --> 00:08:24,090
is actually just from the AI use case.

193
00:08:24,090 --> 00:08:28,200
So we already have 500
wins that we have together,

194
00:08:28,200 --> 00:08:31,020
where Dynatrace has enabled customers

195
00:08:31,020 --> 00:08:35,220
to really leverage the
best of AWS and Dynatrace

196
00:08:35,220 --> 00:08:37,443
to adopt the AI use cases.

197
00:08:39,720 --> 00:08:44,720
And this is around or on all
different kind of layers.

198
00:08:44,730 --> 00:08:47,340
So I think everybody here in
the room is pretty familiar

199
00:08:47,340 --> 00:08:48,600
of how to do observability

200
00:08:48,600 --> 00:08:50,370
or how observability should be done

201
00:08:50,370 --> 00:08:51,420
on the application layer,

202
00:08:51,420 --> 00:08:53,490
and probably also pretty familiar

203
00:08:53,490 --> 00:08:56,070
with how it should be done
on the infrastructure layer.

204
00:08:56,070 --> 00:08:57,660
But I think the important part

205
00:08:57,660 --> 00:08:59,100
is sort of the wedge in between, you know,

206
00:08:59,100 --> 00:09:00,570
on the orchestration level

207
00:09:00,570 --> 00:09:03,070
where you have prompt observability,

208
00:09:03,070 --> 00:09:06,300
or on the semantic layer
where you really can look at

209
00:09:06,300 --> 00:09:09,120
how biased the inputs and the outputs are

210
00:09:09,120 --> 00:09:13,140
when you interact with
your AI applications.

211
00:09:13,140 --> 00:09:14,910
And then of course model health, you know,

212
00:09:14,910 --> 00:09:18,960
how do you continuously monitor
the health of your models

213
00:09:18,960 --> 00:09:21,450
that you use for your applications.

214
00:09:21,450 --> 00:09:24,240
And then I mentioned this already,

215
00:09:24,240 --> 00:09:27,120
Dynatrace launches a
lot of services with us,

216
00:09:27,120 --> 00:09:28,770
and I'm always happy

217
00:09:28,770 --> 00:09:31,200
when we are about to
launch another service,

218
00:09:31,200 --> 00:09:33,547
and the Dynatrace DO is
putting up their hand, like,

219
00:09:33,547 --> 00:09:34,500
"We would like to test it."

220
00:09:34,500 --> 00:09:36,030
I'm like, "Perfect."

221
00:09:36,030 --> 00:09:37,800
So that is happening continuously,

222
00:09:37,800 --> 00:09:39,960
and it's a real joy to see

223
00:09:39,960 --> 00:09:42,510
how much you guys are
leaning in on these things

224
00:09:42,510 --> 00:09:43,650
and giving us feedback as well,

225
00:09:43,650 --> 00:09:45,240
and sort of co-innovate together

226
00:09:45,240 --> 00:09:47,553
in order to make this
easy for our customers.

227
00:09:48,960 --> 00:09:52,770
Speaking of customers, this
is just a couple of metrics.

228
00:09:52,770 --> 00:09:54,750
And it's quite interesting,

229
00:09:54,750 --> 00:09:56,883
when I kind of dove deep on this one,

230
00:09:58,350 --> 00:09:59,640
and eventually, essentially,

231
00:09:59,640 --> 00:10:02,070
this is all essentially working
backwards to money, right?

232
00:10:02,070 --> 00:10:04,870
So if you're reducing this time spent

233
00:10:07,144 --> 00:10:09,780
on your resolutions,

234
00:10:09,780 --> 00:10:13,230
if you're actually reducing
the overall number of incidents

235
00:10:13,230 --> 00:10:15,210
or major incidents,

236
00:10:15,210 --> 00:10:18,120
and if you just have
faster issue resolving

237
00:10:18,120 --> 00:10:20,370
and higher uptime, in the end of the day,

238
00:10:20,370 --> 00:10:24,000
all of this is boiling down to
customer happiness and money.

239
00:10:24,000 --> 00:10:27,240
And this is just some really,
really impressive numbers.

240
00:10:27,240 --> 00:10:29,733
And we're glad that
we're in this together.

241
00:10:30,870 --> 00:10:33,270
And I can continue in talking and talking

242
00:10:33,270 --> 00:10:35,130
about how much customer impact we have,

243
00:10:35,130 --> 00:10:36,610
but we have a customer here

244
00:10:37,887 --> 00:10:39,750
that we're both partnering with.

245
00:10:39,750 --> 00:10:42,500
Alex, if you want to come up
and maybe tell your story.

246
00:10:44,190 --> 00:10:46,860
- Absolutely. Thank you very much, Frank.

247
00:10:46,860 --> 00:10:47,693
So

248
00:10:48,720 --> 00:10:51,270
who here has heard of the
three pillars of observability?

249
00:10:51,270 --> 00:10:55,470
Bit of an outdated term now,
but just a quick show of hands.

250
00:10:55,470 --> 00:10:56,970
Yeah, a few of you.

251
00:10:56,970 --> 00:10:57,810
I'm not gonna talk about

252
00:10:57,810 --> 00:10:59,430
those three pillars of observability.

253
00:10:59,430 --> 00:11:01,320
Instead, I'm gonna talk
about an alternative

254
00:11:01,320 --> 00:11:03,090
three pillars of observability.

255
00:11:03,090 --> 00:11:05,520
And these have come out
of our adoption journey

256
00:11:05,520 --> 00:11:06,360
with Dynatrace.

257
00:11:06,360 --> 00:11:08,310
I say adoption, it's in fact a readoption

258
00:11:08,310 --> 00:11:09,900
because we'd worked with Dynatrace

259
00:11:09,900 --> 00:11:12,120
in a previous iteration before.

260
00:11:12,120 --> 00:11:13,950
I'm willing to bet most
of you have never heard

261
00:11:13,950 --> 00:11:15,720
of Storio group though.

262
00:11:15,720 --> 00:11:18,600
So Storio group is Europe's biggest

263
00:11:18,600 --> 00:11:20,553
photo-gifting company by revenue.

264
00:11:21,540 --> 00:11:24,540
We operate in about a dozen countries

265
00:11:24,540 --> 00:11:26,730
under eight different brands,

266
00:11:26,730 --> 00:11:28,380
some of which you may have heard of:

267
00:11:28,380 --> 00:11:32,643
albelli, Photobox, Hofmann, posterXXL.

268
00:11:33,540 --> 00:11:36,540
We serve over 7 million customers a year.

269
00:11:36,540 --> 00:11:40,833
And in 2022 we delivered
over 10 million orders.

270
00:11:42,450 --> 00:11:44,070
And we've been around
for quite a long while,

271
00:11:44,070 --> 00:11:45,270
but not in the skies.

272
00:11:45,270 --> 00:11:46,770
Our oldest brand is Hofmann,

273
00:11:46,770 --> 00:11:48,450
and that's been in the Spanish market

274
00:11:48,450 --> 00:11:50,070
for over a hundred years now.

275
00:11:50,070 --> 00:11:52,440
So we've got some serious history.

276
00:11:52,440 --> 00:11:56,660
But as Storio group, we've
only came together in 2021

277
00:11:56,660 --> 00:11:58,830
as a merger of two different businesses.

278
00:11:58,830 --> 00:12:01,140
albelli, who were based
in the Netherlands,

279
00:12:01,140 --> 00:12:04,233
and Photobox who were a
group based in the UK.

280
00:12:05,820 --> 00:12:09,150
When we joined together,
we had a massive challenge.

281
00:12:09,150 --> 00:12:12,240
Immediately post-merger, we
looked around our ecosystem,

282
00:12:12,240 --> 00:12:14,490
and we had something like five different

283
00:12:14,490 --> 00:12:17,460
technology platforms
providing, largely speaking,

284
00:12:17,460 --> 00:12:19,410
the same customer experience,

285
00:12:19,410 --> 00:12:22,293
selling photo gifts to end users.

286
00:12:23,370 --> 00:12:26,400
Any organization looking
to innovate and make change

287
00:12:26,400 --> 00:12:29,790
cannot do so when they've
got to do things five times

288
00:12:29,790 --> 00:12:31,620
to roll out one feature.

289
00:12:31,620 --> 00:12:34,260
So we really had a huge
call to action there,

290
00:12:34,260 --> 00:12:37,170
and that call to action was how do we move

291
00:12:37,170 --> 00:12:40,110
into one singular platform?

292
00:12:40,110 --> 00:12:42,540
We also had huge scale to deal with.

293
00:12:42,540 --> 00:12:43,950
Across all of those platforms,

294
00:12:43,950 --> 00:12:47,130
we had over 11 petabytes'
worth of customer data:

295
00:12:47,130 --> 00:12:48,510
that's customer photos,

296
00:12:48,510 --> 00:12:50,510
or memories as we like to refer to them.

297
00:12:52,110 --> 00:12:55,350
We really needed to go on a
massively imperative journey

298
00:12:55,350 --> 00:12:57,570
to reduce the duplication we had,

299
00:12:57,570 --> 00:13:01,290
down from those five platforms
to one singular platform.

300
00:13:01,290 --> 00:13:03,930
And that journey took
up two and a half years,

301
00:13:03,930 --> 00:13:06,780
that first two and a half
years of our journey together,

302
00:13:06,780 --> 00:13:10,773
and exclusive focus, exclusive
and probably ruthless focus.

303
00:13:13,080 --> 00:13:15,210
When we got to the final point

304
00:13:15,210 --> 00:13:17,160
of having one singular platform,

305
00:13:17,160 --> 00:13:19,350
we took a breath for a
moment and looked around

306
00:13:19,350 --> 00:13:22,350
and realized that we had
made many, many trade-offs,

307
00:13:22,350 --> 00:13:24,150
as you have to do when you're on a journey

308
00:13:24,150 --> 00:13:27,243
of such complexity and such pressure.

309
00:13:28,080 --> 00:13:31,590
We previously had a really
mature observability posture

310
00:13:31,590 --> 00:13:34,770
when we were the Photobox
half of the organization.

311
00:13:34,770 --> 00:13:38,250
And the albelli half of the organization

312
00:13:38,250 --> 00:13:40,860
had had a bit of a different strategy.

313
00:13:40,860 --> 00:13:43,230
And when we took moment and we had a look,

314
00:13:43,230 --> 00:13:45,990
we said there are some
things missing here.

315
00:13:45,990 --> 00:13:50,250
We had a platform that
was strongly human-heavy.

316
00:13:50,250 --> 00:13:52,800
It was very, very log-centric.

317
00:13:52,800 --> 00:13:56,970
And with that we looked
at our MTTR and our MTTD,

318
00:13:56,970 --> 00:13:58,260
two core metrics,

319
00:13:58,260 --> 00:14:00,330
and realized that they were
trending in a direction

320
00:14:00,330 --> 00:14:02,130
that we did not like.

321
00:14:02,130 --> 00:14:04,980
MTTR was going massively up,

322
00:14:04,980 --> 00:14:07,170
and simultaneously the number of incidents

323
00:14:07,170 --> 00:14:09,300
we were tracking was dropping.

324
00:14:09,300 --> 00:14:11,820
To me that meant that we were losing track

325
00:14:11,820 --> 00:14:13,350
of what was happening on our platform.

326
00:14:13,350 --> 00:14:16,530
We were not observing as much
as we had previously been

327
00:14:16,530 --> 00:14:18,153
in our Photobox iteration.

328
00:14:20,010 --> 00:14:24,600
To give you an example, Black
Sunday, as we like to call it,

329
00:14:24,600 --> 00:14:27,660
which is yesterday, the
biggest trading day of the year

330
00:14:27,660 --> 00:14:29,820
for photo-gifting companies,

331
00:14:29,820 --> 00:14:32,460
we were looking at engineers

332
00:14:32,460 --> 00:14:36,030
manually trying to
correlate performance data

333
00:14:36,030 --> 00:14:38,220
and understand what was going wrong

334
00:14:38,220 --> 00:14:43,220
across 1.1 billion individual log lines.

335
00:14:43,260 --> 00:14:45,210
Humans looking at that much data

336
00:14:45,210 --> 00:14:48,420
are going to miss not just
one thing, not just 10 things,

337
00:14:48,420 --> 00:14:49,740
but hundreds of things.

338
00:14:49,740 --> 00:14:53,430
And it was intensely resource-intensive

339
00:14:53,430 --> 00:14:55,620
in terms of the computational power needed

340
00:14:55,620 --> 00:14:57,390
and the human power needed,

341
00:14:57,390 --> 00:15:00,573
and it was fundamentally not
solving the problem we had.

342
00:15:03,600 --> 00:15:04,650
The challenge with this

343
00:15:04,650 --> 00:15:07,443
was it created a massive
business risk for us.

344
00:15:08,910 --> 00:15:10,080
Like many businesses,

345
00:15:10,080 --> 00:15:12,480
Black Friday through to the holiday season

346
00:15:12,480 --> 00:15:15,090
is a hugely important period for us.

347
00:15:15,090 --> 00:15:17,340
Across those six weeks,

348
00:15:17,340 --> 00:15:19,350
an outage would've cost us something like,

349
00:15:19,350 --> 00:15:24,350
at any random point it
hit, at least $274,000.

350
00:15:24,750 --> 00:15:26,700
But during our peak hours, yesterday,

351
00:15:26,700 --> 00:15:27,840
as I was flying over here,

352
00:15:27,840 --> 00:15:29,730
I was watching the numbers come in;

353
00:15:29,730 --> 00:15:33,780
we trade up to one and
a half million dollars,

354
00:15:33,780 --> 00:15:35,550
million euros, an hour.

355
00:15:35,550 --> 00:15:37,560
That's a huge number, bearing in mind

356
00:15:37,560 --> 00:15:39,240
that an incident takes on average,

357
00:15:39,240 --> 00:15:42,060
when we were looking at
our observability adoption,

358
00:15:42,060 --> 00:15:43,800
an hour and a half to resolve.

359
00:15:43,800 --> 00:15:45,630
You're talking about way over

360
00:15:45,630 --> 00:15:48,240
two million euros' worth of trading

361
00:15:48,240 --> 00:15:51,000
suddenly blown up by an incident.

362
00:15:51,000 --> 00:15:54,510
That huge risk meant that this
became a massive imperative

363
00:15:54,510 --> 00:15:55,620
to the business.

364
00:15:55,620 --> 00:15:57,810
We needed to reorient what we were doing

365
00:15:57,810 --> 00:16:00,450
and make sure we became the first to know

366
00:16:00,450 --> 00:16:01,833
when trouble was brewing.

367
00:16:04,500 --> 00:16:07,623
So this brings me onto our first
pillar: the culture pillar.

368
00:16:09,300 --> 00:16:11,700
The core problem we had, if
you remember a minute ago,

369
00:16:11,700 --> 00:16:13,020
was that we had engineers

370
00:16:13,020 --> 00:16:15,600
who fundamentally were looking at logs

371
00:16:15,600 --> 00:16:17,650
to work out when things were going wrong.

372
00:16:19,110 --> 00:16:21,750
What that meant was our engineers

373
00:16:21,750 --> 00:16:25,890
couldn't answer the meaningful
question: What broke?

374
00:16:25,890 --> 00:16:27,240
Why did it break?

375
00:16:27,240 --> 00:16:29,163
And how do we fix it?

376
00:16:30,780 --> 00:16:32,310
And one of the biggest challenges we found

377
00:16:32,310 --> 00:16:33,870
was that was in fact a mindset

378
00:16:33,870 --> 00:16:37,470
that many of our engineers did
not understand was a problem.

379
00:16:37,470 --> 00:16:38,640
They thought that this was actually

380
00:16:38,640 --> 00:16:40,170
quite a good way of operating.

381
00:16:40,170 --> 00:16:44,130
They built these amazingly
huge Elasticsearch clusters

382
00:16:44,130 --> 00:16:45,780
and then OpenSearch clusters,

383
00:16:45,780 --> 00:16:49,140
which are great for sorting
through tons of information,

384
00:16:49,140 --> 00:16:50,910
but when you're trying to manually read

385
00:16:50,910 --> 00:16:53,560
all of that information,
you're going to miss things.

386
00:16:55,170 --> 00:16:56,310
We also had a challenge

387
00:16:56,310 --> 00:17:00,573
around how do we deliver
meaningful change to that culture.

388
00:17:02,490 --> 00:17:04,590
Our Anglo-Dutch engineering organization

389
00:17:04,590 --> 00:17:07,200
had a strong sense of its own identity

390
00:17:07,200 --> 00:17:09,030
and was very proud, quite rightly,

391
00:17:09,030 --> 00:17:10,230
of what they had built out

392
00:17:10,230 --> 00:17:12,960
in terms of the technology platform.

393
00:17:12,960 --> 00:17:14,580
And what that meant was people like me

394
00:17:14,580 --> 00:17:17,430
couldn't wander in and
say, "Oh, this is rubbish."

395
00:17:17,430 --> 00:17:19,740
I would've lost all
credibility by doing that.

396
00:17:19,740 --> 00:17:22,350
It would've been interpreted as top-down.

397
00:17:22,350 --> 00:17:26,250
So what we needed to do is take
our engineers on a journey,

398
00:17:26,250 --> 00:17:28,560
something that we've
jokingly started referring to

399
00:17:28,560 --> 00:17:32,250
as the start to the 12-step
program of observability.

400
00:17:32,250 --> 00:17:34,620
First of all, we needed
to help them understand

401
00:17:34,620 --> 00:17:35,820
they had a problem.

402
00:17:35,820 --> 00:17:37,500
It wasn't just me wandering along

403
00:17:37,500 --> 00:17:39,030
saying that this is rubbish.

404
00:17:39,030 --> 00:17:42,120
Each engineer needed to look
at the situation they had

405
00:17:42,120 --> 00:17:45,300
and say, "This is something
that we can make better."

406
00:17:45,300 --> 00:17:48,543
And then we had to help them
become part of the solution.

407
00:17:51,030 --> 00:17:52,983
So let's talk about how we got there.

408
00:17:54,960 --> 00:17:57,720
First thing we did was we
built out a small working group

409
00:17:57,720 --> 00:18:01,380
of five influential engineers
across the organization

410
00:18:01,380 --> 00:18:04,710
who were strong advocates that
we did not have a problem,

411
00:18:04,710 --> 00:18:07,053
the actual people we wanted to persuade.

412
00:18:07,920 --> 00:18:10,740
We asked them to start
looking across the market,

413
00:18:10,740 --> 00:18:13,980
understanding what industry text,

414
00:18:13,980 --> 00:18:15,630
industry research looked like,

415
00:18:15,630 --> 00:18:17,550
doing some internal surveys

416
00:18:17,550 --> 00:18:20,370
and looking at reports
like Gartner and Forrester

417
00:18:20,370 --> 00:18:23,040
to understand what the modern world

418
00:18:23,040 --> 00:18:24,810
of observability looked like.

419
00:18:24,810 --> 00:18:27,180
And then we asked them to
translate all of that data

420
00:18:27,180 --> 00:18:29,190
into a maturity model.

421
00:18:29,190 --> 00:18:31,980
This maturity model rather
handily aligns quite nicely

422
00:18:31,980 --> 00:18:33,723
with Dynatrace's maturity model.

423
00:18:34,680 --> 00:18:36,510
It allowed our teams to grade

424
00:18:36,510 --> 00:18:38,280
where they were in observability,

425
00:18:38,280 --> 00:18:41,013
but with a particular Storio flavor to it.

426
00:18:42,270 --> 00:18:44,400
It was quite quick to identify

427
00:18:44,400 --> 00:18:47,400
that we were operating around
the Level 1, Level 2 position,

428
00:18:47,400 --> 00:18:51,090
that monitoring to base-level
observability space,

429
00:18:51,090 --> 00:18:54,420
when in fact our desire was
to be way up to the Level 3,

430
00:18:54,420 --> 00:18:57,811
if not the Level 4 place in
that causal observability

431
00:18:57,811 --> 00:18:59,883
and proactive observability space.

432
00:19:00,930 --> 00:19:03,270
It also meant that by
building this piece out,

433
00:19:03,270 --> 00:19:05,370
we had a framework, we had a direction,

434
00:19:05,370 --> 00:19:08,280
we knew we wanted to get
to that Level 4 position,

435
00:19:08,280 --> 00:19:10,950
but we knew that we were gonna
have to go through Level 2

436
00:19:10,950 --> 00:19:12,660
and Level 3 to get there.

437
00:19:12,660 --> 00:19:16,173
So we suddenly had a clear
direction to that North Star.

438
00:19:17,790 --> 00:19:21,000
This brings me on to our technology solve.

439
00:19:21,000 --> 00:19:25,410
So I mentioned we were shipping
1.1 billion logs a day.

440
00:19:25,410 --> 00:19:28,620
That was largely coming from
a containerized workload.

441
00:19:28,620 --> 00:19:32,580
We use ECS as our container
orchestration platform.

442
00:19:32,580 --> 00:19:34,470
It's a fairly straightforward solve

443
00:19:34,470 --> 00:19:36,927
when you're looking at shipping
logs from that solution.

444
00:19:36,927 --> 00:19:39,120
AWS provides a product called FireLens,

445
00:19:39,120 --> 00:19:43,110
which is a fantastic
sidecar to your ECS stacks.

446
00:19:43,110 --> 00:19:46,050
It can simply take your existing logs,

447
00:19:46,050 --> 00:19:50,310
and it outputs them using
Fluent Bit as a protocol

448
00:19:50,310 --> 00:19:52,710
that can be ingested
directly by an active gate,

449
00:19:52,710 --> 00:19:57,030
which is Dynatrace's
technology for routing metrics,

450
00:19:57,030 --> 00:19:59,850
logs, and other pieces
into their ecosystem.

451
00:19:59,850 --> 00:20:02,250
And then from there it
can be fed into Grail,

452
00:20:02,250 --> 00:20:05,550
which is Dynatrace's
unified data lakehouse.

453
00:20:05,550 --> 00:20:07,400
Try saying that after a drink or two.

454
00:20:08,610 --> 00:20:10,560
We also had quite a lot
of workload in Lambda;

455
00:20:10,560 --> 00:20:14,400
about 40% of our workload
lives in the Lambda ecosystem.

456
00:20:14,400 --> 00:20:17,010
Again, quite a straightforward solve.

457
00:20:17,010 --> 00:20:19,230
CloudWatch Log groups can be subscribed

458
00:20:19,230 --> 00:20:20,640
into a Kinesis Firehose,

459
00:20:20,640 --> 00:20:23,100
and then that can be
taken off to OpenPipeline,

460
00:20:23,100 --> 00:20:27,090
again, a Dynatrace technology
for pulling material in

461
00:20:27,090 --> 00:20:28,980
and doing transforms on it.

462
00:20:28,980 --> 00:20:31,050
That let us do some preprocessing

463
00:20:31,050 --> 00:20:32,580
and then load that into Grail.

464
00:20:32,580 --> 00:20:35,790
So we've now got all of our
logs from our ECS workload

465
00:20:35,790 --> 00:20:37,860
and all of our logs
from our Lambda workload

466
00:20:37,860 --> 00:20:40,260
flowing nicely into Dynatrace.

467
00:20:40,260 --> 00:20:43,560
Last step was our more
legacy EC2 workload.

468
00:20:43,560 --> 00:20:47,370
We already had an in-place
OTel implementation for that,

469
00:20:47,370 --> 00:20:49,740
and again that was a very
straightforward solve.

470
00:20:49,740 --> 00:20:51,900
We were able to ship
all of that OTel data,

471
00:20:51,900 --> 00:20:54,630
logs and the traces that we had present,

472
00:20:54,630 --> 00:20:57,960
over into Dynatrace, directly
into Dynatrace's API,

473
00:20:57,960 --> 00:21:01,410
and now we've got all
of our 1.1 billion logs

474
00:21:01,410 --> 00:21:04,560
in one singular clear platform.

475
00:21:04,560 --> 00:21:05,493
Simple, right?

476
00:21:06,990 --> 00:21:09,423
This brings us onto pillar number two.

477
00:21:10,410 --> 00:21:11,910
Once we got there,

478
00:21:11,910 --> 00:21:14,127
we started putting our logs into context,

479
00:21:14,127 --> 00:21:16,320
and we immediately started seeing value

480
00:21:16,320 --> 00:21:18,090
from the Dynatrace platform.

481
00:21:18,090 --> 00:21:21,870
We were suddenly seeing events
happening on our platform

482
00:21:21,870 --> 00:21:25,110
being correlated together
in an automagic way.

483
00:21:25,110 --> 00:21:26,703
Brilliant, if you bear in mind,

484
00:21:27,810 --> 00:21:30,457
pastly, we've been doing
that using humans and saying,

485
00:21:30,457 --> 00:21:33,630
"Well, this log and this
log over here correlate."

486
00:21:33,630 --> 00:21:35,700
We're getting that
correlation out of the box

487
00:21:35,700 --> 00:21:38,130
and we're beginning to
understand what's going on.

488
00:21:38,130 --> 00:21:39,870
But it also exposed to us

489
00:21:39,870 --> 00:21:43,050
that we were often using
the wrong signal bearer,

490
00:21:43,050 --> 00:21:44,670
in this particular case, logs,

491
00:21:44,670 --> 00:21:47,340
in situations where something
like another signal bearer

492
00:21:47,340 --> 00:21:49,830
would be a better fit.

493
00:21:49,830 --> 00:21:53,010
A great example is trying
to piece together elements

494
00:21:53,010 --> 00:21:55,800
from a set of calls using logs,

495
00:21:55,800 --> 00:21:57,270
when in fact in reality

496
00:21:57,270 --> 00:22:00,303
you may be able to display
that better using tracers.

497
00:22:02,400 --> 00:22:04,140
Once we had those logs in context,

498
00:22:04,140 --> 00:22:05,107
we could start saying,

499
00:22:05,107 --> 00:22:07,770
"Right, these are the
places in our ecosystem

500
00:22:07,770 --> 00:22:09,510
where this signal does not fit.

501
00:22:09,510 --> 00:22:11,280
Let's replace it with a different signal.

502
00:22:11,280 --> 00:22:13,380
Let's replace this one with some traces.

503
00:22:13,380 --> 00:22:15,840
Let's replace these
ones with some metrics."

504
00:22:15,840 --> 00:22:18,600
And we started to get a richer experience,

505
00:22:18,600 --> 00:22:22,860
as well as simultaneously
decreasing our log volume.

506
00:22:22,860 --> 00:22:25,410
And we started dropping
that 1.1 billion down

507
00:22:25,410 --> 00:22:30,410
into the several hundred
millions and so on downwards.

508
00:22:32,250 --> 00:22:34,740
We also started being able to visualize

509
00:22:34,740 --> 00:22:36,780
for the first time, in a visual way,

510
00:22:36,780 --> 00:22:39,240
these connections between
our distributed services.

511
00:22:39,240 --> 00:22:42,960
So no longer was each
individual distributed service

512
00:22:42,960 --> 00:22:46,350
an isolated little monolith
sat there doing its own thing.

513
00:22:46,350 --> 00:22:48,240
We could see those calls propagating

514
00:22:48,240 --> 00:22:49,623
from service to service.

515
00:22:50,730 --> 00:22:54,060
This set ourselves up
with massive success.

516
00:22:54,060 --> 00:22:56,730
We had this foundation of logs,

517
00:22:56,730 --> 00:22:59,940
but we knew that we would
have to build on it.

518
00:22:59,940 --> 00:23:01,950
And we consciously went out

519
00:23:01,950 --> 00:23:03,930
and made sure that that foundation of logs

520
00:23:03,930 --> 00:23:05,610
was not our end state.

521
00:23:05,610 --> 00:23:09,720
For us, this meant that data unification,

522
00:23:09,720 --> 00:23:10,620
as we like to call it,

523
00:23:10,620 --> 00:23:12,450
moving all of our data into one place

524
00:23:12,450 --> 00:23:16,080
and then picking out the right
signals was a nonnegotiable.

525
00:23:16,080 --> 00:23:19,980
If we wanted to meaningfully
decrease our MTTD and our MTTR,

526
00:23:19,980 --> 00:23:21,430
we had to go on this journey.

527
00:23:24,360 --> 00:23:28,200
Let's talk about AI.

528
00:23:28,200 --> 00:23:29,550
So as we heard earlier,

529
00:23:29,550 --> 00:23:32,400
AI is a core part of
the Dynatrace platform.

530
00:23:32,400 --> 00:23:37,140
Even with our signals now
getting into the millions

531
00:23:37,140 --> 00:23:39,510
rather than the billions territory,

532
00:23:39,510 --> 00:23:41,220
and with all of our noise filtered out

533
00:23:41,220 --> 00:23:42,870
and using the correct barriers,

534
00:23:42,870 --> 00:23:44,820
this would've still
been an impossible task

535
00:23:44,820 --> 00:23:47,523
to look at manually using humans.

536
00:23:48,360 --> 00:23:49,890
We'd known that from day one.

537
00:23:49,890 --> 00:23:51,690
We thought it would be a Sisyphean task.

538
00:23:51,690 --> 00:23:55,380
We would be pushing that
rock up a hill every day

539
00:23:55,380 --> 00:23:58,020
as we work through our
peak trading period.

540
00:23:58,020 --> 00:24:01,350
We knew that for any
system at scale to work,

541
00:24:01,350 --> 00:24:03,150
we needed automation.

542
00:24:03,150 --> 00:24:07,410
And that's what the combination
of predictive and causal AI

543
00:24:07,410 --> 00:24:08,850
that is present within Dynatrace,

544
00:24:08,850 --> 00:24:11,370
the Davis ecosystem, brought to us.

545
00:24:11,370 --> 00:24:15,060
We started to see predictive
bringing in autobaselining.

546
00:24:15,060 --> 00:24:17,010
So we were no longer having to work out

547
00:24:17,010 --> 00:24:19,560
what our signals looked
like from a normal position,

548
00:24:19,560 --> 00:24:21,960
set up manual thresholds,
et cetera, et cetera.

549
00:24:21,960 --> 00:24:24,450
That just happened magically.

550
00:24:24,450 --> 00:24:28,740
We also saw causal AI
starting to understand

551
00:24:28,740 --> 00:24:31,080
the interconnected web
of services we have.

552
00:24:31,080 --> 00:24:32,267
And we've got quite a few microservices

553
00:24:32,267 --> 00:24:34,653
within our ecosystem, over 200.

554
00:24:36,270 --> 00:24:38,130
Manually building out a web

555
00:24:38,130 --> 00:24:40,320
of how these services talk to one another

556
00:24:40,320 --> 00:24:42,030
would've been an impossible task.

557
00:24:42,030 --> 00:24:45,717
But the automated nature of Dynatrace

558
00:24:45,717 --> 00:24:47,970
and Davis operating started connecting

559
00:24:47,970 --> 00:24:50,220
all of these services
together fantastically.

560
00:24:51,420 --> 00:24:52,830
We made a conscious decision

561
00:24:52,830 --> 00:24:57,030
to stop manually setting
up thresholds and alerts

562
00:24:57,030 --> 00:24:57,863
and everything else,

563
00:24:57,863 --> 00:25:01,290
and instead focus on making
the best out of Davis

564
00:25:01,290 --> 00:25:03,150
and the problem cards we could do,

565
00:25:03,150 --> 00:25:05,790
because we knew that that would
be the most impactful thing

566
00:25:05,790 --> 00:25:06,933
in our MTTD.

567
00:25:08,070 --> 00:25:09,630
To give you an example of this,

568
00:25:09,630 --> 00:25:14,490
yesterday, at around about
8:00 PM European time,

569
00:25:15,870 --> 00:25:20,280
we had what would've been
three Severity 1 incidents

570
00:25:20,280 --> 00:25:22,530
start to emerge on our platform,

571
00:25:22,530 --> 00:25:23,930
things we've not considered.

572
00:25:25,470 --> 00:25:26,403
We had no,

573
00:25:27,450 --> 00:25:29,250
as a result of having not considered them,

574
00:25:29,250 --> 00:25:30,990
no ability to have instrumented

575
00:25:30,990 --> 00:25:32,460
and consciously set up thresholds

576
00:25:32,460 --> 00:25:33,540
for checking on these things,

577
00:25:33,540 --> 00:25:35,760
so as we could have
been proactive about it.

578
00:25:35,760 --> 00:25:38,070
But Davis was able to spot them coming,

579
00:25:38,070 --> 00:25:40,140
start raising performance challenges

580
00:25:40,140 --> 00:25:41,370
around a set of services,

581
00:25:41,370 --> 00:25:45,210
and we averted three individual incidents.

582
00:25:45,210 --> 00:25:46,860
Each one of those incidents alone

583
00:25:46,860 --> 00:25:49,830
would've been a 1 1/2 million euro outage.

584
00:25:49,830 --> 00:25:53,850
So in one single fell swoop,

585
00:25:53,850 --> 00:25:56,400
the Dynatrace platform in Davis identified

586
00:25:56,400 --> 00:26:00,870
over 4 1/2 million euros' worth of savings

587
00:26:00,870 --> 00:26:02,013
to us, effectively.

588
00:26:05,070 --> 00:26:07,200
I'm gonna talk you through a scenario

589
00:26:07,200 --> 00:26:08,190
we had a little while ago

590
00:26:08,190 --> 00:26:09,990
that also brings some light to this.

591
00:26:13,110 --> 00:26:15,240
As you may know, in Europe,

592
00:26:15,240 --> 00:26:17,280
each country has different territories

593
00:26:17,280 --> 00:26:19,080
and different VAT rates that apply to it,

594
00:26:19,080 --> 00:26:20,700
in particular when you're doing

595
00:26:20,700 --> 00:26:22,860
territory-to-territory transactions.

596
00:26:22,860 --> 00:26:26,370
We store all of that data
in a giant DynamoDB table.

597
00:26:26,370 --> 00:26:28,020
Not the most elegant of solutions,

598
00:26:28,020 --> 00:26:30,000
but we had to accept imperfections

599
00:26:30,000 --> 00:26:31,750
as we went on our platform journey.

600
00:26:34,770 --> 00:26:36,360
There is also no update method

601
00:26:36,360 --> 00:26:38,370
nor any validation in place

602
00:26:38,370 --> 00:26:40,440
for that particular DynamoDB table.

603
00:26:40,440 --> 00:26:43,190
Again, hold my hands up, not
best engineering practice.

604
00:26:44,400 --> 00:26:47,670
One of our product managers
was meaningfully applying

605
00:26:47,670 --> 00:26:49,650
some updates to our tax rates,

606
00:26:49,650 --> 00:26:51,990
but unfortunately, bit of
finger trouble crept in

607
00:26:51,990 --> 00:26:54,930
and she managed to malform
one of the tax rates

608
00:26:54,930 --> 00:26:56,343
for our German markets.

609
00:26:57,390 --> 00:27:01,260
Davis immediately started
detecting an increase in failures,

610
00:27:01,260 --> 00:27:04,560
of course that DynamoDB table,

611
00:27:04,560 --> 00:27:06,390
and was even able to identify the lines

612
00:27:06,390 --> 00:27:09,333
in that DynamoDB table that
were causing the failure.

613
00:27:11,280 --> 00:27:14,220
We'd just been through a fairly
substantial reorganization,

614
00:27:14,220 --> 00:27:16,110
and the team who actually
owned that service

615
00:27:16,110 --> 00:27:18,900
had only been owning it
for the spell of two weeks.

616
00:27:18,900 --> 00:27:21,570
But they were guided exactly
to where the problem lay.

617
00:27:21,570 --> 00:27:23,400
They understood very immediately

618
00:27:23,400 --> 00:27:25,980
it was related to this DynamoDB table

619
00:27:25,980 --> 00:27:27,960
and a particular change that had happened

620
00:27:27,960 --> 00:27:29,850
within the last 20 minutes,

621
00:27:29,850 --> 00:27:33,240
and they were able to roll back
using point-in-time recovery

622
00:27:33,240 --> 00:27:35,370
to a period before that
change had been made

623
00:27:35,370 --> 00:27:38,130
and restore service within 20 minutes.

624
00:27:38,130 --> 00:27:41,010
Bear in mind we'd done
nothing to build out alerting

625
00:27:41,010 --> 00:27:42,630
for this particular scenario.

626
00:27:42,630 --> 00:27:45,960
So this was all automation
firing off the back

627
00:27:45,960 --> 00:27:50,010
and building out the meaningful
impact that we needed to see

628
00:27:50,010 --> 00:27:53,670
to prevent any impact
happening to our customers.

629
00:27:53,670 --> 00:27:56,010
We were able to solve this
problem within 20 minutes,

630
00:27:56,010 --> 00:27:58,560
and we were really, really
glad of the investment

631
00:27:58,560 --> 00:28:00,543
that we'd made in this space.

632
00:28:02,130 --> 00:28:03,360
There were challenges, though.

633
00:28:03,360 --> 00:28:05,793
We didn't always have some smooth sailing.

634
00:28:07,080 --> 00:28:11,580
We realized that our
existing OTel implementation

635
00:28:11,580 --> 00:28:14,520
combined with Dynatrace left
some things to be desired,

636
00:28:14,520 --> 00:28:17,970
in particular around the
front end to backend space.

637
00:28:17,970 --> 00:28:21,300
That mixed OTel and native ecosystem

638
00:28:21,300 --> 00:28:23,433
is a super hard problem to solve for.

639
00:28:24,660 --> 00:28:27,600
We also found that some of our engineers

640
00:28:27,600 --> 00:28:30,060
who were still in that old-school mindset

641
00:28:30,060 --> 00:28:32,700
of wanting to look at
logs and nothing else

642
00:28:32,700 --> 00:28:35,460
really struggle with our new mindset.

643
00:28:35,460 --> 00:28:37,350
To this day I have two engineers

644
00:28:37,350 --> 00:28:42,000
who have differing views
on the value of logs

645
00:28:42,000 --> 00:28:44,430
versus other signals in our ecosystem.

646
00:28:44,430 --> 00:28:46,110
Each is right and each is valid,

647
00:28:46,110 --> 00:28:48,150
but as we look to move things forward,

648
00:28:48,150 --> 00:28:50,040
the opportunity to use other signals

649
00:28:50,040 --> 00:28:52,653
is one that I think is
really, really valuable.

650
00:28:54,210 --> 00:28:57,540
And lastly, we needed a
major time-bound event

651
00:28:57,540 --> 00:29:00,090
to really bring our organization aligned

652
00:29:00,090 --> 00:29:01,500
around that single goal.

653
00:29:01,500 --> 00:29:03,180
For us, it's our peak trading period,

654
00:29:03,180 --> 00:29:05,940
this run-up to the Black Friday weekend,

655
00:29:05,940 --> 00:29:08,730
and then the run-up to the holiday period.

656
00:29:08,730 --> 00:29:12,390
It laser-focused everybody in
on making a meaningful change.

657
00:29:12,390 --> 00:29:14,280
And we've seen a massive acceleration

658
00:29:14,280 --> 00:29:15,600
over these last couple of months,

659
00:29:15,600 --> 00:29:17,970
since I started writing these slides,

660
00:29:17,970 --> 00:29:19,470
to getting to a point

661
00:29:19,470 --> 00:29:22,353
of having massively great
observability coverage.

662
00:29:24,330 --> 00:29:26,680
So let me talk about some
of our key takeaways.

663
00:29:28,050 --> 00:29:31,530
First of all, our change
had to be culture first,

664
00:29:31,530 --> 00:29:32,820
code second.

665
00:29:32,820 --> 00:29:35,190
Leading with the culture piece

666
00:29:35,190 --> 00:29:38,280
is what has enabled us to
be powerful in our change.

667
00:29:38,280 --> 00:29:40,800
If we'd have tried to introduce
the technology solution

668
00:29:40,800 --> 00:29:42,600
or a process-driven solution first,

669
00:29:42,600 --> 00:29:44,580
we would've failed on this journey.

670
00:29:44,580 --> 00:29:46,410
We had to change our culture

671
00:29:46,410 --> 00:29:49,953
before we could meaningfully
make improvement.

672
00:29:50,790 --> 00:29:54,060
We also had to embrace the
idea of using other signals.

673
00:29:54,060 --> 00:29:56,040
If we only stuck with our logs world,

674
00:29:56,040 --> 00:29:59,220
we would be having a better experience,

675
00:29:59,220 --> 00:30:01,380
but we would be missing so much value

676
00:30:01,380 --> 00:30:03,813
that a modern observability
platform provides.

677
00:30:04,770 --> 00:30:07,770
And that really, when you
get to a certain scale,

678
00:30:07,770 --> 00:30:09,330
you need to use AI.

679
00:30:09,330 --> 00:30:12,120
You can no longer rely on the
humans in your organization

680
00:30:12,120 --> 00:30:14,040
to do the great job that they have done,

681
00:30:14,040 --> 00:30:16,110
because there will be so much data;

682
00:30:16,110 --> 00:30:17,550
even with the best will in the world,

683
00:30:17,550 --> 00:30:19,470
they will not be able to keep up.

684
00:30:19,470 --> 00:30:22,530
AI is essential to enable
engineering productivity.

685
00:30:22,530 --> 00:30:24,270
You do not want your engineers

686
00:30:24,270 --> 00:30:25,980
spending their time reading logs.

687
00:30:25,980 --> 00:30:28,500
You want them there writing code.

688
00:30:28,500 --> 00:30:31,620
And for that you need AI to help you

689
00:30:31,620 --> 00:30:33,183
with that problem of analysis.

690
00:30:35,070 --> 00:30:37,923
Okay, I think I am handing
back over to Jon now.

691
00:30:40,920 --> 00:30:41,913
- Thank you, Alex.

692
00:30:43,620 --> 00:30:45,600
So hopefully we all found
that very interesting.

693
00:30:45,600 --> 00:30:46,560
It was a very good story,

694
00:30:46,560 --> 00:30:48,270
and obviously some very good examples

695
00:30:48,270 --> 00:30:50,910
about how log analytics

696
00:30:50,910 --> 00:30:54,180
and the wider sort of
observability ecosystem

697
00:30:54,180 --> 00:30:56,943
can empower things if
done in the right way.

698
00:30:58,020 --> 00:31:01,770
Now, what we want to sort
of start delving into

699
00:31:01,770 --> 00:31:03,270
and looking at a little bit more here,

700
00:31:03,270 --> 00:31:05,723
so I just realized there's
a squeaky footboard here,

701
00:31:06,720 --> 00:31:10,530
is the traditional log
management side of things

702
00:31:10,530 --> 00:31:13,020
really need reinvention.

703
00:31:13,020 --> 00:31:16,620
It doesn't work in the way
that it needs to nowadays.

704
00:31:16,620 --> 00:31:18,420
things are scaling too much,

705
00:31:18,420 --> 00:31:21,060
the ecosystem getting too complex;

706
00:31:21,060 --> 00:31:24,330
and the more and more
complex your ecosystems get,

707
00:31:24,330 --> 00:31:26,310
the more data they emit.

708
00:31:26,310 --> 00:31:29,610
And obviously, Alex touched on
various of these points here,

709
00:31:29,610 --> 00:31:31,380
but obviously some of them,

710
00:31:31,380 --> 00:31:33,483
with things like inflated costs,

711
00:31:34,440 --> 00:31:37,320
you are having that incredible
amount of technical debt

712
00:31:37,320 --> 00:31:40,410
with these different traditional
log management tooling

713
00:31:40,410 --> 00:31:41,670
that's out there;

714
00:31:41,670 --> 00:31:44,130
not just in terms of
things like licensing,

715
00:31:44,130 --> 00:31:46,980
but more in terms of things
like management and maintenance,

716
00:31:46,980 --> 00:31:48,810
whether that's off the
platforms themselves

717
00:31:48,810 --> 00:31:51,030
or whether that's the tens
of thousands of dashboards

718
00:31:51,030 --> 00:31:53,160
that you're potentially
setting up and looking at.

719
00:31:53,160 --> 00:31:55,290
And obviously there
are thousands of alerts

720
00:31:55,290 --> 00:31:57,450
that effectively create the alert storms

721
00:31:57,450 --> 00:32:00,540
that you were talking about there as well.

722
00:32:00,540 --> 00:32:01,373
Now,

723
00:32:02,430 --> 00:32:06,000
with all of that, it then
comes onto that correlation,

724
00:32:06,000 --> 00:32:07,770
that manual correlation
that you're having to do

725
00:32:07,770 --> 00:32:10,920
to connect these different siloed signals

726
00:32:10,920 --> 00:32:13,200
from different, different tools,

727
00:32:13,200 --> 00:32:14,340
to bring those things together,

728
00:32:14,340 --> 00:32:16,710
to try and work out
where that root cause is.

729
00:32:16,710 --> 00:32:19,680
And again, that of course
has that knock-on impact

730
00:32:19,680 --> 00:32:22,830
to a worse mean time to resolution

731
00:32:22,830 --> 00:32:24,720
and overall worse collaboration

732
00:32:24,720 --> 00:32:26,610
that's taking place between
the different teams,

733
00:32:26,610 --> 00:32:29,253
'cause everybody has
their own source of truth.

734
00:32:30,660 --> 00:32:32,370
And finally, of course,

735
00:32:32,370 --> 00:32:35,013
that brings you onto the
high-complexity side,

736
00:32:35,970 --> 00:32:39,270
because overall the traditional
log management tooling

737
00:32:39,270 --> 00:32:41,460
obviously has been around for
a very long space of time.

738
00:32:41,460 --> 00:32:44,370
It is built in reality for the past.

739
00:32:44,370 --> 00:32:46,800
It doesn't scale in the way that is needed

740
00:32:46,800 --> 00:32:49,800
for the current world with microservices,

741
00:32:49,800 --> 00:32:51,330
with AI agents and so on,

742
00:32:51,330 --> 00:32:55,080
where it is dynamic, it
is massively scalable,

743
00:32:55,080 --> 00:32:56,943
and it is cloud-native.

744
00:32:57,810 --> 00:32:59,130
Because with all of those

745
00:32:59,130 --> 00:33:01,440
you need to understand how they relate.

746
00:33:01,440 --> 00:33:03,690
You need to understand
all the different signals,

747
00:33:03,690 --> 00:33:07,860
and unfortunately logs by
itself isn't effective.

748
00:33:07,860 --> 00:33:09,960
Logs as part of a much wider piece

749
00:33:09,960 --> 00:33:11,310
with all of the other signals

750
00:33:11,310 --> 00:33:13,623
is where things can be
most effectively used.

751
00:33:14,640 --> 00:33:17,310
So when you bring those things together,

752
00:33:17,310 --> 00:33:20,370
you can start to use
the appropriate signals

753
00:33:20,370 --> 00:33:22,200
for their appropriate means

754
00:33:22,200 --> 00:33:24,813
and utilize them all in the right way.

755
00:33:25,890 --> 00:33:29,070
So with Dynatrace, what we're trying to do

756
00:33:29,070 --> 00:33:33,450
is help you scale that cost effectively,

757
00:33:33,450 --> 00:33:36,630
to unify and contextualize all that data,

758
00:33:36,630 --> 00:33:39,270
whether that's the log
data with the topology,

759
00:33:39,270 --> 00:33:41,340
with the metrics, the events.

760
00:33:41,340 --> 00:33:42,173
You know, again,

761
00:33:42,173 --> 00:33:44,130
whether that's the security
side of things as well,

762
00:33:44,130 --> 00:33:47,610
bringing everything together
so you can understand that

763
00:33:47,610 --> 00:33:49,920
all in a single place.

764
00:33:49,920 --> 00:33:53,010
But more importantly, to
enable that proactiveness,

765
00:33:53,010 --> 00:33:56,370
enable AI-driven decisions to be made

766
00:33:56,370 --> 00:33:58,203
and automation to take place.

767
00:34:00,120 --> 00:34:02,910
So really what we're looking at here

768
00:34:02,910 --> 00:34:07,020
is that we all know that the applications,

769
00:34:07,020 --> 00:34:08,250
your cloud environments and so on,

770
00:34:08,250 --> 00:34:11,820
are getting bigger and more complex.

771
00:34:11,820 --> 00:34:14,010
But with that explosion of data,

772
00:34:14,010 --> 00:34:17,640
with the petabytes of logs,
metrics, traces and so on,

773
00:34:17,640 --> 00:34:20,490
it then starts to be a question
of what can you do with it?

774
00:34:20,490 --> 00:34:22,560
Where is the power with that?

775
00:34:22,560 --> 00:34:25,020
And with automation and AI,

776
00:34:25,020 --> 00:34:27,060
it's really trying to utilize that data,

777
00:34:27,060 --> 00:34:29,380
not as effectively noise

778
00:34:30,232 --> 00:34:33,210
or a burden of you having
to work on all of that

779
00:34:33,210 --> 00:34:35,100
to understand what's
going wrong and so on,

780
00:34:35,100 --> 00:34:38,250
but actually trying to
drive automation around it,

781
00:34:38,250 --> 00:34:41,220
actually trying to drive
preventative operations,

782
00:34:41,220 --> 00:34:44,010
empower your teams with the
automation on the back of that,

783
00:34:44,010 --> 00:34:46,530
so that really you can turn your data,

784
00:34:46,530 --> 00:34:49,050
your petabytes and
petabytes of information

785
00:34:49,050 --> 00:34:52,173
really into a business advantage,
a competitive advantage.

786
00:34:54,660 --> 00:34:59,070
And so when we're trying to
unify the data across the board,

787
00:34:59,070 --> 00:35:02,790
unify the logs with the rest
of the observability signals,

788
00:35:02,790 --> 00:35:04,470
with Dynatrace, we
obviously try and make that

789
00:35:04,470 --> 00:35:07,680
as simple and as automated as possible.

790
00:35:07,680 --> 00:35:10,860
One of our primary ways of
course is our Dynatrace OneAgent

791
00:35:10,860 --> 00:35:12,660
which will automatically detect the logs

792
00:35:12,660 --> 00:35:15,390
that you're say operating systems
and applications and so on

793
00:35:15,390 --> 00:35:16,890
are writing out,

794
00:35:16,890 --> 00:35:18,960
be able to automatically ingest those.

795
00:35:18,960 --> 00:35:21,810
But of course it's not just
places you've got OneAgent;

796
00:35:21,810 --> 00:35:23,310
maybe you've got OpenTelemetry

797
00:35:23,310 --> 00:35:25,110
and you've got your
OpenTelemetry logs coming in.

798
00:35:25,110 --> 00:35:26,430
Maybe you've got log shippers

799
00:35:26,430 --> 00:35:28,533
like Fluent Bit and Fluentd and so on.

800
00:35:29,880 --> 00:35:32,340
And bringing all that information in

801
00:35:32,340 --> 00:35:33,720
in whichever way is relevant,

802
00:35:33,720 --> 00:35:36,660
and of course the core one
here as well being, you know,

803
00:35:36,660 --> 00:35:39,090
that direct integration with AWS,

804
00:35:39,090 --> 00:35:41,850
getting all that log information from AWS

805
00:35:41,850 --> 00:35:43,470
through the Firehose.

806
00:35:43,470 --> 00:35:46,080
So again, you can understand
and bring the data in

807
00:35:46,080 --> 00:35:48,120
no matter where it's sitting.

808
00:35:48,120 --> 00:35:51,240
And again, between the
combination of your cloud estates

809
00:35:51,240 --> 00:35:52,320
and your on-premise.

810
00:35:52,320 --> 00:35:54,570
So really, you are
getting that unified view.

811
00:35:55,890 --> 00:35:58,380
Now, what we're then doing

812
00:35:58,380 --> 00:36:01,590
is we're effectively storing
that in our central storage

813
00:36:01,590 --> 00:36:02,670
which we call Grail.

814
00:36:02,670 --> 00:36:06,150
So that's effectively specifically built

815
00:36:06,150 --> 00:36:08,850
for observability, data lakehouse.

816
00:36:08,850 --> 00:36:11,100
And this does allow you to ingest

817
00:36:11,100 --> 00:36:13,590
over a petabyte of
information every single day

818
00:36:13,590 --> 00:36:17,310
into your environments
and have that full power.

819
00:36:17,310 --> 00:36:19,860
Now, what you're able to
do with this, as I said,

820
00:36:19,860 --> 00:36:22,680
is ensure that all data
is stored in there.

821
00:36:22,680 --> 00:36:24,840
You know, whether that's
your logs, metrics, traces,

822
00:36:24,840 --> 00:36:27,270
real user monitoring, whatever it may be,

823
00:36:27,270 --> 00:36:29,370
all stored in context with one another.

824
00:36:29,370 --> 00:36:32,460
Whether it's your log
records and your traces,

825
00:36:32,460 --> 00:36:35,190
we know exactly which trace
has written which log record.

826
00:36:35,190 --> 00:36:36,840
So you're not having to
look through log files

827
00:36:36,840 --> 00:36:38,640
and just look at endless log files,

828
00:36:38,640 --> 00:36:40,380
not knowing what connects to what;

829
00:36:40,380 --> 00:36:42,210
you know exactly which
log record was written

830
00:36:42,210 --> 00:36:44,640
by which trace, which was
connected to which user

831
00:36:44,640 --> 00:36:46,383
in their browser or mobile app.

832
00:36:47,820 --> 00:36:50,010
And we're trying to do this, as I said,

833
00:36:50,010 --> 00:36:51,730
in the most automated way possible

834
00:36:52,590 --> 00:36:55,230
and giving you as much
access as possible as well.

835
00:36:55,230 --> 00:36:57,840
So there's no data storage
tiers for you to manage.

836
00:36:57,840 --> 00:36:59,250
There's no hot and cold storage,

837
00:36:59,250 --> 00:37:01,410
there's no rehydration of data.

838
00:37:01,410 --> 00:37:03,750
You can parse literally the data on read.

839
00:37:03,750 --> 00:37:06,510
So there's no semantics that
you have to previously set up.

840
00:37:06,510 --> 00:37:08,970
There's no indexing and so on and so forth

841
00:37:08,970 --> 00:37:10,800
that you have to set up beforehand.

842
00:37:10,800 --> 00:37:12,180
You can parse that on read.

843
00:37:12,180 --> 00:37:13,890
And what that ends up meaning

844
00:37:13,890 --> 00:37:17,070
is that no matter your
question, no matter what it is,

845
00:37:17,070 --> 00:37:18,690
no matter whether you've
thought of it beforehand,

846
00:37:18,690 --> 00:37:20,490
you can always ask that question.

847
00:37:20,490 --> 00:37:22,680
You don't have to rehydrate data.

848
00:37:22,680 --> 00:37:24,960
You don't have to index
data in appropriate ways

849
00:37:24,960 --> 00:37:26,760
to ask those queries.

850
00:37:26,760 --> 00:37:28,170
You can ask whatever you want,

851
00:37:28,170 --> 00:37:31,080
because you can parse it
on read in near real time.

852
00:37:31,080 --> 00:37:35,190
So really have responsive query

853
00:37:35,190 --> 00:37:36,690
even when you are
looking at your terabytes

854
00:37:36,690 --> 00:37:38,343
or petabytes of information.

855
00:37:39,570 --> 00:37:42,720
And as you can see here, you know,

856
00:37:42,720 --> 00:37:46,200
we are able to do that at that
low ingest cost and retention

857
00:37:46,200 --> 00:37:48,630
and still be able to
retain data up to 10 years.

858
00:37:48,630 --> 00:37:51,480
All of that types of data you
can retain up to 10 years.

859
00:37:51,480 --> 00:37:54,900
So whether that's, you know,
for regulatory standpoints,

860
00:37:54,900 --> 00:37:56,264
you know, with things like

861
00:37:56,264 --> 00:37:57,630
obviously some of the AI regulations

862
00:37:57,630 --> 00:37:59,490
that have been coming
out across the world,

863
00:37:59,490 --> 00:38:01,230
and, you know, obviously some
of the financial services,

864
00:38:01,230 --> 00:38:03,960
especially regulations,
there is a lot of areas

865
00:38:03,960 --> 00:38:06,330
where things like your audit logs

866
00:38:06,330 --> 00:38:09,780
or, you know, your
generative AI connections

867
00:38:09,780 --> 00:38:12,360
of your inputs and outputs and so on.

868
00:38:12,360 --> 00:38:14,040
Understanding those and storing those

869
00:38:14,040 --> 00:38:16,260
may be needed for that space of time.

870
00:38:16,260 --> 00:38:18,460
And again, maybe quite
effective and useful.

871
00:38:22,620 --> 00:38:24,120
But what do we do with all that data?

872
00:38:24,120 --> 00:38:25,080
You know, once it's there,

873
00:38:25,080 --> 00:38:26,730
it's not just about being able to view it

874
00:38:26,730 --> 00:38:28,020
and show it dashboards;

875
00:38:28,020 --> 00:38:31,350
really, this is where the power
of the AI really comes in.

876
00:38:31,350 --> 00:38:33,420
So what we're looking at here

877
00:38:33,420 --> 00:38:36,930
is AI is built into Dynatrace's core

878
00:38:36,930 --> 00:38:39,150
and has been from the beginning.

879
00:38:39,150 --> 00:38:41,790
And we are automatically mapping out

880
00:38:41,790 --> 00:38:43,470
all of that topology information.

881
00:38:43,470 --> 00:38:45,570
We are automatically mapping
out all your different services

882
00:38:45,570 --> 00:38:47,880
and dependencies and everything together.

883
00:38:47,880 --> 00:38:50,610
And all of those telemetry signals

884
00:38:50,610 --> 00:38:53,523
are all mapped against those
topology components as well.

885
00:38:54,480 --> 00:38:56,790
Now, what this means is
Dynatrace is then able to,

886
00:38:56,790 --> 00:39:00,060
on top of all of that,
automatically baseline

887
00:39:00,060 --> 00:39:02,490
and dynamically baseline as well,

888
00:39:02,490 --> 00:39:04,860
full understanding to
automatically detect problems

889
00:39:04,860 --> 00:39:05,910
that are taking place.

890
00:39:05,910 --> 00:39:07,170
Because, at the end of the day,

891
00:39:07,170 --> 00:39:09,000
you don't know what you don't know.

892
00:39:09,000 --> 00:39:11,430
So you can't be setting up
or needing to set up alerts

893
00:39:11,430 --> 00:39:14,610
for every single thing that
could possibly go wrong.

894
00:39:14,610 --> 00:39:17,160
You need to just be told
when something goes wrong.

895
00:39:18,630 --> 00:39:19,950
And then on top of that,

896
00:39:19,950 --> 00:39:22,260
because of that accuracy of the data,

897
00:39:22,260 --> 00:39:23,700
because of that contextualization,

898
00:39:23,700 --> 00:39:25,350
that domain-level insights,

899
00:39:25,350 --> 00:39:29,850
we're also able to provide
precise root cause analysis.

900
00:39:29,850 --> 00:39:32,580
So effectively doing
fault tree-style analysis

901
00:39:32,580 --> 00:39:36,300
so that you are able to get
accurate root cause analysis

902
00:39:36,300 --> 00:39:38,340
rather than effectively
trying to correlate

903
00:39:38,340 --> 00:39:39,640
different events together.

904
00:39:40,620 --> 00:39:42,870
Now, what this means is you're
not getting the alert storms;

905
00:39:42,870 --> 00:39:44,700
you're getting the full insights you need,

906
00:39:44,700 --> 00:39:46,920
you're getting straight to the pinpoint

907
00:39:46,920 --> 00:39:48,930
to be able to resolve those things,

908
00:39:48,930 --> 00:39:52,530
but you're also able to prioritize
these on business impact.

909
00:39:52,530 --> 00:39:54,240
You know, how many users are impacted,

910
00:39:54,240 --> 00:39:55,890
how many business flows, or, you know,

911
00:39:55,890 --> 00:39:56,970
critical business services

912
00:39:56,970 --> 00:39:59,253
are actually impacted by this problem.

913
00:40:00,952 --> 00:40:05,070
And because you've got
accurate root cause,

914
00:40:05,070 --> 00:40:07,530
it also means you can do self-healing.

915
00:40:07,530 --> 00:40:11,100
The issue with self-healing on
let's say general correlation

916
00:40:11,100 --> 00:40:14,670
is always that with the
inherent false positives

917
00:40:14,670 --> 00:40:16,980
that come in, if you try and automate

918
00:40:16,980 --> 00:40:19,350
or remediate on a false positive,

919
00:40:19,350 --> 00:40:21,870
you're going to make the
situation a lot worse.

920
00:40:21,870 --> 00:40:24,600
Most of the time you'll
scale out your AWS instances

921
00:40:24,600 --> 00:40:28,470
and give AWS a lot of
money to pay for that.

922
00:40:28,470 --> 00:40:31,050
But overall it could be a matter

923
00:40:31,050 --> 00:40:32,460
that you are restarting services

924
00:40:32,460 --> 00:40:33,570
that didn't need restarting.

925
00:40:33,570 --> 00:40:35,070
It could mean all sorts
of different things,

926
00:40:35,070 --> 00:40:36,450
where, at the end of the day,

927
00:40:36,450 --> 00:40:38,940
your customers are getting
the worst scenario,

928
00:40:38,940 --> 00:40:42,030
and probably your costs
are also increasing too.

929
00:40:42,030 --> 00:40:43,710
If you know exactly what went wrong,

930
00:40:43,710 --> 00:40:45,660
you can fix the root cause

931
00:40:45,660 --> 00:40:47,490
rather than trying to
fix the various symptoms

932
00:40:47,490 --> 00:40:51,093
that often end up bubbling
up in these alert storms.

933
00:40:55,830 --> 00:40:58,770
And as you'll see through
through these as well,

934
00:40:58,770 --> 00:41:00,900
there is an example here
of a problem taking place

935
00:41:00,900 --> 00:41:03,000
where there is a failure rate increase.

936
00:41:03,000 --> 00:41:08,000
Now, with this, we are
able to have the Davis AI,

937
00:41:08,070 --> 00:41:10,560
the generative AI part, the Davis Copilot

938
00:41:10,560 --> 00:41:12,240
actually explain to you what happened

939
00:41:12,240 --> 00:41:13,350
as part of that problem.

940
00:41:13,350 --> 00:41:15,090
So it's not just a matter of giving you

941
00:41:15,090 --> 00:41:16,350
all this technical information

942
00:41:16,350 --> 00:41:18,030
and you working out what it is,

943
00:41:18,030 --> 00:41:19,860
you're actually getting a
summary of that problem:

944
00:41:19,860 --> 00:41:24,000
what happened, what took
place, why did it take place?

945
00:41:24,000 --> 00:41:25,530
And an actionable set of steps

946
00:41:25,530 --> 00:41:27,090
to actually go ahead and remediate that,

947
00:41:27,090 --> 00:41:30,000
some suggestions on how to remediate that.

948
00:41:30,000 --> 00:41:31,940
Now, what that means of course

949
00:41:31,940 --> 00:41:35,550
is that you don't have to go
and make those reports quickly

950
00:41:35,550 --> 00:41:36,840
when a major incident happens.

951
00:41:36,840 --> 00:41:39,540
You know, you've got that
natural language interpretation

952
00:41:39,540 --> 00:41:41,190
that can easily be put out

953
00:41:41,190 --> 00:41:43,410
so that management and
whoever else is aware

954
00:41:43,410 --> 00:41:44,523
of what's going on.

955
00:41:45,480 --> 00:41:48,810
It also allows you to obviously
focus on the right areas,

956
00:41:48,810 --> 00:41:51,423
in the right places to delve into that.

957
00:41:52,470 --> 00:41:56,373
Now, in this case, it
was a recent deployment,

958
00:41:57,840 --> 00:41:59,940
and so the suggestions here

959
00:41:59,940 --> 00:42:02,040
is basically looking at
reviewing the error logs

960
00:42:02,040 --> 00:42:03,663
and various things like that.

961
00:42:04,830 --> 00:42:07,410
But what it's then able to do with that

962
00:42:07,410 --> 00:42:10,800
is because we've got that
direct access to the logs,

963
00:42:10,800 --> 00:42:14,280
we know exactly which logs
are relevant to that problem,

964
00:42:14,280 --> 00:42:15,570
you know, it's not here's every log

965
00:42:15,570 --> 00:42:17,820
that's relevant to these servers,

966
00:42:17,820 --> 00:42:20,670
it is exactly the logs that
are relevant to this problem,

967
00:42:21,690 --> 00:42:24,390
the logs that were specifically
part of that trace.

968
00:42:24,390 --> 00:42:26,100
'Cause the thing that you'll often have

969
00:42:26,100 --> 00:42:29,670
is you may look at a log file
that you know is relevant,

970
00:42:29,670 --> 00:42:31,770
lots of error messages in there,

971
00:42:31,770 --> 00:42:35,370
but which ones are actually
relevant to the transactions

972
00:42:35,370 --> 00:42:37,500
that are failing, that are
actually having an issue.

973
00:42:37,500 --> 00:42:39,270
'Cause error logs happen all the time,

974
00:42:39,270 --> 00:42:41,520
and quite often you can
get that mismatching

975
00:42:41,520 --> 00:42:42,930
of there are error logs,

976
00:42:42,930 --> 00:42:45,300
I'm assuming that
they're gonna be relevant

977
00:42:45,300 --> 00:42:47,100
to the problem that I'm looking at.

978
00:42:47,100 --> 00:42:48,660
And in many times it's not,

979
00:42:48,660 --> 00:42:51,660
and it leads you down the wrong
sort of rabbit hole as such.

980
00:42:52,710 --> 00:42:55,803
Now, what we can effectively
do with that though is,

981
00:42:56,640 --> 00:42:59,073
as I said, you've got the
visibility into all of the logs

982
00:42:59,073 --> 00:43:01,890
that have taken place
as part of that problem,

983
00:43:01,890 --> 00:43:05,370
and we can start to also
explain those logs as well.

984
00:43:05,370 --> 00:43:09,090
So again, having Dynatrace
automatically tell you

985
00:43:09,090 --> 00:43:11,820
what that log record
actually means, you know?

986
00:43:11,820 --> 00:43:14,220
I'm sure all of you have
seen log lines before.

987
00:43:14,220 --> 00:43:15,420
They vary massively.

988
00:43:15,420 --> 00:43:18,363
Some of them are the
messiest things possible.

989
00:43:19,800 --> 00:43:21,800
But it is able to extract out all

990
00:43:21,800 --> 00:43:23,850
of the relevant information that's useful,

991
00:43:23,850 --> 00:43:26,880
break it all down for you so
you haven't had to preparse it

992
00:43:26,880 --> 00:43:30,150
and preconfigure it and
whatever else it may be;

993
00:43:30,150 --> 00:43:35,150
you're able to be told exactly
what the key information is,

994
00:43:35,190 --> 00:43:38,070
what the possible root cause is for that,

995
00:43:38,070 --> 00:43:40,080
and of course, you know,
any actionable steps

996
00:43:40,080 --> 00:43:42,300
to remediate the specifics of that.

997
00:43:42,300 --> 00:43:44,850
Because this is where
logs are really good.

998
00:43:44,850 --> 00:43:47,340
They're good at providing context.

999
00:43:47,340 --> 00:43:49,680
They're not very good from an
observability point of view

1000
00:43:49,680 --> 00:43:52,050
of creating alerts and creating problems

1001
00:43:52,050 --> 00:43:53,910
because there is so much noise in logs.

1002
00:43:53,910 --> 00:43:56,160
But they are very good
at providing context

1003
00:43:56,160 --> 00:43:57,840
and providing you good insights

1004
00:43:57,840 --> 00:43:59,990
to actually go ahead
and start remediation.

1005
00:44:03,510 --> 00:44:07,170
And finally, on this piece as well,

1006
00:44:07,170 --> 00:44:08,610
the other thing that Dynatrace is doing

1007
00:44:08,610 --> 00:44:13,290
is we're also providing this
idea of troubleshooting guides.

1008
00:44:13,290 --> 00:44:14,123
So you can go ahead

1009
00:44:14,123 --> 00:44:15,540
and you can create a troubleshooting guide

1010
00:44:15,540 --> 00:44:17,460
based on that problem.

1011
00:44:17,460 --> 00:44:19,710
Obviously, you do all your
inputs and your queries

1012
00:44:19,710 --> 00:44:21,840
and whatever else it may be.

1013
00:44:21,840 --> 00:44:24,180
But the benefit of what's
taking place on this

1014
00:44:24,180 --> 00:44:26,610
is that when you have a problem

1015
00:44:26,610 --> 00:44:28,923
that occurs in the future that is similar,

1016
00:44:29,790 --> 00:44:31,950
Dynatrace will automatically recommend

1017
00:44:31,950 --> 00:44:34,440
the previous troubleshooting
guide that you created.

1018
00:44:34,440 --> 00:44:36,720
Because it's looking at the
similarity of those problems

1019
00:44:36,720 --> 00:44:38,400
and basically providing you

1020
00:44:38,400 --> 00:44:39,720
with the previous troubleshooting guide

1021
00:44:39,720 --> 00:44:41,700
so you're not having
to repeat those steps.

1022
00:44:41,700 --> 00:44:43,830
Again, it's really, as I said,

1023
00:44:43,830 --> 00:44:45,870
trying to maximize
automation wherever possible,

1024
00:44:45,870 --> 00:44:47,850
trying to reduce that manual effort,

1025
00:44:47,850 --> 00:44:50,730
that time it takes you
to resolve incidents,

1026
00:44:50,730 --> 00:44:52,410
and so you can get to the root cause

1027
00:44:52,410 --> 00:44:54,310
and resolve it as quickly as possible.

1028
00:44:56,850 --> 00:44:59,820
But it's not just about
fighting those fires.

1029
00:44:59,820 --> 00:45:02,250
You also want to look
at of course predicting

1030
00:45:02,250 --> 00:45:03,480
and preventing those fires

1031
00:45:03,480 --> 00:45:05,700
from taking place in the first place.

1032
00:45:05,700 --> 00:45:09,240
So when we look at
Dynatrace's predictive AI,

1033
00:45:09,240 --> 00:45:13,050
it can forecast on any
time-series data at all

1034
00:45:13,050 --> 00:45:13,883
that Dynatrace has.

1035
00:45:13,883 --> 00:45:15,120
That includes logs.

1036
00:45:15,120 --> 00:45:16,800
So you can literally do prediction

1037
00:45:16,800 --> 00:45:19,620
based on your log records directly.

1038
00:45:19,620 --> 00:45:22,340
Now, what this means is
you can start to understand

1039
00:45:22,340 --> 00:45:24,810
if things are trending
in the wrong direction,

1040
00:45:24,810 --> 00:45:25,920
and you can start to fix issues

1041
00:45:25,920 --> 00:45:29,163
before they actually have
any impact to your end users.

1042
00:45:30,000 --> 00:45:34,260
Now, we can effectively
start to drive automation

1043
00:45:34,260 --> 00:45:37,770
based on that, from the various
logs and events and so on.

1044
00:45:37,770 --> 00:45:39,690
So if Dynatrace detects, in this case,

1045
00:45:39,690 --> 00:45:41,310
that there's S3 buckets

1046
00:45:41,310 --> 00:45:43,470
that are open to the public internet,

1047
00:45:43,470 --> 00:45:45,517
we can of course
automatically remediate that.

1048
00:45:45,517 --> 00:45:49,680
You know, we can use AWS's
action APIs effectively

1049
00:45:49,680 --> 00:45:53,430
to auto-remediate and
change those S3 buckets

1050
00:45:53,430 --> 00:45:55,350
so that you can prevent
that sensitive data

1051
00:45:55,350 --> 00:45:56,733
before it ever gets leaked.

1052
00:45:58,050 --> 00:46:01,920
And we can also do things
like proactively scaling.

1053
00:46:01,920 --> 00:46:02,880
You know, whether you're scaling up

1054
00:46:02,880 --> 00:46:04,980
to increase capacity of
your disk, your memory,

1055
00:46:04,980 --> 00:46:07,440
your CPU, whatever it may be,

1056
00:46:07,440 --> 00:46:10,200
or if the logs are indicating

1057
00:46:10,200 --> 00:46:12,180
that there's a business-specific problem

1058
00:46:12,180 --> 00:46:13,890
or error that are taking place

1059
00:46:13,890 --> 00:46:18,000
due to maybe it's resource
saturation or something else,

1060
00:46:18,000 --> 00:46:22,110
we can start to again
auto-remediate, auto-optimize,

1061
00:46:22,110 --> 00:46:24,690
and auto-improve what is
taking place as part of this,

1062
00:46:24,690 --> 00:46:28,980
and overall drive this automation
and predictive insights

1063
00:46:28,980 --> 00:46:31,650
to really provide the best for you,

1064
00:46:31,650 --> 00:46:35,460
to prevent those fires from
taking place in the first place.

1065
00:46:35,460 --> 00:46:39,570
And overall, what we're getting to here

1066
00:46:39,570 --> 00:46:43,230
is that obviously logs quite
often are very technical,

1067
00:46:43,230 --> 00:46:45,030
utilized for a lot of different things.

1068
00:46:45,030 --> 00:46:48,300
But one of the things we
are seeing a lot more power

1069
00:46:48,300 --> 00:46:49,560
and a lot more benefit with logs

1070
00:46:49,560 --> 00:46:52,467
is because they have so much rich context

1071
00:46:52,467 --> 00:46:54,510
and so much rich information,

1072
00:46:54,510 --> 00:46:56,370
they're quite often powerful

1073
00:46:56,370 --> 00:46:58,470
from a business data perspective.

1074
00:46:58,470 --> 00:47:01,650
And the ability to have that information

1075
00:47:01,650 --> 00:47:04,380
directly connected to your IT information

1076
00:47:04,380 --> 00:47:06,450
is also very powerful.

1077
00:47:06,450 --> 00:47:08,370
Because of course it means that
you can start to understand,

1078
00:47:08,370 --> 00:47:10,560
like Alex was talking about earlier,

1079
00:47:10,560 --> 00:47:13,860
about how you can understand
the revenue impact

1080
00:47:13,860 --> 00:47:17,130
of these incidents or problems
that are taking place.

1081
00:47:17,130 --> 00:47:19,920
You can start to understand in real time

1082
00:47:19,920 --> 00:47:21,420
that actual visibility,

1083
00:47:21,420 --> 00:47:24,690
to understand your entire
end-to-end processes, you know,

1084
00:47:24,690 --> 00:47:26,940
your end-to-end journeys
that customers go through

1085
00:47:26,940 --> 00:47:28,950
and where that impact is taking place,

1086
00:47:28,950 --> 00:47:32,400
where you are having these
significant issues and so on.

1087
00:47:32,400 --> 00:47:36,480
And this has all been
provided because, as I said,

1088
00:47:36,480 --> 00:47:38,610
all this information is
being topology-mapped,

1089
00:47:38,610 --> 00:47:41,790
all this information is
being stored in context

1090
00:47:41,790 --> 00:47:43,830
so that everything is relatable

1091
00:47:43,830 --> 00:47:46,180
and everything is understood
within its domain.

1092
00:47:48,810 --> 00:47:51,333
Now, overall,

1093
00:47:52,200 --> 00:47:54,750
we're also obviously trying
to really drive towards

1094
00:47:54,750 --> 00:47:58,920
that proactive AWS operations.

1095
00:47:58,920 --> 00:48:01,410
So as mentioned earlier,

1096
00:48:01,410 --> 00:48:03,810
we've got that direct AWS integration,

1097
00:48:03,810 --> 00:48:05,280
which allows us to easily capture

1098
00:48:05,280 --> 00:48:09,090
all of the key AWS telemetry information.

1099
00:48:09,090 --> 00:48:12,420
Now, we're also able to leverage
all of those cloud tags.

1100
00:48:12,420 --> 00:48:15,390
So bringing in your actual
cloud tagging strategy

1101
00:48:15,390 --> 00:48:18,000
directly into the Dynatrace platform

1102
00:48:18,000 --> 00:48:19,290
so that you're not having to recreate,

1103
00:48:19,290 --> 00:48:21,270
you're not having to build
new tagging strategies

1104
00:48:21,270 --> 00:48:23,520
and provide additional complexity.

1105
00:48:23,520 --> 00:48:25,530
We can inherit the tagging strategy

1106
00:48:25,530 --> 00:48:27,930
you've already got in your cloud

1107
00:48:27,930 --> 00:48:29,820
and bring it directly into Dynatrace.

1108
00:48:29,820 --> 00:48:32,520
So again, whether you want
to go and look at a log

1109
00:48:32,520 --> 00:48:34,560
or a trace or a metric,

1110
00:48:34,560 --> 00:48:38,430
we're literally pushing those cloud tags

1111
00:48:38,430 --> 00:48:41,220
to the actual signals that
are actually taking place.

1112
00:48:41,220 --> 00:48:43,170
So again, you can query, filter,

1113
00:48:43,170 --> 00:48:45,453
do whatever you want with the information.

1114
00:48:46,770 --> 00:48:48,390
And again,

1115
00:48:48,390 --> 00:48:51,330
our AI can obviously utilize
all that information as well

1116
00:48:51,330 --> 00:48:54,420
so that we can ensure that
you've got that deep insights

1117
00:48:54,420 --> 00:48:58,590
into your AWS estate for
obviously these key three areas.

1118
00:48:58,590 --> 00:49:01,170
So whether that is the sort of

1119
00:49:01,170 --> 00:49:03,180
auto-prevention side of things,

1120
00:49:03,180 --> 00:49:05,970
so whether that's things
like capacity planning

1121
00:49:05,970 --> 00:49:08,880
and forecasting of the events,

1122
00:49:08,880 --> 00:49:10,980
whether it's things like
auto-remediation, you know,

1123
00:49:10,980 --> 00:49:12,360
with the root cause analysis

1124
00:49:12,360 --> 00:49:16,740
and obviously putting out those fires

1125
00:49:16,740 --> 00:49:19,650
before they hopefully
impact any of your users,

1126
00:49:19,650 --> 00:49:23,117
or whether it's auto-optimization
where, you know,

1127
00:49:23,117 --> 00:49:24,767
we are looking at resource optimization,

1128
00:49:24,767 --> 00:49:27,210
we are looking at
identifying misconfigurations

1129
00:49:27,210 --> 00:49:29,070
and improvements that can be made.

1130
00:49:29,070 --> 00:49:31,500
And I think it's always
interesting to see,

1131
00:49:31,500 --> 00:49:34,170
especially when you start to
get down the root of the AI

1132
00:49:34,170 --> 00:49:37,890
and the auto-optimization
side, is where, you know,

1133
00:49:37,890 --> 00:49:40,560
the human oversight
comes into these things.

1134
00:49:40,560 --> 00:49:42,750
Because a lot of the time,

1135
00:49:42,750 --> 00:49:45,240
things like auto-remediation
aren't too difficult,

1136
00:49:45,240 --> 00:49:47,550
you know, whether it's scaling things out,

1137
00:49:47,550 --> 00:49:49,470
spinning up boxes, change configuration

1138
00:49:49,470 --> 00:49:50,880
or whatever it may be.

1139
00:49:50,880 --> 00:49:53,910
But when you start to
get around optimization,

1140
00:49:53,910 --> 00:49:55,050
you have to have goals,

1141
00:49:55,050 --> 00:49:58,230
you have to have insights
provided by a human.

1142
00:49:58,230 --> 00:50:00,930
Because of course if you
just basically give an AI,

1143
00:50:00,930 --> 00:50:04,200
I want to make this as
performant as possible,

1144
00:50:04,200 --> 00:50:05,490
it's gonna cost a lot of money

1145
00:50:05,490 --> 00:50:08,070
'cause it's gonna obviously
scale out everything.

1146
00:50:08,070 --> 00:50:09,870
But if you're gonna, you know,

1147
00:50:09,870 --> 00:50:10,747
obviously go the other way and go,

1148
00:50:10,747 --> 00:50:12,720
"Oh, I want to make it
as secure as possible,"

1149
00:50:12,720 --> 00:50:15,000
it'll probably just switch it all off.

1150
00:50:15,000 --> 00:50:17,400
So again, you know, you
need to give it the realms,

1151
00:50:17,400 --> 00:50:20,070
the goals, the boundaries
that are associated with that,

1152
00:50:20,070 --> 00:50:22,620
and optimization is that balance.

1153
00:50:22,620 --> 00:50:24,150
And so that's always, I find,

1154
00:50:24,150 --> 00:50:27,100
quite an interesting one to
look at from an AI perspective.

1155
00:50:29,100 --> 00:50:32,940
So to sort of finish off and summarize,

1156
00:50:32,940 --> 00:50:34,920
really, Dynatrace is trying to help you

1157
00:50:34,920 --> 00:50:37,900
scale your log management cost effectively

1158
00:50:39,090 --> 00:50:42,180
and unify and contextualize
all of that information,

1159
00:50:42,180 --> 00:50:43,890
all of the log information

1160
00:50:43,890 --> 00:50:46,710
with all of the other signals that exist,

1161
00:50:46,710 --> 00:50:50,430
and really to enable that
proactive and AI-driven insights

1162
00:50:50,430 --> 00:50:52,110
for you to take place.

1163
00:50:52,110 --> 00:50:54,930
Now, not only does this help reduce

1164
00:50:54,930 --> 00:50:56,460
your mean time to resolution,

1165
00:50:56,460 --> 00:50:59,770
but it also helps you track
end-to-end business journeys

1166
00:51:01,080 --> 00:51:04,020
and obviously make those
data-driven business decisions

1167
00:51:04,020 --> 00:51:06,240
that we were talking about before as well.

1168
00:51:06,240 --> 00:51:10,410
And obviously, with AWS,
we can proactively help

1169
00:51:10,410 --> 00:51:14,100
with the AWS operations to
do the auto-remediation,

1170
00:51:14,100 --> 00:51:17,073
the auto-prevention and auto-optimization.

1171
00:51:18,510 --> 00:51:22,290
So I'm gonna invite Frank
and Alex back up on stage.

1172
00:51:22,290 --> 00:51:24,570
We're gonna have a quick sort of Q&A

1173
00:51:24,570 --> 00:51:27,600
sort of style of things
to finish this off.

1174
00:51:27,600 --> 00:51:29,130
- Awesome. If you want to sit down.

1175
00:51:29,130 --> 00:51:30,330
I'm gonna do rapid fire,

1176
00:51:30,330 --> 00:51:32,580
'cause we only have a
couple of minutes left.

1177
00:51:37,020 --> 00:51:38,730
Awesome. Alright.

1178
00:51:38,730 --> 00:51:40,650
I always like this kind of Q&A setup

1179
00:51:40,650 --> 00:51:43,230
because it's a little bit more,
you know, less structured.

1180
00:51:43,230 --> 00:51:45,180
And if there's any questions

1181
00:51:45,180 --> 00:51:46,380
that you really need to be answered,

1182
00:51:46,380 --> 00:51:47,340
you can just scream them in.

1183
00:51:47,340 --> 00:51:49,170
But unfortunately, we don't
have a mic to go around,

1184
00:51:49,170 --> 00:51:51,693
but I have a couple here
that I wanted to clarify.

1185
00:51:52,890 --> 00:51:55,590
And the first one is on culture.

1186
00:51:55,590 --> 00:51:59,460
That was a big thing of
one of your three pillars.

1187
00:51:59,460 --> 00:52:01,233
And I just wanted to know, like,

1188
00:52:02,430 --> 00:52:03,540
how did you actually manage that?

1189
00:52:03,540 --> 00:52:05,160
Because culture is a beautiful thing,

1190
00:52:05,160 --> 00:52:08,160
but it's also really difficult to impact

1191
00:52:08,160 --> 00:52:09,600
and make changes on.

1192
00:52:09,600 --> 00:52:12,600
Like, how did you manage that transition?

1193
00:52:12,600 --> 00:52:14,580
- So for us, I think,

1194
00:52:14,580 --> 00:52:17,520
first of all, it was identifying
the right set of people

1195
00:52:17,520 --> 00:52:21,000
who could be our culture
carriers across the organization,

1196
00:52:21,000 --> 00:52:24,180
and work out how we could
meaningfully engage them

1197
00:52:24,180 --> 00:52:26,730
in the process of looking
at the observability problem

1198
00:52:26,730 --> 00:52:28,800
and working out how we could move forward.

1199
00:52:28,800 --> 00:52:29,850
And then it was a question

1200
00:52:29,850 --> 00:52:31,830
of how do we meaningfully
carry that message

1201
00:52:31,830 --> 00:52:33,753
out to the rest of our organization.

1202
00:52:35,160 --> 00:52:38,370
We did that through things like
Dyna Days, through support,

1203
00:52:38,370 --> 00:52:40,950
through AWS, running GameDays

1204
00:52:40,950 --> 00:52:42,843
in combination with Dynatrace.

1205
00:52:43,830 --> 00:52:46,800
And through sharing
internal documentation,

1206
00:52:46,800 --> 00:52:49,560
things like this maturity
model that you saw earlier.

1207
00:52:49,560 --> 00:52:50,670
It's hard, though.

1208
00:52:50,670 --> 00:52:54,300
Culture eats strategy for breakfast.

1209
00:52:54,300 --> 00:52:56,910
As much as you want to
build out strategies

1210
00:52:56,910 --> 00:52:59,070
that drive observability and other pieces,

1211
00:52:59,070 --> 00:53:01,230
unless you pay attention
to that cultural piece,

1212
00:53:01,230 --> 00:53:02,460
you will fail.

1213
00:53:02,460 --> 00:53:04,290
- How long did it take you?

1214
00:53:04,290 --> 00:53:08,010
- Oh it was a good year-long
process, to be honest.

1215
00:53:08,010 --> 00:53:10,230
- Yeah, and did Dynatrace
somehow contribute

1216
00:53:10,230 --> 00:53:11,670
to making it a little easier?

1217
00:53:11,670 --> 00:53:12,503
- Massively.

1218
00:53:12,503 --> 00:53:13,860
So I mentioned Dyna Day.

1219
00:53:13,860 --> 00:53:16,050
This is a day that hopefully

1220
00:53:16,050 --> 00:53:18,930
you're not all about to
ask Dynatrace to provide,

1221
00:53:18,930 --> 00:53:21,150
but I think they will be in certain cases,

1222
00:53:21,150 --> 00:53:23,760
where some of the Dynatrace
account team come out,

1223
00:53:23,760 --> 00:53:25,620
spend some time with your engineers,

1224
00:53:25,620 --> 00:53:29,310
work out what their challenges are,

1225
00:53:29,310 --> 00:53:31,380
work out what the lever are to pull,

1226
00:53:31,380 --> 00:53:33,510
and help them understand
what great can look like

1227
00:53:33,510 --> 00:53:35,310
from an observability position.

1228
00:53:35,310 --> 00:53:39,510
Things like the Dynatrace-AWS
GameDay, incredibly powerful.

1229
00:53:39,510 --> 00:53:42,870
Running a game day is a
huge thing. It's great fun.

1230
00:53:42,870 --> 00:53:45,480
It's really engaging for
our engineering community.

1231
00:53:45,480 --> 00:53:46,860
And to work with two partners

1232
00:53:46,860 --> 00:53:50,250
like Dynatrace and AWS,
absolutely fantastic.

1233
00:53:50,250 --> 00:53:52,653
- Nice, and are you
running a lot of these?

1234
00:53:54,900 --> 00:53:56,880
- Quite a few, to be fair.

1235
00:53:56,880 --> 00:53:58,920
I mean, it does vary customer by customer.

1236
00:53:58,920 --> 00:54:00,480
I think every customer is different,

1237
00:54:00,480 --> 00:54:05,160
and where the cultural changes
are needed also vary as well.

1238
00:54:05,160 --> 00:54:08,880
I think overall a lot of it is about,

1239
00:54:08,880 --> 00:54:11,550
I guess, enablement and involvement.

1240
00:54:11,550 --> 00:54:13,530
So the more that people can be involved,

1241
00:54:13,530 --> 00:54:15,690
the more people can be
enabled, the easier that is.

1242
00:54:15,690 --> 00:54:17,670
'Cause obviously Alex talks about,

1243
00:54:17,670 --> 00:54:18,990
through the presentation,

1244
00:54:18,990 --> 00:54:21,030
things like the return
on investment was easy

1245
00:54:21,030 --> 00:54:24,000
and sort of insane how how much could be

1246
00:54:24,000 --> 00:54:25,980
sort of returned very quickly.

1247
00:54:25,980 --> 00:54:27,990
But it's all about, you know,

1248
00:54:27,990 --> 00:54:30,780
how much is it of a
benefit to that individual.

1249
00:54:30,780 --> 00:54:32,460
And so how do we bring them in?

1250
00:54:32,460 --> 00:54:34,740
Sometimes it is through the AWS GameDays

1251
00:54:34,740 --> 00:54:37,800
and things like that
where sometimes it is,

1252
00:54:37,800 --> 00:54:40,890
to an extent, a more
playful, fun thing to do,

1253
00:54:40,890 --> 00:54:41,723
to bring the in.

1254
00:54:41,723 --> 00:54:44,790
But they all have a level of enablement

1255
00:54:44,790 --> 00:54:45,870
sort of as part of that.

1256
00:54:45,870 --> 00:54:47,460
- Yeah, and I think there's always so many

1257
00:54:47,460 --> 00:54:49,080
little pockets of knowledge

1258
00:54:49,080 --> 00:54:51,030
that how you can actually be enabled.

1259
00:54:51,030 --> 00:54:52,320
And everybody that's in the audience

1260
00:54:52,320 --> 00:54:53,280
has very much leaned in,

1261
00:54:53,280 --> 00:54:54,450
and you're here and you're, you know,

1262
00:54:54,450 --> 00:54:57,510
but how this is being translated
to the larger organizations

1263
00:54:57,510 --> 00:54:58,650
that are behind you,

1264
00:54:58,650 --> 00:55:01,080
I think that's always nice to kind of get

1265
00:55:01,080 --> 00:55:02,160
the best practices from.

1266
00:55:02,160 --> 00:55:04,620
- I think that's exactly why
we started with detractors,

1267
00:55:04,620 --> 00:55:07,590
people who didn't believe

1268
00:55:07,590 --> 00:55:09,150
that they had an observability problem,

1269
00:55:09,150 --> 00:55:11,550
because they were the most
powerful groups to convince.

1270
00:55:11,550 --> 00:55:15,120
Once you get those people aligned,

1271
00:55:15,120 --> 00:55:17,550
you can bring the rest of
the organization along.

1272
00:55:17,550 --> 00:55:19,810
Those people who are enthusiastic, who are

1273
00:55:21,150 --> 00:55:23,040
promoters of observability,

1274
00:55:23,040 --> 00:55:24,300
quite often are not the people

1275
00:55:24,300 --> 00:55:25,800
you need to persuade to change.

1276
00:55:25,800 --> 00:55:27,270
- Yeah.

1277
00:55:27,270 --> 00:55:29,550
Switching a little bit, five minutes.

1278
00:55:29,550 --> 00:55:32,340
I want to talk a little bit
about like mixed environments.

1279
00:55:32,340 --> 00:55:34,320
You have of course a lot of signals

1280
00:55:34,320 --> 00:55:37,440
coming from OTel collectors
from the OneAgent,

1281
00:55:37,440 --> 00:55:38,763
from different platforms.

1282
00:55:39,650 --> 00:55:40,483
How did you manage that?

1283
00:55:40,483 --> 00:55:43,470
Because no one comes to
the table being just,

1284
00:55:43,470 --> 00:55:45,090
you know, vanilla.

1285
00:55:45,090 --> 00:55:46,680
And then there's a lot
of complexity to it.

1286
00:55:46,680 --> 00:55:48,030
- No, absolutely.

1287
00:55:48,030 --> 00:55:52,020
I spent a lot of time talking
about our previous logs piece.

1288
00:55:52,020 --> 00:55:54,270
We also had a previous
observability platform

1289
00:55:54,270 --> 00:55:55,890
provided by another vendor,

1290
00:55:55,890 --> 00:55:59,160
and we'd gone down a strongly
OTel-opinionated route

1291
00:55:59,160 --> 00:56:00,690
with that implementation.

1292
00:56:00,690 --> 00:56:04,560
OTel was spread out across
all of our backend services.

1293
00:56:04,560 --> 00:56:06,450
Conversely, our frontend services

1294
00:56:06,450 --> 00:56:08,490
had no form of observability coverage

1295
00:56:08,490 --> 00:56:10,530
aside from the logging piece.

1296
00:56:10,530 --> 00:56:14,310
What we really wanted was a
full end-to-end experience.

1297
00:56:14,310 --> 00:56:16,530
And what we learned on our journey

1298
00:56:16,530 --> 00:56:20,013
is that with everything
there is a compromise.

1299
00:56:21,960 --> 00:56:24,420
Our strongly OTel-opinionated pattern

1300
00:56:24,420 --> 00:56:27,330
was great for providing
a standardized interface

1301
00:56:27,330 --> 00:56:29,490
across all of our backend services.

1302
00:56:29,490 --> 00:56:33,120
But when we started trying to
connect frontend user actions

1303
00:56:33,120 --> 00:56:36,210
through to the backend ecosystem,

1304
00:56:36,210 --> 00:56:39,060
staying pure OTel in
that backend ecosystem,

1305
00:56:39,060 --> 00:56:41,580
we were losing as much as we were gaining.

1306
00:56:41,580 --> 00:56:44,700
So for us, we have just been

1307
00:56:44,700 --> 00:56:47,220
through our first big period on Dynatrace.

1308
00:56:47,220 --> 00:56:49,140
We know we have more work to do.

1309
00:56:49,140 --> 00:56:51,780
And I think that work will
be moving towards a position

1310
00:56:51,780 --> 00:56:55,470
where at least our first layer of services

1311
00:56:55,470 --> 00:56:58,680
behind our frontends
on natively implemented

1312
00:56:58,680 --> 00:57:00,450
with Dynatrace OneAgent.

1313
00:57:00,450 --> 00:57:02,910
Because we believe, although
that will compromise

1314
00:57:02,910 --> 00:57:05,430
our singular OTel-everywhere vision,

1315
00:57:05,430 --> 00:57:06,990
we will get a better experience

1316
00:57:06,990 --> 00:57:09,390
and more value from
the Dynatrace platform.

1317
00:57:09,390 --> 00:57:13,740
- Yeah, and speaking of
distributed or, you know,

1318
00:57:13,740 --> 00:57:16,053
tool sprawl have a lot of different ones.

1319
00:57:16,890 --> 00:57:19,020
Like, I would assume that
everybody in the audience

1320
00:57:19,020 --> 00:57:23,160
uses probably more than
one observability provider.

1321
00:57:23,160 --> 00:57:26,400
How do you actually make that unified

1322
00:57:26,400 --> 00:57:28,710
or unifying observability possible.

1323
00:57:28,710 --> 00:57:30,960
Maybe for you, Jon.

1324
00:57:30,960 --> 00:57:32,520
- Yeah, I mean,

1325
00:57:32,520 --> 00:57:33,990
as you said, most companies

1326
00:57:33,990 --> 00:57:37,560
do have a lot of observability
tools and vendors out there.

1327
00:57:37,560 --> 00:57:38,730
It does range across the board.

1328
00:57:38,730 --> 00:57:43,170
I think a lot of the time it is that focus

1329
00:57:43,170 --> 00:57:46,950
on your initial areas or
initial places of focus

1330
00:57:46,950 --> 00:57:48,780
where you can get the most amount of value

1331
00:57:48,780 --> 00:57:50,040
and then expanding from there.

1332
00:57:50,040 --> 00:57:52,650
'Cause I think what you
end up seeing a lot of

1333
00:57:52,650 --> 00:57:57,650
is that it's not just, I say,
the quality of each vendor

1334
00:57:58,170 --> 00:58:00,870
or each tool that's necessarily the issue;

1335
00:58:00,870 --> 00:58:03,120
quite often it is the
siloed nature of that data,

1336
00:58:03,120 --> 00:58:04,710
which is an even bigger issue.

1337
00:58:04,710 --> 00:58:06,990
The fact that each team
has their own tool,

1338
00:58:06,990 --> 00:58:09,870
their own source of truth
means that you end up

1339
00:58:09,870 --> 00:58:11,760
with this whole
finger-pointing and war rooms

1340
00:58:11,760 --> 00:58:13,710
that are taking place.

1341
00:58:13,710 --> 00:58:15,240
And, you know, bringing that together

1342
00:58:15,240 --> 00:58:18,300
is one key part of that, I think.

1343
00:58:18,300 --> 00:58:20,730
And so even if that information

1344
00:58:20,730 --> 00:58:21,960
is coming from different areas,

1345
00:58:21,960 --> 00:58:23,520
that's why I'd say a lot of the time,

1346
00:58:23,520 --> 00:58:25,890
if you can focus on the
applications, the teams

1347
00:58:25,890 --> 00:58:28,950
where you can get the most
amount of value quickly,

1348
00:58:28,950 --> 00:58:32,070
it can then expand and sort of
grow very quickly from that.

1349
00:58:32,070 --> 00:58:33,780
Because you can then start to bring in

1350
00:58:33,780 --> 00:58:35,460
the various different tools
from various different teams

1351
00:58:35,460 --> 00:58:39,420
and grow from that, rather
than taking one layer,

1352
00:58:39,420 --> 00:58:42,003
like logs as an example, across the board.

1353
00:58:43,200 --> 00:58:44,600
Of course, that can be value

1354
00:58:45,590 --> 00:58:47,640
let's say from a licensing
replacement perspective,

1355
00:58:47,640 --> 00:58:49,950
and you know, cost-savings on that front.

1356
00:58:49,950 --> 00:58:51,280
But of course, if you can,

1357
00:58:51,280 --> 00:58:54,390
let's say take everything
within a particular team,

1358
00:58:54,390 --> 00:58:56,460
you know, all the different
telemetry data together,

1359
00:58:56,460 --> 00:58:57,840
you will get significantly more value

1360
00:58:57,840 --> 00:59:01,560
than taking one data
type across the board.

1361
00:59:01,560 --> 00:59:02,670
- Yeah, and I mean,

1362
00:59:02,670 --> 00:59:04,440
I see this from my perspective as well.

1363
00:59:04,440 --> 00:59:07,717
Like, the number of customers
that come to AWS and say,

1364
00:59:07,717 --> 00:59:09,090
"We have so many different vendors.

1365
00:59:09,090 --> 00:59:10,020
How do we figure this out?

1366
00:59:10,020 --> 00:59:11,820
Can you help us consolidate of one?

1367
00:59:11,820 --> 00:59:13,050
Which one do you recommend?"

1368
00:59:13,050 --> 00:59:14,700
Like, that's really kind of top of mind

1369
00:59:14,700 --> 00:59:15,903
for a lot of customers.

1370
00:59:18,060 --> 00:59:20,010
What about automation?

1371
00:59:20,010 --> 00:59:21,420
There's a lot of capability

1372
00:59:21,420 --> 00:59:23,160
that Dynatrace brings to the table.

1373
00:59:23,160 --> 00:59:26,520
Were you able to sort of
walk the whole nine yards

1374
00:59:26,520 --> 00:59:29,670
with automated and automated workflows?

1375
00:59:29,670 --> 00:59:32,040
- It's a journey we've certainly started.

1376
00:59:32,040 --> 00:59:34,770
So when I talked through
our technology stack,

1377
00:59:34,770 --> 00:59:36,720
I failed to mention the
elephant in the room.

1378
00:59:36,720 --> 00:59:39,090
We do have a little bit of Kubernetes.

1379
00:59:39,090 --> 00:59:42,450
We use Kubernetes specifically
for our AI workloads.

1380
00:59:42,450 --> 00:59:45,960
And this is more traditional AI

1381
00:59:45,960 --> 00:59:47,370
where we're using our own models

1382
00:59:47,370 --> 00:59:50,910
and we're running on our own EKS clusters,

1383
00:59:50,910 --> 00:59:54,990
the actual computation
around those models.

1384
00:59:54,990 --> 00:59:57,210
We have a set of challenges around that.

1385
00:59:57,210 --> 01:00:00,120
We scale up and down quite
quickly as an organization.

1386
01:00:00,120 --> 01:00:04,173
And when you're running
on GPU-bound hardware,

1387
01:00:05,010 --> 01:00:07,290
we were finding that scaling up and down

1388
01:00:07,290 --> 01:00:10,590
with the base set of Kubernetes
metrics was not effective.

1389
01:00:10,590 --> 01:00:13,020
The boot-up time is just too long.

1390
01:00:13,020 --> 01:00:16,380
EKS has a fantastic
feature called warm pools

1391
01:00:16,380 --> 01:00:18,780
which allows you to solve this problem.

1392
01:00:18,780 --> 01:00:22,710
But equally, we didn't want
a load of spare G5 instances

1393
01:00:22,710 --> 01:00:24,180
sat around twiddling their thumbs,

1394
01:00:24,180 --> 01:00:26,393
because they're not the
cheapest thing in the world.

1395
01:00:27,570 --> 01:00:30,607
Dynatrace helped us solve
this by predictively saying,

1396
01:00:30,607 --> 01:00:34,997
"We can see pressure
on your NVIDIA drivers

1397
01:00:36,120 --> 01:00:37,230
is looking like this.

1398
01:00:37,230 --> 01:00:38,880
You are getting to the point

1399
01:00:38,880 --> 01:00:40,650
where you're going to need to scale soon.

1400
01:00:40,650 --> 01:00:43,890
I will add one node into the warm pool."

1401
01:00:43,890 --> 01:00:46,080
And that can then scale,

1402
01:00:46,080 --> 01:00:49,653
be elegantly scaled upon
to without a long delay.

1403
01:00:50,610 --> 01:00:53,610
- Got it. Anything to add?

1404
01:00:53,610 --> 01:00:56,310
- I mean, we're basically out of time.

1405
01:00:56,310 --> 01:00:58,920
But yeah, I mean, overall,

1406
01:00:58,920 --> 01:01:00,510
there obviously is a
lot of potential there.

1407
01:01:00,510 --> 01:01:01,680
And I think it is, you know,

1408
01:01:01,680 --> 01:01:03,840
as with Alex and Storio group,

1409
01:01:03,840 --> 01:01:05,400
it is quite often that staged approach.

1410
01:01:05,400 --> 01:01:08,010
You do often start with certain scenarios,

1411
01:01:08,010 --> 01:01:10,620
certain areas where
you can see easy value,

1412
01:01:10,620 --> 01:01:13,200
and then grow it out over
time as things evolve.

1413
01:01:13,200 --> 01:01:15,960
And as of course everybody gets behind it

1414
01:01:15,960 --> 01:01:17,550
and gets involved in that and, you know,

1415
01:01:17,550 --> 01:01:19,830
then automates and gets a lot more value.

1416
01:01:19,830 --> 01:01:21,300
- Awesome.

1417
01:01:21,300 --> 01:01:22,860
You were emphatically raising your hand.

1418
01:01:22,860 --> 01:01:24,573
If you can scream really loudly.

