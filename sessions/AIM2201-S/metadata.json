{
  "title": "AWS re:Invent 2025 - Scaling instantly to 1000 GPUs for Serverless AI inference (AIM2201)",
  "title_cn": "AWS re:Invent 2025 - 为无服务器AI推理即时扩展至1000个GPU (AIM2201)",
  "abstract": "If you’re deploying generative AI models, you need a lot of GPU compute. But GPUs are expensive and production inference can be volatile. How can you maximize cost efficiency while also meeting customer demand? In this session, learn how Modal built a Rust-based container stack from scratch that can spin thousands of GPUs up and down in seconds. By leveraging AWS’s compute and storage products, Modal's platform is able to bring instant, flexible GPU access to ML engineers at companies big and small. This presentation is brought to you by Modal, an AWS Partner.",
  "abstract_cn": "如果您要部署生成式AI模型，您需要大量的GPU计算资源。但GPU价格昂贵，生产推理工作负载波动较大。如何在满足客户需求的同时最大化成本效率？在本次会议中，了解Modal如何从零开始构建基于Rust的容器堆栈，能够在几秒钟内启动和关闭数千个GPU。通过利用AWS的计算和存储产品，Modal的平台能够为大小公司的机器学习工程师提供即时、灵活的GPU访问。本演示由AWS合作伙伴Modal提供。",
  "presenter": [
    {
      "name": "Erik Bernhardsson",
      "title": "Chief Executive Officer",
      "company": "Modal Labs"
    }
  ],
  "attributes": {
    "topic": "Artificial Intelligence",
    "area_of_interest": [
      "Generative AI",
      "Cost Optimization",
      "Machine Learning"
    ],
    "services": [],
    "type": "Lightning talk"
  },
  "video_url": "https://www.youtube.com/watch?v=qb9hZx3ZTb0",
  "session_code": "AIM2201-S",
  "duration_seconds": 959,
  "duration_minutes": 16.0
}