# AWS re:Invent 2025 - 掌握 Amazon Bedrock 中的模型选择

## 会议概述

本次会议由 AWS Amazon Bedrock 团队的 Scott Mason（首席全球 AI 专家）、John Lou（首席产品经理）以及 Coin Market Cap 的 Brian Co（高级 AI 产品经理）共同主讲，重点介绍了在 Amazon Bedrock 中选择和优化 AI 模型的系统化框架。

当前 AI 模型市场呈现爆炸式增长，仅 HuggingFace 平台就有 219 万个公开模型，每 10 秒就有一个新模型发布，相当于每天约 4000 个新模型。面对如此庞大的选择，许多组织在模型选择上缺乏系统化的框架，这导致了时间和资源的浪费，也影响了 AI 项目的成功率。Amazon Bedrock 提供了超过 80 个模型选择，本周还新增了 Google、Minimax、Nvidia 和 Moonshot 四家模型提供商。

会议提出了一个简洁实用的三步框架：识别候选模型（Identify）、评估模型性能（Evaluate）、优化生产部署（Optimize）。通过金融犯罪调查代理这一真实案例，演示了如何将理论框架应用于实践，最终实现了在将推理请求从 5 亿次扩展到 50 亿次/天的同时，降低 80% 的成本。

## 详细时间线与关键要点

### **开场与背景介绍 (0:00-3:30)**
- **0:00** - 会议开始，主持人询问现场观众：有多少人被大量 AI 模型选择所困扰？大部分人举手
- **0:30** - 询问有多少组织拥有系统化的模型选择框架？举手的人明显减少
- **1:00** - 介绍演讲嘉宾：Scott Mason、John Lou 和 Brian Co
- **1:30** - 会议议程：概述挑战、介绍框架、客户案例分享
- **2:19** - 展示当前 AI 模型市场规模：HuggingFace 上有 219 万个公开模型
- **2:30** - HuggingFace 首席产品官透露：每 10 秒就有一个新模型发布，相当于每天 4000 个模型

### **模型选择的挑战 (3:30-5:00)**
- **3:30** - 讨论模型选择困难的原因
- **4:00** - 首次 AI 应用部署的成功对组织至关重要，影响团队士气和组织声誉
- **4:20** - 客户面临的具体问题：缺乏选择框架、创新速度快、需要大规模部署能力
- **4:40** - 如果测试周期需要 4 周，而更好的模型昨天就发布了，这会造成困境

### **Amazon Bedrock 解决方案 (5:00-7:30)**
- **5:00** - Amazon Bedrock 的核心价值：让开发者专注于应用开发
- **5:30** - Bedrock 提供顶级模型选择、优化的推理服务、无服务器扩展能力
- **6:00** - 支持 RAG（检索增强生成）、知识库、模型蒸馏和微调
- **6:20** - 提供 Guardrails 等安全和责任 AI 功能
- **6:40** - 本周推出多项 Agent Core 能力，帮助大规模部署和运营代理
- **7:00** - 与顶级模型提供商合作，持续扩展模型数量
- **7:15** - 本周新增 4 家模型提供商：Google、Minimax、Nvidia、Moonshot
- **7:30** - Bedrock 现已提供超过 80 个模型

### **模型选择框架介绍 (7:30-9:00)**
- **7:45** - 三步框架：识别（Identify）、评估（Evaluate）、优化（Optimize）
- **8:15** - 使用金融犯罪调查代理作为示例用例
- **8:30** - 用例需求：处理结构化和非结构化数据、生成准确简洁的摘要
- **8:45** - 规模要求：每天 50 亿个 token，目标提升 20% 效率

### **第一步：识别候选模型 (9:00-15:00)**
- **9:00** - 识别阶段的三个过滤标准：模态、基准测试、差异化能力
- **9:30** - 按模态过滤：文本、图像、视频、音频等
- **10:00** - Bedrock 按模态分类的模型展示：文本、多模态输入/输出、嵌入模型
- **10:30** - 介绍 Artificial Analysis 作为第三方评估资源
- **11:00** - 展示智能与价格的对比图表：Claude Haiku、DeepSeek、GPT-4o 等表现突出
- **11:30** - 强调详细基准测试的重要性：长上下文推理、代理工具使用
- **12:00** - Claude 和 DeepSeek 在长上下文推理方面表现优异
- **12:20** - 介绍代理能力基准测试：Galileo、Berkeley 排行榜
- **13:00** - 差异化能力类别：推理模型（Reasoning Models）
- **13:20** - DeepSeek 在今年 1 月引发关注，推理模型成为主流
- **13:40** - 代理能力：自主任务完成、工具选择、编排能力
- **14:00** - 可定制性：主要由开源权重模型主导，用于微调和蒸馏
- **14:20** - 领域特定模型：金融、医疗等垂直领域
- **14:40** - Bedrock Marketplace 提供领域特定模型：Upstage（韩语）、Writer Palmira（金融和医疗）

### **金融犯罪用例的模型识别 (15:00-16:00)**
- **15:00** - 应用识别框架到金融犯罪调查代理
- **15:15** - 按模态过滤：文本到文本
- **15:30** - 考虑的基准：摘要、长上下文推理、代理能力
- **15:45** - 最终候选模型：Claude Sonnet、OpenAI GPT-4o、DeepSeek v3.1

### **第二步：评估模型 (16:00-25:00)**
- **16:00** - John Lou 接手讲解评估阶段
- **16:15** - 一些客户已建立 24-48 小时内完成模型评估的框架
- **16:30** - 评估的第一步：创建黄金数据集（Golden Dataset）
- **17:00** - 黄金数据集包含两部分：提示词和真实答案（Ground Truth）
- **17:30** - 黄金数据集创建建议：约 100 个用例，针对特定术语和工作流
- **18:00** - 包含 5% 的对抗性用例来测试模型极限
- **18:20** - 从 10 个用例开始，通过内部反馈扩展到 100 个
- **18:40** - 生产环境可能需要 200-300 个用例

### **使用代理扩展黄金数据集 (19:00-22:00)**
- **19:00** - 人类不擅长的任务：详细的低级任务、重复性工作
- **19:20** - 人类擅长的任务：高层判断、设置规则和标准、改进解决方案
- **19:40** - 提出使用代理系统扩展黄金数据集
- **20:00** - 三个代理角色：用户模拟器代理、任务代理、评判代理
- **20:30** - 用户模拟器：模拟 KYC 专家，生成查询
- **21:00** - 任务代理：执行实际工作，分析文档和交易数据
- **21:20** - 评判代理：根据人类设定的规则评估任务代理的输出
- **21:40** - 评判代理迭代直到输出符合标准，然后将正确答案添加到黄金数据集
- **22:00** - 人类角色：评估整体系统，更新规则和标准

### **AWS 代理技术栈 (22:00-23:00)**
- **22:10** - 客户可以使用开源框架 Strands 快速构建和本地部署代理
- **22:30** - 准备扩展时可以使用 Agent Core，获得企业级安全和专用运行时环境
- **22:50** - 本次 re:Invent 发布了多项 Agent Core 新功能

### **评估指标 (23:00-25:00)**
- **23:00** - 运营指标：成本、延迟、可扩展性（基础指标）
- **23:20** - 发送多样化的推理请求模式，测试 P95、P99 延迟
- **23:40** - 综合评估指标：语义知识、质量准确性、风格可用性、负责任 AI
- **24:00** - 自定义指标：点击率、用户覆盖率等业务 KPI
- **24:20** - 其他重要指标：时间一致性（检测模型漂移）、多模态交互、代理能力
- **24:50** - 强调持续评估的重要性

### **评估方法 (25:00-28:00)**
- **25:00** - 三种评估方法：基准测试、人工评估、LLM 作为评判者
- **25:20** - 基准测试：适用于有明确对错答案的场景（如 MMLU、GSM8K）
- **25:40** - 人工评估：捕捉细微的语义差异
- **26:00** - LLM 作为评判者：利用强大模型的推理能力进行评估
- **26:20** - 可以使用单个强大模型作为评判，或 10-12 个轻量级模型作为陪审团
- **26:40** - Amazon Bedrock 模型评估支持所有三种方法
- **27:00** - 支持自定义指标
- **27:20** - 可以评估 Bedrock 外部的模型响应

### **LLM 作为评判者的实现 (28:00-30:00)**
- **28:00** - Bedrock 为多种评估模型优化了提示词：Anthropic、Meta、Nova 模型
- **28:30** - 输入 JSON Lines 格式的黄金数据集文件
- **29:00** - 在 Bedrock 模型评估中设置：选择评估模型和被评估模型
- **29:20** - 示例：使用 Claude Sonnet 3.7 评估 GPT-4o 12B
- **29:40** - 选择 12 个内置指标或导入自定义指标
- **30:00** - 运行评估并比较不同模型

### **评估结果展示 (30:00-31:00)**
- **30:10** - 展示雷达图比较 GPT-4o 和 DeepSeek v3.1 的性能
- **30:30** - 可以深入查看每个提示的评分（0-1 分，1 分最佳）
- **30:50** - 提供评分解释，说明为什么获得该分数

### **第三步：优化模型 (31:00-35:00)**
- **31:00** - 进入优化阶段
- **31:15** - 优化不仅是单个模型，而是整个系统
- **31:30** - 优化策略：替换单个模型、引入多个模型、优化端到端工作流
- **31:50** - 通过微调和蒸馏进一步定制模型
- **32:10** - 优化推理请求的成本和延迟
- **32:30** - 介绍推理层级（Inference Tiers）：优先层级和弹性层级
- **32:50** - 弹性层级可节省高达 50% 的成本

### **多模型路由策略 (33:00-35:00)**
- **33:00** - 四种路由策略
- **33:15** - 基于规则的路由：简单查询用轻量模型，复杂查询用重型模型
- **33:35** - 基于机器学习的路由：使用分类器训练路由规则
- **33:55** - 基于 LLM 的路由：让 LLM 分析请求并决定路由
- **34:15** - 最佳实践：结合所有三种方法
- **34:30** - 模型定制方法：直接微调、蒸馏、通过 SageMaker 高级定制
- **34:50** - 新发布的 Nova Forge：结合客户数据和 Nova 预训练数据集

### **金融犯罪用例的优化结果 (35:00-36:00)**
- **35:00** - 应用优化框架的最终结果
- **35:15** - 使用 GPT-4o 12B 进行快速分类任务
- **35:30** - 使用 Claude Sonnet 进行复杂分析
- **35:45** - 微调 GPT-4o 模型并优化推理层级
- **36:00** - 最终成果：在将工作负载从 5 亿扩展到 50 亿推理请求/天的同时，成本降低 80%
- **36:15** - 强调这是真实客户案例

### **框架总结 (36:00-结束)**
- **36:20** - 回顾三步框架的实际应用
- **36:30** - 识别阶段：从模型库中筛选候选模型
- **36:40** - 评估阶段：使用黄金数据集和多种评估方法
- **36:50** - 优化阶段：系统级优化，包括多模型编排和推理优化
- **37:00** - 会议结束，准备进入客户案例分享环节（Brian Co 的 Coin Market Cap 案例）