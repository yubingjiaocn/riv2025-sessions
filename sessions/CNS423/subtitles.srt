1
00:00:00,570 --> 00:00:01,403
- I'm Julian.

2
00:00:01,403 --> 00:00:02,730
I'm a Developer Advocate at AWS,

3
00:00:02,730 --> 00:00:04,260
working with the Serverless team.

4
00:00:04,260 --> 00:00:06,330
And I love helping developers turbocharge

5
00:00:06,330 --> 00:00:07,560
how they build applications

6
00:00:07,560 --> 00:00:09,270
using all the cool things Lambda can do.

7
00:00:09,270 --> 00:00:10,103
Rajesh.

8
00:00:10,103 --> 00:00:11,430
- Yeah, hello, everyone.

9
00:00:11,430 --> 00:00:13,710
I'm Rajesh Pandey, Principal
Engineer with AWS Lambda.

10
00:00:13,710 --> 00:00:17,190
I've been with Lambda for six years now,

11
00:00:17,190 --> 00:00:19,200
and in that time I have
watched Lambda grow

12
00:00:19,200 --> 00:00:21,690
from thousands of customers
to like millions of customers,

13
00:00:21,690 --> 00:00:23,850
and it's been an
incredible journey for me,

14
00:00:23,850 --> 00:00:25,860
and I'm looking forward to
sharing some of those fun stories

15
00:00:25,860 --> 00:00:26,970
with you all today.

16
00:00:26,970 --> 00:00:27,810
- Cool, excellent.

17
00:00:27,810 --> 00:00:29,040
Can't wait.
- Yeah.

18
00:00:29,040 --> 00:00:30,630
- So today we're gonna talk,

19
00:00:30,630 --> 00:00:32,760
continue, actually, a
series of different Lambda

20
00:00:32,760 --> 00:00:34,500
under the Hoods over previous re:Invents

21
00:00:34,500 --> 00:00:36,840
to help you understand
Lambda a little bit more.

22
00:00:36,840 --> 00:00:39,240
And I'm gonna go over
some Lambda fundamentals,

23
00:00:39,240 --> 00:00:40,470
how various invokes work

24
00:00:40,470 --> 00:00:43,020
and spend some more time on
our polling capabilities.

25
00:00:43,020 --> 00:00:44,430
And then Rajesh is gonna come back in

26
00:00:44,430 --> 00:00:46,500
and head proper down the rabbit hole

27
00:00:46,500 --> 00:00:49,410
and do some interesting
lessons on how we built Lambda,

28
00:00:49,410 --> 00:00:50,880
how we made the choices we did,

29
00:00:50,880 --> 00:00:53,160
and even more information on the pollers.

30
00:00:53,160 --> 00:00:54,540
Now, to be honest with you,

31
00:00:54,540 --> 00:00:55,890
a part of the magic of Lambda

32
00:00:55,890 --> 00:00:57,600
is a lot of what we're gonna cover today.

33
00:00:57,600 --> 00:00:59,730
You don't actually need to know.

34
00:00:59,730 --> 00:01:01,500
We just handle it all for you.

35
00:01:01,500 --> 00:01:03,060
It's a super amazing service,

36
00:01:03,060 --> 00:01:05,250
and we love explaining how it works

37
00:01:05,250 --> 00:01:07,230
so it doesn't seem so mysterious,

38
00:01:07,230 --> 00:01:09,993
just maybe magical in how it
works at such great scale.

39
00:01:10,950 --> 00:01:12,360
So let's start with a trivia.

40
00:01:12,360 --> 00:01:14,400
Who wants to know probably
the biggest secret

41
00:01:14,400 --> 00:01:15,300
of the serverless?

42
00:01:15,300 --> 00:01:16,560
Anyone?

43
00:01:16,560 --> 00:01:18,840
Well, actually, there are quite
a lot of servers underneath,

44
00:01:18,840 --> 00:01:20,970
sorry to tell you, hundreds
of thousands, in fact.

45
00:01:20,970 --> 00:01:23,400
And you as a consumer never see them.

46
00:01:23,400 --> 00:01:24,570
Because we, AWS,

47
00:01:24,570 --> 00:01:27,180
manage all of this underlying
infrastructure for you,

48
00:01:27,180 --> 00:01:30,900
allowing you to focus on
your business scenarios.

49
00:01:30,900 --> 00:01:31,733
And with Lambda,

50
00:01:31,733 --> 00:01:34,350
we like to think of that
you just bring your egg,

51
00:01:34,350 --> 00:01:36,360
your code, the life of your business.

52
00:01:36,360 --> 00:01:38,760
And we've built an
incredibly resilient nest

53
00:01:38,760 --> 00:01:40,950
at scale to look after your egg.

54
00:01:40,950 --> 00:01:42,190
Because it means

55
00:01:43,292 --> 00:01:45,180
you don't have to build a
whole bunch of things yourself.

56
00:01:45,180 --> 00:01:48,205
Things like auto-scaling
logic, AZ failover,

57
00:01:48,205 --> 00:01:49,380
you know, retry logic, connection pooling,

58
00:01:49,380 --> 00:01:50,580
back pressure,

59
00:01:50,580 --> 00:01:52,560
lease management, all these
different kind of things,

60
00:01:52,560 --> 00:01:54,410
we take that on so you don't have to.

61
00:01:55,260 --> 00:01:57,210
And this obviously runs at massive scale.

62
00:01:57,210 --> 00:01:59,760
More than 15 trillion invokes each month,

63
00:01:59,760 --> 00:02:02,070
with a whopping 1.7 trillion invokes

64
00:02:02,070 --> 00:02:03,680
just on Prime Day alone.

65
00:02:03,680 --> 00:02:07,500
99.99% availability and
resilience built into the service

66
00:02:07,500 --> 00:02:09,543
so you don't have to pay extra for it.

67
00:02:10,830 --> 00:02:14,790
And so let's do a recap of
some Lambda fundamentals.

68
00:02:14,790 --> 00:02:16,890
One of the major technology
foundations of Lambda

69
00:02:16,890 --> 00:02:18,570
is the open-source Firecracker,

70
00:02:18,570 --> 00:02:20,520
but now we're actually
expanding the foundation

71
00:02:20,520 --> 00:02:22,710
to include good old EC2.

72
00:02:22,710 --> 00:02:24,990
Lambda Managed Instances,
announced on Sunday,

73
00:02:24,990 --> 00:02:27,160
gives you Lambda's operational simplicity

74
00:02:28,117 --> 00:02:30,090
with EC2's range of instances.

75
00:02:30,090 --> 00:02:32,490
You can choose to use
Graviton or CPU or memory

76
00:02:32,490 --> 00:02:35,160
or network-optimized instances
for your Lambda functions.

77
00:02:35,160 --> 00:02:38,010
And you don't have to manage
any of these instances for you.

78
00:02:38,010 --> 00:02:40,260
You also get up to 12% savings,

79
00:02:40,260 --> 00:02:42,660
12 times savings for AtScale workloads,

80
00:02:42,660 --> 00:02:43,770
and multi-concurrency,

81
00:02:43,770 --> 00:02:46,770
and EC2 pricing incentives,
all part of the package.

82
00:02:46,770 --> 00:02:48,580
There's also a session, CNS382,

83
00:02:48,580 --> 00:02:50,493
which you can find out more about it.

84
00:02:51,930 --> 00:02:54,000
With normal Lambda, so-called on-demand,

85
00:02:54,000 --> 00:02:56,490
everything runs in the Lambda service VPC.

86
00:02:56,490 --> 00:02:58,050
And invokes come in from clients

87
00:02:58,050 --> 00:03:01,260
and ultimately run on EC2
worker hosts owned by Lambda

88
00:03:01,260 --> 00:03:03,510
in the Lambda service VPC.

89
00:03:03,510 --> 00:03:04,920
With Lambda Managed Instances,

90
00:03:04,920 --> 00:03:06,930
the control plane stays
in the Lambda account,

91
00:03:06,930 --> 00:03:10,290
but we provision and manage
EC2 instances in your account

92
00:03:10,290 --> 00:03:13,140
in your VPC and then
route function invokes

93
00:03:13,140 --> 00:03:14,730
to containers running on them.

94
00:03:14,730 --> 00:03:16,650
So Lambda on-demand with Firecracker

95
00:03:16,650 --> 00:03:18,180
and Lambda Managed Instances,

96
00:03:18,180 --> 00:03:19,380
two different compute models,

97
00:03:19,380 --> 00:03:22,980
but the rest of the architecture
basically stays the same.

98
00:03:22,980 --> 00:03:24,360
Lambda's got three invoke types.

99
00:03:24,360 --> 00:03:26,700
Synchronous, when the
caller calls either directly

100
00:03:26,700 --> 00:03:28,170
or via API gateway

101
00:03:28,170 --> 00:03:30,510
and this sends the request
to the Lambda service,

102
00:03:30,510 --> 00:03:32,610
does its processing, waits for a response

103
00:03:32,610 --> 00:03:34,590
before returning it to the client.

104
00:03:34,590 --> 00:03:36,030
When you do an asynchronous request,

105
00:03:36,030 --> 00:03:37,230
either via client call

106
00:03:37,230 --> 00:03:39,690
or maybe something like
an S3 change notification

107
00:03:39,690 --> 00:03:42,360
or an EventBridge rule
matching, sending an event,

108
00:03:42,360 --> 00:03:44,460
you don't wait for a response
from the function code.

109
00:03:44,460 --> 00:03:45,900
You hand the event off to Lambda

110
00:03:45,900 --> 00:03:47,970
and Lambda just handles the rest,

111
00:03:47,970 --> 00:03:49,740
places the event in an internal queue

112
00:03:49,740 --> 00:03:50,727
and the returns are successful.

113
00:03:50,727 --> 00:03:53,520
The response to the call is
saying, "I got your event,"

114
00:03:53,520 --> 00:03:56,490
and then a separate process
goes and processes it.

115
00:03:56,490 --> 00:03:58,290
And event source mapping
is a Lambda resource

116
00:03:58,290 --> 00:04:01,260
that reads items from a
queue like Kinesis, Dynamo

117
00:04:01,260 --> 00:04:04,350
or from a, sorry, from a
stream like Kinesis and Dynamo

118
00:04:04,350 --> 00:04:05,850
or from a queue like SQS

119
00:04:05,850 --> 00:04:08,430
and a producer application
would've put messages onto this.

120
00:04:08,430 --> 00:04:10,620
And then Lambda-managers the pollers,

121
00:04:10,620 --> 00:04:11,820
which we're gonna cover today.

122
00:04:11,820 --> 00:04:12,690
So read the messages

123
00:04:12,690 --> 00:04:15,693
and then send them onto
your invoke for processing.

124
00:04:16,920 --> 00:04:18,600
The Lambda API is the Frontend

125
00:04:18,600 --> 00:04:20,730
where all requests to
the Lambda service land.

126
00:04:20,730 --> 00:04:23,550
And it is multi-AZ load balancer resolving

127
00:04:23,550 --> 00:04:27,210
to Lambda dot whatever region
name dot amazonaws.com.

128
00:04:27,210 --> 00:04:30,030
And then this routes the invoke
request to the data plane

129
00:04:30,030 --> 00:04:32,100
and then ultimately lands
up on a worker host,

130
00:04:32,100 --> 00:04:33,540
yours or ours.

131
00:04:33,540 --> 00:04:34,373
That's it.

132
00:04:34,373 --> 00:04:35,790
That's the journey of events in Lambda.

133
00:04:35,790 --> 00:04:37,680
Rajesh, I think we're done.

134
00:04:37,680 --> 00:04:39,390
Okay. There's obviously a little bit more

135
00:04:39,390 --> 00:04:40,620
which we can talk to you about today.

136
00:04:40,620 --> 00:04:41,763
So we should continue.

137
00:04:42,900 --> 00:04:44,970
The Frontend load balancer distinguishes

138
00:04:44,970 --> 00:04:47,160
control plane request from invoke requests

139
00:04:47,160 --> 00:04:48,960
and these can be function configuration

140
00:04:48,960 --> 00:04:51,060
or resource management from the console,

141
00:04:51,060 --> 00:04:53,610
CLI tools or an SDK.

142
00:04:53,610 --> 00:04:56,100
The data plane requests the route,

143
00:04:56,100 --> 00:04:59,190
then route to the separate
data plane one for Sync Invokes

144
00:04:59,190 --> 00:05:00,600
and one for Async Invokes.

145
00:05:00,600 --> 00:05:01,740
And the Sync Invoke service

146
00:05:01,740 --> 00:05:04,170
is actually technically called
the Frontend Invoke Service

147
00:05:04,170 --> 00:05:07,080
and Async the Event
Invoke Frontend Service.

148
00:05:07,080 --> 00:05:10,110
But you know, naming things
gets a little bit confusing,

149
00:05:10,110 --> 00:05:11,730
so we're just gonna
stick with sync and async

150
00:05:11,730 --> 00:05:13,050
to keep things a little simpler.

151
00:05:13,050 --> 00:05:14,970
And this is actually a deliberate choice

152
00:05:14,970 --> 00:05:16,590
to separate the data planes,

153
00:05:16,590 --> 00:05:18,660
which was built into the
system a few years ago

154
00:05:18,660 --> 00:05:20,850
when basically we had a huge async spike

155
00:05:20,850 --> 00:05:23,130
which overwhelmed the sync service.

156
00:05:23,130 --> 00:05:24,870
And so now we can protect the sync service

157
00:05:24,870 --> 00:05:26,430
from async floods.

158
00:05:26,430 --> 00:05:28,890
The data planes also
fleet of stateless hosts.

159
00:05:28,890 --> 00:05:31,770
Lambda again is an AZ
service, a multi-AZ service,

160
00:05:31,770 --> 00:05:32,820
so you don't have to worry

161
00:05:32,820 --> 00:05:33,690
about load balancing

162
00:05:33,690 --> 00:05:35,610
your functions across multiple AZs.

163
00:05:35,610 --> 00:05:36,993
Lambda handles it for you.

164
00:05:38,220 --> 00:05:39,630
For performance and stability,

165
00:05:39,630 --> 00:05:42,120
the data plane needs to get
information about functions

166
00:05:42,120 --> 00:05:44,220
from the function versions Dynamo table,

167
00:05:44,220 --> 00:05:45,960
which we cache both on the hosts

168
00:05:45,960 --> 00:05:48,420
and also in an L2 AZ cache.

169
00:05:48,420 --> 00:05:50,940
'Cause we wanna make
invokes as fast as possible,

170
00:05:50,940 --> 00:05:52,560
so we wanna avoid having to look up things

171
00:05:52,560 --> 00:05:55,170
on every single invoke and it
also helps with availability

172
00:05:55,170 --> 00:05:57,183
this way of doing it too.

173
00:05:58,200 --> 00:06:02,040
So let's now look at the
synchronous path in more detail.

174
00:06:02,040 --> 00:06:04,020
So the ALB distributes the invoke request

175
00:06:04,020 --> 00:06:06,840
to a fleet of hosts as part
of the sync invoke service.

176
00:06:06,840 --> 00:06:08,250
I'm simplifying the diagram here

177
00:06:08,250 --> 00:06:09,660
as it's gonna get a little crazy,

178
00:06:09,660 --> 00:06:13,080
but Lambda is built to everything
runs across multiple AZs.

179
00:06:13,080 --> 00:06:15,060
So the sync invoke service first performs

180
00:06:15,060 --> 00:06:17,970
some authentication and some
authorization of the request

181
00:06:17,970 --> 00:06:20,340
and this is gonna ensure
that Lambda is secure,

182
00:06:20,340 --> 00:06:21,720
as only authorized callers

183
00:06:21,720 --> 00:06:24,180
are gonna be able to even get
through Lambda's front door.

184
00:06:24,180 --> 00:06:26,100
The service then is
gonna load the metadata

185
00:06:26,100 --> 00:06:27,183
with their request.

186
00:06:28,077 --> 00:06:30,060
The Frontend also calls what's
called a counting service.

187
00:06:30,060 --> 00:06:30,893
This is gonna check

188
00:06:30,893 --> 00:06:33,090
whether any quota limits
may need to be enforced

189
00:06:33,090 --> 00:06:35,550
based on the account
or reserve concurrency.

190
00:06:35,550 --> 00:06:37,920
The counting service is also
optimized for high throughput,

191
00:06:37,920 --> 00:06:40,380
needs to be under one and
a half millisecond latency

192
00:06:40,380 --> 00:06:42,330
as it's called on each invoke.

193
00:06:42,330 --> 00:06:44,220
And as it's critical to the invoke path,

194
00:06:44,220 --> 00:06:46,440
it's also highly available
across multiple AZs

195
00:06:46,440 --> 00:06:47,493
and super fast.

196
00:06:48,390 --> 00:06:49,950
The Frontend then talks to what is called

197
00:06:49,950 --> 00:06:50,790
the assignment service

198
00:06:50,790 --> 00:06:52,890
and also there's a control plane service,

199
00:06:52,890 --> 00:06:55,590
another component which
manages then the coordination

200
00:06:55,590 --> 00:06:57,270
and manages background tasks.

201
00:06:57,270 --> 00:06:58,830
It handles those control plane things

202
00:06:58,830 --> 00:07:00,060
like creating functions

203
00:07:00,060 --> 00:07:01,680
and then also manages the lifecycle

204
00:07:01,680 --> 00:07:03,393
of assignment service nodes.

205
00:07:04,380 --> 00:07:05,610
Back to the assignment service.

206
00:07:05,610 --> 00:07:08,040
So this is gonna then coordinate
the placement service.

207
00:07:08,040 --> 00:07:09,780
And so for the first invoke of a function

208
00:07:09,780 --> 00:07:11,520
and new execution environment needs

209
00:07:11,520 --> 00:07:13,770
to be started on a worker host.

210
00:07:13,770 --> 00:07:14,640
A placement service

211
00:07:14,640 --> 00:07:16,530
is then gonna create an
execution environment

212
00:07:16,530 --> 00:07:19,140
on a worker with a time-based lease.

213
00:07:19,140 --> 00:07:20,640
It's also gonna monitor worker health

214
00:07:20,640 --> 00:07:23,580
and make the call winter
mark a worker as unhealthy.

215
00:07:23,580 --> 00:07:26,100
So once execution environment
is then up and running,

216
00:07:26,100 --> 00:07:27,360
you can see the in it path.

217
00:07:27,360 --> 00:07:29,700
The assignment service is
then gonna give your supplied

218
00:07:29,700 --> 00:07:33,210
I am role and also environment
variables with the privilege,

219
00:07:33,210 --> 00:07:35,160
the I am roles with the
privileges you define,

220
00:07:35,160 --> 00:07:36,660
and then also the environment variables

221
00:07:36,660 --> 00:07:38,716
and give it to the worker.

222
00:07:38,716 --> 00:07:40,140
The execution environment
is then gonna start

223
00:07:40,140 --> 00:07:41,100
the language runtime,

224
00:07:41,100 --> 00:07:42,840
you know whatever it is, no Java, python,

225
00:07:42,840 --> 00:07:44,490
and then download the function code

226
00:07:44,490 --> 00:07:45,930
or run the container image

227
00:07:45,930 --> 00:07:48,390
and then the function in it process runs.

228
00:07:48,390 --> 00:07:50,520
Then the sync invoke
service runs the invoke

229
00:07:50,520 --> 00:07:51,420
and your function runs

230
00:07:51,420 --> 00:07:53,850
and sends the results back to the caller.

231
00:07:53,850 --> 00:07:54,870
To minimize latency,

232
00:07:54,870 --> 00:07:57,870
it's called directly to an
invoke proxy on the worker hosts.

233
00:07:57,870 --> 00:07:59,520
And then when subsequent requests come in,

234
00:07:59,520 --> 00:08:00,900
well the Frontend's gonna check,

235
00:08:00,900 --> 00:08:01,980
talk to the assignment service,

236
00:08:01,980 --> 00:08:04,830
which says yes, we have already
got a execution environment

237
00:08:04,830 --> 00:08:05,663
all up and running,

238
00:08:05,663 --> 00:08:07,530
and he's gonna route the
invoke payload directly

239
00:08:07,530 --> 00:08:09,390
to that accused execution environment

240
00:08:09,390 --> 00:08:11,250
and again, run the handler.

241
00:08:11,250 --> 00:08:13,350
When the lease of the
execution environment is up

242
00:08:13,350 --> 00:08:14,550
or there's some kind of error,

243
00:08:14,550 --> 00:08:16,800
the assignment service is
able to gradually drain

244
00:08:16,800 --> 00:08:18,990
the connections, spin down
the execution environment,

245
00:08:18,990 --> 00:08:20,643
stopping future invokes.

246
00:08:22,050 --> 00:08:23,610
So let's take a little
bit of a deeper look

247
00:08:23,610 --> 00:08:25,560
at some of the Lambda workers.

248
00:08:25,560 --> 00:08:28,710
So a worker host is just a
standard bare metal EC2 instance

249
00:08:28,710 --> 00:08:30,930
in which we place execution environments.

250
00:08:30,930 --> 00:08:32,220
Now to be efficient

251
00:08:32,220 --> 00:08:34,410
and get invokes happening
as fast as possible,

252
00:08:34,410 --> 00:08:37,170
we actually maintain empty MicroVM pools

253
00:08:37,170 --> 00:08:39,810
on the hosts in various
different memory sizes.

254
00:08:39,810 --> 00:08:40,860
When an invoke comes in,

255
00:08:40,860 --> 00:08:42,690
the host management can then allocate

256
00:08:42,690 --> 00:08:45,720
the smallest memory size
to satisfy the request

257
00:08:45,720 --> 00:08:47,730
and then download the code or image.

258
00:08:47,730 --> 00:08:49,200
And this then speeds up things a lot

259
00:08:49,200 --> 00:08:50,310
as it can be a bit of time

260
00:08:50,310 --> 00:08:52,140
to get an execution
environment up and running

261
00:08:52,140 --> 00:08:53,733
and so we do them in advance.

262
00:08:54,720 --> 00:08:55,830
For Firecracker workers,

263
00:08:55,830 --> 00:08:58,530
Firecracker is the process
that sits above the OS

264
00:08:58,530 --> 00:08:59,430
and then manages

265
00:08:59,430 --> 00:09:02,910
the single secure execution
environment for invokes.

266
00:09:02,910 --> 00:09:04,170
Within the execution environment

267
00:09:04,170 --> 00:09:06,540
we maintain some of what we call sandboxes

268
00:09:06,540 --> 00:09:09,420
and these are basically
privileged container name spaces,

269
00:09:09,420 --> 00:09:12,030
one for your function code,
the runtime and extensions

270
00:09:12,030 --> 00:09:13,650
or your container image.

271
00:09:13,650 --> 00:09:15,840
And then we also maintain
a separate managed sandbox

272
00:09:15,840 --> 00:09:17,190
which your code can't access

273
00:09:17,190 --> 00:09:18,300
but allows us to manage

274
00:09:18,300 --> 00:09:20,700
the things inside that
execution environment.

275
00:09:20,700 --> 00:09:21,630
And an interesting thing

276
00:09:21,630 --> 00:09:23,427
is we've actually completely rearchitected

277
00:09:23,427 --> 00:09:24,750
how this works this year

278
00:09:24,750 --> 00:09:27,390
to improve Lambda for how it works today

279
00:09:27,390 --> 00:09:30,090
and also to build even
future functionality.

280
00:09:30,090 --> 00:09:31,890
And it's been rolling out across our fleet

281
00:09:31,890 --> 00:09:32,910
and you actually wouldn't know it

282
00:09:32,910 --> 00:09:34,410
unless you knew where to look.

283
00:09:34,410 --> 00:09:35,880
Trillions of invocations happening

284
00:09:35,880 --> 00:09:38,070
and we're basically
swapping the tires of Lambda

285
00:09:38,070 --> 00:09:40,683
while the car is moving at crazy speeds.

286
00:09:42,060 --> 00:09:44,370
Outside of the sandboxes, we
also run a management agent

287
00:09:44,370 --> 00:09:46,020
for control plane functionality

288
00:09:46,020 --> 00:09:47,670
and also then a telemetry agent

289
00:09:47,670 --> 00:09:48,930
which allows us to get metrics

290
00:09:48,930 --> 00:09:52,410
and logs outta the execution
environment transparently.

291
00:09:52,410 --> 00:09:54,780
The worker host then has a
bunch of other components,

292
00:09:54,780 --> 00:09:57,180
host management coordinates
with other services,

293
00:09:57,180 --> 00:09:59,640
builds and destroys those
execution environments.

294
00:09:59,640 --> 00:10:01,290
The invoke proxy is the direct link

295
00:10:01,290 --> 00:10:02,970
from Frontend to your code.

296
00:10:02,970 --> 00:10:06,330
We also need to store MicroVM
files like runtime binaries

297
00:10:06,330 --> 00:10:07,830
on the host for faster loading

298
00:10:07,830 --> 00:10:10,320
and then we maintain a
cache for container images

299
00:10:10,320 --> 00:10:13,590
and as many things as
we can for performance.

300
00:10:13,590 --> 00:10:15,630
We also have agents that
then need to talk externally

301
00:10:15,630 --> 00:10:17,400
to the container image chunk plane.

302
00:10:17,400 --> 00:10:19,200
This is for downloading container images

303
00:10:19,200 --> 00:10:21,120
and S3 for zip archives

304
00:10:21,120 --> 00:10:23,400
to get your function code into
the execution environment.

305
00:10:23,400 --> 00:10:25,050
And of course this needs
to connect elsewhere

306
00:10:25,050 --> 00:10:27,950
and so there's networking to
connect to the outside world.

307
00:10:28,800 --> 00:10:30,090
For Lambda Managed Instances,

308
00:10:30,090 --> 00:10:31,260
it's actually pretty much the same,

309
00:10:31,260 --> 00:10:32,670
although we are not using firecracker

310
00:10:32,670 --> 00:10:35,040
but container D for the logical separation

311
00:10:35,040 --> 00:10:36,240
and then containers instead

312
00:10:36,240 --> 00:10:38,853
of actually MicroVM
execution environments.

313
00:10:40,020 --> 00:10:42,870
So let's have a look onto async.

314
00:10:42,870 --> 00:10:44,850
So it works in a similar
way to the sync service

315
00:10:44,850 --> 00:10:47,490
but only handling async event requests.

316
00:10:47,490 --> 00:10:49,110
And we've also recently
increased the payload

317
00:10:49,110 --> 00:10:49,943
to one megabyte,

318
00:10:49,943 --> 00:10:52,233
so you can now use it
for even more use cases.

319
00:10:53,370 --> 00:10:55,140
The Frontend then sends the invoke request

320
00:10:55,140 --> 00:10:56,820
to an internal SQS queue

321
00:10:56,820 --> 00:10:58,260
and responds with an acknowledgement.

322
00:10:58,260 --> 00:10:59,610
But they call it to say that Lambda

323
00:10:59,610 --> 00:11:02,250
is going to invoke your
function asynchronously,

324
00:11:02,250 --> 00:11:03,750
we got your request.

325
00:11:03,750 --> 00:11:05,460
Lambda is gonna manage the SQS queues

326
00:11:05,460 --> 00:11:08,070
and you don't actually even
have visibility of them.

327
00:11:08,070 --> 00:11:11,010
We run a number of queues and
then dynamically scale 'em up

328
00:11:11,010 --> 00:11:13,830
and down depending on the load
and the function concurrency.

329
00:11:13,830 --> 00:11:14,910
Some of the queues are shared,

330
00:11:14,910 --> 00:11:17,280
but we also have sent some
events to dedicated queues

331
00:11:17,280 --> 00:11:18,660
to ensure that Lambda can handle

332
00:11:18,660 --> 00:11:21,270
a huge amount of async invokes

333
00:11:21,270 --> 00:11:23,253
with as little latency as possible.

334
00:11:24,570 --> 00:11:26,910
We then have a fleet of poller
instances which we manage,

335
00:11:26,910 --> 00:11:28,560
we're gonna cover that in more detail.

336
00:11:28,560 --> 00:11:30,060
And these pollers read the messages

337
00:11:30,060 --> 00:11:31,227
from the internal SQS queue

338
00:11:31,227 --> 00:11:34,710
and then determine the function
accounts and the payload

339
00:11:34,710 --> 00:11:36,030
and then ultimately it's gonna send

340
00:11:36,030 --> 00:11:37,890
the invoke request synchronously,

341
00:11:37,890 --> 00:11:39,990
again to the sync invoke service.

342
00:11:39,990 --> 00:11:42,810
As you can see, all Lambda
invokes ultimately land up

343
00:11:42,810 --> 00:11:44,550
as a sync invoke.

344
00:11:44,550 --> 00:11:45,383
And the function uses

345
00:11:45,383 --> 00:11:48,150
the same sync pathway
I talked about before,

346
00:11:48,150 --> 00:11:50,250
returns a function response to the poller,

347
00:11:50,250 --> 00:11:52,620
which then deletes the
message from the queue.

348
00:11:52,620 --> 00:11:54,930
If the request is not successful,

349
00:11:54,930 --> 00:11:56,220
the poller then doesn't actually delete

350
00:11:56,220 --> 00:11:57,210
the message from the queue,

351
00:11:57,210 --> 00:11:59,040
and uses the same visibility timeout

352
00:11:59,040 --> 00:12:00,900
as you would with your own SQS queues.

353
00:12:00,900 --> 00:12:02,340
And then the same or another poller

354
00:12:02,340 --> 00:12:04,240
can pick up the message and try again.

355
00:12:05,310 --> 00:12:08,100
You can also configure event
destinations for async invokes

356
00:12:08,100 --> 00:12:10,320
to provide callbacks after the processing,

357
00:12:10,320 --> 00:12:11,970
whether they invokes are successful

358
00:12:11,970 --> 00:12:15,153
or they failed or after all
the retries are exhausted.

359
00:12:16,560 --> 00:12:18,570
Some other control pain
services are involved.

360
00:12:18,570 --> 00:12:19,440
There's the queue manager

361
00:12:19,440 --> 00:12:21,150
which needs to look after the queues,

362
00:12:21,150 --> 00:12:22,830
it's gonna monitor them
if they any backups

363
00:12:22,830 --> 00:12:25,050
and does the creation and
deletion of the new queues.

364
00:12:25,050 --> 00:12:27,300
And then it works and nicely
with the leasing service,

365
00:12:27,300 --> 00:12:28,920
which then manages which pollers

366
00:12:28,920 --> 00:12:30,780
are processing which queues.

367
00:12:30,780 --> 00:12:32,790
And it can also detect when a poller fails

368
00:12:32,790 --> 00:12:35,790
and so that their work can also
be passed to another poller.

369
00:12:37,020 --> 00:12:39,150
So let's look at now
the event source mapping

370
00:12:39,150 --> 00:12:41,313
or the poller invokes.

371
00:12:42,630 --> 00:12:44,970
A producer application is
gonna then put messages

372
00:12:44,970 --> 00:12:48,150
onto the stream or queue asynchronously.

373
00:12:48,150 --> 00:12:50,780
We then run a number of
slightly different pollers

374
00:12:50,780 --> 00:12:52,050
as we have different clients

375
00:12:52,050 --> 00:12:54,114
depending on what the event source is.

376
00:12:54,114 --> 00:12:56,070
And this is a very similar
architecture to async invokes.

377
00:12:56,070 --> 00:12:57,570
From then on the pollers read the message

378
00:12:57,570 --> 00:12:58,680
from the streamer queue,

379
00:12:58,680 --> 00:13:00,030
can also filter them, batch them up

380
00:13:00,030 --> 00:13:02,610
and then send them up to a single payload,

381
00:13:02,610 --> 00:13:04,650
and sends your function synchronously

382
00:13:04,650 --> 00:13:06,960
with the same sync Frontend service.

383
00:13:06,960 --> 00:13:08,490
Remember all Lambda invokes

384
00:13:08,490 --> 00:13:10,440
eventually land up being synchronous.

385
00:13:10,440 --> 00:13:11,273
Then for queues,

386
00:13:11,273 --> 00:13:12,930
the poller can then delete
the message from the queue

387
00:13:12,930 --> 00:13:15,960
when your function
successfully processes them.

388
00:13:15,960 --> 00:13:17,430
Depending on the event source,

389
00:13:17,430 --> 00:13:20,030
you can then send information
again by the invoke

390
00:13:20,030 --> 00:13:23,160
to SQS, SNS, EventBridge
S3, or even Kafka,

391
00:13:23,160 --> 00:13:25,623
for Kafka sources, which
we announced recently.

392
00:13:27,270 --> 00:13:29,040
There are also a number
of control plane services.

393
00:13:29,040 --> 00:13:30,990
Again, state manager stream tracker,

394
00:13:30,990 --> 00:13:32,340
depending on the event source,

395
00:13:32,340 --> 00:13:34,470
manage the pollers,
manage the event sources,

396
00:13:34,470 --> 00:13:37,950
discovers work, and then handle
scaling the poller fleets.

397
00:13:37,950 --> 00:13:40,260
The leasing service is
going assign pollers

398
00:13:40,260 --> 00:13:42,540
to work on a specific
event or streaming source.

399
00:13:42,540 --> 00:13:43,530
If there's a problem, again,

400
00:13:43,530 --> 00:13:45,930
it's gonna then move its
work to another poller.

401
00:13:47,400 --> 00:13:49,680
And there are also
architectural differences,

402
00:13:49,680 --> 00:13:51,750
but the two types of event sources,

403
00:13:51,750 --> 00:13:53,310
queues and streams.

404
00:13:53,310 --> 00:13:55,320
So queues is for
individual task processing

405
00:13:55,320 --> 00:13:57,270
when each message is independent

406
00:13:57,270 --> 00:13:59,430
and messages are deleted
after the processing.

407
00:13:59,430 --> 00:14:02,460
And streams is then for when you're doing,

408
00:14:02,460 --> 00:14:03,870
when you're multiple consumers

409
00:14:03,870 --> 00:14:05,490
maybe need the same kind of data

410
00:14:05,490 --> 00:14:06,990
and order particularly matters.

411
00:14:06,990 --> 00:14:09,420
And then messages are gonna
be retried for replay,

412
00:14:09,420 --> 00:14:11,970
two different architectural styles.

413
00:14:11,970 --> 00:14:14,580
And the Lambda ESM, provides
a bunch of functionality

414
00:14:14,580 --> 00:14:16,470
across all of these event sources.

415
00:14:16,470 --> 00:14:19,020
We've got filtering, batch
controls, including for streams,

416
00:14:19,020 --> 00:14:21,420
being able to split a batch
to find a faulty record.

417
00:14:21,420 --> 00:14:23,370
We've got choosing where
to start in a stream,

418
00:14:23,370 --> 00:14:26,100
retry and failure handling
analytics for Kinesis

419
00:14:26,100 --> 00:14:28,950
and some platform performance
configuration options.

420
00:14:28,950 --> 00:14:31,080
And the cool thing is
the Lambda ESM handles

421
00:14:31,080 --> 00:14:32,040
this all for you.

422
00:14:32,040 --> 00:14:34,380
You don't need to think about
the different characteristics

423
00:14:34,380 --> 00:14:36,360
beyond just configuring your ESM.

424
00:14:36,360 --> 00:14:39,210
We take on the task of making
everything actually happen.

425
00:14:40,470 --> 00:14:42,090
So let's look a little deeper at the ESM

426
00:14:42,090 --> 00:14:44,970
and see how it works with
both streaming and queuing.

427
00:14:44,970 --> 00:14:46,470
The ESM pulls the event source,

428
00:14:46,470 --> 00:14:48,900
the cure stream for available records.

429
00:14:48,900 --> 00:14:49,860
Now, so we need to think

430
00:14:49,860 --> 00:14:52,384
about even getting actually
to the event source.

431
00:14:52,384 --> 00:14:55,110
SQS and Kinesis are on public endpoints,

432
00:14:55,110 --> 00:14:57,420
but we also have events
sources that are connected

433
00:14:57,420 --> 00:15:01,110
to private subnets or even
outside AWS often with Kafka.

434
00:15:01,110 --> 00:15:02,580
So we need to handle that as well.

435
00:15:02,580 --> 00:15:04,200
As well as the actual functions

436
00:15:04,200 --> 00:15:05,943
which could be connected to a VPC.

437
00:15:07,020 --> 00:15:09,570
Authentication depends
on the event source.

438
00:15:09,570 --> 00:15:10,920
AWS ones we can use IAM,

439
00:15:10,920 --> 00:15:14,520
but for Kafka we want to support
all the possible options,

440
00:15:14,520 --> 00:15:15,540
authorization methods,

441
00:15:15,540 --> 00:15:18,270
so the ESM can actually authenticate.

442
00:15:18,270 --> 00:15:20,430
If there's ordering things like SQS FIFO

443
00:15:20,430 --> 00:15:21,360
or streaming sources,

444
00:15:21,360 --> 00:15:23,760
we always need to ensure that
the ordering is maintained

445
00:15:23,760 --> 00:15:25,860
with the message processing.

446
00:15:25,860 --> 00:15:28,230
For streams, you can configure
the starting position

447
00:15:28,230 --> 00:15:29,730
as part of the ESM config,

448
00:15:29,730 --> 00:15:31,680
from the beginning, from
a particular timestamped

449
00:15:31,680 --> 00:15:33,810
or just getting from the latest records.

450
00:15:33,810 --> 00:15:35,460
And the ESM also may needs to remember

451
00:15:35,460 --> 00:15:36,750
where it was in the stream,

452
00:15:36,750 --> 00:15:38,793
using the offset or sequence number.

453
00:15:40,140 --> 00:15:43,050
We need to handle retries or resharding.

454
00:15:43,050 --> 00:15:44,280
Streams can grow or shrink

455
00:15:44,280 --> 00:15:46,350
and so we need to continually
look at the source

456
00:15:46,350 --> 00:15:48,750
and see if there is maybe
a new Kafka broker online

457
00:15:48,750 --> 00:15:50,940
or new shards or partitions to process

458
00:15:50,940 --> 00:15:52,920
from one of the event sources.

459
00:15:52,920 --> 00:15:55,290
The parent shard is split
creating a new child shard

460
00:15:55,290 --> 00:15:58,050
and we need to maintain
ordering during this process

461
00:15:58,050 --> 00:15:59,370
so we can send the right records

462
00:15:59,370 --> 00:16:00,450
to new Lambda functions

463
00:16:00,450 --> 00:16:02,073
to then scale up the processing.

464
00:16:03,300 --> 00:16:04,133
For Kinesis,

465
00:16:04,133 --> 00:16:05,190
there's a helpful additional setting

466
00:16:05,190 --> 00:16:06,240
called tumbling windows,

467
00:16:06,240 --> 00:16:08,040
which you can use for data aggregation.

468
00:16:08,040 --> 00:16:08,873
You can pass the state

469
00:16:08,873 --> 00:16:10,860
across multiple separate Lambda invokes

470
00:16:10,860 --> 00:16:12,450
to aggregate data in real time.

471
00:16:12,450 --> 00:16:13,950
Sort of quick and easy alternative way

472
00:16:13,950 --> 00:16:15,723
for some analytic solutions.

473
00:16:16,650 --> 00:16:18,990
There are also two ways
to consume from Kinesis.

474
00:16:18,990 --> 00:16:21,630
You've got shared fan art
where you have to have up

475
00:16:21,630 --> 00:16:24,810
to five consumers can share and
read throughput of a stream.

476
00:16:24,810 --> 00:16:27,870
And then each shard supports
a read rate and a data rate,

477
00:16:27,870 --> 00:16:31,320
and this has got a latency
of about 200 milliseconds.

478
00:16:31,320 --> 00:16:33,480
But for faster responses to stream data,

479
00:16:33,480 --> 00:16:37,050
you can actually set up Kinesis
enhanced fan-out or EFO.

480
00:16:37,050 --> 00:16:38,490
This actually changes the flow

481
00:16:38,490 --> 00:16:40,590
from the ESM pulling from Kinesis

482
00:16:40,590 --> 00:16:44,640
to Kinesis sending to the ESM
using an HTTP push mechanism

483
00:16:44,640 --> 00:16:47,160
to get to the data to the ESM faster

484
00:16:47,160 --> 00:16:49,020
as low as 70 milliseconds.

485
00:16:49,020 --> 00:16:51,090
So the ESM figures this out,

486
00:16:51,090 --> 00:16:53,040
manages the change in logic flow

487
00:16:53,040 --> 00:16:54,810
and this is just handled transparently,

488
00:16:54,810 --> 00:16:57,173
something you don't have
to look after yourself.

489
00:16:58,050 --> 00:17:00,420
There's also dedicated
poller offering for SQS

490
00:17:00,420 --> 00:17:01,500
and recent for Kafka

491
00:17:01,500 --> 00:17:03,900
and recently actually SQS
where we can also provision

492
00:17:03,900 --> 00:17:06,813
of pollers in advance to
be able to handle spikes.

493
00:17:07,950 --> 00:17:09,420
You can configure various settings,

494
00:17:09,420 --> 00:17:11,190
mainly the minimum and maximum pollers

495
00:17:11,190 --> 00:17:13,020
to be able to control the throughput,

496
00:17:13,020 --> 00:17:15,420
and this can dramatically
increase your polling, scaling

497
00:17:15,420 --> 00:17:16,560
and throughput.

498
00:17:16,560 --> 00:17:19,980
16 times for SQS up to
20,000 concurrency compared

499
00:17:19,980 --> 00:17:23,010
to the standard polling of 1250 for SQS.

500
00:17:23,010 --> 00:17:24,900
Kafka also simplifies the networking

501
00:17:24,900 --> 00:17:27,030
and the cost models when you enable this.

502
00:17:27,030 --> 00:17:29,310
And there is an additional
charge to be able to do this

503
00:17:29,310 --> 00:17:31,710
and it's based on an event processing unit

504
00:17:31,710 --> 00:17:33,180
which you can manage.

505
00:17:33,180 --> 00:17:36,180
And so provision mode is
great for high traffic sources

506
00:17:36,180 --> 00:17:37,290
where you want low latency

507
00:17:37,290 --> 00:17:40,683
and also this can save you some
polling money at big scale.

508
00:17:42,060 --> 00:17:43,800
And provision mode allows us to build

509
00:17:43,800 --> 00:17:45,960
and bring new functionality
to the pollers,

510
00:17:45,960 --> 00:17:47,490
like supporting schema registries

511
00:17:47,490 --> 00:17:49,260
and allowing efficient binary formats

512
00:17:49,260 --> 00:17:50,823
like Avro and Protobuf.

513
00:17:52,770 --> 00:17:55,140
Okay, so we have done the polling,

514
00:17:55,140 --> 00:17:57,150
then the filtering allows
you to dump records,

515
00:17:57,150 --> 00:17:58,830
you don't want to process, why?

516
00:17:58,830 --> 00:18:00,885
Well, filtering is a CPU intensive process

517
00:18:00,885 --> 00:18:02,040
and you don't really want to do that

518
00:18:02,040 --> 00:18:03,930
and waste Lambda invokes

519
00:18:03,930 --> 00:18:05,520
in your code.

520
00:18:05,520 --> 00:18:07,770
So you can have positive
filtering to include messages

521
00:18:07,770 --> 00:18:08,790
that you do wanna include

522
00:18:08,790 --> 00:18:10,560
or negative to exclude or messages

523
00:18:10,560 --> 00:18:13,830
that don't match a
particular specific criteria.

524
00:18:13,830 --> 00:18:16,470
This is very flexible
using the same syntax

525
00:18:16,470 --> 00:18:17,790
as EventBridge filtering.

526
00:18:17,790 --> 00:18:20,310
And here in this example
we are processing messages

527
00:18:20,310 --> 00:18:22,500
for the tire pressure less than 32

528
00:18:22,500 --> 00:18:24,550
to send that only those to your function.

529
00:18:25,470 --> 00:18:27,226
Next up is batching.

530
00:18:27,226 --> 00:18:28,470
For batching actually the records

531
00:18:28,470 --> 00:18:30,900
for grouping the records
for efficient processing,

532
00:18:30,900 --> 00:18:32,280
and this means you can control

533
00:18:32,280 --> 00:18:34,030
your Lambda invokes for efficiency.

534
00:18:35,100 --> 00:18:36,240
So how are batches defined?

535
00:18:36,240 --> 00:18:37,620
Well you get to configure this

536
00:18:37,620 --> 00:18:39,720
and control this in a
number of three ways.

537
00:18:39,720 --> 00:18:41,070
You can configure the size

538
00:18:41,070 --> 00:18:43,230
and this is one up to a default of 10,000.

539
00:18:43,230 --> 00:18:46,620
We normally start at 10, the
number of items in the batch,

540
00:18:46,620 --> 00:18:48,360
but yeah up to 10,000 that is really big.

541
00:18:48,360 --> 00:18:50,820
And then the batch window is
to help improve efficiency

542
00:18:50,820 --> 00:18:52,860
when you don't actually have much traffic.

543
00:18:52,860 --> 00:18:54,690
So you can at least process messages

544
00:18:54,690 --> 00:18:56,520
before whole batch is formed.

545
00:18:56,520 --> 00:18:57,450
And you need to also stay

546
00:18:57,450 --> 00:18:59,523
within the Lambda six meg payload limit.

547
00:19:00,900 --> 00:19:02,130
So once there is a batch,

548
00:19:02,130 --> 00:19:03,780
we then need to invoke the function

549
00:19:03,780 --> 00:19:05,523
via the sync control plane.

550
00:19:06,450 --> 00:19:08,700
So we need to then
consider different scaling

551
00:19:08,700 --> 00:19:10,230
for the different architectures.

552
00:19:10,230 --> 00:19:13,080
Streaming isn't actually
an upper bound problem.

553
00:19:13,080 --> 00:19:15,060
You wanna process data
with a stream generally

554
00:19:15,060 --> 00:19:16,530
as fast as possible.

555
00:19:16,530 --> 00:19:19,590
And so there's a maximum
throughput you want to factor in.

556
00:19:19,590 --> 00:19:22,020
So with Kinesis and Kafka
streams you're trying to consume

557
00:19:22,020 --> 00:19:25,530
and process messages at
the highest rate possible

558
00:19:25,530 --> 00:19:27,930
to keep up with the incoming data,

559
00:19:27,930 --> 00:19:29,250
and to minimize the lag.

560
00:19:29,250 --> 00:19:32,100
And the challenge is
in scaling up to exceed

561
00:19:32,100 --> 00:19:35,433
or meet at least or possibly
exceed the ingestion rate.

562
00:19:36,390 --> 00:19:37,223
But queuing is different.

563
00:19:37,223 --> 00:19:39,090
This is actually a lower bound problem.

564
00:19:39,090 --> 00:19:41,010
You do want to process data efficiently,

565
00:19:41,010 --> 00:19:42,000
but you also want to ensure

566
00:19:42,000 --> 00:19:44,460
you're not actually
overwhelming downstream services

567
00:19:44,460 --> 00:19:47,550
like you know APIs or databases.

568
00:19:47,550 --> 00:19:50,280
So with SQS the queue is
actually acting as a buffer

569
00:19:50,280 --> 00:19:52,830
or a shock absorber and you
are then controlling the rate

570
00:19:52,830 --> 00:19:53,850
where you wanna drain the queue

571
00:19:53,850 --> 00:19:56,070
to then protect those
downstream resources.

572
00:19:56,070 --> 00:19:58,050
And the challenge is then maintaining

573
00:19:58,050 --> 00:19:59,880
a minimum safe processing rate,

574
00:19:59,880 --> 00:20:01,950
that is gonna respect
those downstream resources.

575
00:20:01,950 --> 00:20:04,200
So two different architectural styles.

576
00:20:04,200 --> 00:20:05,700
And so for SQS to do this,

577
00:20:05,700 --> 00:20:07,290
we want to manage the flow control,

578
00:20:07,290 --> 00:20:09,750
the rate at which Lambda
consumes the messages.

579
00:20:09,750 --> 00:20:12,540
And so you can configure the
max concurrency on the ESM

580
00:20:12,540 --> 00:20:14,590
to control how many concurrent executions

581
00:20:15,649 --> 00:20:18,510
the ESM is then gonna
attempt to send to Lambda,

582
00:20:18,510 --> 00:20:21,180
to prevent overwhelming
downstream services.

583
00:20:21,180 --> 00:20:22,290
And this is actually

584
00:20:22,290 --> 00:20:24,450
the all important buffering control.

585
00:20:24,450 --> 00:20:26,400
Reserve concurrency is
in a separate setting

586
00:20:26,400 --> 00:20:27,233
on the Lambda function,

587
00:20:27,233 --> 00:20:30,480
which you set to reserve
capacity from the function.

588
00:20:30,480 --> 00:20:32,790
And this ensures it can
scale up with needed

589
00:20:32,790 --> 00:20:35,370
within the available account concurrency.

590
00:20:35,370 --> 00:20:38,666
So you actually rather wanna
use the max concurrency

591
00:20:38,666 --> 00:20:40,020
to manage the buffer and the flow,

592
00:20:40,020 --> 00:20:42,030
but you can also use both together.

593
00:20:42,030 --> 00:20:43,290
And if you do, you wanna make sure

594
00:20:43,290 --> 00:20:44,430
that the reserve concurrency

595
00:20:44,430 --> 00:20:46,230
is higher than the maximum concurrency

596
00:20:46,230 --> 00:20:47,913
to prevent any throttling.

597
00:20:49,590 --> 00:20:51,390
So for also streams we need to scale up

598
00:20:51,390 --> 00:20:52,560
how the stream scales.

599
00:20:52,560 --> 00:20:53,550
This is automatically

600
00:20:53,550 --> 00:20:56,070
the ESM just needs to figure this out.

601
00:20:56,070 --> 00:20:57,870
Kinesis scales by adding shards

602
00:20:57,870 --> 00:20:59,520
and Kafka by adding partitions

603
00:20:59,520 --> 00:21:01,980
and Lambda is automatically
gonna figure this all out

604
00:21:01,980 --> 00:21:03,840
and is gonna scale up the consumer pollers

605
00:21:03,840 --> 00:21:05,460
to match their incoming throughput.

606
00:21:05,460 --> 00:21:08,100
Still with message ordering
in each partition or shard,

607
00:21:08,100 --> 00:21:10,863
more shards or partitions
means more processing.

608
00:21:12,480 --> 00:21:14,910
But customers wanted even
more processing for Kinesis.

609
00:21:14,910 --> 00:21:17,700
So we came up with what's
called parallelization factor.

610
00:21:17,700 --> 00:21:20,730
By default it's one function
per individual shard,

611
00:21:20,730 --> 00:21:23,610
but you can scale up to 10
for massive Lambda throughput,

612
00:21:23,610 --> 00:21:25,200
still maintaining the order.

613
00:21:25,200 --> 00:21:27,600
And this is something spills
built specifically for Lambda,

614
00:21:27,600 --> 00:21:29,500
for high throughput stream processing.

615
00:21:30,900 --> 00:21:32,460
For error handling, streams and queues

616
00:21:32,460 --> 00:21:34,020
also have different semantics.

617
00:21:34,020 --> 00:21:36,750
Streams need to produce
or preserve the ordering,

618
00:21:36,750 --> 00:21:38,460
but you actually need
to figure out and decide

619
00:21:38,460 --> 00:21:40,260
what to do with the poison pill message.

620
00:21:40,260 --> 00:21:42,450
Should that actually stop processing?

621
00:21:42,450 --> 00:21:43,860
If it's something like log data

622
00:21:43,860 --> 00:21:45,210
or bank transactions, you know,

623
00:21:45,210 --> 00:21:47,400
absolutely you wanna
stop the steam processing

624
00:21:47,400 --> 00:21:50,070
and you wanna fix the problem
before continuing processing,

625
00:21:50,070 --> 00:21:52,560
but maybe force, you know,
some frequent sensor data

626
00:21:52,560 --> 00:21:56,340
or I don't know, you know
GPS data from a higher car

627
00:21:56,340 --> 00:21:57,720
you can handle some of that data loss

628
00:21:57,720 --> 00:21:58,980
'cause you know, in a few seconds time

629
00:21:58,980 --> 00:22:00,210
the next result's gonna come in

630
00:22:00,210 --> 00:22:02,310
and you can still continue shortly.

631
00:22:02,310 --> 00:22:03,270
So in this scenario,

632
00:22:03,270 --> 00:22:05,190
you don't actually want to stop
the stream because you want

633
00:22:05,190 --> 00:22:07,710
to keep the message processing going.

634
00:22:07,710 --> 00:22:09,660
And then for queues,
generally you don't wanna stop

635
00:22:09,660 --> 00:22:11,490
for a failed message
and you wanna continue

636
00:22:11,490 --> 00:22:12,903
to process the rest.

637
00:22:13,860 --> 00:22:15,480
And

638
00:22:15,480 --> 00:22:18,300
we then have two batch
error handling things

639
00:22:18,300 --> 00:22:19,800
to give you some more control.

640
00:22:20,851 --> 00:22:21,684
Partial batch response

641
00:22:21,684 --> 00:22:23,130
is when you know the failed record

642
00:22:23,130 --> 00:22:25,650
and you're a container
of successful response

643
00:22:25,650 --> 00:22:28,380
and then tell the ESM
which message has failed

644
00:22:28,380 --> 00:22:30,240
and then that's gonna go to be retried

645
00:22:30,240 --> 00:22:31,560
and processing continue.

646
00:22:31,560 --> 00:22:35,010
And AWS power tools for a
WS Lambda is a great utility

647
00:22:35,010 --> 00:22:36,420
for using for lots of languages

648
00:22:36,420 --> 00:22:40,020
and that's got a batch processing
utility to help with this.

649
00:22:40,020 --> 00:22:42,660
And for streams, bisecting
batches can be super useful

650
00:22:42,660 --> 00:22:44,310
when you don't handle the error

651
00:22:44,310 --> 00:22:45,750
and it's not something you normally need

652
00:22:45,750 --> 00:22:47,100
to consider for queues.

653
00:22:47,100 --> 00:22:49,290
So in this example,
message three is a bad one,

654
00:22:49,290 --> 00:22:50,760
although we wouldn't actually know it yet.

655
00:22:50,760 --> 00:22:52,170
And so we process the batch

656
00:22:52,170 --> 00:22:53,610
and get a failed response.

657
00:22:53,610 --> 00:22:55,440
Instead of end endlessly
retrying the same batch.

658
00:22:55,440 --> 00:22:57,780
The ESM can then split the
batch, try the first half

659
00:22:57,780 --> 00:23:01,170
to keep ordering and try to
get more messages processed.

660
00:23:01,170 --> 00:23:03,780
The split batch fails again
as it has the faulty message.

661
00:23:03,780 --> 00:23:06,270
And so the ESM splits again and we now try

662
00:23:06,270 --> 00:23:08,280
and process the new batches.

663
00:23:08,280 --> 00:23:10,950
This time the first batch
processes successfully

664
00:23:10,950 --> 00:23:13,530
and the single batch for the
poison message fails again.

665
00:23:13,530 --> 00:23:16,080
And we can then continue
with the original second half

666
00:23:16,080 --> 00:23:18,030
of the first batch to
keep up that ordering

667
00:23:18,030 --> 00:23:19,920
of the successful messages.

668
00:23:19,920 --> 00:23:21,960
And then can fully
process as many messages

669
00:23:21,960 --> 00:23:24,570
as possible without having
to reject a whole batch

670
00:23:24,570 --> 00:23:27,150
and then deal with a faulty
message if we want to,

671
00:23:27,150 --> 00:23:29,790
really efficient way that you can control.

672
00:23:29,790 --> 00:23:33,270
And then we need to
handle function errors.

673
00:23:33,270 --> 00:23:36,754
So we need to configure
Lambda on failure destinations

674
00:23:36,754 --> 00:23:38,639
for the function in vogue issues.

675
00:23:38,639 --> 00:23:39,472
And then you might think,

676
00:23:39,472 --> 00:23:41,490
well why are there actually
two different error

677
00:23:41,490 --> 00:23:43,260
handing parts for SQS?

678
00:23:43,260 --> 00:23:45,750
Well the SQS deal queue
is gonna capture messages

679
00:23:45,750 --> 00:23:48,900
that fail repeatedly during the polling

680
00:23:48,900 --> 00:23:49,980
or the processing,

681
00:23:49,980 --> 00:23:51,570
while the on failure destinations

682
00:23:51,570 --> 00:23:54,570
is gonna capture invoke errors
with your Lambda function.

683
00:23:54,570 --> 00:23:57,480
And this could be things like
network issues or throttling

684
00:23:57,480 --> 00:23:58,800
or maybe you've deleted the function

685
00:23:58,800 --> 00:24:00,690
or there's a function permission issue.

686
00:24:00,690 --> 00:24:02,760
And so both serve different processes

687
00:24:02,760 --> 00:24:05,660
and can be used together for
comprehensive error handling.

688
00:24:06,720 --> 00:24:08,790
And so that takes us through
the Lambda fundamentals.

689
00:24:08,790 --> 00:24:11,580
We've covered sync,
async, workers, pollers,

690
00:24:11,580 --> 00:24:13,680
and all the features of
the event source mapping.

691
00:24:13,680 --> 00:24:16,500
Again, it just looks like
a configuration option,

692
00:24:16,500 --> 00:24:17,430
but Lambda underneath

693
00:24:17,430 --> 00:24:19,140
the hood is doing a huge amount of work,

694
00:24:19,140 --> 00:24:20,730
doing some pulling, doing some pushing,

695
00:24:20,730 --> 00:24:24,180
doing some pulling, you know,
being as efficient as possible

696
00:24:24,180 --> 00:24:25,470
to be able to get the messages

697
00:24:25,470 --> 00:24:27,330
from the different
architectures of your streams

698
00:24:27,330 --> 00:24:28,740
and your queues.

699
00:24:28,740 --> 00:24:30,690
So let me now hand back over to Rajesh

700
00:24:30,690 --> 00:24:32,550
who's gonna dive deeper into the pollers

701
00:24:32,550 --> 00:24:35,010
and talk about some of the lessons

702
00:24:35,010 --> 00:24:37,253
we learned about when
we were building Lambda.

703
00:24:38,760 --> 00:24:39,593
- Okay.

704
00:24:41,249 --> 00:24:42,082
(audience clapping)

705
00:24:42,082 --> 00:24:43,073
- Hello.

706
00:24:43,073 --> 00:24:43,920
(audience clapping)

707
00:24:43,920 --> 00:24:45,132
- Thanks Julian.

708
00:24:45,132 --> 00:24:45,965
(Rajesh coughing)

709
00:24:45,965 --> 00:24:47,715
Yeah, the cold start.

710
00:24:48,990 --> 00:24:52,350
Yeah, Lambda that cold
start, you know, yeah.

711
00:24:52,350 --> 00:24:53,183
Hi.

712
00:24:53,183 --> 00:24:55,413
Hello everyone again, this is Rajesh.

713
00:24:56,460 --> 00:25:00,960
Today I'll be talking about,

714
00:25:00,960 --> 00:25:03,390
let me give you the overview
of like I have divided

715
00:25:03,390 --> 00:25:05,010
this segment of the talk into two parts.

716
00:25:05,010 --> 00:25:06,660
One is like I'll walk you
through the mental model

717
00:25:06,660 --> 00:25:07,830
of like some of the mental models

718
00:25:07,830 --> 00:25:09,810
that we have used while designing Lambda

719
00:25:09,810 --> 00:25:13,050
and the operational complexity
in that comes with it.

720
00:25:13,050 --> 00:25:18,030
And how that complexity actually
fueled a lot of innovation

721
00:25:18,030 --> 00:25:20,040
that we have baked into Lambda design.

722
00:25:20,040 --> 00:25:22,190
So let's start with
the mental model first.

723
00:25:23,310 --> 00:25:25,410
Last time we stood here

724
00:25:25,410 --> 00:25:27,930
and convinced you that
like Lambda in some ways

725
00:25:27,930 --> 00:25:29,400
is a storage service,

726
00:25:29,400 --> 00:25:31,530
and applies a lot of learnings
from building services

727
00:25:31,530 --> 00:25:35,340
like EBS formulation service like EBS.

728
00:25:35,340 --> 00:25:37,170
And this year I want to convince you

729
00:25:37,170 --> 00:25:39,540
that Lambda is also a queuing service

730
00:25:39,540 --> 00:25:41,850
and like how we have applied a lot

731
00:25:41,850 --> 00:25:43,710
of learnings from
building queuing services

732
00:25:43,710 --> 00:25:46,860
and a queuing theory concepts.

733
00:25:46,860 --> 00:25:48,630
I know compute as a queuing service

734
00:25:48,630 --> 00:25:50,520
seems a bit counterintuitive,

735
00:25:50,520 --> 00:25:52,497
but let me explain what
I actually mean by that.

736
00:25:52,497 --> 00:25:54,810
And I'll walk you through
like the thought process

737
00:25:54,810 --> 00:25:57,150
of designing this Lambda service.

738
00:25:57,150 --> 00:25:58,890
So what's the queue service?

739
00:25:58,890 --> 00:26:00,390
Let's start with that.

740
00:26:00,390 --> 00:26:02,580
I started with Amazon back in 2013,

741
00:26:02,580 --> 00:26:05,190
so it's been more than a decade.

742
00:26:05,190 --> 00:26:07,590
And the first thing that I did
was build a queue processor

743
00:26:07,590 --> 00:26:10,470
and basically a key processor
is there's a message buffer

744
00:26:10,470 --> 00:26:13,500
where a lot of events
and messages are stored

745
00:26:13,500 --> 00:26:16,260
and there is a worker
which actually pulls.

746
00:26:16,260 --> 00:26:18,900
So you write a way to deal with the queue

747
00:26:18,900 --> 00:26:21,450
and then you have a business
logic that you process

748
00:26:22,590 --> 00:26:24,540
your messages and do something with it.

749
00:26:25,740 --> 00:26:29,340
And for that I had to
build a lot of simulations

750
00:26:29,340 --> 00:26:32,700
and during I had to build
a lot of simulations

751
00:26:32,700 --> 00:26:36,376
to like go through model my
upstreams model, my downstreams,

752
00:26:36,376 --> 00:26:37,500
what are the failure points

753
00:26:37,500 --> 00:26:40,830
and like what would it take
to build a resilient service.

754
00:26:40,830 --> 00:26:43,260
Back then Lambda didn't existed,

755
00:26:43,260 --> 00:26:44,820
so I had to do a lot
of like heavy lifting,

756
00:26:44,820 --> 00:26:48,723
and I had to build the key
processor from scratch on my own.

757
00:26:49,590 --> 00:26:54,218
And when I joined Lambda
that is 2018, '19 time,

758
00:26:54,218 --> 00:26:56,700
I realized that the same
pattern shows up everywhere.

759
00:26:56,700 --> 00:26:58,650
Many of the features that we built

760
00:26:58,650 --> 00:27:01,410
and many of the things Julian
just walked you through

761
00:27:01,410 --> 00:27:04,560
are inspired by that same analysis.

762
00:27:04,560 --> 00:27:08,910
I've been doing this since 2013
as I told you over a decade.

763
00:27:08,910 --> 00:27:10,650
So let me quickly walk you
through the queuing theory.

764
00:27:10,650 --> 00:27:11,730
So that will set the foundation

765
00:27:11,730 --> 00:27:13,773
for the next of the rest of the session.

766
00:27:15,240 --> 00:27:17,700
Queuing theory is nothing fancy

767
00:27:17,700 --> 00:27:20,070
but just basically study
of the waiting lines,

768
00:27:20,070 --> 00:27:24,240
how things arrive and
wait and get processed

769
00:27:24,240 --> 00:27:25,590
and leave.

770
00:27:25,590 --> 00:27:27,240
This diagram is basic.

771
00:27:27,240 --> 00:27:29,880
There's a basic shape
of every queuing system,

772
00:27:29,880 --> 00:27:32,400
you have probably you see,

773
00:27:32,400 --> 00:27:34,860
probably you have seen some version of it,

774
00:27:34,860 --> 00:27:37,530
events arrive at some rate and
that rate is never defined.

775
00:27:37,530 --> 00:27:41,490
Like some customers go crazy,
they spike, some are like slow

776
00:27:41,490 --> 00:27:44,943
and has bounded key arrival pattern.

777
00:27:46,470 --> 00:27:48,300
On the right side, what
you see is a worker,

778
00:27:48,300 --> 00:27:50,970
their speed is the service
rate just like arrivals,

779
00:27:50,970 --> 00:27:53,700
this isn't constant like
sometimes your data,

780
00:27:53,700 --> 00:27:55,687
your dependency can take more than

781
00:27:55,687 --> 00:27:57,960
if it is taking like 10 milliseconds.

782
00:27:57,960 --> 00:27:59,430
Sometimes it can take like milliseconds.

783
00:27:59,430 --> 00:28:01,610
So like this is also not like well done.

784
00:28:01,610 --> 00:28:04,170
So you have to model
different things around it.

785
00:28:04,170 --> 00:28:06,780
And next what you see here
is like the buffer bust

786
00:28:06,780 --> 00:28:08,160
and variability.

787
00:28:08,160 --> 00:28:11,700
The variance is when you,

788
00:28:11,700 --> 00:28:14,220
you are building a queuing
service there, there comes a lot

789
00:28:14,220 --> 00:28:16,680
of like variance in terms
of if there's a spike

790
00:28:16,680 --> 00:28:17,730
or if there's a threat issue

791
00:28:17,730 --> 00:28:21,310
or if your services are not,
not able to like hand up, like

792
00:28:22,440 --> 00:28:25,110
deal with the process,
deal with the workload,

793
00:28:25,110 --> 00:28:26,310
then it may run into some issues.

794
00:28:26,310 --> 00:28:28,263
And that vari is a simple one,

795
00:28:29,100 --> 00:28:30,960
variance into the system can lead

796
00:28:30,960 --> 00:28:33,360
to build up of the backlog.

797
00:28:33,360 --> 00:28:35,940
So I'll talk more about it
when we go into like specific

798
00:28:35,940 --> 00:28:36,903
sections of this.

799
00:28:38,040 --> 00:28:39,600
So let's, let's look at the lessons

800
00:28:39,600 --> 00:28:41,310
that Lambda has learned
from the Queuing theory

801
00:28:41,310 --> 00:28:43,533
and like how it applied it.

802
00:28:44,880 --> 00:28:48,210
The first things I want to
talk about is the lesson

803
00:28:48,210 --> 00:28:49,170
that we learned from Queuing Theory

804
00:28:49,170 --> 00:28:51,960
is like buffers is smooth variance,

805
00:28:51,960 --> 00:28:53,520
arrivals are never uniform.

806
00:28:53,520 --> 00:28:55,560
They are busty, they spike,

807
00:28:55,560 --> 00:28:57,420
they're unpredictable,

808
00:28:57,420 --> 00:29:00,720
and so we don't try to
handle them right away.

809
00:29:00,720 --> 00:29:02,130
That's impossible and expensive.

810
00:29:02,130 --> 00:29:03,933
You can always provision for,

811
00:29:05,611 --> 00:29:06,444
like the peak,

812
00:29:06,444 --> 00:29:09,663
but then you would end up
like paying a lot more for it.

813
00:29:10,500 --> 00:29:11,670
The second lesson

814
00:29:11,670 --> 00:29:14,160
is like we specialized
workers for workload types.

815
00:29:14,160 --> 00:29:16,890
And I'll talk about like
how this led to design

816
00:29:16,890 --> 00:29:18,570
of like how Lambda,

817
00:29:18,570 --> 00:29:20,490
in terms of like how we separated pollers

818
00:29:20,490 --> 00:29:22,770
from like the Lambda execution environment

819
00:29:22,770 --> 00:29:25,020
and why the nature of different poller,

820
00:29:25,020 --> 00:29:27,390
the poller workers different,

821
00:29:27,390 --> 00:29:29,820
than the execution environment.

822
00:29:29,820 --> 00:29:31,710
And the third is the control variance

823
00:29:31,710 --> 00:29:33,003
to prevent instability.

824
00:29:33,900 --> 00:29:36,090
Even if an average arrival rate

825
00:29:36,090 --> 00:29:38,100
is less than average service rate,

826
00:29:38,100 --> 00:29:41,280
variance can cause temporary,

827
00:29:41,280 --> 00:29:45,000
very instability and that
variance skill system.

828
00:29:45,000 --> 00:29:47,490
And the fourth is like
coordinate shared resources

829
00:29:47,490 --> 00:29:49,590
through centralized control.

830
00:29:49,590 --> 00:29:51,480
In Queuing theory models,

831
00:29:51,480 --> 00:29:54,540
the work conservation
means if there's a work

832
00:29:54,540 --> 00:29:56,310
and if there's a server.

833
00:29:56,310 --> 00:30:00,060
So work servers should
never sit idle basically,

834
00:30:00,060 --> 00:30:03,300
but theory assumes there is
always exist a global state.

835
00:30:03,300 --> 00:30:05,520
In distributed system,
you have to build it.

836
00:30:05,520 --> 00:30:08,490
And I'll talk about like
how Lambda has to build it.

837
00:30:08,490 --> 00:30:12,120
So let's first start with like
this buffers smooth variance

838
00:30:12,120 --> 00:30:13,740
as I was talking about,
like the topic pattern

839
00:30:13,740 --> 00:30:15,000
is always variable.

840
00:30:15,000 --> 00:30:17,190
There are customers with
the steady state traffic,

841
00:30:17,190 --> 00:30:18,930
there are customers with random traffic,

842
00:30:18,930 --> 00:30:22,923
some customers spike, extremely
spike like they are...

843
00:30:22,923 --> 00:30:26,250
They can spike from like 10 x to like 50 x

844
00:30:26,250 --> 00:30:27,270
and you have to have a system

845
00:30:27,270 --> 00:30:29,520
which actually takes care of this thing.

846
00:30:29,520 --> 00:30:30,360
There are some customers,

847
00:30:30,360 --> 00:30:31,620
those who are like slowly ramping up

848
00:30:31,620 --> 00:30:34,740
and suddenly they become
the real customers.

849
00:30:34,740 --> 00:30:37,440
And, some of the customers

850
00:30:37,440 --> 00:30:40,293
can bust like can have
micro busty patterns.

851
00:30:41,970 --> 00:30:43,410
That you have to have a system

852
00:30:43,410 --> 00:30:44,580
which can actually handle this.

853
00:30:44,580 --> 00:30:45,630
And like,

854
00:30:45,630 --> 00:30:49,470
so what you're basically
building is a ingestion tier

855
00:30:49,470 --> 00:30:52,230
where there are data sources,
there's an ingestion tier

856
00:30:52,230 --> 00:30:53,780
and there's a storage leverage,

857
00:30:54,630 --> 00:30:55,830
where the system,

858
00:30:55,830 --> 00:30:57,120
the traits of that system

859
00:30:57,120 --> 00:31:00,663
is that it can accept highly
variable multi-tenant traffic.

860
00:31:02,280 --> 00:31:03,390
After you've taken the traffic,

861
00:31:03,390 --> 00:31:05,730
you need to normalize
and validate the request

862
00:31:05,730 --> 00:31:08,040
and you have to write the message

863
00:31:08,040 --> 00:31:09,840
or events to the right queue.

864
00:31:09,840 --> 00:31:12,360
You have to persist it and acknowledge it.

865
00:31:12,360 --> 00:31:13,560
Before acknowledging you need to make sure

866
00:31:13,560 --> 00:31:15,600
that like it's persisted durably.

867
00:31:15,600 --> 00:31:17,460
And then you also have to
apply admission control

868
00:31:17,460 --> 00:31:18,960
and back pressure to protect yourself.

869
00:31:18,960 --> 00:31:21,573
So if you have to build
something of this sort,

870
00:31:23,100 --> 00:31:24,480
you would need to...

871
00:31:24,480 --> 00:31:26,970
Like you need to have
a durable architecture

872
00:31:26,970 --> 00:31:27,810
which actually helps you build

873
00:31:27,810 --> 00:31:28,980
this like ingestion tier.

874
00:31:28,980 --> 00:31:30,243
So what Lambda did,

875
00:31:31,170 --> 00:31:34,020
this is like the layer of the customers,

876
00:31:34,020 --> 00:31:35,910
they variable customers.

877
00:31:35,910 --> 00:31:38,220
What we do, we have a load
balancer where like you,

878
00:31:38,220 --> 00:31:40,170
whenever the first time that you invoke,

879
00:31:40,170 --> 00:31:43,110
so like Julian was talking
about like Frontend

880
00:31:43,110 --> 00:31:46,110
and invoke, event invoke Frontend.

881
00:31:46,110 --> 00:31:47,730
So this is the Frontend

882
00:31:47,730 --> 00:31:49,830
that is the event invoke Frontend,

883
00:31:49,830 --> 00:31:51,600
and it has a bunch of things
like the distributed load

884
00:31:51,600 --> 00:31:52,920
like health checks and all.

885
00:31:52,920 --> 00:31:55,623
And once you have that, like
it hits the upper Frontend

886
00:31:55,623 --> 00:31:57,300
like this is called like reverse proxy,

887
00:31:57,300 --> 00:31:58,380
you can call it that,

888
00:31:58,380 --> 00:32:01,020
which is like multi AZ service

889
00:32:01,020 --> 00:32:03,840
and has like bunch of like host on it.

890
00:32:03,840 --> 00:32:08,280
And at the same time it's also
has a like autoscaling rules

891
00:32:08,280 --> 00:32:11,940
so that like if there is
a spike in heat in CPU

892
00:32:11,940 --> 00:32:14,280
or memory or something,
a bunch of these things,

893
00:32:14,280 --> 00:32:16,113
then it can autoscale on its own.

894
00:32:17,910 --> 00:32:20,430
Apart from that, we also have to build,

895
00:32:20,430 --> 00:32:22,890
like once you have gotten
the ingestion tier done,

896
00:32:22,890 --> 00:32:24,720
like you also have to passage the message

897
00:32:24,720 --> 00:32:26,040
as I was talking about.

898
00:32:26,040 --> 00:32:27,450
So you have to have a buffer

899
00:32:27,450 --> 00:32:30,690
and this buffer needs to be durable

900
00:32:30,690 --> 00:32:33,240
in the sense that like
you can't lose message

901
00:32:33,240 --> 00:32:35,940
once you have told it to your customers

902
00:32:35,940 --> 00:32:37,410
that like we have accepted a message,

903
00:32:37,410 --> 00:32:40,083
but also what at the end of the day,

904
00:32:41,082 --> 00:32:42,840
the thing that you want
to do is like it should go

905
00:32:42,840 --> 00:32:44,790
into the queue where you can process it.

906
00:32:44,790 --> 00:32:46,290
We don't want to just invest the message,

907
00:32:46,290 --> 00:32:47,610
but we also want to process it.

908
00:32:47,610 --> 00:32:50,610
So how do we build that like
routing layer in between?

909
00:32:50,610 --> 00:32:51,750
Let me walk you through that.

910
00:32:51,750 --> 00:32:53,700
So first thing is like
there's an invoke request

911
00:32:53,700 --> 00:32:55,770
that comes in with the event message,

912
00:32:55,770 --> 00:32:58,890
then it goes through
a chain of validators,

913
00:32:58,890 --> 00:33:00,000
you do authentication,

914
00:33:00,000 --> 00:33:01,710
then you do validate the payload,

915
00:33:01,710 --> 00:33:03,870
then you look at the function

916
00:33:03,870 --> 00:33:06,630
whether there's active function or not.

917
00:33:06,630 --> 00:33:07,730
And then once you pass

918
00:33:09,300 --> 00:33:10,920
this validation layer,

919
00:33:10,920 --> 00:33:12,570
you need to find the queue

920
00:33:12,570 --> 00:33:14,010
where the message can be ingested.

921
00:33:14,010 --> 00:33:15,780
And again, like I was talking about,

922
00:33:15,780 --> 00:33:17,070
we have millions of customers,

923
00:33:17,070 --> 00:33:18,360
we can't have like millions of queues.

924
00:33:18,360 --> 00:33:20,940
So there has to be like some sort of like,

925
00:33:20,940 --> 00:33:22,140
a multi-tenancy within the queues.

926
00:33:22,140 --> 00:33:24,690
So for that, the most popular pattern

927
00:33:24,690 --> 00:33:27,030
I think you guys might
have seen is like building

928
00:33:27,030 --> 00:33:30,270
a set of queues and then hashing
them onto like a hash ring

929
00:33:30,270 --> 00:33:33,120
and then doing that
multi-tenancy for the customers.

930
00:33:33,120 --> 00:33:36,060
So what we do when massive message passes

931
00:33:36,060 --> 00:33:37,260
through all the validation share,

932
00:33:37,260 --> 00:33:38,610
we find the queue finder,

933
00:33:38,610 --> 00:33:41,500
we hash it onto like the
partition of the hash ring

934
00:33:43,110 --> 00:33:45,213
and once we have found the partition,

935
00:33:47,937 --> 00:33:49,770
we take that message,
we have gotten the queue

936
00:33:49,770 --> 00:33:51,900
and then we write it to that
queue, whatever that queue

937
00:33:51,900 --> 00:33:54,303
that we found on the hash ring.

938
00:33:55,710 --> 00:33:57,120
But this is as you know,

939
00:33:57,120 --> 00:33:59,400
like even the single partition,

940
00:33:59,400 --> 00:34:01,080
can get like really, really hot

941
00:34:01,080 --> 00:34:03,990
because now millions of customers,

942
00:34:03,990 --> 00:34:06,900
a lot of them can overlap
onto like the same partition.

943
00:34:06,900 --> 00:34:08,433
So what we do is,

944
00:34:10,421 --> 00:34:12,780
we went back and then we use

945
00:34:12,780 --> 00:34:13,920
a technique called shuffle sharding,

946
00:34:13,920 --> 00:34:15,720
where shuffle sharding is nothing.

947
00:34:15,720 --> 00:34:17,460
Basically it's like,

948
00:34:17,460 --> 00:34:18,720
instead of liking hashing a customer

949
00:34:18,720 --> 00:34:19,650
onto like a single queue,

950
00:34:19,650 --> 00:34:22,500
now you can hash that customers
onto like multiple queues.

951
00:34:22,500 --> 00:34:25,800
So what we do is we hash a single customer

952
00:34:25,800 --> 00:34:27,210
based on the account already

953
00:34:27,210 --> 00:34:29,700
and there are like some
heuristics that we have.

954
00:34:29,700 --> 00:34:33,180
We hash that customer to
like now two hash ranges.

955
00:34:33,180 --> 00:34:35,280
We pick the queues based
on those hash ranges

956
00:34:35,280 --> 00:34:36,113
and then find the queue

957
00:34:36,113 --> 00:34:38,670
which has the lowest
queue depth at that time,

958
00:34:38,670 --> 00:34:40,380
and then inserts the message in there.

959
00:34:40,380 --> 00:34:41,460
So this is what we do,

960
00:34:41,460 --> 00:34:43,020
this is what we call like shuffle sharding

961
00:34:43,020 --> 00:34:44,670
and it does wonders for us

962
00:34:44,670 --> 00:34:47,280
and it helps us remove like
this like hot partitioning

963
00:34:47,280 --> 00:34:48,900
and it goes away.

964
00:34:48,900 --> 00:34:49,800
In addition to this,

965
00:34:49,800 --> 00:34:52,770
we also have something
called like express lane.

966
00:34:52,770 --> 00:34:55,080
So once we know like,
okay, we know this customer

967
00:34:55,080 --> 00:34:59,190
is going to be noisy and it'll
be having a lot more traffic,

968
00:34:59,190 --> 00:35:01,620
we already create like a
sideline queue for them.

969
00:35:01,620 --> 00:35:04,320
We let them like absorb
and we absorb their traffic

970
00:35:04,320 --> 00:35:07,230
and then once they're done
we can reap their queue

971
00:35:07,230 --> 00:35:10,563
and back them into like
those dynamic shared queues.

972
00:35:13,260 --> 00:35:17,460
And this was what I talked
about so far is like the queues

973
00:35:17,460 --> 00:35:18,630
that are managed by Lambda,

974
00:35:18,630 --> 00:35:20,160
those are like event invoke queues.

975
00:35:20,160 --> 00:35:21,600
So whenever you ingest Lambda,

976
00:35:21,600 --> 00:35:23,310
invoke Lambda in like
asynchronous manners,

977
00:35:23,310 --> 00:35:24,540
that is where it goes.

978
00:35:24,540 --> 00:35:27,162
That's another part of the buffer

979
00:35:27,162 --> 00:35:29,640
which is like helping us like smoothen

980
00:35:29,640 --> 00:35:31,110
the variance is like customer owned.

981
00:35:31,110 --> 00:35:33,480
So this is like the event
source mapping side of things

982
00:35:33,480 --> 00:35:35,070
where like the queues and the streams

983
00:35:35,070 --> 00:35:38,160
and everything, it stays
on the customer boundaries.

984
00:35:38,160 --> 00:35:39,840
All we do is like we have the processing,

985
00:35:39,840 --> 00:35:43,110
and from this point onwards,
the polling becomes same

986
00:35:43,110 --> 00:35:44,220
for both of, both.

987
00:35:44,220 --> 00:35:46,110
Like the, either it is customer-managed

988
00:35:46,110 --> 00:35:49,143
or it is service-managed.

989
00:35:50,430 --> 00:35:54,430
So let's, the second
lesson that we applied

990
00:35:55,350 --> 00:35:58,620
that specialized workers
for workload types.

991
00:35:58,620 --> 00:36:00,990
Let's look at how Lambda work
Lambda applies this lesson.

992
00:36:00,990 --> 00:36:03,101
Number two, to understand it,

993
00:36:03,101 --> 00:36:05,670
let's further understand what a worker is.

994
00:36:05,670 --> 00:36:08,070
It's not typical worker,
but a worker is nothing

995
00:36:08,070 --> 00:36:10,920
but a processor which looks at the queue

996
00:36:10,920 --> 00:36:14,313
and whenever it sees a message
or an event, it processes it.

997
00:36:15,180 --> 00:36:17,010
What queuing theory showed us

998
00:36:17,010 --> 00:36:19,560
that heterogeneous server service

999
00:36:19,560 --> 00:36:22,530
with different service rates
need to be dealt differently

1000
00:36:22,530 --> 00:36:24,300
for best performance.

1001
00:36:24,300 --> 00:36:26,040
We applied this

1002
00:36:26,040 --> 00:36:28,140
and we separated the polling workloads

1003
00:36:28,140 --> 00:36:30,270
from execution workloads.

1004
00:36:30,270 --> 00:36:32,730
Both polling and execution workloads

1005
00:36:32,730 --> 00:36:33,900
are different in nature.

1006
00:36:33,900 --> 00:36:37,950
Polling is continuous
stateful connection heavy.

1007
00:36:37,950 --> 00:36:40,530
On the other hand execution is busty,

1008
00:36:40,530 --> 00:36:42,003
stateless and short-lived.

1009
00:36:43,020 --> 00:36:43,920
So what you see here

1010
00:36:43,920 --> 00:36:46,200
is there are actually two types of worker.

1011
00:36:46,200 --> 00:36:49,230
One is a polling worker
that is managed by Lambda

1012
00:36:49,230 --> 00:36:52,080
and there is a Lambda function
that like customers write.

1013
00:36:53,160 --> 00:36:55,510
Julian talked about
this side of the worker

1014
00:36:57,000 --> 00:36:59,670
and like the Lambda
worker execution round.

1015
00:36:59,670 --> 00:37:01,470
I want to dive David today

1016
00:37:01,470 --> 00:37:03,470
onto like the poller side poller worker.

1017
00:37:04,470 --> 00:37:05,943
So let's jump right in.

1018
00:37:07,290 --> 00:37:10,560
So what are the key
components that make a poller?

1019
00:37:10,560 --> 00:37:13,710
I divided and I looked into
like our, like what we do today.

1020
00:37:13,710 --> 00:37:15,600
So this, this changes all the time.

1021
00:37:15,600 --> 00:37:17,160
We learn new things and we evolve.

1022
00:37:17,160 --> 00:37:18,903
So this is what we have today.

1023
00:37:19,740 --> 00:37:22,710
What we have like a work
acquire this component looks

1024
00:37:22,710 --> 00:37:24,453
at like if there's a active work,

1025
00:37:25,320 --> 00:37:27,450
so it looks into like an
assignment manager store.

1026
00:37:27,450 --> 00:37:29,280
It says that okay there's an SQS cube

1027
00:37:29,280 --> 00:37:31,770
or there's a stream, there's
a work that is there.

1028
00:37:31,770 --> 00:37:35,250
So what it does, it picks up the work

1029
00:37:35,250 --> 00:37:37,530
and it starts the event
source configuration.

1030
00:37:37,530 --> 00:37:40,893
So like for in the case of a
stream, like for KCL Kinesis,

1031
00:37:42,237 --> 00:37:43,980
it's KCL for Kafka, it's Kafka consumer

1032
00:37:43,980 --> 00:37:47,700
for SQS client.

1033
00:37:47,700 --> 00:37:50,193
What it does, like once it's configured,

1034
00:37:51,180 --> 00:37:54,810
it kick starts this work, it
con configures the connector

1035
00:37:54,810 --> 00:37:56,190
and pulls the record

1036
00:37:56,190 --> 00:37:58,680
and then writes it into
like an memory queue buffer.

1037
00:37:58,680 --> 00:38:01,290
So a lot of features that
Julian was talked about

1038
00:38:01,290 --> 00:38:03,210
are being done in this
like in-memory queue

1039
00:38:03,210 --> 00:38:04,260
where we create this view

1040
00:38:04,260 --> 00:38:07,320
so we don't kind of
pressure the event source

1041
00:38:07,320 --> 00:38:08,904
and we build this view.

1042
00:38:08,904 --> 00:38:11,070
And this is where like you
do batching your filtering,

1043
00:38:11,070 --> 00:38:12,300
your schema industry validation.

1044
00:38:12,300 --> 00:38:13,773
If it supports that.

1045
00:38:15,180 --> 00:38:17,520
Then we have this like an
invoke orchestrator layer

1046
00:38:17,520 --> 00:38:19,770
where once you have your batch is ready,

1047
00:38:19,770 --> 00:38:21,813
then batching can be for different ways.

1048
00:38:24,215 --> 00:38:26,759
Batching, Julian talked about like three

1049
00:38:26,759 --> 00:38:27,592
of the ways we do the batching.

1050
00:38:27,592 --> 00:38:30,120
Once you have the batch ready
with your conditions defined,

1051
00:38:30,120 --> 00:38:32,100
you do the invoke orchestrator.

1052
00:38:32,100 --> 00:38:33,720
What it does it like invokes your Lambda

1053
00:38:33,720 --> 00:38:35,700
in synchronous manner with a batch

1054
00:38:35,700 --> 00:38:36,870
and it deals with the record.

1055
00:38:36,870 --> 00:38:38,970
If it is failing, then
it it, it hands it off

1056
00:38:38,970 --> 00:38:43,792
to like the error handling
component which rides to DLQ

1057
00:38:43,792 --> 00:38:46,410
or retries, whatever

1058
00:38:46,410 --> 00:38:48,900
is it appropriate based
on your configuration.

1059
00:38:48,900 --> 00:38:51,240
In addition to this, like if
your message is successful,

1060
00:38:51,240 --> 00:38:53,850
we have a check pointer
manager which actually deals

1061
00:38:53,850 --> 00:38:55,110
with like an advancing checkpoint.

1062
00:38:55,110 --> 00:38:56,730
So that like this says that okay,

1063
00:38:56,730 --> 00:38:58,560
before deleting a message, we make sure

1064
00:38:58,560 --> 00:39:03,000
that like your system is, has processed it

1065
00:39:03,000 --> 00:39:04,710
and we are not losing any message.

1066
00:39:04,710 --> 00:39:08,370
In addition to this, this
is an auto-scaler we have.

1067
00:39:08,370 --> 00:39:10,140
So it looks at the current
state of the system

1068
00:39:10,140 --> 00:39:11,910
and says that, okay, whether I need

1069
00:39:11,910 --> 00:39:14,171
to auto scale if I'm
running hot, I should look

1070
00:39:14,171 --> 00:39:18,480
into the signals of my worker right now

1071
00:39:18,480 --> 00:39:21,660
and say that okay, I
need to scale further.

1072
00:39:21,660 --> 00:39:23,340
And scaling can be in, in two parts.

1073
00:39:23,340 --> 00:39:24,630
One is like vertically scaling.

1074
00:39:24,630 --> 00:39:25,920
If I have a capacity in my host,

1075
00:39:25,920 --> 00:39:28,573
so I look at the footprint
of my memory and CPU

1076
00:39:28,573 --> 00:39:29,910
and say that okay, I can vertically scale

1077
00:39:29,910 --> 00:39:31,980
to absorb some more traffic.

1078
00:39:31,980 --> 00:39:32,813
Or if I cannot,

1079
00:39:32,813 --> 00:39:36,540
then I will signal it to like
some of my global coordinator

1080
00:39:36,540 --> 00:39:38,550
to say that okay, I need more help.

1081
00:39:38,550 --> 00:39:41,370
So can you like spin up new workers for me

1082
00:39:41,370 --> 00:39:45,120
and then like take on the that work?

1083
00:39:45,120 --> 00:39:46,800
Apart from that, if I'm running hot,

1084
00:39:46,800 --> 00:39:48,660
I also want to load-shed
some of the workers.

1085
00:39:48,660 --> 00:39:49,860
So like we have a load-shedding component

1086
00:39:49,860 --> 00:39:52,380
which actually removes the
work if I'm running hot.

1087
00:39:52,380 --> 00:39:55,350
I don't want to impact my customers.

1088
00:39:55,350 --> 00:39:56,183
In addition to that,

1089
00:39:56,183 --> 00:39:58,080
there's like an overall
like health checker

1090
00:39:58,080 --> 00:40:00,270
which keeps looking into like
different parts of the system

1091
00:40:00,270 --> 00:40:04,080
and making that data available
to different components.

1092
00:40:04,080 --> 00:40:06,000
So Auto-scaler looks into
that data and says that,

1093
00:40:06,000 --> 00:40:08,610
okay, oh, I think I have, I'm running hot,

1094
00:40:08,610 --> 00:40:09,870
I need to auto-scale.

1095
00:40:09,870 --> 00:40:10,740
And load-shed said,

1096
00:40:10,740 --> 00:40:12,900
okay, I'm running hot, I
need to load-shed something.

1097
00:40:12,900 --> 00:40:14,400
In memory cube first also,

1098
00:40:14,400 --> 00:40:16,650
like similarly like it builds that view

1099
00:40:16,650 --> 00:40:19,983
and then makes it available
to like reprint components.

1100
00:40:22,862 --> 00:40:26,590
So what we did so far is like workers

1101
00:40:27,450 --> 00:40:28,920
and the view of that worker

1102
00:40:28,920 --> 00:40:32,190
and like what it takes
to build that worker.

1103
00:40:32,190 --> 00:40:34,500
But not every worker is same.

1104
00:40:34,500 --> 00:40:36,420
Some workers are more secure.

1105
00:40:36,420 --> 00:40:41,400
Some need to like go
through the BPC boundaries.

1106
00:40:41,400 --> 00:40:45,240
So what we did is like we
went back to the drawing board

1107
00:40:45,240 --> 00:40:46,890
and looked hard at it

1108
00:40:46,890 --> 00:40:49,530
that what is the an
anatomy of the pollers,

1109
00:40:49,530 --> 00:40:51,303
and what can we done there?

1110
00:40:52,542 --> 00:40:56,024
So if you, this is what we
just saw, and we squinted,

1111
00:40:56,024 --> 00:40:57,450
like literally we squinted,

1112
00:40:57,450 --> 00:40:59,000
and saw that like, okay,

1113
00:40:59,000 --> 00:41:00,540
if we slice the worker horizontally,

1114
00:41:00,540 --> 00:41:02,010
we basically get the bottom up apart

1115
00:41:02,010 --> 00:41:04,740
where all the system touch points are.

1116
00:41:04,740 --> 00:41:06,510
We squinted even harder.

1117
00:41:06,510 --> 00:41:09,000
And I'm talking literally,
I squinted a little harder.

1118
00:41:09,000 --> 00:41:12,060
A bottom half can be further divided

1119
00:41:12,060 --> 00:41:16,110
into three parts with three
different touch points.

1120
00:41:16,110 --> 00:41:19,533
And let me share what I meant
by that if you look closely,

1121
00:41:22,830 --> 00:41:26,430
the one part of that worker

1122
00:41:26,430 --> 00:41:28,530
is actually dealing with event sources,

1123
00:41:28,530 --> 00:41:30,780
and one part of the
worker is actually dealing

1124
00:41:30,780 --> 00:41:34,560
with the internal service communication,

1125
00:41:34,560 --> 00:41:35,850
doing the internal service communication.

1126
00:41:35,850 --> 00:41:37,608
And another part is dealing

1127
00:41:37,608 --> 00:41:39,090
with like the Lambda private invocations,

1128
00:41:39,090 --> 00:41:41,700
and all three have different
security boundaries

1129
00:41:41,700 --> 00:41:43,100
that you are operating with.

1130
00:41:44,940 --> 00:41:47,100
So what we did is we said,

1131
00:41:47,100 --> 00:41:50,280
okay, let's pick this part of the worker

1132
00:41:50,280 --> 00:41:53,580
and wrap it into like one
specific set of security boundary,

1133
00:41:53,580 --> 00:41:55,500
then pick this set up the worker

1134
00:41:55,500 --> 00:41:58,860
and put it into one
specific security boundary.

1135
00:41:58,860 --> 00:42:01,890
And the last part was like
Lambda execution environment,

1136
00:42:01,890 --> 00:42:04,290
which like you can have a
very private VPC function

1137
00:42:04,290 --> 00:42:05,240
that we can invoke.

1138
00:42:06,150 --> 00:42:07,750
And what else?

1139
00:42:07,750 --> 00:42:09,240
Like we applied the same principle

1140
00:42:09,240 --> 00:42:11,370
that we applied to Lambda worker

1141
00:42:11,370 --> 00:42:12,300
and Lambda pollers.

1142
00:42:12,300 --> 00:42:13,410
We applied the same principle

1143
00:42:13,410 --> 00:42:16,080
within the Lambda poller
workers and said that okay,

1144
00:42:16,080 --> 00:42:18,450
even there, there are three parts.

1145
00:42:18,450 --> 00:42:22,650
So now we can automatically
scale a part of that worker,

1146
00:42:22,650 --> 00:42:24,720
which is independent and has nothing to do

1147
00:42:24,720 --> 00:42:27,570
with the rest of the worker.

1148
00:42:27,570 --> 00:42:30,180
So think about like if you
have worked with Kafka,

1149
00:42:30,180 --> 00:42:33,270
then you know the problem
around consumers rebalancing.

1150
00:42:33,270 --> 00:42:34,530
There's like death loop,

1151
00:42:34,530 --> 00:42:35,940
and then like a bunch of
things that have been done,

1152
00:42:35,940 --> 00:42:37,800
like the static stability,

1153
00:42:37,800 --> 00:42:41,550
static membership and then
cooperative rebalance.

1154
00:42:41,550 --> 00:42:46,550
What if we can separate these two things

1155
00:42:47,250 --> 00:42:49,320
in such a way that like you can deal

1156
00:42:49,320 --> 00:42:50,670
with the consumers separately

1157
00:42:50,670 --> 00:42:52,080
then actually increasing throughput

1158
00:42:52,080 --> 00:42:53,530
by invoking Lambda functions?

1159
00:42:54,600 --> 00:42:57,270
So we did that, we wrapped
them into security boundary,

1160
00:42:57,270 --> 00:43:00,000
where like one is like dealing
with the customer resource,

1161
00:43:00,000 --> 00:43:01,770
one is dealing with the internal services

1162
00:43:01,770 --> 00:43:03,600
and one is dealing with
the function execution.

1163
00:43:03,600 --> 00:43:06,120
So we can have a VPC peering
between execution environment

1164
00:43:06,120 --> 00:43:07,410
and the poller workers.

1165
00:43:07,410 --> 00:43:09,030
We can have a cross-account connectivity

1166
00:43:09,030 --> 00:43:10,410
with the customer event source

1167
00:43:10,410 --> 00:43:11,340
and the internal service,

1168
00:43:11,340 --> 00:43:12,930
where like this is within
the same boundaries

1169
00:43:12,930 --> 00:43:14,030
where it is operating.

1170
00:43:15,660 --> 00:43:18,420
Now that we understand poller workers,

1171
00:43:18,420 --> 00:43:20,130
let's look at the next lesson

1172
00:43:20,130 --> 00:43:21,210
that we got from the queuing theory,

1173
00:43:21,210 --> 00:43:23,460
which tells us about
the production behavior

1174
00:43:23,460 --> 00:43:24,603
of these workers.

1175
00:43:25,950 --> 00:43:26,783
Let's look at that.

1176
00:43:26,783 --> 00:43:27,616
The lesson number three

1177
00:43:27,616 --> 00:43:30,660
is the control variance
to prevent instability.

1178
00:43:30,660 --> 00:43:34,530
And as I said like
variance skills systems.

1179
00:43:34,530 --> 00:43:36,360
So let's take a deeper look at it.

1180
00:43:36,360 --> 00:43:38,130
What you see here is a workload

1181
00:43:38,130 --> 00:43:40,050
which has a high latency
once it has gone through

1182
00:43:40,050 --> 00:43:42,900
and like now it started to
absorb a lot more resources

1183
00:43:42,900 --> 00:43:43,733
of the system,

1184
00:43:45,120 --> 00:43:47,220
and that leaves like the workload B

1185
00:43:47,220 --> 00:43:51,120
and workload C with no
resources to work with.

1186
00:43:51,120 --> 00:43:52,650
And as Little's Law dictates

1187
00:43:52,650 --> 00:43:54,960
that if processing latency increases,

1188
00:43:54,960 --> 00:43:58,410
concurrency will spike even
with a constant arrival rate.

1189
00:43:58,410 --> 00:43:59,853
Now, at this point,

1190
00:44:00,750 --> 00:44:03,900
we want to build some sort of
like a fairness in our system.

1191
00:44:03,900 --> 00:44:04,750
So what do we do?

1192
00:44:06,629 --> 00:44:10,840
To deal with, we built multiple
layers of defense control,

1193
00:44:10,840 --> 00:44:15,030
different layers of like mechanisms

1194
00:44:15,030 --> 00:44:17,130
to maintain the stability into the system.

1195
00:44:17,130 --> 00:44:18,660
The first one is like variance reduction.

1196
00:44:18,660 --> 00:44:20,244
The first layer

1197
00:44:20,244 --> 00:44:22,290
is, if you're getting
messages into the queue,

1198
00:44:22,290 --> 00:44:24,900
try to batch them, for example.

1199
00:44:24,900 --> 00:44:27,000
The second one is like
the capacity controls,

1200
00:44:27,000 --> 00:44:30,990
set hard limits, like
capping the concurrency,

1201
00:44:30,990 --> 00:44:33,240
so a single function
can't overrun the fleet.

1202
00:44:34,230 --> 00:44:36,243
Third is control what comes in.

1203
00:44:38,160 --> 00:44:40,950
Throttle requires when you
are full, just like a store,

1204
00:44:40,950 --> 00:44:43,620
locking the door at the
capacity, for example,

1205
00:44:43,620 --> 00:44:47,550
and back pressure, react when overloaded.

1206
00:44:47,550 --> 00:44:48,510
You don't have to suffer.

1207
00:44:48,510 --> 00:44:51,750
Slowdown polling or shared low priority.

1208
00:44:51,750 --> 00:44:54,450
These are like four
layers that we have built.

1209
00:44:54,450 --> 00:44:55,707
Again, these are layers that we have built

1210
00:44:55,707 --> 00:44:58,440
and some of these layer can
be configured by the customers

1211
00:44:58,440 --> 00:44:59,640
and some of these are like owned

1212
00:44:59,640 --> 00:45:02,520
by and managed by service team.

1213
00:45:02,520 --> 00:45:05,157
Now that we have arrived,
looked at the arrival pattern,

1214
00:45:05,157 --> 00:45:06,870
workers and what keep
the service running load,

1215
00:45:06,870 --> 00:45:08,322
one more thing that I talked about

1216
00:45:08,322 --> 00:45:11,700
is coordinate shared resources

1217
00:45:11,700 --> 00:45:13,293
through centralized component.

1218
00:45:15,781 --> 00:45:18,750
What queuing theory says that okay,

1219
00:45:18,750 --> 00:45:20,760
there is always a determination
or stable pattern.

1220
00:45:20,760 --> 00:45:23,400
Whenever one worker dies,
the other worker picks it up.

1221
00:45:23,400 --> 00:45:25,200
We don't have that luxury
in a distributor system.

1222
00:45:25,200 --> 00:45:27,893
So like we have to build
this global state on our own.

1223
00:45:29,100 --> 00:45:31,743
What it takes to build that global state.

1224
00:45:33,030 --> 00:45:35,490
So global state has four traits in it.

1225
00:45:35,490 --> 00:45:39,930
One is work conserving scheduling,

1226
00:45:39,930 --> 00:45:42,180
is like if there is a work
and there's an idle worker,

1227
00:45:42,180 --> 00:45:44,250
we should try to pair them up.

1228
00:45:44,250 --> 00:45:45,570
Conflict-free assignment.

1229
00:45:45,570 --> 00:45:47,190
If one worker is working on it,

1230
00:45:47,190 --> 00:45:48,390
don't give it to someone else

1231
00:45:48,390 --> 00:45:51,243
so that they're not like
thrashing each other.

1232
00:45:52,140 --> 00:45:53,850
The capability of a dispatch,

1233
00:45:53,850 --> 00:45:56,910
like you know, you are
giving work to someone

1234
00:45:56,910 --> 00:45:58,530
and it can handle that specific work.

1235
00:45:58,530 --> 00:46:02,100
Like I don't want to give like
a queuing worker an SQS work

1236
00:46:02,100 --> 00:46:05,193
to an stream poller, for example.

1237
00:46:06,150 --> 00:46:07,380
And the failure detection and recovery.

1238
00:46:07,380 --> 00:46:09,120
If someone has claimed the work

1239
00:46:09,120 --> 00:46:10,650
and it is actually not working on it,

1240
00:46:10,650 --> 00:46:13,050
then I should have a
way to know that someone

1241
00:46:13,050 --> 00:46:15,453
is not actually actively progressing that.

1242
00:46:16,500 --> 00:46:18,360
So what do we do?

1243
00:46:18,360 --> 00:46:19,500
We have a control plane.

1244
00:46:19,500 --> 00:46:20,990
Basically, like whenever you create an ESM

1245
00:46:20,990 --> 00:46:23,460
or you could create an
event in work config,

1246
00:46:23,460 --> 00:46:25,650
it goes to a config store.

1247
00:46:25,650 --> 00:46:28,038
And then we have an ESM lifecycle manager,

1248
00:46:28,038 --> 00:46:30,927
whose job is to bring worker

1249
00:46:30,927 --> 00:46:32,460
and make it available to the system

1250
00:46:32,460 --> 00:46:35,220
so that it can work on it, act on it.

1251
00:46:35,220 --> 00:46:36,750
Then we create an assignment.

1252
00:46:36,750 --> 00:46:38,070
With that assignment,

1253
00:46:38,070 --> 00:46:40,800
there's another separate service
called Assignment Manager,

1254
00:46:40,800 --> 00:46:43,530
where it stores the assignment
details into the config.

1255
00:46:43,530 --> 00:46:46,500
And this is where like we
build that global state.

1256
00:46:46,500 --> 00:46:48,360
And as I talked about
like the poller worker,

1257
00:46:48,360 --> 00:46:49,470
it gets the work,

1258
00:46:49,470 --> 00:46:52,610
then it heartbeats and
sends the master data back

1259
00:46:52,610 --> 00:46:54,210
to the assignment config store,

1260
00:46:54,210 --> 00:46:58,380
which is the source of like
we having the global state.

1261
00:46:58,380 --> 00:46:59,340
And at the same time,

1262
00:46:59,340 --> 00:47:00,840
like this is like an end-to-end drop job,

1263
00:47:00,840 --> 00:47:03,330
which is making sure that everything,

1264
00:47:03,330 --> 00:47:06,060
whoever is claiming the work,

1265
00:47:06,060 --> 00:47:08,040
they have the work and
they're acting on it.

1266
00:47:08,040 --> 00:47:09,060
And if they're not acting on it,

1267
00:47:09,060 --> 00:47:11,130
then I want to involve the human operators

1268
00:47:11,130 --> 00:47:12,930
so that it can take an action on it.

1269
00:47:16,110 --> 00:47:19,650
And this leads to what you
saw, what Julian showed you,

1270
00:47:19,650 --> 00:47:22,170
the Lambda poller worker
architecture looks like.

1271
00:47:22,170 --> 00:47:23,790
And you have seen it like multiple times

1272
00:47:23,790 --> 00:47:26,850
and this was the under-the-hood part

1273
00:47:26,850 --> 00:47:30,600
of building this kind of architecture.

1274
00:47:30,600 --> 00:47:31,983
Let's quickly recap.

1275
00:47:33,840 --> 00:47:35,580
Ingestion tier and buffer,

1276
00:47:35,580 --> 00:47:38,283
you need to have a
buffer-smoothening mechanism.

1277
00:47:39,930 --> 00:47:42,060
There are worker, and
there are different types.

1278
00:47:42,060 --> 00:47:46,500
Some has a different traits.

1279
00:47:46,500 --> 00:47:49,530
So you should like try
to separate them out.

1280
00:47:49,530 --> 00:47:52,140
The stability controls
variance skill your system.

1281
00:47:52,140 --> 00:47:54,450
So you need to have like
controls so that you can maintain

1282
00:47:54,450 --> 00:47:57,690
and build stability to your system.

1283
00:47:57,690 --> 00:47:58,860
And the global state

1284
00:47:58,860 --> 00:48:01,320
that is needed for you to like
make the continuous progress.

1285
00:48:01,320 --> 00:48:05,883
And this is what we needed,
need to keep fleet healthy.

1286
00:48:07,530 --> 00:48:09,750
And this basically tells you
almost everything that you need

1287
00:48:09,750 --> 00:48:12,570
to know about how Lambda applies lessons

1288
00:48:12,570 --> 00:48:13,403
from queuing theory,

1289
00:48:13,403 --> 00:48:14,730
and hope I, by now,

1290
00:48:14,730 --> 00:48:17,310
I give you some idea on
like why I said like Lambda

1291
00:48:17,310 --> 00:48:20,730
is also a queue service in many ways.

1292
00:48:20,730 --> 00:48:21,960
And Lambda has millions of customers

1293
00:48:21,960 --> 00:48:23,730
and millions of events flowing

1294
00:48:23,730 --> 00:48:24,960
through these systems.

1295
00:48:24,960 --> 00:48:27,330
So how do we keep everything running?

1296
00:48:27,330 --> 00:48:30,460
Seeing everything upfront
is really, really hard

1297
00:48:31,980 --> 00:48:34,440
and as a wise person said,

1298
00:48:34,440 --> 00:48:36,822
it's like Everything fails all the time.

1299
00:48:36,822 --> 00:48:39,240
So once you have the
queue, you have the worker.

1300
00:48:39,240 --> 00:48:42,360
What do you do if it fails?

1301
00:48:42,360 --> 00:48:44,330
So this second part of
the talk I talked about

1302
00:48:44,330 --> 00:48:46,950
is like how do we deal
with operational complexity

1303
00:48:46,950 --> 00:48:49,140
on operating a queue service like this?

1304
00:48:49,140 --> 00:48:51,240
Obviously, we had to innovate
on different dimensions

1305
00:48:51,240 --> 00:48:52,260
to run it quickly.

1306
00:48:52,260 --> 00:48:53,791
In queuing theory, it tells us

1307
00:48:53,791 --> 00:48:58,791
that failures are independent,
systems scales linearly

1308
00:48:59,760 --> 00:49:01,413
and saturations happen gradually.

1309
00:49:02,520 --> 00:49:07,520
It's the kind of world you would
like to see as an engineer.

1310
00:49:09,120 --> 00:49:10,380
But the moment you operate

1311
00:49:10,380 --> 00:49:12,120
a real multi-tenant distributed system,

1312
00:49:12,120 --> 00:49:14,640
you realize that everything gets messier.

1313
00:49:14,640 --> 00:49:17,250
The reality is there
are correlated failures.

1314
00:49:17,250 --> 00:49:18,420
If one service fails, you see

1315
00:49:18,420 --> 00:49:20,640
like, oh, other service starts to fail.

1316
00:49:20,640 --> 00:49:22,500
System don't scale linearly.

1317
00:49:22,500 --> 00:49:24,450
If there's a retry loop
happening somewhere,

1318
00:49:24,450 --> 00:49:27,540
then they accelerate faster,

1319
00:49:27,540 --> 00:49:29,280
and the cliff drop.

1320
00:49:29,280 --> 00:49:31,800
It's like you model
something in your simulation,

1321
00:49:31,800 --> 00:49:33,840
you always miss out that one condition

1322
00:49:33,840 --> 00:49:36,870
which leads to like some
sort of like a cliff drop.

1323
00:49:36,870 --> 00:49:38,970
And when I say cliff, I mean this cliff.

1324
00:49:38,970 --> 00:49:42,090
If you remember, back in 2013,

1325
00:49:42,090 --> 00:49:44,010
Lambda had an outage where a latent bug

1326
00:49:44,010 --> 00:49:45,930
in our Frontend free surfaced

1327
00:49:45,930 --> 00:49:48,360
and that led to increasing error rate.

1328
00:49:48,360 --> 00:49:49,590
This is the case I'm talking about.

1329
00:49:49,590 --> 00:49:52,890
It's really, really hard to model

1330
00:49:52,890 --> 00:49:54,723
all the cases while you are testing.

1331
00:49:55,590 --> 00:49:57,420
Same in the case with the queue service.

1332
00:49:57,420 --> 00:50:00,090
You can't model everything,
but what you can do

1333
00:50:00,090 --> 00:50:02,640
is like build a system
which can recover from this.

1334
00:50:04,260 --> 00:50:07,020
While there are many things we do,

1335
00:50:07,020 --> 00:50:09,540
I've captured three
main resiliency pattern

1336
00:50:09,540 --> 00:50:12,993
that we apply to operate Lambda.

1337
00:50:13,920 --> 00:50:15,690
The first one is like
a dependency outages.

1338
00:50:15,690 --> 00:50:17,850
What do we do when the codependency

1339
00:50:17,850 --> 00:50:20,913
that we rely on to build the
skiing service goes away?

1340
00:50:22,110 --> 00:50:25,530
The scale in version, what
if like a bigger fleet,

1341
00:50:25,530 --> 00:50:27,630
it starts to hammer the smaller fleet?

1342
00:50:27,630 --> 00:50:28,650
What do we do?

1343
00:50:28,650 --> 00:50:30,360
And the third is like
availability zone outage.

1344
00:50:30,360 --> 00:50:33,480
It's very special and very
unique to like the problem

1345
00:50:33,480 --> 00:50:35,400
how we solve it and
like how it is different

1346
00:50:35,400 --> 00:50:37,370
for like operating a queuing service.

1347
00:50:37,370 --> 00:50:39,720
So let's start with the dependency outage.

1348
00:50:39,720 --> 00:50:42,546
Let's look at the recent
outage October 2025.

1349
00:50:42,546 --> 00:50:44,543
I think it's pretty fresh
in everyone's memory.

1350
00:50:45,480 --> 00:50:46,830
This was the recent DDB outage

1351
00:50:46,830 --> 00:50:48,150
with a bunch of service got impacted.

1352
00:50:48,150 --> 00:50:49,050
Lambda was one of them.

1353
00:50:49,050 --> 00:50:52,260
We used DDB for various things
to operate our data play.

1354
00:50:52,260 --> 00:50:54,330
The root cause of this issue
was a latent race condition

1355
00:50:54,330 --> 00:50:56,010
in DynamoDB DNS management.

1356
00:50:56,010 --> 00:50:57,570
I think there is a talk
you guys should attend

1357
00:50:57,570 --> 00:51:00,120
if you're interested
in what happened here.

1358
00:51:00,120 --> 00:51:01,860
Now if you think from the point of view

1359
00:51:01,860 --> 00:51:04,110
of queue service operator,

1360
00:51:04,110 --> 00:51:05,860
this becomes even more interesting.

1361
00:51:07,380 --> 00:51:10,380
What you see here is that
when an outage happens,

1362
00:51:10,380 --> 00:51:11,760
we start to accumulate messages.

1363
00:51:11,760 --> 00:51:13,620
Now when the dependency is recovered,

1364
00:51:13,620 --> 00:51:16,740
we have to deal with backlog
that we have accumulated

1365
00:51:16,740 --> 00:51:19,140
with a new traffic, which is coming.

1366
00:51:19,140 --> 00:51:19,973
And to catch up,

1367
00:51:19,973 --> 00:51:22,340
we have to now start like we have

1368
00:51:22,340 --> 00:51:24,270
to work twice as hard, right?

1369
00:51:24,270 --> 00:51:25,770
We have accumulated some messages

1370
00:51:25,770 --> 00:51:27,930
and the new set of messages
are still coming in.

1371
00:51:27,930 --> 00:51:28,780
So what do we do?

1372
00:51:30,570 --> 00:51:31,740
The second kind of thing

1373
00:51:31,740 --> 00:51:34,200
that we see is like a fairness conundrum

1374
00:51:34,200 --> 00:51:37,620
of what I call it is like a noisy customer

1375
00:51:37,620 --> 00:51:38,473
is a noisy customer,

1376
00:51:38,473 --> 00:51:40,740
but a good customer becomes noisy customer

1377
00:51:40,740 --> 00:51:41,820
because now there is no way for us

1378
00:51:41,820 --> 00:51:44,390
to like that noisy customer has 100,000

1379
00:51:44,390 --> 00:51:46,350
of messages or millions of messages.

1380
00:51:46,350 --> 00:51:49,830
A good customer, a good
citizen also has like millions

1381
00:51:49,830 --> 00:51:51,630
of messages now because we were out.

1382
00:51:51,630 --> 00:51:52,740
So we started to accumulate,

1383
00:51:52,740 --> 00:51:54,960
like how do we differentiate between that?

1384
00:51:54,960 --> 00:51:57,090
And so what do we do?

1385
00:51:57,090 --> 00:51:59,040
Like whenever there is an outage,

1386
00:51:59,040 --> 00:52:01,890
we want to clear the
backlog as soon as possible,

1387
00:52:01,890 --> 00:52:04,590
but we also don't want to
hammer our service down

1388
00:52:04,590 --> 00:52:06,340
so that like we delay the recovery.

1389
00:52:07,320 --> 00:52:08,340
It's a balancing act.

1390
00:52:08,340 --> 00:52:10,020
It always turns into balancing act for us

1391
00:52:10,020 --> 00:52:11,520
whenever there's an outage.

1392
00:52:11,520 --> 00:52:13,500
If we are under-flowing, then
we are delay your recovery.

1393
00:52:13,500 --> 00:52:16,080
So your message is
staying in queue longer,

1394
00:52:16,080 --> 00:52:18,420
but if we are overflowing, then
we may tip over the service,

1395
00:52:18,420 --> 00:52:19,920
which is actually because
there is a capacity

1396
00:52:19,920 --> 00:52:22,050
that we are provision and
there is like a finite amount

1397
00:52:22,050 --> 00:52:23,760
of work that you could do.
You can like have buffer

1398
00:52:23,760 --> 00:52:25,620
and you can unlock a lot of capacity,

1399
00:52:25,620 --> 00:52:28,830
but still there is the amount of work log

1400
00:52:28,830 --> 00:52:31,890
that you have accumulated
can take more time.

1401
00:52:31,890 --> 00:52:33,090
There are a few things that we do,

1402
00:52:33,090 --> 00:52:34,680
and I'll talk about like
some of those strategies,

1403
00:52:34,680 --> 00:52:37,200
and this is not exhaustive by any means,

1404
00:52:37,200 --> 00:52:39,270
but there are a few things
which are interesting.

1405
00:52:39,270 --> 00:52:40,623
I wanted to share that.

1406
00:52:41,580 --> 00:52:42,420
One thing that we build

1407
00:52:42,420 --> 00:52:44,910
is, we build a gradual
ramp using a token bucket.

1408
00:52:44,910 --> 00:52:46,410
That token bucket rate

1409
00:52:46,410 --> 00:52:48,660
is controlled through
the view of the systems,

1410
00:52:48,660 --> 00:52:51,930
like how our CPU and
memory pressure is looking.

1411
00:52:51,930 --> 00:52:54,090
How is our sync traffic looking?

1412
00:52:54,090 --> 00:52:56,550
How is our different modes
of invocations are looking?

1413
00:52:56,550 --> 00:52:57,690
And then we build that

1414
00:52:57,690 --> 00:53:01,860
and tune the rate of our
TPS that we are generating

1415
00:53:01,860 --> 00:53:03,192
through this like asynchronous fleet

1416
00:53:03,192 --> 00:53:06,960
to gradually recover so that
we are not tipping over,

1417
00:53:06,960 --> 00:53:08,910
and then we have this handshake mechanism.

1418
00:53:08,910 --> 00:53:11,400
Every time there is an outage,

1419
00:53:11,400 --> 00:53:13,980
we build this automated
handshake mechanism

1420
00:53:13,980 --> 00:53:16,410
where we keep looking into
like dialing up the knob

1421
00:53:16,410 --> 00:53:17,490
and then looking at the rate

1422
00:53:17,490 --> 00:53:19,690
at which like how our
service is recovering?

1423
00:53:20,700 --> 00:53:24,180
We also look at try to
convert a FIFO queue

1424
00:53:24,180 --> 00:53:26,820
into a LIFO queue wherever...

1425
00:53:26,820 --> 00:53:31,140
Like if you have a backlog
and you are okay to go ahead

1426
00:53:31,140 --> 00:53:34,110
and then start looking
into the first messages.

1427
00:53:34,110 --> 00:53:36,750
So we start to try to create
and shed the customer.

1428
00:53:36,750 --> 00:53:39,600
So like old message, the
backlog can drain slowly,

1429
00:53:39,600 --> 00:53:40,770
but your new messages,

1430
00:53:40,770 --> 00:53:43,410
so that's why you see recovery very fast.

1431
00:53:43,410 --> 00:53:45,120
We also sometimes if you have a function,

1432
00:53:45,120 --> 00:53:45,953
you have a version.

1433
00:53:45,953 --> 00:53:48,570
So we try to shard your
cues into like a function

1434
00:53:48,570 --> 00:53:51,150
and a function version and
then an alias so that you start

1435
00:53:51,150 --> 00:53:52,833
to see recovery much faster.

1436
00:53:54,450 --> 00:53:58,740
And then as I was saying,
good customer can become noisy

1437
00:53:58,740 --> 00:54:00,240
and noisy customers is a noisy customer.

1438
00:54:00,240 --> 00:54:02,130
So like how do we isolate these things?

1439
00:54:02,130 --> 00:54:04,650
So for that, like think
about like a customer

1440
00:54:04,650 --> 00:54:05,640
has like millions of messages

1441
00:54:05,640 --> 00:54:07,740
and set their function concurrency to one.

1442
00:54:07,740 --> 00:54:08,820
So how do we deal with that?

1443
00:54:08,820 --> 00:54:11,070
Like now that customer is in the queue,

1444
00:54:11,070 --> 00:54:13,590
we try to isolate that
customers from the own queue

1445
00:54:13,590 --> 00:54:15,120
so it can have its own processing,

1446
00:54:15,120 --> 00:54:16,692
because it's not a noisy customer,

1447
00:54:16,692 --> 00:54:19,110
it's the outage that made it noisy.

1448
00:54:19,110 --> 00:54:22,860
So we have another kind of
like a token-bucket mechanism

1449
00:54:22,860 --> 00:54:25,830
where we assign a set buffers

1450
00:54:25,830 --> 00:54:27,120
or like give tokens based

1451
00:54:27,120 --> 00:54:28,650
on whatever the function concurrency,

1452
00:54:28,650 --> 00:54:30,060
downstream configurations are.

1453
00:54:30,060 --> 00:54:32,220
And we assign that one
single specific token bucket

1454
00:54:32,220 --> 00:54:35,130
to that customer and then
like try to isolate them,

1455
00:54:35,130 --> 00:54:37,260
so that like they have,
even if they're in the CIQ,

1456
00:54:37,260 --> 00:54:39,000
they are now using that token bucket

1457
00:54:39,000 --> 00:54:41,403
to say that this is their
processing capacity.

1458
00:54:43,560 --> 00:54:48,560
So this was about like what
happens when dependency is out?

1459
00:54:48,690 --> 00:54:50,260
There's another kind of outage

1460
00:54:51,754 --> 00:54:55,118
that we have to deal with
is like a scale inversion,

1461
00:54:55,118 --> 00:54:56,790
whereas a big fleet is
depending on a small fleet

1462
00:54:56,790 --> 00:54:58,170
and in small plate goes out

1463
00:54:58,170 --> 00:55:00,180
or big fleet behave weirdly,

1464
00:55:00,180 --> 00:55:03,183
which led to a small fleet going out.

1465
00:55:04,710 --> 00:55:08,250
So this is another outage
where like a large surge

1466
00:55:08,250 --> 00:55:11,130
of connection led to a...

1467
00:55:11,130 --> 00:55:12,780
And a retry storm happened,

1468
00:55:12,780 --> 00:55:15,720
which led to a small fleet going out.

1469
00:55:15,720 --> 00:55:17,580
And then also at the same
time like the control plane

1470
00:55:17,580 --> 00:55:22,580
was like recovering which
would delayed the recovery.

1471
00:55:22,890 --> 00:55:25,317
And for us it becomes, if you
think about like that work,

1472
00:55:25,317 --> 00:55:28,980
the global state view, like
the leasing services out,

1473
00:55:28,980 --> 00:55:30,405
if this is out for like one minute,

1474
00:55:30,405 --> 00:55:31,890
we have accumulated messages

1475
00:55:31,890 --> 00:55:35,010
and we have keep continuing
to getting like new messages.

1476
00:55:35,010 --> 00:55:36,600
So how do we recover from that?

1477
00:55:36,600 --> 00:55:38,350
And it becomes that problem for us.

1478
00:55:40,410 --> 00:55:43,530
And so for that, like we
build static stability.

1479
00:55:43,530 --> 00:55:44,720
What I mean by static stability

1480
00:55:44,720 --> 00:55:46,680
is the ability of a workload to continue

1481
00:55:46,680 --> 00:55:50,640
to correct continue operation
despite dependency impairment,

1482
00:55:50,640 --> 00:55:53,520
without requiring reactive changes.

1483
00:55:53,520 --> 00:55:56,400
The most critical step to
achieving the static stability

1484
00:55:56,400 --> 00:55:58,700
is separating the hot
path from the cold path.

1485
00:55:59,670 --> 00:56:01,950
Data plan doesn't need
to call control plane

1486
00:56:01,950 --> 00:56:02,910
during the processing.

1487
00:56:02,910 --> 00:56:05,940
Control plane only needs to
like do the lifecycle management

1488
00:56:05,940 --> 00:56:09,000
and data plan is sufficient to
operate in the steady state.

1489
00:56:09,000 --> 00:56:11,160
One of the strategies is like
we push down the configuration

1490
00:56:11,160 --> 00:56:12,360
for a Lambda case.

1491
00:56:12,360 --> 00:56:14,640
We push the Lambda
function configuration down

1492
00:56:14,640 --> 00:56:18,270
to the pollers instead of like
poller asking for that work.

1493
00:56:18,270 --> 00:56:21,750
The second is like replicating the state,

1494
00:56:21,750 --> 00:56:24,260
like L1 and L2, I think Julian
talked briefly about it.

1495
00:56:24,260 --> 00:56:26,250
We also use disk as a backup

1496
00:56:26,250 --> 00:56:27,477
so that like we can consume the work

1497
00:56:27,477 --> 00:56:32,477
and like restart the work,
resume the work where it is,

1498
00:56:32,550 --> 00:56:33,960
if there's an outage.

1499
00:56:33,960 --> 00:56:36,900
And then build is stickiness
with a flexible time out.

1500
00:56:36,900 --> 00:56:39,540
So we try to reduce the false positive

1501
00:56:39,540 --> 00:56:41,040
from like the false negatives.

1502
00:56:41,040 --> 00:56:41,970
If there's an outage

1503
00:56:41,970 --> 00:56:43,380
and we say like it's
a (indistinct) outage,

1504
00:56:43,380 --> 00:56:46,230
don't leave your work,
don't release your work.

1505
00:56:46,230 --> 00:56:49,140
Lease-driven independence means partitions

1506
00:56:49,140 --> 00:56:52,920
has their own dedicated worker
or shared dedicated workers.

1507
00:56:52,920 --> 00:56:56,640
And the circuit breaker is
like if you see like the error

1508
00:56:56,640 --> 00:56:58,530
is spread across the whole fleet,

1509
00:56:58,530 --> 00:57:00,570
that means maybe there is nothing common

1510
00:57:00,570 --> 00:57:01,710
with like that one specific host,

1511
00:57:01,710 --> 00:57:04,773
but it is spread across
the whole of the system.

1512
00:57:05,940 --> 00:57:08,790
And the third kind of outage
is like availability outage.

1513
00:57:09,930 --> 00:57:10,763
What I mean with that,

1514
00:57:10,763 --> 00:57:12,540
like availability outage is very weird.

1515
00:57:12,540 --> 00:57:14,067
There can be like a thunderstorm somewhere

1516
00:57:14,067 --> 00:57:19,067
and that led down to outage
of like an availability zone.

1517
00:57:20,280 --> 00:57:23,193
So what happens for us if there is an AZ,

1518
00:57:24,038 --> 00:57:25,680
there are three AZ sample workload.

1519
00:57:25,680 --> 00:57:27,180
If there's one AZ out,

1520
00:57:27,180 --> 00:57:30,540
we have to migrate because
we have millions of queues

1521
00:57:30,540 --> 00:57:32,430
that we are processing and
there are millions of workers

1522
00:57:32,430 --> 00:57:33,390
and that relationship.

1523
00:57:33,390 --> 00:57:35,220
So like we have to migrate
that work active workload

1524
00:57:35,220 --> 00:57:36,993
from that AZ to another AZ,

1525
00:57:38,370 --> 00:57:43,370
and that migration requires
some effort in on our site.

1526
00:57:46,620 --> 00:57:47,453
So what do we do?

1527
00:57:47,453 --> 00:57:48,286
Like we have AZ buffer.

1528
00:57:48,286 --> 00:57:49,230
This is very standard

1529
00:57:49,230 --> 00:57:51,150
like if you have capital
processing capacity

1530
00:57:51,150 --> 00:57:52,860
and you need to move the work,

1531
00:57:52,860 --> 00:57:53,790
you need to have the buffer

1532
00:57:53,790 --> 00:57:55,650
so that you can take on the work.

1533
00:57:55,650 --> 00:57:56,610
The second, and again,

1534
00:57:56,610 --> 00:57:58,890
this is like another balancing act for us

1535
00:57:58,890 --> 00:58:02,940
where like if you are one AZ is impacted,

1536
00:58:02,940 --> 00:58:06,810
you need to move the work
to like the other AZ,

1537
00:58:06,810 --> 00:58:08,899
but you don't want to
impact the other AZ also

1538
00:58:08,899 --> 00:58:11,220
with that outage.

1539
00:58:11,220 --> 00:58:13,520
So we have like hardware-based mechanism

1540
00:58:13,520 --> 00:58:17,100
where like we are reactively
like we let the leases expire,

1541
00:58:17,100 --> 00:58:19,980
and then that becomes
available and then gets shared.

1542
00:58:19,980 --> 00:58:21,030
But it takes longer time.

1543
00:58:21,030 --> 00:58:24,150
And what if like Lambda has
recovered other workers?

1544
00:58:24,150 --> 00:58:25,800
We have moved the workers
from like unhealthy AZ

1545
00:58:25,800 --> 00:58:27,090
to like healthy AZ,

1546
00:58:27,090 --> 00:58:29,010
now we need time to,

1547
00:58:29,010 --> 00:58:31,110
and if we are taking time to migrate,

1548
00:58:31,110 --> 00:58:33,960
then there is no point in
like having that recovery.

1549
00:58:33,960 --> 00:58:37,230
So we need to do, get
more proactive in that.

1550
00:58:37,230 --> 00:58:38,760
We have divided that
into like three parts,

1551
00:58:38,760 --> 00:58:40,560
like detection, we need to first make sure

1552
00:58:40,560 --> 00:58:42,180
that like there is an AZ outage,

1553
00:58:42,180 --> 00:58:44,400
we are very confident about it.

1554
00:58:44,400 --> 00:58:47,610
Then we evacuate, and then we
observe, continuously observe

1555
00:58:47,610 --> 00:58:49,290
that like there is an outage,

1556
00:58:49,290 --> 00:58:52,680
there is a, like the work
has migrated to that new AZ

1557
00:58:52,680 --> 00:58:55,110
and then like if when
the recovery happened

1558
00:58:55,110 --> 00:58:56,940
we need to move it back.

1559
00:58:56,940 --> 00:58:57,990
So the key takeaways

1560
00:58:57,990 --> 00:58:59,610
are, architectural key takeaways

1561
00:58:59,610 --> 00:59:01,740
designed for busty, unpredictable,

1562
00:59:01,740 --> 00:59:04,260
reduce processing variability, isolate.

1563
00:59:04,260 --> 00:59:06,270
And operational key takeaways

1564
00:59:06,270 --> 00:59:09,990
around, multi-tenancy is the default,

1565
00:59:09,990 --> 00:59:11,670
failures are constant,

1566
00:59:11,670 --> 00:59:13,410
and stability comes before scale.

1567
00:59:13,410 --> 00:59:15,930
And with that I'll hand it over to Julian.

1568
00:59:15,930 --> 00:59:16,763
Thank you.
- Excellent.

1569
00:59:16,763 --> 00:59:17,662
Thank you very much.

1570
00:59:17,662 --> 00:59:18,495
(audience applauding)

1571
00:59:18,495 --> 00:59:19,350
We've only got a little bit of time left,

1572
00:59:19,350 --> 00:59:20,400
but I just wanted to quickly go

1573
00:59:20,400 --> 00:59:21,990
through some of the things with Lambda.

1574
00:59:21,990 --> 00:59:24,030
We talked about Lambda Managed Instances,

1575
00:59:24,030 --> 00:59:25,500
all the flexibility of Lambda

1576
00:59:25,500 --> 00:59:27,038
with EC2 control plane as well.

1577
00:59:27,038 --> 00:59:29,190
Lambda durable functions a whole way.

1578
00:59:29,190 --> 00:59:31,973
Messes up all the way you're
thinking about Lambda,

1579
00:59:31,973 --> 00:59:33,930
built for asynchronous processing,

1580
00:59:33,930 --> 00:59:35,580
but all within a single Lambda function.

1581
00:59:35,580 --> 00:59:36,900
Build long for running workflows

1582
00:59:36,900 --> 00:59:38,340
in your favorite programming languages.

1583
00:59:38,340 --> 00:59:40,620
Another session is happening
on that this afternoon.

1584
00:59:40,620 --> 00:59:41,580
Tenant isolation,

1585
00:59:41,580 --> 00:59:44,220
providing execution
isolation between tenants.

1586
00:59:44,220 --> 00:59:45,420
You just give a tenant ID.

1587
00:59:45,420 --> 00:59:46,800
This is great for SaaS customers

1588
00:59:46,800 --> 00:59:48,690
who have maybe hundreds
of thousands of functions,

1589
00:59:48,690 --> 00:59:50,340
makes it much better for them.

1590
00:59:50,340 --> 00:59:51,600
Event source mapping enhancements.

1591
00:59:51,600 --> 00:59:53,610
We've got schema registry
which has been released.

1592
00:59:53,610 --> 00:59:55,500
The provision mode for SQS I mentioned,

1593
00:59:55,500 --> 00:59:58,380
and also a whole bunch of
ESM metrics were available

1594
00:59:58,380 --> 01:00:00,360
to help you building your applications.

1595
01:00:00,360 --> 01:00:02,520
And the future, well, of course,
there's plenty more to do.

1596
01:00:02,520 --> 01:00:04,770
We'll always be helping
with observability,

1597
01:00:04,770 --> 01:00:07,470
bringing new updates, new
runtimes as quickly as we can,

1598
01:00:07,470 --> 01:00:09,690
working hard on price to
make Lambda applicable

1599
01:00:09,690 --> 01:00:10,770
for all workloads.

1600
01:00:10,770 --> 01:00:13,598
More deeper integrations with
AWS and third-party services.

1601
01:00:13,598 --> 01:00:16,920
And always we continue to
listen to your feedback.

1602
01:00:16,920 --> 01:00:19,830
Let us know what you
would like us to make.

1603
01:00:19,830 --> 01:00:22,410
Make to build our Lambda
roadmap is now public available

1604
01:00:22,410 --> 01:00:25,860
on GitHub, so you can help us
to find the future of Lambda.

1605
01:00:25,860 --> 01:00:28,200
Lots of other kind of
sessions, manage instances,

1606
01:00:28,200 --> 01:00:30,900
durable functions, the future
of serverless tomorrow.

1607
01:00:30,900 --> 01:00:32,970
And I did a talk on best
practices yesterday.

1608
01:00:32,970 --> 01:00:35,310
Hopefully that'll be on YouTube
and useful for you as well.

1609
01:00:35,310 --> 01:00:37,740
Another slide here just
on a bunch of whole,

1610
01:00:37,740 --> 01:00:39,060
a lot of serverless kind of things

1611
01:00:39,060 --> 01:00:41,243
to continue your serverless learning.

1612
01:00:41,243 --> 01:00:42,360
But most importantly,

1613
01:00:42,360 --> 01:00:45,090
thank you so much today for coming,

1614
01:00:45,090 --> 01:00:45,923
early in the morning.

1615
01:00:45,923 --> 01:00:46,980
I appreciate you taking the time

1616
01:00:46,980 --> 01:00:49,004
to learn a little bit more about Lambda.

1617
01:00:49,004 --> 01:00:50,970
Hopefully, you can understand
there's a huge amount

1618
01:00:50,970 --> 01:00:52,110
that happens under the hood,

1619
01:00:52,110 --> 01:00:53,340
so you can just consume Lambda

1620
01:00:53,340 --> 01:00:55,170
as a great service without having to worry

1621
01:00:55,170 --> 01:00:56,460
about all the flow control,

1622
01:00:56,460 --> 01:00:58,290
all the other kind of things that Rajesh

1623
01:00:58,290 --> 01:00:59,123
and his team is building.

1624
01:00:59,123 --> 01:01:01,050
So thank you very much
for joining us today,

1625
01:01:01,050 --> 01:01:02,070
and enjoy the rest of your Reinvent.

1626
01:01:02,070 --> 01:01:05,103
Thank you.
(audience clapping)

