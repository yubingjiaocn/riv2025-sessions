# AWS re:Invent 2025 会议总结：使用 Amazon ElastiCache 的语义缓存优化 Agentic AI 应用

## 会议概述

本次技术会议深入探讨了如何通过 Amazon ElastiCache 中的语义缓存技术来优化 Agentic AI 应用的性能和成本。演讲者指出，当前 AI 应用已经从实验阶段进入生产阶段，企业面临的核心挑战从"能否运行"转变为"能否大规模运行"。随着 Agentic AI 应用复杂度的增加，多个代理、多步骤处理和频繁的 LLM 调用导致延迟和成本呈指数级增长。

会议重点介绍了语义缓存作为解决方案的工作原理。与传统缓存使用精确文本匹配不同，语义缓存通过向量嵌入来理解查询的语义含义，即使查询措辞不同但含义相似也能命中缓存。演讲者通过实际案例展示，在拉斯维加斯寻找意大利餐厅的查询场景中，多代理框架需要调用搜索代理、评论代理和可用性代理，每个步骤都涉及大量 LLM 调用。通过语义缓存，相似查询可以直接从缓存中获取结果，避免重复的昂贵计算。

技术实现方面，会议详细介绍了 Valkey（Redis 的开源替代品）在 ElastiCache 中的向量搜索能力，该功能于 2024 年 11 月 17 日正式发布。使用 HNSW（分层可导航小世界）算法，系统能够在数百微秒到几毫秒内完成向量搜索，同时保持 95-99% 的高召回率。演讲者强调，在多代理系统中，每个步骤的高质量结果至关重要——三个代理各自 90% 的准确率只能产生 73% 的总体准确率，而 99% 的准确率则能达到 97% 的总体准确率。成本分析显示，嵌入模型比 LLM 便宜 750 倍，即使缓存命中率仅为 2.5% 就能实现盈亏平衡，更高的命中率则能带来显著的成本节省。

## 详细时间线与关键要点

00:00:00 - 会议开场与问题提出
- 演讲者询问现场观众对其 Agentic AI 应用的时间和成本是否满意，几乎无人举手
- 介绍会议四大主题：Agentic AI 发展历程、传统缓存的局限性、语义缓存工作原理、实施最佳实践

00:01:30 - Agentic AI 的演进历程
- 2022 年：构建基础，推出对话式 AI 工具
- 2022-2024 年：关注质量和多模态数据处理（图像、视频等）
- 2024-2025 年：从聊天转向行动，自然语言转化为工具调用
- 当前阶段：从实验转向规模化部署，关注安全性、治理、延迟和成本控制

00:03:00 - 从演示到部署的三大障碍
- **规模问题**：应用变得更复杂，包含更多代理和工具
- **速度问题**：每个步骤需要更多 LLM 调用、API 调用和工具调用
- **成本问题**：LLM 推理对单次交互或多轮对话极其昂贵

00:04:00 - 复合延迟问题
- 上下文组装：检索记忆、结构化数据、用户个性化
- 规划阶段：决定调用哪些代理、使用什么顺序、调用哪些工具
- 工具调用：实际执行所有工具和代理
- 最终处理：重新排序和组装最终响应
- 这些步骤在单次交互中反复循环，导致延迟和成本无限增长

00:05:30 - 成本增长曲线分析
- 79% 的公司已在生产环境中部署 Agentic AI 应用
- 随着代理和步骤增加，应用智能提升但成本和延迟也随之上升
- 需要优化技术来平衡智能水平和成本效率

00:06:30 - 多种优化技术介绍
- 提示缓存（Prompt caching）
- 使用更便宜的模型
- 模型蒸馏和针对特定任务的微调
- 更好的路由策略
- **语义缓存**（本次会议重点）：在不增加成本的情况下最大化应用智能

00:07:30 - 单代理 vs 多代理框架
- **单代理框架**：简单查询（如"拉斯维加斯有什么活动"），执行网络搜索和数据库查询，成本和延迟相对可控
- **多代理框架**：包含更多 LLM 调用和工具调用，导致更高延迟和成本

00:08:30 - 四种多代理框架类型
- **监督者模型**：主编排器分配任务给专门代理，每个圆圈代表一个代理调用
- **网络模型**：代理协作完成任务，相互通信
- **层级控制**：监督者模型的细化版本
- **顺序模型**：代理按顺序完成任务后移交给下一个代理

00:10:00 - 实际案例：餐厅搜索查询
- 用户查询："在 Bellagio 附近找提供素食选项的意大利餐厅，下午 4-7 点之间"
- 表面简单，实际涉及复杂需求：菜系类型、距离、菜单选项、可用性
- 人类处理方式：分解问题、查看地图、浏览菜单、检查预订应用、做出决策

00:11:30 - Agentic AI 处理流程
- **规划代理**：将问题分解为子任务
- **搜索代理**：查找 Bellagio 附近的意大利餐厅，执行距离计算、网络搜索、API 调用
- **评论代理**：扫描菜单寻找素食选项，通过 RAG 调用和数据库搜索
- **可用性代理**：检查餐厅预订情况
- **最终处理**：使用 LLM 合并所有数据并生成答案

00:13:00 - 模式识别与缓存机会
- 第二个用户提出语义相似的查询（如"素食选项，不同时间"）
- 系统会重复执行相同的代理流程
- 关键发现：可以重用第一个用户的搜索代理响应、评论代理响应，只需更新可用性信息

00:14:30 - 传统缓存解决方案
- 传统应用使用缓存从数据库卸载查询，节省时间和成本
- 流程：检查缓存 → 命中则返回（微秒级）→ 未命中则查询数据库（数百毫秒）→ 将结果缓存
- 缓存数据通常小于 1KB，显著加速后续查询

00:15:30 - Valkey 介绍
- Valkey 是 Redis 的社区开源替代品，许可宽松
- 已被众多云服务商和组织广泛采用
- AWS 于去年在 ElastiCache 中推出 Valkey 支持
- 提供可观测性、安全性、可扩展性等托管服务优势

00:16:30 - ElastiCache 的多种用例
- 会议重点关注机器学习和 AI 用例
- **重大发布**：Valkey 向量搜索功能（2024 年 11 月 17 日上线）
- 在 AWS 上的主流向量数据库中，提供最低延迟、最高召回率和吞吐量，性价比最优

00:17:30 - 向量搜索的四大用例
- **传统 RAG**：获取上下文，提供给 LLM，生成响应
- **实时语义搜索**：用于推荐引擎
- **Agentic 记忆**：基于用户个性化代理
- **语义缓存**：降低成本并提升 Agentic AI 应用性能

00:18:30 - 技术深入：什么是语义缓存
- 与传统数据库缓存类似，但索引方式不同
- 传统缓存：使用 SQL 查询的精确文本作为键
- 语义缓存：使用查询的语义含义作为索引，而非精确文本

00:19:30 - 语义缓存操作流程
- 提取查询的语义
- 在缓存中搜索语义相似的先前答案
- 如果找到足够接近的答案 → 缓存命中，使用该答案
- 如果未找到 → 缓存未命中，调用 LLM 生成新答案并存入缓存

00:20:30 - 语义转换为向量
- 计算机通过位和字节工作，需要将语义转换为数字
- 使用向量表示语义
- 实际应用中向量维度可达数百或数千维（演示使用三维便于理解）
- 关键原理：语义相似的项在向量空间中距离较近

00:22:00 - 向量距离与语义距离
- 三维空间示例：餐饮相关查询聚集在一起
- 查询"最好的意大利餐厅在哪里"在向量空间中有特定位置
- 设定相似度阈值：在此距离内的向量被视为语义相同
- 相似度阈值是获得高质量结果的关键

00:23:30 - 嵌入模型（Embedding Models）
- 将文本查询转换为向量的技术，来自自然语言处理领域
- 术语：向量嵌入、嵌入、向量基本指同一概念
- 可在 Amazon Bedrock 上使用各种嵌入模型
- Amazon 提供两种嵌入模型：文本嵌入模型和多模态模型（支持声音、图像）
- 也可使用其他供应商的模型或训练自定义模型

00:25:00 - 嵌入模型的性能优势
- 比大型语言模型快 750 倍且便宜 750 倍
- 这个数字在后续成本分析中将再次出现

00:25:30 - 完整的语义缓存架构
- 语义提取 → 转换为向量
- 语义搜索 → 向量搜索
- 在相似度阈值内查找向量 → 缓存命中，重用答案
- 超出阈值 → 缓存未命中，调用模型生成答案并存入缓存

00:26:30 - 扩展性挑战：精确向量搜索的局限
- 使用精确向量搜索算法无法良好扩展
- 精确向量搜索没有比暴力搜索更好的已知算法
- 这是一个 75 年未解决的计算机科学问题

00:27:00 - 近似最近邻（ANN）算法
- 如果愿意放弃精确答案，可以获得更好的性能
- 类似布隆过滤器：可能产生假阳性
- ANN 算法有时找不到最近的向量（这就是"近似"的含义）
- 在缓存场景中可接受：未找到最佳答案但在阈值内仍可用，或超出阈值则视为未命中

00:28:30 - 召回率（Recall）指标
- 衡量 ANN 算法质量的标准
- 定义：算法找到正确答案的百分比
- 示例：10 次查找中 8 次正确 = 80% 召回率

00:29:00 - ANN 算法的三维权衡
- 传统计算机科学：时间与空间的二维权衡
- ANN 增加第三维：召回率（质量）
- 可以在时间、空间和质量之间进行权衡
- 缓存场景最重要的是：召回率（影响答案质量）和时间（缓存的核心目的）

00:30:00 - HNSW 算法介绍
- 全称：Hierarchical Navigable Small Worlds（分层可导航小世界）
- 在高召回率和低延迟的权衡空间中表现最佳
- 基于图的算法，根据距离建立向量之间的链接
- 借鉴跳表（Skip List）的思想构建分层结构

00:31:30 - HNSW 工作原理
- 构建金字塔式的分层图结构
- 顶层：数据子集的小图
- 中层：更大的数据子集
- 底层：完整数据集
- 类似跳表的分层搜索机制

00:32:30 - HNSW 搜索演示
- 从顶层随机选择起点
- 在邻域中搜索最接近查询向量的节点
- 移动到该节点并下降到下一层
- 重复此过程直到底层
- 对数时间复杂度，可扩展性强

00:33:30 - HNSW 性能表现
- 在数千万或数亿条目的数据集上
- 响应时间：数百微秒或个位数毫秒
- 在缓存环境中非常有效
- 轻松实现 95-99% 的召回率

00:34:30 - 高召回率的重要性
- HNSW 有多个参数（复杂如博士论文）
- 关键要点：每个步骤的高质量结果都很重要
- 实例分析：餐厅搜索案例分解为三个代理

00:35:30 - 召回率对整体准确性的影响
- 使用 90% 召回率的廉价算法：三个代理各 90% = 总体 73% 准确率（不可接受）
- 使用 99% 召回率：三个代理各 99% = 总体 97% 准确率（可接受）
- 在多代理系统中，高召回率至关重要

00:36:30 - 成本效益分析
- 嵌入模型比 LLM 快 750 倍且便宜 750 倍
- 缓存在微秒或毫秒级运行
- 详细数据可参考相关博客文章

00:37:30 - 成本曲线图分析
- Y 轴：每天成本，X 轴：缓存命中率
- 白线：无缓存系统，成本恒定
- 蓝线：有缓存系统
- 缓存命中率为 0 时成本更高（模型成本 + 嵌入成本 + 缓存成本）
- 盈亏平衡点：2.5% 命中率
- 成本降低几乎与命中率成正比

00:39:00 - ElastiCache 实现：键值存储映射
- Valkey 是键值存储，如何进行向量操作？
- 概念上类似关系表：行和列
- 每个键是表中的一行
- 使用多部分值（JSON 或 Hash）
- 每个字段是键内的元素

00:40:30 - 最小缓存结构
- 向量组件 + 答案引用
- 优化：不直接存储答案（答案可能有几 KB）
- 答案存储在更便宜的外部存储（DynamoDB、S3、文件系统等）
- 缓存只存储向量和答案引用

00:41:30 - 创建索引：基本配置
- 使用 FT.CREATE 命令
- 指定索引名称（可有多个索引）
- 选择数据类型（JSON 或 Hash）
- 设置前缀：限制索引的键空间子集
- 好处：可在同一 ElastiCache 实例中托管多个逻辑缓存

00:42:30 - 定义索引模式
- 指定 JSON 路径
- 为查询语言命名字段
- 设置为向量字段，使用 HNSW 算法
- 指定数据类型（32 位浮点）
- 设置维度（示例为 3 维，实际应用通常为 256、512、1536 等）
- ElastiCache 支持数万维的向量
- 设置距离度量

00:43:30 - 数据加载
- 使用标准 ElastiCache/Valkey 命令（SET、GET、JSON.SET）
- 当键匹配索引前缀时自动索引
- 完成时间：数百微秒或 1-2 毫秒
- **关键优势**：数据立即可查询
- 与其他向量存储不同（可能需要等待数分钟甚至数小时）
- 对于时间敏感或突发事件场景至关重要

00:45:00 - 设置过期时间（TTL）
- 可为缓存数据设置过期时间
- 示例：餐桌预订可能需要快速过期，最佳餐厅推荐可保留一两天
- 根据应用需求选择合适的时间间隔

00:45:30 - 查询语法
- 使用 FT.SEARCH 命令
- 指定要搜索的索引
- 指定要搜索的向量字段
- 支持混合搜索（向量 + 标签字段 + 数值字段）
- 提供参考向量（二进制格式）
- 语言客户端（如 Glide、Python 客户端）会处理大部分细节

00:47:00 - 查询结果处理
- 返回键（可能包含编码信息）
- 返回距离（用于命中/未命中逻辑）
- 返回值（包含答案引用）
- 将距离与相似度阈值比较
- 阈值内 → 缓存命中，获取答案
- 阈值外 → 缓存未命中，调用模型生成答案

00:48:00 - 实施最佳实践
- 回到餐厅搜索用例
- 将分解的子问题（意大利餐厅、素食选项、可用性）创建嵌入
- 将嵌入和答案存储在 Valkey 语义缓存中
- 第二个用户的相似查询通过向量搜索从缓存获取结果
- 避免所有昂贵的 LLM 调用和工具调用
- 只使用向量搜索和嵌入生成路径（显著更便宜）

00:49:30 - 代理特定的缓存配置
- 所有代理遵循相似框架但针对特定目标调优
- 使用不同工具、提示和底层数据
- 语义缓存也需要针对每个代理定制：
  - 选择合适的嵌入模型
  - 选择合适的搜索算法
  - 设置适当的相似度阈值
  - 配置合适的 TTL（过期时间）

00:50:30 - 数据特性差异
- 不同类型的数据需要不同的缓存策略
- 三个不同代理可能需要不同的配置参数
- 需要根据数据特性和业务需求进行调优

会议结束

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


关键技术要点总结：
- 语义缓存通过向量嵌入实现语义理解，而非精确文本匹配
- HNSW 算法提供最佳的召回率和延迟平衡
- 嵌入模型比 LLM 便宜快 750 倍
- 2.5% 缓存命中率即可实现成本盈亏平衡
- 高召回率（95-99%）对多代理系统的整体准确性至关重要
- ElastiCache with Valkey 提供即时索引和查询能力
- 需要针对不同代理和数据类型定制缓存配置