1
00:00:00,510 --> 00:00:02,019
- Good afternoon.

2
00:00:02,019 --> 00:00:03,900
Welcome to re:Invent.

3
00:00:03,900 --> 00:00:05,070
My name's Joe Cudby.

4
00:00:05,070 --> 00:00:06,890
I'm a global go-to market lead

5
00:00:06,890 --> 00:00:10,080
in our next gen dev
experience team here at AWS.

6
00:00:10,080 --> 00:00:12,687
We're the folks that talk
to customers and partners

7
00:00:12,687 --> 00:00:15,360
and so on about their software
development life cycles,

8
00:00:15,360 --> 00:00:18,090
how to take advantage of AI in that space.

9
00:00:18,090 --> 00:00:20,010
Today I'm joined very happy actually

10
00:00:20,010 --> 00:00:21,750
to be joined with Krishna Kannan,

11
00:00:21,750 --> 00:00:24,420
head of product for one of
our partners, Jellyfish,

12
00:00:24,420 --> 00:00:25,740
and also Craig Dahlinger,

13
00:00:25,740 --> 00:00:29,280
one of our joint customers with Genesys.

14
00:00:29,280 --> 00:00:30,130
So let's dive in.

15
00:00:31,020 --> 00:00:33,510
So today we've kind of got
a story in three parts.

16
00:00:33,510 --> 00:00:34,980
I'm gonna tell part one,

17
00:00:34,980 --> 00:00:37,500
which is how have we gone
from this conversation

18
00:00:37,500 --> 00:00:38,700
of developer productivity

19
00:00:38,700 --> 00:00:41,100
that we've had over the
past couple three years

20
00:00:41,100 --> 00:00:45,090
to get to this place now
of talking about AI impact.

21
00:00:45,090 --> 00:00:46,500
What does that journey look like?

22
00:00:46,500 --> 00:00:48,630
How has that changed the
way we think about it?

23
00:00:48,630 --> 00:00:51,060
Then I'm gonna hand over to Krishna

24
00:00:51,060 --> 00:00:52,800
who's gonna talk about the way

25
00:00:52,800 --> 00:00:55,560
that Jellyfish thinks about
this situation, right?

26
00:00:55,560 --> 00:00:57,930
I'm gonna give you the Amazon perspective,

27
00:00:57,930 --> 00:00:59,430
he's gonna give you the
Jellyfish perspective,

28
00:00:59,430 --> 00:01:01,560
and then I'm very excited
to have Craig come up

29
00:01:01,560 --> 00:01:04,590
and then share his
perspective as a customer.

30
00:01:04,590 --> 00:01:06,870
So you're gonna get all three perspectives

31
00:01:06,870 --> 00:01:09,303
in this next sort of 45, 50 minutes or so.

32
00:01:10,620 --> 00:01:13,890
So I've had the pleasure
now of talking to customers

33
00:01:13,890 --> 00:01:16,410
about this space for about three years.

34
00:01:16,410 --> 00:01:19,320
So before we launched
CodeWhisper back in '23,

35
00:01:19,320 --> 00:01:20,610
all the way through '24,

36
00:01:20,610 --> 00:01:23,490
we've been really diving
deep into this space.

37
00:01:23,490 --> 00:01:26,790
And so we've seen this
evolution in tooling, right?

38
00:01:26,790 --> 00:01:30,450
In '23, we saw this idea of
like fancy auto complete,

39
00:01:30,450 --> 00:01:33,180
and then we moved into
'24 with this idea of chat

40
00:01:33,180 --> 00:01:35,370
and more complicated transactions

41
00:01:35,370 --> 00:01:38,100
and interactions to get
through to '25 today.

42
00:01:38,100 --> 00:01:40,980
And now what we're seeing is
all of these sort of more agent

43
00:01:40,980 --> 00:01:44,280
agentic type experiences
with reasoned models

44
00:01:44,280 --> 00:01:45,690
and other things, right?

45
00:01:45,690 --> 00:01:48,480
They become much more
interesting types of experiences.

46
00:01:48,480 --> 00:01:51,390
But it really has challenged this idea

47
00:01:51,390 --> 00:01:55,110
of what does productivity
mean in this age of AI?

48
00:01:55,110 --> 00:01:58,080
You know, when we first started
having these conversations

49
00:01:58,080 --> 00:01:58,980
back in '23,

50
00:01:58,980 --> 00:02:02,160
it was very much like the
developer productivity, right?

51
00:02:02,160 --> 00:02:04,470
The productivity individual human.

52
00:02:04,470 --> 00:02:05,700
At re:Invent in '23,

53
00:02:05,700 --> 00:02:08,550
we started talking about
development productivity,

54
00:02:08,550 --> 00:02:11,970
the productivity of the team, the system.

55
00:02:11,970 --> 00:02:13,680
Last year at re:Invent,

56
00:02:13,680 --> 00:02:16,170
we were very clear in sort of
talking about the distinction

57
00:02:16,170 --> 00:02:19,110
between productivity and toil, right?

58
00:02:19,110 --> 00:02:21,750
Kind of two sides of the same coin.

59
00:02:21,750 --> 00:02:24,120
And then this year at New York Summit,

60
00:02:24,120 --> 00:02:29,120
Amazon released our framework
for how we think about impact

61
00:02:29,730 --> 00:02:31,110
in software development, right?

62
00:02:31,110 --> 00:02:32,730
This idea of cost to serve.

63
00:02:32,730 --> 00:02:35,700
And I'd be diving into
that a little bit later on.

64
00:02:35,700 --> 00:02:37,440
So as the tools have evolved,

65
00:02:37,440 --> 00:02:39,150
the conversations have evolved,

66
00:02:39,150 --> 00:02:41,790
and with all of the conversation
this week about agents

67
00:02:41,790 --> 00:02:43,470
and other types of experiences,

68
00:02:43,470 --> 00:02:47,430
this is only gonna get more
interesting, more complicated,

69
00:02:47,430 --> 00:02:49,740
depending on how you think about it.

70
00:02:49,740 --> 00:02:52,410
So before we get started
on what kind of works,

71
00:02:52,410 --> 00:02:55,260
there's some things
here that I come across

72
00:02:55,260 --> 00:02:57,330
pretty consistently with customers

73
00:02:57,330 --> 00:03:00,120
and this sort of idea of impact myths.

74
00:03:00,120 --> 00:03:03,510
The first one, AI can
fix complexity. Nope.

75
00:03:03,510 --> 00:03:05,280
If your processes are complicated,

76
00:03:05,280 --> 00:03:08,460
lots of steps, lots of
people, lots of interactions,

77
00:03:08,460 --> 00:03:09,813
AI can't fix that.

78
00:03:11,340 --> 00:03:13,650
It's all about creating 100x engineers.

79
00:03:13,650 --> 00:03:16,080
This also is not true.

80
00:03:16,080 --> 00:03:17,520
We see a lot of stories,

81
00:03:17,520 --> 00:03:19,410
maybe a bunch of things
on LinkedIn, right?

82
00:03:19,410 --> 00:03:21,900
About people creating
applications by themselves.

83
00:03:21,900 --> 00:03:26,370
But what we know is
that generally speaking,

84
00:03:26,370 --> 00:03:29,670
you gravitate to the performance
of the team that you're on.

85
00:03:29,670 --> 00:03:32,250
So yeah, you may have
one or two individuals

86
00:03:32,250 --> 00:03:34,890
who are able to do amazing things,

87
00:03:34,890 --> 00:03:38,310
but in the whole of your development org,

88
00:03:38,310 --> 00:03:41,310
to gravitate to the poor
performance of the team.

89
00:03:41,310 --> 00:03:43,170
Focusing on a single measure is enough.

90
00:03:43,170 --> 00:03:45,720
Now, I wish that this were true.

91
00:03:45,720 --> 00:03:47,730
I wish that there was some metric

92
00:03:47,730 --> 00:03:49,470
that we could say like that's the one.

93
00:03:49,470 --> 00:03:52,410
It is simply not the
case. It is not possible.

94
00:03:52,410 --> 00:03:54,900
And as I go through the next
sort of 15 to 20 minutes or so,

95
00:03:54,900 --> 00:03:56,670
I hope you'll get a sense why.

96
00:03:56,670 --> 00:03:59,910
And then also understand
how these other platforms

97
00:03:59,910 --> 00:04:00,810
and our customer

98
00:04:00,810 --> 00:04:03,630
have sort of worked
with a range of metrics.

99
00:04:03,630 --> 00:04:08,520
And then finally this idea
that we can just turn it on

100
00:04:08,520 --> 00:04:10,350
and people will figure it out.

101
00:04:10,350 --> 00:04:12,810
I've had a number of
conversations with customers

102
00:04:12,810 --> 00:04:14,220
when we're talking about this idea

103
00:04:14,220 --> 00:04:15,480
of enablement and training

104
00:04:15,480 --> 00:04:18,090
and how you're gonna let your folks know

105
00:04:18,090 --> 00:04:19,380
how all of these things work.

106
00:04:19,380 --> 00:04:21,810
We're like, nah, I'm
just gonna turn it on.

107
00:04:21,810 --> 00:04:23,070
Doesn't work.

108
00:04:23,070 --> 00:04:25,740
If you look at things like
the DORA report for this year,

109
00:04:25,740 --> 00:04:27,090
there's a lot of conversation there

110
00:04:27,090 --> 00:04:28,470
about structured enablement.

111
00:04:28,470 --> 00:04:30,540
We see it very much as well,

112
00:04:30,540 --> 00:04:32,460
you cannot simply turn these things on

113
00:04:32,460 --> 00:04:34,530
and hope it'll work out.

114
00:04:34,530 --> 00:04:36,540
So that's kind of what doesn't work.

115
00:04:36,540 --> 00:04:38,250
So Joe, what does?

116
00:04:38,250 --> 00:04:41,190
So if there's only one thing

117
00:04:41,190 --> 00:04:42,600
that I would hope you would take away

118
00:04:42,600 --> 00:04:45,330
from this conversation, it's this,

119
00:04:45,330 --> 00:04:47,310
and it's how we sort of think about

120
00:04:47,310 --> 00:04:49,440
applying AI thoughtfully.

121
00:04:49,440 --> 00:04:52,950
Yes, the software development,
but in many ways any process.

122
00:04:52,950 --> 00:04:57,510
First off, this idea of
elimination, what can you eliminate?

123
00:04:57,510 --> 00:04:59,520
I said AI can't fix complexity.

124
00:04:59,520 --> 00:05:01,740
So how do we start to remove some

125
00:05:01,740 --> 00:05:03,960
before we start doing other things?

126
00:05:03,960 --> 00:05:06,450
Manual steps, tools,
processes, checklists,

127
00:05:06,450 --> 00:05:07,860
all of those things.

128
00:05:07,860 --> 00:05:11,220
What are the opportunities
to eliminate those things?

129
00:05:11,220 --> 00:05:13,650
Also, manual interventions.

130
00:05:13,650 --> 00:05:15,750
What are the things
that we found internally

131
00:05:15,750 --> 00:05:18,600
was the very high number
of manual interventions

132
00:05:18,600 --> 00:05:20,040
in pipelines.

133
00:05:20,040 --> 00:05:21,480
How can you get rid of those things?

134
00:05:21,480 --> 00:05:24,330
What do you need to fix to
make those things go away?

135
00:05:24,330 --> 00:05:28,920
Automate, what is it
possible to automate with AI?

136
00:05:28,920 --> 00:05:33,180
These are typically lower
value, repetitive, right?

137
00:05:33,180 --> 00:05:36,300
Maybe relatively low
complexity type of things.

138
00:05:36,300 --> 00:05:39,180
I would say standard
things like documentation,

139
00:05:39,180 --> 00:05:40,500
test generation.

140
00:05:40,500 --> 00:05:43,290
You've also probably heard us talk about

141
00:05:43,290 --> 00:05:46,830
our Java JDK updates that we did.

142
00:05:46,830 --> 00:05:49,560
So automating the sort
of runtime upgrades,

143
00:05:49,560 --> 00:05:52,680
you'll see that in some of
our AWS transform agents.

144
00:05:52,680 --> 00:05:55,170
Those are the things where
we think about automating

145
00:05:55,170 --> 00:05:56,343
through the use of AI.

146
00:05:57,180 --> 00:05:58,923
And then finally, assisting.

147
00:05:59,910 --> 00:06:01,770
This really has been true, I think,

148
00:06:01,770 --> 00:06:03,390
maybe in the last six or eight months

149
00:06:03,390 --> 00:06:05,400
as the models have become better.

150
00:06:05,400 --> 00:06:08,430
But how do we now take advantage of AI

151
00:06:08,430 --> 00:06:10,440
running on your laptop as a dev,

152
00:06:10,440 --> 00:06:12,450
as a thought partner, as an assistant,

153
00:06:12,450 --> 00:06:16,080
as someone or something that
is capable of complexity

154
00:06:16,080 --> 00:06:18,600
and thought and really
being able to achieve

155
00:06:18,600 --> 00:06:21,360
high complexity infrequent tasks.

156
00:06:21,360 --> 00:06:24,630
So, you'll hear me refer to
this, eliminate, automate,

157
00:06:24,630 --> 00:06:25,590
and assist,

158
00:06:25,590 --> 00:06:27,300
but it's a really good way to think about

159
00:06:27,300 --> 00:06:30,033
where you are going to
prioritize and use AI.

160
00:06:31,410 --> 00:06:33,573
So what are some accelerators for impact?

161
00:06:35,190 --> 00:06:36,720
AI is a new way of working.

162
00:06:36,720 --> 00:06:38,400
I've had a couple of
customer conversations

163
00:06:38,400 --> 00:06:41,430
just this morning about this, right?

164
00:06:41,430 --> 00:06:44,730
We are in a place where
everybody's journey

165
00:06:44,730 --> 00:06:46,207
to your "aha" moment where you say,

166
00:06:46,207 --> 00:06:48,540
"Wow, this AI thing is really cool."

167
00:06:48,540 --> 00:06:50,490
Every individual's journey
is different, right?

168
00:06:50,490 --> 00:06:51,630
It's very personal.

169
00:06:51,630 --> 00:06:54,000
So how do we get to that point

170
00:06:54,000 --> 00:06:56,490
where your devs are able to say,

171
00:06:56,490 --> 00:06:58,173
ah, wow, this is really cool?

172
00:06:59,100 --> 00:07:01,200
By giving them permission to play.

173
00:07:01,200 --> 00:07:03,870
This is something that we
do here at Amazon a lot

174
00:07:03,870 --> 00:07:06,630
and we have a leadership
principle, then I'll be curious,

175
00:07:06,630 --> 00:07:08,940
which encourages us to take advantage,

176
00:07:08,940 --> 00:07:11,580
experiment with these new kinds of tools.

177
00:07:11,580 --> 00:07:15,060
What we know is that if
you have a development team

178
00:07:15,060 --> 00:07:17,130
that is on a delivery schedule

179
00:07:17,130 --> 00:07:19,680
and they have story points
and features to deliver,

180
00:07:19,680 --> 00:07:21,330
and then you're asking them to rethink

181
00:07:21,330 --> 00:07:23,760
the way they do the job at the same time,

182
00:07:23,760 --> 00:07:25,140
it's not gonna work.

183
00:07:25,140 --> 00:07:27,390
You have to be willing in the organization

184
00:07:27,390 --> 00:07:29,820
to give your dev team some slack

185
00:07:29,820 --> 00:07:31,950
in order to give them the brain space

186
00:07:31,950 --> 00:07:33,543
to actually be able to play.

187
00:07:34,620 --> 00:07:36,570
Software development is a team sport.

188
00:07:36,570 --> 00:07:38,310
We know this internally,

189
00:07:38,310 --> 00:07:43,310
and what we find is that
developers gravitate

190
00:07:43,350 --> 00:07:48,090
to the performance of the
team as its itself, right?

191
00:07:48,090 --> 00:07:50,250
It'll coalesce to the
performance of the team.

192
00:07:50,250 --> 00:07:52,080
So as you're thinking about these things,

193
00:07:52,080 --> 00:07:55,440
how do you accelerate the
performance of the entire team?

194
00:07:55,440 --> 00:07:57,660
How do we focus on those
kinds of enablements,

195
00:07:57,660 --> 00:07:59,190
that kind of activity, right?

196
00:07:59,190 --> 00:08:02,523
To really drive acceleration.

197
00:08:03,420 --> 00:08:05,610
Embracing sense making.

198
00:08:05,610 --> 00:08:07,443
What do I mean by sense making?

199
00:08:07,443 --> 00:08:10,260
So one of the things that
talking to customers even today

200
00:08:10,260 --> 00:08:11,910
around legacy code

201
00:08:11,910 --> 00:08:14,520
or bringing new developers
into existing projects

202
00:08:14,520 --> 00:08:17,970
or even onboarding new developers
into your organization,

203
00:08:17,970 --> 00:08:21,090
AI can speed up your new hires

204
00:08:21,090 --> 00:08:23,100
or ability to onboard to projects,

205
00:08:23,100 --> 00:08:25,350
to onboard into your organization.

206
00:08:25,350 --> 00:08:26,820
When you're looking at legacy code

207
00:08:26,820 --> 00:08:29,520
and you want to be able to
describe it, understand it,

208
00:08:29,520 --> 00:08:32,520
there's an enormous amount
of value in using AI

209
00:08:32,520 --> 00:08:34,050
in that role, right?

210
00:08:34,050 --> 00:08:39,050
You'll see this showing up
in things like key rules,

211
00:08:39,390 --> 00:08:40,470
steering documents,

212
00:08:40,470 --> 00:08:43,410
all of these various ways
to make your AI better.

213
00:08:43,410 --> 00:08:46,920
You can have your AI
help you do those things.

214
00:08:46,920 --> 00:08:50,130
And encourage AI fluency.
What I mean by that?

215
00:08:50,130 --> 00:08:52,770
So I'm gonna say that
we have the mechanics

216
00:08:52,770 --> 00:08:57,000
of your developers
understanding how to Q, Kiro,

217
00:08:57,000 --> 00:08:58,320
those kinds of tools,

218
00:08:58,320 --> 00:09:01,440
but there is real value to
be had from understanding

219
00:09:01,440 --> 00:09:03,120
more about the underpinnings of that.

220
00:09:03,120 --> 00:09:05,130
My team and I have done
some really deep dives

221
00:09:05,130 --> 00:09:06,600
with customers this year,

222
00:09:06,600 --> 00:09:10,020
and what we found is that
this idea of AI fluency,

223
00:09:10,020 --> 00:09:11,640
which I'll dive into more in a moment,

224
00:09:11,640 --> 00:09:14,700
is really important to get
the most impact possible

225
00:09:14,700 --> 00:09:16,100
out of these kinds of tools.

226
00:09:17,160 --> 00:09:20,340
So, let's talk about
fluency for a little bit.

227
00:09:20,340 --> 00:09:22,110
Two broad buckets here that I think of.

228
00:09:22,110 --> 00:09:23,850
The first one is prompting.

229
00:09:23,850 --> 00:09:26,177
Who heard of prompt
engineering a year ago, right?

230
00:09:26,177 --> 00:09:27,270
It was a big thing.

231
00:09:27,270 --> 00:09:29,310
Everyone was gonna be
like prompt engineers.

232
00:09:29,310 --> 00:09:30,900
What we know that that's helpful.

233
00:09:30,900 --> 00:09:32,850
And understanding how
to write a good prompt

234
00:09:32,850 --> 00:09:34,293
is really valuable,

235
00:09:35,460 --> 00:09:36,870
and the tools can't read your mind.

236
00:09:36,870 --> 00:09:39,930
It's sort of trite to say
it, but it's very true.

237
00:09:39,930 --> 00:09:41,700
And we have to work on this idea

238
00:09:41,700 --> 00:09:44,910
of making the implicit explicit.

239
00:09:44,910 --> 00:09:48,210
I've been an engineer pretty
much my entire career.

240
00:09:48,210 --> 00:09:50,610
I have things in my head,
I don't say them out loud.

241
00:09:50,610 --> 00:09:51,960
And I expect people to know.

242
00:09:51,960 --> 00:09:54,000
AI can't read your mind in that same way.

243
00:09:54,000 --> 00:09:56,040
We have to be precise.

244
00:09:56,040 --> 00:09:57,270
Precision.

245
00:09:57,270 --> 00:09:59,280
If you wanted to write test cases,

246
00:09:59,280 --> 00:10:02,610
tell it write test cases
and the format that you want

247
00:10:02,610 --> 00:10:04,590
and the framework that you use

248
00:10:04,590 --> 00:10:06,900
will make it much, much more effective.

249
00:10:06,900 --> 00:10:08,403
Structured problem solving.

250
00:10:09,420 --> 00:10:11,610
Anyone heard of spec-driven development?

251
00:10:11,610 --> 00:10:13,680
Show of hands, couple folks, right?

252
00:10:13,680 --> 00:10:16,140
So what we're starting to see is moving,

253
00:10:16,140 --> 00:10:18,240
and this happened with recent models,

254
00:10:18,240 --> 00:10:22,380
is the models being able
to build actually a plan

255
00:10:22,380 --> 00:10:23,910
for how they're gonna break things down

256
00:10:23,910 --> 00:10:25,770
and do things in small pieces.

257
00:10:25,770 --> 00:10:28,500
When we are interacting with models,

258
00:10:28,500 --> 00:10:31,140
and I certainly have seen
this in the early days

259
00:10:31,140 --> 00:10:32,280
where you would say something like,

260
00:10:32,280 --> 00:10:33,530
build me a shopping cart.

261
00:10:34,710 --> 00:10:37,110
Sure, but that's not precise.

262
00:10:37,110 --> 00:10:38,460
It doesn't break it into tasks.

263
00:10:38,460 --> 00:10:40,320
It doesn't really tell you what you want.

264
00:10:40,320 --> 00:10:42,750
And what you're trying
to do is have the model

265
00:10:42,750 --> 00:10:45,540
give you what you want and not off-road.

266
00:10:45,540 --> 00:10:46,500
And the way you do that

267
00:10:46,500 --> 00:10:48,660
is with more structured problem solving.

268
00:10:48,660 --> 00:10:49,740
And then finally,

269
00:10:49,740 --> 00:10:51,630
the thing that I think
a lot of folks forget

270
00:10:51,630 --> 00:10:53,640
is you can ask the tools for help.

271
00:10:53,640 --> 00:10:56,310
Hey, can you summarize this
conversation into a prompt?

272
00:10:56,310 --> 00:10:58,980
What else do you need to know?
How else might we write this?

273
00:10:58,980 --> 00:11:00,630
What is missing from this prompt, right?

274
00:11:00,630 --> 00:11:03,600
Using the tools to help
yourself be better with them.

275
00:11:03,600 --> 00:11:04,443
Really powerful.

276
00:11:05,880 --> 00:11:08,640
So let's talk about context,
the other side of this.

277
00:11:08,640 --> 00:11:10,980
Context. What else do you need to know?

278
00:11:10,980 --> 00:11:14,670
So context engineering has
really been a conversation mainly

279
00:11:14,670 --> 00:11:17,070
that I've seen sort of
in this year, really,

280
00:11:17,070 --> 00:11:18,750
as the models have become better,

281
00:11:18,750 --> 00:11:21,633
the context windows have
grown, the reasonings improved,

282
00:11:23,130 --> 00:11:25,170
same problem occurs, right?

283
00:11:25,170 --> 00:11:27,060
We're trying to use additional context

284
00:11:27,060 --> 00:11:30,060
to make implicit explicit.

285
00:11:30,060 --> 00:11:31,560
A couple of examples.

286
00:11:31,560 --> 00:11:33,900
Q could create its own rules files,

287
00:11:33,900 --> 00:11:36,450
Kiro can recreate steering documents.

288
00:11:36,450 --> 00:11:38,760
You may have seen agents.md, right?

289
00:11:38,760 --> 00:11:43,760
Ways to have context in your repos

290
00:11:43,950 --> 00:11:46,233
that can help guide
these tools every time.

291
00:11:47,280 --> 00:11:50,340
More is better, but only to a point.

292
00:11:50,340 --> 00:11:53,250
In the same way, if you are
onboarding a new developer

293
00:11:53,250 --> 00:11:54,870
into your projects,

294
00:11:54,870 --> 00:11:57,510
you wouldn't necessarily
have them read every repo

295
00:11:57,510 --> 00:11:59,190
and every application you've ever built.

296
00:11:59,190 --> 00:12:01,800
You wouldn't ask them to
read all of your confluence.

297
00:12:01,800 --> 00:12:03,510
What you would do would be point them

298
00:12:03,510 --> 00:12:07,350
to more specific examples
that would be valuable.

299
00:12:07,350 --> 00:12:10,890
That's the idea of getting
to the right context.

300
00:12:10,890 --> 00:12:13,470
And you'll see a number of
innovations that we've had with Q

301
00:12:13,470 --> 00:12:17,250
and Kiro this year around
constraining agents in the CLI,

302
00:12:17,250 --> 00:12:18,690
being able to ping context,

303
00:12:18,690 --> 00:12:21,600
being able to be more
specific with tool usage.

304
00:12:21,600 --> 00:12:23,430
All of those things are trying to drive

305
00:12:23,430 --> 00:12:26,163
to the right context, not just more.

306
00:12:27,060 --> 00:12:28,833
MCP is incredibly powerful.

307
00:12:29,730 --> 00:12:32,520
Where we were six or eight months ago was,

308
00:12:32,520 --> 00:12:34,110
well, just more MCP tools, right?

309
00:12:34,110 --> 00:12:37,170
I'm gonna give it connections
to everything I see.

310
00:12:37,170 --> 00:12:41,250
But what we know is that I
can very quickly overwhelm

311
00:12:41,250 --> 00:12:44,850
both the tools and the
models and everything else.

312
00:12:44,850 --> 00:12:46,350
So it's very, very useful.

313
00:12:46,350 --> 00:12:47,850
But be thoughtful.

314
00:12:47,850 --> 00:12:49,890
And again, ask for help.

315
00:12:49,890 --> 00:12:51,210
Which of these tools would be helpful?

316
00:12:51,210 --> 00:12:52,170
What else do you need to know?

317
00:12:52,170 --> 00:12:54,660
I have this other file, does
that make this better, right?

318
00:12:54,660 --> 00:12:58,740
Engage in this idea of it being assistive

319
00:12:58,740 --> 00:13:00,570
rather than you having
to prescribe everything.

320
00:13:00,570 --> 00:13:03,063
Ask it how it can help
you be more effective.

321
00:13:04,110 --> 00:13:07,590
So, gone through what doesn't work.

322
00:13:07,590 --> 00:13:09,510
Some things that we know do work,

323
00:13:09,510 --> 00:13:10,980
in order to get this idea

324
00:13:10,980 --> 00:13:13,070
of how we then measure the impact, right?

325
00:13:13,070 --> 00:13:15,630
We talked about how to make it better.

326
00:13:15,630 --> 00:13:18,930
So measuring impact is a journey.

327
00:13:18,930 --> 00:13:20,850
If any of you have followed the content

328
00:13:20,850 --> 00:13:22,470
that we've had for the
past couple three years,

329
00:13:22,470 --> 00:13:24,120
we've talked about this a lot.

330
00:13:24,120 --> 00:13:28,290
There are industry frameworks
like DORA and SPACE,

331
00:13:28,290 --> 00:13:29,880
a couple of my customer
meetings this morning,

332
00:13:29,880 --> 00:13:32,790
We're talking about these same things.

333
00:13:32,790 --> 00:13:35,220
I'm gonna talk about the Amazon framework,

334
00:13:35,220 --> 00:13:36,690
cost to serve software,

335
00:13:36,690 --> 00:13:38,640
which is something that we've explained

336
00:13:38,640 --> 00:13:39,993
how we think about it.

337
00:13:41,574 --> 00:13:44,010
And Krishna will show you this, right?

338
00:13:44,010 --> 00:13:46,440
It is a basket of measures.

339
00:13:46,440 --> 00:13:48,300
There are a number of metrics

340
00:13:48,300 --> 00:13:50,880
and data points that
depending on where you,

341
00:13:50,880 --> 00:13:54,093
and in fact your teams are
in their journey, will vary.

342
00:13:55,800 --> 00:13:58,440
Qualitative and quantitative indicators

343
00:13:58,440 --> 00:14:00,420
are incredibly valuable.

344
00:14:00,420 --> 00:14:04,260
Qualitative, the why,
quantitative, the what, right?

345
00:14:04,260 --> 00:14:06,600
So quantitative coming out of tools,

346
00:14:06,600 --> 00:14:08,463
qualitative coming from your humans.

347
00:14:10,320 --> 00:14:13,530
And there's a lot of value in
pulling those things together.

348
00:14:13,530 --> 00:14:16,410
And I'll give you an
example in just a sec.

349
00:14:16,410 --> 00:14:19,470
And then if I was gonna say to you today,

350
00:14:19,470 --> 00:14:21,513
which is a lot of customers ask me,

351
00:14:22,410 --> 00:14:26,130
Hey, we were looking for
gains in productivity.

352
00:14:26,130 --> 00:14:28,590
I would say, well, how
do you baseline it today?

353
00:14:28,590 --> 00:14:30,330
What tools do you have in place?

354
00:14:30,330 --> 00:14:32,910
How do you think about a
measure productivity today?

355
00:14:32,910 --> 00:14:37,050
My suspicion is I know many
of my customers say we don't.

356
00:14:37,050 --> 00:14:41,130
It's ad hoc, we guess, we've
got some bits and pieces.

357
00:14:41,130 --> 00:14:42,630
I suspect that a number of you today

358
00:14:42,630 --> 00:14:43,890
are sitting here in that same boat.

359
00:14:43,890 --> 00:14:47,730
You've got some idea, some
tooling, some data points.

360
00:14:47,730 --> 00:14:51,280
But really the idea is that you
baseline where you are today

361
00:14:52,200 --> 00:14:54,980
and then look at those
trends shift over time,

362
00:14:54,980 --> 00:14:56,730
is the way to think about this.

363
00:14:56,730 --> 00:15:00,660
And finally, they're both
leading and lagging indicators.

364
00:15:00,660 --> 00:15:02,880
What we often find is the customers,

365
00:15:02,880 --> 00:15:04,980
and a lot of times it's
the finance people,

366
00:15:04,980 --> 00:15:09,360
want to jump to the ROI piece,
the quantitative impact.

367
00:15:09,360 --> 00:15:11,580
But we haven't gone through
the leading indicators

368
00:15:11,580 --> 00:15:13,620
around things like adoption, engagement.

369
00:15:13,620 --> 00:15:15,720
How are people using
it? What's the feedback?

370
00:15:15,720 --> 00:15:17,310
Do they feel good?

371
00:15:17,310 --> 00:15:18,840
And so they're both lead,

372
00:15:18,840 --> 00:15:20,970
and so I'll explain more about that.

373
00:15:20,970 --> 00:15:23,190
But there's basically this idea of leading

374
00:15:23,190 --> 00:15:24,483
and lagging indicators.

375
00:15:26,130 --> 00:15:29,080
So, lemme give you an example
of how those things play out.

376
00:15:30,210 --> 00:15:31,860
So let's assume you wanted to roll out

377
00:15:31,860 --> 00:15:33,740
Kiro in your organization.

378
00:15:33,740 --> 00:15:35,490
How might you use the metrics

379
00:15:35,490 --> 00:15:37,530
and how might they change over time?

380
00:15:37,530 --> 00:15:40,833
So in the first instance, what
you care about is adoption.

381
00:15:41,670 --> 00:15:43,950
Who's got a license? Who's signed in?

382
00:15:43,950 --> 00:15:45,810
Who has used the tool? Who's enabled it?

383
00:15:45,810 --> 00:15:48,060
Has anyone got authentication problems?

384
00:15:48,060 --> 00:15:51,090
You're working through all
of those basic things, right?

385
00:15:51,090 --> 00:15:53,160
You're basically managing a rollout.

386
00:15:53,160 --> 00:15:56,250
And what you want to do is
observe that early usage

387
00:15:56,250 --> 00:15:58,830
and see if you can get any
interesting insights from it.

388
00:15:58,830 --> 00:15:59,670
But at this stage,

389
00:15:59,670 --> 00:16:01,560
it's a lot more about
does everybody have it?

390
00:16:01,560 --> 00:16:02,640
Do they know how to use it?

391
00:16:02,640 --> 00:16:05,313
Rather than trying to drive
to an ROA conversation.

392
00:16:06,300 --> 00:16:07,860
Then we move to engagement.

393
00:16:07,860 --> 00:16:10,260
Now I'm looking at different metrics.

394
00:16:10,260 --> 00:16:14,790
Now what I'm looking at who
are my highly engaged humans?

395
00:16:14,790 --> 00:16:17,340
Which are the teams that
have strong adoption,

396
00:16:17,340 --> 00:16:19,410
which are the teams that don't, right?

397
00:16:19,410 --> 00:16:21,630
You've heard me describe multiple times

398
00:16:21,630 --> 00:16:23,610
that team is where this is,

399
00:16:23,610 --> 00:16:25,290
you're looking to enable teams, right?

400
00:16:25,290 --> 00:16:26,700
Team impact is important.

401
00:16:26,700 --> 00:16:28,800
So which teams are on
board, which are not?

402
00:16:28,800 --> 00:16:32,250
How are your individual teams
internally sharing wins?

403
00:16:32,250 --> 00:16:33,360
Are you sharing wins?

404
00:16:33,360 --> 00:16:34,740
Do you know of people

405
00:16:34,740 --> 00:16:37,470
that have had some really
interesting stories?

406
00:16:37,470 --> 00:16:40,470
And how can you then mitigate
low engagement, right?

407
00:16:40,470 --> 00:16:42,270
Craig and I were talking
about this internally,

408
00:16:42,270 --> 00:16:44,670
and he has a great story about how Genesys

409
00:16:44,670 --> 00:16:46,380
was able to use these metrics to identify

410
00:16:46,380 --> 00:16:48,150
and mitigate low engagement, right?

411
00:16:48,150 --> 00:16:49,560
You might go and talk to people

412
00:16:49,560 --> 00:16:52,740
and find out that it's something
simple you can remediate.

413
00:16:52,740 --> 00:16:56,010
The qualitative, the why, surveys,

414
00:16:56,010 --> 00:16:58,320
talking to people, walking the halls,

415
00:16:58,320 --> 00:17:00,390
all things that we use internally

416
00:17:00,390 --> 00:17:04,530
and we've seen customers use
to get to understand why.

417
00:17:04,530 --> 00:17:06,120
Look over people's shoulders, right?

418
00:17:06,120 --> 00:17:08,720
Really just observe and
understand what's happening.

419
00:17:09,600 --> 00:17:12,060
And then finally we get
into the quantitative.

420
00:17:12,060 --> 00:17:14,220
So now we're talking
about measuring impact

421
00:17:14,220 --> 00:17:15,660
to your baselines.

422
00:17:15,660 --> 00:17:18,090
In order to have
conversations with executives,

423
00:17:18,090 --> 00:17:20,250
conversations with finance folks,

424
00:17:20,250 --> 00:17:22,410
you need to be able to have a baseline

425
00:17:22,410 --> 00:17:23,640
and then show some improvement.

426
00:17:23,640 --> 00:17:25,653
Hey, we improved X to Y.

427
00:17:26,490 --> 00:17:29,160
That's why we encourage you
to look at the quantitative

428
00:17:29,160 --> 00:17:31,740
as a set of lagging indicators

429
00:17:31,740 --> 00:17:34,680
after you know that people
are using the tools,

430
00:17:34,680 --> 00:17:36,423
they're engaged and they enjoy it.

431
00:17:38,610 --> 00:17:40,923
So, what about the tooling?

432
00:17:42,090 --> 00:17:45,510
So if I think about the
tooling that comes from Kiro

433
00:17:45,510 --> 00:17:46,770
and Q Developer,

434
00:17:46,770 --> 00:17:49,410
and frankly what is likely to come from

435
00:17:49,410 --> 00:17:52,440
all of these various agents
that we're all talking about,

436
00:17:52,440 --> 00:17:55,320
it's going to be activity
that maps to the A

437
00:17:55,320 --> 00:17:57,390
in the SPACE framework, right?

438
00:17:57,390 --> 00:17:59,940
You'll notice there are
four other letters there

439
00:17:59,940 --> 00:18:00,940
that we don't touch.

440
00:18:03,420 --> 00:18:05,880
The data that we provide
can be used to drive

441
00:18:05,880 --> 00:18:07,830
to sort of engage, figure out adoption,

442
00:18:07,830 --> 00:18:09,690
figure out engagement to a point.

443
00:18:09,690 --> 00:18:13,230
And I'll give you an idea of
the differences in a minute.

444
00:18:13,230 --> 00:18:14,580
But what we cannot tell you,

445
00:18:14,580 --> 00:18:16,620
because we don't see
enough of what's happening

446
00:18:16,620 --> 00:18:18,300
inside your organizations,

447
00:18:18,300 --> 00:18:20,640
is the AI impact, right?

448
00:18:20,640 --> 00:18:22,290
You can't tell AI impact

449
00:18:22,290 --> 00:18:24,210
simply by looking at the use of a tool.

450
00:18:24,210 --> 00:18:27,540
You have to look at other things,
other baskets of measures,

451
00:18:27,540 --> 00:18:29,883
other data sources
within your organization.

452
00:18:32,580 --> 00:18:34,200
The other thing that's really interesting

453
00:18:34,200 --> 00:18:37,470
and why this whole conversation
of developer productivity

454
00:18:37,470 --> 00:18:40,020
has moved into AI impact,

455
00:18:40,020 --> 00:18:42,723
is this idea of second order impacts.

456
00:18:43,950 --> 00:18:45,540
When I talk to a lot of executives,

457
00:18:45,540 --> 00:18:49,230
the idea of these tools is,
oh, can I write more code?

458
00:18:49,230 --> 00:18:52,773
Sure, but where else is there
value? What else is happening?

459
00:18:54,030 --> 00:18:56,040
We measure this internally,

460
00:18:56,040 --> 00:18:58,230
I've talked for a number
of customers about it,

461
00:18:58,230 --> 00:19:01,260
and it's this idea of
reduction of onboarding time.

462
00:19:01,260 --> 00:19:04,050
How long does it take for a new developer

463
00:19:04,050 --> 00:19:06,400
to reach the speed of
the team that they're on?

464
00:19:07,740 --> 00:19:10,950
Time to first commit, time to
10th commit, typical measures.

465
00:19:10,950 --> 00:19:14,673
But that can be a huge second
order impact of the use of AI.

466
00:19:16,260 --> 00:19:18,300
Both of these apply here at Amazon.

467
00:19:18,300 --> 00:19:20,190
We tend to move developers around

468
00:19:20,190 --> 00:19:22,230
between projects quite frequently.

469
00:19:22,230 --> 00:19:24,660
And so the ability to
reduce the time it takes

470
00:19:24,660 --> 00:19:28,620
to take a new developer or
a developer onto a new team

471
00:19:28,620 --> 00:19:30,720
and get them up to speed
is really important

472
00:19:30,720 --> 00:19:32,610
and it's impactful for us.

473
00:19:32,610 --> 00:19:34,230
The increase in skill flexibility

474
00:19:34,230 --> 00:19:36,030
is something I was talking to earlier

475
00:19:36,030 --> 00:19:38,310
when a customer was asking
me about skill makeup, right?

476
00:19:38,310 --> 00:19:40,800
What do you think team makeup looks like?

477
00:19:40,800 --> 00:19:43,410
We talk internally about a
couple different stories.

478
00:19:43,410 --> 00:19:46,500
One of which is when we
were writing the QCLI,

479
00:19:46,500 --> 00:19:48,570
we wanted to be able
to write that in Rust.

480
00:19:48,570 --> 00:19:50,550
We had a shortage of Rust folks,

481
00:19:50,550 --> 00:19:54,030
so we were able to take Q
basically and build itself,

482
00:19:54,030 --> 00:19:58,170
and have experienced Rust
programmers do the code reviews.

483
00:19:58,170 --> 00:20:01,470
So we're able to increase the
flexibility of the skills.

484
00:20:01,470 --> 00:20:04,950
You might also see this, for example,

485
00:20:04,950 --> 00:20:07,050
maybe you've got a shortage
of front end folks,

486
00:20:07,050 --> 00:20:09,150
but you've got some
backend folks with time.

487
00:20:09,150 --> 00:20:11,610
Is there opportunity to take
advantage of the tooling

488
00:20:11,610 --> 00:20:14,100
as an assistant to expand the skillset,

489
00:20:14,100 --> 00:20:15,303
increase flexibility.

490
00:20:16,140 --> 00:20:18,123
That's a great second order impact.

491
00:20:19,350 --> 00:20:20,183
Shifting right.

492
00:20:21,360 --> 00:20:23,220
Shifting into Cloud operations.

493
00:20:23,220 --> 00:20:25,800
We've seen a lot of customers
take advantage of tools

494
00:20:25,800 --> 00:20:29,370
like Q and Kiro moving
into their Cloud ops teams.

495
00:20:29,370 --> 00:20:30,900
Site reliability engineering,

496
00:20:30,900 --> 00:20:32,490
Cloud operations, troubleshooting,

497
00:20:32,490 --> 00:20:36,720
all of those things can
take advantage of AI,

498
00:20:36,720 --> 00:20:38,880
context engineering, prompt engineering,

499
00:20:38,880 --> 00:20:40,500
you could take advantage of those.

500
00:20:40,500 --> 00:20:45,150
That's not directly related
to the developer necessarily.

501
00:20:45,150 --> 00:20:48,030
And this would show up in
things like DORA metrics, right?

502
00:20:48,030 --> 00:20:51,360
Other places that would
show you the impact of AI

503
00:20:51,360 --> 00:20:53,160
in your Cloud ops team.

504
00:20:53,160 --> 00:20:56,250
We also see it recently moving left

505
00:20:56,250 --> 00:20:58,980
into product management organizations.

506
00:20:58,980 --> 00:21:00,510
I've talked to a number of folks there

507
00:21:00,510 --> 00:21:01,680
who are trying to understand

508
00:21:01,680 --> 00:21:04,470
what's the value of Vibe Coding, right?

509
00:21:04,470 --> 00:21:06,570
We've got technically
minded product managers

510
00:21:06,570 --> 00:21:07,950
and product owners

511
00:21:07,950 --> 00:21:10,200
who have been able to build prototypes,

512
00:21:10,200 --> 00:21:14,160
and it takes a lot of time out
of that spec design process.

513
00:21:14,160 --> 00:21:15,930
People can react to those.

514
00:21:15,930 --> 00:21:18,810
How are you able to
represent that in the metrics

515
00:21:18,810 --> 00:21:21,873
and data that you capture is
really interesting, right?

516
00:21:24,450 --> 00:21:25,300
One of the things

517
00:21:26,582 --> 00:21:27,960
that when the three of us
were talking about this,

518
00:21:27,960 --> 00:21:29,610
one thing that came up consistently,

519
00:21:29,610 --> 00:21:32,940
what are the projects that
would never have been done

520
00:21:32,940 --> 00:21:35,100
if this tooling didn't exist?

521
00:21:35,100 --> 00:21:38,610
We talk about internally,
there was a team at Amazon

522
00:21:38,610 --> 00:21:41,370
that was responsible for
heating and air conditioning,

523
00:21:41,370 --> 00:21:43,740
who needed some simple app built,

524
00:21:43,740 --> 00:21:45,000
couldn't seem to get time,

525
00:21:45,000 --> 00:21:48,210
so they were able to take AI
and build it themselves, right?

526
00:21:48,210 --> 00:21:49,770
What are the opportunities there

527
00:21:49,770 --> 00:21:51,810
to undertake projects that
would've never happened?

528
00:21:51,810 --> 00:21:55,530
How do you find that? Represent
it, calculate it, right?

529
00:21:55,530 --> 00:21:56,553
There's value there.

530
00:21:58,140 --> 00:22:00,400
And then the other
thing that happens a lot

531
00:22:02,010 --> 00:22:02,970
when I talk to customers,

532
00:22:02,970 --> 00:22:05,940
especially ones who are
undergoing a lot of change

533
00:22:05,940 --> 00:22:07,260
in their dev org,

534
00:22:07,260 --> 00:22:09,750
is how do I isolate the impact of AI

535
00:22:09,750 --> 00:22:12,960
relative to these other changes
that we're making, right?

536
00:22:12,960 --> 00:22:14,820
We have a complicated system.

537
00:22:14,820 --> 00:22:17,700
I might be doing a dev portal
and changing my tooling

538
00:22:17,700 --> 00:22:19,260
and merging with a company.

539
00:22:19,260 --> 00:22:21,010
All of these things add complexity.

540
00:22:22,260 --> 00:22:24,720
So it's really intriguing, right?

541
00:22:24,720 --> 00:22:26,790
We've gone from this idea
of an individual human

542
00:22:26,790 --> 00:22:28,530
getting code completion

543
00:22:28,530 --> 00:22:30,480
to we're able to take
advantage of essentially

544
00:22:30,480 --> 00:22:33,870
the same tools in multiple
places in the organization

545
00:22:33,870 --> 00:22:36,873
to drive a much larger AI impact.

546
00:22:38,220 --> 00:22:40,413
So how do we do it, right?

547
00:22:41,850 --> 00:22:46,440
Luckily here at Amazon we're
a large organization, right?

548
00:22:46,440 --> 00:22:48,060
AWS Amazon, we have lots
of different businesses,

549
00:22:48,060 --> 00:22:49,890
and so we tend to look around

550
00:22:49,890 --> 00:22:51,840
and see where there are ideas of things

551
00:22:51,840 --> 00:22:52,673
that have worked before.

552
00:22:52,673 --> 00:22:54,150
Where can we learn?

553
00:22:54,150 --> 00:22:55,470
And so what we looked around,

554
00:22:55,470 --> 00:22:57,780
we looked at the supply chain work

555
00:22:57,780 --> 00:23:01,200
that we've done with amazon.com

556
00:23:01,200 --> 00:23:02,850
and this idea of cost to serve.

557
00:23:02,850 --> 00:23:05,800
It's something we have talked
about in shareholder letters.

558
00:23:07,825 --> 00:23:09,465
And so the cost to serve at Amazon

559
00:23:09,465 --> 00:23:11,820
is all about this idea of
we have units, packages,

560
00:23:11,820 --> 00:23:13,170
and customer delivery

561
00:23:13,170 --> 00:23:18,170
and how can we reduce the
friction, delay, waste,

562
00:23:18,600 --> 00:23:20,670
and defects in that process.

563
00:23:20,670 --> 00:23:23,850
Anytime we can do that, we are
reducing the cost to serve.

564
00:23:23,850 --> 00:23:25,440
The cost for us

565
00:23:25,440 --> 00:23:28,470
to be able to get value
into our customer's hands.

566
00:23:28,470 --> 00:23:30,870
And so we looked at this and we said, huh,

567
00:23:30,870 --> 00:23:32,400
that's sort of intriguing,

568
00:23:32,400 --> 00:23:34,380
because software, especially here,

569
00:23:34,380 --> 00:23:36,420
is a complicated supply chain.

570
00:23:36,420 --> 00:23:38,670
It's all kinds of different interactions.

571
00:23:38,670 --> 00:23:42,810
We have everything from, you know,

572
00:23:42,810 --> 00:23:47,810
hardware build to software
running on our hardware devices

573
00:23:48,630 --> 00:23:51,300
to AWS to .com,

574
00:23:51,300 --> 00:23:53,190
to all of these various businesses

575
00:23:53,190 --> 00:23:55,020
using a variety of different things,

576
00:23:55,020 --> 00:23:56,433
and it's complicated.

577
00:23:57,540 --> 00:23:59,730
And so if we look at this opportunity

578
00:23:59,730 --> 00:24:03,480
for friction, delay,
defects, and waste, anywhere,

579
00:24:03,480 --> 00:24:05,230
that can improve our cost to serve.

580
00:24:06,150 --> 00:24:07,650
Intriguing.

581
00:24:07,650 --> 00:24:09,750
There is a science paper

582
00:24:09,750 --> 00:24:11,700
and we've published more
information about this

583
00:24:11,700 --> 00:24:13,200
if you really wanna dive into the science.

584
00:24:13,200 --> 00:24:17,520
But the fundamental equation
is the cost to serve software

585
00:24:17,520 --> 00:24:19,260
is the infrastructure costs,

586
00:24:19,260 --> 00:24:21,150
the sort of the human and tool costs,

587
00:24:21,150 --> 00:24:23,343
divided by the number of units.

588
00:24:24,540 --> 00:24:26,640
The number of units will vary

589
00:24:26,640 --> 00:24:30,690
if you are shipping a mobile app to an API

590
00:24:30,690 --> 00:24:33,780
to a monolith to something else.

591
00:24:33,780 --> 00:24:36,450
So the number of units is
really whatever the unit is

592
00:24:36,450 --> 00:24:39,810
that your team typically
thinks of as a unit.

593
00:24:39,810 --> 00:24:40,920
But ultimately,

594
00:24:40,920 --> 00:24:44,040
this is something that
you can track over time

595
00:24:44,040 --> 00:24:46,080
and it does allow for the complexity,

596
00:24:46,080 --> 00:24:48,840
and it does allow the acknowledgement

597
00:24:48,840 --> 00:24:52,020
of some of those second order
impacts that I described.

598
00:24:52,020 --> 00:24:56,070
Luckily we have teams of
folks that can do this work,

599
00:24:56,070 --> 00:24:58,530
but we were also able to
isolate some of the ways

600
00:24:58,530 --> 00:24:59,730
that we improved

601
00:24:59,730 --> 00:25:01,830
or we're able to reduce our cost to serve.

602
00:25:02,910 --> 00:25:05,310
CICD. It works, we have data.

603
00:25:05,310 --> 00:25:06,630
One of my colleagues was very happy

604
00:25:06,630 --> 00:25:08,820
about being able to make that statement.

605
00:25:08,820 --> 00:25:10,470
Managed templates and abstractions

606
00:25:10,470 --> 00:25:12,390
is something we found to be really useful,

607
00:25:12,390 --> 00:25:13,893
and of course, AI.

608
00:25:14,880 --> 00:25:17,100
Any individual one of these was helpful,

609
00:25:17,100 --> 00:25:18,600
but by bringing them all together,

610
00:25:18,600 --> 00:25:21,090
we really were able to
drive a lot of value

611
00:25:21,090 --> 00:25:23,670
and a lot of improvement
across our processes.

612
00:25:23,670 --> 00:25:26,630
Ultimately, with a reduction of almost 16%

613
00:25:26,630 --> 00:25:29,460
on our cost to serve
across the organization.

614
00:25:29,460 --> 00:25:32,280
For a development
organization the size of ours,

615
00:25:32,280 --> 00:25:34,830
that is a very material impact.

616
00:25:34,830 --> 00:25:38,670
But you'll notice I didn't say
a coding companion did that.

617
00:25:38,670 --> 00:25:40,140
This is the impact of change

618
00:25:40,140 --> 00:25:42,303
across your software
development processes.

619
00:25:43,140 --> 00:25:46,440
Now we're lucky that we
have the size and scale

620
00:25:46,440 --> 00:25:49,320
and folks to be able to
do this for ourselves,

621
00:25:49,320 --> 00:25:52,530
but what I'd love to do now is
call Krishna up to the stage

622
00:25:52,530 --> 00:25:55,170
to talk you through how
our partner, Jellyfish,

623
00:25:55,170 --> 00:25:56,133
thinks about this.

624
00:25:57,090 --> 00:25:57,923
Krishna?

625
00:26:02,310 --> 00:26:04,470
- Alright, thanks Joe.

626
00:26:04,470 --> 00:26:07,200
My name's Krishna Kannan,
I work for Jellyfish.

627
00:26:07,200 --> 00:26:09,030
I lead our product organization,

628
00:26:09,030 --> 00:26:10,440
and we're really excited to be here

629
00:26:10,440 --> 00:26:12,360
talking with our friends at Amazon

630
00:26:12,360 --> 00:26:16,230
and our partners at Genesys
about measuring AI impact.

631
00:26:16,230 --> 00:26:19,080
So I'm gonna take a half
step back here for a second

632
00:26:19,080 --> 00:26:21,780
and talk about measuring
engineering productivity,

633
00:26:21,780 --> 00:26:23,940
because that forms the backbone
of what we'll talk about

634
00:26:23,940 --> 00:26:25,233
in terms of measuring AI.

635
00:26:26,730 --> 00:26:29,370
So Jellyfish has been measuring
developer productivity

636
00:26:29,370 --> 00:26:31,260
for several years now,

637
00:26:31,260 --> 00:26:32,880
and thinking about how to do that.

638
00:26:32,880 --> 00:26:36,540
And our approach has been
first to take a look at data

639
00:26:36,540 --> 00:26:39,300
from your issue tracking system,

640
00:26:39,300 --> 00:26:43,500
whether that's a Jira or a
linear app or an Azure DevOps,

641
00:26:43,500 --> 00:26:45,960
and combine that with
your source control data.

642
00:26:45,960 --> 00:26:47,610
And that gives us a couple things.

643
00:26:47,610 --> 00:26:51,900
One, that gives us the business
context and project context

644
00:26:51,900 --> 00:26:53,820
for what you're working on and why.

645
00:26:53,820 --> 00:26:56,160
And then combine that with
the source control data,

646
00:26:56,160 --> 00:26:58,050
like your GitHub or your Bitbucket,

647
00:26:58,050 --> 00:27:01,140
helps us understand how long
it actually took to do that,

648
00:27:01,140 --> 00:27:03,810
what language was in,
all the other metadata.

649
00:27:03,810 --> 00:27:06,240
So we can create a signal
of what folks worked on

650
00:27:06,240 --> 00:27:07,200
through that.

651
00:27:07,200 --> 00:27:10,590
And then we combine that with
data from your HR system,

652
00:27:10,590 --> 00:27:14,460
your CICD pipelines, your
error reports, incidents,

653
00:27:14,460 --> 00:27:15,810
to really get a complete picture

654
00:27:15,810 --> 00:27:18,900
of how engineering's happening
at your organization.

655
00:27:18,900 --> 00:27:21,270
And so we've been doing this
for a number of years now,

656
00:27:21,270 --> 00:27:24,600
and through that we produce
developer productivity metrics

657
00:27:24,600 --> 00:27:25,680
among others.

658
00:27:25,680 --> 00:27:27,600
And I expect many of you

659
00:27:27,600 --> 00:27:30,300
are probably measuring
things like this already,

660
00:27:30,300 --> 00:27:32,910
either with Jellyfish or a similar tool,

661
00:27:32,910 --> 00:27:34,920
or maybe with something
that you built yourself

662
00:27:34,920 --> 00:27:36,990
to understand things like throughput,

663
00:27:36,990 --> 00:27:39,390
cycle time, team velocity.

664
00:27:39,390 --> 00:27:41,250
And so having a good understanding

665
00:27:41,250 --> 00:27:45,030
of engineering productivity
is a prerequisite, of course,

666
00:27:45,030 --> 00:27:47,613
to measuring the impact
that comes from using AI.

667
00:27:50,880 --> 00:27:52,350
So shifting gears here

668
00:27:52,350 --> 00:27:55,560
into our approach for measuring AI impact.

669
00:27:55,560 --> 00:27:57,960
I'll start with a few
general observations,

670
00:27:57,960 --> 00:28:00,870
and then I'll present a
framework that we recommend,

671
00:28:00,870 --> 00:28:03,270
you use that aligns with
what Joe's talked about

672
00:28:03,270 --> 00:28:05,820
and with what Craig's gonna
share in a few minutes.

673
00:28:06,780 --> 00:28:10,980
The first observation is
that across our database of,

674
00:28:10,980 --> 00:28:13,620
you know, six to 700 customers,

675
00:28:13,620 --> 00:28:15,210
you know, I'm hand
waving here a little bit,

676
00:28:15,210 --> 00:28:18,420
but virtually every software
company we've talked to

677
00:28:18,420 --> 00:28:21,570
has adopted AI at an organizational level.

678
00:28:21,570 --> 00:28:23,370
There are a few straggler, of course,

679
00:28:23,370 --> 00:28:27,180
but organizational adoption
is not the barrier today.

680
00:28:27,180 --> 00:28:30,870
The barrier today is
that despite that 100%

681
00:28:30,870 --> 00:28:32,580
organization adoption,

682
00:28:32,580 --> 00:28:35,610
you really only see about
30% of those companies

683
00:28:35,610 --> 00:28:37,440
benefiting at scale

684
00:28:37,440 --> 00:28:40,950
from improved productivity
through AI tools.

685
00:28:40,950 --> 00:28:42,510
You know, this is our conclusion.

686
00:28:42,510 --> 00:28:44,340
This is Jellyfish's conclusion.

687
00:28:44,340 --> 00:28:47,910
But if you look around at other
studies throughout software

688
00:28:47,910 --> 00:28:49,590
or the knowledge economy as a whole,

689
00:28:49,590 --> 00:28:51,450
you get very similar findings.

690
00:28:51,450 --> 00:28:54,180
I read a BCG report just recently

691
00:28:54,180 --> 00:28:57,090
where they interviewed a
public company executive

692
00:28:57,090 --> 00:28:59,700
and found a very similar report.

693
00:28:59,700 --> 00:29:01,800
So a couple things that we're seeing here.

694
00:29:03,240 --> 00:29:05,850
First, Jellyfish did a state

695
00:29:05,850 --> 00:29:09,000
of engineering management
report over the summer

696
00:29:09,000 --> 00:29:12,840
where we talked to a ton
of CTOs, VPs, directors,

697
00:29:12,840 --> 00:29:16,470
and interviewed them about
their AI journey thus far.

698
00:29:16,470 --> 00:29:18,882
And so this is a qualitative statement,

699
00:29:18,882 --> 00:29:22,500
but the question was
what percent improvement

700
00:29:22,500 --> 00:29:24,750
in productivity are you getting today

701
00:29:24,750 --> 00:29:28,770
or do you expect to get over
the next year through AI?

702
00:29:28,770 --> 00:29:30,000
And the surprising thing here

703
00:29:30,000 --> 00:29:31,770
was that it was a very open-ended question

704
00:29:31,770 --> 00:29:34,530
that sort of begged for
an optimistic response.

705
00:29:34,530 --> 00:29:36,480
And yet from that,

706
00:29:36,480 --> 00:29:40,800
only 30% said they were getting
50% improvement or greater,

707
00:29:40,800 --> 00:29:43,140
which of course, great for that 30%,

708
00:29:43,140 --> 00:29:45,210
but you see that the bulk of the curve

709
00:29:45,210 --> 00:29:49,290
is actually at 10 to 25 to
50% gains, which is good.

710
00:29:49,290 --> 00:29:50,850
But that's not the expectation

711
00:29:50,850 --> 00:29:54,090
that we're getting from the
hype over the last two years

712
00:29:54,090 --> 00:29:55,110
about what AI could do.

713
00:29:55,110 --> 00:29:57,423
We expect more from this.

714
00:29:59,670 --> 00:30:01,350
We also see that of the companies

715
00:30:01,350 --> 00:30:06,350
that have adopted AI fully
across their organization,

716
00:30:06,390 --> 00:30:09,990
you actually do see fairly
dramatic improvements

717
00:30:09,990 --> 00:30:11,400
in productivity.

718
00:30:11,400 --> 00:30:13,350
This is a quantitative take here.

719
00:30:13,350 --> 00:30:15,210
This is from another study we did

720
00:30:15,210 --> 00:30:20,210
across 13,000 company
engineer week observations,

721
00:30:20,340 --> 00:30:24,870
and found that over a 2x
gain in PRs per engineer

722
00:30:24,870 --> 00:30:27,180
when you get to that top end of the curve.

723
00:30:27,180 --> 00:30:30,000
So the improvement in
productivity is there,

724
00:30:30,000 --> 00:30:31,860
yet many are still struggling.

725
00:30:31,860 --> 00:30:35,340
And so we've been calling that
the AI paradox at Jellyfish

726
00:30:35,340 --> 00:30:37,290
and try to help companies through that.

727
00:30:39,360 --> 00:30:41,100
And so in thinking about that,

728
00:30:41,100 --> 00:30:43,110
we interviewed a ton of companies,

729
00:30:43,110 --> 00:30:44,640
talked to a lot of company leaders,

730
00:30:44,640 --> 00:30:46,650
talked to a lot of developers,

731
00:30:46,650 --> 00:30:47,790
and what we found

732
00:30:47,790 --> 00:30:51,930
is that the gap between
organizational adoption

733
00:30:51,930 --> 00:30:53,550
and individual adoption

734
00:30:53,550 --> 00:30:57,150
is generally not one of effort

735
00:30:57,150 --> 00:31:00,900
or interest in using the new tools, right?

736
00:31:00,900 --> 00:31:04,290
Engineers by personality, training,

737
00:31:04,290 --> 00:31:06,660
many of us are engineers here, of course,

738
00:31:06,660 --> 00:31:09,420
are tinkerers and experimenters,

739
00:31:09,420 --> 00:31:12,240
and we want to use new
tools, we want to use AI,

740
00:31:12,240 --> 00:31:13,323
we want it to work.

741
00:31:14,460 --> 00:31:17,280
But we're also many of us skeptics, right?

742
00:31:17,280 --> 00:31:19,770
We're not gonna use a thing
'cause someone told us to.

743
00:31:19,770 --> 00:31:22,020
We're not gonna use a thing
that makes our jobs harder,

744
00:31:22,020 --> 00:31:23,910
slower or worse.

745
00:31:23,910 --> 00:31:27,330
So the key to unlocking greater
adoption and productivity

746
00:31:27,330 --> 00:31:32,040
doesn't come through organizational
mandate or fiat, right?

747
00:31:32,040 --> 00:31:34,590
It comes from understanding
what are the blockers,

748
00:31:34,590 --> 00:31:36,300
and then how do we get past those blockers

749
00:31:36,300 --> 00:31:37,563
to get true success.

750
00:31:38,940 --> 00:31:41,670
So through our work with companies,

751
00:31:41,670 --> 00:31:46,290
we identified a three-step
journey to go from adoption

752
00:31:46,290 --> 00:31:47,520
to productivity

753
00:31:47,520 --> 00:31:50,193
and then to business
outcomes in that order.

754
00:31:53,490 --> 00:31:54,810
And then we generalize that

755
00:31:54,810 --> 00:31:56,517
into an AI measurement framework.

756
00:31:56,517 --> 00:31:59,280
And so I'm gonna talk about
what that framework is,

757
00:31:59,280 --> 00:32:02,640
and I encourage folks
to hear the framework

758
00:32:02,640 --> 00:32:06,240
and figure out how it can
work in their organization,

759
00:32:06,240 --> 00:32:08,760
'cause it's not a specific
set of instructions

760
00:32:08,760 --> 00:32:11,610
around metrics to track
or numbers to look at,

761
00:32:11,610 --> 00:32:13,560
but it's a way of thinking
about those metrics

762
00:32:13,560 --> 00:32:15,243
to try and drive success for you.

763
00:32:17,670 --> 00:32:20,700
The first part of the
framework is adoption.

764
00:32:20,700 --> 00:32:21,900
And when we say adoption,

765
00:32:21,900 --> 00:32:25,560
we are really thinking about our engineers

766
00:32:25,560 --> 00:32:28,800
using the AI tools that you've procured.

767
00:32:28,800 --> 00:32:30,720
How many tools are they using?

768
00:32:30,720 --> 00:32:32,640
How often are they using them?

769
00:32:32,640 --> 00:32:35,160
And then what percent of their actual work

770
00:32:35,160 --> 00:32:37,230
comes through the use of those tools.

771
00:32:37,230 --> 00:32:39,240
So you can come up with
a whole litany of metrics

772
00:32:39,240 --> 00:32:41,490
that sort of satisfy this criteria.

773
00:32:41,490 --> 00:32:44,003
So I have an example I
wanna walk through with you.

774
00:32:46,650 --> 00:32:49,620
This example here is from a sample company

775
00:32:49,620 --> 00:32:51,270
that I worked with closely

776
00:32:51,270 --> 00:32:53,700
and then anonymized for this purpose.

777
00:32:53,700 --> 00:32:54,533
And what they did

778
00:32:54,533 --> 00:32:58,500
was they said out of all the
potential adoption metrics,

779
00:32:58,500 --> 00:33:01,350
they wanted to focus on
four, which is a good number,

780
00:33:01,350 --> 00:33:03,570
two to five metrics is a good number.

781
00:33:03,570 --> 00:33:05,550
They wanted to say first,

782
00:33:05,550 --> 00:33:08,460
are we actually trying
enough different tools?

783
00:33:08,460 --> 00:33:10,410
And so in their case they're trying five,

784
00:33:10,410 --> 00:33:12,030
which is awesome.

785
00:33:12,030 --> 00:33:13,890
Then they wanted to say,

786
00:33:13,890 --> 00:33:17,220
are enough engineers using
something at least weekly

787
00:33:17,220 --> 00:33:19,440
to begin to learn how it's working.

788
00:33:19,440 --> 00:33:22,020
From that they wanted
to go to power usage,

789
00:33:22,020 --> 00:33:25,050
which we equate to daily usage,

790
00:33:25,050 --> 00:33:28,080
and then actual output from those folks.

791
00:33:28,080 --> 00:33:30,510
So what percent of the work they're doing

792
00:33:30,510 --> 00:33:32,880
was assisted by AI in some way.

793
00:33:32,880 --> 00:33:34,140
So they selected four

794
00:33:34,140 --> 00:33:35,700
that aligned with their sort of methodical

795
00:33:35,700 --> 00:33:38,070
step-by-step approach,

796
00:33:38,070 --> 00:33:40,470
and they compare them
each to an industry trend.

797
00:33:41,340 --> 00:33:43,560
Now the good news is they
actually compare very favorably

798
00:33:43,560 --> 00:33:46,710
to those industry trends
as you can see here.

799
00:33:46,710 --> 00:33:49,230
But there's still room
for improvement, right?

800
00:33:49,230 --> 00:33:52,590
Even though they have
44% of their organization

801
00:33:52,590 --> 00:33:54,990
using AI daily,

802
00:33:54,990 --> 00:33:56,970
what about the other 56%?

803
00:33:56,970 --> 00:33:58,770
Why is that the case?

804
00:33:58,770 --> 00:34:00,990
And so this is where segmenting the data

805
00:34:00,990 --> 00:34:03,630
becomes your friend, right?

806
00:34:03,630 --> 00:34:06,990
A naive organization might
go in and create a mandate

807
00:34:06,990 --> 00:34:09,030
or some sort of demand about it.

808
00:34:09,030 --> 00:34:12,120
But we recommend that you identify the 56%

809
00:34:12,120 --> 00:34:15,840
that are not using AI daily
by segmenting your data,

810
00:34:15,840 --> 00:34:17,310
cohorting your data,

811
00:34:17,310 --> 00:34:20,460
and understanding is that specific teams,

812
00:34:20,460 --> 00:34:23,970
specific locations, tenure of employee,

813
00:34:23,970 --> 00:34:25,410
or if you could even understand

814
00:34:25,410 --> 00:34:27,240
if it's a certain type of work

815
00:34:27,240 --> 00:34:30,840
where AI is simply not effective
yet for your engineers.

816
00:34:30,840 --> 00:34:34,560
And that's why that
connection between your GitHub

817
00:34:34,560 --> 00:34:36,240
and your GIT repositories

818
00:34:36,240 --> 00:34:38,640
and your issue tracking
is super important.

819
00:34:38,640 --> 00:34:41,070
So even understand what types of projects

820
00:34:41,070 --> 00:34:42,720
maybe AI is not being used,

821
00:34:42,720 --> 00:34:44,100
where maybe it's less effective,

822
00:34:44,100 --> 00:34:46,050
and maybe that's driving these numbers.

823
00:34:49,710 --> 00:34:51,960
The second part of the
framework is adoption.

824
00:34:53,640 --> 00:34:55,410
And so when we talk about adoption,

825
00:34:55,410 --> 00:34:59,340
we're talk, excuse me, it's
productivity obviously.

826
00:34:59,340 --> 00:35:01,050
The second part is productivity.

827
00:35:01,050 --> 00:35:02,370
When we talk about productivity,

828
00:35:02,370 --> 00:35:04,860
we're thinking about the actual outputs

829
00:35:04,860 --> 00:35:06,630
from your engineering team.

830
00:35:06,630 --> 00:35:09,870
So are you getting more
things through your process

831
00:35:09,870 --> 00:35:11,130
per unit time?

832
00:35:11,130 --> 00:35:13,890
So cycle time, throughput,

833
00:35:13,890 --> 00:35:16,440
counts of commits, PRs, et cetera.

834
00:35:16,440 --> 00:35:20,073
These are examples of productivity
of your engineering team.

835
00:35:21,780 --> 00:35:23,760
Now, in our example here,

836
00:35:23,760 --> 00:35:27,120
this sample company
centered on the pull request

837
00:35:27,120 --> 00:35:28,983
as their atomic unit of measurement.

838
00:35:30,000 --> 00:35:31,440
And it's important that they aligned

839
00:35:31,440 --> 00:35:34,530
on what was important for them to measure.

840
00:35:34,530 --> 00:35:35,670
Because there's a lot of different ways

841
00:35:35,670 --> 00:35:36,780
you could approach this.

842
00:35:36,780 --> 00:35:39,840
We've companies look at cycle time,

843
00:35:39,840 --> 00:35:43,620
they might look at issue
throughput, epic throughput,

844
00:35:43,620 --> 00:35:45,000
any number of things.

845
00:35:45,000 --> 00:35:46,890
And it's important for your culture

846
00:35:46,890 --> 00:35:49,770
to all be aligned on the same
thing that you wanna look at.

847
00:35:49,770 --> 00:35:51,630
They chose pull requests

848
00:35:51,630 --> 00:35:54,960
because that is something where
they are generally smaller,

849
00:35:54,960 --> 00:35:57,360
they're voluminous enough that you know,

850
00:35:57,360 --> 00:35:59,730
outliers and other
noise sort of drops out,

851
00:35:59,730 --> 00:36:00,720
and you can get a good signal

852
00:36:00,720 --> 00:36:02,730
on what productivity looks like.

853
00:36:02,730 --> 00:36:04,170
And so they were looking at PR throughput

854
00:36:04,170 --> 00:36:06,990
as their main metric here.

855
00:36:06,990 --> 00:36:07,823
Now in this case,

856
00:36:07,823 --> 00:36:09,600
despite the fact that
they're going faster,

857
00:36:09,600 --> 00:36:11,763
they are trailing the industry trend.

858
00:36:12,660 --> 00:36:13,800
And in this case,

859
00:36:13,800 --> 00:36:16,440
our recommendation was
less about segmenting

860
00:36:16,440 --> 00:36:19,560
on cohorts of individuals.

861
00:36:19,560 --> 00:36:22,050
We assumed it was less of this group,

862
00:36:22,050 --> 00:36:24,030
either junior or senior engineers,

863
00:36:24,030 --> 00:36:25,980
this location or that location,

864
00:36:25,980 --> 00:36:28,890
and much more likely
about the type of work.

865
00:36:28,890 --> 00:36:31,950
'Cause what we found is that the age

866
00:36:31,950 --> 00:36:34,380
and complexity of the
repo you're working in,

867
00:36:34,380 --> 00:36:37,410
whether it's an old code
base or a new code base,

868
00:36:37,410 --> 00:36:38,940
what type of project it is,

869
00:36:38,940 --> 00:36:42,420
often matters more for productivity gains

870
00:36:42,420 --> 00:36:44,220
once you've got that
base level of adoption

871
00:36:44,220 --> 00:36:45,990
and engagement for your engineers.

872
00:36:45,990 --> 00:36:48,450
And so in this case, we
recommended they segment

873
00:36:48,450 --> 00:36:50,500
by the type of work they were looking at.

874
00:36:54,270 --> 00:36:56,730
The final one is business outcomes.

875
00:36:56,730 --> 00:37:00,090
And business outcomes that are
often the most misunderstood,

876
00:37:00,090 --> 00:37:03,390
because this is really where
your organization's culture

877
00:37:03,390 --> 00:37:06,960
and what you're trying to get
out of AI matters the most.

878
00:37:06,960 --> 00:37:10,110
'Cause once you have people
using it through adoption,

879
00:37:10,110 --> 00:37:12,483
they're shipping more work more quickly,

880
00:37:13,500 --> 00:37:16,080
you're probably expecting
some sort of change to occur

881
00:37:16,080 --> 00:37:17,670
in your company.

882
00:37:17,670 --> 00:37:20,190
Now when Joe talked about
this a few minutes ago,

883
00:37:20,190 --> 00:37:23,100
he talked about Amazon's
cost to serve model, right?

884
00:37:23,100 --> 00:37:26,010
So that's just one way to
look at a business outcome is,

885
00:37:26,010 --> 00:37:28,890
are you able to produce
software more cost effectively?

886
00:37:28,890 --> 00:37:31,050
That would count as an outcome here.

887
00:37:31,050 --> 00:37:34,830
Other outcomes might be
things like R and D savings,

888
00:37:34,830 --> 00:37:37,380
they might be type of
work you're delivering

889
00:37:37,380 --> 00:37:39,390
or you're delivering more innovation work

890
00:37:39,390 --> 00:37:41,253
per unit time, et cetera.

891
00:37:43,110 --> 00:37:46,170
And so that's what this
company was looking at as well.

892
00:37:46,170 --> 00:37:49,170
This company was very concerned
with the amount of time

893
00:37:49,170 --> 00:37:52,650
they were spending on maintenance work,

894
00:37:52,650 --> 00:37:55,620
keeping the lights on
work, bug fixing work.

895
00:37:55,620 --> 00:37:59,970
And so the entire goal of
driving AI and driving adoption

896
00:37:59,970 --> 00:38:02,470
was less about cost for this company

897
00:38:03,728 --> 00:38:06,810
and more about could they
produce more innovation roadmap

898
00:38:06,810 --> 00:38:10,770
and growth features per
engineer than they could before.

899
00:38:10,770 --> 00:38:13,500
And so in this case, they had
an improvement there of 37%,

900
00:38:13,500 --> 00:38:15,960
which was awesome ahead of the trend.

901
00:38:15,960 --> 00:38:17,010
Interestingly,

902
00:38:17,010 --> 00:38:21,990
they wanted to do that
through using AI to fix bugs.

903
00:38:21,990 --> 00:38:23,400
Now they were doing that,

904
00:38:23,400 --> 00:38:25,890
but they actually trailed
the industry trend here

905
00:38:25,890 --> 00:38:26,940
by a little bit.

906
00:38:26,940 --> 00:38:29,400
So they were succeeding
at their top line goal

907
00:38:29,400 --> 00:38:32,370
despite that intermediate
goal not being met.

908
00:38:32,370 --> 00:38:34,350
So here there's an opportunity to find out

909
00:38:34,350 --> 00:38:35,460
why were they were succeeding

910
00:38:35,460 --> 00:38:37,770
despite the initial strategy

911
00:38:37,770 --> 00:38:40,260
not actually playing out
the way they expected it to.

912
00:38:40,260 --> 00:38:41,550
So this was good news.

913
00:38:41,550 --> 00:38:42,660
They could then investigate that

914
00:38:42,660 --> 00:38:44,880
and find out if they
could actually go faster

915
00:38:44,880 --> 00:38:45,980
with the benefit here.

916
00:38:48,750 --> 00:38:51,090
So before I pass it off to Craig,

917
00:38:51,090 --> 00:38:53,970
one thing I wanna note here
at the end is, you know,

918
00:38:53,970 --> 00:38:57,090
Joe talked about how
we went from, you know,

919
00:38:57,090 --> 00:39:02,090
auto complete to code assist,
now to an agentic world.

920
00:39:02,580 --> 00:39:04,680
And I think, you know,
the early data suggests

921
00:39:04,680 --> 00:39:07,230
that there's a lot of talk about agents,

922
00:39:07,230 --> 00:39:10,170
but the actual adoption of
those agents is not quite there

923
00:39:10,170 --> 00:39:11,910
where the the code assist
tools might be yet,

924
00:39:11,910 --> 00:39:13,530
but it's probably coming.

925
00:39:13,530 --> 00:39:16,080
But the point is that
we actually don't know

926
00:39:16,080 --> 00:39:19,410
what mode the AI tools will
take over the next 12 months,

927
00:39:19,410 --> 00:39:22,680
let alone the next, you
know, 24 or 36 months.

928
00:39:22,680 --> 00:39:25,410
So the framework that we've presented,

929
00:39:25,410 --> 00:39:29,070
we really intend to be sort
of mode agnostic, right?

930
00:39:29,070 --> 00:39:32,370
It doesn't matter if you're
using code complete or chat

931
00:39:32,370 --> 00:39:34,173
or agents or augmentation,

932
00:39:36,270 --> 00:39:38,430
if you're not adopting those tools,

933
00:39:38,430 --> 00:39:39,780
seeing increased productivity

934
00:39:39,780 --> 00:39:41,910
and then seeing some
type of business result

935
00:39:41,910 --> 00:39:43,170
on the other end.

936
00:39:43,170 --> 00:39:46,320
So regardless of what tool you adopt,

937
00:39:46,320 --> 00:39:47,760
what mode it works in,

938
00:39:47,760 --> 00:39:49,590
we recommend some type
of framework like this

939
00:39:49,590 --> 00:39:52,200
to help measure progress and drive impact.

940
00:39:52,200 --> 00:39:54,270
So with that, I'm gonna pass off to Craig

941
00:39:54,270 --> 00:39:56,420
to take you through
the journey at Genesys.

942
00:39:58,470 --> 00:39:59,303
- Thanks man.

943
00:40:03,570 --> 00:40:05,460
Hi, my name is Craig Dahlinger,

944
00:40:05,460 --> 00:40:08,670
I'm the senior director of
platform engineering at Genesys.

945
00:40:08,670 --> 00:40:10,413
So I'm gonna walk you through,

946
00:40:12,030 --> 00:40:14,790
basically take you through
what our journey was

947
00:40:14,790 --> 00:40:16,053
when we first adopted Q.

948
00:40:17,250 --> 00:40:20,196
Then I'm gonna show you
how we scaled all of it out

949
00:40:20,196 --> 00:40:22,020
with as far as Jellyfish
with our reporting metrics.

950
00:40:22,020 --> 00:40:23,550
And then along the way I'm
gonna share the lessons

951
00:40:23,550 --> 00:40:26,670
we learned and how we
achieved where we're headed,

952
00:40:26,670 --> 00:40:29,010
and kind of makeshift reporting
we did in the beginning

953
00:40:29,010 --> 00:40:30,933
and why we're using Jellyfish now.

954
00:40:33,330 --> 00:40:36,390
So when we first brought
Q on, our goal was simple.

955
00:40:36,390 --> 00:40:38,880
We wanted to give developers
a boost in their productivity

956
00:40:38,880 --> 00:40:39,843
and their velocity.

957
00:40:40,710 --> 00:40:42,150
We also wanted a way to chip away

958
00:40:42,150 --> 00:40:43,770
at some of the technical debt we had.

959
00:40:43,770 --> 00:40:45,870
We wanted to take care of
some of the SDK migrations

960
00:40:45,870 --> 00:40:48,660
going from AWS SDK v1 to v2,

961
00:40:48,660 --> 00:40:51,813
even some deprecation of lambda
runtime, stuff like that.

962
00:40:53,040 --> 00:40:54,750
So we started getting
some unexpected results

963
00:40:54,750 --> 00:40:57,450
where we started getting
better documentation in repos.

964
00:40:57,450 --> 00:40:58,950
We started getting much
stronger opportunities

965
00:40:58,950 --> 00:41:00,000
for our unit testings.

966
00:41:00,000 --> 00:41:03,123
So Q was being adopted,
developers were happy with it,

967
00:41:03,990 --> 00:41:05,430
but the challenge was we didn't understand

968
00:41:05,430 --> 00:41:07,320
how it was being used or who was using it,

969
00:41:07,320 --> 00:41:10,680
and who was kind of the power
users or the idle users.

970
00:41:10,680 --> 00:41:12,030
So the gap in visibility

971
00:41:12,030 --> 00:41:13,800
was basically the
problem we need to solve.

972
00:41:13,800 --> 00:41:15,960
How can I tell my devs

973
00:41:15,960 --> 00:41:18,360
or the teams are actually engaged in Q

974
00:41:18,360 --> 00:41:19,910
and getting the most out of it.

975
00:41:21,870 --> 00:41:25,830
So we realized there was no clear way

976
00:41:25,830 --> 00:41:27,240
to understand how they're
actually using it.

977
00:41:27,240 --> 00:41:29,730
We started looking at the Q metrics.

978
00:41:29,730 --> 00:41:31,440
We use AWS SSO for sign in.

979
00:41:31,440 --> 00:41:33,543
So the users were Google IDs.

980
00:41:34,380 --> 00:41:36,810
We couldn't really tell how
the engineers were engaged,

981
00:41:36,810 --> 00:41:40,140
so we kind of hacked together
this kind of makeshift

982
00:41:40,140 --> 00:41:42,150
to metrics database,

983
00:41:42,150 --> 00:41:45,750
so we can tell our higher
ups how Q is being used.

984
00:41:45,750 --> 00:41:47,610
We basically ended up
taking the data from Q,

985
00:41:47,610 --> 00:41:49,890
piping it to Postgres,

986
00:41:49,890 --> 00:41:51,690
and then our internal teams,

987
00:41:51,690 --> 00:41:54,090
they started visualizing
everything in QuickSight.

988
00:41:54,090 --> 00:41:56,855
But we started realizing
we can't scale that,

989
00:41:56,855 --> 00:41:57,810
'cause every team lead
wanted a different view,

990
00:41:57,810 --> 00:41:59,250
so we had to make all
different dashboards.

991
00:41:59,250 --> 00:42:02,554
So it was becoming more
and more of a real pain

992
00:42:02,554 --> 00:42:05,190
to scale that out to
everybody's different needs.

993
00:42:05,190 --> 00:42:07,920
So in the end, we really
knew this was a stop gap,

994
00:42:07,920 --> 00:42:10,920
and we needed something basically
automated enterprise level

995
00:42:10,920 --> 00:42:13,020
for us to actually do the reporting on Q

996
00:42:13,020 --> 00:42:15,120
and understand how our devs were using it.

997
00:42:16,500 --> 00:42:18,483
So one of the Q check-ins with Amazon,

998
00:42:19,320 --> 00:42:20,400
actually Joe was saying

999
00:42:20,400 --> 00:42:22,050
that we're partnering with Jellyfish.

1000
00:42:22,050 --> 00:42:24,930
We already used Jellyfish
for our Jira intake

1001
00:42:24,930 --> 00:42:27,120
and seemed like a perfect
opportunity for us

1002
00:42:27,120 --> 00:42:29,190
to start using their AI dashboard.

1003
00:42:29,190 --> 00:42:30,660
So we joined the program,

1004
00:42:30,660 --> 00:42:33,780
we're able to basically
retire that Postgres stuff

1005
00:42:33,780 --> 00:42:36,783
and move the metrics right
over to the Jellyfish AI.

1006
00:42:38,040 --> 00:42:40,020
So the impact right now, we have it,

1007
00:42:40,020 --> 00:42:41,790
consumes all the queue data,

1008
00:42:41,790 --> 00:42:43,320
and basically give us our leadership

1009
00:42:43,320 --> 00:42:45,330
in the realtime stats on the Q.

1010
00:42:45,330 --> 00:42:47,880
And since they're already involved

1011
00:42:47,880 --> 00:42:49,950
in our Jira taxonomy information,

1012
00:42:49,950 --> 00:42:52,170
we were able to roll everything
up to our team service

1013
00:42:52,170 --> 00:42:54,544
and actually get the Jira taxonomy.

1014
00:42:54,544 --> 00:42:55,800
So as far as our leadership groups,

1015
00:42:55,800 --> 00:42:57,000
they were also able to understand

1016
00:42:57,000 --> 00:42:59,490
how Q is being used by their teams,

1017
00:42:59,490 --> 00:43:00,990
by their product taxonomy.

1018
00:43:00,990 --> 00:43:04,110
So it helps solve a lot of the questions

1019
00:43:04,110 --> 00:43:05,210
that were being asked.

1020
00:43:07,260 --> 00:43:08,730
Once we actually connected,

1021
00:43:08,730 --> 00:43:11,250
we basically were able to
see who our power users were,

1022
00:43:11,250 --> 00:43:12,720
who are idols.

1023
00:43:12,720 --> 00:43:14,100
It sparked a lot of conversation

1024
00:43:14,100 --> 00:43:15,510
with different team members,

1025
00:43:15,510 --> 00:43:17,130
how they wanted to use Jellyfish,

1026
00:43:17,130 --> 00:43:19,560
I mean how they wanted to use Q,

1027
00:43:19,560 --> 00:43:22,590
and people who were really
not adopting it that fast.

1028
00:43:22,590 --> 00:43:23,850
We were able to engage in conversation,

1029
00:43:23,850 --> 00:43:25,500
understand why they weren't.

1030
00:43:25,500 --> 00:43:28,440
So basically also answered a lot,

1031
00:43:28,440 --> 00:43:31,080
probably 90% of the questions
that were coming to my team

1032
00:43:31,080 --> 00:43:33,180
we were able to answer with the dashboard.

1033
00:43:34,380 --> 00:43:36,120
And something happened
that was really important

1034
00:43:36,120 --> 00:43:38,700
was it sparked a lot of
conversation with teams,

1035
00:43:38,700 --> 00:43:40,470
basically trying to see how AI

1036
00:43:40,470 --> 00:43:42,690
was being amplifying their work.

1037
00:43:42,690 --> 00:43:43,830
So it's not a replacement,

1038
00:43:43,830 --> 00:43:47,220
it's basically amplifying
the developer and engagement,

1039
00:43:47,220 --> 00:43:49,470
and then basically the
engagement with the teams leads

1040
00:43:49,470 --> 00:43:51,480
and the developers started
growing from there,

1041
00:43:51,480 --> 00:43:55,113
and we able to push Amazon
Q to adopt it faster.

1042
00:43:56,970 --> 00:44:00,300
So looking ahead, we basically embedded AI

1043
00:44:00,300 --> 00:44:02,750
across our entire software
development lifecycle.

1044
00:44:05,040 --> 00:44:05,873
AWS and Jellyfish,

1045
00:44:05,873 --> 00:44:07,290
they ensure our consistent unified metrics

1046
00:44:07,290 --> 00:44:08,190
across all our tools.

1047
00:44:08,190 --> 00:44:10,263
So it's one stop shop for dashboards.

1048
00:44:12,390 --> 00:44:13,560
It reduces the time

1049
00:44:13,560 --> 00:44:16,020
that the developers spend
on the mundane tasks.

1050
00:44:16,020 --> 00:44:17,370
And I usually tell my team members too,

1051
00:44:17,370 --> 00:44:20,220
like we're not replacing
developers, we're amplifying them.

1052
00:44:20,220 --> 00:44:21,840
So it's just another
tool in their tool belt

1053
00:44:21,840 --> 00:44:24,000
to make them better at
creating the features

1054
00:44:24,000 --> 00:44:25,800
that they want to create,

1055
00:44:25,800 --> 00:44:28,200
and help solve the hard
problems moving forward.

1056
00:44:31,890 --> 00:44:35,010
So what you're seeing there
is basically how AI metrics

1057
00:44:35,010 --> 00:44:37,650
helps us qualify the
real productivity gains.

1058
00:44:37,650 --> 00:44:40,383
Things like faster code
reviews, faster PRs.

1059
00:44:41,310 --> 00:44:43,680
The improvements are
theoretical, they're visible,

1060
00:44:43,680 --> 00:44:46,200
we can actually see the
teams making progress.

1061
00:44:46,200 --> 00:44:48,270
And it gave us something we've
been missing for a long time

1062
00:44:48,270 --> 00:44:51,150
is clear proof that AI
was helping individuals.

1063
00:44:51,150 --> 00:44:52,650
And if it wasn't helping individuals,

1064
00:44:52,650 --> 00:44:54,510
we knew we had engaged
in the conversations

1065
00:44:54,510 --> 00:44:56,430
to understand why it wasn't helping them.

1066
00:44:56,430 --> 00:44:57,570
Maybe it was prompt design,

1067
00:44:57,570 --> 00:45:00,240
maybe they just didn't want to use it.

1068
00:45:00,240 --> 00:45:03,270
So the point is the metrics
aren't black and white.

1069
00:45:03,270 --> 00:45:05,520
It engages you in
conversation with the devs.

1070
00:45:08,340 --> 00:45:12,720
Later on, Amazon Kiro came out,
we adopted that right away.

1071
00:45:12,720 --> 00:45:13,980
That took off.

1072
00:45:13,980 --> 00:45:16,170
Everyone in the team really liked it.

1073
00:45:16,170 --> 00:45:17,700
They integrated right into VS Code,

1074
00:45:17,700 --> 00:45:19,800
introduced spec-based
program and Vibe Coding.

1075
00:45:19,800 --> 00:45:21,630
So two modes.

1076
00:45:21,630 --> 00:45:24,210
We started seeing teams like
whip out really cool utilities

1077
00:45:24,210 --> 00:45:25,800
and tools,

1078
00:45:25,800 --> 00:45:27,390
but it really changed the developers

1079
00:45:27,390 --> 00:45:30,510
and how they thought about
building stuff with AI.

1080
00:45:30,510 --> 00:45:31,830
It was actually with spec-based program,

1081
00:45:31,830 --> 00:45:34,230
it's more structured and intentional.

1082
00:45:34,230 --> 00:45:36,530
Developers came more
efficient and productive.

1083
00:45:37,800 --> 00:45:39,570
And we make a point of describing Kiro

1084
00:45:39,570 --> 00:45:43,470
the same way we describe
all of our other AI tools.

1085
00:45:43,470 --> 00:45:46,020
It's an amplifier, it's not a replacement,

1086
00:45:46,020 --> 00:45:47,430
it's not just an assistant,

1087
00:45:47,430 --> 00:45:50,070
but we tell the devs just
another tool in your tool belt

1088
00:45:50,070 --> 00:45:51,020
to make you better.

1089
00:45:53,400 --> 00:45:55,860
We're still waiting on
some of the Kiro metrics.

1090
00:45:55,860 --> 00:45:58,050
Once we consume those,
we work with Jellyfish,

1091
00:45:58,050 --> 00:46:00,330
and Jellyfish will consume
those Kiro metrics.

1092
00:46:00,330 --> 00:46:03,810
So we'll have actually Q usage
and Kiro usage in one place.

1093
00:46:03,810 --> 00:46:05,940
So makes life a lot easier,

1094
00:46:05,940 --> 00:46:07,920
and everything also be
broken down the same way

1095
00:46:07,920 --> 00:46:10,590
is that for our team
taxonomy and everything.

1096
00:46:10,590 --> 00:46:12,270
So one unified dashboard,

1097
00:46:12,270 --> 00:46:14,760
one single space for usage adoption,

1098
00:46:14,760 --> 00:46:17,360
your return on investment
across the whole platform.

1099
00:46:18,630 --> 00:46:20,370
For us it meant teams
can choose the AI tools

1100
00:46:20,370 --> 00:46:21,570
they wanna work with,

1101
00:46:21,570 --> 00:46:24,510
and the leads and directs can
actually go to one dashboard

1102
00:46:24,510 --> 00:46:26,610
for reporting to see how
their actually developers

1103
00:46:26,610 --> 00:46:27,610
are using the tools.

1104
00:46:29,400 --> 00:46:33,510
So internally using Q in developer Kiro,

1105
00:46:33,510 --> 00:46:34,610
for us it's a journey.

1106
00:46:35,640 --> 00:46:37,830
AI's helping us build a culture internally

1107
00:46:37,830 --> 00:46:41,250
with creativity at the
forefront and speed.

1108
00:46:41,250 --> 00:46:44,970
And basically we wanna move
forward from experimentation

1109
00:46:44,970 --> 00:46:47,670
to basically real measurable
impact within the groups.

1110
00:46:51,270 --> 00:46:53,170
And I'm gonna hand it back to Joe now.

1111
00:46:57,690 --> 00:46:58,640
- Thank you, Craig.

1112
00:46:59,970 --> 00:47:04,200
So what are some key takeaways
from this conversation?

1113
00:47:04,200 --> 00:47:06,570
Some things I hope
you'll take away from it.

1114
00:47:06,570 --> 00:47:10,140
The first one is measuring
AI a journey, right?

1115
00:47:10,140 --> 00:47:11,460
You've heard me describe it,

1116
00:47:11,460 --> 00:47:15,000
you've heard Craig and and
Krishna all describe this.

1117
00:47:15,000 --> 00:47:17,343
So baseline what you have,

1118
00:47:18,180 --> 00:47:21,194
pick a framework and start, right?

1119
00:47:21,194 --> 00:47:23,160
It's not gonna get any better,

1120
00:47:23,160 --> 00:47:25,350
just go work with what you have.

1121
00:47:25,350 --> 00:47:27,300
And then think about this idea of leading

1122
00:47:27,300 --> 00:47:29,130
and lagging indicators, right?

1123
00:47:29,130 --> 00:47:30,750
A lot of the finance folks

1124
00:47:30,750 --> 00:47:32,640
are gonna want the
lagging indicators, right?

1125
00:47:32,640 --> 00:47:34,737
The ROI improvements, all of these things.

1126
00:47:34,737 --> 00:47:36,600
But what are the leading indicators

1127
00:47:36,600 --> 00:47:38,490
that will make sense in your organization

1128
00:47:38,490 --> 00:47:39,940
to help you move you forward?

1129
00:47:41,370 --> 00:47:43,620
Map your usage to your internal taxonomy?

1130
00:47:43,620 --> 00:47:45,300
I think one of the most interesting things

1131
00:47:45,300 --> 00:47:49,410
that I found working with Jellyfish,

1132
00:47:49,410 --> 00:47:51,540
is the ability to overlay the usage

1133
00:47:51,540 --> 00:47:53,820
across both organizational and product

1134
00:47:53,820 --> 00:47:55,290
and feature taxonomies.

1135
00:47:55,290 --> 00:47:58,677
It's been really interesting
to be able to see how adoption

1136
00:47:58,677 --> 00:48:01,410
and engagement works, right?

1137
00:48:01,410 --> 00:48:03,420
Impact is not gonna be universal

1138
00:48:03,420 --> 00:48:05,100
and it's not gonna be consistent.

1139
00:48:05,100 --> 00:48:09,570
So being able to explain to the execs,

1140
00:48:09,570 --> 00:48:12,300
like here's why this
team appears to be lower,

1141
00:48:12,300 --> 00:48:14,790
here's why this team appears to be faster,

1142
00:48:14,790 --> 00:48:15,990
and what are the lessons,

1143
00:48:15,990 --> 00:48:19,590
are incredibly important in
those kind of conversations.

1144
00:48:19,590 --> 00:48:21,210
And then as Craig talked about, right?

1145
00:48:21,210 --> 00:48:22,863
Think about your organization.

1146
00:48:23,700 --> 00:48:27,450
Having the good data enables
you to help leadership

1147
00:48:27,450 --> 00:48:30,120
understand what does and does not matter,

1148
00:48:30,120 --> 00:48:31,833
what is and is not relevant.

1149
00:48:32,730 --> 00:48:36,753
And finally, have a very clear
vision and a clear message.

1150
00:48:38,190 --> 00:48:41,430
So with that, thank you
very much for your time.

1151
00:48:41,430 --> 00:48:43,140
If you want to learn more,

1152
00:48:43,140 --> 00:48:45,123
we have a chalk talk session tomorrow,

1153
00:48:46,650 --> 00:48:49,170
which is myself and a colleague
where we're, you know,

1154
00:48:49,170 --> 00:48:51,600
gonna be able to talk about
all these various things more.

1155
00:48:51,600 --> 00:48:54,390
And if you want to learn
more about cost to serve,

1156
00:48:54,390 --> 00:48:56,850
there is a session tomorrow, DVT 207,

1157
00:48:56,850 --> 00:48:58,410
which is a session like this

1158
00:48:58,410 --> 00:49:00,150
where my colleagues from ASBX

1159
00:49:00,150 --> 00:49:02,520
will go super deep into
how we think about,

1160
00:49:02,520 --> 00:49:06,840
measure and manage our
cost to serve framework.

1161
00:49:06,840 --> 00:49:09,000
So with that, thank you very much.

1162
00:49:09,000 --> 00:49:10,592
Please enjoy re:Invent.

1163
00:49:10,592 --> 00:49:12,878
(audience clapping)

