1
00:00:00,000 --> 00:00:01,650
(audience applauds)

2
00:00:01,650 --> 00:00:04,653
- Hello, everyone. Thank you
for joining this session.

3
00:00:05,760 --> 00:00:08,880
What I thought I'd do
today is walk you through

4
00:00:08,880 --> 00:00:13,880
a set of examples that we
have built and deployed

5
00:00:14,010 --> 00:00:16,380
across multiple Fortune 500 companies,

6
00:00:16,380 --> 00:00:21,180
specifically focused
around agentic applications

7
00:00:21,180 --> 00:00:24,930
which are on data science
and analytics, right?

8
00:00:24,930 --> 00:00:29,580
Roughly around late last year,
we started with this thesis

9
00:00:29,580 --> 00:00:34,580
on the fact that classical
AI, data science,

10
00:00:35,220 --> 00:00:37,653
and gen AI are converging into agentic AI.

11
00:00:39,000 --> 00:00:43,110
And then when we look at that
from that standpoint of view,

12
00:00:43,110 --> 00:00:45,540
likely most of the decision engines

13
00:00:45,540 --> 00:00:49,380
that have been built over the
last maybe around 15 years

14
00:00:49,380 --> 00:00:50,760
will probably be reimagined

15
00:00:50,760 --> 00:00:53,220
from an agentic AI standpoint of view.

16
00:00:53,220 --> 00:00:57,753
And largely this is
going to be multimodal,

17
00:00:58,860 --> 00:01:00,780
multi-agent systems that are capturing

18
00:01:00,780 --> 00:01:02,190
many of these enterprise workflows.

19
00:01:02,190 --> 00:01:04,470
So that's essentially the thesis

20
00:01:04,470 --> 00:01:06,363
that we started with last year.

21
00:01:07,440 --> 00:01:11,130
And over the last 12 months, or this year,

22
00:01:11,130 --> 00:01:14,340
we are seeing a number of key developments

23
00:01:14,340 --> 00:01:17,490
that are accelerating
that particular trend,

24
00:01:17,490 --> 00:01:18,900
especially from an analytics

25
00:01:18,900 --> 00:01:20,880
and data science standpoint of view.

26
00:01:20,880 --> 00:01:23,590
And specifically I think five things

27
00:01:24,600 --> 00:01:29,580
that started around
December last year to now

28
00:01:29,580 --> 00:01:32,340
have helped accelerate
this particular process.

29
00:01:32,340 --> 00:01:35,610
One probably is more on reasoning.

30
00:01:35,610 --> 00:01:40,610
Today, we are able to execute
complex multi-step analysis.

31
00:01:41,880 --> 00:01:45,060
So you ask a question,
then the application

32
00:01:45,060 --> 00:01:46,350
is able to think through saying that here

33
00:01:46,350 --> 00:01:48,960
are the five second order or third order

34
00:01:48,960 --> 00:01:51,510
underlying question that
I need to investigate,

35
00:01:51,510 --> 00:01:54,570
and then reason around those.

36
00:01:54,570 --> 00:01:56,340
And then the second
thing that also came out

37
00:01:56,340 --> 00:02:01,020
was more around agentic workflows,
which became mainstream.

38
00:02:01,020 --> 00:02:02,940
I mean, if we look at last year,

39
00:02:02,940 --> 00:02:06,000
probably most of the workflows
are relayed around NA10,

40
00:02:06,000 --> 00:02:07,500
and those are the ones
that are going around.

41
00:02:07,500 --> 00:02:11,340
Whereas now we have workflows
on the mainstream side.

42
00:02:11,340 --> 00:02:13,740
And then we have domain infusion,

43
00:02:13,740 --> 00:02:17,100
which is going through
knowledge graphs and ontologies,

44
00:02:17,100 --> 00:02:21,120
which is kind of helping us
put not only the domain part

45
00:02:21,120 --> 00:02:23,250
into the agentic application,

46
00:02:23,250 --> 00:02:26,160
but also any
enterprise-specific information

47
00:02:26,160 --> 00:02:28,620
that we need to plug in to these agents

48
00:02:28,620 --> 00:02:30,510
from a context standpoint of view.

49
00:02:30,510 --> 00:02:34,770
This combined with MCP servers and evals

50
00:02:34,770 --> 00:02:37,860
is giving us very clear
deterministic performance

51
00:02:37,860 --> 00:02:41,010
where the LLMs are not hallucinating,

52
00:02:41,010 --> 00:02:43,500
but we are able to
drive specific questions

53
00:02:43,500 --> 00:02:46,470
and specific answers that
are running through it.

54
00:02:46,470 --> 00:02:47,700
And then if you look at this

55
00:02:47,700 --> 00:02:51,090
from a analytic standpoint of view,

56
00:02:51,090 --> 00:02:53,400
we need reasoning and
then also specifically

57
00:02:53,400 --> 00:02:55,170
from a fine tuning standpoint of view

58
00:02:55,170 --> 00:02:58,320
where I can ask the question
to the bigger model,

59
00:02:58,320 --> 00:03:00,960
but then it triggers a
smaller fine tune model

60
00:03:00,960 --> 00:03:04,140
which is answering some
of the specific things

61
00:03:04,140 --> 00:03:06,720
that are necessary for that to perform.

62
00:03:06,720 --> 00:03:08,370
And then you have the workflows,

63
00:03:08,370 --> 00:03:10,050
the domain infusion that is there.

64
00:03:10,050 --> 00:03:12,780
And then I need to combine
this with the compliance

65
00:03:12,780 --> 00:03:17,100
and the security layer,
because if you look at taking

66
00:03:17,100 --> 00:03:20,400
an application into production
on the enterprise side,

67
00:03:20,400 --> 00:03:24,120
compliance and legal needs
to approve these agents

68
00:03:24,120 --> 00:03:25,230
that are running around.

69
00:03:25,230 --> 00:03:29,520
So for example, one of the
application that we have deployed

70
00:03:29,520 --> 00:03:34,520
was on the healthcare side
where the reps are using it,

71
00:03:34,560 --> 00:03:36,660
the medical service liaisons are using it,

72
00:03:36,660 --> 00:03:39,960
and the home office is
also using the application.

73
00:03:39,960 --> 00:03:41,940
And then the compliance team needs to have

74
00:03:41,940 --> 00:03:45,480
configurable guardrails
which they can control

75
00:03:45,480 --> 00:03:48,420
and see how the applications are doing.

76
00:03:48,420 --> 00:03:52,470
This, combined with context engineering

77
00:03:52,470 --> 00:03:55,680
and also the LLM ops
and the telemetry side,

78
00:03:55,680 --> 00:04:00,000
is helping us drive that
performance of the agentic apps

79
00:04:00,000 --> 00:04:02,013
that we are deploying at this juncture.

80
00:04:03,090 --> 00:04:05,460
So today, I thought I
will just walk you guys

81
00:04:05,460 --> 00:04:08,730
through four specific examples.

82
00:04:08,730 --> 00:04:11,970
One is on demand customer analytics,

83
00:04:11,970 --> 00:04:13,820
and many of you would've
done customer analytics

84
00:04:13,820 --> 00:04:15,060
in the past.

85
00:04:15,060 --> 00:04:17,880
It always takes a lot of
time for us to execute it.

86
00:04:17,880 --> 00:04:20,670
But now with the agentic AI applications,

87
00:04:20,670 --> 00:04:22,863
we're able to take complex questions

88
00:04:22,863 --> 00:04:26,200
that are coming from home
office and then drive that

89
00:04:27,540 --> 00:04:30,900
analysis end to end and then
bring it back to the table.

90
00:04:30,900 --> 00:04:33,900
And all these four applications
I think are in my mind,

91
00:04:33,900 --> 00:04:37,620
are kind of increasing in sophistication.

92
00:04:37,620 --> 00:04:40,320
So I'm able to do the analysis.

93
00:04:40,320 --> 00:04:42,960
Then in the case of a
marketing standpoint of view,

94
00:04:42,960 --> 00:04:46,110
you're able to generate the
right content and the response,

95
00:04:46,110 --> 00:04:49,470
and then execute it from a CRM side.

96
00:04:49,470 --> 00:04:50,820
And then segmentation is something

97
00:04:50,820 --> 00:04:53,730
that we are using from a
marketing analytics side.

98
00:04:53,730 --> 00:04:57,000
And then also forecasting
probably is the most nuanced

99
00:04:57,000 --> 00:05:00,300
because there are multiple
stakeholders that are involved.

100
00:05:00,300 --> 00:05:03,630
Everybody has an opinion on
what the forecast needs to be

101
00:05:03,630 --> 00:05:05,677
or what are some of the
drivers that are driving it.

102
00:05:05,677 --> 00:05:09,660
So those, the four examples I
thought would be interesting

103
00:05:09,660 --> 00:05:10,950
to share with you guys.

104
00:05:10,950 --> 00:05:13,020
All of these are currently deployed

105
00:05:13,020 --> 00:05:15,600
in multiple Fortune 500 companies.

106
00:05:15,600 --> 00:05:18,270
They are in production at this juncture,

107
00:05:18,270 --> 00:05:20,910
and they're driving impact.

108
00:05:20,910 --> 00:05:24,010
So the first one that I would
like to walk you guys through

109
00:05:24,960 --> 00:05:26,973
is on customer analytics.

110
00:05:27,960 --> 00:05:32,960
And, so in this case you can
see that I've asked a question

111
00:05:33,450 --> 00:05:37,020
and then the application
is thinking through

112
00:05:37,020 --> 00:05:39,510
what are the different steps
that I need to execute.

113
00:05:39,510 --> 00:05:41,970
So this includes, is this a question

114
00:05:41,970 --> 00:05:44,370
that I can actually
answer that is approved,

115
00:05:44,370 --> 00:05:47,490
or maybe I'm not allowed
to answer this question?

116
00:05:47,490 --> 00:05:48,720
It thinks through that.

117
00:05:48,720 --> 00:05:50,940
If there's secondary steps,
it's going through that,

118
00:05:50,940 --> 00:05:54,330
and then generates a summary
from that standpoint of view.

119
00:05:54,330 --> 00:05:56,613
If you look back two years back,

120
00:05:58,410 --> 00:06:00,210
the best ChatGPT video generating,

121
00:06:00,210 --> 00:06:02,670
well, like a Python chart at best,

122
00:06:02,670 --> 00:06:05,070
but in this case, now
we are able to provide

123
00:06:05,070 --> 00:06:06,900
full scale visualization.

124
00:06:06,900 --> 00:06:09,990
And then it's also integrated
into the CRM system

125
00:06:09,990 --> 00:06:10,830
at this point.

126
00:06:10,830 --> 00:06:12,840
So you have the agentic AI application

127
00:06:12,840 --> 00:06:16,380
which the reps can access
directly from Salesforce

128
00:06:16,380 --> 00:06:19,680
or in this case Veeva
on the healthcare side.

129
00:06:19,680 --> 00:06:22,140
And they're able to interact
with those applications.

130
00:06:22,140 --> 00:06:26,250
And similarly, when we
look at medical reps

131
00:06:26,250 --> 00:06:29,520
that are accessing it, they're
combining both structured

132
00:06:29,520 --> 00:06:34,200
and unstructured data together
and driving those insights.

133
00:06:34,200 --> 00:06:36,690
So it's not just structured data,

134
00:06:36,690 --> 00:06:38,610
but you can also access unstructured data,

135
00:06:38,610 --> 00:06:41,310
which is sitting in a
different application,

136
00:06:41,310 --> 00:06:44,910
and then drive the analysis
completely on this case.

137
00:06:44,910 --> 00:06:47,070
So that's kind of where we are looking at

138
00:06:47,070 --> 00:06:50,550
from a on-demand customer
analytics standpoint of view.

139
00:06:50,550 --> 00:06:53,790
This is something that we have
deployed not only on the US

140
00:06:53,790 --> 00:06:56,430
but also in APAC like
Japan and other areas

141
00:06:56,430 --> 00:07:01,430
where you have language that
needs to be taken into account.

142
00:07:01,860 --> 00:07:04,560
Because on the Japan side,

143
00:07:04,560 --> 00:07:06,510
the context is a little bit different.

144
00:07:06,510 --> 00:07:08,820
The reps behave a little bit differently.

145
00:07:08,820 --> 00:07:10,950
The home office behaves
a little bit differently.

146
00:07:10,950 --> 00:07:12,480
In fact, one of the challenges that we had

147
00:07:12,480 --> 00:07:15,000
is that Japanese language is quite formal

148
00:07:15,000 --> 00:07:16,950
when an LLM answers a question.

149
00:07:16,950 --> 00:07:19,920
It is not kind of what they're used to it.

150
00:07:19,920 --> 00:07:22,740
So you are to take that
into the consideration.

151
00:07:22,740 --> 00:07:25,500
At the same time, the
compliance departments

152
00:07:25,500 --> 00:07:28,860
needs to be able to look
at all the questions

153
00:07:28,860 --> 00:07:30,180
everybody is asking.

154
00:07:30,180 --> 00:07:32,250
So the last six to nine months,

155
00:07:32,250 --> 00:07:34,500
I think close to 100,000
questions have been asked

156
00:07:34,500 --> 00:07:37,560
by reps and medical reps and home office.

157
00:07:37,560 --> 00:07:40,530
They need to be able to look
into what are the answers

158
00:07:40,530 --> 00:07:43,470
that are coming out, trace
answers on an ongoing basis,

159
00:07:43,470 --> 00:07:46,650
and see if there are any
issues that we need to monitor.

160
00:07:46,650 --> 00:07:48,600
That's one of the
application that we are able

161
00:07:48,600 --> 00:07:50,370
to deploy into production.

162
00:07:50,370 --> 00:07:52,720
The second one that's interesting is

163
00:07:53,730 --> 00:07:56,460
more on marketing standpoint of view.

164
00:07:56,460 --> 00:08:00,030
So this has been deployed
with a very large

165
00:08:00,030 --> 00:08:00,930
real estate company.

166
00:08:00,930 --> 00:08:03,750
I think they're the second
largest or the third largest

167
00:08:03,750 --> 00:08:07,470
real estate company in the US.

168
00:08:07,470 --> 00:08:10,470
And the way they're using it is they scan

169
00:08:10,470 --> 00:08:13,530
all the new information that is happening

170
00:08:13,530 --> 00:08:15,270
at any given point in time.

171
00:08:15,270 --> 00:08:19,590
And then they go back and
see what are the new leads

172
00:08:19,590 --> 00:08:20,880
that are able to generate.

173
00:08:20,880 --> 00:08:22,860
If you're looking at it from
a marketing standpoint of view

174
00:08:22,860 --> 00:08:26,880
in the past, any context
enrichment that is being done,

175
00:08:26,880 --> 00:08:30,570
that has always been a separate
process that has been done.

176
00:08:30,570 --> 00:08:33,000
So now you're able to identify the lead,

177
00:08:33,000 --> 00:08:35,910
take that particular information,

178
00:08:35,910 --> 00:08:38,490
and then check it with Apollo or Zoominfo,

179
00:08:38,490 --> 00:08:40,440
depending upon what they're using,

180
00:08:40,440 --> 00:08:44,310
and then enrich that context
for that application.

181
00:08:44,310 --> 00:08:46,653
And then take those leads,

182
00:08:49,530 --> 00:08:52,650
and you are able to take
your existing emails

183
00:08:52,650 --> 00:08:54,279
that you might have sent.

184
00:08:54,279 --> 00:08:55,770
So if there is an MD or
someone who has written

185
00:08:55,770 --> 00:09:00,390
a lot of emails, the agent
is able to learn that context

186
00:09:00,390 --> 00:09:04,410
on what is the style in which
that emails are being written,

187
00:09:04,410 --> 00:09:06,720
and then draft the appropriate response.

188
00:09:06,720 --> 00:09:09,720
In this case, they're not
just automated emails,

189
00:09:09,720 --> 00:09:14,010
but you are continuously
learning based on what

190
00:09:14,010 --> 00:09:17,940
the lead is doing, enrich
more context around it,

191
00:09:17,940 --> 00:09:20,760
and then write the right response,

192
00:09:20,760 --> 00:09:25,410
including images or text
that is going to them, right?

193
00:09:25,410 --> 00:09:27,810
So that improves the
response rate quite a bit.

194
00:09:27,810 --> 00:09:32,010
So in this case, the real
estate company that is using it,

195
00:09:32,010 --> 00:09:35,940
they're able to scale up the
total number of marketing leads

196
00:09:35,940 --> 00:09:39,030
that they're generating,
enrich them on a daily basis,

197
00:09:39,030 --> 00:09:41,940
and then send those responses

198
00:09:41,940 --> 00:09:45,030
pretty at scale at this juncture.

199
00:09:45,030 --> 00:09:49,440
Now, if we shift this a
little bit from a marketing

200
00:09:49,440 --> 00:09:52,290
to segmentation, which most
marketing analytics company,

201
00:09:52,290 --> 00:09:54,330
marketing analytics divisions look at.

202
00:09:54,330 --> 00:09:57,120
So this is a case where we are able to run

203
00:09:57,120 --> 00:09:59,253
a full segmentation end-to-end,

204
00:10:00,360 --> 00:10:02,400
depending upon what the questions are.

205
00:10:02,400 --> 00:10:06,240
So this can be a question
around what is the,

206
00:10:06,240 --> 00:10:07,980
based on latest
information that is coming,

207
00:10:07,980 --> 00:10:09,540
and let's say your sales are down

208
00:10:09,540 --> 00:10:13,830
or there's a new segment that
you believe that is coming up

209
00:10:13,830 --> 00:10:16,350
based on demographics, you're able to look

210
00:10:16,350 --> 00:10:20,400
into a planning engine
which looks at all the data

211
00:10:20,400 --> 00:10:23,820
that's coming in, create new personas,

212
00:10:23,820 --> 00:10:25,830
reason through those personas,

213
00:10:25,830 --> 00:10:30,360
and look into what are some of the drivers

214
00:10:30,360 --> 00:10:33,570
that might be affecting that
particular demographic segment

215
00:10:33,570 --> 00:10:36,570
and what are the product
lines that are resonating

216
00:10:36,570 --> 00:10:39,840
with that segment, and
complete this entire process.

217
00:10:39,840 --> 00:10:44,320
So we kind of designed
this segmentation workflow

218
00:10:45,780 --> 00:10:50,160
which captures how a data
analyst or a data scientist

219
00:10:50,160 --> 00:10:53,340
does segmentation using multiple models,

220
00:10:53,340 --> 00:10:57,090
and then trigger that from
an agent standpoint of view

221
00:10:57,090 --> 00:11:00,210
and refresh the segmentation
on an ongoing basis.

222
00:11:00,210 --> 00:11:02,010
So in the past, if you look at it,

223
00:11:02,010 --> 00:11:04,230
it used to be probably a
three or four-member team

224
00:11:04,230 --> 00:11:06,720
or a five-member team which does this work

225
00:11:06,720 --> 00:11:11,460
or three or four months
on an ongoing basis.

226
00:11:11,460 --> 00:11:15,210
And that always used to be a lot of work.

227
00:11:15,210 --> 00:11:17,370
You cannot run multiple
segmentation personas

228
00:11:17,370 --> 00:11:18,960
at any given point in time.

229
00:11:18,960 --> 00:11:23,790
And the latency to do it has
been large at this juncture.

230
00:11:23,790 --> 00:11:28,790
Now that entire workflow
of the segmentation

231
00:11:28,950 --> 00:11:30,730
can be executed within a day or two

232
00:11:31,860 --> 00:11:34,080
across multiple datasets.

233
00:11:34,080 --> 00:11:37,470
The datasets can be in your
main data warehouse in AWS

234
00:11:37,470 --> 00:11:40,170
and also across CRM and other elements.

235
00:11:40,170 --> 00:11:42,660
And then we are able to bring
all that context together

236
00:11:42,660 --> 00:11:46,083
and then drive the full
segmentation exercise.

237
00:11:46,083 --> 00:11:50,880
A slightly more complex
engine is forecasting.

238
00:11:50,880 --> 00:11:53,970
So in this case, forecasting,

239
00:11:53,970 --> 00:11:56,640
let's say tomorrow is a
particular event that has happened

240
00:11:56,640 --> 00:11:58,550
from a pricing standpoint of view,

241
00:11:58,550 --> 00:12:03,270
or some product has a stockout scenario.

242
00:12:03,270 --> 00:12:07,110
Then I need to review how the
forecast is from last quarter

243
00:12:07,110 --> 00:12:09,060
that somebody might have done,

244
00:12:09,060 --> 00:12:11,280
look at the actuals that are going in.

245
00:12:11,280 --> 00:12:14,400
And then also there is a lot of opinions

246
00:12:14,400 --> 00:12:17,010
when we do forecasting
in an ongoing basis.

247
00:12:17,010 --> 00:12:19,920
So I want to go back and see
if a particular assumption

248
00:12:19,920 --> 00:12:22,470
is still accurate or not accurate.

249
00:12:22,470 --> 00:12:25,350
If it is not accurate,
somebody might want to smoothen

250
00:12:25,350 --> 00:12:26,700
that particular aspect.

251
00:12:26,700 --> 00:12:31,440
So that entire aspect is now
encoded into the workflow.

252
00:12:31,440 --> 00:12:33,570
This is where we are looking
at from a domain infusion

253
00:12:33,570 --> 00:12:37,020
standpoint of view where a
knowledge graph is capturing

254
00:12:37,020 --> 00:12:41,780
all the steps that today a
forecasting team is performing.

255
00:12:42,780 --> 00:12:45,810
And then take that context

256
00:12:45,810 --> 00:12:47,970
and then plug that into the system.

257
00:12:47,970 --> 00:12:50,640
In this particular scenario,

258
00:12:50,640 --> 00:12:53,940
if the business has a
question tomorrow around

259
00:12:53,940 --> 00:12:57,600
how would my forecast change
next quarter or next month

260
00:12:57,600 --> 00:13:01,200
because of a disruption
or a particular activity

261
00:13:01,200 --> 00:13:04,380
that has happened, we are
able to run that scenario

262
00:13:04,380 --> 00:13:06,573
end to end at this juncture,

263
00:13:07,620 --> 00:13:10,830
look at the risks and the
opportunities that are there,

264
00:13:10,830 --> 00:13:13,110
and how that would affect the performance

265
00:13:13,110 --> 00:13:16,140
for a given product or a business line.

266
00:13:16,140 --> 00:13:20,310
And this is being done with
human in the loop, right?

267
00:13:20,310 --> 00:13:22,560
So the team is able to supervise

268
00:13:22,560 --> 00:13:25,050
what are the different
algorithms that are running

269
00:13:25,050 --> 00:13:27,090
from an agentic standpoint of view

270
00:13:27,090 --> 00:13:31,050
and also adjust those
across the different steps

271
00:13:31,050 --> 00:13:32,040
that are happening, right?

272
00:13:32,040 --> 00:13:35,820
So this gives complete control
and transparency to the teams

273
00:13:35,820 --> 00:13:37,220
which are doing forecasting.

274
00:13:39,287 --> 00:13:41,220
And in my mind at least,
forecasting is probably

275
00:13:41,220 --> 00:13:44,580
the most nuanced and most opinionated

276
00:13:44,580 --> 00:13:45,960
if you're doing marketing analytics,

277
00:13:45,960 --> 00:13:48,780
because business has an opinion,

278
00:13:48,780 --> 00:13:50,520
the data scientist has an opinion,

279
00:13:50,520 --> 00:13:52,290
and operations gather an opinion.

280
00:13:52,290 --> 00:13:54,300
And all of this needs to be tallied again

281
00:13:54,300 --> 00:13:56,880
next quarter, next month
on an ongoing basis.

282
00:13:56,880 --> 00:14:00,600
So that, these are couple of reasons

283
00:14:00,600 --> 00:14:04,800
why I believe that
analytics and data science

284
00:14:04,800 --> 00:14:06,600
is at an inflection point

285
00:14:06,600 --> 00:14:08,790
from an agentic AI standpoint of view.

286
00:14:08,790 --> 00:14:12,090
Because if you look at all
analytics that we have done,

287
00:14:12,090 --> 00:14:16,020
and I've been in the industry
for I think 20 years now,

288
00:14:16,020 --> 00:14:20,133
pre-2010, we used to do
all of the analysis in SAS.

289
00:14:21,360 --> 00:14:23,070
Most of the work is logistic regression

290
00:14:23,070 --> 00:14:24,540
or linear regression.

291
00:14:24,540 --> 00:14:27,289
And I would say the first
inflection point was around 2011

292
00:14:27,289 --> 00:14:30,960
or 2012 when the first big
data clusters came into play

293
00:14:30,960 --> 00:14:34,050
where we had access to much larger data.

294
00:14:34,050 --> 00:14:36,780
And then probably around
2016, 2017 is when the cloud

295
00:14:36,780 --> 00:14:39,810
and the ML side took off.

296
00:14:39,810 --> 00:14:43,050
So I think probably in the next few years

297
00:14:43,050 --> 00:14:45,900
is where we can re-look at
all the (indistinct) engines

298
00:14:45,900 --> 00:14:48,900
that have been created
over the last 15, 20 years,

299
00:14:48,900 --> 00:14:51,963
which will be redone from
agentic AI standpoint of view.

300
00:14:53,610 --> 00:14:58,610
So it's not just me saying
it, but what we did is,

301
00:14:58,980 --> 00:15:02,610
in the last two months, we
asked Forrester to run a survey

302
00:15:02,610 --> 00:15:07,110
across roughly 400 different
analytics organizations.

303
00:15:07,110 --> 00:15:11,100
And primarily the survey
respondents are all decision makers

304
00:15:11,100 --> 00:15:13,710
or they're running the
particular analytics departments.

305
00:15:13,710 --> 00:15:17,250
And overwhelmingly most of
them are basically saying that,

306
00:15:17,250 --> 00:15:21,330
you know, they expect
human-in-the-loop agents

307
00:15:21,330 --> 00:15:23,940
to be coming online over the next year

308
00:15:23,940 --> 00:15:26,850
and then probably many
of them also getting

309
00:15:26,850 --> 00:15:30,210
to autonomous levels
in the next 24 months.

310
00:15:30,210 --> 00:15:33,870
So this is across 400
different organizations

311
00:15:33,870 --> 00:15:38,870
on what they expect to happen
in the next one to two years.

312
00:15:39,270 --> 00:15:43,020
And then they're also
expecting agentic analytics

313
00:15:43,020 --> 00:15:46,980
to play a significant role
in the next two years.

314
00:15:46,980 --> 00:15:50,490
Because if you look at
it, they're expecting

315
00:15:50,490 --> 00:15:54,900
agentic analytics to automate
many of the routine analytics

316
00:15:54,900 --> 00:15:57,330
that the departments are doing today.

317
00:15:57,330 --> 00:16:00,420
And they're also expecting
many of these core workflows

318
00:16:00,420 --> 00:16:05,420
to be able to generate
insights at a much faster pace.

319
00:16:06,780 --> 00:16:08,580
Like roughly half of
them are basically saying

320
00:16:08,580 --> 00:16:12,910
that the speed in time
to get those insights

321
00:16:13,770 --> 00:16:16,410
should be like half the
time what it takes today.

322
00:16:16,410 --> 00:16:19,650
And probably it should roughly cost half

323
00:16:19,650 --> 00:16:23,370
of what it costs us today to do analytics.

324
00:16:23,370 --> 00:16:26,890
So that's a significant chunk of people

325
00:16:27,750 --> 00:16:29,340
that are working on this.

326
00:16:29,340 --> 00:16:34,340
And one of the questions
I would ask the audience,

327
00:16:34,680 --> 00:16:36,810
I would leave this with this question,

328
00:16:36,810 --> 00:16:40,050
is that if every big tech organization

329
00:16:40,050 --> 00:16:43,650
is saying that they're
going to have a mid-level

330
00:16:43,650 --> 00:16:46,350
software developer or a
junior software developer

331
00:16:46,350 --> 00:16:48,263
to come online in the next 12 months,

332
00:16:48,263 --> 00:16:50,580
and the question that I'm
asking is that how long

333
00:16:50,580 --> 00:16:55,580
it would be before you have
fully functioning AI analysts

334
00:16:55,830 --> 00:16:57,843
which are doing analytics at scale?

335
00:16:59,310 --> 00:17:01,320
My guess is probably
another two or three years.

336
00:17:01,320 --> 00:17:04,290
I would say next two years
is going to be mostly

337
00:17:04,290 --> 00:17:06,450
human-in-the-loop systems,
like the ones that you guys

338
00:17:06,450 --> 00:17:09,090
have seen, whether it's
segmentation or forecasting.

339
00:17:09,090 --> 00:17:11,970
I would say most of the
analytics that we are doing

340
00:17:11,970 --> 00:17:15,210
can be redone using
human-in-the-loop systems.

341
00:17:15,210 --> 00:17:19,710
And then as we get more
comfortable with these systems,

342
00:17:19,710 --> 00:17:23,370
I would expect them to kind
of be more and more autonomous

343
00:17:23,370 --> 00:17:26,340
where you can say that,
okay, there's a system

344
00:17:26,340 --> 00:17:29,310
that's running pricing
on an ongoing basis.

345
00:17:29,310 --> 00:17:31,710
You'll have an analyst
who is monitoring it

346
00:17:31,710 --> 00:17:33,390
maybe once every two days or three days

347
00:17:33,390 --> 00:17:35,610
and see if it is everything is going fine.

348
00:17:35,610 --> 00:17:38,700
But then they're largely
kind of running that function

349
00:17:38,700 --> 00:17:40,140
on an ongoing basis.

350
00:17:40,140 --> 00:17:43,930
And that's broadly what we are expecting

351
00:17:44,910 --> 00:17:47,730
on how analytics is going to change

352
00:17:47,730 --> 00:17:49,620
and data science is going
to change over the next

353
00:17:49,620 --> 00:17:51,120
maybe four or five years.

354
00:17:51,120 --> 00:17:54,810
So those are a couple of
things I wanted to share today.

355
00:17:54,810 --> 00:17:56,940
If there are any questions,
I'm happy to answer

356
00:17:56,940 --> 00:17:58,980
on how we have developed and deployed

357
00:17:58,980 --> 00:18:02,463
some of these analytic
systems over the last year.

