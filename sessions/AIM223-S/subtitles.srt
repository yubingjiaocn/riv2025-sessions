1
00:00:00,720 --> 00:00:02,010
- [Deborah] Well thank
you all for coming today.

2
00:00:02,010 --> 00:00:04,410
I appreciate that's, I'm
hearing like, seven things,

3
00:00:04,410 --> 00:00:06,150
so hopefully I'll be able
to stay focused here.

4
00:00:06,150 --> 00:00:07,410
We'll see how good that works.

5
00:00:07,410 --> 00:00:10,740
But hopefully I'll get your
attention first when we start

6
00:00:10,740 --> 00:00:12,120
off with, what if I told you,

7
00:00:12,120 --> 00:00:13,320
and maybe you agree with

8
00:00:13,320 --> 00:00:16,830
that many businesses
are using gen AI wrong.

9
00:00:16,830 --> 00:00:18,660
Maybe not the right
conference to say that at,

10
00:00:18,660 --> 00:00:21,450
but I'm gonna go so bold to to say that.

11
00:00:21,450 --> 00:00:24,900
I do believe, and actually
industry insight would tell us

12
00:00:24,900 --> 00:00:27,390
that we're actually
utilizing gen AI wrong.

13
00:00:27,390 --> 00:00:30,420
And maybe we could think
about something in the past

14
00:00:30,420 --> 00:00:32,130
that actually tells us this is true.

15
00:00:32,130 --> 00:00:35,190
So if you think about maybe
when electricity arrived.

16
00:00:35,190 --> 00:00:37,530
You know, how did we want
to establish the history

17
00:00:37,530 --> 00:00:38,550
of electricity?

18
00:00:38,550 --> 00:00:41,730
The original intent of
electricity was literally

19
00:00:41,730 --> 00:00:43,290
to make the room brighter,

20
00:00:43,290 --> 00:00:45,180
but was that actually the best innovation

21
00:00:45,180 --> 00:00:46,590
that we had from electricity?

22
00:00:46,590 --> 00:00:49,590
We moved from candle to
light bulb, from light bulb

23
00:00:49,590 --> 00:00:51,780
to generating power
plants, to power plants

24
00:00:51,780 --> 00:00:55,020
creating new cities to cities,
creating new opportunities

25
00:00:55,020 --> 00:00:56,430
for us around the globe.

26
00:00:56,430 --> 00:01:00,030
So when you think about what
perhaps started out as a candle

27
00:01:00,030 --> 00:01:02,430
to a light bulb, amazing
innovation, I'm not trying

28
00:01:02,430 --> 00:01:05,340
to diminish the light
bulb, amazing innovation.

29
00:01:05,340 --> 00:01:07,920
However, the real transformation

30
00:01:07,920 --> 00:01:10,530
of light became when we actually started

31
00:01:10,530 --> 00:01:12,510
to exploit what that was.

32
00:01:12,510 --> 00:01:14,820
And I would say that
today in gen AI world,

33
00:01:14,820 --> 00:01:17,100
we're actually facing a similar moment,

34
00:01:17,100 --> 00:01:19,410
not to actually improve
the old, which is what

35
00:01:19,410 --> 00:01:22,200
a lot of people are focused
on, but how do I identify

36
00:01:22,200 --> 00:01:23,130
the new?

37
00:01:23,130 --> 00:01:25,650
And so as we think about that
today, we're gonna talk about

38
00:01:25,650 --> 00:01:27,510
what have you done with gen AI?

39
00:01:27,510 --> 00:01:30,510
And more importantly,
who's going to imagine

40
00:01:30,510 --> 00:01:32,130
what we have yet to do?

41
00:01:32,130 --> 00:01:34,380
And again, for me, that is
the most important thing

42
00:01:34,380 --> 00:01:36,390
that we can take away from
this is thinking about

43
00:01:36,390 --> 00:01:39,060
what we accomplish.

44
00:01:39,060 --> 00:01:40,203
So where to start?

45
00:01:42,630 --> 00:01:44,580
As we think about gen
AI, one of the things

46
00:01:44,580 --> 00:01:46,740
I wanted to talk about for a
moment are the three different

47
00:01:46,740 --> 00:01:47,700
levels of gen AI.

48
00:01:47,700 --> 00:01:50,010
A lot of things that, again,
many of you have probably heard

49
00:01:50,010 --> 00:01:53,898
about, the three levels as
we sit there and talk about

50
00:01:53,898 --> 00:01:56,551
is how do we unpack that Gen AI?

51
00:01:56,551 --> 00:01:58,380
We have summarizing existing data sets,

52
00:01:58,380 --> 00:02:00,600
repacking data into insights

53
00:02:00,600 --> 00:02:03,090
and finding new insights
and collaborations.

54
00:02:03,090 --> 00:02:06,270
And we know that most
organizations today, actually 75

55
00:02:06,270 --> 00:02:10,410
or north of 75% of them are
focused on the first two.

56
00:02:10,410 --> 00:02:14,790
And if we use that example of
my candle to the light bulb,

57
00:02:14,790 --> 00:02:16,440
you'd realize that again,

58
00:02:16,440 --> 00:02:19,110
perhaps the true value is in number three.

59
00:02:19,110 --> 00:02:21,000
One and two are table stakes.

60
00:02:21,000 --> 00:02:24,060
Again, not diminishing the
importance of them, table stakes.

61
00:02:24,060 --> 00:02:25,980
But let's unpack it for a minute.

62
00:02:25,980 --> 00:02:29,650
Show of hands maybe how many
of you have your greatest ideas

63
00:02:30,708 --> 00:02:31,800
baked on a baked strategy,

64
00:02:31,800 --> 00:02:34,500
greatest ideas, most impulsive ideas?

65
00:02:34,500 --> 00:02:36,540
Does it align to your strategy

66
00:02:36,540 --> 00:02:39,540
or is it maybe when you're
in the forever shower

67
00:02:39,540 --> 00:02:43,890
or you're taking a jog or
you're not tangled to your desk?

68
00:02:43,890 --> 00:02:45,660
It's all baked in a strategy.

69
00:02:45,660 --> 00:02:47,340
So yes, you have a strategy,

70
00:02:47,340 --> 00:02:51,270
but most of your best ideas
actually come from the edges

71
00:02:51,270 --> 00:02:52,770
when you're actually
not thinking about that.

72
00:02:52,770 --> 00:02:55,230
When you're allowing people
to think outside of what

73
00:02:55,230 --> 00:02:57,600
they're most known to think about.

74
00:02:57,600 --> 00:02:59,280
We tend to run our businesses based on

75
00:02:59,280 --> 00:03:01,320
a very regimented strategy.

76
00:03:01,320 --> 00:03:03,960
What I will tell you is actually
that's eclipsing number one

77
00:03:03,960 --> 00:03:07,500
and number two, it's actually
not allowing number three

78
00:03:07,500 --> 00:03:09,540
to happen at its fullest extent.

79
00:03:09,540 --> 00:03:12,300
So we're gonna uncover
a couple of examples

80
00:03:12,300 --> 00:03:14,550
where we might be able to help
your organizations look at

81
00:03:14,550 --> 00:03:15,960
number three.

82
00:03:15,960 --> 00:03:17,730
When we think about status quo.

83
00:03:17,730 --> 00:03:20,430
Status quo today is
built on linear thinking.

84
00:03:20,430 --> 00:03:21,780
And so if I were to ask how many

85
00:03:21,780 --> 00:03:23,970
of you are non-traditional,
non-linear thinkers?

86
00:03:23,970 --> 00:03:25,620
A lot of people raise their hands,

87
00:03:25,620 --> 00:03:27,480
particularly in the tech field.

88
00:03:27,480 --> 00:03:29,670
However, most organizations are not built

89
00:03:29,670 --> 00:03:31,050
for non-linear thinkers.

90
00:03:31,050 --> 00:03:33,390
Actually almost no
organizations, I won't say all,

91
00:03:33,390 --> 00:03:35,940
but most are non built
for non-linear thinking.

92
00:03:35,940 --> 00:03:37,950
When you think about
the hard work, how many

93
00:03:37,950 --> 00:03:40,050
of you are stuck in pilot purgatory,

94
00:03:40,050 --> 00:03:41,280
as everybody likes to call it?

95
00:03:41,280 --> 00:03:42,900
You don't have to do
all the show of hands,

96
00:03:42,900 --> 00:03:45,840
but latest statistics would say 95%

97
00:03:45,840 --> 00:03:48,690
of organizations are
stuck in pilot purgatory.

98
00:03:48,690 --> 00:03:50,580
I would argue that part
of the reason for that

99
00:03:50,580 --> 00:03:51,870
is that we haven't done the hard work

100
00:03:51,870 --> 00:03:54,060
of actually changing the infrastructure

101
00:03:54,060 --> 00:03:57,510
to actually enable people
to think non-linearly.

102
00:03:57,510 --> 00:03:59,940
Gen AI is built on
non-linear thinking, right?

103
00:03:59,940 --> 00:04:02,790
Everybody's been talking about,
well we did this with cloud

104
00:04:02,790 --> 00:04:05,310
or we did it with cyber,
we've done it previously.

105
00:04:05,310 --> 00:04:06,900
Those are actually all technologies

106
00:04:06,900 --> 00:04:08,370
that are built on linear thinking.

107
00:04:08,370 --> 00:04:09,960
They're 0's and One's thoughts.

108
00:04:09,960 --> 00:04:12,930
They actually are predicting
an if then statement.

109
00:04:12,930 --> 00:04:14,400
AI, as we all know, is not.

110
00:04:14,400 --> 00:04:17,460
It's an unpredictable learning mechanism.

111
00:04:17,460 --> 00:04:20,550
So how is it and why are
we surprised that 95%

112
00:04:20,550 --> 00:04:23,010
of our pilots are stuck in purgatory?

113
00:04:23,010 --> 00:04:24,750
They're working on an infrastructure

114
00:04:24,750 --> 00:04:27,750
and an organization that's
built for linear thinking.

115
00:04:27,750 --> 00:04:30,360
So how do we actually disrupt
that a bit to enable us

116
00:04:30,360 --> 00:04:32,010
to think about the edges
and actually do some

117
00:04:32,010 --> 00:04:33,450
of the harder work?

118
00:04:33,450 --> 00:04:35,460
Which again I would argue
would be restructuring your

119
00:04:35,460 --> 00:04:37,860
organization, restructuring
your infrastructure

120
00:04:37,860 --> 00:04:41,400
and unlearning the things
that you think you know.

121
00:04:41,400 --> 00:04:42,723
So as we think about that,

122
00:04:43,890 --> 00:04:47,340
what is this productive unpredictability?

123
00:04:47,340 --> 00:04:49,260
Again, I've talked to countless amounts

124
00:04:49,260 --> 00:04:52,050
of whether it be executives or architects

125
00:04:52,050 --> 00:04:54,840
or just humans who are
interested in gen AI.

126
00:04:54,840 --> 00:04:59,010
The value of AI is that it
actually democratizes innovation.

127
00:04:59,010 --> 00:05:01,950
It allows people who don't
necessarily need to be coders

128
00:05:01,950 --> 00:05:06,360
or thinkers or people
educated in AI tech to be able

129
00:05:06,360 --> 00:05:08,040
to actually explore.

130
00:05:08,040 --> 00:05:10,140
But again, when you think
about organizations,

131
00:05:10,140 --> 00:05:12,750
those organizations
aren't necessarily built

132
00:05:12,750 --> 00:05:15,300
to not just envelop that thought,

133
00:05:15,300 --> 00:05:16,800
'cause if I were to ask
any of you, how many

134
00:05:16,800 --> 00:05:18,150
of you appreciate diverse thinking?

135
00:05:18,150 --> 00:05:19,470
I hope all of you raise your hand

136
00:05:19,470 --> 00:05:21,570
and say we appreciate diverse thinking.

137
00:05:21,570 --> 00:05:23,430
But again, the organizations
aren't built that way.

138
00:05:23,430 --> 00:05:25,530
Performance expectations
aren't built that way.

139
00:05:25,530 --> 00:05:27,720
Infrastructures aren't built to learn.

140
00:05:27,720 --> 00:05:29,550
People aren't built to advance

141
00:05:29,550 --> 00:05:31,560
because they think you
should have a certain title

142
00:05:31,560 --> 00:05:33,960
or a certain role or a certain goal.

143
00:05:33,960 --> 00:05:37,530
All those things put parameters
on unpredictable thinking.

144
00:05:37,530 --> 00:05:39,270
So again, if we go back to the notion

145
00:05:39,270 --> 00:05:41,070
that AI is built on unpredictability

146
00:05:41,070 --> 00:05:44,070
and it will truly thrive
in unpredictable ways,

147
00:05:44,070 --> 00:05:45,990
if the organization and the
infrastructure isn't built

148
00:05:45,990 --> 00:05:48,450
to support that, we're
limiting our thinking.

149
00:05:48,450 --> 00:05:50,280
I would say we're limiting our opportunity

150
00:05:50,280 --> 00:05:51,600
and we're actually doing things based on

151
00:05:51,600 --> 00:05:54,690
what we think we know
versus allowing our egos.

152
00:05:54,690 --> 00:05:56,310
And I mean that 'cause
everybody has an ego,

153
00:05:56,310 --> 00:05:57,143
it's a good thing.

154
00:05:57,143 --> 00:05:58,920
It's a good thing to have egos.

155
00:05:58,920 --> 00:06:01,720
Is to put that aside to unlearn
what you think you know.

156
00:06:03,570 --> 00:06:05,880
So when we think about how
to go about doing that.

157
00:06:05,880 --> 00:06:08,280
So as I often think about, you know what,

158
00:06:08,280 --> 00:06:09,113
what can people do?

159
00:06:09,113 --> 00:06:10,297
I get asked a lot.

160
00:06:10,297 --> 00:06:11,880
"Yeah, I believe in all that, Deb.

161
00:06:11,880 --> 00:06:13,500
I believe in non-linear thinking.

162
00:06:13,500 --> 00:06:15,540
I believe in changing our infrastructure,

163
00:06:15,540 --> 00:06:17,280
but we don't know what to do."

164
00:06:17,280 --> 00:06:19,080
I constantly get told by people

165
00:06:19,080 --> 00:06:20,730
that they're stuck in cement.

166
00:06:20,730 --> 00:06:22,950
And so I look at how do we unharness

167
00:06:22,950 --> 00:06:25,530
or harness the unpredictable thinking?

168
00:06:25,530 --> 00:06:26,820
A couple, I'll talk through a couple

169
00:06:26,820 --> 00:06:28,650
of different assumptions that again,

170
00:06:28,650 --> 00:06:31,020
I would say we need to unlearn.

171
00:06:31,020 --> 00:06:33,870
So thinking about how to spot the edges.

172
00:06:33,870 --> 00:06:37,290
I dunno about you, but we all
say we wanna spot new talent.

173
00:06:37,290 --> 00:06:39,870
Look at examples like boards.

174
00:06:39,870 --> 00:06:41,460
Most boards are set up by people

175
00:06:41,460 --> 00:06:43,710
that are sitting in executive management,

176
00:06:43,710 --> 00:06:46,710
not actually new people who
could bring new thinking

177
00:06:46,710 --> 00:06:49,290
and or new processes to
enable that new thinking.

178
00:06:49,290 --> 00:06:51,360
And it's not to say that
boards are bad things,

179
00:06:51,360 --> 00:06:54,030
it's to say again, how do we
rethink structure to be able

180
00:06:54,030 --> 00:06:55,890
to not just spot the edges,

181
00:06:55,890 --> 00:06:58,890
but to be able to
translate them into sparks.

182
00:06:58,890 --> 00:07:00,697
I often have this conversation
with people who are like,

183
00:07:00,697 --> 00:07:02,910
"We need to do an ideathon, a hackathon."

184
00:07:02,910 --> 00:07:05,670
How many of you do that
to come up with ideas?

185
00:07:05,670 --> 00:07:07,180
The number one problem with hackathons

186
00:07:07,180 --> 00:07:09,540
and ideathons, they
actually almost never get

187
00:07:09,540 --> 00:07:12,300
implemented and they almost
never actually all scale.

188
00:07:12,300 --> 00:07:15,060
So if you were an employee who
came up with an amazing idea

189
00:07:15,060 --> 00:07:17,970
in this hackathon, how do you
think it feels for that person

190
00:07:17,970 --> 00:07:20,880
to never have anything come
to fruition with their idea?

191
00:07:20,880 --> 00:07:24,510
So when we think about is
the system, in that instance

192
00:07:24,510 --> 00:07:28,080
of the hackathon or the
ideathon, matching the outcome,

193
00:07:28,080 --> 00:07:29,850
we actually wanna change our organization,

194
00:07:29,850 --> 00:07:31,500
bring ideas to fruition.

195
00:07:31,500 --> 00:07:32,760
What usually happens in those?

196
00:07:32,760 --> 00:07:34,950
I mean I'll tell you 'cause
I've been part of many of them.

197
00:07:34,950 --> 00:07:36,540
What usually happens
is you tell the person

198
00:07:36,540 --> 00:07:38,340
with the brilliant idea that
their idea is brilliant,

199
00:07:38,340 --> 00:07:40,500
but we can't do anything with it.

200
00:07:40,500 --> 00:07:42,870
I dunno about you, but that
probably defeats the purpose.

201
00:07:42,870 --> 00:07:46,200
So if instead how do we
actually look at bending that

202
00:07:46,200 --> 00:07:49,500
and using AI to help enable those humans

203
00:07:49,500 --> 00:07:52,980
who have these amusing ideas
to bring them into fruition?

204
00:07:52,980 --> 00:07:55,080
But we're seeing a lot on
the gen AI front right now.

205
00:07:55,080 --> 00:07:57,870
Again, table stakes for
operational efficiency.

206
00:07:57,870 --> 00:07:59,730
Most of that operational efficiency

207
00:07:59,730 --> 00:08:02,820
is to automate existing processes.

208
00:08:02,820 --> 00:08:04,200
Why are we not stopping to say,

209
00:08:04,200 --> 00:08:06,180
do we need the process at all?

210
00:08:06,180 --> 00:08:08,070
Why do we not stop and
say, is there a better way

211
00:08:08,070 --> 00:08:10,020
to connect these dots
so that perhaps instead

212
00:08:10,020 --> 00:08:12,150
of just automating the broken process,

213
00:08:12,150 --> 00:08:14,610
we actually are looking at
completely restructuring the way

214
00:08:14,610 --> 00:08:16,560
we think and the way that we operate

215
00:08:16,560 --> 00:08:18,780
to not just get operational efficiency,

216
00:08:18,780 --> 00:08:20,610
but to actually create net new processes

217
00:08:20,610 --> 00:08:23,730
that will actually be able
to spot people at the edges.

218
00:08:23,730 --> 00:08:28,710
So harder work but bigger
return on the outside.

219
00:08:28,710 --> 00:08:30,960
The other one is how do
we translate the sparks?

220
00:08:30,960 --> 00:08:33,330
So again, you don't have to show of hands,

221
00:08:33,330 --> 00:08:35,490
but how many of you got
your greatest insight

222
00:08:35,490 --> 00:08:37,650
from a pilot or a prototype?

223
00:08:37,650 --> 00:08:41,070
If you did, how did you
actually put that into action?

224
00:08:41,070 --> 00:08:42,690
Again, the statistics are high.

225
00:08:42,690 --> 00:08:45,600
Seven out of 10 pilots and
prototypes that are found

226
00:08:45,600 --> 00:08:48,300
are often not actually put into execution.

227
00:08:48,300 --> 00:08:49,950
And I would say based on other research,

228
00:08:49,950 --> 00:08:51,840
the numbers are actually even higher.

229
00:08:51,840 --> 00:08:53,040
So why is that?

230
00:08:53,040 --> 00:08:55,620
Is it because a person
didn't like the idea?

231
00:08:55,620 --> 00:08:57,450
Is it because the system
and the organization

232
00:08:57,450 --> 00:08:59,010
wasn't allowing it to happen?

233
00:08:59,010 --> 00:09:01,050
But we do it over and over and over again

234
00:09:01,050 --> 00:09:02,820
because we want predictability.

235
00:09:02,820 --> 00:09:05,730
The question I'll also ask, how
many of you need a playbook?

236
00:09:05,730 --> 00:09:07,800
Okay, playbooks are things of the past.

237
00:09:07,800 --> 00:09:09,540
AI can't work on a playbook.

238
00:09:09,540 --> 00:09:11,790
It's not built to build
on a playbook, right?

239
00:09:11,790 --> 00:09:13,800
Again, back to the unpredictable learning.

240
00:09:13,800 --> 00:09:16,710
But as organizations we are
used to having playbooks.

241
00:09:16,710 --> 00:09:18,360
How do we run it? How do we scale it?

242
00:09:18,360 --> 00:09:19,620
How do we thrive it?

243
00:09:19,620 --> 00:09:20,700
Is there a playbook for that?

244
00:09:20,700 --> 00:09:22,440
Is there a rule set for that?

245
00:09:22,440 --> 00:09:25,110
How instead do we
actually start to question

246
00:09:25,110 --> 00:09:27,060
how do we have creative thinking?

247
00:09:27,060 --> 00:09:28,380
So perhaps we don't need a playbook,

248
00:09:28,380 --> 00:09:30,300
but that doesn't mean we have chaos.

249
00:09:30,300 --> 00:09:32,410
Chaos actually turns to a very different

250
00:09:33,300 --> 00:09:36,300
trajectory if not actually
managed in some way.

251
00:09:36,300 --> 00:09:38,910
And the last one is how do
we embed the breakthroughs?

252
00:09:38,910 --> 00:09:42,750
So the best thing I love
to talk about is metrics.

253
00:09:42,750 --> 00:09:44,430
Again, you don't have to show of hands,

254
00:09:44,430 --> 00:09:46,140
but if we were to talk
about metrics, how many

255
00:09:46,140 --> 00:09:48,210
of you have been told
that your best metric

256
00:09:48,210 --> 00:09:49,983
for AI implementation is speed?

257
00:09:50,940 --> 00:09:52,020
My follow up question would be,

258
00:09:52,020 --> 00:09:53,550
how many of you know your actual baseline

259
00:09:53,550 --> 00:09:56,160
for what you're measuring speed against?

260
00:09:56,160 --> 00:09:58,140
Almost nobody does by the way.

261
00:09:58,140 --> 00:10:00,000
And so when you sit there
and think about that,

262
00:10:00,000 --> 00:10:03,510
if you're told you need to
get AI into the implementation

263
00:10:03,510 --> 00:10:05,790
into your environment faster,
faster, faster, faster.

264
00:10:05,790 --> 00:10:08,160
Fast was never a metric
unless you actually know

265
00:10:08,160 --> 00:10:09,420
what you're measuring it.

266
00:10:09,420 --> 00:10:11,790
So again, how do we rethink metrics?

267
00:10:11,790 --> 00:10:13,500
How do we rethink performance?

268
00:10:13,500 --> 00:10:17,250
How do we rethink if we do
need it fast, what is fast?

269
00:10:17,250 --> 00:10:19,440
Is fast in seconds, is fast in months,

270
00:10:19,440 --> 00:10:22,350
is fast the same for my
company it is for your company?

271
00:10:22,350 --> 00:10:23,310
What does fast mean?

272
00:10:23,310 --> 00:10:25,590
Because we're running towards this notion

273
00:10:25,590 --> 00:10:27,780
of fast without actually
understanding how to embed

274
00:10:27,780 --> 00:10:30,180
breakthroughs into our organization.

275
00:10:30,180 --> 00:10:32,317
So again, I often get the question of,

276
00:10:32,317 --> 00:10:35,220
"Okay Deb, these all
sound great, like logical,

277
00:10:35,220 --> 00:10:37,467
but we can't do that in our organization."

278
00:10:38,370 --> 00:10:39,930
Again, why?

279
00:10:39,930 --> 00:10:41,970
It's typically because
we want the status quo

280
00:10:41,970 --> 00:10:43,860
and we don't like to have friction.

281
00:10:43,860 --> 00:10:46,263
AI will inevitably create friction.

282
00:10:47,190 --> 00:10:49,110
It does, it's positive friction.

283
00:10:49,110 --> 00:10:51,270
And I would say embrace that chaos

284
00:10:51,270 --> 00:10:54,600
because positive friction
actually can create change.

285
00:10:54,600 --> 00:10:57,000
We are afraid of what we don't know.

286
00:10:57,000 --> 00:11:00,600
I sat on board last
week, fortune 50 company,

287
00:11:00,600 --> 00:11:03,600
and the organizations like,
"Deb, we don't even understand

288
00:11:03,600 --> 00:11:05,070
how to utilize gen AI."

289
00:11:05,070 --> 00:11:07,410
We spent 15 minutes
utilizing AI to be able

290
00:11:07,410 --> 00:11:10,020
to create a candy company.

291
00:11:10,020 --> 00:11:11,460
They're not in the candy
business by the way,

292
00:11:11,460 --> 00:11:13,140
but just to create a candy company.

293
00:11:13,140 --> 00:11:16,860
We did demographic research,
we created a presentation,

294
00:11:16,860 --> 00:11:18,540
I had a jingle and a logo.

295
00:11:18,540 --> 00:11:20,820
We did all kinds of things in 15 minutes.

296
00:11:20,820 --> 00:11:22,740
And I did it simply so
that people could start

297
00:11:22,740 --> 00:11:26,040
to understand the value
of what they could see.

298
00:11:26,040 --> 00:11:27,810
I also did the opposite though.

299
00:11:27,810 --> 00:11:30,720
It's not just letting AI
run for the sake of AI.

300
00:11:30,720 --> 00:11:32,520
How do we actually make
them understand things

301
00:11:32,520 --> 00:11:34,140
around hallucinations,

302
00:11:34,140 --> 00:11:37,770
and how do you actually think
in an unpredictable scenario?

303
00:11:37,770 --> 00:11:40,020
So we created this candy company
that all of a sudden I had

304
00:11:40,020 --> 00:11:42,870
taking over the world in
about seven and a half minutes

305
00:11:42,870 --> 00:11:45,330
to help show the point of now how do you

306
00:11:45,330 --> 00:11:47,100
think about hallucinations?

307
00:11:47,100 --> 00:11:49,320
How do you start to think
about what AI is learning

308
00:11:49,320 --> 00:11:51,150
when you're not paying attention to it?

309
00:11:51,150 --> 00:11:52,530
How do you start looking at AI

310
00:11:52,530 --> 00:11:56,130
and asking different questions
so that if you have a space

311
00:11:56,130 --> 00:11:58,890
or a typo by the way, you
get a different answer.

312
00:11:58,890 --> 00:12:00,870
These are things that people
aren't paying attention to

313
00:12:00,870 --> 00:12:03,240
because they're like, "Yeah,
it's great, I can type

314
00:12:03,240 --> 00:12:06,030
and misspell and I still get an answer."

315
00:12:06,030 --> 00:12:07,680
So I went to another screen, I said,

316
00:12:07,680 --> 00:12:08,880
well what if you type it correctly?

317
00:12:08,880 --> 00:12:11,250
Do you realize you get two
very different answers?

318
00:12:11,250 --> 00:12:13,230
Do you also understand that
you have hallucinations?

319
00:12:13,230 --> 00:12:15,060
If I happen to word my question

320
00:12:15,060 --> 00:12:17,910
and ask what kind of pet
should I get tomorrow?

321
00:12:17,910 --> 00:12:20,610
If I happen to put an
infer that I like a cat,

322
00:12:20,610 --> 00:12:21,660
I'll get a cat as an answer.

323
00:12:21,660 --> 00:12:25,380
If I happen to infer I get a
dog, I get a dog as an answer.

324
00:12:25,380 --> 00:12:26,910
I'll ask again by show of hands, how many

325
00:12:26,910 --> 00:12:28,500
of you thank your AI?

326
00:12:28,500 --> 00:12:29,333
Most people don't.

327
00:12:29,333 --> 00:12:31,260
If you thank it, you get better answers.

328
00:12:31,260 --> 00:12:34,140
So there's lots of ways here that yes,

329
00:12:34,140 --> 00:12:36,750
AI is going to allow us
to democratize innovation

330
00:12:36,750 --> 00:12:39,600
and put new technology
into our organizations,

331
00:12:39,600 --> 00:12:41,700
but you have to understand how to now

332
00:12:41,700 --> 00:12:45,150
kind of sort of manipulate
this unpredictable learning.

333
00:12:45,150 --> 00:12:48,060
So I did this whole candy
store in seven minutes,

334
00:12:48,060 --> 00:12:50,370
and then we did it again
with all misspellings.

335
00:12:50,370 --> 00:12:52,170
I misspelled the whole thing

336
00:12:52,170 --> 00:12:54,360
and we got a totally
different candy company.

337
00:12:54,360 --> 00:12:56,190
And I said, "These are the
things you need to understand,

338
00:12:56,190 --> 00:12:58,440
not because you should
be afraid of using AI

339
00:12:58,440 --> 00:13:00,210
or allowing people to play around with it

340
00:13:00,210 --> 00:13:02,400
or do different things
with it, but understand

341
00:13:02,400 --> 00:13:04,440
what it really means to
have human in the loop."

342
00:13:04,440 --> 00:13:06,810
Historically, when we think
about human in the loop,

343
00:13:06,810 --> 00:13:08,910
we think about a human in
the loop like as a checks

344
00:13:08,910 --> 00:13:10,020
and balances.

345
00:13:10,020 --> 00:13:12,060
You had five things, I
need to have five things.

346
00:13:12,060 --> 00:13:15,120
Human in the loop in an AI
world is not just checks

347
00:13:15,120 --> 00:13:17,910
and balances, it's you
looking at that scenario

348
00:13:17,910 --> 00:13:20,520
and saying, "Did someone write the AI code

349
00:13:20,520 --> 00:13:21,930
And all of a sudden I ended up with a dog

350
00:13:21,930 --> 00:13:23,100
when I wanted to have a cat?"

351
00:13:23,100 --> 00:13:27,120
And so understanding what that
difference is super important

352
00:13:27,120 --> 00:13:29,820
as we think about how to
put these into practice.

353
00:13:29,820 --> 00:13:33,390
And so I'll spend a little
bit of time talking about

354
00:13:33,390 --> 00:13:37,110
how you can actually go
about doing it realistically.

355
00:13:37,110 --> 00:13:38,880
So as we forge this path,

356
00:13:38,880 --> 00:13:40,650
and I know we're talking
a little bit about gen AI,

357
00:13:40,650 --> 00:13:44,040
but it would be applicable
for broader AI, agentic AI,

358
00:13:44,040 --> 00:13:46,177
the number one thing
that we tend to get is,

359
00:13:46,177 --> 00:13:47,760
"Well, it's gonna replace people."

360
00:13:47,760 --> 00:13:51,900
It will change our talent
trajectory as a co-pilot.

361
00:13:51,900 --> 00:13:54,600
I don't actually ever talk
about it in the factor of like,

362
00:13:54,600 --> 00:13:56,130
humanity will be gone tomorrow.

363
00:13:56,130 --> 00:13:57,660
It is actually a co-pilot for us

364
00:13:57,660 --> 00:13:59,490
as we think about innovation.

365
00:13:59,490 --> 00:14:02,340
So how do we spot the
things that others overlook?

366
00:14:02,340 --> 00:14:04,440
I'm often an outlier. I love it.

367
00:14:04,440 --> 00:14:07,380
I wear that with a badge
of honor to be an outlier.

368
00:14:07,380 --> 00:14:09,570
But we want other outliers.

369
00:14:09,570 --> 00:14:12,270
We want to be able to
spot that there are people

370
00:14:12,270 --> 00:14:15,930
who are hungry to bring new
ideas into your organization.

371
00:14:15,930 --> 00:14:18,420
It's not always easy as
we think about traversing

372
00:14:18,420 --> 00:14:21,390
an organization, again,
there's performance metrics,

373
00:14:21,390 --> 00:14:23,640
there's titles, there's roles,

374
00:14:23,640 --> 00:14:26,400
there's things that fit
into neat little boxes.

375
00:14:26,400 --> 00:14:27,990
There's homework that needs to be done

376
00:14:27,990 --> 00:14:29,460
to restructure organizations

377
00:14:29,460 --> 00:14:31,650
while we're working on
operational efficiencies

378
00:14:31,650 --> 00:14:34,080
and we're working on all the table stakes.

379
00:14:34,080 --> 00:14:35,700
The competitive advantage is gonna be

380
00:14:35,700 --> 00:14:37,680
to create new businesses.

381
00:14:37,680 --> 00:14:38,580
It is.

382
00:14:38,580 --> 00:14:40,950
The business that you see today
probably isn't gonna exist

383
00:14:40,950 --> 00:14:43,560
in that same way 3, 5, 10 years.

384
00:14:43,560 --> 00:14:45,420
I actually would think
three to five years,

385
00:14:45,420 --> 00:14:47,130
studies would say 10 years,

386
00:14:47,130 --> 00:14:49,110
but at some point in time in the future.

387
00:14:49,110 --> 00:14:51,060
Most people will say, "Well
Deb, that's not my problem,

388
00:14:51,060 --> 00:14:52,560
it's the next leader's problem."

389
00:14:52,560 --> 00:14:54,900
It's everybody's opportunity.

390
00:14:54,900 --> 00:14:57,810
Flip the script, unlearn what
you think you know to be able

391
00:14:57,810 --> 00:14:58,950
to get there.

392
00:14:58,950 --> 00:15:01,230
To that point, how do
you translate discoveries

393
00:15:01,230 --> 00:15:02,970
into new ways of working?

394
00:15:02,970 --> 00:15:04,650
I'm a huge believer of sandboxes,

395
00:15:04,650 --> 00:15:07,440
but I'm a huge believer
of sandboxes in actually

396
00:15:07,440 --> 00:15:09,330
representing your organization.

397
00:15:09,330 --> 00:15:12,090
Most sandboxes, again, I
think it's eight out of 10,

398
00:15:12,090 --> 00:15:13,800
don't replicate your current environment

399
00:15:13,800 --> 00:15:16,500
by people process, technology,
infrastructure, ecosystem,

400
00:15:16,500 --> 00:15:18,150
third parties.

401
00:15:18,150 --> 00:15:19,560
Part of what's gonna be advantageous

402
00:15:19,560 --> 00:15:21,360
as we think about an AI forward world

403
00:15:21,360 --> 00:15:22,800
is gonna be an ecosystem.

404
00:15:22,800 --> 00:15:24,030
What better place to talk about

405
00:15:24,030 --> 00:15:27,060
that on this floor when
all you see is ecosystems,

406
00:15:27,060 --> 00:15:30,330
but I mean looking at your
competitors as your partners,

407
00:15:30,330 --> 00:15:32,070
not as competitors.

408
00:15:32,070 --> 00:15:35,040
What you think is competitive
advantage is probably not

409
00:15:35,040 --> 00:15:37,350
gonna be competitive advantage in a year.

410
00:15:37,350 --> 00:15:40,230
And I don't say it to be like,
we should all be worried,

411
00:15:40,230 --> 00:15:42,090
concerned, dystopian.

412
00:15:42,090 --> 00:15:44,460
It is to look at the opportunity.

413
00:15:44,460 --> 00:15:47,640
How do you unlearn what
competitors mean in this world

414
00:15:47,640 --> 00:15:49,050
of AI?

415
00:15:49,050 --> 00:15:50,970
And then lastly, embed what works.

416
00:15:50,970 --> 00:15:54,030
We're very afraid to
put gen AI directly into

417
00:15:54,030 --> 00:15:56,190
an environment and actually test it.

418
00:15:56,190 --> 00:15:58,380
Test it and learn from it,
test it and learn from it

419
00:15:58,380 --> 00:16:02,460
and make decisions in 30
days, 45 days, 90 days.

420
00:16:02,460 --> 00:16:04,530
It's not the traditional six months.

421
00:16:04,530 --> 00:16:06,090
I don't know about many of you,

422
00:16:06,090 --> 00:16:08,250
how many of you walk through an airport,

423
00:16:08,250 --> 00:16:10,560
a bathroom, a doctor's office,
you see the happy button.

424
00:16:10,560 --> 00:16:12,120
So those buttons like,
"Are you happy today?"

425
00:16:12,120 --> 00:16:13,620
I'm like, "I don't know, I
just walked out of the bathroom

426
00:16:13,620 --> 00:16:14,453
of an airport.

427
00:16:14,453 --> 00:16:16,200
I don't know if I'm supposed to be happy."

428
00:16:16,200 --> 00:16:19,770
But the way that this is doing
is because in today's age,

429
00:16:19,770 --> 00:16:21,420
the constant feedback loop.

430
00:16:21,420 --> 00:16:25,080
So your consumer, your customer
wants constant feedback.

431
00:16:25,080 --> 00:16:27,570
They wanna be involved in the
process, they wanna understand

432
00:16:27,570 --> 00:16:29,490
what your product is
gonna look like tomorrow.

433
00:16:29,490 --> 00:16:31,800
The question for you as leaders

434
00:16:31,800 --> 00:16:34,500
and as organizations, are
you gonna change your product

435
00:16:34,500 --> 00:16:35,760
development lifecycle?

436
00:16:35,760 --> 00:16:37,560
Are you gonna actually
take all that feedback?

437
00:16:37,560 --> 00:16:39,510
What happens if you lose your customer?

438
00:16:39,510 --> 00:16:43,470
Can I utilize AI to change
what this experience is?

439
00:16:43,470 --> 00:16:46,710
Consumers want more impact.
They want more input.

440
00:16:46,710 --> 00:16:48,480
I start hitting the unhappy button a lot,

441
00:16:48,480 --> 00:16:49,620
will I get an email?

442
00:16:49,620 --> 00:16:51,360
Will I have input into
what this new bathroom

443
00:16:51,360 --> 00:16:52,740
is gonna look like?

444
00:16:52,740 --> 00:16:54,330
Candidly, no, nothing actually happens.

445
00:16:54,330 --> 00:16:56,556
I've gone and talked to some
of those companies as well.

446
00:16:56,556 --> 00:16:57,540
I'm like, "What are you expecting?

447
00:16:57,540 --> 00:16:58,920
What's your metric?"

448
00:16:58,920 --> 00:17:01,350
And candidly, their metric
is like, just a happy place.

449
00:17:01,350 --> 00:17:02,640
I'm like, "Well, what's happy for me

450
00:17:02,640 --> 00:17:04,170
might not be happy for you."

451
00:17:04,170 --> 00:17:07,410
So again, what's the
metric of measurement?

452
00:17:07,410 --> 00:17:10,140
And then as we think about
what the future holds

453
00:17:10,140 --> 00:17:12,450
and we think about
automating predictability,

454
00:17:12,450 --> 00:17:15,753
I am the hugest believer
in opportunity here.

455
00:17:17,130 --> 00:17:18,630
My mother died when I was young

456
00:17:18,630 --> 00:17:20,850
and no child should ever have to suffer

457
00:17:20,850 --> 00:17:21,810
anything of that nature.

458
00:17:21,810 --> 00:17:25,050
And if we can use technology
to solve for problems

459
00:17:25,050 --> 00:17:27,180
that we've never imagined solvable,

460
00:17:27,180 --> 00:17:30,060
we are in that moment right
now to be able to solve

461
00:17:30,060 --> 00:17:31,020
for tomorrow.

462
00:17:31,020 --> 00:17:33,360
So all these are to say of diving in

463
00:17:33,360 --> 00:17:35,010
and how to make that work.

464
00:17:35,010 --> 00:17:37,530
The winners, quote unquote, won't be those

465
00:17:37,530 --> 00:17:40,320
that automate the unpredictability out.

466
00:17:40,320 --> 00:17:42,210
It's gonna be those that
can actually embrace

467
00:17:42,210 --> 00:17:43,920
what predictability thinks about

468
00:17:43,920 --> 00:17:46,200
and unpredictability by design.

469
00:17:46,200 --> 00:17:49,710
How do you build that
unpredictability into your people,

470
00:17:49,710 --> 00:17:52,440
into your process, into
what an org structure

471
00:17:52,440 --> 00:17:53,310
even looks like.

472
00:17:53,310 --> 00:17:55,320
I was working with somebody
a couple weeks ago,

473
00:17:55,320 --> 00:17:57,840
what if we got rid of an org structure?

474
00:17:57,840 --> 00:17:59,940
Three people in that
conversation's mind exploded.

475
00:17:59,940 --> 00:18:01,590
They're like, "We can't get
rid of an org structure.

476
00:18:01,590 --> 00:18:02,423
What would happen?"

477
00:18:02,423 --> 00:18:04,740
I'm like, "I don't know,
what would happen?"

478
00:18:04,740 --> 00:18:07,080
Perhaps we can scenario
what that would look like.

479
00:18:07,080 --> 00:18:08,880
Perhaps we could use AI
to help us figure out

480
00:18:08,880 --> 00:18:10,170
what that would look like.

481
00:18:10,170 --> 00:18:13,230
But are we willing to break
what we think we know in order

482
00:18:13,230 --> 00:18:15,840
to see the biggest amount
of competitive advantage?

483
00:18:15,840 --> 00:18:18,330
I'd encourage all of you to
think about all the amazing

484
00:18:18,330 --> 00:18:19,530
ideas you've had.

485
00:18:19,530 --> 00:18:22,350
Again, likely in the shower
on a run, doing something that

486
00:18:22,350 --> 00:18:24,840
where your brain is free to
be able to think about it,

487
00:18:24,840 --> 00:18:28,560
and think about how you can
employ AI while still doing it

488
00:18:28,560 --> 00:18:31,350
trustworthy, in a secure and safe fashion,

489
00:18:31,350 --> 00:18:34,320
and at the same time showing that value

490
00:18:34,320 --> 00:18:36,300
and showing that growth
to others around you.

491
00:18:36,300 --> 00:18:38,400
That will be your competitive advantage.

492
00:18:38,400 --> 00:18:40,770
With that, I'm like, talk like
the speed of light to be able

493
00:18:40,770 --> 00:18:42,000
to get that done in 20 minutes.

494
00:18:42,000 --> 00:18:43,350
I think I've got 60 seconds.

495
00:18:43,350 --> 00:18:45,810
But that being said, that
is my discussion today.

496
00:18:45,810 --> 00:18:47,910
If you have any questions
at all, please feel free

497
00:18:47,910 --> 00:18:49,950
to find me or to find me on LinkedIn.

498
00:18:49,950 --> 00:18:52,920
Otherwise, I greatly appreciate
all of your time today

499
00:18:52,920 --> 00:18:54,360
and that's it.

500
00:18:54,360 --> 00:18:55,353
So thank you.

