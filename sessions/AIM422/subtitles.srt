1
00:00:01,008 --> 00:00:02,379
OK. Um, good afternoon,

2
00:00:02,660 --> 00:00:04,820
everyone, and welcome to the session on agentic

3
00:00:04,820 --> 00:00:06,980
AI meets responsible AI strategy

4
00:00:06,980 --> 00:00:08,050
and best practices.

5
00:00:08,579 --> 00:00:10,679
So continuing the theme of reinvent where

6
00:00:11,130 --> 00:00:13,528
it seems that all things agentic are

7
00:00:13,528 --> 00:00:14,519
front and center.

8
00:00:14,819 --> 00:00:16,888
Um, today, um, along with

9
00:00:16,888 --> 00:00:18,978
my colleague Peter Hainan, who's sitting over here

10
00:00:18,978 --> 00:00:20,379
and will take over shortly,

11
00:00:20,739 --> 00:00:21,260
um,

12
00:00:21,699 --> 00:00:23,489
uh, I'll outline things in a second. We,

13
00:00:23,859 --> 00:00:25,478
we don't just tell you some,

14
00:00:26,260 --> 00:00:28,260
you know, some basics about agentic AI.

15
00:00:28,870 --> 00:00:30,908
And I'm gonna talk a little bit about the scientific

16
00:00:30,908 --> 00:00:32,380
challenges behind it

17
00:00:32,668 --> 00:00:35,189
and, and possibly anticipated

18
00:00:35,189 --> 00:00:37,310
solutions to those and then Peter will talk

19
00:00:37,310 --> 00:00:37,929
more about

20
00:00:38,289 --> 00:00:41,090
industrial evolution and, and practicalities.

21
00:00:41,789 --> 00:00:43,840
Um So just the agenda,

22
00:00:43,848 --> 00:00:45,969
I'm gonna start and just kind of level set us

23
00:00:45,969 --> 00:00:47,759
all and talk about what

24
00:00:48,168 --> 00:00:50,590
what agents are and what agentic AI means,

25
00:00:50,939 --> 00:00:53,529
um, then spend most of my time talking about scientific

26
00:00:53,529 --> 00:00:55,539
frontiers or more, more accurately

27
00:00:55,539 --> 00:00:56,310
challenges,

28
00:00:56,689 --> 00:00:58,689
um, and then Peter will take over and,

29
00:00:58,759 --> 00:01:00,969
and talk about more practical topics.

30
00:01:01,918 --> 00:01:02,639
Um,

31
00:01:03,118 --> 00:01:05,168
so just to

32
00:01:05,558 --> 00:01:07,760
kind of, um, give some kind of spectrum

33
00:01:07,760 --> 00:01:09,838
along which we can arrange agentic AI,

34
00:01:09,859 --> 00:01:12,040
I, I think the word agentic

35
00:01:12,040 --> 00:01:14,528
and agentic AI is probably, um,

36
00:01:14,879 --> 00:01:16,918
terminology that's a little bit too crude

37
00:01:16,918 --> 00:01:18,980
for the vast spectrum of things that are covered.

38
00:01:19,359 --> 00:01:21,400
I'm sure many of you are familiar with are familiar with

39
00:01:21,400 --> 00:01:23,650
and are perhaps regular users of large

40
00:01:23,650 --> 00:01:24,638
language models.

41
00:01:25,040 --> 00:01:27,480
And so we're all used to the phenomenon of interacting

42
00:01:27,480 --> 00:01:29,680
with a large language model in a dialogue and a

43
00:01:29,680 --> 00:01:30,819
turn taking basis.

44
00:01:31,948 --> 00:01:34,349
I think the logical extreme of agentic AI

45
00:01:34,349 --> 00:01:35,969
is an LLM based system

46
00:01:36,459 --> 00:01:38,668
that has access to unfettered

47
00:01:38,668 --> 00:01:40,750
access to external resources

48
00:01:40,750 --> 00:01:42,989
information and can take consequential,

49
00:01:43,069 --> 00:01:45,069
possibly irreversible actions in

50
00:01:45,069 --> 00:01:45,959
the real world.

51
00:01:46,388 --> 00:01:49,069
So think things like, you know, engaging in financial

52
00:01:49,069 --> 00:01:51,400
transactions on your behalf, signing

53
00:01:51,400 --> 00:01:52,379
contracts on your behalf,

54
00:01:52,668 --> 00:01:55,189
or even more consequentially taking military

55
00:01:55,189 --> 00:01:57,500
actions or medical decision making actions

56
00:01:57,500 --> 00:01:58,308
autonomously.

57
00:02:00,010 --> 00:02:02,250
I don't know of any agentic system

58
00:02:02,250 --> 00:02:04,329
right now that that touches that

59
00:02:04,329 --> 00:02:06,329
logical extreme or comes even close

60
00:02:06,329 --> 00:02:08,449
to it, which is why I think we need language for the

61
00:02:08,449 --> 00:02:09,569
stuff that's in between.

62
00:02:10,008 --> 00:02:12,050
But in this, in this chart what

63
00:02:12,050 --> 00:02:14,219
we're doing is sort of arranging systems

64
00:02:14,219 --> 00:02:16,330
along the particular dimension of what level

65
00:02:16,330 --> 00:02:18,050
of autonomy the agent has.

66
00:02:18,528 --> 00:02:20,639
So at one end we have sort of

67
00:02:20,639 --> 00:02:23,199
just turn taking interactions with an LLM possibly

68
00:02:23,199 --> 00:02:23,830
in service

69
00:02:24,210 --> 00:02:26,469
of completing a task or getting information,

70
00:02:26,969 --> 00:02:29,300
um. And in the middle I think um

71
00:02:29,300 --> 00:02:31,379
is where most of the action will be in the near

72
00:02:31,379 --> 00:02:33,379
term and that is around what I'm gonna shortly

73
00:02:33,379 --> 00:02:35,409
call kind of planning agents. These

74
00:02:35,409 --> 00:02:36,399
are agents that

75
00:02:36,819 --> 00:02:39,038
can do that you don't need to hold their hand

76
00:02:39,038 --> 00:02:40,439
on a turn by turn dialogue make um

77
00:02:42,490 --> 00:02:43,919
dialogue making basis.

78
00:02:44,300 --> 00:02:46,538
You can give them a high level goal and

79
00:02:46,538 --> 00:02:48,659
ask them to plan and execute the steps

80
00:02:48,659 --> 00:02:51,038
to it and expect that it, you know, will

81
00:02:51,338 --> 00:02:53,419
make forward progress, possibly get stuck and

82
00:02:53,419 --> 00:02:55,399
revise its plan and continue,

83
00:02:55,939 --> 00:02:58,679
um. And so, um,

84
00:02:58,689 --> 00:03:00,808
what are some examples of agents that I kind of have

85
00:03:00,808 --> 00:03:03,069
in mind? Well, I think one example

86
00:03:03,069 --> 00:03:05,169
that that people often think in mind is

87
00:03:05,169 --> 00:03:06,050
sort of a,

88
00:03:06,338 --> 00:03:08,610
um agentic executive assistant.

89
00:03:08,819 --> 00:03:11,088
So what do I mean by an agentic executive assistant?

90
00:03:11,629 --> 00:03:13,830
This would be somebody that has access to and

91
00:03:13,830 --> 00:03:15,990
knows everything about your schedule, your

92
00:03:15,990 --> 00:03:17,669
calendar, your commitments,

93
00:03:18,149 --> 00:03:20,149
your travel preferences, the con, you know,

94
00:03:20,229 --> 00:03:22,308
whether you like to sit in aisle or window seats,

95
00:03:22,419 --> 00:03:24,508
the conditions under which you might be willing to

96
00:03:24,508 --> 00:03:26,868
pay for an upgrade to first class on a long

97
00:03:26,868 --> 00:03:27,758
international flight,

98
00:03:28,110 --> 00:03:28,758
and the like,

99
00:03:29,110 --> 00:03:31,550
and actually has the ability to again

100
00:03:31,550 --> 00:03:33,588
engage in transactions for you, to go out

101
00:03:33,588 --> 00:03:35,788
and find tickets that you, the, the

102
00:03:35,788 --> 00:03:37,868
tickets you might have bought yourself and purchased them

103
00:03:37,868 --> 00:03:40,229
for you. Um, by advanced

104
00:03:40,229 --> 00:03:42,409
planning agents, I mean agents that can,

105
00:03:42,508 --> 00:03:43,179
as I said,

106
00:03:43,550 --> 00:03:45,679
just take a high level specification

107
00:03:45,679 --> 00:03:46,219
and

108
00:03:46,868 --> 00:03:49,278
plan the steps to execute that task.

109
00:03:49,629 --> 00:03:50,449
And so let me

110
00:03:50,990 --> 00:03:53,058
digress for a second and tell you about a

111
00:03:53,058 --> 00:03:54,889
little experience I had over the summer.

112
00:03:55,349 --> 00:03:57,349
Um, so, um, as you might have noticed on

113
00:03:57,349 --> 00:03:59,710
the title slide, I am what's known as an Amazon

114
00:03:59,710 --> 00:04:01,830
scholar, which means that I divide my

115
00:04:01,830 --> 00:04:03,250
time between Amazon.

116
00:04:03,639 --> 00:04:05,649
And the computer science faculty at the

117
00:04:05,649 --> 00:04:07,110
University of Pennsylvania. So I'm a

118
00:04:07,479 --> 00:04:09,649
career-long machine learning researcher. I started

119
00:04:09,649 --> 00:04:12,069
working in the field in the late 1980s. So

120
00:04:12,528 --> 00:04:14,599
suffice to say I've seen an alarming amount of

121
00:04:14,599 --> 00:04:16,509
change in the last four decades or so.

122
00:04:17,389 --> 00:04:18,449
And um

123
00:04:19,069 --> 00:04:21,069
this summer a faculty colleague and

124
00:04:21,069 --> 00:04:23,309
I and a visiting graduate student actually

125
00:04:23,309 --> 00:04:24,949
wrote from scratch

126
00:04:25,230 --> 00:04:28,108
a roughly 50 page mathematical

127
00:04:28,108 --> 00:04:29,639
paper from scratch

128
00:04:30,019 --> 00:04:32,108
with the help of kind of a coding based

129
00:04:32,108 --> 00:04:34,108
LLM tool and so I'm gonna tell you a little

130
00:04:34,108 --> 00:04:35,569
bit more about that experience.

131
00:04:36,149 --> 00:04:38,309
So many of you I'm sure are quite familiar with

132
00:04:38,309 --> 00:04:40,509
and perhaps are regular users again of

133
00:04:40,509 --> 00:04:42,548
coding agents where you can basically

134
00:04:42,548 --> 00:04:44,548
say, well, I wanna write some Python code

135
00:04:44,548 --> 00:04:46,769
that does the following and you interact with it.

136
00:04:47,139 --> 00:04:49,178
Um, in, in natural language, and

137
00:04:49,178 --> 00:04:51,259
it actually writes the code for you and

138
00:04:51,259 --> 00:04:52,899
can run experiments for you and the like.

139
00:04:53,759 --> 00:04:55,798
So I, I think really just in the last

140
00:04:55,798 --> 00:04:57,879
year these systems have also become

141
00:04:57,879 --> 00:05:00,230
quite proficient at proving theorems,

142
00:05:00,399 --> 00:05:02,759
OK? So we wrote this paper

143
00:05:02,759 --> 00:05:04,879
describing kind of what the high level

144
00:05:04,879 --> 00:05:07,119
results we thought were like what the theorem statements

145
00:05:07,119 --> 00:05:09,278
would look like, and we didn't know all the details

146
00:05:09,278 --> 00:05:11,519
because you don't know all the details and you act till you actually

147
00:05:11,519 --> 00:05:13,899
prove the theorem. So we didn't know things like what the exact

148
00:05:14,319 --> 00:05:16,428
convergence rate of our proposed algorithm

149
00:05:16,428 --> 00:05:17,778
would be, but we wanted to

150
00:05:18,160 --> 00:05:20,449
basically prove that it would converge to something

151
00:05:20,449 --> 00:05:22,720
good and then, you know, kind of quantify the rate.

152
00:05:23,678 --> 00:05:25,778
And so we did this in sort of a turn taking

153
00:05:25,778 --> 00:05:27,879
way. So we would say like, well, here's the high

154
00:05:27,879 --> 00:05:30,040
level statement of the theorem, here are what we think the

155
00:05:30,040 --> 00:05:31,220
steps of the proof are,

156
00:05:31,519 --> 00:05:33,720
we think you'll need this tool from probability

157
00:05:33,720 --> 00:05:35,798
theory in this step and you know, why don't you try and

158
00:05:35,798 --> 00:05:37,829
then we would interact with it and correct it and go

159
00:05:37,829 --> 00:05:38,540
back and forth.

160
00:05:39,290 --> 00:05:41,329
And then somewhere midway

161
00:05:41,329 --> 00:05:43,410
through this project, my colleague noticed that there was

162
00:05:43,410 --> 00:05:45,579
a little button at the bottom of the interface that

163
00:05:45,579 --> 00:05:46,798
said turbo mode,

164
00:05:47,129 --> 00:05:49,309
and it turns out that in turbo mode you could just say,

165
00:05:49,769 --> 00:05:51,988
here's the theorem I want you to prove. I'm going

166
00:05:51,988 --> 00:05:54,290
away for a few hours, why don't you try to prove it?

167
00:05:54,649 --> 00:05:56,678
And as you may, if you've used these systems,

168
00:05:56,699 --> 00:05:58,970
you know, they talk to themselves and so suddenly

169
00:05:58,970 --> 00:06:00,970
scrolling starts happening in grayed out

170
00:06:00,970 --> 00:06:03,209
text and you see the model talking to

171
00:06:03,209 --> 00:06:05,250
itself and saying, oh, that didn't work. I

172
00:06:05,250 --> 00:06:07,588
see what I did wrong. I should go back and fix this.

173
00:06:08,269 --> 00:06:10,329
And sometimes when we did this, we would

174
00:06:10,329 --> 00:06:12,809
come back and it would still just, it would be churning

175
00:06:12,809 --> 00:06:15,088
away, you know, nonsensically we would be stuck

176
00:06:15,088 --> 00:06:16,678
in some very, very bad state.

177
00:06:17,009 --> 00:06:19,048
Other times we would come back and we'll see

178
00:06:19,048 --> 00:06:21,250
that, you know, actually 45 minutes after

179
00:06:21,250 --> 00:06:23,369
we left, it had done exactly what we asked

180
00:06:23,369 --> 00:06:25,379
it to. And I think we'll see

181
00:06:25,379 --> 00:06:27,579
a lot of this kind of agent

182
00:06:27,579 --> 00:06:29,579
in the near future where you just

183
00:06:29,579 --> 00:06:31,439
specify the high level goal

184
00:06:31,778 --> 00:06:33,778
um maybe with as much detail as

185
00:06:33,778 --> 00:06:35,939
you're able to specify and then you

186
00:06:35,939 --> 00:06:38,048
just let the model run and plan

187
00:06:38,048 --> 00:06:40,100
its own steps and backtrack

188
00:06:40,100 --> 00:06:42,178
and and as needed and

189
00:06:42,189 --> 00:06:43,000
and go forward.

190
00:06:44,399 --> 00:06:46,298
So let me now turn to

191
00:06:46,600 --> 00:06:48,910
what I consider to be some of the

192
00:06:48,910 --> 00:06:51,079
interesting scientific challenges of agentic

193
00:06:51,079 --> 00:06:53,238
AI. And as a scientist, by

194
00:06:53,238 --> 00:06:55,838
the way, I'm, I'm, I'm usually most comfortable

195
00:06:55,838 --> 00:06:57,879
talking about known science, about, you know,

196
00:06:57,959 --> 00:06:58,738
what's out there

197
00:06:59,238 --> 00:07:00,778
on a particular topic.

198
00:07:01,160 --> 00:07:03,199
Um, in this case, I can't do that

199
00:07:03,199 --> 00:07:05,759
because despite the buzz

200
00:07:05,759 --> 00:07:08,119
around agentic AI, not just at reinvent,

201
00:07:08,160 --> 00:07:09,838
but in the scientific community.

202
00:07:10,220 --> 00:07:12,379
There's not a lot of literature

203
00:07:12,379 --> 00:07:14,819
specifically on the topic of agentic

204
00:07:14,819 --> 00:07:15,899
AI, OK?

205
00:07:16,338 --> 00:07:18,338
And um that's obviously going to

206
00:07:18,338 --> 00:07:19,358
change very soon.

207
00:07:20,079 --> 00:07:22,439
But let me start, start off by talking

208
00:07:22,439 --> 00:07:24,738
about the native language of AI embeddings,

209
00:07:24,759 --> 00:07:27,088
uh, of AI agents or embeddings.

210
00:07:27,559 --> 00:07:29,879
So when we interact with a large

211
00:07:29,879 --> 00:07:31,959
language model in in a natural

212
00:07:31,959 --> 00:07:33,178
language like English,

213
00:07:33,480 --> 00:07:35,639
um, the LLMs don't actually

214
00:07:35,639 --> 00:07:37,949
directly work on, on that,

215
00:07:37,959 --> 00:07:39,699
um, English language content.

216
00:07:40,319 --> 00:07:42,439
They actually map anytime you

217
00:07:42,439 --> 00:07:44,559
give an LLM input, whether it's in the form

218
00:07:44,559 --> 00:07:46,778
of an English prompt or an image

219
00:07:46,920 --> 00:07:48,379
or you point it at a

220
00:07:48,660 --> 00:07:50,678
code base or you point it at all

221
00:07:50,678 --> 00:07:52,920
of the documents so far in the legal

222
00:07:52,920 --> 00:07:55,278
case, the first thing it does is

223
00:07:55,278 --> 00:07:57,928
transform that data into

224
00:07:58,519 --> 00:08:00,639
a still high dimensional but much lower

225
00:08:00,639 --> 00:08:02,639
dimensional space, and the primary

226
00:08:02,639 --> 00:08:04,379
purpose of that transformation

227
00:08:04,639 --> 00:08:06,720
is to preserve similarities

228
00:08:06,720 --> 00:08:07,559
between content.

229
00:08:08,129 --> 00:08:10,199
So for instance, in an image embedding

230
00:08:10,199 --> 00:08:13,048
model, 222

231
00:08:13,048 --> 00:08:15,170
photos of different families would be

232
00:08:15,170 --> 00:08:17,329
placed nearer to each other in embedding

233
00:08:17,329 --> 00:08:19,588
space than either one of those family photos

234
00:08:19,809 --> 00:08:21,639
would be to say a landscape.

235
00:08:22,048 --> 00:08:24,170
Two romance novels would be put

236
00:08:24,170 --> 00:08:26,569
closer to each other in a language embedding

237
00:08:26,569 --> 00:08:28,649
space than they either would be to

238
00:08:28,649 --> 00:08:30,959
a car user manual, for instance.

239
00:08:31,459 --> 00:08:33,489
Um, and so on, and, and so

240
00:08:33,489 --> 00:08:35,859
the high level idea of embeddings is to

241
00:08:35,859 --> 00:08:37,200
re-represent data

242
00:08:37,500 --> 00:08:39,700
in a way that captures similarity between

243
00:08:39,700 --> 00:08:41,820
data rather than the raw data itself. And, and

244
00:08:41,820 --> 00:08:42,440
by the way,

245
00:08:42,940 --> 00:08:45,178
if you've ever had the sort of disturbing experience

246
00:08:45,178 --> 00:08:46,239
of an LLM

247
00:08:46,940 --> 00:08:48,979
seeming to completely lose its place

248
00:08:48,979 --> 00:08:50,979
in the dialogue that it's having with you

249
00:08:51,219 --> 00:08:53,298
or suddenly return to some

250
00:08:53,298 --> 00:08:55,418
topic that you were discussing hours or

251
00:08:55,418 --> 00:08:56,200
days ago

252
00:08:56,619 --> 00:08:57,700
completely out of the blue.

253
00:08:58,048 --> 00:09:00,558
It's because actually there's no memorization

254
00:09:00,558 --> 00:09:02,658
of the dialogue so far by the LLM

255
00:09:02,658 --> 00:09:05,099
there is just the embedding of the dialogue

256
00:09:05,099 --> 00:09:07,219
so far, and this leads to these kind of

257
00:09:07,219 --> 00:09:09,279
cognitive ticks that you sometimes see

258
00:09:09,619 --> 00:09:11,700
less as time goes on, but they're, they're

259
00:09:11,700 --> 00:09:12,239
still there.

260
00:09:12,989 --> 00:09:13,580
Um,

261
00:09:13,908 --> 00:09:16,229
and so we should anticipate that

262
00:09:16,229 --> 00:09:18,349
agentic AI, since it's based on

263
00:09:18,349 --> 00:09:20,928
LLMs, will also basically reason

264
00:09:20,928 --> 00:09:23,269
in embedding space rather than in

265
00:09:23,269 --> 00:09:25,369
the, the sort of superficial

266
00:09:25,469 --> 00:09:27,830
input space that we give it, whether it be language

267
00:09:27,830 --> 00:09:30,070
or images or code or what have you.

268
00:09:30,849 --> 00:09:33,609
Um, and so I think there's some interesting implications

269
00:09:33,609 --> 00:09:35,149
to this. I, I don't think we,

270
00:09:35,649 --> 00:09:37,889
you know, one of the things that's hard about AI

271
00:09:37,889 --> 00:09:39,969
and LLMs and deep learning in general

272
00:09:40,210 --> 00:09:42,408
is that these re-representations of the data

273
00:09:42,408 --> 00:09:43,590
are very powerful,

274
00:09:43,899 --> 00:09:46,038
but we still have very poor understanding of,

275
00:09:46,129 --> 00:09:48,494
of the, of what, what's really. Going

276
00:09:48,494 --> 00:09:50,494
on and what they mean. So in particular,

277
00:09:50,774 --> 00:09:52,254
in the same way if we,

278
00:09:52,524 --> 00:09:54,854
you know, opened up your cortex

279
00:09:54,854 --> 00:09:57,173
and pointed a particular neuron in your cortex

280
00:09:57,173 --> 00:09:59,335
for the most part except in very simple cases

281
00:09:59,335 --> 00:10:01,364
like low level visual processing,

282
00:10:01,625 --> 00:10:03,734
we don't actually know what the particular

283
00:10:03,734 --> 00:10:05,354
purpose of that neuron is

284
00:10:05,695 --> 00:10:07,553
in the, in the same way we really don't know

285
00:10:08,053 --> 00:10:10,075
what the meaning of embedding space is in the

286
00:10:10,075 --> 00:10:12,033
in the cartoon here on the right where

287
00:10:12,335 --> 00:10:14,094
um um the.

288
00:10:14,619 --> 00:10:16,658
The idea in this cartoon is that you know

289
00:10:16,658 --> 00:10:18,859
several different users have eaten at different

290
00:10:18,859 --> 00:10:21,009
restaurants and maybe given them different different

291
00:10:21,009 --> 00:10:22,678
ratings and preferences

292
00:10:23,019 --> 00:10:24,080
and so, um,

293
00:10:24,379 --> 00:10:26,658
you know, you have that raw data and then you wanna map

294
00:10:26,658 --> 00:10:27,879
that to embedding space

295
00:10:28,298 --> 00:10:30,298
and in this particular cartoon one thing that's

296
00:10:30,298 --> 00:10:31,460
inaccurate about it

297
00:10:31,739 --> 00:10:33,960
just for the purposes of simplification is that

298
00:10:34,090 --> 00:10:36,418
the axes are labeled by particular

299
00:10:36,418 --> 00:10:38,418
properties we know like what what is the quality

300
00:10:38,418 --> 00:10:40,979
of the food, what is the ambiance of the restaurant, etc.

301
00:10:41,440 --> 00:10:43,479
But embedding space, you know, the label the

302
00:10:43,479 --> 00:10:45,849
axes would basically be entirely unlabeled,

303
00:10:45,928 --> 00:10:48,048
they would have no direct semantics that we

304
00:10:48,048 --> 00:10:48,729
could interpret.

305
00:10:49,548 --> 00:10:51,048
And so one of the, I think,

306
00:10:51,509 --> 00:10:53,609
um, near term implications of this

307
00:10:53,609 --> 00:10:54,889
is that when you think about

308
00:10:55,190 --> 00:10:57,710
agents going out on our behalf and consuming

309
00:10:57,710 --> 00:10:59,710
or finding content and interacting

310
00:10:59,710 --> 00:11:00,690
with other agents,

311
00:11:01,219 --> 00:11:03,668
you know, even though they're quite proficient at conversing

312
00:11:03,668 --> 00:11:04,779
with us in English,

313
00:11:05,070 --> 00:11:07,070
they won't converse with each other in

314
00:11:07,070 --> 00:11:09,389
English, and they won't, and they don't

315
00:11:09,389 --> 00:11:11,548
need English in order to understand

316
00:11:11,548 --> 00:11:13,629
content. And so for instance, I, I

317
00:11:13,629 --> 00:11:15,979
predict that at some point in the not too distant

318
00:11:15,979 --> 00:11:16,529
future.

319
00:11:16,969 --> 00:11:18,548
Every website on the Internet

320
00:11:19,090 --> 00:11:21,149
will, you know, have a little place where

321
00:11:21,149 --> 00:11:23,149
that basically says if you're an agent,

322
00:11:23,288 --> 00:11:25,408
here is the pre-computed embedding of all

323
00:11:25,408 --> 00:11:27,019
of the content on this website

324
00:11:27,489 --> 00:11:29,690
because it makes no sense for every single agent

325
00:11:29,690 --> 00:11:31,969
that goes to that website to have to recomput

326
00:11:31,969 --> 00:11:32,989
the embedding itself,

327
00:11:33,500 --> 00:11:35,529
you know, you may as well just pre-comput it and put it

328
00:11:35,529 --> 00:11:37,690
there. And you know this

329
00:11:37,690 --> 00:11:40,168
can't happen today because these embedding models

330
00:11:40,168 --> 00:11:41,330
are, you know,

331
00:11:41,649 --> 00:11:43,820
proprietary and so they're not

332
00:11:43,820 --> 00:11:46,168
standardized, but I predict that there will be

333
00:11:46,168 --> 00:11:48,570
some amount of standardization that will make this possible

334
00:11:48,570 --> 00:11:50,729
because it's just one of these things that makes practical

335
00:11:50,729 --> 00:11:52,479
sense. Um

336
00:11:54,149 --> 00:11:56,460
OK. Um, and I, I think another,

337
00:11:57,369 --> 00:12:00,320
another area where embeddings

338
00:12:00,320 --> 00:12:00,899
are.

339
00:12:01,788 --> 00:12:04,190
Important and that we don't

340
00:12:04,190 --> 00:12:06,250
understand quite well yet is when it comes

341
00:12:06,250 --> 00:12:08,330
to things like privacy and security.

342
00:12:08,989 --> 00:12:11,308
So if you think about embeddings,

343
00:12:11,349 --> 00:12:13,048
you know, taking the original data

344
00:12:13,469 --> 00:12:15,590
and mapping it to some abstract space

345
00:12:15,590 --> 00:12:17,710
whose primary goal is to preserve

346
00:12:17,710 --> 00:12:18,548
distances,

347
00:12:19,019 --> 00:12:21,029
you know, the data is gonna get warped

348
00:12:21,029 --> 00:12:23,308
through that transformation, and we don't understand

349
00:12:23,308 --> 00:12:25,548
sort of the shape or properties of that warping.

350
00:12:25,830 --> 00:12:27,908
And so in particular, you know, we have all

351
00:12:27,908 --> 00:12:30,029
these divisions in our mind about different types

352
00:12:30,029 --> 00:12:32,029
of data. So for instance, when I think about my

353
00:12:32,029 --> 00:12:34,658
data, I think of my, you know, financial

354
00:12:34,658 --> 00:12:35,649
history as being

355
00:12:35,989 --> 00:12:38,029
generally quite separate from my medical

356
00:12:38,029 --> 00:12:40,190
history. These are different types of data for different types

357
00:12:40,190 --> 00:12:40,969
of functions.

358
00:12:41,529 --> 00:12:43,729
There's no guarantee when my data

359
00:12:43,729 --> 00:12:46,200
gets mapped into embedding spaces that

360
00:12:46,210 --> 00:12:48,279
that sort of clear separation that I have in

361
00:12:48,279 --> 00:12:49,969
my mind will still exist

362
00:12:50,288 --> 00:12:52,609
and so this has privacy implications. So

363
00:12:52,609 --> 00:12:53,548
in particular

364
00:12:53,830 --> 00:12:56,048
when your, you know, agentic executive

365
00:12:56,048 --> 00:12:58,048
system assistant is shopping

366
00:12:58,048 --> 00:12:58,950
around for

367
00:12:59,369 --> 00:13:01,599
plane tickets for you and deciding whether you,

368
00:13:01,729 --> 00:13:03,769
you, you would want to pay for an upgrade to

369
00:13:03,769 --> 00:13:04,548
first class.

370
00:13:05,080 --> 00:13:07,200
You know, if a

371
00:13:07,200 --> 00:13:09,428
human executive assistant was doing that,

372
00:13:09,639 --> 00:13:11,719
they wouldn't take into account, you know, how your

373
00:13:11,719 --> 00:13:14,000
stock portfolio had been performing recently,

374
00:13:14,038 --> 00:13:16,739
even though in principle that is financial

375
00:13:16,739 --> 00:13:18,879
information that might have bearing on your willingness to pay

376
00:13:18,879 --> 00:13:20,038
for a first class ticket.

377
00:13:20,658 --> 00:13:23,058
There's kind of, I, I think without safeguards

378
00:13:23,058 --> 00:13:25,109
put in place, there's no particular guarantee

379
00:13:25,109 --> 00:13:27,279
that your agentic executive assistant

380
00:13:27,658 --> 00:13:29,658
would realize that that same boundary

381
00:13:29,658 --> 00:13:32,090
should be obeyed because of this blurring of

382
00:13:32,099 --> 00:13:34,279
of different data types and the blurring

383
00:13:34,279 --> 00:13:36,340
of boundaries in data space and

384
00:13:36,340 --> 00:13:38,340
obviously this has privacy and security

385
00:13:38,340 --> 00:13:39,879
implications as well

386
00:13:40,298 --> 00:13:42,298
and there's a very, very small

387
00:13:42,298 --> 00:13:44,619
literature. The last time I checked a couple of papers

388
00:13:44,619 --> 00:13:46,099
on the very good question

389
00:13:46,580 --> 00:13:49,298
of if I have some data that's privacy

390
00:13:49,298 --> 00:13:50,178
sensitive.

391
00:13:50,558 --> 00:13:52,639
And I compute an embedding of it into some

392
00:13:52,639 --> 00:13:54,038
lower dimensional space,

393
00:13:54,440 --> 00:13:56,519
how much can you reverse engineer

394
00:13:56,519 --> 00:13:59,119
about the original raw data just from the embedding?

395
00:13:59,399 --> 00:14:01,479
And you know, the reason that there's a paper on this

396
00:14:01,479 --> 00:14:03,058
is that the answer is, you know,

397
00:14:03,440 --> 00:14:05,479
you, you can do some reverse engineering

398
00:14:05,479 --> 00:14:07,099
and so we'll need to start

399
00:14:07,399 --> 00:14:09,869
rethinking, I think, basic tenets

400
00:14:09,869 --> 00:14:12,119
and models around things like privacy,

401
00:14:12,389 --> 00:14:14,538
security, encryption, and the like

402
00:14:14,798 --> 00:14:16,658
in an agentic AI world.

403
00:14:19,548 --> 00:14:21,820
OK. So I've used a couple of

404
00:14:21,820 --> 00:14:23,979
examples when your agent was going out and

405
00:14:23,979 --> 00:14:25,788
performing transactions for you,

406
00:14:26,058 --> 00:14:28,379
and we might also anticipate a future

407
00:14:28,379 --> 00:14:30,700
where your agent is going out and negotiating

408
00:14:30,700 --> 00:14:32,700
things for you, essentially engaged, not

409
00:14:32,700 --> 00:14:35,200
just kind of going out and engaging in transactions.

410
00:14:35,629 --> 00:14:37,830
And you might expect that in the same way for

411
00:14:37,840 --> 00:14:39,038
for many years now

412
00:14:39,308 --> 00:14:41,668
we've had websites that basically try to crawl

413
00:14:41,668 --> 00:14:44,129
and find the best prices for you on flights

414
00:14:44,349 --> 00:14:46,428
that there will be agents that try to do that for

415
00:14:46,428 --> 00:14:48,428
you and so you can imagine your agentic

416
00:14:48,428 --> 00:14:50,509
AI assistant in service

417
00:14:50,509 --> 00:14:52,710
of get making travel arrangements for you

418
00:14:52,940 --> 00:14:55,009
kind of bartering or negotiating

419
00:14:55,820 --> 00:14:57,349
with an a uh another agent.

420
00:14:58,048 --> 00:15:00,090
So, you know, what might that look

421
00:15:00,090 --> 00:15:01,229
like? Well,

422
00:15:01,609 --> 00:15:02,190
um,

423
00:15:02,690 --> 00:15:04,849
there's a, there's an also a small

424
00:15:04,849 --> 00:15:07,269
but pretty rapidly growing literature,

425
00:15:07,529 --> 00:15:08,769
not just in the AI

426
00:15:09,099 --> 00:15:11,408
machine learning community, but in the social sciences

427
00:15:11,408 --> 00:15:12,109
as well.

428
00:15:12,428 --> 00:15:14,639
In which there there are studies that

429
00:15:14,649 --> 00:15:16,729
that try to replicate

430
00:15:17,029 --> 00:15:19,479
or study the analog

431
00:15:19,479 --> 00:15:21,479
of human behavior in agents

432
00:15:21,479 --> 00:15:23,519
and LLMs. OK, and so let me tell

433
00:15:23,519 --> 00:15:25,558
you about the first example of this

434
00:15:25,558 --> 00:15:27,599
that I, I know from about 4

435
00:15:27,599 --> 00:15:29,719
years ago from a colleague and friend of mine.

436
00:15:30,918 --> 00:15:32,960
So in behavioral game theory

437
00:15:32,960 --> 00:15:35,129
there's something known as the ultimatum game.

438
00:15:35,279 --> 00:15:37,389
What is the ultimatum game? An ultimatum game is,

439
00:15:37,710 --> 00:15:39,960
well, first of all, it's a very simple kind of math

440
00:15:39,960 --> 00:15:41,950
problem that I'll describe to you in a second,

441
00:15:42,239 --> 00:15:44,460
but it's also more interestingly

442
00:15:44,719 --> 00:15:47,320
a behavioral experiment that economists

443
00:15:47,320 --> 00:15:49,558
have been running for many, many decades now

444
00:15:49,558 --> 00:15:52,798
in many, many cultures, varying

445
00:15:52,798 --> 00:15:54,918
certain conditions of the setup and so on. But

446
00:15:54,918 --> 00:15:56,580
the basic setup is the following.

447
00:15:57,038 --> 00:15:59,038
You bring two players into a lab, let's call

448
00:15:59,038 --> 00:16:00,279
them Alice and Bob.

449
00:16:00,729 --> 00:16:02,899
And you say, OK, Alice, you get to move first

450
00:16:02,899 --> 00:16:05,058
and you can propose any split

451
00:16:05,058 --> 00:16:07,399
of $10 that you want with Bob,

452
00:16:07,639 --> 00:16:09,969
OK? Then Bob

453
00:16:09,969 --> 00:16:12,349
responds and Bob can either accept

454
00:16:12,349 --> 00:16:14,369
the split that Alice has proposed, in

455
00:16:14,369 --> 00:16:16,408
which case you actually pay the two

456
00:16:16,408 --> 00:16:18,808
participants their respective pieces.

457
00:16:20,119 --> 00:16:21,269
Or Bob can reject

458
00:16:21,808 --> 00:16:23,229
and then both players get nothing,

459
00:16:23,570 --> 00:16:25,379
OK? So,

460
00:16:25,979 --> 00:16:28,500
you know, what does game theory or microeconomics

461
00:16:28,500 --> 00:16:30,658
predict about this? It's very, very simple, you

462
00:16:30,658 --> 00:16:32,859
know, you know, Bob,

463
00:16:33,009 --> 00:16:35,259
Alice and Bob being perfectly

464
00:16:35,259 --> 00:16:37,058
rational beings, right?

465
00:16:37,408 --> 00:16:39,690
Alice, you know, should reason as follows.

466
00:16:39,899 --> 00:16:41,899
Since Bob is perfectly rational,

467
00:16:42,219 --> 00:16:44,739
I should offer the smallest non-zero

468
00:16:44,739 --> 00:16:47,058
amounts to Bob, say a penny, OK?

469
00:16:47,599 --> 00:16:50,158
And so Alice would propose 9.99

470
00:16:50,158 --> 00:16:50,840
and a penny.

471
00:16:51,239 --> 00:16:52,500
Bob, being rational,

472
00:16:52,759 --> 00:16:54,859
you know, would rather have a penny than not have a

473
00:16:54,859 --> 00:16:56,859
penny, and so Bob accepts. OK.

474
00:16:57,399 --> 00:16:59,139
This isn't that interesting.

475
00:16:59,558 --> 00:17:01,719
What is interesting is that when you run this

476
00:17:01,719 --> 00:17:04,000
game in the lab, nothing even remotely

477
00:17:04,000 --> 00:17:06,199
close to this happens, but what does

478
00:17:06,199 --> 00:17:08,309
happen is extremely consistent

479
00:17:08,309 --> 00:17:10,348
across many conditions and cultures.

480
00:17:10,680 --> 00:17:11,779
What happens is

481
00:17:12,279 --> 00:17:14,598
almost all of the time, let's say close to 90%

482
00:17:14,598 --> 00:17:16,500
of the time, Alice will offer.

483
00:17:16,949 --> 00:17:19,640
To Bob something between $5.03

484
00:17:19,640 --> 00:17:20,318
dollars,

485
00:17:21,219 --> 00:17:23,380
very rarely more than $5

486
00:17:23,469 --> 00:17:25,598
and so you know there's, there's sort of a priming for

487
00:17:25,598 --> 00:17:27,598
both players already that Alice being in a

488
00:17:27,598 --> 00:17:29,660
position of power by moving first

489
00:17:30,920 --> 00:17:33,118
might, might deserve more of the, the, the pie

490
00:17:33,118 --> 00:17:34,199
than than half.

491
00:17:35,338 --> 00:17:37,630
Conditioned on Alice's proposal

492
00:17:37,630 --> 00:17:39,750
to Bob being within that 3 to $5

493
00:17:39,750 --> 00:17:40,259
range,

494
00:17:40,588 --> 00:17:42,670
Bob almost always accepts the

495
00:17:42,670 --> 00:17:44,410
acceptance rate is extremely high,

496
00:17:45,029 --> 00:17:47,229
and in the 10 10% or

497
00:17:47,229 --> 00:17:49,150
so cases where Alice

498
00:17:49,469 --> 00:17:51,630
gets more aggressive and offers

499
00:17:51,630 --> 00:17:53,709
Bob something less than $3

500
00:17:54,029 --> 00:17:56,390
the rejection rate of Bob skyrockets

501
00:17:56,390 --> 00:17:57,729
to close to 100%.

502
00:17:58,338 --> 00:18:00,420
So you can go look this

503
00:18:00,420 --> 00:18:02,500
up. It's been performed. This experiment

504
00:18:02,500 --> 00:18:04,699
has been run hundreds, possibly thousands of

505
00:18:04,699 --> 00:18:05,318
times.

506
00:18:05,858 --> 00:18:07,699
And one of the things that's interesting about it,

507
00:18:08,019 --> 00:18:10,098
is that first of all, it deviates from the

508
00:18:10,098 --> 00:18:12,140
mathematical prediction by a lot.

509
00:18:12,630 --> 00:18:14,709
The other thing is that it's very, very

510
00:18:14,709 --> 00:18:15,368
consistent

511
00:18:16,189 --> 00:18:18,390
across different subjects and cultures and

512
00:18:18,390 --> 00:18:21,130
so it's as if we all neurologically

513
00:18:21,130 --> 00:18:23,108
have a hardwired sense,

514
00:18:23,400 --> 00:18:25,750
uh, a shared hardwired sense of what's

515
00:18:25,750 --> 00:18:27,009
fair in this game,

516
00:18:27,459 --> 00:18:29,549
even though probably most of you have never heard

517
00:18:29,549 --> 00:18:31,449
of the ultimatum game before right now.

518
00:18:32,130 --> 00:18:34,209
So my colleague basically went out and

519
00:18:34,209 --> 00:18:35,910
replicated this experiment

520
00:18:36,529 --> 00:18:38,769
using LLMs instead of Alice

521
00:18:38,769 --> 00:18:39,549
and Bob.

522
00:18:40,049 --> 00:18:42,170
So he would say to one LLM and he would give them

523
00:18:42,170 --> 00:18:44,118
personas also, right? He would say, you know,

524
00:18:44,449 --> 00:18:46,509
to, to sort of match the variability

525
00:18:46,509 --> 00:18:48,759
of human subjects that you would get in a laboratory

526
00:18:48,759 --> 00:18:49,410
experiment.

527
00:18:49,809 --> 00:18:52,000
So he would give a prompt to one LM and

528
00:18:52,000 --> 00:18:52,529
say, you know.

529
00:18:52,900 --> 00:18:53,568
Uh, you know,

530
00:18:53,939 --> 00:18:56,019
your name is Melissa. You're a 37

531
00:18:56,019 --> 00:18:58,449
year old medical technician living in Lexington,

532
00:18:58,578 --> 00:19:00,660
Kentucky. You get to propose any

533
00:19:00,660 --> 00:19:02,939
split, and then he would, you know, give a, a prompt

534
00:19:02,939 --> 00:19:03,789
to the other LLM,

535
00:19:04,259 --> 00:19:06,098
and you know, guess what happens?

536
00:19:06,500 --> 00:19:09,459
The LLMs actually statistically

537
00:19:09,459 --> 00:19:11,699
almost perfectly match the behavior of

538
00:19:11,699 --> 00:19:13,088
human subjects that I mentioned.

539
00:19:14,118 --> 00:19:16,660
And so now this is becoming quite an area

540
00:19:16,660 --> 00:19:18,689
within not just AI and machine learning but

541
00:19:18,689 --> 00:19:20,848
the social sciences more generally and

542
00:19:20,848 --> 00:19:22,969
so people are starting to study what happens when

543
00:19:22,969 --> 00:19:25,088
agents negotiate with each other when they

544
00:19:25,088 --> 00:19:27,400
bargain with each other and

545
00:19:27,400 --> 00:19:29,529
interesting phenomena are, are starting

546
00:19:29,529 --> 00:19:30,559
to arise. In general,

547
00:19:30,880 --> 00:19:33,410
the tracking of human behavior is quite good

548
00:19:33,689 --> 00:19:35,729
and, and also sort of tracking of

549
00:19:35,729 --> 00:19:37,848
human foibles as well. So for

550
00:19:37,848 --> 00:19:38,479
instance, it's

551
00:19:38,868 --> 00:19:40,890
there was a recent paper about a year ago that

552
00:19:40,890 --> 00:19:43,098
showed. That LLMs

553
00:19:43,098 --> 00:19:45,259
engaged in kind of a price negotiation

554
00:19:45,259 --> 00:19:47,848
game essentially arrived at outcomes

555
00:19:47,848 --> 00:19:49,979
that would be considered collusion and

556
00:19:49,979 --> 00:19:52,479
price fixing from a regulatory standpoint

557
00:19:52,979 --> 00:19:55,338
and, and you know this is both amusing

558
00:19:55,338 --> 00:19:57,180
but I think it's also worth thinking about.

559
00:19:57,588 --> 00:19:59,598
That when we think about agentic

560
00:19:59,598 --> 00:20:00,189
AI,

561
00:20:00,469 --> 00:20:02,670
we don't just need to think about what happens with

562
00:20:02,670 --> 00:20:04,670
individual agents in, you know, in

563
00:20:04,670 --> 00:20:07,068
the same way that we think in game theory or finance

564
00:20:07,068 --> 00:20:09,618
about collective behavior or emergent phenomena.

565
00:20:09,900 --> 00:20:12,098
We need to think about that with agentic

566
00:20:12,098 --> 00:20:13,039
AI as well.

567
00:20:13,309 --> 00:20:15,449
And so, you know, you might wonder.

568
00:20:16,035 --> 00:20:18,233
You know, if we soon arrive at a world

569
00:20:18,233 --> 00:20:20,814
where a lot of activity, including financial

570
00:20:21,114 --> 00:20:23,094
and strategic activity, is

571
00:20:23,473 --> 00:20:24,894
mediated by agents,

572
00:20:25,275 --> 00:20:27,055
you know, whether we might,

573
00:20:27,674 --> 00:20:29,693
how will we think about things like hurting behavior

574
00:20:29,693 --> 00:20:31,814
in stock markets or things like

575
00:20:31,814 --> 00:20:33,973
system, the systemic risk that led

576
00:20:33,973 --> 00:20:34,555
to

577
00:20:35,555 --> 00:20:37,555
the 2008 financial crash, you

578
00:20:37,555 --> 00:20:39,875
know, so these are examples of kind of cases

579
00:20:39,875 --> 00:20:42,289
where. The behavior of any

580
00:20:42,289 --> 00:20:44,660
individual agent might seem perfectly

581
00:20:44,660 --> 00:20:46,729
natural or rational, but you

582
00:20:46,729 --> 00:20:48,989
arrive at some collective outcome that is,

583
00:20:49,019 --> 00:20:50,469
is quite undesirable.

584
00:20:52,348 --> 00:20:54,390
So the, the, the last topic I

585
00:20:54,390 --> 00:20:56,430
wanna mention is maybe the one that's

586
00:20:56,430 --> 00:20:58,509
fuzziest and that on which I really

587
00:20:58,509 --> 00:20:59,368
don't know any

588
00:20:59,750 --> 00:21:00,608
kind of current

589
00:21:01,029 --> 00:21:02,430
kind of science around,

590
00:21:02,789 --> 00:21:05,259
which is the challenge of what I would call subjective

591
00:21:05,259 --> 00:21:06,108
common sense.

592
00:21:06,430 --> 00:21:08,670
And so let me first talk about common sense

593
00:21:08,670 --> 00:21:10,410
rather than subjective common sense.

594
00:21:11,160 --> 00:21:11,709
So

595
00:21:12,439 --> 00:21:14,439
you know, I, I will use the fact

596
00:21:14,439 --> 00:21:16,670
that I've been around forever in this field to,

597
00:21:17,118 --> 00:21:18,118
to tell you that

598
00:21:18,759 --> 00:21:21,078
for the longest time, the, the,

599
00:21:21,199 --> 00:21:21,789
the

600
00:21:22,180 --> 00:21:24,979
AI and machine learning were kind of confounded

601
00:21:25,279 --> 00:21:27,439
by the problem of imbuing models with

602
00:21:27,439 --> 00:21:29,259
common everyday common sense.

603
00:21:29,559 --> 00:21:31,000
And so by common sense I mean

604
00:21:31,279 --> 00:21:33,479
facts that we would all agree on even

605
00:21:33,479 --> 00:21:35,509
if we'd never thought about them before. So

606
00:21:35,509 --> 00:21:37,559
for instance, you know, in the series of pictures.

607
00:21:38,019 --> 00:21:40,140
You know, if I, if I show you a

608
00:21:40,140 --> 00:21:42,219
glass of water sitting on a surface on a

609
00:21:42,219 --> 00:21:43,140
table, let's say,

610
00:21:43,459 --> 00:21:45,459
and then I shift it in one direction,

611
00:21:45,578 --> 00:21:46,818
say I shift it to the right,

612
00:21:47,180 --> 00:21:49,039
you know, we would all still agree

613
00:21:49,549 --> 00:21:51,618
that it's still a glass of water on

614
00:21:51,618 --> 00:21:53,858
that surface, right? The fact that, you know, that

615
00:21:53,858 --> 00:21:56,019
I've moved it somewhere on the table doesn't

616
00:21:56,019 --> 00:21:58,019
change the fact that it's a glass of water

617
00:21:58,019 --> 00:21:59,019
sitting on the table.

618
00:21:59,338 --> 00:22:01,500
And we would also all know that, you know, if I tip

619
00:22:01,500 --> 00:22:02,439
it on its side,

620
00:22:03,019 --> 00:22:05,059
it's still a glass on a table, but it's no

621
00:22:05,059 --> 00:22:07,400
longer a glass of water anymore, OK.

622
00:22:07,890 --> 00:22:09,578
And so for the longest time in AI,

623
00:22:09,900 --> 00:22:12,140
you know, I think dating back to the

624
00:22:12,140 --> 00:22:13,279
beginning of the field,

625
00:22:14,059 --> 00:22:16,299
the, the people struggled with the

626
00:22:16,299 --> 00:22:18,380
question of how to imbue

627
00:22:18,380 --> 00:22:20,449
models and AI systems,

628
00:22:20,818 --> 00:22:23,199
you know, not just with the ability to perform

629
00:22:23,699 --> 00:22:25,818
extremely targeted tasks very well like

630
00:22:25,818 --> 00:22:27,900
face recognition, but to just have this sort

631
00:22:27,900 --> 00:22:30,219
of ability to do common sense everyday

632
00:22:30,219 --> 00:22:32,078
reasoning that any human being could do.

633
00:22:32,828 --> 00:22:34,890
And somehow, you know, in the last

634
00:22:34,979 --> 00:22:37,430
15 years or so that problem

635
00:22:37,430 --> 00:22:39,439
has largely been solved. I mean, I,

636
00:22:39,549 --> 00:22:41,828
I have a colleague at at Duke who

637
00:22:41,828 --> 00:22:42,910
spends, as far as I can tell,

638
00:22:43,189 --> 00:22:45,549
spends all of his time posting on social

639
00:22:45,549 --> 00:22:46,088
media

640
00:22:46,539 --> 00:22:48,410
examples of LLM's

641
00:22:48,670 --> 00:22:50,789
kind of failing basic common sense things,

642
00:22:50,828 --> 00:22:52,848
but for the most part this has gotten much better.

643
00:22:53,229 --> 00:22:55,318
And I think this is just due to the fact that you

644
00:22:55,318 --> 00:22:57,719
know if a if a model sees

645
00:22:57,719 --> 00:22:59,828
a fact or you know images of

646
00:22:59,828 --> 00:23:01,949
glasses on tables sitting upright

647
00:23:01,949 --> 00:23:04,118
and tipped on their side enough, it'll learn

648
00:23:04,118 --> 00:23:06,318
this. By subjective common

649
00:23:06,318 --> 00:23:08,098
sense, I mean kind of

650
00:23:08,799 --> 00:23:11,118
heuristics that you have around your own

651
00:23:11,118 --> 00:23:13,660
behavior that are perfectly sensible,

652
00:23:13,838 --> 00:23:16,078
that you have good reasons for, but

653
00:23:16,078 --> 00:23:18,118
are personal to you, are not shared

654
00:23:18,118 --> 00:23:20,180
like our knowledge about glasses of water

655
00:23:20,180 --> 00:23:21,078
on tables.

656
00:23:21,430 --> 00:23:23,588
And so as an example, I will just

657
00:23:23,588 --> 00:23:25,259
use my own behavior around,

658
00:23:26,000 --> 00:23:28,078
you know, whether I leave doors.

659
00:23:28,180 --> 00:23:30,289
Open or closed or locked or unlocked,

660
00:23:30,420 --> 00:23:32,500
OK, and I invite you to think about your own

661
00:23:32,500 --> 00:23:33,449
behaviors in this regard,

662
00:23:33,779 --> 00:23:35,880
you know, it's actually quite complicated,

663
00:23:36,140 --> 00:23:38,199
right? So in my office,

664
00:23:38,449 --> 00:23:40,630
um, you know, when I leave for the day,

665
00:23:40,739 --> 00:23:42,979
I, I lock my office behind me when

666
00:23:42,979 --> 00:23:44,660
I'm there during the day, you know,

667
00:23:45,529 --> 00:23:47,660
sometimes I leave the door open and I'm in my

668
00:23:47,660 --> 00:23:49,739
office. Sometimes I'm in my office and

669
00:23:49,739 --> 00:23:51,759
I close the door, but I don't lock it. Because I'm

670
00:23:51,759 --> 00:23:53,818
taking a meeting or I wanna be able to focus.

671
00:23:54,160 --> 00:23:56,539
And sometimes when I'm just gonna be nearby

672
00:23:56,539 --> 00:23:58,828
on the floor or at a whiteboard out in the hall,

673
00:23:59,000 --> 00:24:01,000
I will leave my door open and

674
00:24:01,000 --> 00:24:01,779
unlocked

675
00:24:02,039 --> 00:24:04,140
to kind of signal that I'm around

676
00:24:04,140 --> 00:24:06,410
but just not in there at the moment, OK?

677
00:24:06,759 --> 00:24:09,000
And, and you, you, you probably all

678
00:24:09,000 --> 00:24:11,000
have similar heuristics and they

679
00:24:11,000 --> 00:24:13,338
will probably all differ slightly

680
00:24:13,559 --> 00:24:14,519
than, than mine.

681
00:24:14,979 --> 00:24:17,150
And so I, I think that kind of

682
00:24:17,150 --> 00:24:19,209
imbuing agentic AI with

683
00:24:19,209 --> 00:24:21,608
this sort of personalized subjective

684
00:24:21,608 --> 00:24:22,459
common sense

685
00:24:22,729 --> 00:24:24,729
is gonna be a real challenge because it can't

686
00:24:24,729 --> 00:24:26,848
be unlike things like

687
00:24:26,848 --> 00:24:28,848
facts about glasses of water on tables,

688
00:24:29,250 --> 00:24:31,489
it can't be learned from data. There's not

689
00:24:31,489 --> 00:24:33,608
mass, there's, there's not a massive

690
00:24:33,608 --> 00:24:35,868
trove of data out there

691
00:24:36,049 --> 00:24:38,328
on my particular habits regarding,

692
00:24:38,449 --> 00:24:39,459
you know, um,

693
00:24:39,890 --> 00:24:42,009
uh, doors and locking, OK.

694
00:24:42,259 --> 00:24:44,380
And whether lest this example

695
00:24:44,380 --> 00:24:46,650
sort of seems too far removed or abstract,

696
00:24:46,699 --> 00:24:48,489
I invite you to sort of just swap

697
00:24:48,979 --> 00:24:51,059
doors and locking with accounts

698
00:24:51,059 --> 00:24:52,380
and passwords, right?

699
00:24:52,699 --> 00:24:55,059
So perhaps many of you choose

700
00:24:55,059 --> 00:24:55,640
to share,

701
00:24:56,219 --> 00:24:58,338
you know, the passwords to your Spotify

702
00:24:58,338 --> 00:25:00,618
or Netflix accounts with other family

703
00:25:00,618 --> 00:25:02,250
members or people that you're close to,

704
00:25:02,578 --> 00:25:04,660
but you wouldn't do the same thing with your bank

705
00:25:04,660 --> 00:25:05,979
account probably, OK?

706
00:25:06,420 --> 00:25:08,529
And so how agentic AI

707
00:25:08,529 --> 00:25:10,549
is gonna somehow, I, I think we're kind of

708
00:25:10,549 --> 00:25:12,789
returning to the 70s, right? where nobody

709
00:25:12,789 --> 00:25:14,930
had any idea how we were ever going

710
00:25:14,930 --> 00:25:16,949
to get general common

711
00:25:16,949 --> 00:25:19,108
sense into models and then that turned

712
00:25:19,108 --> 00:25:21,150
machine learning on just sufficiently

713
00:25:21,150 --> 00:25:23,328
massive troves of data turned out to be

714
00:25:23,509 --> 00:25:24,709
a good solution to that.

715
00:25:24,989 --> 00:25:26,989
I don't see that solution working

716
00:25:26,989 --> 00:25:29,568
anytime soon for, you know, your

717
00:25:29,568 --> 00:25:30,689
agentic AI.

718
00:25:31,910 --> 00:25:33,959
OK, and so I think I'm gonna turn

719
00:25:33,959 --> 00:25:36,420
it over to Peter now to come talk about

720
00:25:36,420 --> 00:25:39,059
more practical topics and, and sort of the evolution

721
00:25:39,598 --> 00:25:41,670
of agentic AI in, in particular the

722
00:25:41,670 --> 00:25:43,000
ways in which it's different

723
00:25:43,400 --> 00:25:45,439
than um just LLM based

724
00:25:45,439 --> 00:25:46,279
degenerative AI.

725
00:25:48,500 --> 00:25:52,549
Uh-huh. Um,

726
00:25:52,949 --> 00:25:54,868
so I'm. I'm not on.

727
00:25:55,328 --> 00:25:57,209
Do I need to now you are all right,

728
00:25:57,489 --> 00:25:58,959
but I am too. OK,

729
00:25:59,549 --> 00:26:01,250
well, we can, we can talk together then.

730
00:26:01,539 --> 00:26:03,750
All right, so I'm Peter Helen and I'm the director

731
00:26:03,750 --> 00:26:05,809
of responsible AI for AWS

732
00:26:05,809 --> 00:26:07,809
and I lead a team of scientists and

733
00:26:07,809 --> 00:26:08,809
engineers and

734
00:26:09,130 --> 00:26:11,170
various folks who are interested in

735
00:26:11,170 --> 00:26:13,410
tackling the open science problems

736
00:26:13,410 --> 00:26:15,568
and then translating solutions into

737
00:26:15,568 --> 00:26:16,699
best practices,

738
00:26:17,049 --> 00:26:19,130
uh, for our internal teams

739
00:26:19,130 --> 00:26:20,709
and for customers,

740
00:26:21,088 --> 00:26:23,269
um. So let's

741
00:26:23,269 --> 00:26:25,469
talk a little bit about, uh,

742
00:26:25,739 --> 00:26:27,858
so the, the science challenges I, I hope

743
00:26:27,858 --> 00:26:29,979
are becoming clearer, right? Let's talk about

744
00:26:29,979 --> 00:26:31,358
some of the other challenges

745
00:26:31,618 --> 00:26:32,479
that we face.

746
00:26:33,239 --> 00:26:33,959
Um,

747
00:26:34,959 --> 00:26:37,078
All right, so, so where are we

748
00:26:37,078 --> 00:26:39,338
going in the nearer term with agents?

749
00:26:39,729 --> 00:26:42,140
Let's take a look, um, there's lots of,

750
00:26:42,199 --> 00:26:44,239
uh, effective agents

751
00:26:44,239 --> 00:26:46,358
today or things that are that are

752
00:26:46,358 --> 00:26:48,608
called agents that are effective, right? Here's

753
00:26:48,608 --> 00:26:51,500
a few from, from Amazon, Quiro,

754
00:26:51,959 --> 00:26:52,618
um.

755
00:26:53,189 --> 00:26:55,348
What do we have here? Uh, I can see down here,

756
00:26:55,380 --> 00:26:57,880
OK, uh, the SageMaker model customization

757
00:26:57,880 --> 00:27:00,259
agent, SageMaker data agent, there's

758
00:27:00,259 --> 00:27:03,068
one we just wrote a blog about which is a compliance

759
00:27:03,068 --> 00:27:05,420
screening agent so across our businesses

760
00:27:05,420 --> 00:27:07,459
we have to be very careful about, you know,

761
00:27:07,578 --> 00:27:08,828
the transactions.

762
00:27:09,098 --> 00:27:11,098
So now we have an internal agent that takes care

763
00:27:11,098 --> 00:27:11,729
of that.

764
00:27:12,098 --> 00:27:14,170
But what, what is sort of the,

765
00:27:14,239 --> 00:27:16,500
the primary characteristic of these

766
00:27:16,500 --> 00:27:17,539
agents, right?

767
00:27:17,900 --> 00:27:19,979
Is that they have very close human

768
00:27:19,979 --> 00:27:20,818
interaction.

769
00:27:21,779 --> 00:27:24,189
OK, and you have single vendor

770
00:27:24,189 --> 00:27:25,049
accountability.

771
00:27:25,828 --> 00:27:28,039
So the world that we were just talking

772
00:27:28,039 --> 00:27:30,459
about where we have, you know, lots of critters

773
00:27:30,459 --> 00:27:32,578
running around all doing useful work for

774
00:27:32,578 --> 00:27:34,838
us is

775
00:27:34,838 --> 00:27:36,959
a bit different than this world here. Now this world here

776
00:27:36,959 --> 00:27:39,459
is yielding some value, quite a bit of value,

777
00:27:39,779 --> 00:27:40,338
um,

778
00:27:40,618 --> 00:27:42,848
but we have a ways to go, all right?

779
00:27:43,338 --> 00:27:43,959
So

780
00:27:44,818 --> 00:27:47,160
how might things evolve,

781
00:27:47,420 --> 00:27:49,539
uh, in the over the next year or

782
00:27:49,539 --> 00:27:51,699
two? The first thing is

783
00:27:51,699 --> 00:27:53,779
obviously sort of more task types,

784
00:27:53,818 --> 00:27:56,479
right? So consider an example

785
00:27:56,479 --> 00:27:58,699
where you want to,

786
00:27:58,719 --> 00:28:00,759
you know, use your miles to

787
00:28:00,759 --> 00:28:02,858
get business class tickets,

788
00:28:03,160 --> 00:28:05,199
you know, to Paris, say if you're living in

789
00:28:05,199 --> 00:28:07,078
the US for December, right?

790
00:28:07,449 --> 00:28:08,400
You can do it,

791
00:28:08,759 --> 00:28:10,993
but the airlines release. Those tickets

792
00:28:10,993 --> 00:28:12,993
sort of slowly over time according to

793
00:28:12,993 --> 00:28:14,993
their own schedules. This is a

794
00:28:14,993 --> 00:28:17,723
task where you have to sort of continuously

795
00:28:17,723 --> 00:28:18,973
go back and forth.

796
00:28:19,324 --> 00:28:21,375
Um, it's a bit of drudgery

797
00:28:21,375 --> 00:28:23,515
and something you can forget to do and then you

798
00:28:23,515 --> 00:28:25,594
go oh darn because you missed it, what have you.

799
00:28:26,410 --> 00:28:28,469
So this is the kind of thing where

800
00:28:28,880 --> 00:28:31,209
you know the agent should be released

801
00:28:31,209 --> 00:28:31,949
to do

802
00:28:32,250 --> 00:28:34,430
something over a period of

803
00:28:34,430 --> 00:28:36,890
time. It's not a, it's not a fast

804
00:28:36,890 --> 00:28:38,920
response. Another one might be, hey,

805
00:28:39,009 --> 00:28:41,269
look, there's interesting social media

806
00:28:41,269 --> 00:28:43,729
conversations, but you want the comments to accumulate

807
00:28:43,729 --> 00:28:45,880
in some way before you spend your time.

808
00:28:46,130 --> 00:28:48,390
You don't want to go back to it repeatedly.

809
00:28:48,838 --> 00:28:50,838
So this is, this is a trend I

810
00:28:50,838 --> 00:28:52,868
would call sort of fast to slow.

811
00:28:53,039 --> 00:28:55,039
You could call it short term to long term if

812
00:28:55,039 --> 00:28:55,799
you want to,

813
00:28:56,118 --> 00:28:58,439
but it's, uh, it's, it's a different

814
00:28:58,439 --> 00:28:59,969
type of, of task.

815
00:29:00,400 --> 00:29:02,479
Here's a second trend, and by the way, the

816
00:29:02,479 --> 00:29:04,959
three trends I'm gonna talk about right here are correlated,

817
00:29:05,000 --> 00:29:06,180
they're not independent,

818
00:29:06,559 --> 00:29:08,598
um. But the second type

819
00:29:08,598 --> 00:29:09,299
is

820
00:29:09,640 --> 00:29:11,818
that not every task

821
00:29:11,920 --> 00:29:14,239
needs to be explicitly specified

822
00:29:14,239 --> 00:29:15,189
by the user,

823
00:29:15,568 --> 00:29:18,039
so you could trigger a log analysis

824
00:29:18,039 --> 00:29:18,660
from a ticket,

825
00:29:19,039 --> 00:29:21,219
right? We have an agent, DevOps agent,

826
00:29:21,239 --> 00:29:23,039
that's, uh, doing things like that,

827
00:29:23,400 --> 00:29:25,759
or you know, you have an agent

828
00:29:25,759 --> 00:29:28,078
that listening to traffic reports and

829
00:29:28,078 --> 00:29:30,259
in this sort of ideal personal assistant,

830
00:29:30,890 --> 00:29:33,000
uh, view, you know, it, it

831
00:29:33,000 --> 00:29:35,118
updates your restaurant reservation because it knows you're

832
00:29:35,118 --> 00:29:36,439
not gonna get there on time.

833
00:29:36,890 --> 00:29:39,400
Um, so these are inferred tasks,

834
00:29:39,500 --> 00:29:41,000
right? And there's sort of an implicit,

835
00:29:41,259 --> 00:29:43,618
uh, assumption here that they're

836
00:29:43,618 --> 00:29:45,618
accessing information and

837
00:29:45,618 --> 00:29:47,939
tracking it so that you don't have to,

838
00:29:48,539 --> 00:29:50,578
um, and so this I would think of as sort

839
00:29:50,578 --> 00:29:52,880
of reactive to proactive,

840
00:29:53,618 --> 00:29:54,318
OK,

841
00:29:54,699 --> 00:29:56,608
um, a third one,

842
00:29:57,618 --> 00:29:59,939
is more connectivity to third party

843
00:29:59,939 --> 00:30:00,880
workflows.

844
00:30:01,410 --> 00:30:03,430
All right, so here we have a nice little

845
00:30:03,430 --> 00:30:05,529
remote, uh, restaurant

846
00:30:05,529 --> 00:30:07,068
off on a lovely beach.

847
00:30:07,368 --> 00:30:09,108
Um, they do have a telephone.

848
00:30:09,680 --> 00:30:12,088
Uh, a landline old school, right,

849
00:30:12,328 --> 00:30:14,410
and then you have your, your state

850
00:30:14,410 --> 00:30:16,759
of the art travel agent that's helping

851
00:30:16,759 --> 00:30:18,848
you out and you ring them up

852
00:30:18,969 --> 00:30:21,009
and then you know someone else has their

853
00:30:21,009 --> 00:30:23,118
state of the art travel agent and then a third

854
00:30:23,118 --> 00:30:25,160
person has and then a whole bunch of

855
00:30:25,160 --> 00:30:27,239
other people have and you've kind of DDoS

856
00:30:27,239 --> 00:30:27,989
the poor

857
00:30:28,529 --> 00:30:30,568
restaurant, right? They just can't cope with

858
00:30:30,568 --> 00:30:31,108
this.

859
00:30:31,394 --> 00:30:32,705
Um, and so

860
00:30:33,076 --> 00:30:35,286
you know this connectivity to third party

861
00:30:35,286 --> 00:30:37,566
workflows is an implicit

862
00:30:37,566 --> 00:30:39,224
part of all the

863
00:30:39,526 --> 00:30:42,195
sort of the magic and the excitement of agents

864
00:30:42,195 --> 00:30:44,244
right when we talk about all the, all

865
00:30:44,244 --> 00:30:46,566
the drudgery that we could eliminate, but

866
00:30:46,566 --> 00:30:49,144
we need to understand how it impacts,

867
00:30:49,326 --> 00:30:51,326
you know, those other systems that we're

868
00:30:51,326 --> 00:30:52,026
talking to.

869
00:30:52,326 --> 00:30:54,481
So here you know. What happens to the restaurant

870
00:30:54,481 --> 00:30:56,362
if Travel Agent one cancels,

871
00:30:56,760 --> 00:30:58,801
you know, it really impacts their

872
00:30:58,801 --> 00:31:00,821
business. They don't have that many tables,

873
00:31:01,602 --> 00:31:03,801
and so you begin to wonder if that

874
00:31:03,801 --> 00:31:05,842
offline restaurant really has

875
00:31:05,842 --> 00:31:08,801
to go get its own matrad agent

876
00:31:09,040 --> 00:31:11,082
to or go out of business or how

877
00:31:11,082 --> 00:31:13,192
it handles this, but it has a business

878
00:31:13,192 --> 00:31:14,740
decision that it needs to confront, right?

879
00:31:15,259 --> 00:31:17,559
And this applies to online workflows

880
00:31:17,559 --> 00:31:19,719
going back to airlines for a minute. You know some

881
00:31:19,719 --> 00:31:21,920
airlines have a policy where you buy the ticket

882
00:31:21,920 --> 00:31:24,140
and then you can change your mind within 24 hours.

883
00:31:24,709 --> 00:31:26,719
Well, that policy is crafted

884
00:31:26,719 --> 00:31:29,059
to deliver, you know, the, the flyer

885
00:31:29,680 --> 00:31:31,989
benefit. It's a, it's a nice policy,

886
00:31:32,229 --> 00:31:34,598
but it's very carefully, you know, driven

887
00:31:34,598 --> 00:31:37,309
by data and what's the likelihood of refusing,

888
00:31:37,318 --> 00:31:39,400
and now they're holding a seat, you know, who do they

889
00:31:39,400 --> 00:31:41,400
risk not selling it, etc. etc.

890
00:31:41,759 --> 00:31:43,838
All of a sudden you put agents in it, the math

891
00:31:43,838 --> 00:31:44,400
changes.

892
00:31:44,719 --> 00:31:46,759
It assuredly changes and

893
00:31:46,759 --> 00:31:49,039
so you know how do these policies, so

894
00:31:49,039 --> 00:31:51,199
we talk about agents, but then we have to also

895
00:31:51,199 --> 00:31:53,199
talk about all the things whether or not

896
00:31:53,199 --> 00:31:55,390
they are agents that the agents are interacting

897
00:31:55,390 --> 00:31:57,318
with and how that change happens, right?

898
00:31:57,858 --> 00:31:59,318
Um, and so,

899
00:31:59,588 --> 00:32:00,108
you know,

900
00:32:00,400 --> 00:32:02,828
from today, you know, we're, we're really

901
00:32:02,828 --> 00:32:04,989
sort of nailing it in a lot of domains

902
00:32:04,989 --> 00:32:07,049
on, on well understood workflows

903
00:32:07,309 --> 00:32:09,670
to the future where we have these sophisticated

904
00:32:09,670 --> 00:32:10,289
goals.

905
00:32:10,709 --> 00:32:11,269
Um,

906
00:32:11,588 --> 00:32:13,949
we see sort of this general trend,

907
00:32:13,989 --> 00:32:16,358
right, and on the X axis. Axis

908
00:32:16,358 --> 00:32:18,719
you have kind of the complexity of the

909
00:32:18,719 --> 00:32:21,430
task from simple to

910
00:32:21,439 --> 00:32:23,598
complex or the complexity

911
00:32:23,598 --> 00:32:25,598
of the user request on

912
00:32:25,598 --> 00:32:27,838
the vertical axis you

913
00:32:27,838 --> 00:32:30,358
have kind of the robustness,

914
00:32:30,439 --> 00:32:32,479
the resiliency, the strength

915
00:32:32,479 --> 00:32:34,019
of the infrastructure.

916
00:32:34,390 --> 00:32:36,689
That you support the agents with

917
00:32:36,910 --> 00:32:38,459
so if you think about that,

918
00:32:38,750 --> 00:32:41,568
you know, the, the, the fastest slow,

919
00:32:41,739 --> 00:32:44,469
well that thing had to be up and running continuously

920
00:32:44,469 --> 00:32:46,068
for a long period of time.

921
00:32:46,588 --> 00:32:47,239
OK,

922
00:32:47,630 --> 00:32:49,828
it's not quite the same as just, you

923
00:32:49,828 --> 00:32:51,910
know, interacting with an LLM if it goes

924
00:32:51,910 --> 00:32:53,989
down the next day that maybe that's fine or what have

925
00:32:53,989 --> 00:32:56,390
you, but like this really takes infrastructure

926
00:32:56,390 --> 00:32:57,209
to support,

927
00:32:57,750 --> 00:32:59,890
um. So anyway,

928
00:32:59,969 --> 00:33:01,969
that's kind of the general trend that

929
00:33:01,969 --> 00:33:03,150
we see happening

930
00:33:03,608 --> 00:33:04,328
now.

931
00:33:05,049 --> 00:33:07,588
How do we realize the benefits while minimizing

932
00:33:07,588 --> 00:33:08,368
the risks,

933
00:33:08,689 --> 00:33:10,189
uh, as this rolls out?

934
00:33:10,519 --> 00:33:11,088
OK,

935
00:33:11,608 --> 00:33:13,209
so a couple of things.

936
00:33:13,529 --> 00:33:15,529
The first observation I wanna

937
00:33:15,529 --> 00:33:16,049
make

938
00:33:16,410 --> 00:33:17,348
is that

939
00:33:17,930 --> 00:33:19,809
when Jen and I hit

940
00:33:20,170 --> 00:33:22,229
the world by storm a few years ago,

941
00:33:22,338 --> 00:33:22,868
um.

942
00:33:24,660 --> 00:33:27,019
It really broadened

943
00:33:27,019 --> 00:33:29,380
the number of folks who are engaging

944
00:33:29,380 --> 00:33:30,219
with AI,

945
00:33:30,900 --> 00:33:32,519
um, and.

946
00:33:33,729 --> 00:33:34,250
You know,

947
00:33:34,650 --> 00:33:36,689
turned it from a sort of a very small

948
00:33:36,689 --> 00:33:38,709
community to a very large community

949
00:33:39,009 --> 00:33:41,209
with, uh, sort of a,

950
00:33:41,608 --> 00:33:44,009
shall I say a lack of understanding

951
00:33:44,009 --> 00:33:45,469
of the difference between

952
00:33:45,969 --> 00:33:48,209
statistical inference, right, and

953
00:33:48,209 --> 00:33:49,430
rule-based logic

954
00:33:49,809 --> 00:33:50,750
and so there's a major mind.

955
00:33:51,265 --> 00:33:53,644
Shift which the industry as a whole

956
00:33:53,805 --> 00:33:56,084
is still in the process of making

957
00:33:56,463 --> 00:33:58,703
right where people realize

958
00:33:58,703 --> 00:34:00,785
that AI is not

959
00:34:00,785 --> 00:34:02,864
just smart software,

960
00:34:03,104 --> 00:34:05,295
it is actually a different kind of beast,

961
00:34:05,384 --> 00:34:07,703
all right, and to highlight some of the, the

962
00:34:07,703 --> 00:34:08,724
changes here,

963
00:34:09,344 --> 00:34:11,543
OK, obviously traditional

964
00:34:11,543 --> 00:34:13,704
software rule-based logic, if

965
00:34:13,704 --> 00:34:16,023
you have a statistical model based on statistical

966
00:34:16,023 --> 00:34:18,103
inference, we'll get to a deeper

967
00:34:18,103 --> 00:34:19,635
implication of that in a second.

968
00:34:20,030 --> 00:34:22,360
You know your specking changes, right?

969
00:34:22,750 --> 00:34:24,789
With the traditional model you can write it down

970
00:34:24,789 --> 00:34:25,610
in English.

971
00:34:25,907 --> 00:34:28,028
You know, with AI you're using

972
00:34:28,028 --> 00:34:30,467
data sets to so to Michael's earlier

973
00:34:30,467 --> 00:34:32,789
point, you know, about where are the data

974
00:34:32,789 --> 00:34:34,239
sets that encode,

975
00:34:34,509 --> 00:34:36,548
you know, all the full range of

976
00:34:36,548 --> 00:34:38,548
your personal preferences and even

977
00:34:38,548 --> 00:34:40,938
do you want to have such data sets,

978
00:34:41,228 --> 00:34:43,387
you know, lying around? I mean that's also a

979
00:34:43,387 --> 00:34:45,548
question, right? But, but the data sets

980
00:34:45,548 --> 00:34:46,289
are key here.

981
00:34:46,778 --> 00:34:48,829
A third point, right, is that with

982
00:34:48,829 --> 00:34:50,208
traditional software

983
00:34:50,668 --> 00:34:52,849
you expect it to work out of the box.

984
00:34:53,509 --> 00:34:54,887
The developer

985
00:34:55,217 --> 00:34:57,467
is responsible for the quality

986
00:34:57,467 --> 00:34:58,458
of that product,

987
00:34:58,829 --> 00:35:01,068
but with the statistical model it's a diff

988
00:35:01,068 --> 00:35:04,447
it's different. The developer anticipates

989
00:35:04,748 --> 00:35:07,108
the kinds of input distributions

990
00:35:07,108 --> 00:35:09,568
that they'll get, but they can't know for sure

991
00:35:09,748 --> 00:35:12,208
their privacy constraints. Like we don't look

992
00:35:12,387 --> 00:35:14,289
at the prompts that come into,

993
00:35:14,867 --> 00:35:17,208
uh, you know. Our bedrock models,

994
00:35:17,619 --> 00:35:19,668
right? And so what you

995
00:35:19,668 --> 00:35:21,708
what you see as a result of this is

996
00:35:21,708 --> 00:35:23,708
that deployers and

997
00:35:23,708 --> 00:35:25,949
in some cases end users

998
00:35:25,949 --> 00:35:28,070
have to also test because they're

999
00:35:28,070 --> 00:35:30,208
the ones that understand

1000
00:35:30,409 --> 00:35:31,769
their particular data,

1001
00:35:32,228 --> 00:35:33,869
right? So a third point,

1002
00:35:34,228 --> 00:35:35,449
sorry, a 4th point

1003
00:35:35,978 --> 00:35:37,260
is, OK,

1004
00:35:37,550 --> 00:35:38,110
again

1005
00:35:38,389 --> 00:35:40,389
you release a piece of software, version N

1006
00:35:40,389 --> 00:35:42,429
+ 1, you expect it to work better than

1007
00:35:42,429 --> 00:35:44,969
version N on each and every input.

1008
00:35:46,110 --> 00:35:46,760
OK,

1009
00:35:47,110 --> 00:35:49,438
but for statistical software,

1010
00:35:49,628 --> 00:35:51,869
you're, uh, a statistical model,

1011
00:35:52,030 --> 00:35:54,148
you're going to expect it

1012
00:35:54,429 --> 00:35:56,510
to work on average

1013
00:35:56,510 --> 00:35:58,750
better than, and that's a

1014
00:35:58,750 --> 00:35:59,688
big difference.

1015
00:36:00,739 --> 00:36:02,978
And it might not be on average, it could be your metric

1016
00:36:02,978 --> 00:36:03,728
of choice,

1017
00:36:04,099 --> 00:36:06,418
but you know you can have, you can

1018
00:36:06,418 --> 00:36:08,579
release an improved model, and some

1019
00:36:08,579 --> 00:36:10,199
users of it may see

1020
00:36:10,750 --> 00:36:12,099
you worse performance.

1021
00:36:13,228 --> 00:36:13,929
Um,

1022
00:36:14,228 --> 00:36:16,429
and then a final point I want to make about this

1023
00:36:16,429 --> 00:36:17,550
kind of mind shift

1024
00:36:17,829 --> 00:36:19,000
is that,

1025
00:36:19,389 --> 00:36:21,708
you know, with rule-based software,

1026
00:36:21,780 --> 00:36:24,250
traditional software, there is a single

1027
00:36:25,019 --> 00:36:27,269
explanation for how

1028
00:36:27,269 --> 00:36:29,550
you got a particular output from a particular

1029
00:36:29,550 --> 00:36:31,550
input. The code can be implemented

1030
00:36:31,550 --> 00:36:32,889
in different ways, yes,

1031
00:36:33,148 --> 00:36:35,188
but the chain of logic is the same,

1032
00:36:35,510 --> 00:36:37,668
whereas on this, in the statistical side,

1033
00:36:37,699 --> 00:36:39,708
it's different. In fact, you release

1034
00:36:39,708 --> 00:36:40,389
a model,

1035
00:36:40,750 --> 00:36:42,889
OK. And that model

1036
00:36:42,889 --> 00:36:44,889
does have a logic inside

1037
00:36:44,889 --> 00:36:47,168
it, but there's also tens

1038
00:36:47,168 --> 00:36:49,289
of thousands or millions of

1039
00:36:49,289 --> 00:36:51,530
models that had the exact same

1040
00:36:51,530 --> 00:36:53,750
performance that you did not release

1041
00:36:53,969 --> 00:36:56,050
that had other chains of logic

1042
00:36:56,050 --> 00:36:58,128
inside them, maybe a little bit different, maybe

1043
00:36:58,128 --> 00:36:58,889
very different.

1044
00:36:59,769 --> 00:37:02,168
Um, so the question of explainability

1045
00:37:02,168 --> 00:37:03,409
becomes more complicated.

1046
00:37:03,969 --> 00:37:06,128
So, a couple of, of

1047
00:37:06,128 --> 00:37:07,599
implications of this.

1048
00:37:07,929 --> 00:37:09,969
First of all, on the, on

1049
00:37:09,969 --> 00:37:10,789
the sort of,

1050
00:37:11,409 --> 00:37:13,510
uh, on the for the AI side,

1051
00:37:14,030 --> 00:37:16,260
um. There

1052
00:37:16,260 --> 00:37:18,340
are two stacks that you

1053
00:37:18,340 --> 00:37:19,510
have to worry about,

1054
00:37:20,099 --> 00:37:22,500
uh, and so if we think about an AI

1055
00:37:22,500 --> 00:37:23,159
system

1056
00:37:23,458 --> 00:37:25,519
and it's you're sort of improving

1057
00:37:25,519 --> 00:37:27,539
it version by version on the

1058
00:37:27,539 --> 00:37:29,659
horizontal axis and then you're looking at

1059
00:37:29,659 --> 00:37:30,878
its performance

1060
00:37:31,659 --> 00:37:33,699
along a particular for a particular metric

1061
00:37:33,699 --> 00:37:34,978
on the vertical axis,

1062
00:37:35,349 --> 00:37:37,418
right, and you, you test it on some data

1063
00:37:37,418 --> 00:37:39,619
set, maybe you get this kind of performance,

1064
00:37:39,860 --> 00:37:41,739
but you test it on a different data set.

1065
00:37:42,119 --> 00:37:44,918
You get a different performance trajectory,

1066
00:37:45,510 --> 00:37:48,159
right? You test it on a 3rd data set and

1067
00:37:48,159 --> 00:37:50,280
maybe it's altogether, you know, it's

1068
00:37:50,280 --> 00:37:51,099
going down.

1069
00:37:51,659 --> 00:37:53,760
Right, so performance is a function

1070
00:37:53,760 --> 00:37:56,539
of the system and the, the evaluation

1071
00:37:56,539 --> 00:37:58,739
data set. It is not a function

1072
00:37:58,739 --> 00:38:00,188
of the system by itself,

1073
00:38:00,500 --> 00:38:02,539
and I know if some of you have heard us talk

1074
00:38:02,539 --> 00:38:03,500
before about this, right,

1075
00:38:03,769 --> 00:38:06,099
we're pounding, we're pounding, you know, this point

1076
00:38:06,099 --> 00:38:08,179
again and again, but we still need to

1077
00:38:08,179 --> 00:38:10,219
get this clear because the result of this,

1078
00:38:10,340 --> 00:38:12,340
right, is that you have, if

1079
00:38:12,340 --> 00:38:14,119
you're, if you're a developer

1080
00:38:14,500 --> 00:38:16,579
of an AI solution, a gen.

1081
00:38:16,853 --> 00:38:19,253
Take, you know, uh, generative

1082
00:38:19,253 --> 00:38:21,434
traditional, right, you've got your system

1083
00:38:21,434 --> 00:38:23,954
stack and then you've got your evaluation stack

1084
00:38:24,235 --> 00:38:26,833
and your evaluation stack as the developer

1085
00:38:26,833 --> 00:38:29,224
represents what you anticipate

1086
00:38:29,224 --> 00:38:30,905
downstream folks to use,

1087
00:38:31,235 --> 00:38:33,813
but then you have downstream folks

1088
00:38:34,074 --> 00:38:35,534
who are going to

1089
00:38:36,474 --> 00:38:38,715
have to have their own stack with

1090
00:38:38,715 --> 00:38:40,715
their data to check what's going

1091
00:38:40,715 --> 00:38:41,715
on, right.

1092
00:38:42,179 --> 00:38:44,378
And as a whole, we are, we are

1093
00:38:44,378 --> 00:38:46,378
moving in this direction as an industry,

1094
00:38:46,458 --> 00:38:48,898
but this is not a well established

1095
00:38:48,898 --> 00:38:50,659
design pattern at this point in time.

1096
00:38:51,800 --> 00:38:52,340
All right.

1097
00:38:52,679 --> 00:38:55,039
So, what's another implication?

1098
00:38:55,389 --> 00:38:57,478
And here we get into what is our sort

1099
00:38:57,478 --> 00:38:59,800
of traditional responsible AI slide,

1100
00:38:59,878 --> 00:39:02,599
right? The fact is you, you know, statistical

1101
00:39:02,599 --> 00:39:03,199
models,

1102
00:39:03,478 --> 00:39:05,898
you know, have a variety of technical

1103
00:39:05,898 --> 00:39:06,599
properties.

1104
00:39:06,869 --> 00:39:09,030
It's not just overall performance.

1105
00:39:09,320 --> 00:39:11,878
It includes issues around AI privacy,

1106
00:39:11,958 --> 00:39:14,260
AI security, safety, fairness,

1107
00:39:14,349 --> 00:39:16,519
veracity, etc. etc. There's a bunch

1108
00:39:16,519 --> 00:39:17,378
of these things.

1109
00:39:17,878 --> 00:39:20,070
And the models released have

1110
00:39:20,070 --> 00:39:21,510
these properties,

1111
00:39:21,989 --> 00:39:24,070
whether or not you as a

1112
00:39:24,070 --> 00:39:25,978
developer made a decision about the properties.

1113
00:39:28,550 --> 00:39:30,469
OK, and that's a very important point.

1114
00:39:30,789 --> 00:39:32,168
So if we go back here

1115
00:39:32,668 --> 00:39:34,909
to the stack downstream, they can measure

1116
00:39:34,909 --> 00:39:36,949
for properties that you may not have made

1117
00:39:36,949 --> 00:39:39,039
an explicit design decision about.

1118
00:39:39,389 --> 00:39:39,909
All right.

1119
00:39:40,188 --> 00:39:42,708
And so when we think of responsible AI,

1120
00:39:42,989 --> 00:39:45,389
what we're really thinking about is

1121
00:39:45,389 --> 00:39:48,269
getting folks to make explicit

1122
00:39:48,269 --> 00:39:50,989
decisions about balancing the, you know, the,

1123
00:39:51,110 --> 00:39:53,349
the, the, the desired benefits and

1124
00:39:53,349 --> 00:39:54,969
the potential risks,

1125
00:39:55,389 --> 00:39:57,070
uh, of a service, right.

1126
00:39:57,500 --> 00:39:59,539
Um, that's the goal. You want to make

1127
00:39:59,539 --> 00:40:01,208
these decisions explicitly,

1128
00:40:01,550 --> 00:40:02,659
not implicitly.

1129
00:40:04,059 --> 00:40:04,639
OK.

1130
00:40:05,019 --> 00:40:07,300
And I think the last point, uh, is

1131
00:40:07,300 --> 00:40:09,570
probably very self-evident, but,

1132
00:40:09,780 --> 00:40:12,199
you know, from a product management point of view,

1133
00:40:12,539 --> 00:40:13,099
um.

1134
00:40:13,849 --> 00:40:16,208
You know life got just got a little more complicated.

1135
00:40:16,449 --> 00:40:18,769
You had all the stuff that you had to worry about

1136
00:40:18,769 --> 00:40:20,849
before with traditional software and now

1137
00:40:20,849 --> 00:40:22,369
you have these other properties.

1138
00:40:22,728 --> 00:40:25,289
OK, now it doesn't necessarily mean you have to optimize

1139
00:40:25,289 --> 00:40:27,489
every one of these properties. It's perfectly fine

1140
00:40:27,489 --> 00:40:29,489
to make a decision that, you know, this

1141
00:40:29,489 --> 00:40:31,489
one we'll deal with later or what have you,

1142
00:40:31,809 --> 00:40:32,389
um,

1143
00:40:33,019 --> 00:40:34,969
uh, as long as you disclose that,

1144
00:40:35,409 --> 00:40:37,648
right? Um, but it is

1145
00:40:37,648 --> 00:40:38,188
still

1146
00:40:38,478 --> 00:40:40,969
additional, these are still additional considerations.

1147
00:40:41,489 --> 00:40:43,489
OK, so that was mind

1148
00:40:43,489 --> 00:40:45,800
shift one and we haven't yet

1149
00:40:45,800 --> 00:40:47,590
completed that mind shift as an industry.

1150
00:40:48,039 --> 00:40:49,800
Now come agents because why wait?

1151
00:40:50,090 --> 00:40:51,570
Life is exciting, OK?

1152
00:40:51,889 --> 00:40:53,469
Um, what is the second

1153
00:40:53,929 --> 00:40:56,090
mind shift that's provoked by agents?

1154
00:40:56,570 --> 00:40:57,909
Well, there is one,

1155
00:40:58,929 --> 00:41:00,938
and let's go back to Alice and

1156
00:41:00,938 --> 00:41:02,469
Bob, our canonical,

1157
00:41:03,010 --> 00:41:05,010
you know, science friends as we

1158
00:41:05,010 --> 00:41:06,750
work through some of these issues, right?

1159
00:41:07,489 --> 00:41:09,579
There are things that Agent Alice

1160
00:41:10,050 --> 00:41:12,699
uh would like to know about Agent Bob

1161
00:41:13,179 --> 00:41:15,260
before Alice interacts with Bob,

1162
00:41:15,300 --> 00:41:15,849
OK?

1163
00:41:16,139 --> 00:41:17,228
Who are you,

1164
00:41:17,699 --> 00:41:19,378
right? Who owns you?

1165
00:41:20,559 --> 00:41:21,929
How can you help me,

1166
00:41:22,369 --> 00:41:24,789
right? And the reason for this is because

1167
00:41:25,090 --> 00:41:27,289
Alice is there to do actual work

1168
00:41:27,610 --> 00:41:29,610
and there are decisions that she

1169
00:41:29,610 --> 00:41:31,688
needs to make, right? How much

1170
00:41:31,688 --> 00:41:32,949
information do I share with you?

1171
00:41:34,139 --> 00:41:36,260
Right, that, that does matter. How do I

1172
00:41:36,260 --> 00:41:37,039
negotiate

1173
00:41:37,539 --> 00:41:39,829
a fair resource exchange with you?

1174
00:41:40,179 --> 00:41:42,219
OK, so for example, if you're

1175
00:41:42,219 --> 00:41:44,500
gonna have a zero trust posture,

1176
00:41:44,860 --> 00:41:46,978
right, are, are you gonna just

1177
00:41:46,978 --> 00:41:49,219
like, is it like walking up to a

1178
00:41:49,219 --> 00:41:51,219
counter at 7-Eleven and just buying

1179
00:41:51,219 --> 00:41:52,978
it and walking off? Maybe not,

1180
00:41:53,418 --> 00:41:55,418
uh, maybe not at all, um.

1181
00:41:57,780 --> 00:42:00,119
How do you enforce the terms of the exchange?

1182
00:42:01,418 --> 00:42:02,438
How much

1183
00:42:02,860 --> 00:42:04,320
time or

1184
00:42:04,619 --> 00:42:06,978
cost or information

1185
00:42:07,378 --> 00:42:09,510
does the agent Alice

1186
00:42:10,179 --> 00:42:12,260
want to invest in

1187
00:42:12,260 --> 00:42:14,458
this particular transaction, right?

1188
00:42:14,539 --> 00:42:15,800
What happens if it gets

1189
00:42:16,179 --> 00:42:18,378
detailed and like, like they're sort of going

1190
00:42:18,378 --> 00:42:20,398
back and forth on a lot of things

1191
00:42:20,659 --> 00:42:23,099
and the agent budget is sitting

1192
00:42:23,099 --> 00:42:25,619
there ticking down, right? Like the transaction

1193
00:42:25,619 --> 00:42:26,559
will cost money.

1194
00:42:26,978 --> 00:42:28,619
At the end of the day, right,

1195
00:42:28,978 --> 00:42:30,978
so trust becomes rather

1196
00:42:30,978 --> 00:42:31,918
critical here

1197
00:42:32,378 --> 00:42:32,918
now,

1198
00:42:33,570 --> 00:42:36,019
you know, this isn't the only situation in which

1199
00:42:36,019 --> 00:42:37,239
issues of trust come up,

1200
00:42:37,539 --> 00:42:39,699
and there's, you know, a variety of

1201
00:42:39,699 --> 00:42:41,938
mechanisms to address these. We have

1202
00:42:41,938 --> 00:42:44,300
norms. An example of a norm, for example,

1203
00:42:44,458 --> 00:42:46,820
like with the Internet is a robots.text

1204
00:42:46,820 --> 00:42:49,969
file. OK, there's not a law around robots.text.

1205
00:42:50,260 --> 00:42:52,159
It's a norm and it has

1206
00:42:52,458 --> 00:42:54,769
some issues. But it, it has, has worked

1207
00:42:54,769 --> 00:42:55,728
relatively well.

1208
00:42:56,090 --> 00:42:58,228
There's also standards, you know,

1209
00:42:58,530 --> 00:43:00,789
um, MCP is an emerging standard,

1210
00:43:01,090 --> 00:43:01,599
uh,

1211
00:43:01,969 --> 00:43:04,148
TCP IP in the early days

1212
00:43:04,148 --> 00:43:06,349
of the Internet, I think some people

1213
00:43:06,728 --> 00:43:08,728
maybe regret how long

1214
00:43:08,728 --> 00:43:10,110
TCPIP persisted,

1215
00:43:10,369 --> 00:43:12,550
right, because it has the, the doubling thing,

1216
00:43:12,929 --> 00:43:16,050
um, and then there can be existing AI regulations,

1217
00:43:16,489 --> 00:43:17,429
um, but.

1218
00:43:17,878 --> 00:43:20,519
This, the, the mindset

1219
00:43:20,519 --> 00:43:21,300
shift

1220
00:43:21,559 --> 00:43:23,639
that I'm talking about here is

1221
00:43:23,639 --> 00:43:24,168
that

1222
00:43:24,840 --> 00:43:27,199
you can't, these are not

1223
00:43:27,199 --> 00:43:27,898
necessarily.

1224
00:43:29,570 --> 00:43:31,938
Core technical problems,

1225
00:43:32,250 --> 00:43:34,289
right? These are problems

1226
00:43:34,289 --> 00:43:36,289
about privacy, about

1227
00:43:36,289 --> 00:43:38,329
the exchange of information, about what do

1228
00:43:38,329 --> 00:43:38,969
I do.

1229
00:43:39,239 --> 00:43:41,289
There can be a variety, there can be research

1230
00:43:41,289 --> 00:43:43,289
that supports the optimization of

1231
00:43:43,289 --> 00:43:44,989
these decision making processes,

1232
00:43:45,409 --> 00:43:47,610
but you know people themselves make these

1233
00:43:47,610 --> 00:43:49,760
kinds of decisions all the time.

1234
00:43:49,769 --> 00:43:52,188
They may not be perfectly optimal, um,

1235
00:43:52,369 --> 00:43:53,909
but they are functional,

1236
00:43:54,250 --> 00:43:55,389
right? We have a civilization.

1237
00:43:56,050 --> 00:43:58,090
So this is the

1238
00:43:58,090 --> 00:44:00,250
kind of thing that we have to work on, but

1239
00:44:00,250 --> 00:44:02,539
we're not going to succeed on it

1240
00:44:02,800 --> 00:44:04,909
if we, you know, tackle everything

1241
00:44:04,909 --> 00:44:06,969
with a zero trust model and hunker

1242
00:44:06,969 --> 00:44:09,610
down. You'll never get that vision of interactions

1243
00:44:09,610 --> 00:44:10,849
that are productive, right?

1244
00:44:11,489 --> 00:44:14,250
So what does this mean in practice?

1245
00:44:14,648 --> 00:44:16,648
So as we think about, you know, sort of

1246
00:44:16,648 --> 00:44:18,929
a very simple agent ecosystem

1247
00:44:18,929 --> 00:44:20,929
here, we've got 4 agents and 2

1248
00:44:20,929 --> 00:44:21,570
humans,

1249
00:44:21,909 --> 00:44:24,010
right? The first thing that you're going to need is

1250
00:44:24,010 --> 00:44:26,250
the agent infrastructure that we talked

1251
00:44:26,250 --> 00:44:28,360
about from the earlier diagram, right?

1252
00:44:28,449 --> 00:44:30,128
You need real support.

1253
00:44:30,905 --> 00:44:33,144
For a variety of either

1254
00:44:33,144 --> 00:44:35,563
short running or long running

1255
00:44:36,583 --> 00:44:37,695
interactions,

1256
00:44:37,985 --> 00:44:40,224
um, this is the kind of thing that that agent

1257
00:44:40,224 --> 00:44:42,503
cop is is shooting towards, but

1258
00:44:42,503 --> 00:44:44,563
you also need infrastructure

1259
00:44:44,824 --> 00:44:47,135
to help people steward

1260
00:44:47,135 --> 00:44:49,083
this emerging ecosystem,

1261
00:44:49,465 --> 00:44:51,503
and I use the word stewardship

1262
00:44:51,503 --> 00:44:53,164
instead of governance because

1263
00:44:53,744 --> 00:44:54,445
you know

1264
00:44:54,744 --> 00:44:56,744
this is, this is not this is not

1265
00:44:56,744 --> 00:44:57,804
a pure.

1266
00:44:58,148 --> 00:45:00,219
Uh, regulatory,

1267
00:45:00,360 --> 00:45:02,659
uh, you know, challenge, um,

1268
00:45:02,750 --> 00:45:05,228
it's not a challenge that has

1269
00:45:05,228 --> 00:45:05,978
one person who's,

1270
00:45:06,469 --> 00:45:08,260
uh, accountable for this,

1271
00:45:08,550 --> 00:45:10,849
you know, it's sort of a community working

1272
00:45:10,849 --> 00:45:12,438
together to, to have it happen.

1273
00:45:12,750 --> 00:45:14,789
So let me give you one specific example

1274
00:45:14,789 --> 00:45:16,250
of what I mean. So today.

1275
00:45:16,739 --> 00:45:18,739
You know, if you look, go to an Amazon

1276
00:45:18,739 --> 00:45:19,809
Nova model, right,

1277
00:45:20,228 --> 00:45:22,688
the, the images are watermarked,

1278
00:45:22,860 --> 00:45:24,978
and so there's a service out there and you

1279
00:45:24,978 --> 00:45:27,780
can submit the image and

1280
00:45:27,780 --> 00:45:30,070
it'll decide whether or not that

1281
00:45:30,139 --> 00:45:32,219
that image actually has the Nova

1282
00:45:32,219 --> 00:45:33,300
watermark, OK.

1283
00:45:33,860 --> 00:45:35,320
Now what happens

1284
00:45:35,610 --> 00:45:37,750
um and so we have that sort of piece

1285
00:45:37,750 --> 00:45:38,800
of infrastructure,

1286
00:45:39,228 --> 00:45:41,409
you know, colored in in blue up

1287
00:45:41,409 --> 00:45:42,688
there on the screen, right?

1288
00:45:42,978 --> 00:45:45,458
So what happens if every agent

1289
00:45:45,458 --> 00:45:47,539
starts checking, you know,

1290
00:45:47,978 --> 00:45:50,280
for the provenance of any image

1291
00:45:50,280 --> 00:45:52,340
that it accepts either from a human

1292
00:45:52,340 --> 00:45:54,780
or another thing because it needs to know, right? Say

1293
00:45:54,780 --> 00:45:56,860
for, for, for IP reasons

1294
00:45:56,860 --> 00:45:58,159
where that image came from.

1295
00:45:58,478 --> 00:46:00,458
Um, all right, well,

1296
00:46:00,898 --> 00:46:03,398
right now that infrastructure for the Watermark

1297
00:46:03,398 --> 00:46:05,639
checker is designed for

1298
00:46:05,639 --> 00:46:07,840
pretty much, you know, it has an API and it's

1299
00:46:07,840 --> 00:46:09,918
humans, but it's not designed for

1300
00:46:09,918 --> 00:46:10,539
billions

1301
00:46:11,199 --> 00:46:13,280
of agents to all of a sudden

1302
00:46:13,280 --> 00:46:15,250
start talking to it. It's going to go down.

1303
00:46:15,599 --> 00:46:17,875
So how. Is that gonna be supported,

1304
00:46:18,114 --> 00:46:18,753
developed,

1305
00:46:19,074 --> 00:46:21,215
funded, right? You can imagine other

1306
00:46:21,215 --> 00:46:23,534
kinds of checking infrastructure where

1307
00:46:23,864 --> 00:46:25,894
it's, it's engaged in

1308
00:46:25,894 --> 00:46:28,155
each transaction that involves

1309
00:46:28,155 --> 00:46:30,914
novel content or maybe even, you know, non-novel

1310
00:46:30,914 --> 00:46:33,313
content and there can be other pieces of infrastructure

1311
00:46:33,313 --> 00:46:34,014
as well,

1312
00:46:34,594 --> 00:46:36,954
um, that that may turn out to be relevant,

1313
00:46:37,074 --> 00:46:39,793
but all this has yet to be invented,

1314
00:46:40,304 --> 00:46:40,833
OK.

1315
00:46:41,820 --> 00:46:43,128
So, um.

1316
00:46:44,409 --> 00:46:46,489
Anyway, so what do we do today? Let's

1317
00:46:46,489 --> 00:46:48,188
get really down to brass tacks,

1318
00:46:48,688 --> 00:46:50,688
um, because we're still working through

1319
00:46:50,688 --> 00:46:53,050
Mindshift one and because agents

1320
00:46:53,050 --> 00:46:55,489
at their core are based on foundation

1321
00:46:55,489 --> 00:46:56,128
models,

1322
00:46:56,449 --> 00:46:57,119
OK,

1323
00:46:57,449 --> 00:46:57,969
um.

1324
00:46:59,849 --> 00:47:02,199
You know, a lot of the work

1325
00:47:02,199 --> 00:47:04,869
of building an individual agent

1326
00:47:05,128 --> 00:47:07,489
is similar to the work of building

1327
00:47:07,489 --> 00:47:09,128
any other AI solution.

1328
00:47:09,610 --> 00:47:11,728
All right, so we have at

1329
00:47:11,728 --> 00:47:12,829
this reinvent

1330
00:47:13,188 --> 00:47:14,510
released a

1331
00:47:14,849 --> 00:47:17,369
um what I would call sort of a framework

1332
00:47:17,369 --> 00:47:19,070
for best practices

1333
00:47:19,409 --> 00:47:19,949
to help

1334
00:47:20,489 --> 00:47:22,188
non-ML experts

1335
00:47:22,449 --> 00:47:24,780
uh develop AI

1336
00:47:24,918 --> 00:47:26,929
applications and it's available

1337
00:47:26,929 --> 00:47:29,030
online via our well architected

1338
00:47:29,329 --> 00:47:29,949
tool

1339
00:47:30,289 --> 00:47:32,449
so anyone can go to that uh

1340
00:47:32,449 --> 00:47:33,188
QR code

1341
00:47:33,849 --> 00:47:35,228
URL and see it,

1342
00:47:35,489 --> 00:47:37,389
but the notion is that.

1343
00:47:38,978 --> 00:47:41,219
We want to provide

1344
00:47:41,219 --> 00:47:43,378
people with a set of

1345
00:47:43,378 --> 00:47:43,918
questions.

1346
00:47:44,659 --> 00:47:46,769
To help them think about

1347
00:47:47,228 --> 00:47:49,349
the decisions that they need to

1348
00:47:49,349 --> 00:47:51,349
make and then for each question

1349
00:47:51,590 --> 00:47:53,398
we offer some best practices,

1350
00:47:53,708 --> 00:47:55,570
right? That will, that will sort of

1351
00:47:55,949 --> 00:47:57,949
uh help answer those

1352
00:47:57,949 --> 00:48:00,179
questions. So

1353
00:48:00,179 --> 00:48:00,878
let's,

1354
00:48:01,300 --> 00:48:03,340
let's go through a couple of

1355
00:48:03,340 --> 00:48:05,469
examples on the agent side.

1356
00:48:05,579 --> 00:48:07,699
So one of these, and, and I should say that

1357
00:48:07,699 --> 00:48:10,099
the, the design of the framework

1358
00:48:10,099 --> 00:48:11,070
is such that

1359
00:48:11,340 --> 00:48:13,530
you start with a narrowly defined use

1360
00:48:13,530 --> 00:48:14,039
case,

1361
00:48:14,579 --> 00:48:17,519
right? Because if you want to actually

1362
00:48:17,820 --> 00:48:19,719
um evaluate,

1363
00:48:20,019 --> 00:48:21,478
uh, and you know,

1364
00:48:21,898 --> 00:48:23,978
make a, a data-driven

1365
00:48:23,978 --> 00:48:26,059
decision about how well a

1366
00:48:26,059 --> 00:48:27,219
particular system.

1367
00:48:27,559 --> 00:48:30,079
Is serving your use case, the narrower

1368
00:48:30,079 --> 00:48:31,148
the use case,

1369
00:48:31,438 --> 00:48:33,418
the more effective your analysis.

1370
00:48:34,449 --> 00:48:34,969
All right,

1371
00:48:35,329 --> 00:48:36,128
so you know,

1372
00:48:36,409 --> 00:48:39,289
for example, building a face recognition,

1373
00:48:39,579 --> 00:48:41,610
uh, service that's not

1374
00:48:41,610 --> 00:48:42,708
a use case

1375
00:48:43,090 --> 00:48:45,168
that doesn't necessarily help you

1376
00:48:45,168 --> 00:48:47,289
tune how well that system is

1377
00:48:47,289 --> 00:48:48,050
gonna work

1378
00:48:48,360 --> 00:48:50,610
building a system or or specifying

1379
00:48:50,610 --> 00:48:52,648
a use case where you're recognizing the

1380
00:48:52,648 --> 00:48:54,688
face of a missing child or

1381
00:48:54,688 --> 00:48:56,708
you're trying to look up, uh, say

1382
00:48:57,128 --> 00:48:59,148
an actor in a library

1383
00:48:59,148 --> 00:49:01,019
of video, um,

1384
00:49:01,329 --> 00:49:03,364
those are more specific use. Cases you

1385
00:49:03,364 --> 00:49:05,364
can tune precision recall, you

1386
00:49:05,364 --> 00:49:07,284
can adapt your data sets, etc.

1387
00:49:07,833 --> 00:49:09,885
right so you want to go narrow and then you

1388
00:49:09,885 --> 00:49:12,224
want to assess the benefits and risks

1389
00:49:12,523 --> 00:49:13,224
of

1390
00:49:13,864 --> 00:49:15,864
of that particular use case

1391
00:49:16,043 --> 00:49:18,485
with as little dependency on the specific

1392
00:49:18,485 --> 00:49:20,344
AI solution as possible,

1393
00:49:20,675 --> 00:49:22,844
OK, and then you want to translate

1394
00:49:22,844 --> 00:49:24,844
the benefits and the potential risks

1395
00:49:24,844 --> 00:49:26,543
into release criteria

1396
00:49:26,844 --> 00:49:29,724
which are typically statistical

1397
00:49:29,724 --> 00:49:31,304
measures of a property.

1398
00:49:31,829 --> 00:49:33,869
Right? And so you have to worry about

1399
00:49:33,869 --> 00:49:36,300
confidence levels, confidence intervals,

1400
00:49:36,539 --> 00:49:38,708
thresholds, things like that, and

1401
00:49:38,708 --> 00:49:40,840
these, these properties will be intention,

1402
00:49:41,329 --> 00:49:43,668
right? But if you have that set of

1403
00:49:43,668 --> 00:49:45,750
release criteria, then you can think

1404
00:49:45,750 --> 00:49:48,269
about what data sets do I need to

1405
00:49:48,269 --> 00:49:50,309
support assessing those criteria, what

1406
00:49:50,309 --> 00:49:52,349
data sets do I need to build the system, and

1407
00:49:52,349 --> 00:49:54,349
you can think about the architecture of the system

1408
00:49:54,349 --> 00:49:56,378
more effectively, and then you proceed to

1409
00:49:56,378 --> 00:49:58,389
evaluation and, and, and through it.

1410
00:49:58,898 --> 00:49:59,500
Um,

1411
00:49:59,820 --> 00:50:02,478
so, benefit and risk analysis.

1412
00:50:03,019 --> 00:50:05,019
All right, I'll try and cover this quickly so

1413
00:50:05,019 --> 00:50:06,478
we can handle more questions.

1414
00:50:06,820 --> 00:50:09,139
Um, so an example of a

1415
00:50:09,139 --> 00:50:11,340
best practice in the framework, right, is

1416
00:50:11,340 --> 00:50:13,228
it's asking the.

1417
00:50:13,860 --> 00:50:16,458
Asking the builders to identify

1418
00:50:16,458 --> 00:50:18,800
potential harmful events that might

1419
00:50:18,800 --> 00:50:19,938
impact privacy,

1420
00:50:20,260 --> 00:50:22,289
OK, and you know if

1421
00:50:22,289 --> 00:50:24,409
we skip down to the agentic

1422
00:50:25,019 --> 00:50:26,039
AI

1423
00:50:27,050 --> 00:50:29,699
situation, right, you might wanna wonder,

1424
00:50:29,898 --> 00:50:32,010
well. OK,

1425
00:50:32,369 --> 00:50:34,469
let's suppose that I have

1426
00:50:34,648 --> 00:50:36,728
an exchange that Alice has an exchange

1427
00:50:36,728 --> 00:50:38,449
of information with Bob,

1428
00:50:38,969 --> 00:50:41,099
right, and appropriately shares

1429
00:50:41,099 --> 00:50:43,329
some information with Bob. It's a credit card

1430
00:50:43,329 --> 00:50:45,719
number, and then Alice has an, uh,

1431
00:50:45,728 --> 00:50:47,969
has an exchange of information with

1432
00:50:47,969 --> 00:50:49,289
Chris, another agent.

1433
00:50:49,579 --> 00:50:51,889
And appropriately shares

1434
00:50:51,889 --> 00:50:53,969
information related to that transaction,

1435
00:50:54,050 --> 00:50:56,168
maybe I don't know, Social Security number,

1436
00:50:56,489 --> 00:50:58,728
you know, but then it turns out that Bob

1437
00:50:58,728 --> 00:50:59,789
and Chris,

1438
00:51:00,219 --> 00:51:00,769
you know,

1439
00:51:01,050 --> 00:51:03,329
somehow later shared the same

1440
00:51:03,329 --> 00:51:05,489
memory store owned by the same

1441
00:51:05,489 --> 00:51:07,849
person and the information got aggregated.

1442
00:51:08,000 --> 00:51:08,530
OK,

1443
00:51:08,878 --> 00:51:11,250
like what is the risk of that happening,

1444
00:51:11,289 --> 00:51:12,478
right? Could that happen?

1445
00:51:12,769 --> 00:51:14,829
How would you drive that risk to zero, right?

1446
00:51:14,929 --> 00:51:17,010
These are, these are the kinds of questions you want

1447
00:51:17,010 --> 00:51:18,168
to ask early on.

1448
00:51:18,489 --> 00:51:20,070
Um, in this case,

1449
00:51:20,530 --> 00:51:23,030
you know, when you're thinking about an agent

1450
00:51:23,030 --> 00:51:25,050
and hopefully this is, this is pretty

1451
00:51:25,050 --> 00:51:27,208
obvious, right, you're gonna wanna invest

1452
00:51:27,208 --> 00:51:29,449
more time as a builder thinking

1453
00:51:29,449 --> 00:51:30,590
about issues

1454
00:51:30,849 --> 00:51:33,050
of adversaries and

1455
00:51:33,050 --> 00:51:35,090
resource constraints like what happens if I do

1456
00:51:35,090 --> 00:51:37,070
run out of money while running my agent,

1457
00:51:37,489 --> 00:51:39,148
uh, how do I recover from that,

1458
00:51:39,409 --> 00:51:41,010
um, and shared components.

1459
00:51:41,860 --> 00:51:43,039
All right, let's pick another

1460
00:51:43,500 --> 00:51:46,199
uh focus area, the release criteria,

1461
00:51:46,619 --> 00:51:48,769
um. You know, this is,

1462
00:51:48,898 --> 00:51:50,478
this is one of the really.

1463
00:51:52,219 --> 00:51:53,438
challenging.

1464
00:51:54,199 --> 00:51:56,849
Um, exercises in

1465
00:51:56,849 --> 00:51:58,090
building an agent

1466
00:51:58,579 --> 00:51:59,429
and

1467
00:52:00,050 --> 00:52:02,208
because of what it's asking you to do is

1468
00:52:02,208 --> 00:52:04,628
think about all the possible properties,

1469
00:52:05,090 --> 00:52:07,289
all right, and come up with

1470
00:52:07,289 --> 00:52:09,289
metrics for those properties

1471
00:52:09,289 --> 00:52:11,489
that you are most concerned about,

1472
00:52:12,010 --> 00:52:14,250
all right, and there is

1473
00:52:14,250 --> 00:52:16,250
like the science literature in many for

1474
00:52:16,250 --> 00:52:18,289
many pro for many properties is just

1475
00:52:18,289 --> 00:52:19,769
chock full of options.

1476
00:52:20,510 --> 00:52:22,539
you have, it's the inverse problem from

1477
00:52:22,539 --> 00:52:24,579
the one that Michael was talking about where a

1478
00:52:24,579 --> 00:52:26,619
lot of these, these deeper issues don't have

1479
00:52:26,619 --> 00:52:28,739
much research. Some of this has too much, and

1480
00:52:28,739 --> 00:52:30,280
what do you do as a builder, right?

1481
00:52:30,780 --> 00:52:32,780
Um, but you have to do the work.

1482
00:52:32,938 --> 00:52:35,250
You have to sort of go through and think about,

1483
00:52:35,539 --> 00:52:37,619
OK, if my agent is going to be

1484
00:52:37,619 --> 00:52:39,780
narrowly focused, how effective

1485
00:52:39,780 --> 00:52:42,619
are my safeguards at actually rejecting

1486
00:52:42,619 --> 00:52:44,699
requests that ask, you know, for example,

1487
00:52:44,840 --> 00:52:46,340
uh, an IT,

1488
00:52:46,659 --> 00:52:48,780
uh, based agent to go, you

1489
00:52:48,780 --> 00:52:50,789
know, answer something about a legal

1490
00:52:50,789 --> 00:52:52,978
issue, you know, you just don't want that happening, so

1491
00:52:52,978 --> 00:52:55,280
you wanna measure that particular

1492
00:52:55,898 --> 00:52:56,699
safeguard,

1493
00:52:57,019 --> 00:52:58,619
right? So you need metrics for it.

1494
00:52:59,030 --> 00:53:01,039
And you basically wanna map every

1495
00:53:01,039 --> 00:53:03,159
expected benefit and potential risk

1496
00:53:03,159 --> 00:53:05,019
to one or more release criteria,

1497
00:53:05,719 --> 00:53:07,719
all right, so that's very challenging, but if you

1498
00:53:07,719 --> 00:53:08,699
do that work,

1499
00:53:09,398 --> 00:53:11,260
everything goes way more smoothly.

1500
00:53:11,679 --> 00:53:12,219
All right.

1501
00:53:12,610 --> 00:53:14,958
Another area just to cherry pick a little bit,

1502
00:53:15,000 --> 00:53:16,168
data set planning

1503
00:53:16,628 --> 00:53:18,260
here with agents.

1504
00:53:19,168 --> 00:53:20,269
Because again,

1505
00:53:20,610 --> 00:53:22,648
the idea we're thinking about

1506
00:53:22,648 --> 00:53:25,619
the future where the agents are interacting,

1507
00:53:26,000 --> 00:53:28,050
you know, with different

1508
00:53:28,050 --> 00:53:30,239
information sources, other agents, what have you,

1509
00:53:30,570 --> 00:53:32,409
you need a simulation system.

1510
00:53:33,079 --> 00:53:35,208
You've got to simulate the environment,

1511
00:53:35,530 --> 00:53:38,110
all right, with, with Gen AI, with,

1512
00:53:38,119 --> 00:53:38,780
uh,

1513
00:53:39,159 --> 00:53:41,239
with traditional AI you

1514
00:53:41,239 --> 00:53:43,539
can have a more static approach

1515
00:53:43,760 --> 00:53:45,030
to your data sets,

1516
00:53:45,438 --> 00:53:46,260
but here.

1517
00:53:47,148 --> 00:53:48,449
Here you can't,

1518
00:53:48,829 --> 00:53:49,389
um,

1519
00:53:49,668 --> 00:53:51,639
or at least in many cases you cannot

1520
00:53:51,909 --> 00:53:54,369
and so there, there are simulation

1521
00:53:54,369 --> 00:53:56,519
environments that are beginning to pop up. The

1522
00:53:56,519 --> 00:53:58,550
UK Safety Institute has, has

1523
00:53:58,550 --> 00:54:00,668
one, Meta has recently

1524
00:54:00,668 --> 00:54:01,648
written by one,

1525
00:54:01,909 --> 00:54:03,909
but I would expect to see a lot more

1526
00:54:03,909 --> 00:54:06,590
of these kinds of sort of evaluation

1527
00:54:06,590 --> 00:54:08,570
frameworks begin to crop up.

1528
00:54:09,188 --> 00:54:11,219
Um, and then on the system planning

1529
00:54:11,219 --> 00:54:12,860
one, I mean, there's,

1530
00:54:13,239 --> 00:54:15,239
you know, I, I, I think sort of

1531
00:54:15,239 --> 00:54:16,860
the major message here

1532
00:54:17,159 --> 00:54:19,280
is that you want to talk, think

1533
00:54:19,280 --> 00:54:21,320
through the flow of every single

1534
00:54:21,320 --> 00:54:23,159
type of data through your system.

1535
00:54:23,530 --> 00:54:26,110
Like the flow through interactions

1536
00:54:26,478 --> 00:54:27,148
with different

1537
00:54:27,750 --> 00:54:30,050
um with different agents, different

1538
00:54:30,050 --> 00:54:30,869
humans,

1539
00:54:31,208 --> 00:54:31,719
um,

1540
00:54:32,090 --> 00:54:34,208
the flow through components and

1541
00:54:34,208 --> 00:54:36,320
the flow through logs and this is this is

1542
00:54:36,320 --> 00:54:38,969
something that is sort of part of best

1543
00:54:38,969 --> 00:54:41,378
engineering practices in many

1544
00:54:41,378 --> 00:54:42,918
situations so it shouldn't be,

1545
00:54:43,208 --> 00:54:45,530
you know, that much of a surprise

1546
00:54:45,530 --> 00:54:46,628
or a challenge

1547
00:54:46,929 --> 00:54:48,969
but it is very important in this case

1548
00:54:48,969 --> 00:54:51,369
where privacy is, is such a central

1549
00:54:51,369 --> 00:54:53,809
issue. Anyway, I'm gonna pause

1550
00:54:53,809 --> 00:54:56,050
right here and if we have questions, let's

1551
00:54:56,050 --> 00:54:57,369
take them. Michael, do you wanna come up?

