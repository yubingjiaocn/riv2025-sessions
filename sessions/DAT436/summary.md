# AWS re:Invent 2025 DynamoDB 分布式系统设计分享总结

## 会议概述

本次演讲由 DynamoDB 团队的 Amrit 和 Craig 共同呈现，深入探讨了在构建大规模分布式数据库系统时所面临的设计决策和权衡。两位演讲者以轻松幽默的互动方式，分享了 DynamoDB 如何在保证安全性、持久性和可用性的前提下，实现可预测的低延迟性能。

演讲的核心理念是：在大型组织中构建服务时，不能依赖集中式决策。DynamoDB 团队通过建立明确的设计原则（tenets），使团队成员能够独立做出符合服务目标的决策。这些原则按优先级排序为：安全性、持久性、可用性和可预测的低延迟。前三项是不可协商的，而第四项是 DynamoDB 的核心特性——在任何规模下都能提供可预测的个位数毫秒级延迟。

演讲通过三个主要方面展开：请求路由机制、元数据结构设计，以及系统限制的原因。DynamoDB 目前服务规模巨大，有数百个表每秒处理超过 50 万次请求，Amazon 商店业务在 Prime Day 峰值达到每秒 1.51 亿次请求。在这样的规模下，结合分布式系统中的状态管理，带来了独特的挑战。

## 详细时间线与关键要点

### 开场与核心原则 (00:00 - 05:30)

00:00 - 01:30 - 演讲开始，Amrit 和 Craig 介绍自己，并以幽默的方式展示了他们的合作风格，预示了整场演讲的互动性质。

01:30 - 03:45 - Amrit 阐述了 DynamoDB 的核心设计原则（tenets）：
- 第一优先级：安全性
- 第二优先级：持久性
- 第三优先级：可用性
- 第四优先级：可预测的低延迟

03:45 - 05:30 - 强调这些原则如何帮助团队实现分布式决策制定，避免集中式瓶颈。前三项原则是不可协商的，任何功能或实现选择都不能牺牲这些原则。

### 分布式系统扩展基础 (05:30 - 15:00)

05:30 - 07:00 - Craig 介绍 DynamoDB 的规模：数百个表每秒处理超过 50 万次请求，Amazon 商店在 Prime Day 达到每秒 1.51 亿次请求。

07:00 - 09:30 - 讨论状态管理的演进：
- 单实例模式：应用和状态在同一实例上
- 分离模式：状态移到独立实例，提高扩展性但降低可用性
- 主从模式：引入主节点和从节点

09:30 - 12:00 - 探讨节点数量的选择：
- 两个节点：无法处理分区问题
- 三个节点：可以容忍单点故障，需要多数节点（2/3）确认写入
- 四个节点：偶数节点会导致"脑裂"问题
- 五个节点：可以容忍两个故障，但增加了复杂性和成本

12:00 - 15:00 - 解释为什么选择多个三节点组而不是大型集群：
- 三节点组具有理想的容错特性
- 避免了需要 500+ 节点确认写入的不现实场景
- 更容易快速替换故障节点

### 请求路由与元数据管理 (15:00 - 25:00)

15:00 - 17:30 - 讨论请求路由挑战：
- 客户端如何知道将请求发送到哪个三节点组
- 引入元数据层的概念
- 元数据存储数据位置信息

17:30 - 20:00 - 元数据查询的性能问题：
- 每个数据请求都需要先查询元数据，导致双倍请求量
- 元数据系统需要与数据系统相同的扩展能力
- 引入缓存解决方案

20:00 - 22:30 - 缓存策略：
- 在客户端和元数据之间添加缓存层
- 数据集变化不频繁，适合缓存
- 减少对元数据系统的请求压力

22:30 - 25:00 - 节点故障处理：
- 当节点故障时，新节点需要更新元数据
- 引入最终一致性的概念
- 讨论权威记录系统与指向它的最终一致性系统

### 元数据架构选择 (25:00 - 30:00)

25:00 - 27:30 - 两种元数据架构方案：
- 方案一：元数据也作为分区表，写入简单但路由复杂
- 方案二：完整副本模式，每个元数据节点包含全部元数据

27:30 - 30:00 - 完整副本模式的优势：
- 客户端可以连接任意元数据节点
- 查询问题变得简单
- 权衡是写入更新需要同步到所有副本，增加最终一致性

### DynamoDB 请求路由实现 (30:00 - 45:00)

30:00 - 33:00 - DynamoDB 架构概览：
- 存储节点集群：数千个节点，以三节点组形式组织
- 请求路由器集群：处理认证授权和元数据查询
- 客户端通过负载均衡器连接到随机请求路由器

33:00 - 36:00 - 完整的路由层次：
- DNS 层：作为负载均衡器的负载均衡器
- 网络负载均衡器层
- 请求路由器层
- 存储节点层

36:00 - 39:00 - 可用区（AZ）架构：
- 区域包含多个可用区
- 可用区是独立的故障域
- 每个分区的三个副本分布在三个不同的可用区
- 这是保证可用性和持久性的关键

39:00 - 42:00 - 网络延迟优化：
- 可用区之间的网络距离显著大于可用区内部
- 跨可用区通信会增加延迟
- DynamoDB 的服务器端处理时间与网络延迟处于相同数量级
- 减少网络距离是降低延迟的重要手段

42:00 - 45:00 - 分区 DNS（Split Horizon DNS）：
- 根据查询来源返回不同的 DNS 结果
- 将客户端路由到同一可用区的负载均衡器
- DNS 成为可用区选择器

### 故障场景处理 (45:00 - 55:00)

45:00 - 48:00 - 理想流量分布：
- 每个可用区处理 1/3 的流量
- 便于容量规划和扩展

48:00 - 51:00 - 可用区故障场景：
- 当一个可用区故障时，流量重新分配到其他两个可用区
- 每个健康可用区需要处理 50% 的额外流量
- 作为区域服务，必须预留容量应对此场景
- 这种容量成本已包含在 DynamoDB 定价中

51:00 - 53:30 - 流量倾斜问题：
- 如果某个可用区流量显著更高，有两种选择：
  - 选项一：在该可用区增加服务器，但故障时需要双倍容量
  - 选项二：限制每个可用区最多 1/3 流量，增加 DNS 管理复杂性

53:30 - 55:00 - 实际情况：
- 大数定律帮助平衡流量
- 来自互联网的流量可以灵活分配
- 通过 DNS 复杂性填补流量缺口

### 存储节点路由优化 (55:00 - 62:00)

55:00 - 57:30 - 第二跳优化（请求路由器到存储节点）：
- 理想情况下每个可用区 1/3 流量
- 强一致性读取必须访问 leader 节点
- 最终一致性读取可以访问任意副本

57:30 - 59:30 - 容量利用策略：
- 强一致性读取：单个节点容量
- 最终一致性读取：可以使用两个节点容量（排除一个可能故障的节点）
- 这解释了为什么最终一致性读取限制更高、价格更低

59:30 - 62:00 - 本地路由实现：
- 请求路由器选择同一可用区的副本
- 客户端和服务器都由 DynamoDB 控制，实现相对简单
- 监控服务器端统计数据，检测流量过载
- 必要时回退到随机路由模式，优先保证可用性

### 请求处理决策 (62:00 - 70:00)

62:00 - 64:30 - Amrit 接管讲解，强调 leader 分布：
- DynamoDB 在每个可用区保持 1/3 的 leader 节点
- 即使强一致性读取也能均匀分布流量

64:30 - 67:00 - 请求路由器的三个关键决策：
1. 身份验证：验证 SIGV4 签名
2. 授权：检查权限
3. 限流：检查是否在限制范围内

67:00 - 70:00 - 性能要求：
- 数百个客户每秒超过 50 万次请求
- 每天数十亿次决策
- 必须极快完成以保证可预测的低延迟

### 存储节点处理 (70:00 - 结束)

70:00 - 72:30 - 分区路由挑战：
- 表分布在多个分区中
- 分区分布在数十万个存储节点上
- 所有客户数据共享基础设施
- 每秒数亿次需要决定请求路由

72:30 - 75:00 - 存储节点处理流程：
- 所有数据始终静态加密（安全性原则）
- 如果客户未提供密钥，系统会提供默认密钥
- 获取加密密钥
- 检查分区级别的速率限制
- 决定是接受请求还是限流

75:00 - 结束 - 演讲在此处字幕截断，但已涵盖了主要的架构决策和设计权衡。

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


## 核心要点总结

1. 设计原则驱动决策：明确的优先级（安全性 > 持久性 > 可用性 > 低延迟）使团队能够分布式决策
2. 三节点复制组：在容错性、成本和复杂性之间的最佳平衡
3. 最终一致性是必然：在大规模系统中，元数据和路由信息的最终一致性不可避免
4. 网络拓扑优化：通过可用区感知路由显著降低延迟
5. 容量规划考虑故障：必须预留容量应对可用区级别故障
6. 共享基础设施：所有客户数据共存于同一存储节点集群，通过精细的路由和隔离保证性能