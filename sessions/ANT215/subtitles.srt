1
00:00:02,220 --> 00:00:03,573
- Good afternoon, everyone.

2
00:00:04,530 --> 00:00:07,440
Today, we will be
discussing about AI agents

3
00:00:07,440 --> 00:00:08,823
and AWS analytics.

4
00:00:09,960 --> 00:00:11,373
Let me start with a number.

5
00:00:12,720 --> 00:00:16,560
Data teams spend 60 to 70% of their time

6
00:00:16,560 --> 00:00:18,453
on undifferentiated task.

7
00:00:19,620 --> 00:00:23,400
Data engineers are spending
months doing upgrades.

8
00:00:23,400 --> 00:00:26,310
Data scientists are spending
a bunch of their time

9
00:00:26,310 --> 00:00:30,300
on preparing data or analysis
that they can use for it.

10
00:00:30,300 --> 00:00:33,360
And then, platform teams
are firefighting issues.

11
00:00:33,360 --> 00:00:37,260
Now, today, we do have AI assistance.

12
00:00:37,260 --> 00:00:41,400
We do have AI assistance, which
can take you the first step,

13
00:00:41,400 --> 00:00:43,170
which can give you the code.

14
00:00:43,170 --> 00:00:45,930
But a lot of times, these
AI coding assistants,

15
00:00:45,930 --> 00:00:49,260
they're not aware of your
data, of your resources,

16
00:00:49,260 --> 00:00:51,990
of your things that you have done already

17
00:00:51,990 --> 00:00:53,370
in your environment.

18
00:00:53,370 --> 00:00:55,560
And that's why, a lot of the times,

19
00:00:55,560 --> 00:00:57,570
they will give you the
code that will not work

20
00:00:57,570 --> 00:01:00,900
and then you end up repeating
and changing the code,

21
00:01:00,900 --> 00:01:03,933
trying to customize it for your
environment to make it work.

22
00:01:04,920 --> 00:01:07,320
But today, we are going to change that.

23
00:01:07,320 --> 00:01:11,490
I'm Shubham Mehta, Product
Manager for AWS Analytics

24
00:01:11,490 --> 00:01:14,790
and I have been focused on
bringing AI agents to analytics

25
00:01:14,790 --> 00:01:16,740
directly within your workflows.

26
00:01:16,740 --> 00:01:20,220
Today, we are going to
talk about AI agents

27
00:01:20,220 --> 00:01:22,260
that can actually solve the problem

28
00:01:22,260 --> 00:01:23,400
that you're looking to solve.

29
00:01:23,400 --> 00:01:25,470
They can help you upgrade your Spark code.

30
00:01:25,470 --> 00:01:27,633
They can help you build code faster.

31
00:01:29,910 --> 00:01:32,190
Before we get into details,

32
00:01:32,190 --> 00:01:33,990
let's quickly look at the agenda.

33
00:01:33,990 --> 00:01:37,470
We are going to talk about
overall overarching strategy

34
00:01:37,470 --> 00:01:41,100
of how we are approaching AI
agents in the analytics space.

35
00:01:41,100 --> 00:01:43,290
We are going to look
into one of the agents,

36
00:01:43,290 --> 00:01:44,940
which is SageMaker data agent,

37
00:01:44,940 --> 00:01:48,060
and then we are going to look
into a Spark upgrade agent

38
00:01:48,060 --> 00:01:51,720
for Amazon EMR, which can
help you upgrade Amazon,

39
00:01:51,720 --> 00:01:53,870
which can help you
upgrade your Spark code.

40
00:01:56,700 --> 00:01:58,860
Now, before we get into details,

41
00:01:58,860 --> 00:02:01,983
let me first describe the
problem a little bit further.

42
00:02:02,850 --> 00:02:04,290
When we talk with customers,

43
00:02:04,290 --> 00:02:07,290
we hear problems in three specific areas.

44
00:02:07,290 --> 00:02:10,563
First, we are seeing workflow complexity.

45
00:02:11,460 --> 00:02:14,400
Every task that you want
to do in your organization

46
00:02:14,400 --> 00:02:18,510
requires multiple things,
multiple tools to be used,

47
00:02:18,510 --> 00:02:20,820
which means that if you're
trying to build an ML model,

48
00:02:20,820 --> 00:02:23,220
you will end up having
to use four, five tools

49
00:02:23,220 --> 00:02:24,210
for that simple task.

50
00:02:24,210 --> 00:02:26,880
If you're trying to build in Spark,

51
00:02:26,880 --> 00:02:28,350
write an Spark application

52
00:02:28,350 --> 00:02:29,880
or upgrade an Spark application,

53
00:02:29,880 --> 00:02:31,590
you need to understand your build system,

54
00:02:31,590 --> 00:02:33,120
you need to write the Spark code,

55
00:02:33,120 --> 00:02:36,240
you need to upgrade and change
the Spark code, et cetera.

56
00:02:36,240 --> 00:02:39,270
And then, the knowledge gap.

57
00:02:39,270 --> 00:02:41,400
You don't have a single engineer

58
00:02:41,400 --> 00:02:43,140
who is expert in everything.

59
00:02:43,140 --> 00:02:44,730
You have multiple engineers

60
00:02:44,730 --> 00:02:47,100
who are expert in
different, different aspects

61
00:02:47,100 --> 00:02:49,350
of the entire workflow,

62
00:02:49,350 --> 00:02:51,240
where some of them might be good in Spark,

63
00:02:51,240 --> 00:02:53,130
some of them might be good in SQL.

64
00:02:53,130 --> 00:02:56,460
And lastly, we do see the capacity crunch

65
00:02:56,460 --> 00:03:01,460
where the data is growing
10 times every three years,

66
00:03:01,830 --> 00:03:04,710
but the data teams are not
growing at the same scale.

67
00:03:04,710 --> 00:03:09,390
Now, we think that AI agents
can actually solve this problem

68
00:03:09,390 --> 00:03:13,020
because AI agents, the agents
that we are trying to build,

69
00:03:13,020 --> 00:03:16,230
they can orchestrate workflows end to end.

70
00:03:16,230 --> 00:03:19,920
They can actually embed deep
expertise for Spark, SQL,

71
00:03:19,920 --> 00:03:22,710
and other complex engines within them

72
00:03:22,710 --> 00:03:24,453
and they can scale infinitely.

73
00:03:26,880 --> 00:03:29,490
Now, before we get into the agents,

74
00:03:29,490 --> 00:03:31,890
let's look at what are the
guiding principles that we have

75
00:03:31,890 --> 00:03:34,590
for AI agents and the AWS analytics.

76
00:03:34,590 --> 00:03:36,750
Now, four things guide our approach.

77
00:03:36,750 --> 00:03:40,650
First of them, the domain-specific agents.

78
00:03:40,650 --> 00:03:43,950
We think that over the years,

79
00:03:43,950 --> 00:03:45,090
we have learned a lot

80
00:03:45,090 --> 00:03:48,750
about what are the problems
customer face with Spark,

81
00:03:48,750 --> 00:03:51,090
what are the problems
customer face with SQL?

82
00:03:51,090 --> 00:03:52,710
And that's why we are trying to embed

83
00:03:52,710 --> 00:03:54,840
all that expertise in the domain,

84
00:03:54,840 --> 00:03:57,030
in the domain-specific agents.

85
00:03:57,030 --> 00:03:57,863
Second thing,

86
00:03:57,863 --> 00:04:01,560
we want the agents to be adapted
to your role, to your tool.

87
00:04:01,560 --> 00:04:03,870
If you are a data engineer
or a data scientist,

88
00:04:03,870 --> 00:04:05,220
you get a different experience

89
00:04:05,220 --> 00:04:09,030
than what you get as a software engineer.

90
00:04:09,030 --> 00:04:12,180
Thirdly, the multi-agent ecosystem.

91
00:04:12,180 --> 00:04:13,290
We don't think

92
00:04:13,290 --> 00:04:16,170
that one agent can solve all the problems.

93
00:04:16,170 --> 00:04:18,510
Not one engineer was
enough to solve everything.

94
00:04:18,510 --> 00:04:20,880
So, we are building an ecosystem of agents

95
00:04:20,880 --> 00:04:22,650
that can work with each other

96
00:04:22,650 --> 00:04:24,720
to solve certain aspects of the problem

97
00:04:24,720 --> 00:04:25,620
and then collaborate

98
00:04:25,620 --> 00:04:28,710
to solve the entire end-to-end
analytical workflow.

99
00:04:28,710 --> 00:04:32,790
And lastly, we believe in
MCP-based interoperability

100
00:04:32,790 --> 00:04:36,210
because we know that you
are used to using your IDEs,

101
00:04:36,210 --> 00:04:37,800
you're used to using your tools

102
00:04:37,800 --> 00:04:40,500
and you want to utilize these
agents where you're working

103
00:04:40,500 --> 00:04:45,500
rather than going to AWS console
for each and every thing.

104
00:04:46,170 --> 00:04:48,810
Now, let's look into...

105
00:04:48,810 --> 00:04:51,270
We have covered the high-level principles.

106
00:04:51,270 --> 00:04:53,040
Let's see what are the launches

107
00:04:53,040 --> 00:04:55,740
that we have done around
this so that you...

108
00:04:55,740 --> 00:04:56,940
It's not just principles.

109
00:04:56,940 --> 00:04:58,980
We have actually followed these principles

110
00:04:58,980 --> 00:05:01,470
and we are actually making it reality.

111
00:05:01,470 --> 00:05:04,860
So, the first agent that we
have launched just yesterday,

112
00:05:04,860 --> 00:05:07,470
we are proud to announce
Apache Spark upgrade agent.

113
00:05:07,470 --> 00:05:12,120
This is industry's first
Spark automated upgrade agent

114
00:05:12,120 --> 00:05:15,240
where it can take you
through a complex process

115
00:05:15,240 --> 00:05:18,750
of a Spark upgrades from
planning to code edits

116
00:05:18,750 --> 00:05:20,700
to building your Spark application

117
00:05:20,700 --> 00:05:23,010
and doing the data quality test

118
00:05:23,010 --> 00:05:26,580
across the data, across your application.

119
00:05:26,580 --> 00:05:30,300
And this agent is
available as of yesterday

120
00:05:30,300 --> 00:05:33,450
for Amazon EMR on EC2 and EMR serverless,

121
00:05:33,450 --> 00:05:37,350
and it can take you from
Spark 2.4 to Spark 3.5.

122
00:05:37,350 --> 00:05:40,080
Spark 4.0, support is coming soon.

123
00:05:40,080 --> 00:05:44,550
And this entire agent
is based on MCP tooling

124
00:05:44,550 --> 00:05:46,350
where you can...

125
00:05:46,350 --> 00:05:48,180
We are launching a remote MCP server

126
00:05:48,180 --> 00:05:50,460
where you can configure
this remote MCP server

127
00:05:50,460 --> 00:05:54,720
in IDEs of your choice and
you can use it there directly.

128
00:05:54,720 --> 00:05:56,910
Then, after you have
set up the MCP server,

129
00:05:56,910 --> 00:05:59,790
you just say that I want to
upgrade my Spark application

130
00:05:59,790 --> 00:06:01,500
from this version to that version,

131
00:06:01,500 --> 00:06:04,050
and here is the Spark
application in the project

132
00:06:04,050 --> 00:06:05,250
and it will read the code

133
00:06:05,250 --> 00:06:06,600
and it will go through all the steps.

134
00:06:06,600 --> 00:06:08,900
We'll look at the steps
in detail in a second.

135
00:06:10,530 --> 00:06:13,050
Now, it takes you through four steps.

136
00:06:13,050 --> 00:06:16,680
First of them is planning
and orchestration,

137
00:06:16,680 --> 00:06:18,870
where as you give your project,

138
00:06:18,870 --> 00:06:20,640
we analyze the structure of the project,

139
00:06:20,640 --> 00:06:25,320
we see how you're doing
the Spark submit to EMR,

140
00:06:25,320 --> 00:06:28,770
because based on that, the
approach to the agent differs.

141
00:06:28,770 --> 00:06:31,560
We see what kind of language you're using,

142
00:06:31,560 --> 00:06:34,170
do you have integration
test in your project or not,

143
00:06:34,170 --> 00:06:37,650
and based on that, we define
what are the steps we will do

144
00:06:37,650 --> 00:06:39,330
during the entire upgrade process.

145
00:06:39,330 --> 00:06:41,827
And here, you can actually
give the feedback that,

146
00:06:41,827 --> 00:06:44,160
"No, I don't want to take this step,

147
00:06:44,160 --> 00:06:46,470
I want to ignore the integration test.

148
00:06:46,470 --> 00:06:48,870
Can you actually update the upgrade plan?"

149
00:06:48,870 --> 00:06:51,270
And it will upgrade the upgrade plan.

150
00:06:51,270 --> 00:06:53,160
Then, it will look at your dependencies,

151
00:06:53,160 --> 00:06:56,160
whether you have pom.xml
or requirements.txt,

152
00:06:56,160 --> 00:06:58,267
it will go through them,
it will identify that,

153
00:06:58,267 --> 00:07:00,180
"Okay, these are the dependencies

154
00:07:00,180 --> 00:07:02,400
that need change for Spark 3.5.

155
00:07:02,400 --> 00:07:04,020
Let me make those changes."

156
00:07:04,020 --> 00:07:06,750
And it will then go
through your build process

157
00:07:06,750 --> 00:07:08,790
and actually build your application.

158
00:07:08,790 --> 00:07:10,620
Right now, it's supports Maven-based

159
00:07:10,620 --> 00:07:11,820
and SBT-based build,

160
00:07:11,820 --> 00:07:13,890
but because it is MCP-based,

161
00:07:13,890 --> 00:07:15,870
you can bring in your MCPs

162
00:07:15,870 --> 00:07:18,600
to hook into your own custom build process

163
00:07:18,600 --> 00:07:20,610
and actually ask the agent

164
00:07:20,610 --> 00:07:24,000
to use your build process
to build the application.

165
00:07:24,000 --> 00:07:28,140
And then, once it has gone
through all the dependencies,

166
00:07:28,140 --> 00:07:30,630
it will give you a list
of updated dependencies

167
00:07:30,630 --> 00:07:31,830
that you can then verify

168
00:07:31,830 --> 00:07:33,660
and see that, okay, this makes sense,

169
00:07:33,660 --> 00:07:35,130
let's go to the next step.

170
00:07:35,130 --> 00:07:38,700
In the next step, it
actually looks at your code

171
00:07:38,700 --> 00:07:43,020
and looks at all the breaking changes

172
00:07:43,020 --> 00:07:44,760
that Spark has introduced,

173
00:07:44,760 --> 00:07:46,470
and make sure that your code

174
00:07:46,470 --> 00:07:49,230
is not having any of
those breaking changes.

175
00:07:49,230 --> 00:07:51,450
And this entire process
of code modification

176
00:07:51,450 --> 00:07:53,280
is based on error-driven loop,

177
00:07:53,280 --> 00:07:54,990
where we try to run the code

178
00:07:54,990 --> 00:07:57,340
in the EMR cluster that you have given

179
00:07:58,777 --> 00:08:00,060
for the target version.

180
00:08:00,060 --> 00:08:00,960
And we see, okay,

181
00:08:00,960 --> 00:08:03,120
what is the error your
application is facing?

182
00:08:03,120 --> 00:08:04,270
And then, we have built

183
00:08:05,893 --> 00:08:08,730
a first-of-it's-kind knowledge base

184
00:08:08,730 --> 00:08:10,920
of all the Spark breaking changes

185
00:08:10,920 --> 00:08:12,030
where we make sure

186
00:08:12,030 --> 00:08:14,760
that we make the minimal
changes in your code

187
00:08:14,760 --> 00:08:17,640
in order to make sure your
code runs successfully

188
00:08:17,640 --> 00:08:19,110
in the target platform.

189
00:08:19,110 --> 00:08:20,910
And then, the last step

190
00:08:20,910 --> 00:08:22,590
is we make sure that the data

191
00:08:22,590 --> 00:08:26,010
that the application is
producing after the upgrade

192
00:08:26,010 --> 00:08:30,840
actually is exactly same as
the data that you started with

193
00:08:30,840 --> 00:08:32,880
in your prior Spark version.

194
00:08:32,880 --> 00:08:34,440
Because it's not just about

195
00:08:34,440 --> 00:08:36,330
running the application successfully,

196
00:08:36,330 --> 00:08:37,800
it's also about making sure

197
00:08:37,800 --> 00:08:39,750
that the data that it is producing

198
00:08:39,750 --> 00:08:42,303
actually is same as what
it was producing before.

199
00:08:44,310 --> 00:08:46,800
The next release that we
have done in this segment

200
00:08:46,800 --> 00:08:49,080
is data agent.

201
00:08:49,080 --> 00:08:52,890
Now, the previous one was
a domain-specific agent.

202
00:08:52,890 --> 00:08:53,850
In this case,

203
00:08:53,850 --> 00:08:56,970
we have built an agent that is
specific to data engineering

204
00:08:56,970 --> 00:08:59,523
and data scientists and
data analyst persona.

205
00:09:00,360 --> 00:09:02,190
What makes this agent different

206
00:09:02,190 --> 00:09:04,920
is that it is aware of your
business data and catalog.

207
00:09:04,920 --> 00:09:07,230
It uses MCP-based tooling

208
00:09:07,230 --> 00:09:09,540
to get all the information
from your catalog,

209
00:09:09,540 --> 00:09:12,660
what tables you have, what's
the schema of your tables.

210
00:09:12,660 --> 00:09:14,730
And then when you ask
it to write the query,

211
00:09:14,730 --> 00:09:16,020
it actually writes a query

212
00:09:16,020 --> 00:09:19,140
that can run without
you modifying anything.

213
00:09:19,140 --> 00:09:22,140
Now, this agent can also
do multi-step planning.

214
00:09:22,140 --> 00:09:23,880
This can take you from...

215
00:09:23,880 --> 00:09:25,890
This divides the complex task of,

216
00:09:25,890 --> 00:09:29,460
let's say, building a
machine learning pipeline.

217
00:09:29,460 --> 00:09:31,800
It will divide it into five, six steps.

218
00:09:31,800 --> 00:09:33,750
It will take you with each of those steps

219
00:09:33,750 --> 00:09:36,210
and we'll see those in action in a second.

220
00:09:36,210 --> 00:09:38,310
It will take you through
each of those steps

221
00:09:38,310 --> 00:09:40,650
and it will help you
actually write the code

222
00:09:40,650 --> 00:09:42,480
for each of those steps separately,

223
00:09:42,480 --> 00:09:45,210
giving you verification that
things are working fine or not.

224
00:09:45,210 --> 00:09:47,190
And if you run into an issue,

225
00:09:47,190 --> 00:09:48,900
you can actually troubleshoot

226
00:09:48,900 --> 00:09:50,850
using a Spark troubleshooting agent

227
00:09:50,850 --> 00:09:52,410
that is running behind it.

228
00:09:52,410 --> 00:09:54,210
So, let's say you're writing a Spark code

229
00:09:54,210 --> 00:09:55,560
and you ran into an issue.

230
00:09:55,560 --> 00:09:56,393
In this case,

231
00:09:56,393 --> 00:09:58,650
we actually rely on a Spark
troubleshooting agent,

232
00:09:58,650 --> 00:10:01,050
which is the third agent that we have

233
00:10:01,050 --> 00:10:02,790
to fix the Spark-specific issue.

234
00:10:02,790 --> 00:10:04,620
And if it's not a Spark-specific issue,

235
00:10:04,620 --> 00:10:07,620
we resolve it without relying
on the troubleshooting agent.

236
00:10:07,620 --> 00:10:11,400
And it also has security
guardrails built in

237
00:10:11,400 --> 00:10:15,420
where it prevents any destructive
action in your account

238
00:10:15,420 --> 00:10:17,790
where if you ask it to write a code

239
00:10:17,790 --> 00:10:19,130
that deletes your table and all,

240
00:10:19,130 --> 00:10:21,210
it will give you appropriate warnings

241
00:10:21,210 --> 00:10:22,680
and make sure that you're aware

242
00:10:22,680 --> 00:10:24,380
of the actions that you're taking.

243
00:10:26,130 --> 00:10:29,520
Now, I'll go over the demo.

244
00:10:29,520 --> 00:10:33,300
In this demo, we'll take up
a role of a data scientist

245
00:10:33,300 --> 00:10:36,150
who is trying to build
a machine learning model

246
00:10:36,150 --> 00:10:39,360
to predict lifetime value of customers.

247
00:10:39,360 --> 00:10:41,940
And the reason we want to
predict this lifetime value

248
00:10:41,940 --> 00:10:45,660
is because we want to
provide the customers

249
00:10:45,660 --> 00:10:48,480
the right incentive early in their journey

250
00:10:48,480 --> 00:10:50,580
so that we can make sure they are growing

251
00:10:50,580 --> 00:10:52,280
like our business grows with them.

252
00:10:53,700 --> 00:10:55,263
Let's go over the demo quickly.

253
00:10:56,970 --> 00:11:00,123
Now, in this case, we are
in the notebook interface.

254
00:11:01,048 --> 00:11:01,881
On the on the right-hand side,

255
00:11:01,881 --> 00:11:05,130
you have the SageMaker data agent.

256
00:11:05,130 --> 00:11:06,090
We are simply asking,

257
00:11:06,090 --> 00:11:08,250
going through the discovery
phase where we are asking it,

258
00:11:08,250 --> 00:11:10,920
can you list all the tables
that I have in my database?

259
00:11:10,920 --> 00:11:11,753
And it finds out,

260
00:11:11,753 --> 00:11:13,890
"Okay, you have three
tables in your database

261
00:11:13,890 --> 00:11:15,420
that you wanted to look at.

262
00:11:15,420 --> 00:11:16,320
And in this case,

263
00:11:16,320 --> 00:11:19,050
we found out that there's a
digital_wallet_ltv table."

264
00:11:19,050 --> 00:11:20,940
Now, the first thing is I want to see

265
00:11:20,940 --> 00:11:22,650
what is the sample data in that table.

266
00:11:22,650 --> 00:11:26,340
Here, I actually ask it to use Athena SQL

267
00:11:26,340 --> 00:11:29,340
to write a query for this
digital_wallet_ltv table,

268
00:11:29,340 --> 00:11:31,770
and the agent gives me
a very simple query,

269
00:11:31,770 --> 00:11:33,720
we'll SELECT * from sagemaker_sample_db

270
00:11:34,738 --> 00:11:37,530
and this is the new notebook
that we have launched

271
00:11:37,530 --> 00:11:39,390
which actually have interactive,

272
00:11:39,390 --> 00:11:42,360
like, renders the data
frames as interactive tables.

273
00:11:42,360 --> 00:11:44,580
And you can see that what
are the columns I have?

274
00:11:44,580 --> 00:11:46,920
I have this customer_satisfaction_score

275
00:11:46,920 --> 00:11:50,550
and support_ticket and
preferred_payment_method.

276
00:11:50,550 --> 00:11:51,637
Now, I asked the agent,

277
00:11:51,637 --> 00:11:53,130
"I want to explore more.

278
00:11:53,130 --> 00:11:54,930
Can you actually help
me analyze the impact

279
00:11:54,930 --> 00:11:58,530
of customer satisfaction
score on the LTV trends?"

280
00:11:58,530 --> 00:12:00,390
Now, the agent will actually understand

281
00:12:00,390 --> 00:12:02,850
that you are trying to use the data

282
00:12:02,850 --> 00:12:05,610
that you have already
loaded in your notebook

283
00:12:05,610 --> 00:12:09,120
and it will use the data
to actually build further.

284
00:12:09,120 --> 00:12:12,097
Now, if you see, the
agent actually says that,

285
00:12:12,097 --> 00:12:14,250
"Let me create the visualization to show

286
00:12:14,250 --> 00:12:16,590
and I will create the multiple charts."

287
00:12:16,590 --> 00:12:19,890
And in this case, the agent
used the same data frame

288
00:12:19,890 --> 00:12:22,050
that you had loaded using the SQL,

289
00:12:22,050 --> 00:12:24,780
and this data frame is
now used in Python code.

290
00:12:24,780 --> 00:12:27,390
So, these notebooks are
actually polyglot in nature

291
00:12:27,390 --> 00:12:29,970
where you can use the work
that you have done in SQL

292
00:12:29,970 --> 00:12:32,220
and you can use them in Python.

293
00:12:32,220 --> 00:12:35,610
Now, in this case, agent
comes up with graphs, okay?

294
00:12:35,610 --> 00:12:36,690
Like, you can actually see

295
00:12:36,690 --> 00:12:38,400
that with customer satisfaction score,

296
00:12:38,400 --> 00:12:40,410
the LTV is actually increasing.

297
00:12:40,410 --> 00:12:42,660
And the agent has
created this entire code,

298
00:12:42,660 --> 00:12:44,910
I didn't have to edit one single line.

299
00:12:44,910 --> 00:12:47,130
This entire code was written by agent

300
00:12:47,130 --> 00:12:48,540
and running successfully.

301
00:12:48,540 --> 00:12:51,840
And here, the agent also
created this high-level overview

302
00:12:51,840 --> 00:12:54,090
of what is the minimum and max LTV,

303
00:12:54,090 --> 00:12:56,670
median LTV for different
satisfaction score.

304
00:12:56,670 --> 00:12:58,140
Now, we get to the final task.

305
00:12:58,140 --> 00:13:00,970
Okay, I want to build a
linear regression model

306
00:13:01,950 --> 00:13:06,950
to predict LTV and I want
to do a 80 to 20 split,

307
00:13:07,200 --> 00:13:11,000
and I want to use one-hot encoding.

308
00:13:11,000 --> 00:13:14,250
In this case, the agent will
come up with multiple steps

309
00:13:14,250 --> 00:13:16,110
in order to do that task.

310
00:13:16,110 --> 00:13:17,283
So in this case,

311
00:13:18,810 --> 00:13:22,620
the agent comes up with the
code for each of the steps.

312
00:13:22,620 --> 00:13:24,530
The first step is actually...

313
00:13:26,460 --> 00:13:27,630
So in this case,

314
00:13:27,630 --> 00:13:29,940
the agent actually comes
up with multiple steps

315
00:13:29,940 --> 00:13:33,753
and I'll go over the code in a second.

316
00:13:36,330 --> 00:13:37,260
So, you can see,

317
00:13:37,260 --> 00:13:39,090
in the first, code we
are trying to understand,

318
00:13:39,090 --> 00:13:40,560
okay, what are the categorical features

319
00:13:40,560 --> 00:13:44,580
so that I know how to deal
with these categorical features

320
00:13:44,580 --> 00:13:45,870
in my actual run.

321
00:13:45,870 --> 00:13:47,580
So, I identified that there are four

322
00:13:47,580 --> 00:13:48,930
or five categorical features

323
00:13:48,930 --> 00:13:51,270
and I will be using one-hot encoding.

324
00:13:51,270 --> 00:13:52,890
The agent uses one-hot encoding

325
00:13:52,890 --> 00:13:54,990
to actually divide these
categorical features

326
00:13:54,990 --> 00:13:57,603
into multiple steps.

327
00:13:59,160 --> 00:14:04,110
And the agent has produced the
entire code with 23 features

328
00:14:04,110 --> 00:14:06,870
and 18 features were total,

329
00:14:06,870 --> 00:14:08,400
and then the rest of the features

330
00:14:08,400 --> 00:14:11,670
were categorical features
created using one-hot encoding.

331
00:14:11,670 --> 00:14:16,620
And it has created the model
and the model is 81% R2 score,

332
00:14:16,620 --> 00:14:19,953
which means that it is able to
explain the 81% variability.

333
00:14:20,910 --> 00:14:24,030
And finally, it created the
graph for the predicted LTV

334
00:14:24,030 --> 00:14:27,900
where it is able to show
that this was the LTV trend,

335
00:14:27,900 --> 00:14:30,000
and these were the LTV trends.

336
00:14:30,000 --> 00:14:31,590
Like, this were the predicted LTV

337
00:14:31,590 --> 00:14:32,820
and these were the LTV trends.

338
00:14:32,820 --> 00:14:34,890
So in this case, we created a simple model

339
00:14:34,890 --> 00:14:35,730
but we could have gone

340
00:14:35,730 --> 00:14:38,340
and created a more complex model as well.

341
00:14:38,340 --> 00:14:40,740
And here, we can actually
see the feature importance

342
00:14:40,740 --> 00:14:44,010
where we can see that the low income level

343
00:14:44,010 --> 00:14:45,960
actually decreases the LTV.

344
00:14:45,960 --> 00:14:48,420
But if you have a middle income level,

345
00:14:48,420 --> 00:14:49,740
it also decreases LTV.

346
00:14:49,740 --> 00:14:52,380
But if you have a high
customer satisfaction score,

347
00:14:52,380 --> 00:14:54,030
it increases LTV.

348
00:14:54,030 --> 00:14:55,200
So, you're able to see

349
00:14:55,200 --> 00:14:59,340
that entire code was created
by the agent end to end.

350
00:14:59,340 --> 00:15:02,700
It went through the splitting,
the training and test split.

351
00:15:02,700 --> 00:15:04,410
It went through feature engineering,

352
00:15:04,410 --> 00:15:07,050
it prepared the data for one-hot encoding,

353
00:15:07,050 --> 00:15:10,110
and then it actually did the analysis

354
00:15:10,110 --> 00:15:12,120
and gave you a feature importance.

355
00:15:12,120 --> 00:15:13,200
Now, the same agent,

356
00:15:13,200 --> 00:15:16,140
you can use it for
building data pipelines.

357
00:15:16,140 --> 00:15:18,510
If you're trying to build,
let's say, data pipelines

358
00:15:18,510 --> 00:15:21,870
or you want to run some query on S3 tables

359
00:15:21,870 --> 00:15:24,120
or on Glue catalog,

360
00:15:24,120 --> 00:15:27,000
the same agent can
actually write a Spark code

361
00:15:27,000 --> 00:15:29,670
or DuckDB code or Polaris code

362
00:15:29,670 --> 00:15:32,760
to run those data transformations

363
00:15:32,760 --> 00:15:34,830
using any engine that you prefer.

364
00:15:34,830 --> 00:15:35,730
And in this notebook,

365
00:15:35,730 --> 00:15:39,330
we have actually pre-installed
around 320 packages,

366
00:15:39,330 --> 00:15:41,730
like all the essential
packages that you would need

367
00:15:41,730 --> 00:15:46,730
for your complex analytical
task including SageMaker SDK,

368
00:15:47,040 --> 00:15:48,660
and we allow you, of course,

369
00:15:48,660 --> 00:15:50,280
the freedom of installing more things

370
00:15:50,280 --> 00:15:51,530
in case you would prefer.

371
00:15:52,500 --> 00:15:55,650
Now, if you want to learn
more about these two agents,

372
00:15:55,650 --> 00:15:58,290
I would highly recommend
scanning these QR codes.

373
00:15:58,290 --> 00:16:01,530
These QR codes will lead
you to documentation

374
00:16:01,530 --> 00:16:04,620
and you can see more about,

375
00:16:04,620 --> 00:16:06,990
like, what are the other
capabilities data agent has

376
00:16:06,990 --> 00:16:09,720
and how you can set up
the Spark upgrade agent.

377
00:16:09,720 --> 00:16:11,850
In the Spark upgrade agent documentation,

378
00:16:11,850 --> 00:16:14,670
you can actually see the
remote MCP server configuration

379
00:16:14,670 --> 00:16:15,630
that we have.

380
00:16:15,630 --> 00:16:17,580
So, I would highly recommend you go in,

381
00:16:17,580 --> 00:16:20,430
you use VS code or whatever IDE you have,

382
00:16:20,430 --> 00:16:22,110
you set up the MCP server

383
00:16:22,110 --> 00:16:23,770
and actually use this agent

384
00:16:24,630 --> 00:16:28,320
to do the upgrade for
your sample application

385
00:16:28,320 --> 00:16:31,293
and then use it to upgrade
your production applications.

386
00:16:32,640 --> 00:16:36,990
Now, this is a wrap of
our lightning talk today

387
00:16:36,990 --> 00:16:39,210
on how you accelerate data engineering,

388
00:16:39,210 --> 00:16:41,460
and I really appreciate all of your time

389
00:16:41,460 --> 00:16:43,713
for coming in here and listening to me.

