1
00:00:04,470 --> 00:00:06,540
- Well, hello, everyone,

2
00:00:06,540 --> 00:00:09,780
and welcome on the session DEV 308.

3
00:00:09,780 --> 00:00:11,310
My name is Oleksii Ivanchenko,

4
00:00:11,310 --> 00:00:14,970
I'm Solution Architect here in AWS,

5
00:00:14,970 --> 00:00:17,070
and with me, Vadym Kazulkin,

6
00:00:17,070 --> 00:00:19,830
he is Head of Development in IP Labs,

7
00:00:19,830 --> 00:00:23,010
and as well, AWS Serverless Hero.

8
00:00:23,010 --> 00:00:25,500
And for me, as the Solution Architect,

9
00:00:25,500 --> 00:00:29,610
main goal is to help my customer
for scaling their business,

10
00:00:29,610 --> 00:00:32,040
and help them modernize their workload

11
00:00:32,040 --> 00:00:35,140
without them spending
too much time and effort

12
00:00:35,140 --> 00:00:38,430
on these processes.

13
00:00:38,430 --> 00:00:40,440
So, today, we are going to talk about

14
00:00:40,440 --> 00:00:42,630
how to build modern application

15
00:00:42,630 --> 00:00:44,580
with the help of Aurora DSQL.

16
00:00:49,950 --> 00:00:52,500
For that, we are going to dive deep

17
00:00:52,500 --> 00:00:57,500
into building blocks of this
database and its architecture.

18
00:00:57,900 --> 00:01:00,810
We will go through few code samples,

19
00:01:00,810 --> 00:01:03,180
because this is 300-level session.

20
00:01:03,180 --> 00:01:07,350
And we will go through the
performance and challenges

21
00:01:07,350 --> 00:01:10,839
that you can see within the DSQL.

22
00:01:10,839 --> 00:01:12,330
And of course, we'll do the prop up

23
00:01:12,330 --> 00:01:14,700
on what we have learned today.

24
00:01:14,700 --> 00:01:17,288
And with that, I'm
passing the word to Vadym.

25
00:01:17,288 --> 00:01:18,121
Vadym?

26
00:01:19,260 --> 00:01:20,730
- So, the question is

27
00:01:20,730 --> 00:01:23,703
why we need another serverless database?

28
00:01:24,810 --> 00:01:28,860
We have already several offerings on AWS,

29
00:01:28,860 --> 00:01:30,450
the list is not complete,

30
00:01:30,450 --> 00:01:33,930
you see Amazon, DynamoDB,
RDS, Aurora Provisioned,

31
00:01:33,930 --> 00:01:37,200
and Amazon Aurora Serverless v2.

32
00:01:37,200 --> 00:01:39,360
I would like to talk
about these databases,

33
00:01:39,360 --> 00:01:42,570
but especially in terms how they fit

34
00:01:42,570 --> 00:01:44,070
for the serverless workflows.

35
00:01:44,070 --> 00:01:46,590
And I'm working a lot with AWS Lambda,

36
00:01:46,590 --> 00:01:50,640
so just how these
databases can be approached

37
00:01:50,640 --> 00:01:53,070
and talked to from AWS Lambda.

38
00:01:53,070 --> 00:01:56,220
So let's start with DynamoDB.

39
00:01:56,220 --> 00:02:00,300
It's really the ideal fit
for serverless workloads.

40
00:02:00,300 --> 00:02:03,210
You don't have any
infrastructure management here,

41
00:02:03,210 --> 00:02:04,200
it's the database,

42
00:02:04,200 --> 00:02:06,360
it scales up and down for you,

43
00:02:06,360 --> 00:02:08,461
and if it scales down,

44
00:02:08,461 --> 00:02:11,490
it scales up without any
cold start of the database,

45
00:02:11,490 --> 00:02:13,290
so it's not noticeable for you.

46
00:02:13,290 --> 00:02:17,640
You have single millisecond
digit performance,

47
00:02:17,640 --> 00:02:21,120
so that's really cool database.

48
00:02:21,120 --> 00:02:25,800
Also in terms of other
things, I really prefer it,

49
00:02:25,800 --> 00:02:28,620
but there are also challenges around that,

50
00:02:28,620 --> 00:02:30,540
so everything is a trade off.

51
00:02:30,540 --> 00:02:32,790
So, for example, it's NoSQL database,

52
00:02:32,790 --> 00:02:33,870
and with that,

53
00:02:33,870 --> 00:02:37,710
there's another mindset comparing
to relational databases,

54
00:02:37,710 --> 00:02:39,480
how you design those tables,

55
00:02:39,480 --> 00:02:42,150
how you involve the schema afterwards.

56
00:02:42,150 --> 00:02:45,450
Yeah? It's not very easy.

57
00:02:45,450 --> 00:02:48,630
And there are also
challenges on certain things,

58
00:02:48,630 --> 00:02:52,740
like by default, DynamoDB
is eventually consistent,

59
00:02:52,740 --> 00:02:56,670
you can turn various things
to be strong consistent,

60
00:02:56,670 --> 00:02:57,900
and pay more for it,

61
00:02:57,900 --> 00:03:00,611
but for example, there are restrictions,

62
00:03:00,611 --> 00:03:05,611
global secondary indices
cannot be strong consistent,

63
00:03:05,880 --> 00:03:06,713
for example.

64
00:03:06,713 --> 00:03:09,180
And there are also limitations
in term of transaction,

65
00:03:09,180 --> 00:03:11,880
how many items can be
put in one transaction,

66
00:03:11,880 --> 00:03:14,193
so there are certain
limitations with that.

67
00:03:16,230 --> 00:03:18,240
Now moving to other databases,

68
00:03:18,240 --> 00:03:20,580
they're all relational
databases, and we love them.

69
00:03:20,580 --> 00:03:21,420
I personally come

70
00:03:21,420 --> 00:03:24,657
from working 10 or more
years with Postgres,

71
00:03:24,657 --> 00:03:28,170
and I like these joins, and many tables,

72
00:03:28,170 --> 00:03:31,140
and all that stuff, and
it has its benefits.

73
00:03:31,140 --> 00:03:35,550
But especially in terms of how RDS fits

74
00:03:35,550 --> 00:03:37,170
into serverless stack roles,

75
00:03:37,170 --> 00:03:38,610
there are certain limitations.

76
00:03:38,610 --> 00:03:41,520
For example, there is a lot
of infrastructure work to do.

77
00:03:41,520 --> 00:03:44,583
For example, you need to
file VPC security groups,

78
00:03:45,540 --> 00:03:48,780
and you need to define the size

79
00:03:48,780 --> 00:03:50,820
and the family of the instance,

80
00:03:50,820 --> 00:03:52,470
and so on.

81
00:03:52,470 --> 00:03:54,330
And also, with that,

82
00:03:54,330 --> 00:03:57,660
you cannot scale compute and
storage with this database,

83
00:03:57,660 --> 00:03:59,340
so you have the certain size,

84
00:03:59,340 --> 00:04:01,530
and then you need to increase the size,

85
00:04:01,530 --> 00:04:03,060
we can have multi-AZ,

86
00:04:03,060 --> 00:04:05,880
just that you don't
have kind of the outage

87
00:04:05,880 --> 00:04:07,530
by increasing one side,

88
00:04:07,530 --> 00:04:12,000
and then the next side will
be increased automatically.

89
00:04:12,000 --> 00:04:15,330
And there are also challenges
around connection management

90
00:04:15,330 --> 00:04:19,803
if you talk from her RDS
database from Lambda,

91
00:04:20,942 --> 00:04:23,820
the number of connection to the database

92
00:04:23,820 --> 00:04:28,410
is a factor of the database,
of the family size.

93
00:04:28,410 --> 00:04:30,840
And if you have a lot of Lambda function,

94
00:04:30,840 --> 00:04:32,400
they scale simultaneously,

95
00:04:32,400 --> 00:04:36,480
you can run out of connections
talking to RDS database.

96
00:04:36,480 --> 00:04:38,910
There are solutions how to mitigate it,

97
00:04:38,910 --> 00:04:40,500
for example RDS proxy,

98
00:04:40,500 --> 00:04:45,000
by providing something in
between, like for multiplexing.

99
00:04:45,000 --> 00:04:47,340
But of course, it's another component,

100
00:04:47,340 --> 00:04:48,240
it's a managed one,

101
00:04:48,240 --> 00:04:50,140
but it also has a cost
associated with that,

102
00:04:50,140 --> 00:04:53,430
there can be even latency, and so on.

103
00:04:53,430 --> 00:04:57,150
So there are other challenges with RDS.

104
00:04:57,150 --> 00:05:00,630
Now moving to Amazon
Aurora, provisioned one,

105
00:05:00,630 --> 00:05:04,350
it's mainly the same
advantages and disadvantages

106
00:05:04,350 --> 00:05:05,340
as with RDS,

107
00:05:05,340 --> 00:05:06,360
the only one exception,

108
00:05:06,360 --> 00:05:10,800
you can scale their storage
independently of compute

109
00:05:10,800 --> 00:05:11,973
with that database.

110
00:05:13,110 --> 00:05:18,110
Now moving to Amazon Aurora Serverless v2,

111
00:05:18,197 --> 00:05:20,520
we are kind of approaching this world

112
00:05:20,520 --> 00:05:23,280
where the relational
database are more suitable

113
00:05:23,280 --> 00:05:24,450
for the serverless world.

114
00:05:24,450 --> 00:05:27,900
Here, you can scale compute
and storage automatically.

115
00:05:27,900 --> 00:05:29,820
You don't need VPC.

116
00:05:29,820 --> 00:05:30,870
You might have VPC

117
00:05:30,870 --> 00:05:32,580
if you would like to have private traffic,

118
00:05:32,580 --> 00:05:34,980
but you're not required to do that.

119
00:05:34,980 --> 00:05:37,020
You'll also not define the instance sizes,

120
00:05:37,020 --> 00:05:40,200
this is more Aurora capacity
units, and you can say,

121
00:05:40,200 --> 00:05:44,460
I would like to scale within
this ACU and that ACU.

122
00:05:44,460 --> 00:05:47,340
You can scale to zero, this database,

123
00:05:47,340 --> 00:05:50,760
which is not possible for
RDS and Aurora provisioned,

124
00:05:50,760 --> 00:05:52,140
and it's sometimes very important,

125
00:05:52,140 --> 00:05:57,140
because on your store
and testing environments,

126
00:05:58,380 --> 00:05:59,213
you would like to,

127
00:05:59,213 --> 00:06:00,046
if you don't use it,

128
00:06:00,046 --> 00:06:01,590
you don't want to pay it.

129
00:06:01,590 --> 00:06:03,840
So that's why, especially
for RDS and Aurora,

130
00:06:03,840 --> 00:06:06,460
you provision the smaller
instances for those

131
00:06:07,440 --> 00:06:08,970
test staging environments.

132
00:06:08,970 --> 00:06:10,590
For Amazon Aurora Serverless v2,

133
00:06:10,590 --> 00:06:14,160
you can specify scaling down to zero,

134
00:06:14,160 --> 00:06:16,050
but then you have the disadvantages.

135
00:06:16,050 --> 00:06:18,300
It might take up to 15 minutes,

136
00:06:18,300 --> 00:06:20,940
15 seconds, sorry, to scale up

137
00:06:20,940 --> 00:06:23,340
in case you specify it
to scale down to zero.

138
00:06:23,340 --> 00:06:27,780
By default, it scales to 0.5 ACUs.

139
00:06:27,780 --> 00:06:30,030
And also, if you scale down to zero,

140
00:06:30,030 --> 00:06:34,110
you will lose the cache
right near the instance,

141
00:06:34,110 --> 00:06:37,710
and by scaling up the
performance of the first queries,

142
00:06:37,710 --> 00:06:39,150
might be slower,

143
00:06:39,150 --> 00:06:42,990
because yeah, the cache needs to build up.

144
00:06:42,990 --> 00:06:45,480
So you see here different challenges,

145
00:06:45,480 --> 00:06:46,560
and Aurora Serverless v2,

146
00:06:46,560 --> 00:06:49,290
two you can use RDS proxy with that,

147
00:06:49,290 --> 00:06:52,020
but they also offer data API

148
00:06:52,020 --> 00:06:54,780
so you can talk to the
Aurora Serverless v2

149
00:06:54,780 --> 00:06:57,660
like you talk to DynamoDB via HTTP.

150
00:06:57,660 --> 00:06:59,910
This is this kind of solution,

151
00:06:59,910 --> 00:07:01,470
in parallel to RDS proxy,

152
00:07:01,470 --> 00:07:02,303
you can decide,

153
00:07:02,303 --> 00:07:06,150
you can use still RDS proxy, or data API.

154
00:07:06,150 --> 00:07:08,610
But data API is complete separate API,

155
00:07:08,610 --> 00:07:10,170
so you need to rewrite your code,

156
00:07:10,170 --> 00:07:12,120
in case you would like to adjust,

157
00:07:12,120 --> 00:07:14,240
and RDS proxy,

158
00:07:14,240 --> 00:07:19,240
you need only to change the endpoint.

159
00:07:19,560 --> 00:07:23,790
So the question that is
kind of being asked is,

160
00:07:23,790 --> 00:07:26,010
can we have AWS database offering

161
00:07:26,010 --> 00:07:28,440
which is as serverless as DynamoDB,

162
00:07:28,440 --> 00:07:32,553
in terms of not having to
deal with the infrastructure,

163
00:07:33,543 --> 00:07:36,330
or scale up and down very quickly

164
00:07:36,330 --> 00:07:38,220
without cold starts of the database,

165
00:07:38,220 --> 00:07:41,010
but provides the benefits
of the relational databases

166
00:07:41,010 --> 00:07:44,700
like these ACID things.

167
00:07:44,700 --> 00:07:46,080
And with that,

168
00:07:46,080 --> 00:07:49,050
I will pass the word to Oleksii.

169
00:07:49,050 --> 00:07:50,700
- Thank you.

170
00:07:50,700 --> 00:07:51,533
Yeah.

171
00:07:51,533 --> 00:07:55,500
So, for that, to address these questions,

172
00:07:55,500 --> 00:08:00,500
we have introduced Aurora
DSQL last re:Invent,

173
00:08:00,690 --> 00:08:03,090
and we made it generally available

174
00:08:03,090 --> 00:08:04,740
in the middle of this year.

175
00:08:04,740 --> 00:08:06,840
So what is the DSQL?

176
00:08:06,840 --> 00:08:09,600
DSQL is the Serverless database,

177
00:08:09,600 --> 00:08:11,280
which means that you don't need to have

178
00:08:11,280 --> 00:08:13,590
any infrastructure to manage,

179
00:08:13,590 --> 00:08:15,750
you don't need to bring your database down

180
00:08:15,750 --> 00:08:17,553
for patching or updating.

181
00:08:20,258 --> 00:08:21,270
It's handled by us.

182
00:08:21,270 --> 00:08:24,120
It's proud of five nines of availability,

183
00:08:24,120 --> 00:08:29,120
and you get scaling, compute,
and storage separately.

184
00:08:30,060 --> 00:08:32,910
You scale the reads and write separately,

185
00:08:32,910 --> 00:08:35,940
which provides you
virtually endless scale.

186
00:08:35,940 --> 00:08:37,560
And through all the scale,

187
00:08:37,560 --> 00:08:38,640
with all the scale,

188
00:08:38,640 --> 00:08:42,030
you get strongly consistency all the time.

189
00:08:42,030 --> 00:08:47,030
So when you would like
to look for the DSQL?

190
00:08:47,490 --> 00:08:51,000
First of all you would
like to look for DSQL

191
00:08:51,000 --> 00:08:53,100
If you are searching for a database

192
00:08:53,100 --> 00:08:56,520
that supports ACID transactions.

193
00:08:56,520 --> 00:08:58,260
Through multi-region.

194
00:08:58,260 --> 00:09:00,240
You want to feed the database

195
00:09:00,240 --> 00:09:04,920
for your serverless architecture
or microservice design,

196
00:09:04,920 --> 00:09:06,930
or you have the application

197
00:09:06,930 --> 00:09:09,270
that is following event-driven approach,

198
00:09:09,270 --> 00:09:13,050
or you don't know how spiky
will be your traffic tomorrow,

199
00:09:13,050 --> 00:09:14,490
or the day after tomorrow.

200
00:09:14,490 --> 00:09:18,150
And of course, if you want
to continue to utilize

201
00:09:18,150 --> 00:09:22,683
already existing tooling and
continue to work with SQL.

202
00:09:23,640 --> 00:09:28,440
And now this all sounds
pretty interesting,

203
00:09:28,440 --> 00:09:31,980
but how exactly we achieving all this?

204
00:09:31,980 --> 00:09:36,737
For that, let's dive into
the design of the DSQL.

205
00:09:36,737 --> 00:09:40,050
And we will start with
single region cluster.

206
00:09:40,050 --> 00:09:43,020
Single region cluster is active-active,

207
00:09:43,020 --> 00:09:46,380
multi-writer cluster,

208
00:09:46,380 --> 00:09:49,380
distributed across multiple
availability zones.

209
00:09:49,380 --> 00:09:54,000
The data is replicated across three zones.

210
00:09:54,000 --> 00:09:56,940
You have one endpoint
for reads and writes,

211
00:09:56,940 --> 00:09:59,910
it supports simultaneously
and concurrently.

212
00:09:59,910 --> 00:10:04,680
And you don't have any
instances to provision.

213
00:10:04,680 --> 00:10:08,520
You can send just
requests to the database,

214
00:10:08,520 --> 00:10:12,783
and everything else is
handled by it underneath.

215
00:10:14,010 --> 00:10:16,380
With no instances to provision,

216
00:10:16,380 --> 00:10:19,680
you don't have anything to
stop for patching or updating,

217
00:10:19,680 --> 00:10:22,620
we also do it by ourself.

218
00:10:22,620 --> 00:10:26,070
And with the single-region set up,

219
00:10:26,070 --> 00:10:29,820
you are getting four
nines of availability.

220
00:10:29,820 --> 00:10:32,553
In the same time with the,

221
00:10:34,110 --> 00:10:39,110
sorry, but behind the single endpoint,

222
00:10:39,540 --> 00:10:44,223
you have not only,

223
00:10:45,150 --> 00:10:50,150
not just one, or multiple
Postgres instances

224
00:10:50,430 --> 00:10:52,560
that are hidden and running,

225
00:10:52,560 --> 00:10:54,600
no, instead of that,

226
00:10:54,600 --> 00:10:56,820
what's really powers DSQL

227
00:10:56,820 --> 00:10:59,970
is the distributed
disaggregated architecture.

228
00:10:59,970 --> 00:11:04,970
We took critical components
of monolithic OTP database,

229
00:11:05,940 --> 00:11:08,820
and separated them into separate services.

230
00:11:08,820 --> 00:11:13,820
We separated them into
the connection management,

231
00:11:14,130 --> 00:11:17,250
we separated into the query processing,

232
00:11:17,250 --> 00:11:22,250
isolation enforcement, transaction
journaling, and storage.

233
00:11:22,560 --> 00:11:26,280
Each of these components
works independently,

234
00:11:26,280 --> 00:11:30,300
and are tuned for each specific role.

235
00:11:30,300 --> 00:11:32,310
Each of these components

236
00:11:32,310 --> 00:11:36,360
is working with the fleet
of the computer resources

237
00:11:36,360 --> 00:11:38,310
that can scale independently,

238
00:11:38,310 --> 00:11:41,103
and can adjust to the workloads.

239
00:11:43,170 --> 00:11:47,340
Yeah, you get one end point
for both reads and writes,

240
00:11:47,340 --> 00:11:52,110
so that means that your reads
and writes are always local,

241
00:11:52,110 --> 00:11:55,380
and the only thing which
is traveling across AZ

242
00:11:55,380 --> 00:11:57,751
is transaction commit.

243
00:11:57,751 --> 00:12:00,630
And also, as I said,

244
00:12:00,630 --> 00:12:04,770
you are getting four nines of availability

245
00:12:04,770 --> 00:12:07,110
with the single endpoint.

246
00:12:07,110 --> 00:12:08,280
In the same time,

247
00:12:08,280 --> 00:12:10,050
with multi-regional cluster,

248
00:12:10,050 --> 00:12:13,590
you are getting five
nines of availability,

249
00:12:13,590 --> 00:12:18,030
and multi-regional consistent writes.

250
00:12:18,030 --> 00:12:20,940
It provides two regional endpoints,

251
00:12:20,940 --> 00:12:22,440
both of these endpoints

252
00:12:22,440 --> 00:12:25,230
support concurrency reads and writes,

253
00:12:25,230 --> 00:12:27,750
and together it's represented

254
00:12:27,750 --> 00:12:30,690
as the single logical database.

255
00:12:30,690 --> 00:12:35,100
There is also the third region
there, called witness region.

256
00:12:35,100 --> 00:12:38,850
Witness region participates
in the write quorum,

257
00:12:38,850 --> 00:12:42,092
and works as the tiebreaker

258
00:12:42,092 --> 00:12:46,200
to decide which region
can continue to write

259
00:12:46,200 --> 00:12:49,113
in case of the network partitioning.

260
00:12:50,310 --> 00:12:52,410
The witness region receives the data

261
00:12:52,410 --> 00:12:56,340
from the regional cluster.

262
00:12:56,340 --> 00:12:58,290
It also hold limited amount

263
00:12:58,290 --> 00:13:00,840
of the encrypted transaction log,

264
00:13:00,840 --> 00:13:05,073
but it doesn't have any
cluster or any end points.

265
00:13:06,060 --> 00:13:09,960
So if we will look on the
high-level architecture,

266
00:13:09,960 --> 00:13:12,795
or building blocks, of DSQL,

267
00:13:12,795 --> 00:13:14,400
you will find their front-end,

268
00:13:14,400 --> 00:13:17,910
you will find Query Processor,
Adjudicator, Journal,

269
00:13:17,910 --> 00:13:20,610
Crossbar, and the storage.

270
00:13:20,610 --> 00:13:24,480
Query processor does most
of the SQL processing job,

271
00:13:24,480 --> 00:13:28,980
it acts as the dedicated Postgres engine

272
00:13:28,980 --> 00:13:30,690
for each transaction.

273
00:13:30,690 --> 00:13:33,690
And there can be as many query processors

274
00:13:33,690 --> 00:13:37,380
as there are concurrent
connections to the database.

275
00:13:37,380 --> 00:13:39,330
Adjudicator determines

276
00:13:39,330 --> 00:13:42,030
whether the transaction can be committed

277
00:13:42,030 --> 00:13:44,313
while following the isolation rules.

278
00:13:45,780 --> 00:13:48,780
Each shard key in database is owned

279
00:13:48,780 --> 00:13:53,760
exactly by the one adjudicator
at any given moment of time.

280
00:13:53,760 --> 00:13:57,510
Journal makes all the
transactions durable,

281
00:13:57,510 --> 00:14:01,320
and replicates the data
between availability zones

282
00:14:01,320 --> 00:14:02,523
and regions.

283
00:14:03,660 --> 00:14:08,280
Each transaction, always
assigned to the single journal.

284
00:14:08,280 --> 00:14:11,640
Crossbar merge the data streams

285
00:14:11,640 --> 00:14:15,450
and direct them into the storage nodes.

286
00:14:15,450 --> 00:14:16,683
Storage nodes,

287
00:14:17,670 --> 00:14:20,310
storage nodes provide access to your data,

288
00:14:20,310 --> 00:14:25,200
storage nodes hold the
replicas, multiple replicas,

289
00:14:25,200 --> 00:14:30,120
and each storage node
contains the specific range

290
00:14:30,120 --> 00:14:33,363
of the data based on the database key.

291
00:14:35,490 --> 00:14:38,880
It may looks little bit overwhelming,

292
00:14:38,880 --> 00:14:40,950
or sounds a little bit overwhelming,

293
00:14:40,950 --> 00:14:42,150
but let's take a look,

294
00:14:42,150 --> 00:14:44,940
how does all these
components works together?

295
00:14:44,940 --> 00:14:48,840
Let's start with the
standard read transaction

296
00:14:48,840 --> 00:14:51,600
and basic select statement.

297
00:14:51,600 --> 00:14:56,600
Let's imagine that we are
the user in the us-west-1,

298
00:14:57,000 --> 00:15:02,000
and we are willing to order
food from a local Pizza service,

299
00:15:04,226 --> 00:15:07,593
and somewhere here in the Las Vegas.

300
00:15:12,212 --> 00:15:15,000
So what's happening there?

301
00:15:15,000 --> 00:15:19,320
We're deciding that we
want to get the restaurants

302
00:15:19,320 --> 00:15:22,380
that have the rating four or higher,

303
00:15:22,380 --> 00:15:24,540
so we're executing the query for

304
00:15:24,540 --> 00:15:28,950
select * from restaurants
where rating equals

305
00:15:28,950 --> 00:15:30,930
or higher than 4.0.

306
00:15:30,930 --> 00:15:33,840
So what happens behind the scenes?

307
00:15:33,840 --> 00:15:38,840
Your application making the
connection to the DSQL frontend.

308
00:15:39,420 --> 00:15:42,780
Frontend allocates the query processor,

309
00:15:42,780 --> 00:15:46,800
and pass your statement
to the query processor.

310
00:15:46,800 --> 00:15:50,430
Query processor reads the local clock

311
00:15:50,430 --> 00:15:54,183
and sets the transaction
start time, tau start.

312
00:15:57,886 --> 00:15:59,310
Sorry. (coughs)

313
00:15:59,310 --> 00:16:03,300
Now your read-only transaction begins.

314
00:16:03,300 --> 00:16:05,310
The storage is automatically,

315
00:16:05,310 --> 00:16:08,610
and what's important, transparent to you,

316
00:16:08,610 --> 00:16:12,030
are partitioned according
to the database key.

317
00:16:12,030 --> 00:16:17,030
That's why Query Processor
consult with the shard map

318
00:16:18,000 --> 00:16:20,040
to understand where your data,

319
00:16:20,040 --> 00:16:22,500
in which storage node it is located.

320
00:16:22,500 --> 00:16:25,920
And because this is read-only transaction,

321
00:16:25,920 --> 00:16:29,520
Query Processor doesn't need
to follow the writer path,

322
00:16:29,520 --> 00:16:33,000
it doesn't need to go through
the Adjudicator or Journal,

323
00:16:33,000 --> 00:16:38,000
it can go straight to the
storage to retrieve your data.

324
00:16:38,580 --> 00:16:41,640
Storage nodes doesn't return pages,

325
00:16:41,640 --> 00:16:43,200
it returns rows.

326
00:16:43,200 --> 00:16:48,200
Storage nodes also can
process predicated push downs,

327
00:16:49,500 --> 00:16:52,510
filtering, aggregation of the rows

328
00:16:53,400 --> 00:16:57,210
before sending them
back to Query Processor.

329
00:16:57,210 --> 00:17:00,900
Together, all this stuff
significantly decreases

330
00:17:00,900 --> 00:17:02,070
the amount of the data

331
00:17:02,070 --> 00:17:04,053
that need to travel across the network,

332
00:17:05,585 --> 00:17:08,580
and decrease the amount of work

333
00:17:08,580 --> 00:17:11,610
that needs to be done
by the Query Processor.

334
00:17:11,610 --> 00:17:16,610
So, and comparing to traditional database,

335
00:17:17,250 --> 00:17:22,250
we don't have any monolithic
cache sitting aside.

336
00:17:22,680 --> 00:17:27,030
So when query processor
needs to get the data,

337
00:17:27,030 --> 00:17:30,270
it goes to the storage node,

338
00:17:30,270 --> 00:17:32,430
which is located in the same region,

339
00:17:32,430 --> 00:17:35,160
in the same availability zone

340
00:17:35,160 --> 00:17:37,440
where the query processor
is located as well.

341
00:17:37,440 --> 00:17:39,843
So your reads are always local,

342
00:17:41,220 --> 00:17:44,730
so then Query Processor return,

343
00:17:44,730 --> 00:17:48,390
then Query Processor return the results

344
00:17:48,390 --> 00:17:49,770
from the storage node,

345
00:17:49,770 --> 00:17:54,150
they all merge together,
and return it to you.

346
00:17:54,150 --> 00:17:56,280
So, what's happening next?

347
00:17:56,280 --> 00:17:57,930
Next, at this point,

348
00:17:57,930 --> 00:18:01,260
if you continue to
interact with the frontend,

349
00:18:01,260 --> 00:18:03,960
you are building the
interactive transaction.

350
00:18:03,960 --> 00:18:06,600
You start the transaction,

351
00:18:06,600 --> 00:18:08,880
you do some work with database,

352
00:18:08,880 --> 00:18:10,230
you do some work with the client,

353
00:18:10,230 --> 00:18:13,080
you maybe go back and
forth multiple times,

354
00:18:13,080 --> 00:18:15,243
and eventually, you commit.

355
00:18:17,100 --> 00:18:18,930
How does it look?

356
00:18:18,930 --> 00:18:21,870
How does it look, for
example, in our case,

357
00:18:21,870 --> 00:18:24,207
when you decided order the pizza?

358
00:18:24,207 --> 00:18:27,360
You select the restaurant,

359
00:18:27,360 --> 00:18:28,650
you browse the menu,

360
00:18:28,650 --> 00:18:29,760
you choose the item,

361
00:18:29,760 --> 00:18:33,660
you add the item into your order,

362
00:18:33,660 --> 00:18:35,880
you place the order,

363
00:18:35,880 --> 00:18:38,343
and basically, that's hitting the commit.

364
00:18:39,360 --> 00:18:43,530
So what, in traditional database,

365
00:18:43,530 --> 00:18:46,020
how it will look in traditional database?

366
00:18:46,020 --> 00:18:49,650
Most probably, you will
have to place some logs

367
00:18:49,650 --> 00:18:54,650
and do some checks,
potentially across the region,

368
00:18:55,500 --> 00:18:58,440
for each statement in your transaction.

369
00:18:58,440 --> 00:19:01,530
These add latency to your operation.

370
00:19:01,530 --> 00:19:03,720
Query Processor acts differently.

371
00:19:03,720 --> 00:19:06,060
It acts as the holding tongue

372
00:19:06,060 --> 00:19:09,330
for all your statements
in your transaction.

373
00:19:09,330 --> 00:19:12,690
It reads data from the local storage nodes

374
00:19:12,690 --> 00:19:15,870
and saves it into the local memory.

375
00:19:15,870 --> 00:19:18,450
When it needs to write the data,

376
00:19:18,450 --> 00:19:20,493
it's already writing the data,

377
00:19:22,320 --> 00:19:27,320
it changed the data of the
local saved data in the memory,

378
00:19:28,200 --> 00:19:32,520
and it accumulates all the
changes within this local memory

379
00:19:32,520 --> 00:19:34,050
and waits for the commit.

380
00:19:34,050 --> 00:19:37,410
And then, when it needs to commit it,

381
00:19:37,410 --> 00:19:39,540
just follow the writing pass,

382
00:19:39,540 --> 00:19:41,940
and sends the data to the Adjudicator

383
00:19:41,940 --> 00:19:44,250
to check the full transaction,

384
00:19:44,250 --> 00:19:48,390
to the Adjudicator to check the data.

385
00:19:48,390 --> 00:19:49,920
So, as you can see,

386
00:19:49,920 --> 00:19:53,160
Query Processor do a lot of heavy lifting,

387
00:19:53,160 --> 00:19:57,060
it's basically the heart
of the DSQL architecture.

388
00:19:57,060 --> 00:20:02,060
And Query Processor is
running in Firecracker,

389
00:20:02,820 --> 00:20:07,260
lightweight virtual machines,
called also micro VMs,

390
00:20:07,260 --> 00:20:12,260
and they are running on the
bare metal Query Processor host.

391
00:20:13,500 --> 00:20:18,000
And we can run thousands of
pre-provisioned micro VMs

392
00:20:18,000 --> 00:20:19,800
on the single host.

393
00:20:19,800 --> 00:20:23,280
So every time when you connect to the DSQL

394
00:20:23,280 --> 00:20:25,680
and start to use it,

395
00:20:25,680 --> 00:20:29,460
we are making sure that there
is enough micro VMs running

396
00:20:29,460 --> 00:20:30,900
to handle your workloads,

397
00:20:30,900 --> 00:20:34,263
and we can scale them
automatically as it is needed.

398
00:20:35,730 --> 00:20:40,730
Each Query Processor is fully
independent and isolated,

399
00:20:40,740 --> 00:20:43,560
so they never communicate to each other.

400
00:20:43,560 --> 00:20:45,556
And talking about the isolation,

401
00:20:45,556 --> 00:20:49,110
in DSQL, we are supporting
transaction-level isolation

402
00:20:49,110 --> 00:20:51,000
called snapshot isolation.

403
00:20:51,000 --> 00:20:52,680
Each transaction operates

404
00:20:52,680 --> 00:20:56,940
on the consistent snapshot of the database

405
00:20:56,940 --> 00:21:00,810
as it was at the beginning,

406
00:21:00,810 --> 00:21:02,820
at the start of the transaction.

407
00:21:02,820 --> 00:21:04,440
Your transaction begins

408
00:21:04,440 --> 00:21:07,645
and proceeds through
the SQL execution paths,

409
00:21:07,645 --> 00:21:10,290
where all the reads are happening

410
00:21:10,290 --> 00:21:13,710
always with the consistent snapshot.

411
00:21:13,710 --> 00:21:15,060
At the storage level,

412
00:21:15,060 --> 00:21:17,880
these reads are implemented

413
00:21:17,880 --> 00:21:22,800
using the technical multi-versioning
concurrency control,

414
00:21:22,800 --> 00:21:24,960
or multi-versioning.

415
00:21:24,960 --> 00:21:29,960
The storage engine keeps multiple
versions of the same row,

416
00:21:30,600 --> 00:21:35,550
and allowing access to the
older versions of the row

417
00:21:35,550 --> 00:21:40,023
while not blocking of
creating newer versions.

418
00:21:43,710 --> 00:21:47,520
And when, basically,
write operation occurs,

419
00:21:47,520 --> 00:21:50,190
like insert or update,

420
00:21:50,190 --> 00:21:53,040
it's not immediately
written to the storage,

421
00:21:53,040 --> 00:21:54,720
applied to the storage.

422
00:21:54,720 --> 00:21:58,110
Query Processor spools all the changes

423
00:21:58,110 --> 00:22:01,500
into the local storage,
into the local memory,

424
00:22:01,500 --> 00:22:05,160
and creates a private workspace for it,

425
00:22:05,160 --> 00:22:08,520
a private workspace for this transaction.

426
00:22:08,520 --> 00:22:11,970
And this approach basically enables you

427
00:22:11,970 --> 00:22:14,280
read your write's capabilities.

428
00:22:14,280 --> 00:22:19,280
So for all subsequent reads
within the same transaction,

429
00:22:19,336 --> 00:22:24,060
you're working with the pending changes,

430
00:22:24,060 --> 00:22:26,310
your read transactions
see the pending changes

431
00:22:26,310 --> 00:22:28,983
that you already applied
through the writes.

432
00:22:30,030 --> 00:22:34,050
So when when your transaction
issues the commit,

433
00:22:34,050 --> 00:22:36,720
DSQL needs to understand

434
00:22:36,720 --> 00:22:39,420
whether all the changes that were applied

435
00:22:39,420 --> 00:22:42,540
within the transaction and spooled locally

436
00:22:42,540 --> 00:22:44,220
can be applied to the storage,

437
00:22:44,220 --> 00:22:46,260
can be written to the storage.

438
00:22:46,260 --> 00:22:51,260
And that's where the
Adjudicator comes into the play.

439
00:22:51,450 --> 00:22:54,810
It sits in the writer path of DSQLs,

440
00:22:54,810 --> 00:22:59,203
and decides will the transaction
be committed and written.

441
00:22:59,203 --> 00:23:03,330
Its job to detect and
resolve the conflicts

442
00:23:03,330 --> 00:23:06,450
that are happening
between the transactions

443
00:23:06,450 --> 00:23:08,763
to ensure the consistent writes.

444
00:23:10,680 --> 00:23:11,513
Yeah.

445
00:23:11,513 --> 00:23:13,380
So, to do this,

446
00:23:13,380 --> 00:23:16,170
the Query Processor forms the payload,

447
00:23:16,170 --> 00:23:17,550
creates the payload,

448
00:23:17,550 --> 00:23:19,230
to send it to the Adjudicator,

449
00:23:19,230 --> 00:23:23,370
and this payload contains
all the necessary information

450
00:23:23,370 --> 00:23:24,960
to make this decision.

451
00:23:24,960 --> 00:23:26,940
We have the write set there,

452
00:23:26,940 --> 00:23:31,560
which contains all the items
modified by transaction.

453
00:23:31,560 --> 00:23:33,840
We have the post image set

454
00:23:33,840 --> 00:23:37,050
which contains the copy of the table rows

455
00:23:37,050 --> 00:23:39,300
after applying transaction changes,

456
00:23:39,300 --> 00:23:42,960
and we also have the start
time of the transaction,

457
00:23:42,960 --> 00:23:43,980
tau start,

458
00:23:43,980 --> 00:23:47,640
which plays the crucial, critical role

459
00:23:47,640 --> 00:23:52,593
in deciding which will be
transaction committed or aborted.

460
00:23:54,150 --> 00:23:58,113
And we will talk about this
part a little bit later.

461
00:23:59,490 --> 00:24:04,490
So now, when two transactions comes,

462
00:24:05,580 --> 00:24:07,877
what basically happens?

463
00:24:07,877 --> 00:24:10,770
Here we have two transactions, A and B.

464
00:24:10,770 --> 00:24:15,770
They started almost at
the same moment of time,

465
00:24:16,500 --> 00:24:18,720
but Transaction A was committed

466
00:24:18,720 --> 00:24:21,570
slightly before Transaction B.

467
00:24:21,570 --> 00:24:24,330
So what happened in this case?

468
00:24:24,330 --> 00:24:29,010
In this case, Adjudicator
will analyze the payload

469
00:24:29,010 --> 00:24:31,020
of both of the transactions,

470
00:24:31,020 --> 00:24:35,640
examining their writer set and start time.

471
00:24:35,640 --> 00:24:39,720
It'll look for the all the changes

472
00:24:39,720 --> 00:24:43,770
that were made after the
start time, tau start,

473
00:24:43,770 --> 00:24:46,440
and it will look for overlapping

474
00:24:46,440 --> 00:24:50,370
or matching rows that was changed.

475
00:24:50,370 --> 00:24:53,430
And it sees that both of the transactions

476
00:24:53,430 --> 00:24:55,950
are trying to change the same row,

477
00:24:55,950 --> 00:24:59,250
and because they cannot both
of them change the same row

478
00:24:59,250 --> 00:25:00,483
at the same time,

479
00:25:01,350 --> 00:25:04,323
because Transaction A was applied,

480
00:25:05,250 --> 00:25:08,850
committed slightly before
Transaction B, need to be aborted.

481
00:25:08,850 --> 00:25:10,230
At that moment of time,

482
00:25:10,230 --> 00:25:12,780
Transaction A is allowed to proceed

483
00:25:12,780 --> 00:25:17,223
and decide the commit
timestamp, tau commit.

484
00:25:25,410 --> 00:25:29,250
If there is two transactions

485
00:25:29,250 --> 00:25:31,320
which have different writer sets,

486
00:25:31,320 --> 00:25:35,310
or these writer sets
contains the not overlapping,

487
00:25:35,310 --> 00:25:40,310
or not collapsing, not
conflicting writes, or rows,

488
00:25:40,980 --> 00:25:44,967
in this case, both
transactions can be committed,

489
00:25:46,440 --> 00:25:47,520
can be allowed to proceed,

490
00:25:47,520 --> 00:25:52,083
and both transactions will
be assigned the tau commit.

491
00:25:53,190 --> 00:25:56,460
So, once Adjudicator decides

492
00:25:56,460 --> 00:25:58,890
that your transaction's able to proceed,

493
00:25:58,890 --> 00:26:01,530
what exactly makes it durable?

494
00:26:01,530 --> 00:26:03,270
In traditional database,

495
00:26:03,270 --> 00:26:06,960
durability happens on the storage level.

496
00:26:06,960 --> 00:26:08,760
Transaction is considered committed

497
00:26:08,760 --> 00:26:13,110
only when it's written
durably to the storage.

498
00:26:13,110 --> 00:26:16,770
The storage layer is then responsible

499
00:26:16,770 --> 00:26:21,770
for recovering your committed transaction

500
00:26:22,320 --> 00:26:23,939
in case of failure.

501
00:26:23,939 --> 00:26:26,940
But these things comes with a price,

502
00:26:26,940 --> 00:26:30,300
for that, you need to
have the checkpoint in,

503
00:26:30,300 --> 00:26:32,310
you have to have the logging,

504
00:26:32,310 --> 00:26:35,790
you need to carefully coordinate

505
00:26:35,790 --> 00:26:39,360
between the memory and the
storage synchronization

506
00:26:39,360 --> 00:26:42,930
to be able to recover
in case of the failure.

507
00:26:42,930 --> 00:26:45,870
And all this adds lots of latency

508
00:26:45,870 --> 00:26:48,843
and lots of complexity to the system.

509
00:26:49,950 --> 00:26:54,950
So DSQL manages this complexity
by using the journal.

510
00:26:56,790 --> 00:26:59,430
Journal is independent,

511
00:26:59,430 --> 00:27:04,430
sorry, internal component that
was written for the decade,

512
00:27:06,240 --> 00:27:10,777
and was optimized for ordered
replication of the data

513
00:27:15,030 --> 00:27:18,990
between multiple AZs and
between multiple regions.

514
00:27:18,990 --> 00:27:21,270
It scales horizontally,

515
00:27:21,270 --> 00:27:26,190
so transaction can come
into any of the instances

516
00:27:26,190 --> 00:27:27,480
of this journal.

517
00:27:27,480 --> 00:27:31,620
Journal coordinate to
maintain ordered streams

518
00:27:31,620 --> 00:27:34,230
of communication of
committed transactions,

519
00:27:34,230 --> 00:27:37,865
and then Crossbar, which
is pulling the changes,

520
00:27:37,865 --> 00:27:41,670
pulling the transactions from the journal,

521
00:27:41,670 --> 00:27:44,283
routing this transaction to the storage,

522
00:27:45,630 --> 00:27:49,740
and using this order to ensure
that all the transactions

523
00:27:49,740 --> 00:27:54,123
are written to the storage
in the proper sequence.

524
00:27:55,200 --> 00:27:57,570
So once Adjudicator decides

525
00:27:57,570 --> 00:27:59,790
that your transaction is committed,

526
00:27:59,790 --> 00:28:04,590
it sends the payload and commit
timestamp to the journal.

527
00:28:04,590 --> 00:28:07,830
Once the journal acknowledge the write,

528
00:28:07,830 --> 00:28:09,630
acknowledges your transaction,

529
00:28:09,630 --> 00:28:13,983
your transaction is durable
and atomically committed.

530
00:28:14,880 --> 00:28:19,880
The journal sends success code
back to the Query Processor,

531
00:28:20,340 --> 00:28:22,440
and Crossbar, at this moment,

532
00:28:22,440 --> 00:28:25,080
begins to pull the data from the journal

533
00:28:25,080 --> 00:28:27,750
and draw it to the storage.

534
00:28:27,750 --> 00:28:30,003
The Query Processor, at this moment,

535
00:28:31,110 --> 00:28:35,910
sends success code back
to you, to the user,

536
00:28:35,910 --> 00:28:38,610
and for you, from this perspective,

537
00:28:38,610 --> 00:28:41,853
the transaction is successfully committed.

538
00:28:43,320 --> 00:28:45,420
And now I want to draw your attention

539
00:28:45,420 --> 00:28:50,420
to two components that are
crucial in all the workflow.

540
00:28:53,550 --> 00:28:56,100
Remember that tau start and tau commit.

541
00:28:56,100 --> 00:28:57,300
This is the time.

542
00:28:57,300 --> 00:29:02,010
This is crucial because you
cannot get strongly consistency

543
00:29:02,010 --> 00:29:05,340
without synchronizing on the time.

544
00:29:05,340 --> 00:29:08,940
You cannot coordinate them

545
00:29:08,940 --> 00:29:13,830
if you don't have the reference
clock which you can trust,

546
00:29:13,830 --> 00:29:16,170
and you know that you can trust

547
00:29:16,170 --> 00:29:19,230
to this tau start and tau commit.

548
00:29:19,230 --> 00:29:22,140
For that for solving this issue

549
00:29:22,140 --> 00:29:26,580
and achieving this reference
clock which we can trust,

550
00:29:26,580 --> 00:29:29,730
we have introduced
Amazon Time Sync Service.

551
00:29:29,730 --> 00:29:32,010
With Time Sync Service,

552
00:29:32,010 --> 00:29:36,813
with dedicated timing hardware
that are located across AWS

553
00:29:38,460 --> 00:29:40,770
and AWS Nitro system,

554
00:29:40,770 --> 00:29:45,770
we having the direct GPS
dictated reference clock

555
00:29:48,060 --> 00:29:51,420
that are located on the EC2s.

556
00:29:51,420 --> 00:29:54,150
In DSQL, it effectively allows us

557
00:29:54,150 --> 00:29:58,980
to have globally-synchronized clock

558
00:29:58,980 --> 00:30:02,640
with the microseconds accuracy time.

559
00:30:02,640 --> 00:30:04,050
With all of this,

560
00:30:04,050 --> 00:30:06,120
I'm passing the word to Vadym

561
00:30:06,120 --> 00:30:10,260
to talk about identification
and authorization,

562
00:30:10,260 --> 00:30:11,700
how it's happening.

563
00:30:11,700 --> 00:30:12,533
Thank you.

564
00:30:14,040 --> 00:30:17,010
- Yeah so, let's look
into Aurora DSQL more

565
00:30:17,010 --> 00:30:19,890
from developer's perspective.

566
00:30:19,890 --> 00:30:22,113
The slides are not switching.

567
00:30:26,130 --> 00:30:26,963
Something?

568
00:30:29,700 --> 00:30:30,533
Oh?

569
00:30:31,730 --> 00:30:33,030
Okay, now it's working.

570
00:30:33,030 --> 00:30:34,170
So, thanks.

571
00:30:34,170 --> 00:30:36,240
So now let's talk about authentication,

572
00:30:36,240 --> 00:30:41,240
how we normally authenticate
to the relational database.

573
00:30:42,812 --> 00:30:45,690
Okay, it's now not working once again.

574
00:30:45,690 --> 00:30:47,250
Okay, now.

575
00:30:47,250 --> 00:30:50,970
So, normally we use user and password,

576
00:30:50,970 --> 00:30:53,730
and with that it might happen

577
00:30:53,730 --> 00:30:57,240
that somebody can get access
to this password and misuse it.

578
00:30:57,240 --> 00:30:59,970
So, normally we need to
rotate this password,

579
00:30:59,970 --> 00:31:01,623
which we sometimes forget,

580
00:31:02,580 --> 00:31:04,890
and this offers the attack vector there.

581
00:31:04,890 --> 00:31:08,010
So DSQL doesn't use any password,

582
00:31:08,010 --> 00:31:11,250
but what does it use instead?

583
00:31:11,250 --> 00:31:14,070
It uses tokens to authenticate.

584
00:31:14,070 --> 00:31:17,220
And the tokens are generated by AWS SDK,

585
00:31:17,220 --> 00:31:19,680
and is fast, and what's very important,

586
00:31:19,680 --> 00:31:21,720
the local cryptography.

587
00:31:21,720 --> 00:31:24,390
And this is more or less the same

588
00:31:24,390 --> 00:31:27,660
that is used behind the scenes
if we talk to S3 DynamoDB,

589
00:31:27,660 --> 00:31:29,670
but these tokens will be generated for us,

590
00:31:29,670 --> 00:31:30,840
we don't deal with that.

591
00:31:30,840 --> 00:31:34,353
In case of DSQL, we will
probably need to deal with that.

592
00:31:35,430 --> 00:31:37,440
What's important is
they're very short lived.

593
00:31:37,440 --> 00:31:39,120
If somebody intercepts a token,

594
00:31:39,120 --> 00:31:41,973
it's probably already expired,
which is a good start.

595
00:31:43,230 --> 00:31:46,200
So now let's look how
to generate this token.

596
00:31:46,200 --> 00:31:50,580
This is one of the examples
we use here, AWS CLI request,

597
00:31:50,580 --> 00:31:55,500
we send here aws dsql
generate-db-connect-admin-auth-token,

598
00:31:55,500 --> 00:31:59,400
give the hostname, and
how much time to expire.

599
00:31:59,400 --> 00:32:01,500
As short as possible is also okay.

600
00:32:01,500 --> 00:32:06,330
What we'll get back here
is basically the response,

601
00:32:06,330 --> 00:32:08,190
and the response is the whole token,

602
00:32:08,190 --> 00:32:11,010
and we can use this token to authenticate

603
00:32:11,010 --> 00:32:12,333
instead of the password.

604
00:32:13,290 --> 00:32:14,520
So, for example,

605
00:32:14,520 --> 00:32:18,370
you can use that to
authenticate using psql.

606
00:32:18,370 --> 00:32:21,150
Yeah? This is how we talk
to the Postgres database.

607
00:32:21,150 --> 00:32:25,140
psql, we simply export it
to the environment variable

608
00:32:25,140 --> 00:32:26,040
called PGPASSWORD,

609
00:32:26,910 --> 00:32:30,990
and then if we do psql -quiet connect,

610
00:32:30,990 --> 00:32:35,073
then this token will be
used instead of password.

611
00:32:36,630 --> 00:32:39,000
In case you use something like the DBeaver

612
00:32:39,000 --> 00:32:40,590
or some other tool to authenticate,

613
00:32:40,590 --> 00:32:43,770
you simply grab this token and
use it instead of password.

614
00:32:43,770 --> 00:32:45,480
Yeah, this is how it works.

615
00:32:45,480 --> 00:32:48,210
The cool thing is that DSQL team released

616
00:32:48,210 --> 00:32:52,680
the integrated query editor, SQL editor,

617
00:32:52,680 --> 00:32:55,080
in the browser several weeks ago.

618
00:32:55,080 --> 00:32:57,570
So you can simply go to the DSQL page,

619
00:32:57,570 --> 00:33:02,570
and then you can simply do all
the SQL stuff in the browser,

620
00:33:03,300 --> 00:33:06,600
and this token thing will be done for you

621
00:33:06,600 --> 00:33:07,800
basically behind the scenes,

622
00:33:07,800 --> 00:33:09,360
so you don't need to bother with that.

623
00:33:09,360 --> 00:33:10,629
But, of course,

624
00:33:10,629 --> 00:33:13,260
sometimes we would like to have
powerful tools like DBeaver,

625
00:33:13,260 --> 00:33:14,093
and use that.

626
00:33:14,970 --> 00:33:16,110
What I would like to show you

627
00:33:16,110 --> 00:33:19,050
is how easy it is to
create Aurora DSQL cluster,

628
00:33:19,050 --> 00:33:22,170
especially single-region cluster.

629
00:33:22,170 --> 00:33:24,270
You see we don't have any scrollbar here,

630
00:33:24,270 --> 00:33:25,440
which is surprising.

631
00:33:25,440 --> 00:33:28,410
If you created RDS,

632
00:33:28,410 --> 00:33:31,320
then you probably know how many
choices do you need to make.

633
00:33:31,320 --> 00:33:34,710
Here's the only thing more
or less that you need to do,

634
00:33:34,710 --> 00:33:38,610
is just more or less give
the cluster the name.

635
00:33:38,610 --> 00:33:41,070
And everything else is already there,

636
00:33:41,070 --> 00:33:43,200
yeah, there is enabled
deletion protection,

637
00:33:43,200 --> 00:33:45,960
which is a good stuff, you can remove it.

638
00:33:45,960 --> 00:33:48,330
You can customize encryption settings.

639
00:33:48,330 --> 00:33:50,610
And you can add the text.

640
00:33:50,610 --> 00:33:51,900
That's basically it.

641
00:33:51,900 --> 00:33:53,520
Of course, for the multi-region cluster,

642
00:33:53,520 --> 00:33:54,750
it's a bit more complicated,

643
00:33:54,750 --> 00:33:56,700
because you need another region

644
00:33:56,700 --> 00:33:57,960
just to select another region,

645
00:33:57,960 --> 00:33:59,640
then select the witness region.

646
00:33:59,640 --> 00:34:02,040
But, basically, the
settings will be the same,

647
00:34:02,040 --> 00:34:03,663
so, really, really easy.

648
00:34:04,620 --> 00:34:07,140
I would like to show you code examples.

649
00:34:07,140 --> 00:34:10,473
Please raise your hands who
can understand the Java code?

650
00:34:11,700 --> 00:34:13,800
Okay, lots of people, which is good,

651
00:34:13,800 --> 00:34:18,800
because I'm the Java guy,
and I feel understood myself.

652
00:34:18,810 --> 00:34:22,410
So now let's take this
subset of application

653
00:34:22,410 --> 00:34:24,453
that Oleksii showed you.

654
00:34:25,380 --> 00:34:27,780
And this will be kind of
only the ordering part.

655
00:34:27,780 --> 00:34:30,810
So you see API gateway and
a bunch of Lambda functions,

656
00:34:30,810 --> 00:34:33,600
how to create order, get order by ID,

657
00:34:33,600 --> 00:34:36,180
or get orders by created range.

658
00:34:36,180 --> 00:34:39,420
Like give me all the orders
from the 1st of November

659
00:34:39,420 --> 00:34:44,040
until the 13th of November.

660
00:34:44,040 --> 00:34:48,360
And we store that in the Aurora DSQL.

661
00:34:48,360 --> 00:34:50,220
I will show you example with AWS SAM,

662
00:34:50,220 --> 00:34:54,030
but you can use SDK, you can
use CloudFormation Terraform.

663
00:34:54,030 --> 00:34:56,100
Basically this is a very generic example,

664
00:34:56,100 --> 00:34:58,980
you can reuse it for all applications,

665
00:34:58,980 --> 00:35:01,770
especially if you talk
from Lambda to Aurora DSQL.

666
00:35:01,770 --> 00:35:02,760
First of all,

667
00:35:02,760 --> 00:35:06,930
you need to pass the Aurora
DSQL cluster a variable,

668
00:35:06,930 --> 00:35:07,830
environment variable.

669
00:35:07,830 --> 00:35:09,090
This is the cluster ID.

670
00:35:09,090 --> 00:35:14,090
You create the Aurora
cluster, DSQL cluster,

671
00:35:14,190 --> 00:35:15,840
and then you can grab the cluster id,

672
00:35:15,840 --> 00:35:17,430
you can just provide

673
00:35:17,430 --> 00:35:20,553
as a default value
environment variable here.

674
00:35:21,510 --> 00:35:23,430
And then you can pass it

675
00:35:23,430 --> 00:35:26,220
to create Aurora DSQL cluster endpoint,

676
00:35:26,220 --> 00:35:28,590
which is kind of, you
can use this variable,

677
00:35:28,590 --> 00:35:31,950
it always follows the same
pattern here to create this.

678
00:35:31,950 --> 00:35:34,643
And I export that as
an environment variable

679
00:35:34,643 --> 00:35:37,743
for all Lambda functions
of my application.

680
00:35:39,060 --> 00:35:41,610
And as you see below,
this is the Java code.

681
00:35:41,610 --> 00:35:43,443
I simply grab it by System getenv,

682
00:35:44,490 --> 00:35:46,340
and then I use it to create this JDBC,

683
00:35:46,340 --> 00:35:51,030
or LJDBC as a protocol for
Java to talk to database,

684
00:35:51,030 --> 00:35:52,470
to relational database.

685
00:35:52,470 --> 00:35:54,990
So there already DSQL cluster endpoint

686
00:35:54,990 --> 00:35:57,150
that I passed as environment variable

687
00:35:57,150 --> 00:35:59,313
as a kind of substring of that.

688
00:36:00,360 --> 00:36:02,970
And what I need to decide also to give

689
00:36:02,970 --> 00:36:06,420
is the authorization stuff.

690
00:36:06,420 --> 00:36:09,390
So, here, I simply allow Lambda function

691
00:36:09,390 --> 00:36:12,660
to talk to the Aurora DSQL,

692
00:36:12,660 --> 00:36:15,000
and here I need to provide resource ARN

693
00:36:15,000 --> 00:36:17,160
in this Aurora cluster,

694
00:36:17,160 --> 00:36:22,160
and cluster ID is also kind of
used as a variable for that.

695
00:36:24,719 --> 00:36:26,790
So, pretty generic logic.

696
00:36:26,790 --> 00:36:30,030
Now let's look into the Java example.

697
00:36:30,030 --> 00:36:32,100
So basically it's a good practice

698
00:36:32,100 --> 00:36:35,250
to use and instantiate

699
00:36:35,250 --> 00:36:40,250
the so-called server-side
connection pooling.

700
00:36:40,530 --> 00:36:42,809
And just not only for Java,

701
00:36:42,809 --> 00:36:44,820
there are solutions for Python,

702
00:36:44,820 --> 00:36:46,230
for all programming languages.

703
00:36:46,230 --> 00:36:47,880
The reason that it still makes sense,

704
00:36:47,880 --> 00:36:51,900
because the creating of the
connection is still not free,

705
00:36:51,900 --> 00:36:53,760
and the connection might expire

706
00:36:53,760 --> 00:36:56,520
and may be kind of fetched once again.

707
00:36:56,520 --> 00:36:58,680
So it's still a good practice to use that,

708
00:36:58,680 --> 00:37:01,380
and I use it for Java
Hikari data source pool,

709
00:37:01,380 --> 00:37:04,440
and I create that and
say how many connections,

710
00:37:04,440 --> 00:37:06,900
you can even say one
connection in the pool,

711
00:37:06,900 --> 00:37:09,510
because basically it's a single thread,

712
00:37:09,510 --> 00:37:11,250
or single-process application,

713
00:37:11,250 --> 00:37:15,420
and that's why Lambda only needs
one connection just a time,

714
00:37:15,420 --> 00:37:18,003
and then it does the thing sequentially.

715
00:37:19,830 --> 00:37:21,720
So I create this here,

716
00:37:21,720 --> 00:37:24,270
and this is the logic
to get the connection.

717
00:37:24,270 --> 00:37:26,070
Yeah, you see here,

718
00:37:26,070 --> 00:37:28,230
in both places what we are doing here,

719
00:37:28,230 --> 00:37:30,990
we are fetching the token,

720
00:37:30,990 --> 00:37:33,420
and set it as a password.

721
00:37:33,420 --> 00:37:36,990
And there is Aurora DSQL SDK,

722
00:37:36,990 --> 00:37:40,920
which does the same what
I showed you with AWS CLI.

723
00:37:40,920 --> 00:37:42,660
This is basically you need this

724
00:37:42,660 --> 00:37:45,750
to create this generated
auth token request,

725
00:37:45,750 --> 00:37:48,540
provide endpoint, time to live,

726
00:37:48,540 --> 00:37:52,740
and then this token will be
generated for you each time,

727
00:37:52,740 --> 00:37:54,210
and you need to pass it.

728
00:37:54,210 --> 00:37:55,800
What you already see here,

729
00:37:55,800 --> 00:38:00,800
we need to fetch this token in two places.

730
00:38:00,810 --> 00:38:05,700
First, creating the connection pooling,

731
00:38:05,700 --> 00:38:08,040
and second, each time
that we get the connection

732
00:38:08,040 --> 00:38:09,570
to talk to the database.

733
00:38:09,570 --> 00:38:11,970
And it seems like a boilerplate code,

734
00:38:11,970 --> 00:38:16,230
and that's why AWS team improved that,

735
00:38:16,230 --> 00:38:17,370
they started with Java,

736
00:38:17,370 --> 00:38:19,429
but now also for other languages

737
00:38:19,429 --> 00:38:21,840
they released the code connectors.

738
00:38:21,840 --> 00:38:24,000
And these connectors help you

739
00:38:24,000 --> 00:38:26,820
to handle this token generation
automatically for you,

740
00:38:26,820 --> 00:38:28,710
so you don't need that.

741
00:38:28,710 --> 00:38:31,256
And if you looked in the previous slide,

742
00:38:31,256 --> 00:38:36,256
that was the only dependency to AWS SDK,

743
00:38:37,260 --> 00:38:39,270
is to fetch this token.

744
00:38:39,270 --> 00:38:42,150
All other code is simple JDBC code.

745
00:38:42,150 --> 00:38:44,790
So simply, you can reuse
that what you already had,

746
00:38:44,790 --> 00:38:46,650
for example, if you
migrate to the database.

747
00:38:46,650 --> 00:38:51,650
There was no dependency to DSQL.

748
00:38:52,020 --> 00:38:53,490
And by removing that,

749
00:38:53,490 --> 00:38:56,670
we can remove all dependencies to DSQL,

750
00:38:56,670 --> 00:38:58,230
so there is no password management,

751
00:38:58,230 --> 00:39:00,750
and now it works since
last week also for Python,

752
00:39:00,750 --> 00:39:03,960
for Python, from PG, and PG2,

753
00:39:03,960 --> 00:39:08,187
and also for Node.js,
there is Node.js, Postgres,

754
00:39:08,187 --> 00:39:11,340
and Postgres JS driver.

755
00:39:11,340 --> 00:39:13,170
So how does it look?

756
00:39:13,170 --> 00:39:15,330
We need to add the dependency

757
00:39:15,330 --> 00:39:18,210
to this connector there in
our dependency management,

758
00:39:18,210 --> 00:39:22,140
but now our code doesn't
have any dependency to DSQL,

759
00:39:22,140 --> 00:39:24,510
it's a pure JDBC, pure Java code

760
00:39:24,510 --> 00:39:26,160
how we talk to the application.

761
00:39:26,160 --> 00:39:27,330
You see we simply,

762
00:39:27,330 --> 00:39:30,480
we don't have this get
authenticated auth token,

763
00:39:30,480 --> 00:39:32,670
and we don't have set of token.

764
00:39:32,670 --> 00:39:35,070
But how our application understands

765
00:39:35,070 --> 00:39:37,650
that this connector should be used?

766
00:39:37,650 --> 00:39:40,770
The whole magic is in this red line.

767
00:39:40,770 --> 00:39:45,600
We additionally say it's
AWS DSQL in the path,

768
00:39:45,600 --> 00:39:48,000
and with that, this
connector will be triggered,

769
00:39:48,000 --> 00:39:50,790
instead of pure JDBC Postgres driver.

770
00:39:50,790 --> 00:39:52,710
So this is kind of the only dependency,

771
00:39:52,710 --> 00:39:54,840
this explicit dependency in the string

772
00:39:54,840 --> 00:39:58,350
which says that this
connector should do the work,

773
00:39:58,350 --> 00:40:01,860
and it will fetch the auth token

774
00:40:01,860 --> 00:40:04,950
and place it instead
of the password for us.

775
00:40:04,950 --> 00:40:07,860
So, huge improvement of, I would say,

776
00:40:07,860 --> 00:40:09,843
readability of the code.

777
00:40:11,160 --> 00:40:13,980
So what are other benefits of DSQL?

778
00:40:13,980 --> 00:40:15,780
It works really well

779
00:40:15,780 --> 00:40:20,780
with familiar object
relation mapping frameworks.

780
00:40:20,910 --> 00:40:22,500
You probably all use them,

781
00:40:22,500 --> 00:40:24,000
you have some entity, like order,

782
00:40:24,000 --> 00:40:26,250
you can annotate them the properties,

783
00:40:26,250 --> 00:40:30,990
like order ID with the
column in the database,

784
00:40:30,990 --> 00:40:33,030
and simply save the stuff.

785
00:40:33,030 --> 00:40:34,410
And it works with all of them,

786
00:40:34,410 --> 00:40:37,440
for example, for Java, the
Hibernate is a popular one.

787
00:40:37,440 --> 00:40:42,440
For Python developers,
SQLAlchemy, or Django.

788
00:40:43,320 --> 00:40:45,120
Yeah, and it will all works with them,

789
00:40:45,120 --> 00:40:47,073
so we don't need to change our code.

790
00:40:49,080 --> 00:40:51,900
Let's talk briefly about
performance of this database,

791
00:40:51,900 --> 00:40:54,780
because Oleksii explained
the architecture,

792
00:40:54,780 --> 00:40:56,460
and it could sound like a magic.

793
00:40:56,460 --> 00:40:57,390
So the question is,

794
00:40:57,390 --> 00:40:59,340
just is it performant database,

795
00:40:59,340 --> 00:41:03,150
or it's all this magic
may impact performance?

796
00:41:03,150 --> 00:41:03,983
So first of all,

797
00:41:03,983 --> 00:41:07,470
my goal was to measure
performance from Lambda function.

798
00:41:07,470 --> 00:41:09,960
Yeah so, I measured performance
of the Lambda function

799
00:41:09,960 --> 00:41:14,960
which talks to the DSQL so
the SQL time is kind of there.

800
00:41:17,310 --> 00:41:19,230
I used Java as a programming language,

801
00:41:19,230 --> 00:41:21,660
I only focus on measuring
warm start times,

802
00:41:21,660 --> 00:41:24,510
I didn't focus on the cold
start time of the runtime,

803
00:41:24,510 --> 00:41:27,090
like Java, it has implications,
but it wasn't the goal.

804
00:41:27,090 --> 00:41:30,870
That's why I only also
focused on 99th percentile,

805
00:41:30,870 --> 00:41:34,020
just to exclude the impact
of the programming language.

806
00:41:34,020 --> 00:41:35,460
And I did the performance measurement

807
00:41:35,460 --> 00:41:38,793
for single and multi-region cluster.

808
00:41:39,720 --> 00:41:40,920
What was not the goal?

809
00:41:40,920 --> 00:41:41,910
That's very important.

810
00:41:41,910 --> 00:41:44,730
I didn't want to compare the
performance of the databases

811
00:41:44,730 --> 00:41:45,873
that we talked about.

812
00:41:46,742 --> 00:41:50,370
RDS, DynamoDB, Aurora
Provisioned, Serverless v2,

813
00:41:50,370 --> 00:41:52,897
because it deserves the whole talk,

814
00:41:52,897 --> 00:41:54,840
and sometimes you cannot compare

815
00:41:54,840 --> 00:41:58,290
like DynamoDB with relational databases.

816
00:41:58,290 --> 00:42:00,450
And I didn't want to test
the performance at scale,

817
00:42:00,450 --> 00:42:03,870
just not putting 10,000
transaction per second,

818
00:42:03,870 --> 00:42:07,680
or just queries per second,

819
00:42:07,680 --> 00:42:09,780
and I didn't want to test
performance when scaling,

820
00:42:09,780 --> 00:42:14,130
I don't have an impact
how storage is scaled,

821
00:42:14,130 --> 00:42:15,530
Query Processor, Adjudicator,

822
00:42:15,530 --> 00:42:17,610
it all happens behind the scenes.

823
00:42:17,610 --> 00:42:21,120
So my goal was just to do some basic stuff

824
00:42:21,120 --> 00:42:22,473
and see how quick that is.

825
00:42:23,520 --> 00:42:27,540
So, I showed you this
application with ordering.

826
00:42:27,540 --> 00:42:29,490
The database structure is very simple,

827
00:42:29,490 --> 00:42:30,930
I only have two tables,

828
00:42:30,930 --> 00:42:33,090
order, and order items.

829
00:42:33,090 --> 00:42:37,110
And order has order id as a primary key,

830
00:42:37,110 --> 00:42:41,280
it has created timestamp
values, and such a thing,

831
00:42:41,280 --> 00:42:45,439
and order items has product id, and so on.

832
00:42:45,439 --> 00:42:48,510
And I have two indexes on the items table,

833
00:42:48,510 --> 00:42:49,500
like order id,

834
00:42:49,500 --> 00:42:51,750
because I fetch items by order id,

835
00:42:51,750 --> 00:42:54,810
and also I have one
index on the order table,

836
00:42:54,810 --> 00:42:58,170
on the timestamp, because
I fetch the results,

837
00:42:58,170 --> 00:43:01,560
like timestamp between
one day and another day.

838
00:43:01,560 --> 00:43:03,300
So, very simple structure,

839
00:43:03,300 --> 00:43:05,010
I don't have user table,

840
00:43:05,010 --> 00:43:06,150
I don't have product table,

841
00:43:06,150 --> 00:43:07,020
only two tables.

842
00:43:07,020 --> 00:43:09,540
But the first measurement was,

843
00:43:09,540 --> 00:43:14,100
I am doing like, creation of
one order containing two items.

844
00:43:14,100 --> 00:43:17,340
So, basically, these are
three inserts in there

845
00:43:17,340 --> 00:43:18,180
in one transaction,

846
00:43:18,180 --> 00:43:20,100
one insert into order,

847
00:43:20,100 --> 00:43:22,023
and two inserts into order item.

848
00:43:22,890 --> 00:43:24,450
If you see the results,

849
00:43:24,450 --> 00:43:26,610
let's look into this Lila one, it's p90.

850
00:43:26,610 --> 00:43:29,272
90% of the cases, we have that result

851
00:43:29,272 --> 00:43:34,272
that is not kind of worse than that.

852
00:43:34,320 --> 00:43:39,030
And you see, it's just for
p90, for single-region cluster,

853
00:43:39,030 --> 00:43:40,890
20 milliseconds or something around,

854
00:43:40,890 --> 00:43:42,150
and for-multi region cluster,

855
00:43:42,150 --> 00:43:44,940
40 milliseconds for three selects.

856
00:43:44,940 --> 00:43:47,040
So, considering a single-region cluster,

857
00:43:47,040 --> 00:43:50,043
it's kind of seven
milliseconds per insert,

858
00:43:50,940 --> 00:43:54,390
which is, I would say okay,
it's comparable to DynamoDB.

859
00:43:54,390 --> 00:43:55,590
For multi-region cluster,

860
00:43:55,590 --> 00:43:57,090
it needs to commit in another region,

861
00:43:57,090 --> 00:44:00,270
so the performance is
kind of twice as slow,

862
00:44:00,270 --> 00:44:01,530
which is normal,

863
00:44:01,530 --> 00:44:03,123
in case you need multi-region.

864
00:44:04,020 --> 00:44:07,110
Another measurement was get order by id,

865
00:44:07,110 --> 00:44:08,580
and then get two items.

866
00:44:08,580 --> 00:44:10,590
I can do it with one join, no problem,

867
00:44:10,590 --> 00:44:12,278
I wanted just to do two joins,

868
00:44:12,278 --> 00:44:15,270
without joins, like
getting the order by ID,

869
00:44:15,270 --> 00:44:16,140
and with that ID,

870
00:44:16,140 --> 00:44:19,560
then getting two items
with another select.

871
00:44:19,560 --> 00:44:21,300
So, there are two selects,

872
00:44:21,300 --> 00:44:23,880
and you see then, p90, Lila 1,

873
00:44:23,880 --> 00:44:28,880
12 milliseconds for two
selects by using primary key

874
00:44:29,520 --> 00:44:30,690
for getting the order,

875
00:44:30,690 --> 00:44:33,780
and then index for
getting the order items.

876
00:44:33,780 --> 00:44:35,550
You see that the performance is the same

877
00:44:35,550 --> 00:44:38,070
for single and multi-region cluster

878
00:44:38,070 --> 00:44:39,720
because it's the read operation.

879
00:44:39,720 --> 00:44:41,370
And the read operation is always local,

880
00:44:41,370 --> 00:44:43,830
it always uses the local endpoint,

881
00:44:43,830 --> 00:44:45,510
so the performance of all reads

882
00:44:45,510 --> 00:44:48,480
is the same for single
and multi-region cluster.

883
00:44:48,480 --> 00:44:51,565
So I would say two
selects, 12 milliseconds,

884
00:44:51,565 --> 00:44:53,430
a good use case.

885
00:44:53,430 --> 00:44:55,290
This is a more sophisticated,

886
00:44:55,290 --> 00:44:58,050
just get orders by created date range,

887
00:44:58,050 --> 00:45:01,110
and I intentionally limited the query

888
00:45:01,110 --> 00:45:04,200
that it returns only 100 orders maximum,

889
00:45:04,200 --> 00:45:06,720
and then for each order,
it gets the items,

890
00:45:06,720 --> 00:45:10,200
and I have only two
different items for each row.

891
00:45:10,200 --> 00:45:11,310
So, basic for each order.

892
00:45:11,310 --> 00:45:13,860
So, basically, I have 101 selects,

893
00:45:13,860 --> 00:45:15,570
one select for order,

894
00:45:15,570 --> 00:45:19,287
and then for each of 100
orders, I get two items.

895
00:45:19,287 --> 00:45:21,360
And you see, 100 selects,

896
00:45:21,360 --> 00:45:26,360
p90, 200 milliseconds, or 250
milliseconds for 101 selects.

897
00:45:27,240 --> 00:45:30,663
I think it's a very compatible result.

898
00:45:32,370 --> 00:45:33,900
Another thing I wanted to test,

899
00:45:33,900 --> 00:45:37,320
because the specification says

900
00:45:37,320 --> 00:45:39,990
there is no cold start of the database.

901
00:45:39,990 --> 00:45:41,640
And you can measure it.

902
00:45:41,640 --> 00:45:43,890
I did that, so my goal was I did not,

903
00:45:43,890 --> 00:45:47,010
I created single multi-region cluster,

904
00:45:47,010 --> 00:45:49,050
and did some operations,

905
00:45:49,050 --> 00:45:52,410
and then I didn't touch it for three days.

906
00:45:52,410 --> 00:45:54,600
And then I did the first select

907
00:45:54,600 --> 00:45:55,710
via the console just,

908
00:45:55,710 --> 00:45:56,940
not via the Lambda function

909
00:45:56,940 --> 00:45:59,100
because it has performance implication,

910
00:45:59,100 --> 00:46:02,640
I simply just used psql and
said, select order by ID,

911
00:46:02,640 --> 00:46:04,500
and some existing order.

912
00:46:04,500 --> 00:46:07,980
And I just did it four
or five times the same,

913
00:46:07,980 --> 00:46:12,090
just three days, pause, and then select.

914
00:46:12,090 --> 00:46:12,923
What you see here,

915
00:46:12,923 --> 00:46:17,280
the medium latency in this case

916
00:46:17,280 --> 00:46:20,490
was for single-region
cluster, 165 milliseconds,

917
00:46:20,490 --> 00:46:23,550
for multi-region cluster,
169 milliseconds.

918
00:46:23,550 --> 00:46:25,410
So, of course there is a small latency,

919
00:46:25,410 --> 00:46:28,260
because things needs to be warmed up

920
00:46:28,260 --> 00:46:29,310
if I don't use something.

921
00:46:29,310 --> 00:46:30,930
Something needs to be persisted,

922
00:46:30,930 --> 00:46:32,100
but it's not noticeable.

923
00:46:32,100 --> 00:46:35,438
It's not 15 seconds like
Aurora Serverless v2,

924
00:46:35,438 --> 00:46:37,500
it's just one time for the first select,

925
00:46:37,500 --> 00:46:41,550
or first operation, 160 milliseconds.

926
00:46:41,550 --> 00:46:44,580
So I would say no cold
starts of the database,

927
00:46:44,580 --> 00:46:45,963
it's simply not there.

928
00:46:47,160 --> 00:46:50,340
So now let's talk about
there are also constraints,

929
00:46:50,340 --> 00:46:52,315
and you need to understand them

930
00:46:52,315 --> 00:46:57,150
because that might have
certain implications.

931
00:46:57,150 --> 00:47:00,420
So let's first start with quotas.

932
00:47:00,420 --> 00:47:03,030
Each AWS service has its quotas.

933
00:47:03,030 --> 00:47:04,127
And there is even service,

934
00:47:04,127 --> 00:47:06,150
an AWS service called Service Quotas,

935
00:47:06,150 --> 00:47:08,430
where you can go there,
select the service,

936
00:47:08,430 --> 00:47:11,100
and you can see these
quotas or constraints.

937
00:47:11,100 --> 00:47:13,170
Yeah? And DSQL is no exception.

938
00:47:13,170 --> 00:47:14,940
So there are some important ones

939
00:47:14,940 --> 00:47:16,740
that I would like to raise the attention.

940
00:47:16,740 --> 00:47:18,750
Maximum number of connections,

941
00:47:18,750 --> 00:47:21,690
it's by default 10,000.

942
00:47:21,690 --> 00:47:22,830
It's a huge number,

943
00:47:22,830 --> 00:47:24,420
and you can even adjust it.

944
00:47:24,420 --> 00:47:26,273
Yeah, all other databases,

945
00:47:26,273 --> 00:47:29,370
relational that we talked
about, don't have that value,

946
00:47:29,370 --> 00:47:30,600
and you cannot raise it.

947
00:47:30,600 --> 00:47:32,700
So it scales beautifully.

948
00:47:32,700 --> 00:47:34,020
Yeah, there is no problem

949
00:47:34,020 --> 00:47:36,660
just to have 10,000 Lambda functions

950
00:47:36,660 --> 00:47:38,610
talking to it in parallel
to this database,

951
00:47:38,610 --> 00:47:41,550
it's just due to the
architecture of querying process,

952
00:47:41,550 --> 00:47:43,443
it just scales beautifully.

953
00:47:44,310 --> 00:47:47,997
Maximum transaction time
is five minutes, maximum.

954
00:47:47,997 --> 00:47:51,060
Oleksii will be talking why
those limits are as they are,

955
00:47:51,060 --> 00:47:52,880
but you need to think of it

956
00:47:52,880 --> 00:47:56,940
as just mostly for the
microservice application.

957
00:47:56,940 --> 00:47:58,590
So in case you have some analytics job

958
00:47:58,590 --> 00:48:01,050
that needs to run more than five minutes,

959
00:48:01,050 --> 00:48:04,050
probably it's not the best
fit for this database.

960
00:48:04,050 --> 00:48:07,320
But for certain normal
microservice type of things,

961
00:48:07,320 --> 00:48:09,780
five minutes for transaction
seems to be okay.

962
00:48:09,780 --> 00:48:12,582
Maximum connection duration is 60 minutes,

963
00:48:12,582 --> 00:48:14,190
so one hour.

964
00:48:14,190 --> 00:48:16,920
In the case you designed a
server-side connection pool,

965
00:48:16,920 --> 00:48:20,370
you define the time to
live of the connection,

966
00:48:20,370 --> 00:48:23,610
please define it no
longer than 60 minutes.

967
00:48:23,610 --> 00:48:25,350
Because you will be disconnected,

968
00:48:25,350 --> 00:48:26,670
and in case you define lower,

969
00:48:26,670 --> 00:48:28,410
the pool will take care of this.

970
00:48:28,410 --> 00:48:30,720
And the same if you're
talking through the database

971
00:48:30,720 --> 00:48:33,600
through psql, after one hour
you will be disconnected

972
00:48:33,600 --> 00:48:34,920
and need another token.

973
00:48:34,920 --> 00:48:36,210
Yeah, you need to grab another token,

974
00:48:36,210 --> 00:48:39,000
then you are in for one hour.

975
00:48:39,000 --> 00:48:41,520
Maximum number of rows
change in the transaction,

976
00:48:41,520 --> 00:48:45,240
in one transaction, is only 3,000 rows.

977
00:48:45,240 --> 00:48:48,990
And by changing, I mean delete and update.

978
00:48:48,990 --> 00:48:50,910
So in case you have such a use case,

979
00:48:50,910 --> 00:48:55,410
like I would like to have
to delete one time per year

980
00:48:55,410 --> 00:48:58,200
all orders and items
older than five years,

981
00:48:58,200 --> 00:49:01,020
because I don't need them from
the accounting point of view,

982
00:49:01,020 --> 00:49:04,080
you need to think about how
to divide the job into parts

983
00:49:04,080 --> 00:49:07,530
so that no more than 3,000
rows will be changed.

984
00:49:07,530 --> 00:49:10,680
That brings you just to think
from the backend perspective

985
00:49:10,680 --> 00:49:11,850
how to divide it.

986
00:49:11,850 --> 00:49:13,230
Yeah, because normally you would say,

987
00:49:13,230 --> 00:49:16,393
delete from timestamp,

988
00:49:16,393 --> 00:49:19,050
you will get an exception
if the number of rows

989
00:49:19,050 --> 00:49:20,400
will be more than 3,000.

990
00:49:20,400 --> 00:49:21,423
The same for update.

991
00:49:22,710 --> 00:49:26,608
Maximum size of modified data
in one transaction, 10 MiB,

992
00:49:26,608 --> 00:49:27,441
so you need to think,

993
00:49:27,441 --> 00:49:28,950
just if it's normal data,

994
00:49:28,950 --> 00:49:31,263
then you probably will not hit that limit.

995
00:49:32,400 --> 00:49:34,080
There are also other things

996
00:49:34,080 --> 00:49:35,460
that are currently not supported,

997
00:49:35,460 --> 00:49:37,050
and I will stress that "currently,"

998
00:49:37,050 --> 00:49:39,840
there are no reason why
it cannot be supported,

999
00:49:39,840 --> 00:49:41,490
it might come every day,

1000
00:49:41,490 --> 00:49:44,340
but until today, it's the case.

1001
00:49:44,340 --> 00:49:46,680
No support for JSON and JSONB.

1002
00:49:46,680 --> 00:49:48,660
Yeah, so it's not 100%,

1003
00:49:48,660 --> 00:49:50,010
it's Postgres-compatible,

1004
00:49:50,010 --> 00:49:53,100
but DSQL doesn't support all features.

1005
00:49:53,100 --> 00:49:55,740
There are no columns with default values.

1006
00:49:55,740 --> 00:49:57,750
There are no temporary tables.

1007
00:49:57,750 --> 00:50:00,720
There are no triggers currently.

1008
00:50:00,720 --> 00:50:04,080
Personally, I used them a lot previously,

1009
00:50:04,080 --> 00:50:07,080
but now it just brings the
business logic to the trigger,

1010
00:50:07,080 --> 00:50:08,670
it's difficult to refactor.

1011
00:50:08,670 --> 00:50:12,120
But I use for example triggers
to set the timestamps to now,

1012
00:50:12,120 --> 00:50:13,530
as a default value.

1013
00:50:13,530 --> 00:50:16,320
It was kind of one of
my favorite use cases.

1014
00:50:16,320 --> 00:50:18,420
I can do it in the backend
as well, but anyway.

1015
00:50:18,420 --> 00:50:20,040
There are no sequences,

1016
00:50:20,040 --> 00:50:21,540
that's unusual.

1017
00:50:21,540 --> 00:50:25,650
You can use time-based UUID
version 7 instead of this,

1018
00:50:25,650 --> 00:50:27,090
this is 36-digit number,

1019
00:50:27,090 --> 00:50:32,043
and version seven, it supports
that you can order by time.

1020
00:50:33,090 --> 00:50:34,080
And sometimes it's okay,

1021
00:50:34,080 --> 00:50:38,460
for example, you can design
order item IDs like your UID,

1022
00:50:38,460 --> 00:50:40,800
because you don't expose
them, that's fine.

1023
00:50:40,800 --> 00:50:42,300
But for the things like order ID

1024
00:50:42,300 --> 00:50:44,550
that you would like to expose via email,

1025
00:50:44,550 --> 00:50:46,627
and somebody can call
the call center and say,

1026
00:50:46,627 --> 00:50:48,000
"Something is wrong with my order,"

1027
00:50:48,000 --> 00:50:51,090
they probably don't want
to say a 36-digit number.

1028
00:50:51,090 --> 00:50:53,850
So you need to think how to
map in this rare situation

1029
00:50:53,850 --> 00:50:56,430
to the normal value, yeah?

1030
00:50:56,430 --> 00:50:58,050
Maybe they will be support later.

1031
00:50:58,050 --> 00:50:59,280
There are no partitions,

1032
00:50:59,280 --> 00:51:01,140
in case you have multi-tenant application,

1033
00:51:01,140 --> 00:51:02,880
it might be interesting, yeah?

1034
00:51:02,880 --> 00:51:04,950
In case not, probably you don't use it.

1035
00:51:04,950 --> 00:51:08,970
And you can use only
functions written in SQL,

1036
00:51:08,970 --> 00:51:10,230
which is probably fine.

1037
00:51:10,230 --> 00:51:11,895
Yeah, you can use probably C++,

1038
00:51:11,895 --> 00:51:13,350
but it's not supported.

1039
00:51:13,350 --> 00:51:15,483
But I think SQL is okay.

1040
00:51:17,130 --> 00:51:19,020
And other things that are not supported,

1041
00:51:19,020 --> 00:51:20,070
constraints or commands.

1042
00:51:20,070 --> 00:51:23,460
Foreign keys, that's something
really unusual probably.

1043
00:51:23,460 --> 00:51:24,660
You can do your joins,

1044
00:51:24,660 --> 00:51:26,490
but there is no foreign key support

1045
00:51:26,490 --> 00:51:29,190
which will ensure the data integrity.

1046
00:51:29,190 --> 00:51:30,720
And that might be crucial,

1047
00:51:30,720 --> 00:51:32,400
because I like foreign keys,

1048
00:51:32,400 --> 00:51:35,100
and I sometimes, or nearly
always, I would say,

1049
00:51:35,100 --> 00:51:37,980
define something on delete cascade there.

1050
00:51:37,980 --> 00:51:39,360
For example, if I delete the order,

1051
00:51:39,360 --> 00:51:40,890
I would like that all order items

1052
00:51:40,890 --> 00:51:42,180
will be deleted automatically,

1053
00:51:42,180 --> 00:51:45,450
and I can do this with
foreign key on delete cascade.

1054
00:51:45,450 --> 00:51:46,890
It's not supported.

1055
00:51:46,890 --> 00:51:47,820
Another way to do it

1056
00:51:47,820 --> 00:51:49,290
is to do it via trigger,

1057
00:51:49,290 --> 00:51:51,000
which is also not supported.

1058
00:51:51,000 --> 00:51:51,833
Yeah?

1059
00:51:51,833 --> 00:51:53,280
So this is then up to you to think

1060
00:51:53,280 --> 00:51:55,380
and write this in the business logic,

1061
00:51:55,380 --> 00:51:56,370
that you delete the order,

1062
00:51:56,370 --> 00:51:59,400
and you delete all order
items in the business logic,

1063
00:51:59,400 --> 00:52:02,400
not relying on foreign key on cascade.

1064
00:52:02,400 --> 00:52:03,390
Currently, yeah?

1065
00:52:03,390 --> 00:52:04,380
There is no truncate,

1066
00:52:04,380 --> 00:52:07,530
like we can delete the
whole data in the table,

1067
00:52:07,530 --> 00:52:08,910
and there is no vacuum,

1068
00:52:08,910 --> 00:52:13,910
but the architecture of DSQL
just doesn't require vacuum

1069
00:52:14,640 --> 00:52:16,560
because of the limitations and constraints

1070
00:52:16,560 --> 00:52:17,640
that we talked about.

1071
00:52:17,640 --> 00:52:20,130
There is also no PGVector support

1072
00:52:20,130 --> 00:52:21,450
in case you would like to use some like,

1073
00:52:21,450 --> 00:52:23,340
something for embeddings and so on,

1074
00:52:23,340 --> 00:52:25,590
this database is currently
not for this use case.

1075
00:52:25,590 --> 00:52:29,820
You can use other
Postgres-compatible databases,

1076
00:52:29,820 --> 00:52:34,410
RDS, Aurora, or OpenSearch
that support that.

1077
00:52:34,410 --> 00:52:38,160
So this is for normal
microservice applications.

1078
00:52:38,160 --> 00:52:41,100
And with that, we have
some final thoughts.

1079
00:52:41,100 --> 00:52:41,933
Oleksii?

1080
00:52:41,933 --> 00:52:44,250
So, once again, these
features might come, yes?

1081
00:52:44,250 --> 00:52:46,080
But now, you need to think about it.

1082
00:52:46,080 --> 00:52:48,264
- Yeah, with all of that,

1083
00:52:48,264 --> 00:52:52,050
let's see what we have
learned today on this session.

1084
00:52:52,050 --> 00:52:54,150
So first thing which is very important,

1085
00:52:54,150 --> 00:52:57,060
that thanks to the Query Processor running

1086
00:52:57,060 --> 00:53:00,180
into the Firecracker microVM,

1087
00:53:00,180 --> 00:53:03,690
we are getting virtually endless scale.

1088
00:53:03,690 --> 00:53:07,170
We are not only able to
scale it to whatever size,

1089
00:53:07,170 --> 00:53:08,550
we can do it fast,

1090
00:53:08,550 --> 00:53:11,841
and these results from the tests,

1091
00:53:11,841 --> 00:53:15,450
the cold start is almost nothing.

1092
00:53:15,450 --> 00:53:17,160
And thanks to the architecture,

1093
00:53:17,160 --> 00:53:21,300
we can adjust different
components of the system

1094
00:53:21,300 --> 00:53:24,720
very dynamically, depends on the workload,

1095
00:53:24,720 --> 00:53:28,680
and clean up resources
after us very effectively.

1096
00:53:28,680 --> 00:53:31,860
And, as in any serverless service,

1097
00:53:31,860 --> 00:53:36,330
we currently have some
constraints that are needed

1098
00:53:36,330 --> 00:53:39,540
to get the better performance
and get the better results.

1099
00:53:39,540 --> 00:53:40,950
So here, for example,

1100
00:53:40,950 --> 00:53:43,900
five-minute length of the transaction

1101
00:53:45,210 --> 00:53:49,410
allows us to get the
better write consistency,

1102
00:53:49,410 --> 00:53:52,937
and get the better performance from them

1103
00:53:52,937 --> 00:53:55,500
when the adjudicators need to decide

1104
00:53:55,500 --> 00:53:57,540
and resolve the conflicts.

1105
00:53:57,540 --> 00:54:00,690
It also helps us to clean up the resources

1106
00:54:00,690 --> 00:54:02,610
using the garbage collector,

1107
00:54:02,610 --> 00:54:05,040
which resulting in the fact that

1108
00:54:05,040 --> 00:54:08,400
now we don't need vacuum by design,

1109
00:54:08,400 --> 00:54:12,330
because all the reference
will be just cleared.

1110
00:54:12,330 --> 00:54:16,260
And with the 3,000 rows
that can be changed

1111
00:54:16,260 --> 00:54:17,670
within the transaction,

1112
00:54:17,670 --> 00:54:20,220
we have the stable performance

1113
00:54:20,220 --> 00:54:24,000
with only one flush of
the buffered memory.

1114
00:54:24,000 --> 00:54:28,320
And to get the best of your usage of DSQL,

1115
00:54:28,320 --> 00:54:32,727
you should create many
connections to your front end,

1116
00:54:34,650 --> 00:54:37,050
to parallel concurrent connections,

1117
00:54:37,050 --> 00:54:39,810
which will result in the
a lot of Query Processors,

1118
00:54:39,810 --> 00:54:42,810
which will basically do
the horizontal scaling.

1119
00:54:42,810 --> 00:54:44,940
You should consider to use small rows

1120
00:54:44,940 --> 00:54:46,770
and small transactions,

1121
00:54:46,770 --> 00:54:50,940
so it will decrease the
load to the Adjudicator

1122
00:54:50,940 --> 00:54:55,530
for deciding which
transactions are in conflict,

1123
00:54:55,530 --> 00:54:57,780
and to resolve this conflict.

1124
00:54:57,780 --> 00:54:59,430
For the same reason,

1125
00:54:59,430 --> 00:55:02,400
you may consider to use separate clusters

1126
00:55:02,400 --> 00:55:03,990
for each of your microservices,

1127
00:55:03,990 --> 00:55:05,910
again, to have less conflict,

1128
00:55:05,910 --> 00:55:08,550
and to have less aborted transactions.

1129
00:55:08,550 --> 00:55:10,890
And if you will find any bottlenecks,

1130
00:55:10,890 --> 00:55:15,120
or you would like to know
like, where the money goes,

1131
00:55:15,120 --> 00:55:17,730
Explain Analyze will help you with this.

1132
00:55:17,730 --> 00:55:18,840
And, of course,

1133
00:55:18,840 --> 00:55:22,620
don't forget about client-side
connection pooling,

1134
00:55:22,620 --> 00:55:25,350
this also will help you
with your performance.

1135
00:55:25,350 --> 00:55:28,473
And with that, I'm passing
the mic to Vadym to wrap up.

1136
00:55:29,490 --> 00:55:32,070
- So, one benefit of Explain Analyze,

1137
00:55:32,070 --> 00:55:34,500
you can use Explain Analyze Verbose,

1138
00:55:34,500 --> 00:55:38,460
and it will also display
you the price, the DPUs now,

1139
00:55:38,460 --> 00:55:42,660
and you can do it in this browser editor.

1140
00:55:42,660 --> 00:55:44,940
We didn't touch about pricing,

1141
00:55:44,940 --> 00:55:47,010
but it helps you just to understand

1142
00:55:47,010 --> 00:55:49,320
how much does it cost to
run that and that query,

1143
00:55:49,320 --> 00:55:50,430
by the way.

1144
00:55:50,430 --> 00:55:53,490
So, okay, I go back.

1145
00:55:53,490 --> 00:55:56,010
So yeah, to wrap up,

1146
00:55:56,010 --> 00:55:58,740
DSQL is very easy to
set up and get started.

1147
00:55:58,740 --> 00:56:00,600
Nearly no infrastructure management,

1148
00:56:00,600 --> 00:56:04,719
multi-region out of support
right from the beginning.

1149
00:56:04,719 --> 00:56:08,220
ACID support is there
completely, as we like,

1150
00:56:08,220 --> 00:56:11,490
especially no eventually
consistency at any scale,

1151
00:56:11,490 --> 00:56:12,540
which is very important,

1152
00:56:12,540 --> 00:56:14,790
any scale for single and
multi-region cluster.

1153
00:56:14,790 --> 00:56:17,760
And high availability is also there.

1154
00:56:17,760 --> 00:56:20,580
Now I will provide you
opinionated comparison

1155
00:56:20,580 --> 00:56:22,710
of the databases that we talked about,

1156
00:56:22,710 --> 00:56:25,482
DynamoDB, RDS, Aurora,
Aurora Serverless v2,

1157
00:56:25,482 --> 00:56:28,770
and now DSQL, but only for
the serverless workload.

1158
00:56:28,770 --> 00:56:29,790
Please understand,

1159
00:56:29,790 --> 00:56:31,380
three stars doesn't mean

1160
00:56:31,380 --> 00:56:33,390
it's three times better than one star,

1161
00:56:33,390 --> 00:56:35,910
but three stars means
it's just a better fit

1162
00:56:35,910 --> 00:56:37,080
for the serverless workload,

1163
00:56:37,080 --> 00:56:38,970
or solves the problem
better than two stars,

1164
00:56:38,970 --> 00:56:41,550
and two stars solves the
problem better than one star.

1165
00:56:41,550 --> 00:56:42,990
So, set-up experience,

1166
00:56:42,990 --> 00:56:45,990
Aurora DSQL, brilliant, like DynamoDB.

1167
00:56:45,990 --> 00:56:48,630
Simply is just quick, and you are there.

1168
00:56:48,630 --> 00:56:51,210
Auto-scaling experience,
up and down, automatically,

1169
00:56:51,210 --> 00:56:53,220
you don't need to do anything

1170
00:56:53,220 --> 00:56:55,710
to decide how much to scale,

1171
00:56:55,710 --> 00:56:57,630
and how much to scale down,

1172
00:56:57,630 --> 00:56:59,670
it's just no cold starts.

1173
00:56:59,670 --> 00:57:00,503
Beautiful.

1174
00:57:00,503 --> 00:57:02,100
ACID support, completely there,

1175
00:57:02,100 --> 00:57:04,290
like for all relational databases.

1176
00:57:04,290 --> 00:57:06,750
No need for connection-side pooling.

1177
00:57:06,750 --> 00:57:08,070
You don't need RDS proxy,

1178
00:57:08,070 --> 00:57:09,789
there is no data API,

1179
00:57:09,789 --> 00:57:11,310
it scales beautifully,

1180
00:57:11,310 --> 00:57:14,280
we can have tens of
thousand Lambda functions

1181
00:57:14,280 --> 00:57:17,160
talking to a database, and it works.

1182
00:57:17,160 --> 00:57:19,290
And yeah, it works with familiar drivers

1183
00:57:19,290 --> 00:57:23,250
and object-relational mapping
framework, as we like.

1184
00:57:23,250 --> 00:57:25,412
But of course, yeah, it looks beautiful,

1185
00:57:25,412 --> 00:57:30,412
like Aurora DSQL kind of
supports every use case.

1186
00:57:30,720 --> 00:57:33,510
There are challenges you
need to be aware of there.

1187
00:57:33,510 --> 00:57:34,950
The first one, as we talked,

1188
00:57:34,950 --> 00:57:38,430
the feature gap to be
100% Postgres-compatible

1189
00:57:38,430 --> 00:57:40,170
is currently quite big.

1190
00:57:40,170 --> 00:57:41,250
Yeah, you need to follow,

1191
00:57:41,250 --> 00:57:44,370
especially if you start now from scratch

1192
00:57:44,370 --> 00:57:45,810
with your application,

1193
00:57:45,810 --> 00:57:47,910
if you know that you can
adopt to this, I think,

1194
00:57:47,910 --> 00:57:48,967
but if you would like to say,

1195
00:57:48,967 --> 00:57:50,970
"I would like to migrate
your application,"

1196
00:57:50,970 --> 00:57:52,710
you will probably run into situation

1197
00:57:52,710 --> 00:57:55,290
that you need sequences,
you need foreign keys.

1198
00:57:55,290 --> 00:57:59,850
Yeah? You run into this 3,000
updates in one transaction.

1199
00:57:59,850 --> 00:58:00,717
You need to rewrite code.

1200
00:58:00,717 --> 00:58:02,553
And it poses the question,

1201
00:58:02,553 --> 00:58:05,520
is it the right time to migrate?

1202
00:58:05,520 --> 00:58:06,390
Yeah?

1203
00:58:06,390 --> 00:58:08,130
But if you start from scratch,

1204
00:58:08,130 --> 00:58:09,480
and you know that you can start,

1205
00:58:09,480 --> 00:58:10,830
and I'm pretty confident

1206
00:58:10,830 --> 00:58:13,170
that the features will come with time.

1207
00:58:13,170 --> 00:58:14,670
And, of course, the service quota

1208
00:58:14,670 --> 00:58:17,580
are a bit restrictive for
serverless database users,

1209
00:58:17,580 --> 00:58:20,340
like five minutes
transaction time, 3,000 rows.

1210
00:58:20,340 --> 00:58:24,840
We know these quotas from DynamoDB,

1211
00:58:24,840 --> 00:58:25,890
and we appreciate them,

1212
00:58:25,890 --> 00:58:27,270
because they are there for a reason,

1213
00:58:27,270 --> 00:58:28,830
to get the best performance.

1214
00:58:28,830 --> 00:58:31,260
But for relational database users,

1215
00:58:31,260 --> 00:58:34,170
more or less would like to do
everything with the database

1216
00:58:34,170 --> 00:58:35,670
without any limitations,

1217
00:58:35,670 --> 00:58:37,140
which is sometimes also not a good thing,

1218
00:58:37,140 --> 00:58:39,690
because the query
performance will be worse

1219
00:58:39,690 --> 00:58:41,580
if you will join how you would like,

1220
00:58:41,580 --> 00:58:43,530
it just also doesn't work like this.

1221
00:58:43,530 --> 00:58:45,840
And these limitation also are there

1222
00:58:45,840 --> 00:58:49,950
to just deliver the best experience.

1223
00:58:49,950 --> 00:58:51,900
But anyway, you need to know them,

1224
00:58:51,900 --> 00:58:53,040
you need to understand them

1225
00:58:53,040 --> 00:58:54,723
when you design or migrate.

1226
00:58:56,010 --> 00:59:00,570
So this is the QR code
of my GitHub repository

1227
00:59:00,570 --> 00:59:01,770
with the examples,

1228
00:59:01,770 --> 00:59:04,200
yeah, with the code that I have shown you,

1229
00:59:04,200 --> 00:59:06,180
with connectors and without connectors.

1230
00:59:06,180 --> 00:59:07,378
So you can scan,

1231
00:59:07,378 --> 00:59:10,620
or you can write to me,

1232
00:59:10,620 --> 00:59:12,480
you can find me, like on LinkedIn,

1233
00:59:12,480 --> 00:59:17,480
and I will give you the link
to the repository if you wish.

1234
00:59:18,030 --> 00:59:20,070
And for that, thank you very much.

1235
00:59:20,070 --> 00:59:21,600
Don't forget to rate the session.

1236
00:59:21,600 --> 00:59:23,610
- It's very important for us.

1237
00:59:23,610 --> 00:59:24,443
- Yeah.

1238
00:59:24,443 --> 00:59:26,340
To receive the feedback.

1239
00:59:26,340 --> 00:59:28,530
And we are here to answer your questions,

1240
00:59:28,530 --> 00:59:30,960
I think it's kind of the silent sessions,

1241
00:59:30,960 --> 00:59:32,429
but we are here.

1242
00:59:32,429 --> 00:59:33,684
Thank you very much.
- Thank you.

1243
00:59:33,684 --> 00:59:36,851
(audience applauding)

