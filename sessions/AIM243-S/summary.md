# AWS re:Invent 2025 - 在生产环境中运行和扩展 Agentic AI 应用

## 会议概述

本次会议由 HashiCorp 的 Cole Morrison(品牌故事负责人兼首席布道师)和 Kyle Ruddy(产品营销负责人)主讲,主题聚焦于如何在生产环境中运行和扩展 Agentic AI 应用。两位演讲者分别来自软件开发和运维背景,为观众提供了从 DevOps 双重视角看待 AI 工作负载的独特见解。

会议首先回顾了 AI 的历史起源,介绍了 Frank Rosenblatt 在 1950-60 年代开发的感知机(Perceptron)——这是机器学习的雏形。演讲者强调,尽管 AI 技术经历了漫长的发展历程,但其核心原理(如反向传播和神经元网络)早已奠定基础。当前的 AI 热潮虽然喧嚣,但技术本身是真实可行的。会议的核心观点是:**AI 应用本质上只是另一种工作负载**。对于运维人员来说,管理 AI 应用与管理传统应用并无本质区别,关键在于理解模型层(Model Plane)和运维层(Ops Plane)的分离,以及如何应用现有的基础设施管理实践来控制 AI 工作负载的爆炸半径(blast radius)。

演讲者通过实际演示展示了如何使用 HashiCorp 云平台(HCP)和 Terraform 来管理 AI 工作负载,包括在 EKS 集群上部署小型语言模型(SLM),以及如何通过平台化方法实现人工审核循环,确保 AI 生成的基础设施代码符合安全和成本控制要求。会议强调了速度、成本和风险三个关键考量因素,并指出 AI 项目的采用周期比云计算项目更加紧迫和压缩。

## 详细时间线

### 开场与背景介绍
- **00:00:00** - 会议开始,Cole Morrison 和 Kyle Ruddy 介绍自己的背景:Cole 来自软件开发转向运维,Kyle 则是从帮助台起步的系统工程师
- **00:01:30** - 讨论 AI 领域的三类人群:狂热支持者、怀疑论者(包括资深工程师和父母辈),以及长期从事相关研究的专业人士

### AI 历史回顾
- **00:02:45** - 介绍 Frank Rosenblatt 及其在 1950-60 年代开发的感知机(Perceptron)
- **00:03:30** - 讲述 Rosenblatt 使用 IBM 704 大型机,通过 400 个光传感器实现简单的是/否判断
- **00:04:15** - 介绍反向传播(back propagation)概念:机器在做出判断后调整权重
- **00:05:00** - 提到 Rosenblatt 在 AI 寒冬期间的争议性实验(试图在老鼠间转移训练),以及他在 1970 年代早逝,未能看到自己工作的成果

### 核心理念:两个真相
- **00:06:00** - 提出"两个真相":AI 炒作很响亮,但技术本身是真实的
- **00:06:45** - Kyle 分享从怀疑者转变为认可者的过程,承认 AI 在特定工作流程中的价值
- **00:07:30** - 核心问题:AI 在哪里适用?它不是通用智能,也不是卢德派所反对的织布机,而是介于两者之间的颠覆性技术

### 对运维人员的意义
- **00:08:30** - 关键信息:AI 只是另一种工作负载
- **00:09:00** - 讨论爆炸半径(blast radius)的差异:AI 工作负载的影响范围与传统应用不同
- **00:09:45** - 会议结构预告:模型层与运维层、爆炸半径、达到稳定状态的方法

### 模型层(Model Plane)详解
- **00:10:30** - 强调区分"思考层"(大脑/模型)和"运行层"(基础设施)的重要性
- **00:11:15** - 模型层组件介绍:
  - Prompt(提示词)
  - Model(模型本身——"最华丽的自动补全")
  - Reasoning & Tools(推理和工具)
  - Memory(记忆/上下文)
  - Evaluators(评估器)
- **00:12:30** - 介绍常见模式:
  - Supervisor pattern(监督者模式)
  - MCP(Model Context Protocol)服务器
  - RAG(检索增强生成)
  - Retries(重试机制)
- **00:13:15** - 关键观点:对运维人员来说,这些都不重要,因为它只是另一个工作负载

### 运维层(Ops Plane)详解
- **00:13:45** - Kyle 介绍运维层的熟悉组件:网络、计算资源、内存、数据库
- **00:14:30** - 强调这些基础设施组件已经存在十多年,没有改变
- **00:15:00** - 询问现场观众:谁在从事数据科学/软件开发?谁在编写 AI 代码?

### AI 工作负载的特殊要求
- **00:15:45** - 运维层的细微变化:
  - 计算:不仅是 CPU,还需要 GPU
  - 身份:从人类身份扩展到非人类代理(agent)身份
  - 数据库:从 Postgres 到向量数据库
- **00:16:30** - 强调这些只是同类事物的不同风格

### 模型部署选项
- **00:17:00** - 使用托管 API(如 OpenAI、AWS Bedrock):最简单的方式,只需 HTTP 调用
- **00:17:45** - 在现有集群中部署:
  - 需要 GPU 运行时
  - 优势:接近性、更快的访问速度、更容易的访问管理
- **00:18:30** - 独立运行时部署:
  - 在单独的 EKS 集群或 EC2 实例上
  - 优势:关注点分离,更容易推理
  - 劣势:需要管理分离的基础设施
- **00:19:15** - 边缘设备部署:
  - 讨论大型语言模型(LLM)vs 小型语言模型(SLM)
  - 使用案例:制造工厂的质量检测摄像头
  - 示例:IBM Granite 模型仅 3GB,可在边缘设备运行

### 三大考量因素
- **00:20:30** - Kyle 总结 AI 工作负载的三大关键因素:
  - **速度(Speed)**:用户访问资源的速度
  - **成本(Cost)**:维护资源的成本
  - **风险(Risk)**:安全性,防止代理失控并产生高额云成本

### HashiCorp 云平台(HCP)介绍
- **00:21:15** - 介绍 HCP 作为统一控制平面,提供一致的运营模型
- **00:21:45** - 强调平台不仅限于 AWS,还支持 Kubernetes 集群等多种环境
- **00:22:15** - Kyle 将 AI 采用与云采用进行类比:
  - 都经历了标准化和规模化过程
  - HashiCorp 在云转型中一直陪伴客户
- **00:23:00** - 关键差异:AI 项目的时间线大幅压缩,企业需要在本财年或日历年内完成部署

### 实际演示
- **00:23:45** - Cole 切换到演示环境,展示实际应用
- **00:24:15** - 演示应用架构:
  - 前端应用连接到监督代理(supervisor agent)
  - 监督代理使用 AWS Bedrock(大型语言模型)处理人类语言
  - 后台连接到专门的基础设施代理
  - 基础设施代理使用部署在 EKS 上的小型语言模型
- **00:25:30** - 基础设施代理的功能:
  - 理解来自 LLM 的指令
  - 思考基础设施层面的问题
  - 访问 Terraform 部署的上下文信息

### HCP Terraform 详解
- **00:26:00** - Kyle 介绍 HCP Terraform:托管服务,用于集中规划和应用 Terraform 操作
- **00:26:30** - Cole 询问现场观众:谁在使用 Terraform?(大量举手)
- **00:27:00** - Terraform 的优势:
  - 代码形式表示基础设施状态
  - AI 生成的代码可以持久化
  - 便于与 AI 助手(如 Copilot、GPT)协作
- **00:27:45** - HCP Terraform 作为"人工审核循环"(human in the loop):
  - 成本估算
  - 策略检查
  - 确保 AI 生成的代码通过审核才能部署

### 架构深入分析
- **00:28:30** - 展示 EKS 集群架构
- **00:28:45** - Kyle 解释两个节点组:
  - CPU 节点组:运行常规工作负载
  - GPU 节点组:运行模型
- **00:29:15** - Cole 澄清:
  - 如果只使用 Bedrock 等托管服务,可以跳过 GPU 配置
  - 代理本身运行在 CPU 节点上
  - 只有模型需要 GPU
- **00:30:00** - 使用 IBM Granite 的原因:
  - 仅 3GB 大小
  - 可在 AWS 最小的 GPU 实例上运行
  - Cole 提到自己的 MacBook 上也在运行 Granite,电池消耗很小
- **00:30:45** - Kyle 补充:SLM 可以在 Raspberry Pi 上运行,适合低成本、低风险的实验环境

### 确定性 vs 随机性
- **00:31:30** - 返回幻灯片,讨论确定性程序 vs 随机性代理
- **00:32:00** - Kyle 的观点:作为运维人员,偏好确定性路径(从 A 点到 B 点的固定路径)
- **00:32:45** - Cole 解释代理程序的本质:
  - 业务逻辑仍然是"如果 A 使用此工具,如果 B 使用彼工具"
  - **唯一区别:控制流由 LLM 决定,而非程序员硬编码**
  - 给 LLM 提供工具、目标和上下文,让它决定使用哪个工具
- **00:33:45** - 使用"随机性"(stochastic)一词的原因:避免重复使用"非确定性"
- **00:34:15** - 何时选择代理:当需要处理**混乱的输入(messy input)**时
- **00:34:45** - 示例:客户投诉可能有数百种表达方式,代理可以更好地处理这种多样性,而不需要程序员为每种情况编写控制流

### 会议结束
- **00:35:00** - 演示和讨论在此处中断(字幕未完整)

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


核心要点总结:
1. AI 应用本质上只是另一种工作负载,运维人员应使用现有技能管理
2. 区分模型层(思考)和运维层(执行)至关重要
3. 速度、成本、风险是管理 AI 工作负载的三大支柱
4. 小型语言模型为边缘部署和成本优化提供了新选择
5. 平台化方法(如 HCP Terraform)可以为 AI 生成的基础设施提供必要的治理和控制