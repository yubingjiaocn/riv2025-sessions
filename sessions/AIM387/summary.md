# AWS re:Invent 2025 - 构建高质量、成本效益的智能体应用

## 会议概述

本次会议面向数据科学家和AI开发者，重点介绍如何定制模型并大规模部署，以构建高质量且成本效益的智能体应用。会议由AWS SageMaker模型运营和推理高级经理Amit Modi主持，世界级GenAI专家Dan和SGLang联合创始人Ying共同参与。

会议指出了当前企业智能体AI应用面临的主要挑战：缺乏标准化的模型定制工具、推理栈配置复杂、可观测性工具分散、实验到生产的流程断层，以及企业级治理能力不足。为解决这些问题，AWS推出了一系列SageMaker功能，包括模型训练和微调、AI推理、MLflow集成、管道编排和模型注册等核心能力。

## 详细时间线与关键要点

### 0:00-10:00 - 开场和市场趋势分析
- 智能体AI在企业软件应用中的快速采用，预计从2024年的1%增长到2028年的33%
- 大多数应用无法投入生产的关键挑战：
  - 缺乏标准化的模型定制工具
  - 推理栈配置复杂，需要持续优化
  - 可观测性工具分散，难以调试
  - 实验代码难以扩展到生产环境
  - 缺乏企业级治理和合规能力

### 10:00-20:00 - SageMaker核心能力介绍
- SageMaker提供最广泛的模型选择，包括开源模型和Amazon Nova模型
- 支持多种微调配方和训练基础设施选择
- HyperPod集群的自愈能力：自动检查点、节点故障自动恢复
- 微调配方简化了常用模型和技术的定制流程

### 20:00-30:00 - SageMaker AI推理功能
- 支持在托管实例上快速部署开源或微调模型
- 多模型端点：在同一端点部署数百个模型，智能路由确保性能
- 推测解码技术：使用草稿模型预测，基础模型验证，实现高达2.5倍的吞吐量提升
- 支持vLLM、SGLang等流行框架的托管容器

### 30:00-40:00 - MLflow和可观测性
- 推出无服务器MLflow，完全托管且免费
- 实验跟踪：比较训练运行，识别最佳候选模型
- AgentCore集成：自动发送追踪到MLflow，符合开放遥测规范
- 分层视图提供智能体每个步骤的完整可见性

### 40:00-50:00 - 生产化和治理
- SageMaker管道：通过注释将实验代码转换为可扩展的生产工作流
- 内置缓存机制：失败重试时跳过已成功执行的步骤
- 模型注册表：跨账户访问，追踪所有环境中的模型版本
- 端到端血缘关系：完整的训练作业详细信息追踪

### 50:00-62:00 - 实际演示和SGLang亮点
- 医疗保健场景演示：使用Llama-3.2-3B构建临床顾问智能体
- 使用QLora适配进行微调，SGLang容器部署
- Strands SDK和原生MCP集成展示
- SGLang V2功能：分层KV缓存、推测解码V2、预填充-解码分离
- 支持GB200等新硬件的机架级部署优化
- SGLang Diffusion：图像和视频生成加速1.2-6倍