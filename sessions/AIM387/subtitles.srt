1
00:00:00,840 --> 00:00:01,890
- Welcome everyone.

2
00:00:01,890 --> 00:00:04,920
This session is for data scientists

3
00:00:04,920 --> 00:00:07,950
and AI developers that are
looking to customize models

4
00:00:07,950 --> 00:00:11,040
and deploy at scale to build high quality

5
00:00:11,040 --> 00:00:13,533
and cost effective agentic applications.

6
00:00:14,640 --> 00:00:16,590
Let's get started.

7
00:00:16,590 --> 00:00:19,923
We are seeing two big trends
in the market, in the industry.

8
00:00:21,870 --> 00:00:26,490
First, we are seeing rapid
adoption of agentic AI

9
00:00:26,490 --> 00:00:28,290
in enterprise software apps.

10
00:00:28,290 --> 00:00:33,090
This adoption is expected
to grow to 33% by 2028,

11
00:00:33,090 --> 00:00:37,710
from 1% in 2024 to 33% by 2028.

12
00:00:37,710 --> 00:00:42,000
That's a 33x increase in just four years.

13
00:00:42,000 --> 00:00:44,580
To build these agentic applications,

14
00:00:44,580 --> 00:00:48,360
you need models that are high quality,

15
00:00:48,360 --> 00:00:50,283
cost effective and fast,

16
00:00:51,210 --> 00:00:55,470
and customers are increasingly
relying on open weight models

17
00:00:55,470 --> 00:00:57,093
to build out these applications.

18
00:00:59,490 --> 00:01:01,710
Despite huge opportunity

19
00:01:01,710 --> 00:01:05,670
and clear line of sight on how
to build these applications,

20
00:01:05,670 --> 00:01:08,520
we continue to see that
majority of applications

21
00:01:08,520 --> 00:01:09,783
never reach production.

22
00:01:10,800 --> 00:01:13,470
Let's take a look at what
are the key challenges

23
00:01:13,470 --> 00:01:16,443
that customer face which
leads to these failures.

24
00:01:17,640 --> 00:01:21,660
First, customers lack standardized tools

25
00:01:21,660 --> 00:01:24,630
to customize models with
different techniques.

26
00:01:24,630 --> 00:01:27,720
So they end up spending time
building out these workflows,

27
00:01:27,720 --> 00:01:29,040
standardized tools,

28
00:01:29,040 --> 00:01:32,640
which delays the time
to customize the models.

29
00:01:32,640 --> 00:01:35,490
Once the models are customized,

30
00:01:35,490 --> 00:01:38,160
you need an inference
stack to host these models

31
00:01:38,160 --> 00:01:40,740
with the right price performance ratio.

32
00:01:40,740 --> 00:01:43,110
It takes a lot of effort
to find the right instance,

33
00:01:43,110 --> 00:01:45,060
right container configuration,

34
00:01:45,060 --> 00:01:48,750
constantly adapt new inference
optimization techniques

35
00:01:48,750 --> 00:01:50,100
that are coming in

36
00:01:50,100 --> 00:01:52,380
to ensure that your inference stack

37
00:01:52,380 --> 00:01:54,990
is meeting the latest requirements,

38
00:01:54,990 --> 00:01:57,300
which is complex and
requires ton of effort,

39
00:01:57,300 --> 00:02:00,063
which further delays the
timelines for these products.

40
00:02:02,160 --> 00:02:04,530
After the model is deployed,

41
00:02:04,530 --> 00:02:07,500
customers need tools to track

42
00:02:07,500 --> 00:02:10,380
the behavior of these
models and the agents.

43
00:02:10,380 --> 00:02:12,600
Often the observability is fragmented

44
00:02:12,600 --> 00:02:14,070
across two different tools,

45
00:02:14,070 --> 00:02:15,720
which makes root causing

46
00:02:15,720 --> 00:02:18,720
and debugging of the behavior much harder,

47
00:02:18,720 --> 00:02:20,583
further delaying any progress.

48
00:02:21,900 --> 00:02:24,330
And then once you
identify all these issues,

49
00:02:24,330 --> 00:02:25,320
now it's time to take

50
00:02:25,320 --> 00:02:28,710
your model customization
workflow into production.

51
00:02:28,710 --> 00:02:32,190
And often the experimentation
work is done with glue code,

52
00:02:32,190 --> 00:02:33,270
which doesn't scale well

53
00:02:33,270 --> 00:02:35,100
when you try to deploy that to production.

54
00:02:35,100 --> 00:02:37,383
So often these pipelines
need to be rebuilt,

55
00:02:38,280 --> 00:02:41,010
and this disconnect
leads to further delays.

56
00:02:41,010 --> 00:02:45,060
And lastly, to deploy these
models at an enterprise scale,

57
00:02:45,060 --> 00:02:47,190
you need the right governance practices.

58
00:02:47,190 --> 00:02:50,160
You need capabilities to track, audit

59
00:02:50,160 --> 00:02:54,060
and version your models'
generative AI assets

60
00:02:54,060 --> 00:02:57,750
so that you can ensure as
they move from dev to staging

61
00:02:57,750 --> 00:03:00,210
to production environments,
you can keep right track

62
00:03:00,210 --> 00:03:02,640
and ensure the meet compliance
and governance requirements

63
00:03:02,640 --> 00:03:05,283
before they can be
deployed to the customers.

64
00:03:07,140 --> 00:03:08,640
I'm Amit Modi,

65
00:03:08,640 --> 00:03:12,000
senior manager for SageMaker
Model Operations and Inference.

66
00:03:12,000 --> 00:03:13,230
And I have with me Dan,

67
00:03:13,230 --> 00:03:15,690
who's our worldwide specialist for GenAI,

68
00:03:15,690 --> 00:03:18,870
and Ying who leads the
co-creator of SGLang.

69
00:03:18,870 --> 00:03:21,030
In this session we'll do a quick overview

70
00:03:21,030 --> 00:03:22,650
of all these maker capabilities

71
00:03:22,650 --> 00:03:25,260
that can help you address
some of these challenges.

72
00:03:25,260 --> 00:03:27,870
Then Dan will do a demo

73
00:03:27,870 --> 00:03:30,000
that will bring all these
capabilities to life.

74
00:03:30,000 --> 00:03:32,790
And then Ying will talk about
some of the key highlights

75
00:03:32,790 --> 00:03:34,860
for SGLang that helps you host

76
00:03:34,860 --> 00:03:36,603
cost optimized inference at scale.

77
00:03:37,860 --> 00:03:38,880
So let's dive in.

78
00:03:38,880 --> 00:03:41,580
SageMaker offers capabilities to train

79
00:03:41,580 --> 00:03:43,770
and fine tune these models.

80
00:03:43,770 --> 00:03:47,820
SageMaker offers the
broadest selection of models,

81
00:03:47,820 --> 00:03:49,710
all popular open source models,

82
00:03:49,710 --> 00:03:51,240
as well as Amazon Nova models

83
00:03:51,240 --> 00:03:53,100
that you can use to get started.

84
00:03:53,100 --> 00:03:55,200
They meet all the enterprise requirements.

85
00:03:55,200 --> 00:03:57,330
We've already gone through
the security scanning

86
00:03:57,330 --> 00:04:00,540
and so on, so you can quickly get started.

87
00:04:00,540 --> 00:04:04,530
You can choose from the
broadest range of recipes

88
00:04:04,530 --> 00:04:07,083
to fine tune your model
and customize them.

89
00:04:08,040 --> 00:04:10,650
We'll take a bit of,

90
00:04:10,650 --> 00:04:12,450
little bit of a deeper
dive into these recipes

91
00:04:12,450 --> 00:04:14,250
in coming slides.

92
00:04:14,250 --> 00:04:15,720
And you can also choose

93
00:04:15,720 --> 00:04:18,570
from the right training infrastructure

94
00:04:18,570 --> 00:04:19,680
to train these models,

95
00:04:19,680 --> 00:04:21,210
whether you want ephemeral compute

96
00:04:21,210 --> 00:04:22,590
with fully managed training jobs

97
00:04:22,590 --> 00:04:25,440
or you can have persistent
clusters to run large scale jobs

98
00:04:25,440 --> 00:04:26,673
with SageMaker HyperPod.

99
00:04:28,994 --> 00:04:31,950
SageMaker has built in resiliency

100
00:04:31,950 --> 00:04:33,270
into these infrastructure,

101
00:04:33,270 --> 00:04:36,360
which helps you train your models faster.

102
00:04:36,360 --> 00:04:37,260
Let's take a look at

103
00:04:37,260 --> 00:04:39,450
some of the key capabilities of training.

104
00:04:39,450 --> 00:04:41,880
So the first key capability
that SageMaker offers

105
00:04:41,880 --> 00:04:44,370
is when you got these
training jobs in HyperPod,

106
00:04:44,370 --> 00:04:46,890
it constantly checkpoints these jobs.

107
00:04:46,890 --> 00:04:50,640
So in case there is a node
failure on the cluster,

108
00:04:50,640 --> 00:04:54,630
the cluster self heals by
replacing with a good node

109
00:04:54,630 --> 00:04:57,690
and then resumes the training
job from the last checkpoint.

110
00:04:57,690 --> 00:04:59,670
So your work is never lost

111
00:04:59,670 --> 00:05:01,410
and it also accelerates the training

112
00:05:01,410 --> 00:05:03,510
because there are multiple node failures

113
00:05:03,510 --> 00:05:04,530
that happen during the day.

114
00:05:04,530 --> 00:05:06,450
The self-healing cluster makes,

115
00:05:06,450 --> 00:05:09,270
it takes away the burden from
you to identify those nodes,

116
00:05:09,270 --> 00:05:11,580
replace them, resume the job
from the last checkpoint.

117
00:05:11,580 --> 00:05:14,480
All of this is taken care for
you as a managed capability.

118
00:05:15,750 --> 00:05:18,870
Another key capability is
these fine tuning recipes.

119
00:05:18,870 --> 00:05:21,420
So SageMaker offers fine tuning recipes

120
00:05:21,420 --> 00:05:23,310
to customize your model

121
00:05:23,310 --> 00:05:26,160
for most commonly used popular models

122
00:05:26,160 --> 00:05:28,140
and for most common techniques.

123
00:05:28,140 --> 00:05:30,990
You can get started by
putting your data training

124
00:05:30,990 --> 00:05:33,750
and validation data into
the right directories.

125
00:05:33,750 --> 00:05:37,650
Identify the right recipe
for you, for your use case

126
00:05:37,650 --> 00:05:39,990
and then kick off the recipe on either

127
00:05:39,990 --> 00:05:41,490
on the fully managed training jobs

128
00:05:41,490 --> 00:05:42,780
or on the HyperPod clusters,

129
00:05:42,780 --> 00:05:44,780
depending on your computer requirements.

130
00:05:45,900 --> 00:05:48,510
Next, let's take a look
at SageMaker AI inference.

131
00:05:48,510 --> 00:05:50,910
We talked about the key
challenge with inference was

132
00:05:50,910 --> 00:05:53,490
to host cost-effective inference.

133
00:05:53,490 --> 00:05:54,480
You need capabilities

134
00:05:54,480 --> 00:05:57,420
so that you can deploy
your models faster quickly.

135
00:05:57,420 --> 00:06:00,720
SageMaker AI inference offers
capabilities that allow you

136
00:06:00,720 --> 00:06:02,820
to deploy open source

137
00:06:02,820 --> 00:06:05,874
or fine tune models with a couple of steps

138
00:06:05,874 --> 00:06:07,743
onto a managed instance,

139
00:06:08,820 --> 00:06:11,430
either through UI or through SDK.

140
00:06:11,430 --> 00:06:14,250
You can choose to host
any model on any framework

141
00:06:14,250 --> 00:06:17,040
on any different type of infrastructure

142
00:06:17,040 --> 00:06:19,490
to make sure you're getting
the best performance.

143
00:06:20,340 --> 00:06:23,790
SageMaker out of the box
offers high throughput

144
00:06:23,790 --> 00:06:26,220
and low latency for your endpoints,

145
00:06:26,220 --> 00:06:27,750
and we'll take a little bit of deeper dive

146
00:06:27,750 --> 00:06:29,150
into some of these features.

147
00:06:30,594 --> 00:06:33,390
With SageMaker, you can
deploy multiple models

148
00:06:33,390 --> 00:06:36,060
onto the same endpoint.

149
00:06:36,060 --> 00:06:40,230
This ensures that you can
scale the one endpoint

150
00:06:40,230 --> 00:06:41,950
to all your use cases

151
00:06:42,870 --> 00:06:45,663
to drive the maximum
utilization out of GPUs.

152
00:06:46,830 --> 00:06:49,230
The intelligent routing
built into the endpoint

153
00:06:49,230 --> 00:06:51,690
redirects the request to the right model

154
00:06:51,690 --> 00:06:54,480
and ensures there is
no performance penalty.

155
00:06:54,480 --> 00:06:57,240
You can scale up to hundreds
of models in the same endpoint

156
00:06:57,240 --> 00:06:59,090
depending on the memory requirements.

157
00:07:01,114 --> 00:07:05,490
SageMaker also emits metrics
for each of these models.

158
00:07:05,490 --> 00:07:08,250
So you can configure autoscaling policies

159
00:07:08,250 --> 00:07:11,100
for each model separately.

160
00:07:11,100 --> 00:07:13,890
This allows your SageMaker
inference endpoint

161
00:07:13,890 --> 00:07:17,280
to autoscale as the traffic grows.

162
00:07:17,280 --> 00:07:19,980
In this use case, you can see
the foundation model three has

163
00:07:19,980 --> 00:07:21,900
high traffic and it can autoscale only

164
00:07:21,900 --> 00:07:23,400
for the foundation model three

165
00:07:23,400 --> 00:07:25,400
and make sure it meets the requirements.

166
00:07:29,100 --> 00:07:32,130
Next, we recently launched a
capability that also allows you

167
00:07:32,130 --> 00:07:36,393
to get better throughput out
of these, out of these models.

168
00:07:37,500 --> 00:07:39,510
the expectation for
customer for inference is

169
00:07:39,510 --> 00:07:42,450
that they need fast
inference and fast response.

170
00:07:42,450 --> 00:07:46,380
In reality, these LLMs
generate one token at a time,

171
00:07:46,380 --> 00:07:50,310
which makes it really hard to
meet the customer expectation.

172
00:07:50,310 --> 00:07:53,070
We launched speculative decoding recently.

173
00:07:53,070 --> 00:07:55,050
Speculative wording works this way.

174
00:07:55,050 --> 00:07:58,260
So you have a foundation model
and you have a draft model.

175
00:07:58,260 --> 00:08:01,980
The draft model is a smaller
model that does predictions

176
00:08:01,980 --> 00:08:03,990
for the prompt

177
00:08:03,990 --> 00:08:06,150
and the foundation model
validates this prediction

178
00:08:06,150 --> 00:08:07,680
with calculating some probabilities.

179
00:08:07,680 --> 00:08:10,030
It accepts some of the
tokens, it rejects REST.

180
00:08:11,280 --> 00:08:13,620
This is a simplistic
view of how this works.

181
00:08:13,620 --> 00:08:15,810
In reality, you can have draft model

182
00:08:15,810 --> 00:08:18,090
actually produce out multiple variations

183
00:08:18,090 --> 00:08:20,190
that certain foundation
models can choose from.

184
00:08:20,190 --> 00:08:22,260
So let's take a quick look at an example.

185
00:08:22,260 --> 00:08:24,057
In this case, the foundation model

186
00:08:24,057 --> 00:08:26,250
and the draft model get the prompt.

187
00:08:26,250 --> 00:08:29,910
The draft model produces
the next set of tokens.

188
00:08:29,910 --> 00:08:31,740
Foundation model decides which one,

189
00:08:31,740 --> 00:08:33,180
what is the probability for each token

190
00:08:33,180 --> 00:08:35,550
and accepts some of these tokens.

191
00:08:35,550 --> 00:08:37,470
And this is how speculative code occurs.

192
00:08:37,470 --> 00:08:39,630
This leads to latency reduction

193
00:08:39,630 --> 00:08:44,010
and up to 2.5x higher
throughput for your model

194
00:08:44,010 --> 00:08:46,713
without compromising on the accuracy.

195
00:08:48,060 --> 00:08:50,460
And all these capabilities
work out of the box.

196
00:08:50,460 --> 00:08:53,850
So the way it works is you
bring your own data set,

197
00:08:53,850 --> 00:08:57,450
which maps to your traffic patterns

198
00:08:57,450 --> 00:09:00,570
and use that data set to
fine tune the draft model

199
00:09:00,570 --> 00:09:03,330
so the draft model can
do better predictions.

200
00:09:03,330 --> 00:09:05,040
You run SageMaker,

201
00:09:05,040 --> 00:09:07,350
kicks off an asynchronous training job

202
00:09:07,350 --> 00:09:09,300
to fine tune your draft model.

203
00:09:09,300 --> 00:09:10,860
After the draft model is fine tuned,

204
00:09:10,860 --> 00:09:12,720
you can look at the evaluation metrics,

205
00:09:12,720 --> 00:09:15,870
choose to deploy that on
the same SageMaker endpoint

206
00:09:15,870 --> 00:09:18,090
without provisioning a new instance,

207
00:09:18,090 --> 00:09:20,943
and then the speculative
recording gets started.

208
00:09:23,010 --> 00:09:25,770
Out of the box, SageMaker
offers managed containers

209
00:09:25,770 --> 00:09:28,547
to host more popular
frameworks like vLLM or SGLang.

210
00:09:29,998 --> 00:09:32,520
SageMaker offers open source containers

211
00:09:32,520 --> 00:09:34,800
with deep learning containers,

212
00:09:34,800 --> 00:09:37,500
which offer these two flavors
of most common frameworks

213
00:09:37,500 --> 00:09:39,660
or using a proprietary LMI container

214
00:09:39,660 --> 00:09:42,217
that also offers TensorRT LLM.

215
00:09:44,100 --> 00:09:45,840
Today we also announced the launch

216
00:09:45,840 --> 00:09:48,907
of serverless MLflow on SageMaker.

217
00:09:50,032 --> 00:09:51,633
With serverless MLflow, it's
fully managed experience

218
00:09:53,190 --> 00:09:56,100
so you don't need to worry about
managing any infrastructure

219
00:09:56,100 --> 00:09:57,180
and it's all serverless

220
00:09:57,180 --> 00:10:00,060
so it scales up and down
based on your requirements

221
00:10:00,060 --> 00:10:02,820
for when you're running a
large scale training job

222
00:10:02,820 --> 00:10:05,850
or you are sending out the agent traffic.

223
00:10:05,850 --> 00:10:08,400
We'll talk about those use cases in a bit.

224
00:10:08,400 --> 00:10:11,880
You can log your experiments,
run evaluation results

225
00:10:11,880 --> 00:10:14,580
or agent traces all in one place.

226
00:10:14,580 --> 00:10:16,350
And serverless MLflow is free.

227
00:10:16,350 --> 00:10:18,963
It does not have any additional charges.

228
00:10:20,880 --> 00:10:22,470
So here is a screenshot of

229
00:10:22,470 --> 00:10:24,660
how the experiment
tracking works with MLflow.

230
00:10:24,660 --> 00:10:27,000
You can see within an
experiment all the runs,

231
00:10:27,000 --> 00:10:29,145
which is basically each training run,

232
00:10:29,145 --> 00:10:31,710
you can compare and
contrast the training run

233
00:10:31,710 --> 00:10:34,080
to identify the best candidate model

234
00:10:34,080 --> 00:10:36,270
that you want to take to production.

235
00:10:36,270 --> 00:10:38,220
So you have all the
observability from the time

236
00:10:38,220 --> 00:10:39,900
of model customization.

237
00:10:39,900 --> 00:10:43,830
If you're using AgentCore
for deploying your agents

238
00:10:43,830 --> 00:10:47,130
and SageMaker AI for
customizing the models,

239
00:10:47,130 --> 00:10:49,230
AgentCore already emits all the traces

240
00:10:49,230 --> 00:10:51,600
into AgentCore observability dashboards

241
00:10:51,600 --> 00:10:54,090
and you can also send them to MLflow

242
00:10:54,090 --> 00:10:55,740
because they meet the OTel requirements

243
00:10:55,740 --> 00:10:58,650
or the open telemetry specifications.

244
00:10:58,650 --> 00:11:02,460
SageMaker also offers partner
AI apps as 1P capabilities,

245
00:11:02,460 --> 00:11:03,930
which are third party apps

246
00:11:03,930 --> 00:11:06,750
that can also help you monitor agents

247
00:11:06,750 --> 00:11:09,483
as well as the model performance.

248
00:11:10,380 --> 00:11:12,360
So now let's take a
look at how this works.

249
00:11:12,360 --> 00:11:14,730
You can now see on the left side

250
00:11:14,730 --> 00:11:16,830
there's a complete trace tree

251
00:11:16,830 --> 00:11:19,200
starting with the InvokeAgent at the top,

252
00:11:19,200 --> 00:11:22,140
drilling down through the
workflow build process,

253
00:11:22,140 --> 00:11:25,140
capturing each step of
the LangChain operations

254
00:11:25,140 --> 00:11:26,670
and even showing the tool calls,

255
00:11:26,670 --> 00:11:29,580
and you can see multiple
assistant interactions too.

256
00:11:29,580 --> 00:11:32,250
This hierarchical view gives
you the complete visibility

257
00:11:32,250 --> 00:11:34,380
into every step of the agent,

258
00:11:34,380 --> 00:11:36,810
including the conditional
branches and iterations,

259
00:11:36,810 --> 00:11:39,780
which makes it easy to
root cause any issues

260
00:11:39,780 --> 00:11:40,613
within the agents.

261
00:11:40,613 --> 00:11:43,080
And you can go down to
the model customization,

262
00:11:43,080 --> 00:11:45,480
see which model was used,
what dataset was used,

263
00:11:45,480 --> 00:11:47,430
which makes debugging much more easier.

264
00:11:49,260 --> 00:11:51,990
Now that you have your
experimental workflows figured out,

265
00:11:51,990 --> 00:11:54,270
you have all your
observability in one place.

266
00:11:54,270 --> 00:11:56,490
You need capabilities to productionize

267
00:11:56,490 --> 00:11:58,500
all these experimental workflows

268
00:11:58,500 --> 00:12:01,530
into scalable and repeatable workflows.

269
00:12:01,530 --> 00:12:04,770
SageMaker AI offers pipelines.

270
00:12:04,770 --> 00:12:09,480
You can simply convert your
existing experimental code

271
00:12:09,480 --> 00:12:11,880
into pipeline steps by annotating,

272
00:12:11,880 --> 00:12:14,640
whether add the rate
step annotator in code

273
00:12:14,640 --> 00:12:17,880
or drag, drop all of these steps into a UI

274
00:12:17,880 --> 00:12:19,480
to create a end-to-end pipeline.

275
00:12:20,460 --> 00:12:23,700
SageMaker also offers
built-in steps for training,

276
00:12:23,700 --> 00:12:27,210
evaluation and deployment that
eliminates the need for you

277
00:12:27,210 --> 00:12:28,890
to write any redundant wrapper code

278
00:12:28,890 --> 00:12:32,100
for spinning up the training
jobs and any infrastructure,

279
00:12:32,100 --> 00:12:34,590
and Pipelines is all
serverless orchestration,

280
00:12:34,590 --> 00:12:36,810
so you don't need to
manage any infrastructure.

281
00:12:36,810 --> 00:12:38,740
We emit all the telemetry to CloudWatch

282
00:12:38,740 --> 00:12:42,480
where you can observe the
behavior of each pipeline step.

283
00:12:42,480 --> 00:12:45,140
SageMaker Pipelines also
have caching built in it.

284
00:12:45,140 --> 00:12:48,420
So if a pipeline fails and
you re-trigger the pipeline,

285
00:12:48,420 --> 00:12:49,500
it'll skip the steps

286
00:12:49,500 --> 00:12:51,543
that were already successfully executed.

287
00:12:54,090 --> 00:12:56,580
Lastly, for governance we
talked about how you need

288
00:12:56,580 --> 00:12:58,560
to keep track of

289
00:12:58,560 --> 00:13:01,260
and manage each version of
the generative AI assets.

290
00:13:01,260 --> 00:13:03,240
So SageMaker offers Model Registry,

291
00:13:03,240 --> 00:13:05,190
which is the single source of truth

292
00:13:05,190 --> 00:13:06,873
for all your model versions.

293
00:13:07,830 --> 00:13:11,070
SageMaker AI Model
Registry is a central hub

294
00:13:11,070 --> 00:13:12,690
to manage the entire lifecycle

295
00:13:12,690 --> 00:13:13,890
of the machine learning models.

296
00:13:13,890 --> 00:13:15,750
We'll take a look into some
of those key capabilities

297
00:13:15,750 --> 00:13:17,040
in a moment.

298
00:13:17,040 --> 00:13:19,080
You can track all the
metadata corresponding

299
00:13:19,080 --> 00:13:20,700
to the training jobs, evaluation runs

300
00:13:20,700 --> 00:13:23,880
and so on into SageMaker Model Registry.

301
00:13:23,880 --> 00:13:26,520
So here with SageMaker Model Registry,

302
00:13:26,520 --> 00:13:29,910
you can set up cross account
access that allows you

303
00:13:29,910 --> 00:13:32,640
to track all the models
in all your environments

304
00:13:32,640 --> 00:13:33,810
in one place,

305
00:13:33,810 --> 00:13:35,850
so you can track not only the environment

306
00:13:35,850 --> 00:13:37,590
that you're working in
but also your staging

307
00:13:37,590 --> 00:13:39,190
and the production environments.

308
00:13:41,580 --> 00:13:44,210
Model Registry also enables
you to capture all the details

309
00:13:44,210 --> 00:13:46,950
of the training job so you
can have end-to-end lineage of

310
00:13:46,950 --> 00:13:49,470
what the model went through,
which makes it much more easier

311
00:13:49,470 --> 00:13:52,260
to debug in case you run into any issues.

312
00:13:52,260 --> 00:13:55,170
Next, I will hand it over
to Dan who will walk us

313
00:13:55,170 --> 00:13:57,450
through the demo to bring all
these capabilities to life.

314
00:13:57,450 --> 00:13:58,283
Thank you.

315
00:14:05,282 --> 00:14:06,115
- Yeah.

316
00:14:13,410 --> 00:14:14,243
Hello, everyone.

317
00:14:15,120 --> 00:14:18,330
Today we're gonna be going
through this demo which showcases

318
00:14:18,330 --> 00:14:21,600
a lot of the capabilities
that Amit was talking about.

319
00:14:21,600 --> 00:14:23,490
Let's imagine that we are some kind

320
00:14:23,490 --> 00:14:25,860
of clinical healthcare
provider, maybe a clinic

321
00:14:25,860 --> 00:14:28,560
or possibly an emergency room.

322
00:14:28,560 --> 00:14:31,530
And we want to accelerate
the amount of people

323
00:14:31,530 --> 00:14:34,350
that we can see and we wanna
do this using machine learning,

324
00:14:34,350 --> 00:14:36,090
artificial intelligence.

325
00:14:36,090 --> 00:14:38,220
So what we're gonna do
is increase the amount

326
00:14:38,220 --> 00:14:42,030
of patients we can see by
developing some kind of agent,

327
00:14:42,030 --> 00:14:44,850
using a lot of the techniques
that Amit had mentioned.

328
00:14:44,850 --> 00:14:48,190
We're gonna start by
hosting a language model

329
00:14:49,380 --> 00:14:51,330
onto a SageMaker managed endpoint.

330
00:14:51,330 --> 00:14:53,040
We're gonna fine tune it
and we're gonna orchestrate

331
00:14:53,040 --> 00:14:55,653
that entire process using
SageMaker Pipelines.

332
00:14:56,700 --> 00:14:58,170
We're gonna make some
friends along the way.

333
00:14:58,170 --> 00:15:02,130
We're gonna incorporate MLflow
for observability and tracing

334
00:15:02,130 --> 00:15:05,340
and then we're going to deploy that model

335
00:15:05,340 --> 00:15:08,010
onto not just the managed endpoint,

336
00:15:08,010 --> 00:15:12,480
but we're gonna log it
inside a model registry.

337
00:15:12,480 --> 00:15:15,180
We'll look at SageMaker's
Model Registry as well

338
00:15:15,180 --> 00:15:17,580
as MLflow's model registry

339
00:15:17,580 --> 00:15:19,680
and how we can incorporate governance,

340
00:15:19,680 --> 00:15:23,400
governance considerations like
a model card in both of them.

341
00:15:23,400 --> 00:15:26,820
Then once our model is,
once our endpoint is hosted

342
00:15:26,820 --> 00:15:28,170
and up and running,

343
00:15:28,170 --> 00:15:30,360
we'll examine how we can
connect that endpoint

344
00:15:30,360 --> 00:15:32,760
to something like an agent.

345
00:15:32,760 --> 00:15:36,120
We'll start with an agent
that's built off Strands SDK.

346
00:15:36,120 --> 00:15:38,370
If anyone's ever never
worked with Strands SDK,

347
00:15:38,370 --> 00:15:40,320
it's a very lightweight capability,

348
00:15:40,320 --> 00:15:43,380
a lightweight open source tool

349
00:15:43,380 --> 00:15:46,230
that lets you build agents very quickly.

350
00:15:46,230 --> 00:15:49,650
We'll also examine how you use Native MCP

351
00:15:49,650 --> 00:15:50,880
and connect all of this with some

352
00:15:50,880 --> 00:15:53,613
of the capabilities
within Bedrock AgentCore.

353
00:15:54,810 --> 00:15:56,190
Another thing we're gonna do

354
00:15:56,190 --> 00:15:58,470
that's a little interesting
is take advantage

355
00:15:58,470 --> 00:16:01,650
of SageMaker's "bring your
own container" paradigm.

356
00:16:01,650 --> 00:16:03,270
We're gonna use a new,

357
00:16:03,270 --> 00:16:07,050
a new repository called
ml-container-creator.

358
00:16:07,050 --> 00:16:08,580
The QR code links to that

359
00:16:08,580 --> 00:16:10,890
if you wanted to take a look at it.

360
00:16:10,890 --> 00:16:12,750
It's a very, very new,

361
00:16:12,750 --> 00:16:15,780
just launched like two weeks
ago, open source repository.

362
00:16:15,780 --> 00:16:18,060
So if any of you are
interested in contributing

363
00:16:18,060 --> 00:16:19,380
to the open source world,

364
00:16:19,380 --> 00:16:22,803
this is a great place to get
started because it is very new.

365
00:16:24,720 --> 00:16:29,170
So without further ado, let's
take a look at this first part

366
00:16:30,690 --> 00:16:33,123
of launching our model.

367
00:16:35,400 --> 00:16:36,990
This is the part of the architecture

368
00:16:36,990 --> 00:16:38,070
that we're gonna be focusing on

369
00:16:38,070 --> 00:16:39,600
in this first part of the demo,

370
00:16:39,600 --> 00:16:43,860
where we're going to be
training a Llama-3.2-3B

371
00:16:43,860 --> 00:16:48,630
instruct model using QLora adaptation.

372
00:16:48,630 --> 00:16:53,220
And we're gonna host it
on a SageMaker endpoint

373
00:16:53,220 --> 00:16:55,740
using SGLang for serving.

374
00:16:55,740 --> 00:16:58,320
So the SGLang container that
we're using is actually one

375
00:16:58,320 --> 00:16:59,460
that we're bringing ourselves.

376
00:16:59,460 --> 00:17:01,950
We're building it ourselves using some

377
00:17:01,950 --> 00:17:03,240
of the assets generated

378
00:17:03,240 --> 00:17:05,463
by the ml-container-creator repository.

379
00:17:06,630 --> 00:17:09,870
So let's dive into how
this actually works.

380
00:17:09,870 --> 00:17:14,283
The first part of any notebook
is a big list of imports.

381
00:17:15,390 --> 00:17:18,990
Don't really bother yourself
with too many of these except

382
00:17:18,990 --> 00:17:21,390
for these steps.

383
00:17:21,390 --> 00:17:23,880
These are the pipeline definition files

384
00:17:23,880 --> 00:17:26,610
that we're importing from our file system

385
00:17:26,610 --> 00:17:30,810
that tell us how to execute
each part of the pipeline.

386
00:17:30,810 --> 00:17:33,060
And we'll see in a moment
how they come together

387
00:17:33,060 --> 00:17:35,310
to form a direct to day cyclic graph.

388
00:17:35,310 --> 00:17:38,400
Kind of like what Amit had
shown on the slides before.

389
00:17:38,400 --> 00:17:40,860
Each one of these has
special instructions about

390
00:17:40,860 --> 00:17:44,850
how each step is supposed to
run, the inputs of one step,

391
00:17:44,850 --> 00:17:48,360
get processed and become
the inputs to the next step

392
00:17:48,360 --> 00:17:49,890
or the outputs of the first step become

393
00:17:49,890 --> 00:17:51,213
inputs to the next step.

394
00:17:52,290 --> 00:17:54,510
And I'll take a look, I'll
show you what one of these,

395
00:17:54,510 --> 00:17:56,940
each one of these or one
of these steps looks like

396
00:17:56,940 --> 00:17:57,773
in a moment.

397
00:17:59,730 --> 00:18:01,710
As we continue, we're basically setting up

398
00:18:01,710 --> 00:18:02,910
the rest of our environments

399
00:18:02,910 --> 00:18:06,600
so that you know, we have
a couple of Boto3 clients

400
00:18:06,600 --> 00:18:10,038
that we can work with a
fancy, you know, timer

401
00:18:10,038 --> 00:18:12,360
for keeping track of things.

402
00:18:12,360 --> 00:18:15,040
Where this really gets help interesting is

403
00:18:16,050 --> 00:18:20,670
where we initialize our MLflow
environment for this demo.

404
00:18:20,670 --> 00:18:23,430
We need to give our code a direction

405
00:18:23,430 --> 00:18:27,780
or place to point all of
its telemetry requests,

406
00:18:27,780 --> 00:18:30,990
all of its parameters and
metrics that we want to log

407
00:18:30,990 --> 00:18:32,970
through each of these pipeline steps.

408
00:18:32,970 --> 00:18:34,470
So what we're actually doing here is

409
00:18:34,470 --> 00:18:36,833
we're capturing the MLflow ARN,

410
00:18:36,833 --> 00:18:40,710
this is using a server
bound MLflow instance.

411
00:18:40,710 --> 00:18:43,710
But now with serverless,
you can do the same thing.

412
00:18:43,710 --> 00:18:46,380
And we're setting the tracking URI

413
00:18:46,380 --> 00:18:49,230
and experiment name in our environment.

414
00:18:49,230 --> 00:18:52,050
And this is this, these two lines of code,

415
00:18:52,050 --> 00:18:52,883
you'll see them in the steps.

416
00:18:52,883 --> 00:18:55,650
They're included here for completeness,

417
00:18:55,650 --> 00:18:57,810
but they're actually used in the steps

418
00:18:57,810 --> 00:19:00,810
where we're publishing metrics and logs

419
00:19:00,810 --> 00:19:02,640
and configuration items

420
00:19:02,640 --> 00:19:05,253
to MLflow within each pipeline step.

421
00:19:06,300 --> 00:19:08,610
So this is where we set these up

422
00:19:08,610 --> 00:19:10,685
and you'll see it more
in each step as they,

423
00:19:10,685 --> 00:19:12,243
as we look through those.

424
00:19:14,040 --> 00:19:16,500
Then we continue to identify

425
00:19:16,500 --> 00:19:18,300
where we're getting our configuration

426
00:19:18,300 --> 00:19:21,150
or container assets from.

427
00:19:21,150 --> 00:19:24,690
I took the liberty of
using ml-container-creator

428
00:19:24,690 --> 00:19:27,030
to generate an SGLang container

429
00:19:27,030 --> 00:19:29,220
and then I uploaded those files to S3.

430
00:19:29,220 --> 00:19:31,890
It's really just a Docker
file, a couple of serving file.

431
00:19:31,890 --> 00:19:36,890
A serving file basically
spins up an SGLang server

432
00:19:37,140 --> 00:19:40,170
onto whatever instance you deploy it on.

433
00:19:40,170 --> 00:19:43,200
So I put all of those onto S3 already

434
00:19:43,200 --> 00:19:46,710
and we are going to define
where we're creating that

435
00:19:46,710 --> 00:19:48,990
inside Elastic Container Registry

436
00:19:48,990 --> 00:19:50,430
'cause we're creating our own container

437
00:19:50,430 --> 00:19:52,860
and deploying it onto
a SageMaker endpoint.

438
00:19:52,860 --> 00:19:56,430
And we're gonna be using
Llama-3.2-3B instruct

439
00:19:56,430 --> 00:19:58,080
for this fine tuning job.

440
00:19:58,080 --> 00:20:00,510
So this is the model_id.

441
00:20:00,510 --> 00:20:02,850
We're gonna get it directly
off of Hugging Face

442
00:20:02,850 --> 00:20:05,043
inside our fine tuning job.

443
00:20:07,320 --> 00:20:08,950
This was my Hugging Face key

444
00:20:10,440 --> 00:20:15,240
and this wall of text is
the pipeline definition.

445
00:20:15,240 --> 00:20:17,910
So each one of these blocks

446
00:20:17,910 --> 00:20:21,930
is effectively a method call,
method call to the files

447
00:20:21,930 --> 00:20:24,150
that we imported at the very beginning.

448
00:20:24,150 --> 00:20:25,950
I'm not gonna spend too much time on this

449
00:20:25,950 --> 00:20:28,620
'cause if you haven't worked
with Pipelines before,

450
00:20:28,620 --> 00:20:30,600
you're not really going to appreciate

451
00:20:30,600 --> 00:20:31,920
this big wall of text

452
00:20:31,920 --> 00:20:34,353
like you would appreciate this diagram.

453
00:20:35,340 --> 00:20:38,130
This is the directed acyclic graph

454
00:20:38,130 --> 00:20:41,340
that is generated by the big
wall of text that we just saw.

455
00:20:41,340 --> 00:20:45,300
And effectively it's starting
with building our container,

456
00:20:45,300 --> 00:20:47,610
pre-processing a data set for fine tuning.

457
00:20:47,610 --> 00:20:50,575
That data set is the freedom
intelligence data set,

458
00:20:50,575 --> 00:20:53,760
FreedomIntelligence/medical-o1-reasoning-SFT
data
set.

459
00:20:53,760 --> 00:20:57,840
It's effectively a series
of questions and answers

460
00:20:57,840 --> 00:20:59,130
using chain of thought reasoning

461
00:20:59,130 --> 00:21:00,600
that we're gonna use
to fine tune our model

462
00:21:00,600 --> 00:21:02,703
to become a better clinical advisor.

463
00:21:03,750 --> 00:21:07,650
So we're gonna pre-process that
to work for our Llama model.

464
00:21:07,650 --> 00:21:10,020
We're gonna do QLora adaptation on it

465
00:21:10,020 --> 00:21:12,220
and then we're gonna deploy it using our

466
00:21:13,320 --> 00:21:15,600
custom SGLang container.

467
00:21:15,600 --> 00:21:16,950
We're also gonna register this

468
00:21:16,950 --> 00:21:18,915
inside SageMaker Model Registry

469
00:21:18,915 --> 00:21:20,883
and the MLflow model registry.

470
00:21:22,230 --> 00:21:25,230
So all of this stuff
that you, all of this,

471
00:21:25,230 --> 00:21:26,880
these steps are represented

472
00:21:26,880 --> 00:21:29,973
and defined in these method calls.

473
00:21:30,960 --> 00:21:35,080
We stitch them together using
a SageMaker pipeline object

474
00:21:35,940 --> 00:21:39,160
and you can see the steps
that we've imported earlier on

475
00:21:40,170 --> 00:21:41,790
and we upsert the existing role.

476
00:21:41,790 --> 00:21:43,290
Now this role has permission to do

477
00:21:43,290 --> 00:21:44,970
all of the things we need it to do

478
00:21:44,970 --> 00:21:46,620
within each part of the pipeline,

479
00:21:48,120 --> 00:21:51,150
and the magic happens.

480
00:21:51,150 --> 00:21:52,743
We run pipeline.start.

481
00:21:53,850 --> 00:21:55,860
This initializes a pipeline run.

482
00:21:55,860 --> 00:21:56,970
This creates the pipeline.

483
00:21:56,970 --> 00:21:59,220
If it hasn't been created
for the first time,

484
00:22:00,180 --> 00:22:01,800
I'm sorry, that's already been created,

485
00:22:01,800 --> 00:22:04,800
this init executes a new pipeline run

486
00:22:04,800 --> 00:22:07,530
and once it's successful,

487
00:22:07,530 --> 00:22:10,290
'cause it eventually will be successful,

488
00:22:10,290 --> 00:22:13,297
each of these steps will
show a green tick mark

489
00:22:13,297 --> 00:22:15,690
and then you can kind of
double click onto each of these

490
00:22:15,690 --> 00:22:17,520
and take a look at what has happened.

491
00:22:17,520 --> 00:22:19,710
It'll point you to the CloudWatch logs,

492
00:22:19,710 --> 00:22:23,610
other parameters for each pipeline step,

493
00:22:23,610 --> 00:22:25,560
and this is how it looks within SageMaker.

494
00:22:25,560 --> 00:22:27,000
We can go into the console now

495
00:22:27,000 --> 00:22:30,150
and take a look at all of my failed runs

496
00:22:30,150 --> 00:22:31,923
and my one successful run.

497
00:22:33,390 --> 00:22:36,870
Or we can look at how
this looks in MLflow.

498
00:22:36,870 --> 00:22:39,270
MLflow has a very similar layout.

499
00:22:39,270 --> 00:22:42,900
Instead of the graph, we have
this list of pipeline steps

500
00:22:42,900 --> 00:22:45,420
that have run under a run_id

501
00:22:45,420 --> 00:22:48,510
and you can see the BYOC container data,

502
00:22:48,510 --> 00:22:51,120
pre-processing QLora adaptation,

503
00:22:51,120 --> 00:22:53,040
SGLang server model registration.

504
00:22:53,040 --> 00:22:56,703
All of the same steps are
also logged in MLflow.

505
00:22:59,580 --> 00:23:01,770
So let's look at one of these steps

506
00:23:01,770 --> 00:23:04,710
to see how this was set up.

507
00:23:04,710 --> 00:23:07,110
I'm gonna pull out the QLora fine tuning

508
00:23:07,110 --> 00:23:09,180
orchestration step.

509
00:23:09,180 --> 00:23:11,880
Each of these steps is about the same.

510
00:23:11,880 --> 00:23:14,520
The only difference is
what's actually happening,

511
00:23:14,520 --> 00:23:18,633
what you're actually instructing
the training job to do.

512
00:23:19,530 --> 00:23:21,600
In this step, we're
instructing the training job

513
00:23:21,600 --> 00:23:24,750
to fine tune the model,

514
00:23:24,750 --> 00:23:28,380
but every training step is gonna have,

515
00:23:28,380 --> 00:23:30,300
or every pipeline step is gonna have

516
00:23:30,300 --> 00:23:32,070
some of the same elements.

517
00:23:32,070 --> 00:23:34,800
We're gonna have an @Step annotation.

518
00:23:34,800 --> 00:23:38,490
This allows us to define a step
separately from our notebook

519
00:23:38,490 --> 00:23:40,650
or from our pipeline context

520
00:23:40,650 --> 00:23:43,590
so that you don't have lots
and lots of very long methods

521
00:23:43,590 --> 00:23:44,940
in the same file.

522
00:23:44,940 --> 00:23:47,670
You can import them the
way we should before

523
00:23:47,670 --> 00:23:50,280
as long as you define
a set of requirements

524
00:23:50,280 --> 00:23:52,560
because what you're doing is
you're creating a training job

525
00:23:52,560 --> 00:23:54,240
and you're executing this code inside

526
00:23:54,240 --> 00:23:55,683
that SageMaker training job.

527
00:23:57,300 --> 00:24:00,000
We're also passing information

528
00:24:00,000 --> 00:24:02,430
about the MLflow tracking server.

529
00:24:02,430 --> 00:24:05,550
This allows this step to publish metrics

530
00:24:05,550 --> 00:24:07,893
and telemetry data back to MLflow.

531
00:24:09,480 --> 00:24:11,130
Then some of the things
you'd come to expect

532
00:24:11,130 --> 00:24:13,860
from a fine tuning job, training data set,

533
00:24:13,860 --> 00:24:17,760
test data set, the model_id
that we're actually training.

534
00:24:17,760 --> 00:24:19,770
In this scenario, I'm
using my Hugging Face,

535
00:24:19,770 --> 00:24:21,900
we're pulling this
straight from Hugging Face.

536
00:24:21,900 --> 00:24:24,210
So we have a Hugging Face
token that we're passing

537
00:24:24,210 --> 00:24:25,053
and a role.

538
00:24:28,350 --> 00:24:31,603
Here again, we see we're
setting the tracking_uri

539
00:24:33,222 --> 00:24:35,910
and the experiment_name
just like we showed before

540
00:24:35,910 --> 00:24:37,440
in the previous notebook.

541
00:24:37,440 --> 00:24:42,150
This is how the execution
context knows how

542
00:24:42,150 --> 00:24:46,893
to publish metrics and log data to MLflow.

543
00:24:49,320 --> 00:24:51,720
Then we define the MLflow context.

544
00:24:51,720 --> 00:24:55,080
We're also setting up automatic
system metrics logging.

545
00:24:55,080 --> 00:25:00,080
This captures system metrics
like memory used, disk used,

546
00:25:00,120 --> 00:25:05,100
networking network bytes I/O

547
00:25:05,100 --> 00:25:09,460
and we're setting up the MLflow context

548
00:25:10,740 --> 00:25:12,960
and we begin running the job.

549
00:25:12,960 --> 00:25:14,943
And this is all the kinds of,

550
00:25:17,670 --> 00:25:20,130
this is where we would set up
our training configuration.

551
00:25:20,130 --> 00:25:22,920
And you can see eventually
as we get down to the end,

552
00:25:22,920 --> 00:25:25,650
we're going to kick off a training job.

553
00:25:25,650 --> 00:25:27,780
We're defining our compute input_data

554
00:25:27,780 --> 00:25:30,570
and eventually a model trainer

555
00:25:30,570 --> 00:25:33,360
before we actually call train.

556
00:25:33,360 --> 00:25:35,610
Now all of the things that
are happening before that

557
00:25:35,610 --> 00:25:40,610
are logs, pieces of
information being built

558
00:25:40,650 --> 00:25:42,270
sent out to MLflow

559
00:25:42,270 --> 00:25:44,250
to make it easier to understand

560
00:25:44,250 --> 00:25:45,750
what has happened in this run.

561
00:25:46,920 --> 00:25:50,040
We can take a look at
some of the system metrics

562
00:25:50,040 --> 00:25:51,483
for the fine tuning job.

563
00:25:53,850 --> 00:25:55,170
We can take a look at some of the metrics

564
00:25:55,170 --> 00:25:58,980
that are published and tagged.

565
00:25:58,980 --> 00:26:00,120
So these are the kinds of things

566
00:26:00,120 --> 00:26:03,270
that you'll see when
you're writing these steps

567
00:26:03,270 --> 00:26:05,220
for your SageMaker pipeline.

568
00:26:05,220 --> 00:26:06,820
This is where it's gonna end up.

569
00:26:11,550 --> 00:26:14,433
So now that we've defined our pipeline,

570
00:26:15,840 --> 00:26:17,190
let's actually tell,

571
00:26:17,190 --> 00:26:19,770
oh, I forgot to mention,
this is the model card

572
00:26:19,770 --> 00:26:23,610
that was set up in the
model registration step.

573
00:26:23,610 --> 00:26:25,680
This is an important piece to make note of

574
00:26:25,680 --> 00:26:27,659
because this is a medical scenario

575
00:26:27,659 --> 00:26:31,620
where an AI agent is advising
healthcare professionals.

576
00:26:31,620 --> 00:26:35,800
And so model card should have
something similar to this

577
00:26:36,990 --> 00:26:38,490
in, not exactly this,

578
00:26:38,490 --> 00:26:41,400
but similar to this
when you're implementing

579
00:26:41,400 --> 00:26:43,050
appropriate governance standards.

580
00:26:45,630 --> 00:26:47,610
So we'll test the endpoint.

581
00:26:47,610 --> 00:26:50,670
I have set up this endpoint a while ago,

582
00:26:50,670 --> 00:26:52,830
that's why I'm hard
coding the endpoint name

583
00:26:52,830 --> 00:26:54,030
and inference component.

584
00:26:55,080 --> 00:26:57,630
And then we just create a
standard predictor like you would

585
00:26:57,630 --> 00:26:59,073
with the SageMaker SDK,

586
00:27:00,300 --> 00:27:04,833
and we will run a basic
prompt against this endpoint.

587
00:27:05,790 --> 00:27:07,260
You're gonna see this prompt a lot,

588
00:27:07,260 --> 00:27:09,420
45-year-old male with fever and cough,

589
00:27:09,420 --> 00:27:10,710
temperature's 101.3

590
00:27:10,710 --> 00:27:12,000
and heart rate is 98.

591
00:27:12,000 --> 00:27:14,220
Give me a treatment
recommendation and a report.

592
00:27:14,220 --> 00:27:16,890
Upload the report to S3 when done.

593
00:27:16,890 --> 00:27:20,210
Now this is just a simple
model that's deployed on to,

594
00:27:20,210 --> 00:27:24,780
an SGLang endpoint, doesn't
have tools capabilities,

595
00:27:24,780 --> 00:27:27,000
it's just been fine tuned.

596
00:27:27,000 --> 00:27:30,690
So we know that the model
should have the ability

597
00:27:30,690 --> 00:27:33,000
to perform some basic medical reasoning

598
00:27:33,000 --> 00:27:34,620
because of the fine tuning job,

599
00:27:34,620 --> 00:27:37,290
but it's not enabled with tools to upload

600
00:27:37,290 --> 00:27:38,973
or report to S3 yet.

601
00:27:40,290 --> 00:27:43,137
So this is the output here.

602
00:27:43,137 --> 00:27:44,790
You can see that the patient details

603
00:27:44,790 --> 00:27:47,850
have been set up in a
nice report style format,

604
00:27:47,850 --> 00:27:50,700
some kind of basic markdown syntax.

605
00:27:50,700 --> 00:27:52,170
And the model did a fairly good job.

606
00:27:52,170 --> 00:27:56,040
It's doing a, performing
a preliminary assessment

607
00:27:56,040 --> 00:27:58,864
of the patient's symptoms to
show to suggest that it ha

608
00:27:58,864 --> 00:28:00,960
that the patient is exhibiting signs

609
00:28:00,960 --> 00:28:02,943
of pneumonia or something similar.

610
00:28:04,590 --> 00:28:07,230
So this is the first part of our pipeline.

611
00:28:07,230 --> 00:28:11,820
The model is deployed onto an endpoint.

612
00:28:11,820 --> 00:28:14,490
It's using the BYOC container for SGLang

613
00:28:14,490 --> 00:28:15,840
that we built ourselves.

614
00:28:15,840 --> 00:28:17,010
We fine tuned it

615
00:28:17,010 --> 00:28:20,337
and we have tracked our
entire experiment with MLflow

616
00:28:20,337 --> 00:28:22,350
and we can go back and
take another look at it

617
00:28:22,350 --> 00:28:23,430
if we want to.

618
00:28:23,430 --> 00:28:25,320
Our model card is registered and logged

619
00:28:25,320 --> 00:28:28,270
and I will show you the
model registry as well

620
00:28:32,220 --> 00:28:33,540
within Mlflow

621
00:28:33,540 --> 00:28:36,540
that shows you more
information about the model

622
00:28:36,540 --> 00:28:37,533
and where it lives.

623
00:28:38,760 --> 00:28:39,843
Version one.

624
00:28:41,700 --> 00:28:43,440
So if you have, if you do this again

625
00:28:43,440 --> 00:28:45,540
and you publish a new
version of that model,

626
00:28:45,540 --> 00:28:49,170
it'll be added to the version list,

627
00:28:49,170 --> 00:28:51,370
the model card might
get updated, et cetera.

628
00:28:52,440 --> 00:28:55,530
So this is the first
part of our demo, right?

629
00:28:55,530 --> 00:28:58,830
We've successfully
deployed a fine tuned agent

630
00:28:58,830 --> 00:29:01,920
or a fine tuned model to SageMaker.

631
00:29:01,920 --> 00:29:04,443
Let's give it some more capabilities.

632
00:29:06,750 --> 00:29:08,730
Let's build out an agent,

633
00:29:08,730 --> 00:29:10,860
an agent that can do some of the act,

634
00:29:10,860 --> 00:29:11,700
take some of the actions

635
00:29:11,700 --> 00:29:15,510
that we were talking
about like uploading to,

636
00:29:15,510 --> 00:29:19,890
uploading the initial
diagnostic report to S3

637
00:29:19,890 --> 00:29:22,500
for a clinician to review at a later time.

638
00:29:22,500 --> 00:29:24,660
We might also want to give it
some additional capabilities

639
00:29:24,660 --> 00:29:26,790
around patient lookups.

640
00:29:26,790 --> 00:29:28,620
If a patient's already been there before,

641
00:29:28,620 --> 00:29:29,730
how does the model understand?

642
00:29:29,730 --> 00:29:32,070
How does the agent know
who this patient is?

643
00:29:32,070 --> 00:29:35,250
Maybe there's a patient
database that we can query

644
00:29:35,250 --> 00:29:37,980
to get more information
about that patient.

645
00:29:37,980 --> 00:29:39,420
And so really what we're building out is

646
00:29:39,420 --> 00:29:40,623
this piece down here.

647
00:29:42,120 --> 00:29:44,670
We're gonna be using Bedrock AgentCore

648
00:29:44,670 --> 00:29:48,933
to deploy a Strands agent
onto the AgentCore Runtime.

649
00:29:50,100 --> 00:29:52,320
The Strands agent is something
we're gonna be building out

650
00:29:52,320 --> 00:29:53,490
inside this notebook.

651
00:29:53,490 --> 00:29:58,110
It's gonna be a pretty
large block of code.

652
00:29:58,110 --> 00:30:01,080
I'm gonna walk you guys
through what that looks like.

653
00:30:01,080 --> 00:30:04,110
And then we're gonna
use AgentCore Telemetry

654
00:30:04,110 --> 00:30:07,650
to capture telemetry
data about each request

655
00:30:07,650 --> 00:30:09,633
that the agent is handling.

656
00:30:11,670 --> 00:30:14,669
So AgentCore Telemetry kind
of complimenting Mlflow

657
00:30:14,669 --> 00:30:17,790
for observability.

658
00:30:17,790 --> 00:30:20,763
Again, we have this big
long list of imports,

659
00:30:22,170 --> 00:30:25,083
nothing really to pay too
much attention to here.

660
00:30:26,910 --> 00:30:29,430
Same kind of environment set up.

661
00:30:29,430 --> 00:30:34,050
And what I've done is I've
copied the endpoint listing

662
00:30:34,050 --> 00:30:35,310
inside SageMaker.

663
00:30:35,310 --> 00:30:37,230
This is the endpoint that's been deployed.

664
00:30:37,230 --> 00:30:40,140
The name and the inference
component that we're working on,

665
00:30:40,140 --> 00:30:42,810
I forgot to mention this model
is deployed on an inference

666
00:30:42,810 --> 00:30:46,440
component so it scales
independent of the infrastructure

667
00:30:46,440 --> 00:30:48,780
so long as there is capacity on

668
00:30:48,780 --> 00:30:50,230
that infrastructure to scale.

669
00:30:52,530 --> 00:30:55,170
So we define the endpoint name
and the inference component.

670
00:30:55,170 --> 00:30:58,050
I hard code them for
the sake of this demo.

671
00:30:58,050 --> 00:31:00,990
And now we're going to
build out the Strand agent.

672
00:31:00,990 --> 00:31:04,410
The Strand agent is an f-string in Python.

673
00:31:04,410 --> 00:31:06,930
So we're basically injecting variables

674
00:31:06,930 --> 00:31:09,660
inside this f-string so
that it's instead of kind

675
00:31:09,660 --> 00:31:10,950
of flipping between different files,

676
00:31:10,950 --> 00:31:13,413
just easier to, for demo purposes.

677
00:31:17,490 --> 00:31:19,530
So you can see we're
passing in the region,

678
00:31:19,530 --> 00:31:24,480
the endpoint name an
S3 bucket for artifacts

679
00:31:24,480 --> 00:31:27,183
and the inference component
is in here as well.

680
00:31:29,820 --> 00:31:32,250
We've defined a patient dataclass,

681
00:31:32,250 --> 00:31:35,580
basically some data information
about who the patient is,

682
00:31:35,580 --> 00:31:37,200
what kind of symptoms they have.

683
00:31:37,200 --> 00:31:39,390
You can imagine that
this might be analogous

684
00:31:39,390 --> 00:31:42,150
to a record in a patient database.

685
00:31:42,150 --> 00:31:44,370
I'm not a healthcare
professional so I have no idea

686
00:31:44,370 --> 00:31:46,380
what you might find in a patient database,

687
00:31:46,380 --> 00:31:48,897
but I took a stab at
what it might look like,

688
00:31:48,897 --> 00:31:50,547
and this is what we came up with.

689
00:31:52,380 --> 00:31:55,770
And I'm gonna skip to
the more interesting part

690
00:31:55,770 --> 00:31:58,050
where we are...

691
00:31:58,050 --> 00:31:58,883
Here we go.

692
00:32:02,700 --> 00:32:05,310
We are invoking the SageMaker endpoint

693
00:32:05,310 --> 00:32:06,543
that we defined earlier.

694
00:32:07,530 --> 00:32:11,170
This is written inside of a method

695
00:32:12,300 --> 00:32:14,250
called invoke_sagemaker_endpoint_async.

696
00:32:15,690 --> 00:32:18,150
This method invokes the SageMaker endpoint

697
00:32:18,150 --> 00:32:19,500
that we deployed earlier,

698
00:32:19,500 --> 00:32:20,640
but there's a couple of other things

699
00:32:20,640 --> 00:32:21,840
that are happening here.

700
00:32:22,860 --> 00:32:26,280
First, we're defining our prompt.

701
00:32:26,280 --> 00:32:29,160
And the prompt is
basically that same query

702
00:32:29,160 --> 00:32:30,870
that I showed you guys earlier.

703
00:32:30,870 --> 00:32:32,550
It's the same one we're gonna be using.

704
00:32:32,550 --> 00:32:34,440
I'm not changing the the prompt,

705
00:32:34,440 --> 00:32:36,553
it's a man with a,

706
00:32:36,553 --> 00:32:40,980
a 47-year-old man with a
temperature exhibiting a cough.

707
00:32:40,980 --> 00:32:43,353
What we're doing is wrapping this up in a,

708
00:32:45,150 --> 00:32:46,800
in like in inside a,

709
00:32:46,800 --> 00:32:48,243
a super prompt,

710
00:32:50,250 --> 00:32:51,510
which basically says

711
00:32:51,510 --> 00:32:54,180
that this is a clinical training scenario.

712
00:32:54,180 --> 00:32:56,040
What would you do in this situation

713
00:32:56,040 --> 00:32:58,473
as a clinical healthcare provider?

714
00:33:00,570 --> 00:33:03,480
Once we have that in place,
we're gonna define our tracer.

715
00:33:03,480 --> 00:33:04,500
Now tracers are

716
00:33:04,500 --> 00:33:08,670
how AgentCore observability
captures trace data

717
00:33:08,670 --> 00:33:11,433
and trace information inside your runtime.

718
00:33:13,200 --> 00:33:15,250
So we've hard coded some attributes here

719
00:33:16,680 --> 00:33:17,850
like the SageMaker endpoint

720
00:33:17,850 --> 00:33:21,270
and inference component that
is servicing the request.

721
00:33:21,270 --> 00:33:23,220
This might be really
helpful if you have an agent

722
00:33:23,220 --> 00:33:26,760
that's making multiple
calls to several endpoints.

723
00:33:26,760 --> 00:33:29,670
You don't know exactly
which one was called when.

724
00:33:29,670 --> 00:33:33,900
This is a great piece of
information to keep inside a trace

725
00:33:33,900 --> 00:33:37,110
as well as some of the hyper
parameters like max tokens,

726
00:33:37,110 --> 00:33:40,290
the temperature and what
the actual prompt was.

727
00:33:40,290 --> 00:33:44,250
These are examples of what
you might want to put in a,

728
00:33:44,250 --> 00:33:45,933
inside an agent trace.

729
00:33:47,580 --> 00:33:48,900
Then we define our payload.

730
00:33:48,900 --> 00:33:50,700
We're using the Messages API.

731
00:33:50,700 --> 00:33:52,740
So we have a system,

732
00:33:52,740 --> 00:33:54,570
we have a component which
is the system prompt,

733
00:33:54,570 --> 00:33:56,850
and we have a component
which is the user prompt,

734
00:33:56,850 --> 00:34:00,450
again, that wrapped up
prompt of the gentleman

735
00:34:00,450 --> 00:34:02,253
with symptoms of pneumonia.

736
00:34:04,110 --> 00:34:07,233
And then we invoke our endpoint.

737
00:34:08,460 --> 00:34:10,530
Now Strands allows us to do this

738
00:34:10,530 --> 00:34:15,530
because we've defined an endpoint.

739
00:34:15,750 --> 00:34:18,450
This is how Strands allows
us to define an endpoint

740
00:34:18,450 --> 00:34:20,500
that we're going to use within our agent.

741
00:34:22,560 --> 00:34:24,060
And once we are done

742
00:34:24,060 --> 00:34:27,206
getting our response back from the model,

743
00:34:27,206 --> 00:34:31,050
we set additional information
on our span object.

744
00:34:31,050 --> 00:34:32,620
This is the context that

745
00:34:35,070 --> 00:34:36,453
captures the trace.

746
00:34:37,920 --> 00:34:40,320
We're going to set
information like the duration

747
00:34:40,320 --> 00:34:43,890
of the model's response,
the length of the response

748
00:34:43,890 --> 00:34:45,360
and additional metadata

749
00:34:45,360 --> 00:34:47,493
about the success of the query.

750
00:34:49,140 --> 00:34:52,590
So now we're gonna configure
this actual agent, right?

751
00:34:52,590 --> 00:34:54,660
All we did was define a way

752
00:34:54,660 --> 00:34:57,600
to query the SageMaker
endpoint which we deployed.

753
00:34:57,600 --> 00:34:59,950
Let's configure the actual
agent using Strands.

754
00:35:01,080 --> 00:35:03,780
Now I've set this up to
run inside the local mode

755
00:35:03,780 --> 00:35:07,680
and inside a AgentCore Runtime context.

756
00:35:07,680 --> 00:35:10,800
Agent core runtime uses FastAPI.

757
00:35:10,800 --> 00:35:11,633
So you'll see that

758
00:35:11,633 --> 00:35:13,680
there's a little bit of
overlap in this f-string

759
00:35:13,680 --> 00:35:17,400
so that we can kind of illustrate
how to run this locally

760
00:35:17,400 --> 00:35:19,863
and how to run this on AgentCore Runtime.

761
00:35:21,120 --> 00:35:24,030
Here we're defining a SageMaker AI model.

762
00:35:24,030 --> 00:35:27,420
This is the class that Strands SDK uses

763
00:35:27,420 --> 00:35:29,490
to define an agent

764
00:35:29,490 --> 00:35:34,490
or an LLM hosted on a SageMaker endpoint.

765
00:35:35,040 --> 00:35:38,100
This was introduced
into Strands in August,

766
00:35:38,100 --> 00:35:39,720
I believe it was.

767
00:35:39,720 --> 00:35:40,920
So it's a fairly new capability

768
00:35:40,920 --> 00:35:43,540
and it allows you to use
language models outside of

769
00:35:45,120 --> 00:35:47,913
API based providers
such as Amazon Bedrock.

770
00:35:49,170 --> 00:35:52,320
And then we just pass over
our prompt back to the model

771
00:35:52,320 --> 00:35:55,863
in much the same way
that we just illustrated.

772
00:35:57,596 --> 00:35:59,430
You can see we have
additional tracer information,

773
00:35:59,430 --> 00:36:02,163
additional examples of how to use traces,

774
00:36:04,470 --> 00:36:07,530
identifying some attributes on this trace,

775
00:36:07,530 --> 00:36:10,500
and then defining a span
context within which

776
00:36:10,500 --> 00:36:13,923
we are performing additional executions.

777
00:36:16,260 --> 00:36:18,210
Now if we're not in local mode,

778
00:36:18,210 --> 00:36:21,210
we're gonna create the
Bedrock AgentCore app.

779
00:36:21,210 --> 00:36:25,420
This is effectively
creating a FastAPI server

780
00:36:26,460 --> 00:36:28,710
and we're gonna call the
create_medical_agent.

781
00:36:28,710 --> 00:36:31,890
That's that block of code that
creates the SageMaker model

782
00:36:31,890 --> 00:36:35,613
or SageMaker AI model
object, the Strands agent.

783
00:36:36,720 --> 00:36:38,280
And then we define our entry point,

784
00:36:38,280 --> 00:36:41,760
which is again going to perform
effectively the same task

785
00:36:41,760 --> 00:36:46,230
of passing a prompt over to the model.

786
00:36:46,230 --> 00:36:49,290
Except this code will only
ever run when we deploy this

787
00:36:49,290 --> 00:36:50,913
onto Bedrock AgentCore.

788
00:36:50,913 --> 00:36:53,340
When we run this locally, which
I'll show you in a moment,

789
00:36:53,340 --> 00:36:54,750
it's not gonna execute this block,

790
00:36:54,750 --> 00:36:56,910
it's gonna execute
effectively the same logic,

791
00:36:56,910 --> 00:36:58,623
just run in a local context.

792
00:37:00,360 --> 00:37:03,723
Whoa, zoom out.

793
00:37:06,175 --> 00:37:07,008
Okay.

794
00:37:09,720 --> 00:37:14,013
So what I do here
eventually is write this,

795
00:37:15,030 --> 00:37:17,330
write this big giant
f-string to a Python file

796
00:37:18,630 --> 00:37:23,630
and now I'm going to actually run this.

797
00:37:24,300 --> 00:37:27,870
We're gonna run this using
Shell Magic to run the Python,

798
00:37:27,870 --> 00:37:30,180
run the Python code using the local flag.

799
00:37:30,180 --> 00:37:34,530
And we're passing in
the same friendly prompt

800
00:37:34,530 --> 00:37:36,180
that we've come to know and love.

801
00:37:37,950 --> 00:37:42,630
And when we run this,
we're spinning up the file

802
00:37:42,630 --> 00:37:45,720
inside our Python runtime

803
00:37:45,720 --> 00:37:48,420
and it's passing out the message back

804
00:37:48,420 --> 00:37:50,520
to the language model endpoint.

805
00:37:50,520 --> 00:37:54,510
This is exactly what the agent is gonna do

806
00:37:54,510 --> 00:37:56,640
once it's deployed to AgentCore Runtime.

807
00:37:56,640 --> 00:38:00,690
We're just not, we're just
not accessing this agent code

808
00:38:00,690 --> 00:38:02,283
over the runtime API yet.

809
00:38:04,380 --> 00:38:05,850
So what it comes back with

810
00:38:05,850 --> 00:38:08,040
is basically the same thing we saw earlier

811
00:38:08,040 --> 00:38:09,690
based on the clinical scenario provided,

812
00:38:09,690 --> 00:38:11,820
a 45-year-old male presents
with fever and cough,

813
00:38:11,820 --> 00:38:13,890
his temperature is 101.3,

814
00:38:13,890 --> 00:38:16,290
suggests some kind of
respiratory infection,

815
00:38:16,290 --> 00:38:17,853
et cetera, et cetera.

816
00:38:18,840 --> 00:38:20,280
So this is all running locally.

817
00:38:20,280 --> 00:38:21,363
Now we deploy it.

818
00:38:22,740 --> 00:38:24,450
Everything checks out so far.

819
00:38:24,450 --> 00:38:27,183
Let's use this on the
Bedrock AgentCore Runtime.

820
00:38:28,050 --> 00:38:30,270
So we're gonna use something
called the Bedrock AgentCore

821
00:38:30,270 --> 00:38:31,473
Starter Toolkit.

822
00:38:32,400 --> 00:38:34,200
If you've never seen this before,

823
00:38:34,200 --> 00:38:35,670
it's a pretty fancy wrapper

824
00:38:35,670 --> 00:38:37,570
around some of the

825
00:38:41,169 --> 00:38:43,980
AWS CLI commands or API
commands you might need

826
00:38:43,980 --> 00:38:46,673
to run, to deploy an AgentCore,

827
00:38:48,182 --> 00:38:49,950
to deploy an agent to
the AgentCore Runtime

828
00:38:49,950 --> 00:38:51,630
for the first time.

829
00:38:51,630 --> 00:38:54,270
Effectively you're
defining a runtime object,

830
00:38:54,270 --> 00:38:55,950
configuring your agent

831
00:38:55,950 --> 00:39:00,516
and basically wrapping up the
Python code we wrote alongside

832
00:39:00,516 --> 00:39:04,113
some requirements files and
other runtime information.

833
00:39:05,175 --> 00:39:07,470
And then all we do

834
00:39:07,470 --> 00:39:10,920
after we've defined this runtime object

835
00:39:10,920 --> 00:39:13,473
is we execute the launch command.

836
00:39:14,340 --> 00:39:17,116
Now I'm using launch with CodeBuild.

837
00:39:17,116 --> 00:39:19,830
What this means is that
CodeBuild is gonna receive

838
00:39:19,830 --> 00:39:21,867
the objects that we've built out

839
00:39:21,867 --> 00:39:25,290
and it's gonna execute a
new CodeBuild execution

840
00:39:25,290 --> 00:39:27,660
that effectively takes
all of the information

841
00:39:27,660 --> 00:39:31,260
that we've built and walk through just now

842
00:39:31,260 --> 00:39:32,970
and builds it out and deploys it onto

843
00:39:32,970 --> 00:39:35,220
the Runtime control plane.

844
00:39:35,220 --> 00:39:37,020
The AgentCore Runtime Control Plane.

845
00:39:38,130 --> 00:39:39,197
Now I'm not gonna actually do this

846
00:39:39,197 --> 00:39:41,370
'cause it takes a little bit to run,

847
00:39:41,370 --> 00:39:43,140
but you can see what it looks like.

848
00:39:43,140 --> 00:39:46,590
It's walking you through all of the steps

849
00:39:46,590 --> 00:39:50,700
that CodeBuild takes to
actually build this agent.

850
00:39:50,700 --> 00:39:52,750
Eventually we'll get an ARN at the end

851
00:39:53,730 --> 00:39:58,000
and that ARN lets us run

852
00:40:00,510 --> 00:40:02,130
or invoke the agent

853
00:40:02,130 --> 00:40:03,753
using the AgentCore Runtime.

854
00:40:04,620 --> 00:40:05,820
So I'm gonna invoke this

855
00:40:07,170 --> 00:40:10,350
and when you deploy this inside AgentCore,

856
00:40:10,350 --> 00:40:12,753
you're gonna see something like this.

857
00:40:13,890 --> 00:40:17,010
The Strands medical agent, no
tools when it was deployed,

858
00:40:17,010 --> 00:40:19,210
all the different
versions that are up there

859
00:40:20,790 --> 00:40:24,810
as well as the active endpoint.

860
00:40:24,810 --> 00:40:27,570
The active endpoint is what's
capturing the telemetry data

861
00:40:27,570 --> 00:40:29,850
from our trace objects.

862
00:40:29,850 --> 00:40:32,860
The active endpoint is
what's actually accruing logs

863
00:40:34,320 --> 00:40:35,430
and the active endpoint is what

864
00:40:35,430 --> 00:40:38,340
we're actually making calls to

865
00:40:38,340 --> 00:40:43,340
when we invoke the AgentCore
Runtime with our prompts.

866
00:40:43,470 --> 00:40:46,380
So we're gonna get probably
the same exact response

867
00:40:46,380 --> 00:40:49,683
or very similar to the same
response that we showed earlier.

868
00:40:57,090 --> 00:40:59,040
While we're waiting for that to finish,

869
00:41:00,180 --> 00:41:01,140
you can see an example

870
00:41:01,140 --> 00:41:03,840
of what some of the
telemetry data looks like

871
00:41:03,840 --> 00:41:06,540
that we've captured inside our agent.

872
00:41:06,540 --> 00:41:10,320
So this is a screenshot of
the observability window

873
00:41:10,320 --> 00:41:14,040
inside CloudWatch.

874
00:41:14,040 --> 00:41:18,750
When you select the
observability dashboard

875
00:41:18,750 --> 00:41:22,500
inside of your agent's
view inside AgentCore,

876
00:41:22,500 --> 00:41:23,880
it's gonna take you to CloudWatch

877
00:41:23,880 --> 00:41:25,800
and you'll see something a lot like this,

878
00:41:25,800 --> 00:41:28,890
the different sessions and
traces that have happened

879
00:41:28,890 --> 00:41:31,830
within the agent's lifetime.

880
00:41:31,830 --> 00:41:33,810
Different layers of invocation.

881
00:41:33,810 --> 00:41:38,280
For example, the post request
to the SageMaker endpoint,

882
00:41:38,280 --> 00:41:40,730
which is like the final
layer of that invocation.

883
00:41:41,970 --> 00:41:46,603
It's superseded by an API
call to the SageMaker API

884
00:41:48,510 --> 00:41:52,020
and then it's wrapped
up in some other layers.

885
00:41:52,020 --> 00:41:55,080
And so we can kind of pull
this apart at different layers

886
00:41:55,080 --> 00:41:59,010
to see what's happening At
each layer of the invocation.

887
00:41:59,010 --> 00:42:03,060
We can also see the amount of load

888
00:42:03,060 --> 00:42:06,120
or the amount of resources
consumed by the agents.

889
00:42:06,120 --> 00:42:10,290
So how many virtual CPU
hours it's consumed,

890
00:42:10,290 --> 00:42:14,673
how much memory it's consumed
in gigabyte hours, et cetera.

891
00:42:16,530 --> 00:42:18,960
And wow, this is still going.

892
00:42:18,960 --> 00:42:19,793
There we go.

893
00:42:21,031 --> 00:42:22,940
Well, it wouldn't be a demo...

894
00:42:24,778 --> 00:42:27,195
(Dan laughs)

895
00:42:28,860 --> 00:42:30,870
We'll come back to that.

896
00:42:30,870 --> 00:42:33,783
If I were a braver man, I'd
run the same thing with Bodo3.

897
00:42:34,860 --> 00:42:36,480
At one point, this worked.

898
00:42:36,480 --> 00:42:39,480
So take a look at the output

899
00:42:39,480 --> 00:42:42,423
and this is an example of what
the trace might look like.

900
00:42:44,040 --> 00:42:48,600
This is a trace with an error,
not a timeout, unfortunately.

901
00:42:48,600 --> 00:42:50,580
This is an error that shows us

902
00:42:50,580 --> 00:42:52,740
we have malformed JSON.

903
00:42:52,740 --> 00:42:55,230
Some of the JSON keys or maybe values

904
00:42:55,230 --> 00:42:58,380
don't have the right number
of quotes around them.

905
00:42:58,380 --> 00:43:01,020
So it's gonna throw an error.

906
00:43:01,020 --> 00:43:04,320
We fixed that and now the
entire chain runs nicely.

907
00:43:04,320 --> 00:43:07,530
So this is a great example
of how you can use tracing

908
00:43:07,530 --> 00:43:09,213
to improve your,

909
00:43:10,800 --> 00:43:14,763
to kind of identify issues
deep within your execution.

910
00:43:16,350 --> 00:43:18,510
Now we have a few more minutes left.

911
00:43:18,510 --> 00:43:20,430
We haven't actually
built out any tools yet,

912
00:43:20,430 --> 00:43:24,360
so I'm gonna go through
the tools part of this

913
00:43:24,360 --> 00:43:26,940
and I'm gonna switch
gears away from Strands

914
00:43:26,940 --> 00:43:28,287
into Native MCP.

915
00:43:28,287 --> 00:43:29,610
And the reason I wanna do this is

916
00:43:29,610 --> 00:43:31,020
to showcase the capabilities

917
00:43:31,020 --> 00:43:33,363
of AgentCore Gateway at the very end.

918
00:43:34,680 --> 00:43:36,120
Effectively what we're doing here

919
00:43:36,120 --> 00:43:38,010
is building out another agent

920
00:43:38,010 --> 00:43:40,500
in almost exactly the same
way that we did before.

921
00:43:40,500 --> 00:43:42,420
But instead of using Strands

922
00:43:42,420 --> 00:43:44,970
and the SageMaker AI model object,

923
00:43:44,970 --> 00:43:49,620
we're building a custom MCP server,

924
00:43:49,620 --> 00:43:51,360
as well as an MCP client

925
00:43:51,360 --> 00:43:53,730
that will make calls to the server

926
00:43:53,730 --> 00:43:58,710
which has methods coded
into handle requests

927
00:43:58,710 --> 00:44:02,800
to the tools list and tools call endpoints

928
00:44:04,050 --> 00:44:05,763
as part of the MCP protocol.

929
00:44:07,590 --> 00:44:09,140
So again, we have much the same

930
00:44:10,860 --> 00:44:13,713
initialization logic at the
beginning of our notebook.

931
00:44:14,580 --> 00:44:17,850
We're gonna be creating
a Bedrock AgentCore role.

932
00:44:17,850 --> 00:44:20,378
This is a pretty standard policy document

933
00:44:20,378 --> 00:44:23,610
for a lambda function

934
00:44:23,610 --> 00:44:25,350
that's gonna be making calls

935
00:44:25,350 --> 00:44:28,143
to the Bedrock AgentCore Control Plane.

936
00:44:29,790 --> 00:44:32,403
And then we'll define our MCP server.

937
00:44:33,293 --> 00:44:34,980
Now this is where I wanna
spend a little time,

938
00:44:34,980 --> 00:44:37,110
and we only have five minutes left,

939
00:44:37,110 --> 00:44:39,270
so I'm gonna spend some
time walking through

940
00:44:39,270 --> 00:44:42,120
how we're implementing these
two methods, tools/list

941
00:44:42,120 --> 00:44:43,203
and tools/call.

942
00:44:44,280 --> 00:44:47,520
Tools/list prints out the list of tools

943
00:44:47,520 --> 00:44:52,170
that our MCP server knows
how to execute, right?

944
00:44:52,170 --> 00:44:55,646
Patient_search and save_medical_report.

945
00:44:55,646 --> 00:44:58,440
Patient_search is taking
in some general identifier,

946
00:44:58,440 --> 00:45:03,440
probably a compound key for
a patient_search database.

947
00:45:03,480 --> 00:45:06,510
In this scenario it's just
generically identifier

948
00:45:06,510 --> 00:45:08,460
and it doesn't matter
'cause I've hard coded

949
00:45:08,460 --> 00:45:11,703
to return one patient for demo purposes,

950
00:45:12,600 --> 00:45:13,800
and then save_medical_report,

951
00:45:13,800 --> 00:45:17,790
which takes in the content that
we're trying to upload to S3

952
00:45:17,790 --> 00:45:19,440
where we're uploading it to

953
00:45:19,440 --> 00:45:20,670
and what kind of format.

954
00:45:20,670 --> 00:45:22,470
Perhaps we wanna support more formats

955
00:45:22,470 --> 00:45:24,180
besides just markdown.

956
00:45:24,180 --> 00:45:25,800
You can imagine a world
where we're supporting

957
00:45:25,800 --> 00:45:29,010
lots of different patient portals.

958
00:45:29,010 --> 00:45:32,253
So we might wanna support
different kinds of file formats.

959
00:45:34,110 --> 00:45:36,120
Now we implementing, that was tools/list.

960
00:45:36,120 --> 00:45:41,120
Tools/call is taking in requests
from the lambda function

961
00:45:41,790 --> 00:45:44,940
and identifying which tool
we're actually calling,

962
00:45:44,940 --> 00:45:47,703
grabbing the parameters
that are passed into that,

963
00:45:48,750 --> 00:45:50,340
into that request

964
00:45:50,340 --> 00:45:53,490
and forwarding them on
to the method itself.

965
00:45:53,490 --> 00:45:56,190
So in the scenario where we're calling

966
00:45:56,190 --> 00:45:59,070
the save_medical_report tool,

967
00:45:59,070 --> 00:46:01,770
we're extracting the arguments

968
00:46:01,770 --> 00:46:04,833
and passing them to the
save_medical_report method.

969
00:46:05,730 --> 00:46:07,920
And the same for patient search.

970
00:46:07,920 --> 00:46:10,440
These methods are implemented down here.

971
00:46:10,440 --> 00:46:14,520
You can see patient_search
is just returning John Doe

972
00:46:14,520 --> 00:46:19,140
with a couple of, you know,
basic medical history.

973
00:46:19,140 --> 00:46:23,100
And save_medical_report is basically

974
00:46:23,100 --> 00:46:26,730
dropping the response
from the language model

975
00:46:26,730 --> 00:46:28,800
into a markdown object.

976
00:46:28,800 --> 00:46:31,560
Here it is, whatever that content is,

977
00:46:31,560 --> 00:46:34,830
it's getting dropped into a markdown file

978
00:46:34,830 --> 00:46:36,903
and uploaded to S3.

979
00:46:37,740 --> 00:46:39,393
Fairly basic capability,

980
00:46:40,890 --> 00:46:43,530
fairly basic MCP tool call that would,

981
00:46:43,530 --> 00:46:46,440
our model is now where our
agent will now be equipped

982
00:46:46,440 --> 00:46:48,333
to perform for us.

983
00:46:50,232 --> 00:46:53,977
So this has already been
deployed to AWS Lambda

984
00:46:57,360 --> 00:47:00,993
and we can now make calls to this.

985
00:47:01,950 --> 00:47:04,833
You can see it's all the same
code that we wrote before.

986
00:47:10,170 --> 00:47:12,750
We deploy the, this is the code block

987
00:47:12,750 --> 00:47:14,410
that deploys the Lambda function

988
00:47:16,170 --> 00:47:20,820
and then we recreate an agent.

989
00:47:20,820 --> 00:47:23,580
So the agent we're recreating here

990
00:47:23,580 --> 00:47:27,450
is effectively the same
agent that we had before.

991
00:47:27,450 --> 00:47:29,610
He uses that f-string notation

992
00:47:29,610 --> 00:47:33,573
to make it a little easier
to read inside of a notebook,

993
00:47:34,740 --> 00:47:38,130
but we're doing the same kind
of idea as what we did before.

994
00:47:38,130 --> 00:47:39,603
Now what makes this,

995
00:47:40,920 --> 00:47:42,390
when we're adding the ability

996
00:47:42,390 --> 00:47:45,740
for tools to be called here is...

997
00:47:51,567 --> 00:47:52,400
It's just searching through

998
00:47:52,400 --> 00:47:55,323
because it's a little
hard to find everything.

999
00:47:56,430 --> 00:48:00,690
We're applying a little bit
of determinism to our agent,

1000
00:48:00,690 --> 00:48:02,940
telling it which tools
to call in what order.

1001
00:48:05,340 --> 00:48:07,627
We're telling it which
patients have come in,

1002
00:48:07,627 --> 00:48:12,627
the agent is receiving the initial prompt

1003
00:48:12,840 --> 00:48:16,800
from whatever interface
that we've built out

1004
00:48:16,800 --> 00:48:18,490
and it's calling the mcp_gateway

1005
00:48:19,860 --> 00:48:22,293
to get the patient information,

1006
00:48:24,300 --> 00:48:27,240
perform some analysis on
that patient information

1007
00:48:27,240 --> 00:48:29,073
and generate a report out of that.

1008
00:48:32,640 --> 00:48:33,720
So all of this is,

1009
00:48:33,720 --> 00:48:35,790
I'm running out of time so I apologize,

1010
00:48:35,790 --> 00:48:39,603
I'm gonna skip to the actual
report that's generated.

1011
00:48:41,340 --> 00:48:44,970
And this is the markdown report
that's generated from this.

1012
00:48:44,970 --> 00:48:46,650
Can see the same information

1013
00:48:46,650 --> 00:48:47,550
that's printed out

1014
00:48:50,130 --> 00:48:52,170
inside our notebook is
now printed out here.

1015
00:48:52,170 --> 00:48:54,810
I downloaded this directly from S3

1016
00:48:54,810 --> 00:48:56,839
so it's not something that
I saved in my file system,

1017
00:48:56,839 --> 00:48:57,963
trust me.

1018
00:48:59,310 --> 00:49:01,060
It's definitely downloaded from S3.

1019
00:49:02,070 --> 00:49:04,960
And now we've enabled our model

1020
00:49:05,910 --> 00:49:10,290
to make tool calls within AgentCore,

1021
00:49:10,290 --> 00:49:12,030
within the AgentCore Runtime.

1022
00:49:12,030 --> 00:49:15,240
Now if we wanted to, we
can actually extend this

1023
00:49:15,240 --> 00:49:17,850
to not use native MCP

1024
00:49:17,850 --> 00:49:20,220
but to actually build a gateway.

1025
00:49:20,220 --> 00:49:22,800
We can actually build a
gateway using AgentCore.

1026
00:49:22,800 --> 00:49:25,740
So I've already taken
the liberty of doing that

1027
00:49:25,740 --> 00:49:27,840
and registered this lambda function

1028
00:49:27,840 --> 00:49:29,793
as an MCP target on the gateway.

1029
00:49:30,810 --> 00:49:35,380
If we go down to the bottom,
we're using the Request API

1030
00:49:38,610 --> 00:49:43,610
to make calls to the
gateway like list_tools()

1031
00:49:43,840 --> 00:49:45,007
and call_tool.

1032
00:49:46,920 --> 00:49:50,200
We're defining our gateway URL here

1033
00:49:51,879 --> 00:49:54,633
and using requests, we can
actually make calls to that.

1034
00:49:56,370 --> 00:49:58,020
And this is what the output looks like.

1035
00:49:58,020 --> 00:50:01,800
This is the JSON that
comes back from the gateway

1036
00:50:01,800 --> 00:50:03,810
using the MCP protocol.

1037
00:50:03,810 --> 00:50:05,520
These are the tools that are supported,

1038
00:50:05,520 --> 00:50:08,700
the patient_search and
generate_medical_report.

1039
00:50:08,700 --> 00:50:11,823
And this is the output
of calling those tools.

1040
00:50:13,500 --> 00:50:17,820
Patient search returns PATIENT-12345

1041
00:50:17,820 --> 00:50:20,550
named John Doe, et cetera.

1042
00:50:20,550 --> 00:50:24,090
And this is the structured
report located at this location

1043
00:50:24,090 --> 00:50:25,023
in S3.

1044
00:50:27,030 --> 00:50:29,790
So I apologize, I went
a little bit over time.

1045
00:50:29,790 --> 00:50:33,150
I'm going to pass it
over to Ying from SGLang

1046
00:50:33,150 --> 00:50:35,493
to take us home.

1047
00:50:51,180 --> 00:50:52,013
- Yeah.

1048
00:50:55,680 --> 00:50:56,589
Thank you.

1049
00:50:56,589 --> 00:50:57,960
Thank you for Amit and Dan

1050
00:50:57,960 --> 00:51:01,136
for the great introduction
about SageMaker ai.

1051
00:51:01,136 --> 00:51:04,530
I'd also like to give you some insights

1052
00:51:04,530 --> 00:51:06,360
about the underlying SGLang

1053
00:51:06,360 --> 00:51:08,340
so that you have a better idea of how

1054
00:51:08,340 --> 00:51:11,013
to use leverages advantage.

1055
00:51:12,690 --> 00:51:15,570
We, the SGLang, has been,

1056
00:51:15,570 --> 00:51:18,180
is a community building
open source project.

1057
00:51:18,180 --> 00:51:21,090
So the development momentum is continuous.

1058
00:51:21,090 --> 00:51:25,137
And I'd like to talk a
little bit about the,

1059
00:51:25,137 --> 00:51:29,100
the last quarter roadmap
of SGLang development.

1060
00:51:29,100 --> 00:51:31,890
And there are two focuses here.

1061
00:51:31,890 --> 00:51:35,280
First is we have spent
a lot of effort recently

1062
00:51:35,280 --> 00:51:37,950
on improving user experiences

1063
00:51:37,950 --> 00:51:40,810
and also we collaborating
with all the open source model

1064
00:51:41,820 --> 00:51:43,650
providers so that we aim

1065
00:51:43,650 --> 00:51:46,674
to run our latest open models efficiently

1066
00:51:46,674 --> 00:51:49,200
at large scale from day zero.

1067
00:51:49,200 --> 00:51:53,850
And second, we spend a lot recent focus

1068
00:51:53,850 --> 00:51:55,440
on big refactoring.

1069
00:51:55,440 --> 00:51:59,010
So to make all advanced features
compatible with each other

1070
00:51:59,010 --> 00:52:02,850
to achieving high
performance and usability.

1071
00:52:02,850 --> 00:52:06,780
So there are a list of
five major components here,

1072
00:52:06,780 --> 00:52:08,100
PD disaggregation

1073
00:52:08,100 --> 00:52:10,950
that we are keep improving the
compatibility now we adding

1074
00:52:10,950 --> 00:52:15,180
more features that was
supported in non-PD scenarios,

1075
00:52:15,180 --> 00:52:17,310
now also in PD scenarios

1076
00:52:17,310 --> 00:52:18,420
And speculative decoding,

1077
00:52:18,420 --> 00:52:22,757
we are implementing the
V2 version so that it's a,

1078
00:52:22,757 --> 00:52:24,520
with a better engineering design

1079
00:52:25,560 --> 00:52:27,650
to accommodate different algorithms,

1080
00:52:27,650 --> 00:52:32,136
speculative algorithms and
all kinds of parallelism.

1081
00:52:32,136 --> 00:52:36,270
We are doing Refactor on
PP for pipeline parallelism

1082
00:52:36,270 --> 00:52:38,100
and EP for export parallelism.

1083
00:52:38,100 --> 00:52:40,080
So that reduce the bubble

1084
00:52:40,080 --> 00:52:42,450
and make it more efficient.

1085
00:52:42,450 --> 00:52:44,130
For all kinds of memory pool,

1086
00:52:44,130 --> 00:52:48,990
the refactor work on the
memory pool V2 will support

1087
00:52:48,990 --> 00:52:50,880
different attention mechanisms,

1088
00:52:50,880 --> 00:52:53,070
especially optimized for
the hybrid attention,

1089
00:52:53,070 --> 00:52:54,630
including veneer attention,

1090
00:52:54,630 --> 00:52:56,550
window attention, full attention.

1091
00:52:56,550 --> 00:52:59,940
And overlap scheduler is
a unique feature in SGLang

1092
00:52:59,940 --> 00:53:03,780
that we carefully designed
the schedule logic

1093
00:53:03,780 --> 00:53:05,566
so that we overlap the CPU overhead

1094
00:53:05,566 --> 00:53:08,010
under GPU computation.

1095
00:53:08,010 --> 00:53:11,730
So achieve zero overhead in CPU

1096
00:53:11,730 --> 00:53:14,460
and then we are now working on making

1097
00:53:14,460 --> 00:53:17,793
all those five components to
be compatible with each other.

1098
00:53:19,170 --> 00:53:22,540
And this here emphasized on how

1099
00:53:24,030 --> 00:53:26,820
we focus on refactoring

1100
00:53:26,820 --> 00:53:30,000
because real engineering is
at the core of our philosophy.

1101
00:53:30,000 --> 00:53:33,450
Also like to give you some
highlight on other features,

1102
00:53:33,450 --> 00:53:36,270
one of them is hierarchical KV Cache.

1103
00:53:36,270 --> 00:53:39,000
This feature is, it will be useful,

1104
00:53:39,000 --> 00:53:42,810
very useful for agentic
application that have many turns

1105
00:53:42,810 --> 00:53:45,120
and huge cache reuse opportunities.

1106
00:53:45,120 --> 00:53:48,150
Amit also mentioned
that in previous slides.

1107
00:53:48,150 --> 00:53:51,720
SGLang for this feature,
we utilize multiple layers

1108
00:53:51,720 --> 00:53:53,460
of the storage to store the KV Cache,

1109
00:53:53,460 --> 00:53:55,080
including the GPU memory, CPU memory

1110
00:53:55,080 --> 00:53:59,580
and also the remote memory
that utilizing NVME.

1111
00:53:59,580 --> 00:54:03,060
We support multiple
backends, the DeepSeek 3FS,

1112
00:54:03,060 --> 00:54:04,770
Mooncake and NVIDIA NIXL.

1113
00:54:04,770 --> 00:54:08,820
And this is one experimental
result on a certain workload

1114
00:54:08,820 --> 00:54:12,870
that we can see it achieves
a better, much better latency

1115
00:54:12,870 --> 00:54:15,120
and also much higher throughput

1116
00:54:15,120 --> 00:54:16,830
because within this feature

1117
00:54:16,830 --> 00:54:18,753
we achieve much higher cache hit rate.

1118
00:54:20,730 --> 00:54:23,628
Another highlight,
Speculative Decoding V2,

1119
00:54:23,628 --> 00:54:25,320
Amit also mentioned before

1120
00:54:25,320 --> 00:54:26,670
that this Speculative Decoding

1121
00:54:26,670 --> 00:54:29,790
is a very powerful optimization,

1122
00:54:29,790 --> 00:54:31,200
especially for online serving

1123
00:54:31,200 --> 00:54:33,900
when you have a very small batch size.

1124
00:54:33,900 --> 00:54:37,230
It can give you usually two
to three times speed up.

1125
00:54:37,230 --> 00:54:41,850
And especially when we came
into the reasoning model,

1126
00:54:41,850 --> 00:54:44,400
the reasoning model usually
have tons of multiple tokens

1127
00:54:44,400 --> 00:54:46,503
and Speculative Decoding is, will be the,

1128
00:54:47,460 --> 00:54:48,293
a great way,

1129
00:54:51,073 --> 00:54:53,970
a very good way to
accelerate in that case.

1130
00:54:53,970 --> 00:54:58,010
And in V2 design we add one more feature,

1131
00:54:58,010 --> 00:55:00,360
so we make it compatible
with the overlap schedule.

1132
00:55:00,360 --> 00:55:02,190
So previously the Speculative Decoding

1133
00:55:02,190 --> 00:55:05,640
was implemented in a way that we

1134
00:55:05,640 --> 00:55:07,530
iterate for one decoding step

1135
00:55:07,530 --> 00:55:09,270
and then SGLang's CPU scheduling

1136
00:55:09,270 --> 00:55:11,370
and then another decoding step.

1137
00:55:11,370 --> 00:55:13,440
And in this new version we have been able

1138
00:55:13,440 --> 00:55:17,672
to decouple further for the CPU,

1139
00:55:17,672 --> 00:55:20,280
for the CPU control plan
and GPU compute plan.

1140
00:55:20,280 --> 00:55:22,353
So now we,

1141
00:55:23,280 --> 00:55:27,510
hide the CPU scheduling part
under the GPU computation

1142
00:55:27,510 --> 00:55:30,630
and not only remain one mapping

1143
00:55:30,630 --> 00:55:34,650
that store the value in GPU
and only the pointers in CPU.

1144
00:55:34,650 --> 00:55:36,330
So we only maintain that map,

1145
00:55:36,330 --> 00:55:38,790
then after each iteration
we just have a small step

1146
00:55:38,790 --> 00:55:41,760
to store the future
tensor to map to this map

1147
00:55:41,760 --> 00:55:44,229
and then read the future
tensors from the map.

1148
00:55:44,229 --> 00:55:49,223
This help us reduce the
overhead by further 10 to 20%.

1149
00:55:51,540 --> 00:55:54,690
So when we came into the
large scale serving world,

1150
00:55:54,690 --> 00:55:57,057
that's also always the focus of SGLang,

1151
00:55:57,057 --> 00:56:01,980
and we have been deployed in
production serving scenarios

1152
00:56:01,980 --> 00:56:04,260
for really very large scale.

1153
00:56:04,260 --> 00:56:06,210
So prefilled decode desegregation

1154
00:56:06,210 --> 00:56:07,470
and Wide Expert Parallelism

1155
00:56:07,470 --> 00:56:09,840
are two core features that have been used,

1156
00:56:09,840 --> 00:56:13,140
especially for the DeepSeek use cases.

1157
00:56:13,140 --> 00:56:16,620
Separate prefill and
decode endings can help us

1158
00:56:16,620 --> 00:56:18,660
to achieve better specialization

1159
00:56:18,660 --> 00:56:21,330
for prefill phase and the decode phase.

1160
00:56:21,330 --> 00:56:24,540
And so also we can have
a better latency control

1161
00:56:24,540 --> 00:56:27,480
and those for the Expert Parallelism

1162
00:56:27,480 --> 00:56:30,060
use a large degree of the EP to,

1163
00:56:30,060 --> 00:56:32,760
we can increase the
concurrency and the throughput.

1164
00:56:32,760 --> 00:56:35,550
And then the result is, we,

1165
00:56:35,550 --> 00:56:36,930
in the first half of this year,

1166
00:56:36,930 --> 00:56:39,000
we have been the first
open source framework

1167
00:56:39,000 --> 00:56:41,880
to match the official DeepSeek results.

1168
00:56:41,880 --> 00:56:44,550
And this is a demo so that
we can have the deployment

1169
00:56:44,550 --> 00:56:47,280
for the prefill with a
different paralleism strategy

1170
00:56:47,280 --> 00:56:48,960
with one paralleism strategy

1171
00:56:48,960 --> 00:56:51,750
and the decode with a
different paralleism strategy

1172
00:56:51,750 --> 00:56:54,123
and within the KB transfer
engine in between.

1173
00:56:55,050 --> 00:56:58,620
So moving forward we also
have first optimize this large

1174
00:56:58,620 --> 00:57:01,470
scale deployment on new
hardware especially so

1175
00:57:01,470 --> 00:57:04,683
that we came into the, we
go into the next slide that,

1176
00:57:06,900 --> 00:57:07,890
we came into the new world

1177
00:57:07,890 --> 00:57:10,440
of the Rack-Scale
Blackwell GPU deployments.

1178
00:57:10,440 --> 00:57:12,330
And the future of inference,

1179
00:57:12,330 --> 00:57:16,020
will be at rack scale
such as the GB200 MVL72.

1180
00:57:16,020 --> 00:57:18,600
So previously like we
deployed model on one,

1181
00:57:18,600 --> 00:57:22,080
no, four GPUs, but now we
have the 72 GPUs in one rack,

1182
00:57:22,080 --> 00:57:23,700
they're connected by unlink.

1183
00:57:23,700 --> 00:57:26,490
So that enables us to do more optimization

1184
00:57:26,490 --> 00:57:28,230
for the large, really large scale PDNA,

1185
00:57:28,230 --> 00:57:30,630
especially the EP, Expert Parallelism.

1186
00:57:30,630 --> 00:57:33,330
SGLang optimizes it
then with prefill-decode

1187
00:57:33,330 --> 00:57:35,490
like the PD that we mentioned

1188
00:57:35,490 --> 00:57:36,960
and also the wide Expert Parallelism,

1189
00:57:36,960 --> 00:57:39,720
we think by utilizing the new features,

1190
00:57:39,720 --> 00:57:41,370
new kernels for Blackwell

1191
00:57:41,370 --> 00:57:42,250
and also the new

1192
00:57:45,173 --> 00:57:47,133
auto communication kernels.

1193
00:57:48,420 --> 00:57:52,590
And this is one result we
published in summer that shows

1194
00:57:52,590 --> 00:57:56,850
how much performance gain
we have for the GB200.

1195
00:57:56,850 --> 00:57:58,863
We're now also working on the GB300.

1196
00:58:01,440 --> 00:58:02,280
So that will be,

1197
00:58:02,280 --> 00:58:05,400
I think that I can just
summarize here for the,

1198
00:58:05,400 --> 00:58:07,020
those highlight features

1199
00:58:07,020 --> 00:58:09,420
and those, some of the features

1200
00:58:09,420 --> 00:58:11,130
are still in experimental status,

1201
00:58:11,130 --> 00:58:13,680
so they're not always
turned on by default.

1202
00:58:13,680 --> 00:58:16,107
So if you, I always
encourage you to go to our,

1203
00:58:16,107 --> 00:58:17,730
our documentation website

1204
00:58:17,730 --> 00:58:19,590
to see what's the most recent updates

1205
00:58:19,590 --> 00:58:21,660
and the guides

1206
00:58:21,660 --> 00:58:23,463
to turn on those optimizations.

1207
00:58:25,230 --> 00:58:27,480
Yeah, and the last thing is we recently,

1208
00:58:27,480 --> 00:58:29,730
also recent last month we released

1209
00:58:29,730 --> 00:58:32,790
the component in addition
to language model.

1210
00:58:32,790 --> 00:58:35,640
We now support image and video generation,

1211
00:58:35,640 --> 00:58:37,380
also image in, image out.

1212
00:58:37,380 --> 00:58:40,740
And this we call this SGLang Diffusion

1213
00:58:40,740 --> 00:58:42,303
that accelerates these,

1214
00:58:43,350 --> 00:58:47,670
this image video generation
by 1.2 times generally

1215
00:58:47,670 --> 00:58:51,390
and sometimes in special cases
could up to six times a bot.

1216
00:58:51,390 --> 00:58:54,240
In future models we think
will be multimodal-in,

1217
00:58:54,240 --> 00:58:57,150
multimodal-out that are combining

1218
00:58:57,150 --> 00:59:01,050
in auto regressive language
models and diffusion models.

1219
00:59:01,050 --> 00:59:04,950
And this, in this release we
have from this metric benchmark

1220
00:59:04,950 --> 00:59:07,260
that we compared with
the existing diffuser,

1221
00:59:07,260 --> 00:59:10,080
Hugging Face diffuser, you
can see the performance gain

1222
00:59:10,080 --> 00:59:14,730
and also we the, on the
right figure we list

1223
00:59:14,730 --> 00:59:18,030
how those different
optimization techniques can help

1224
00:59:18,030 --> 00:59:20,880
with the performance throughput.

1225
00:59:20,880 --> 00:59:23,250
And majorly there are two techniques.

1226
00:59:23,250 --> 00:59:26,490
One is a sequential sequence parallelism

1227
00:59:26,490 --> 00:59:29,550
and another one is CFG parallelism.

1228
00:59:29,550 --> 00:59:31,620
And when we combine them, combine the two,

1229
00:59:31,620 --> 00:59:35,643
we got even higher improvement,
even better improvement.

1230
00:59:39,330 --> 00:59:41,583
Yeah, I think that ends my part then.

1231
00:59:48,390 --> 00:59:50,673
- Okay, so we covered
a lot of ground today.

1232
00:59:51,570 --> 00:59:54,060
We, I'm just gonna do a
quick recap of the challenges

1233
00:59:54,060 --> 00:59:56,973
that we referred to in the very beginning.

1234
00:59:59,160 --> 00:59:59,993
Here we go.

1235
01:00:00,990 --> 01:00:04,230
Model customization can be
a time consuming process.

1236
01:00:04,230 --> 01:00:07,620
It can be a little expensive at times

1237
01:00:07,620 --> 01:00:12,603
and it may be unclear how
you optimize that workflow.

1238
01:00:14,280 --> 01:00:16,320
What we've discussed today is really meant

1239
01:00:16,320 --> 01:00:18,360
to give you a couple of tips and tricks

1240
01:00:18,360 --> 01:00:20,760
on how you might do that, right?

1241
01:00:20,760 --> 01:00:22,520
Taking into consideration...

1242
01:00:23,730 --> 01:00:25,140
I think the clicker stopped working.

1243
01:00:25,140 --> 01:00:25,973
There we go.

1244
01:00:27,180 --> 01:00:29,820
How we might leverage
fully managed training jobs

1245
01:00:29,820 --> 01:00:33,660
to allow you to continuously
repeat these experiments

1246
01:00:33,660 --> 01:00:34,493
over and over

1247
01:00:34,493 --> 01:00:37,020
and optimize that way
without having to manage all

1248
01:00:37,020 --> 01:00:38,733
of the experimentation harness.

1249
01:00:39,900 --> 01:00:42,240
We talked about how you might host

1250
01:00:42,240 --> 01:00:43,650
SageMaker models more effectively.

1251
01:00:43,650 --> 01:00:45,720
And Ying just went through
several great stats about

1252
01:00:45,720 --> 01:00:49,050
how SGLang is a really
great model serving engine

1253
01:00:49,050 --> 01:00:52,413
for efficient model serving and inference,

1254
01:00:53,970 --> 01:00:55,980
end to end observability
with managed MLflow

1255
01:00:55,980 --> 01:00:58,503
and now managed MLflow
serverless, which is a,

1256
01:01:01,115 --> 01:01:03,810
I think it's a little
more cost effective than

1257
01:01:03,810 --> 01:01:05,793
hosted MLflow.

1258
01:01:07,115 --> 01:01:09,930
And we talked about using
SageMaker Pipelines to be able

1259
01:01:09,930 --> 01:01:11,970
to repeat these experiments over and over

1260
01:01:11,970 --> 01:01:13,680
and over again so

1261
01:01:13,680 --> 01:01:16,893
that you can effectively
scale your experimentation.

1262
01:01:18,540 --> 01:01:22,300
Finally, tracking and auditing

1263
01:01:23,190 --> 01:01:26,220
and versioning models within
SageMaker Model Registry,

1264
01:01:26,220 --> 01:01:29,100
MLflow model registry,
giving you the ability

1265
01:01:29,100 --> 01:01:30,810
to determine exactly

1266
01:01:30,810 --> 01:01:33,840
where you're gonna focus
your optimization efforts.

1267
01:01:33,840 --> 01:01:35,310
So thank you all for attending.

1268
01:01:35,310 --> 01:01:38,670
This is a link to the demo video.

1269
01:01:38,670 --> 01:01:41,220
The code is not available yet on GitHub.

1270
01:01:41,220 --> 01:01:44,190
It will be at some point soon,

1271
01:01:44,190 --> 01:01:46,980
but this is a link to the demo video

1272
01:01:46,980 --> 01:01:50,820
and if you go into your
app, the AWS events app,

1273
01:01:50,820 --> 01:01:55,820
don't forget to leave a
review for the session so

1274
01:01:55,830 --> 01:01:58,143
that we know how to
improve it for next time.

1275
01:01:59,610 --> 01:02:01,405
Thank you very much.

1276
01:02:01,405 --> 01:02:03,886
(audience claps)

