1
00:00:01,110 --> 00:00:03,870
- Thank you, thank you for
coming to this session.

2
00:00:03,870 --> 00:00:08,280
It's so early on a Monday
and it's almost like

3
00:00:08,280 --> 00:00:11,640
all of us here are kicking
off this re:Invent 2025.

4
00:00:11,640 --> 00:00:14,160
So, you know, kudos to all of us.

5
00:00:14,160 --> 00:00:16,230
My name is Raghu,

6
00:00:16,230 --> 00:00:20,007
I'm a technical product manager at,

7
00:00:20,007 --> 00:00:22,140
a product marketing manager at AWS.

8
00:00:22,140 --> 00:00:27,140
I've been with the company
almost 10 years in a few weeks.

9
00:00:27,450 --> 00:00:30,930
So, welcome to today's session,

10
00:00:30,930 --> 00:00:33,210
which is Enabling AI Innovation

11
00:00:33,210 --> 00:00:36,623
with Amazon SageMaker
Unified Studio, okay?

12
00:00:37,830 --> 00:00:39,150
I'll kick things off,

13
00:00:39,150 --> 00:00:42,960
and I have two other speakers here today.

14
00:00:42,960 --> 00:00:44,790
Terri Sutherland.

15
00:00:44,790 --> 00:00:49,200
She is the general
manager for data platforms

16
00:00:49,200 --> 00:00:52,080
at Commonwealth Bank of Australia.

17
00:00:52,080 --> 00:00:55,350
So for short, you know,
we'll call it CommBank.

18
00:00:55,350 --> 00:00:56,580
So she's gonna come on,

19
00:00:56,580 --> 00:01:00,543
and then we have our star
solution architect, Praveen,

20
00:01:02,130 --> 00:01:03,630
who is also from Australia.

21
00:01:03,630 --> 00:01:07,350
So very thankful to Terri
and Praveen for coming here

22
00:01:07,350 --> 00:01:12,330
all the way from Australia
to do this session with us.

23
00:01:12,330 --> 00:01:16,353
So, oh, I should have done this first.

24
00:01:17,850 --> 00:01:19,200
So this is today's agenda.

25
00:01:19,200 --> 00:01:22,140
So like I said, I'm gonna kick
things off, set the stage,

26
00:01:22,140 --> 00:01:25,200
and then I'm going to
hand this off to Terri.

27
00:01:25,200 --> 00:01:29,880
Terri is going to walk us
through CommBank's journey

28
00:01:29,880 --> 00:01:33,030
using SageMaker Unified Studio

29
00:01:33,030 --> 00:01:35,823
and some of the other
capabilities within SageMaker.

30
00:01:36,930 --> 00:01:40,200
Problem statement, outcomes,

31
00:01:40,200 --> 00:01:42,360
impact that they have had in the bank.

32
00:01:42,360 --> 00:01:45,060
And then Praveen is going to come on

33
00:01:45,060 --> 00:01:47,910
and he is going to walk us
through the architecture

34
00:01:47,910 --> 00:01:52,320
that they built within
CommBank and also do a demo.

35
00:01:52,320 --> 00:01:55,560
So this is going to get very technical

36
00:01:55,560 --> 00:01:58,260
once Praveen comes on,
and I'm sure, you know,

37
00:01:58,260 --> 00:02:01,920
those bits are going to be
some of the more exciting parts

38
00:02:01,920 --> 00:02:04,080
of today's presentation.

39
00:02:04,080 --> 00:02:04,913
So.

40
00:02:06,752 --> 00:02:08,040
Okay.

41
00:02:08,040 --> 00:02:11,190
so we're gonna start off with
some data, some research.

42
00:02:11,190 --> 00:02:13,920
So this is a research analysis done

43
00:02:13,920 --> 00:02:17,580
by Harvard Business Review this year.

44
00:02:17,580 --> 00:02:19,770
And basically what they are telling us

45
00:02:19,770 --> 00:02:24,770
is 89% of CDOs have an initiative

46
00:02:25,320 --> 00:02:28,950
to build applications
using AI and generative AI.

47
00:02:28,950 --> 00:02:33,420
So majority of them are
building applications with AI

48
00:02:33,420 --> 00:02:38,420
and gen AI, but an
overwhelming 52% of those CDOs

49
00:02:39,720 --> 00:02:43,860
don't think they are ready
to embark on the AI journey.

50
00:02:43,860 --> 00:02:48,860
And the reason for that is
they don't have enough trust

51
00:02:49,140 --> 00:02:50,700
in their own data.

52
00:02:50,700 --> 00:02:53,880
They don't have enough
faith in their own data

53
00:02:53,880 --> 00:02:56,190
because they don't know their data.

54
00:02:56,190 --> 00:03:00,750
So as much as we all want
to do exciting things

55
00:03:00,750 --> 00:03:05,750
with AI and generative AI,
agentic this and agentic that,

56
00:03:06,030 --> 00:03:08,040
I hate to break it to you.

57
00:03:08,040 --> 00:03:10,500
We kind of have to go back to the basics

58
00:03:10,500 --> 00:03:13,980
and get our house in order with our data.

59
00:03:13,980 --> 00:03:15,423
So what does that mean?

60
00:03:16,680 --> 00:03:21,030
It means building a sound data foundation

61
00:03:21,030 --> 00:03:23,580
within the organization, okay?

62
00:03:23,580 --> 00:03:28,170
And at AWS, we look at this as
two very broad things, okay?

63
00:03:28,170 --> 00:03:29,880
Well, how do you build a data foundation?

64
00:03:29,880 --> 00:03:32,520
We're gonna talk about that a little bit.

65
00:03:32,520 --> 00:03:35,010
And then we look at this
as two very broad things.

66
00:03:35,010 --> 00:03:39,093
One is improving data literacy
within the organization,

67
00:03:40,290 --> 00:03:43,020
which is, do you know about your data?

68
00:03:43,020 --> 00:03:44,400
Do you know what's good?

69
00:03:44,400 --> 00:03:45,900
Do you know what good looks like?

70
00:03:45,900 --> 00:03:48,510
These are not easy
questions to answer, okay?

71
00:03:48,510 --> 00:03:50,880
So improve the data literacy
within the organization,

72
00:03:50,880 --> 00:03:55,650
and the second is build
a data culture, okay?

73
00:03:55,650 --> 00:03:58,470
And we will look at these
two parts, you know,

74
00:03:58,470 --> 00:04:01,020
a brief overview of what this means,

75
00:04:01,020 --> 00:04:05,313
and then, you know, we'll go
to CBA story or CommBank story.

76
00:04:06,180 --> 00:04:07,410
Okay.

77
00:04:07,410 --> 00:04:12,030
So I may have spent majority of my time

78
00:04:12,030 --> 00:04:15,390
here at AWS doing this with our customers.

79
00:04:15,390 --> 00:04:19,950
And I believe that most folks who are here

80
00:04:19,950 --> 00:04:22,500
can relate to many projects that

81
00:04:22,500 --> 00:04:27,360
you may be currently working
on that does this exact thing.

82
00:04:27,360 --> 00:04:31,200
So there are seven data actions

83
00:04:31,200 --> 00:04:34,500
as part of improving data
literacy within the organization.

84
00:04:34,500 --> 00:04:36,960
This is not an exhaustive list, you know,

85
00:04:36,960 --> 00:04:41,100
this is some of the main
items that most customers do

86
00:04:41,100 --> 00:04:43,440
to improve the data literacy
within their organization.

87
00:04:43,440 --> 00:04:45,780
Let's just kind of go
through this one by one,

88
00:04:45,780 --> 00:04:48,180
you know, 20, 30 seconds each.

89
00:04:48,180 --> 00:04:49,860
Build a data inventory.

90
00:04:49,860 --> 00:04:51,003
What does that mean?

91
00:04:52,710 --> 00:04:54,600
It can mean different things,

92
00:04:54,600 --> 00:04:56,700
but broadly this is what it is.

93
00:04:56,700 --> 00:05:00,390
It is a directory, a
list of all your data,

94
00:05:00,390 --> 00:05:04,050
and that includes tables,
that includes S3 buckets,

95
00:05:04,050 --> 00:05:07,230
you know, with structured
data, S3 buckets with images.

96
00:05:07,230 --> 00:05:11,070
It also includes
applications like dashboards,

97
00:05:11,070 --> 00:05:14,013
models, all of it, okay?

98
00:05:14,848 --> 00:05:18,750
You should build a list of all that data.

99
00:05:18,750 --> 00:05:21,450
We call them data assets, okay?

100
00:05:21,450 --> 00:05:24,540
This in itself can be a
pretty elaborate topic,

101
00:05:24,540 --> 00:05:27,480
but I'm gonna just do this really quickly.

102
00:05:27,480 --> 00:05:29,670
You can build this a few different ways.

103
00:05:29,670 --> 00:05:32,910
One is you can start at
the bottom with the data

104
00:05:32,910 --> 00:05:35,820
and build an inventory
off that, which is tables,

105
00:05:35,820 --> 00:05:37,110
S3 buckets, and so on.

106
00:05:37,110 --> 00:05:39,360
Some customers like to do it from the top,

107
00:05:39,360 --> 00:05:41,070
which is from the application side.

108
00:05:41,070 --> 00:05:44,580
They like to build an inventory
off their BI dashboards

109
00:05:44,580 --> 00:05:47,310
and models and then work their way down.

110
00:05:47,310 --> 00:05:50,310
Some customers just start
with a line of business,

111
00:05:50,310 --> 00:05:52,650
figure it out, and get a process going

112
00:05:52,650 --> 00:05:54,540
in terms of building that inventory

113
00:05:54,540 --> 00:05:56,580
and then rinse and repeat that

114
00:05:56,580 --> 00:05:59,070
from line of business to line of business.

115
00:05:59,070 --> 00:06:01,710
And CommBank is going to talk
a little bit more about that.

116
00:06:01,710 --> 00:06:05,280
So that's what building
a data inventory means

117
00:06:05,280 --> 00:06:06,540
at a high level.

118
00:06:06,540 --> 00:06:08,490
Second is data annotation.

119
00:06:08,490 --> 00:06:11,460
And this is probably the most boring part

120
00:06:11,460 --> 00:06:14,490
of the whole journey, but
there is generative AI

121
00:06:14,490 --> 00:06:16,740
to help all of us, but
what does that mean?

122
00:06:16,740 --> 00:06:19,290
When you build a data
inventory, what does it contain?

123
00:06:19,290 --> 00:06:22,110
It just contains metadata, you know?

124
00:06:22,110 --> 00:06:24,420
Just cryptic table names and column names,

125
00:06:24,420 --> 00:06:26,850
and no one can really make sense out of it

126
00:06:26,850 --> 00:06:29,910
whether humans or agents

127
00:06:29,910 --> 00:06:31,860
can make sense out of technical data.

128
00:06:31,860 --> 00:06:33,960
And in most cases, we
will all get it wrong.

129
00:06:33,960 --> 00:06:38,960
So data annotation means putting,

130
00:06:38,970 --> 00:06:43,290
describing, describing technical data

131
00:06:43,290 --> 00:06:45,360
and giving it business context.

132
00:06:45,360 --> 00:06:48,120
This is customer table for product X,

133
00:06:48,120 --> 00:06:51,000
this is a regional
customer table, et cetera.

134
00:06:51,000 --> 00:06:55,740
Someone or something will have
to go and annotate this data.

135
00:06:55,740 --> 00:06:58,983
And like I said, not very
exciting, but very important.

136
00:07:01,230 --> 00:07:03,810
From there, detection of PII data.

137
00:07:03,810 --> 00:07:06,390
If you're going to be
doing modeling, training,

138
00:07:06,390 --> 00:07:08,910
you're retraining a foundational model.

139
00:07:08,910 --> 00:07:13,910
You do not want PII data
as part of that project.

140
00:07:14,370 --> 00:07:17,010
So detection of PII data, super important.

141
00:07:17,010 --> 00:07:20,580
Once you detect PII data,
what do you do next?

142
00:07:20,580 --> 00:07:22,230
You categorize it.

143
00:07:22,230 --> 00:07:25,140
Most sensitive, least
sensitive, et cetera,

144
00:07:25,140 --> 00:07:30,140
that fits your company's security SLAs.

145
00:07:30,270 --> 00:07:32,970
From there, data quality.

146
00:07:32,970 --> 00:07:34,170
How good is this data?

147
00:07:34,170 --> 00:07:35,883
We all talk about bad data.

148
00:07:36,930 --> 00:07:38,700
How bad is it?

149
00:07:38,700 --> 00:07:43,700
So data quality is part of
building the data literacy

150
00:07:43,710 --> 00:07:45,690
within the organization.

151
00:07:45,690 --> 00:07:49,320
Then lineage, most people
here probably know what it is.

152
00:07:49,320 --> 00:07:52,200
Data origin, data transformation.

153
00:07:52,200 --> 00:07:54,150
Where did this data come from?

154
00:07:54,150 --> 00:07:55,830
You know, that's being tracked.

155
00:07:55,830 --> 00:08:00,830
And then finally,
publishing new data assets.

156
00:08:00,960 --> 00:08:03,360
So this is one of the areas

157
00:08:03,360 --> 00:08:06,900
where I have seen customers struggle with.

158
00:08:06,900 --> 00:08:10,860
You do all of it, but your
data inventory is not static.

159
00:08:10,860 --> 00:08:13,410
Your data inventory is ever changing.

160
00:08:13,410 --> 00:08:16,260
So you have to build the practice

161
00:08:16,260 --> 00:08:17,363
where when you do all of this,

162
00:08:17,363 --> 00:08:20,010
this is a significant investment.

163
00:08:20,010 --> 00:08:21,720
New assets are created every day.

164
00:08:21,720 --> 00:08:25,410
You gotta put it back in the inventory.

165
00:08:25,410 --> 00:08:28,080
So it's a practice, it's a habit.

166
00:08:28,080 --> 00:08:30,240
Many customers do it really well.

167
00:08:30,240 --> 00:08:34,290
A lot of customers are still
trying to figure it out.

168
00:08:34,290 --> 00:08:35,123
Okay.

169
00:08:36,120 --> 00:08:39,510
Like I said, been doing
this for a long time.

170
00:08:39,510 --> 00:08:44,160
And I just want to just
leave you with this,

171
00:08:44,160 --> 00:08:46,470
you know, and a couple more slides.

172
00:08:46,470 --> 00:08:48,330
Choose your own journey with this

173
00:08:48,330 --> 00:08:51,390
in improving the data literacy
within the organization.

174
00:08:51,390 --> 00:08:55,560
What I have on the screen,
it's just a recommendation,

175
00:08:55,560 --> 00:08:58,890
it's a suggestion, but
choose your own adventure

176
00:08:58,890 --> 00:09:02,400
in improving the data literacy
within your companies.

177
00:09:02,400 --> 00:09:04,560
Most companies start with PII

178
00:09:04,560 --> 00:09:07,410
because they want to de-risk, okay?

179
00:09:07,410 --> 00:09:10,440
Then they go to data
quality and then inventory.

180
00:09:10,440 --> 00:09:11,790
So, so on, right?

181
00:09:11,790 --> 00:09:15,870
So pick your journey and then stick to it.

182
00:09:15,870 --> 00:09:18,990
So this is, if you take a
step back and look at it,

183
00:09:18,990 --> 00:09:23,010
and even if you did, four
or five of these things,

184
00:09:23,010 --> 00:09:24,810
you will be significantly improving

185
00:09:24,810 --> 00:09:27,450
the data literacy footprint
within the organization.

186
00:09:27,450 --> 00:09:28,530
Okay.

187
00:09:28,530 --> 00:09:30,720
So that's data literacy.

188
00:09:30,720 --> 00:09:33,750
Second, building a data culture.

189
00:09:33,750 --> 00:09:35,403
The best way to explain this,

190
00:09:37,740 --> 00:09:42,720
we all have learned to be
very social digitally, right?

191
00:09:42,720 --> 00:09:47,523
You know, with Facebooks and
in LinkedIns of the world.

192
00:09:48,900 --> 00:09:53,880
We all have built a pretty
good social culture.

193
00:09:53,880 --> 00:09:55,590
The same thing applies to data culture.

194
00:09:55,590 --> 00:09:57,903
I'm just gonna draw some parallels here.

195
00:09:59,580 --> 00:10:04,580
When you want to make a social platform,

196
00:10:04,920 --> 00:10:09,720
okay, either it is for videos
or whether it's for data,

197
00:10:09,720 --> 00:10:11,400
you really have three things.

198
00:10:11,400 --> 00:10:14,160
You have producers, content creators,

199
00:10:14,160 --> 00:10:17,820
you have consumers who consume
what the producers create,

200
00:10:17,820 --> 00:10:20,100
and they like, comment, use.

201
00:10:20,100 --> 00:10:23,820
And then there is a third party,
which is the central team.

202
00:10:23,820 --> 00:10:27,180
The central team provides
you with the platform.

203
00:10:27,180 --> 00:10:30,840
They provide you with guardrails,
policies, best practices,

204
00:10:30,840 --> 00:10:34,590
do's and don'ts, so the whole activity

205
00:10:34,590 --> 00:10:38,310
of being social is sustainable.

206
00:10:38,310 --> 00:10:43,310
CommBank built this and
they will walk you through

207
00:10:43,500 --> 00:10:45,900
their story, how they started,

208
00:10:45,900 --> 00:10:49,440
and how they built once
they kind of got it right,

209
00:10:49,440 --> 00:10:52,840
and how they kind of
expanded their expertise

210
00:10:53,995 --> 00:10:56,220
and built the data culture
within the organization.

211
00:10:56,220 --> 00:10:58,380
So this is extremely important.

212
00:10:58,380 --> 00:11:02,910
And again, when you build a data culture,

213
00:11:02,910 --> 00:11:07,500
it really needs to be in
the organization's DNA.

214
00:11:07,500 --> 00:11:12,000
It needs to sustain executive churns.

215
00:11:12,000 --> 00:11:15,750
So like I said, these things take time.

216
00:11:15,750 --> 00:11:17,820
You have to build it over time

217
00:11:17,820 --> 00:11:22,173
and you gotta make that a habit
within the entire company.

218
00:11:23,160 --> 00:11:27,150
Okay, so we talked about data foundations,

219
00:11:27,150 --> 00:11:29,700
we talked about data literacy,

220
00:11:29,700 --> 00:11:33,090
we talked about how you
build a data culture,

221
00:11:33,090 --> 00:11:35,970
and none of these things
had anything to do

222
00:11:35,970 --> 00:11:37,380
with AWS services.

223
00:11:37,380 --> 00:11:40,050
These are just in a good data practices.

224
00:11:40,050 --> 00:11:42,480
So how can we help you?

225
00:11:42,480 --> 00:11:44,820
If this is important to you,

226
00:11:44,820 --> 00:11:47,730
if PII detection,
building a data inventory,

227
00:11:47,730 --> 00:11:50,190
building the data culture
is important to you,

228
00:11:50,190 --> 00:11:51,750
well, how do you get started?

229
00:11:51,750 --> 00:11:55,713
Well, last re:Invent, re:Invent 2024,

230
00:11:57,078 --> 00:11:59,100
we released, we rereleased,

231
00:11:59,100 --> 00:12:02,760
relaunched, I should say,
Amazon SageMaker, okay?

232
00:12:02,760 --> 00:12:07,170
It is our data and AI platform.

233
00:12:07,170 --> 00:12:10,260
It's got lots of features and functions

234
00:12:10,260 --> 00:12:14,790
and we broadly look at Amazon SageMaker

235
00:12:14,790 --> 00:12:17,160
like a three-layer cake, okay?

236
00:12:17,160 --> 00:12:18,903
We'll go from bottom up.

237
00:12:19,920 --> 00:12:24,003
At the bottom, we have the
lakehouse architecture,

238
00:12:25,650 --> 00:12:29,850
all Iceberg-based data architectures,

239
00:12:29,850 --> 00:12:32,880
Iceberg REST Catalog functionality,

240
00:12:32,880 --> 00:12:36,753
lot of managed capabilities
built into our lakehouse.

241
00:12:37,590 --> 00:12:39,990
So we got sessions on lakehouse,

242
00:12:39,990 --> 00:12:42,150
so definitely check those out.

243
00:12:42,150 --> 00:12:45,750
And on top of it is the
data in AI governance.

244
00:12:45,750 --> 00:12:50,670
Most of what I just talked
about in the last 10, 12 minutes

245
00:12:50,670 --> 00:12:54,570
fits into this data and
AI governance capabilities

246
00:12:54,570 --> 00:12:57,900
built in Amazon SageMaker, okay?

247
00:12:57,900 --> 00:13:02,900
And at the top, we have
unified data experience.

248
00:13:03,030 --> 00:13:04,980
Well, what does that mean?

249
00:13:04,980 --> 00:13:06,810
So you got a lakehouse,

250
00:13:06,810 --> 00:13:09,270
you have good data
governance capabilities,

251
00:13:09,270 --> 00:13:10,950
which means you can search data,

252
00:13:10,950 --> 00:13:14,520
you can find data, get access to data.

253
00:13:14,520 --> 00:13:16,230
Then what do you do with it?

254
00:13:16,230 --> 00:13:20,253
Well, that's where the
UnifiedUI comes into picture.

255
00:13:21,120 --> 00:13:25,950
SQL capabilities, data processing,
building data pipelines,

256
00:13:25,950 --> 00:13:29,670
generative AI app development are all part

257
00:13:29,670 --> 00:13:32,280
of our Unified user experience.

258
00:13:32,280 --> 00:13:34,200
But obviously we are not stopping there.

259
00:13:34,200 --> 00:13:37,800
We are, you know,
building more capabilities

260
00:13:37,800 --> 00:13:41,310
like streaming and BI
and even search analytics

261
00:13:41,310 --> 00:13:45,270
and some of these capabilities
will come in 2026.

262
00:13:45,270 --> 00:13:49,500
So this is just zooming into

263
00:13:49,500 --> 00:13:53,910
the Unified Studio
experience within SageMaker.

264
00:13:53,910 --> 00:13:56,670
As you can imagine, this
is meant for data workers,

265
00:13:56,670 --> 00:13:58,020
whether you are doing SQL,

266
00:13:58,020 --> 00:14:02,790
whether you're doing modeling,
training, building pipelines,

267
00:14:02,790 --> 00:14:05,040
we have notebooks as you can imagine.

268
00:14:05,040 --> 00:14:09,270
We have notebook experiences
built into the studio.

269
00:14:09,270 --> 00:14:11,880
We have query editors
built into the studio.

270
00:14:11,880 --> 00:14:13,533
We have, this is my favorite,

271
00:14:14,820 --> 00:14:18,060
visual low-code, no-code experience

272
00:14:18,060 --> 00:14:19,440
built into the studio as well,

273
00:14:19,440 --> 00:14:23,353
which means you can build data pipelines,

274
00:14:26,310 --> 00:14:30,513
you can build, you know, ETL
jobs with absolutely no code.

275
00:14:32,070 --> 00:14:35,100
And then we also have
generative AI chatbot

276
00:14:35,100 --> 00:14:37,980
building experiences as well.

277
00:14:37,980 --> 00:14:40,410
And then it's my last slide

278
00:14:40,410 --> 00:14:43,410
before I, you know,
get Terri on the stage.

279
00:14:43,410 --> 00:14:47,610
So this is the second
layer of Amazon SageMaker,

280
00:14:47,610 --> 00:14:52,440
and this is, like I said,
most of what I discussed,

281
00:14:52,440 --> 00:14:56,310
which is data literacy and
building the data culture

282
00:14:56,310 --> 00:14:59,487
within the organization
are all part of the data

283
00:14:59,487 --> 00:15:02,850
and AI governance
functionality within SageMaker.

284
00:15:02,850 --> 00:15:07,500
You can do PII detection, you
can do data quality checks.

285
00:15:07,500 --> 00:15:10,353
We have an awesome business data catalog,

286
00:15:11,400 --> 00:15:14,700
awesomely priced, almost free.

287
00:15:14,700 --> 00:15:18,870
So it's got great capabilities, you know,

288
00:15:18,870 --> 00:15:20,550
definitely check those out.

289
00:15:20,550 --> 00:15:22,860
And then, you know,
hopefully you start building

290
00:15:22,860 --> 00:15:26,370
your amazing data practice
on Amazon SageMaker.

291
00:15:26,370 --> 00:15:29,973
Before I get Terri on, I
do wanna mention one thing.

292
00:15:31,410 --> 00:15:32,700
You saw the three layers.

293
00:15:32,700 --> 00:15:35,070
I should have said this
a couple of slides ago.

294
00:15:35,070 --> 00:15:37,230
You saw the three layers.

295
00:15:37,230 --> 00:15:42,230
You don't have to use all
of it all at the same time.

296
00:15:42,270 --> 00:15:45,300
You can start your journey anywhere, okay?

297
00:15:45,300 --> 00:15:49,980
If data governance is the
most burning problem for you,

298
00:15:49,980 --> 00:15:51,840
then you can start with data governance.

299
00:15:51,840 --> 00:15:54,270
If lakehouse, then start with lakehouse.

300
00:15:54,270 --> 00:15:57,498
And then you don't have to use everything.

301
00:15:57,498 --> 00:15:59,100
We truly believe if you use all of it,

302
00:15:59,100 --> 00:16:01,050
you will get the best experience,

303
00:16:01,050 --> 00:16:02,040
but you don't have to.

304
00:16:02,040 --> 00:16:06,480
You can start somewhere and
then expand your overall usage

305
00:16:06,480 --> 00:16:09,780
of Amazon SageMaker as the needs come.

306
00:16:09,780 --> 00:16:10,710
Terri.

307
00:16:10,710 --> 00:16:11,543
Thanks, folks.

308
00:16:12,603 --> 00:16:14,880
(audience applauds)

309
00:16:14,880 --> 00:16:15,713
- Oh, thank you.

310
00:16:16,950 --> 00:16:17,823
Thanks very much.

311
00:16:19,350 --> 00:16:20,490
Hi everyone.

312
00:16:20,490 --> 00:16:21,840
Thank you, Raghu.

313
00:16:21,840 --> 00:16:23,490
Really pleased to be here.

314
00:16:23,490 --> 00:16:24,960
My name's Terri Sutherland.

315
00:16:24,960 --> 00:16:27,630
I'm the general manager of
the cloud data platforms

316
00:16:27,630 --> 00:16:29,940
at the Commonwealth Bank of Australia

317
00:16:29,940 --> 00:16:31,890
and I'm super excited to be here today

318
00:16:31,890 --> 00:16:36,890
to talk to you about our
ambitious strategy and AI roadmap

319
00:16:37,140 --> 00:16:40,620
and transformation journey.

320
00:16:40,620 --> 00:16:42,810
But first let me tell you a little bit

321
00:16:42,810 --> 00:16:44,670
about the bank itself.

322
00:16:44,670 --> 00:16:46,800
Though many of you have
probably never heard

323
00:16:46,800 --> 00:16:50,310
of the Commonwealth Bank of
Australia or CBA as we call it,

324
00:16:50,310 --> 00:16:52,860
we are Australia's largest bank.

325
00:16:52,860 --> 00:16:56,670
Australia's population
is 27 million people.

326
00:16:56,670 --> 00:17:01,380
And CBA services 17.5 million customers.

327
00:17:01,380 --> 00:17:03,750
That means one in three Australians

328
00:17:03,750 --> 00:17:06,453
and one in three businesses bank with us.

329
00:17:07,290 --> 00:17:12,270
Overall, 50% of
transactions go through CBA,

330
00:17:12,270 --> 00:17:16,560
and on the global stage we
are the 13th largest bank

331
00:17:16,560 --> 00:17:19,653
by value, by market value.

332
00:17:20,940 --> 00:17:24,000
And I'm excited to say
that we've just been named

333
00:17:24,000 --> 00:17:27,330
top four bank globally for AI maturity.

334
00:17:27,330 --> 00:17:28,163
Oops, sorry.

335
00:17:30,390 --> 00:17:32,730
Didn't realize you hadn't moved the slide.

336
00:17:32,730 --> 00:17:37,042
Given the importance of CBA
to the Australian economy,

337
00:17:37,042 --> 00:17:38,580
our data and AI strategy

338
00:17:38,580 --> 00:17:40,980
is one of the bank's most valuable assets.

339
00:17:40,980 --> 00:17:43,380
It underpins everything we do.

340
00:17:43,380 --> 00:17:46,210
From protecting our customers
against frauds and scam

341
00:17:47,610 --> 00:17:51,060
to delivering seamless
personalized experiences.

342
00:17:51,060 --> 00:17:54,210
Whether you're accessing cash at an ATM,

343
00:17:54,210 --> 00:17:58,020
paying for groceries, or
applying for a home loan.

344
00:17:58,020 --> 00:18:02,220
Data and AI powers the services

345
00:18:02,220 --> 00:18:04,173
that our customers rely on every day.

346
00:18:06,000 --> 00:18:10,290
At the heart of this strategy
are three core pillars.

347
00:18:10,290 --> 00:18:13,953
Firstly, and always most
importantly, our people.

348
00:18:15,120 --> 00:18:18,870
Like many organizations, we
realized central engineering

349
00:18:18,870 --> 00:18:21,423
and AI teams just couldn't scale.

350
00:18:22,590 --> 00:18:26,850
So our first step of our
strategy was to decentralize

351
00:18:26,850 --> 00:18:30,390
and embed our data engineers
and data scientists

352
00:18:30,390 --> 00:18:32,673
across the lines of business in the bank.

353
00:18:33,870 --> 00:18:36,750
By doing this, we brought the data and AI

354
00:18:36,750 --> 00:18:38,970
closer to the people who use it

355
00:18:38,970 --> 00:18:41,883
and closer to the people
we serve, our customers.

356
00:18:42,990 --> 00:18:45,930
Our second pillar is safeguards.

357
00:18:45,930 --> 00:18:50,610
We're a bank, we manage
sensitive customer information.

358
00:18:50,610 --> 00:18:52,860
So we've designed governance and controls

359
00:18:52,860 --> 00:18:55,680
to be inherent in every single stage

360
00:18:55,680 --> 00:19:00,680
of the data and AI life cycle,
ensuring safety by design.

361
00:19:01,110 --> 00:19:04,710
And finally, our third pillar, technology.

362
00:19:04,710 --> 00:19:08,430
This is where our partnership
with AWS comes in.

363
00:19:08,430 --> 00:19:11,640
We needed to take decades of rich data

364
00:19:11,640 --> 00:19:14,400
spread across hundreds of source systems

365
00:19:14,400 --> 00:19:15,720
and put it into the hands

366
00:19:15,720 --> 00:19:18,183
of our federated data and AI teams.

367
00:19:19,200 --> 00:19:23,730
To achieve this, we established
a data mesh ecosystem

368
00:19:23,730 --> 00:19:27,873
that empowers our federated
teams to operate independently.

369
00:19:28,740 --> 00:19:30,540
It moves data seamlessly,

370
00:19:30,540 --> 00:19:33,000
ensures access for people and machine,

371
00:19:33,000 --> 00:19:36,093
all the while enforcing strict governance.

372
00:19:37,980 --> 00:19:41,430
We call this ecosystem CommBank.data.

373
00:19:41,430 --> 00:19:46,290
Today it provides our 40 lines
of business across the bank,

374
00:19:46,290 --> 00:19:49,200
the freedom to produce and use data

375
00:19:49,200 --> 00:19:54,200
all within a trusted,
traceable, controlled framework.

376
00:19:54,330 --> 00:19:56,340
By decentralizing, we adopted

377
00:19:56,340 --> 00:20:00,060
a clear producer-consumer model.

378
00:20:00,060 --> 00:20:02,910
Each business unit now owns managers

379
00:20:02,910 --> 00:20:05,880
and manages this data as a product

380
00:20:05,880 --> 00:20:09,180
with defined roles and responsibilities.

381
00:20:09,180 --> 00:20:12,360
We also introduce
self-service data sharing

382
00:20:12,360 --> 00:20:15,390
through a unified data marketplace,

383
00:20:15,390 --> 00:20:20,190
a single pane of glass where
users can discover, request,

384
00:20:20,190 --> 00:20:24,543
and consume data across
the entire AWS ecosystem.

385
00:20:25,830 --> 00:20:29,160
But here's the reality, data has gravity.

386
00:20:29,160 --> 00:20:32,730
Where the data lives is where
every single data engineer

387
00:20:32,730 --> 00:20:35,670
and data scientist in the bank will work.

388
00:20:35,670 --> 00:20:38,040
Historically, CBA had decades

389
00:20:38,040 --> 00:20:41,550
of rich data held in on-premise platforms.

390
00:20:41,550 --> 00:20:44,190
Platforms that lacked interoperability

391
00:20:44,190 --> 00:20:46,560
or could not scale for AI.

392
00:20:46,560 --> 00:20:48,360
And so we made a bold move.

393
00:20:48,360 --> 00:20:52,080
We migrated 61,000
on-premise data pipelines

394
00:20:52,080 --> 00:20:56,550
to our AWS mesh ecosystem
that's equivalent

395
00:20:56,550 --> 00:20:58,683
of 10 petabytes of data.

396
00:20:59,820 --> 00:21:02,220
The migration took us nine months

397
00:21:02,220 --> 00:21:06,420
with 100% of data pipelines
tested at least three times.

398
00:21:06,420 --> 00:21:09,810
That's 229,000 tests.

399
00:21:09,810 --> 00:21:13,260
And in so doing, we moved
our entire data engineering

400
00:21:13,260 --> 00:21:15,663
and AI workforce to AWS cloud.

401
00:21:17,040 --> 00:21:19,260
To make all of this real, we had to run

402
00:21:19,260 --> 00:21:24,003
two major programs in parallel,
migration and marketplace.

403
00:21:25,350 --> 00:21:27,900
We migrated over 10 petabytes of data

404
00:21:27,900 --> 00:21:30,270
from on-premise to AWS cloud

405
00:21:30,270 --> 00:21:31,950
and built the data marketplace

406
00:21:31,950 --> 00:21:36,180
aligned to our federated,
decentralized operating model.

407
00:21:36,180 --> 00:21:38,310
We started with migration.

408
00:21:38,310 --> 00:21:41,250
Earlier last year, we kicked
off a series of workshops

409
00:21:41,250 --> 00:21:45,390
with AWS to test our
most complex data flows

410
00:21:45,390 --> 00:21:48,780
and AI use cases to see if the migration

411
00:21:48,780 --> 00:21:52,560
to AWS native technologies was possible.

412
00:21:52,560 --> 00:21:54,873
We call this approach steel threads.

413
00:21:56,280 --> 00:21:59,310
Like an MVP or a proof of concept,

414
00:21:59,310 --> 00:22:02,670
steel threads proves the technology fit,

415
00:22:02,670 --> 00:22:05,613
but also productionalizes the outcome.

416
00:22:06,450 --> 00:22:11,450
We built AI and generative
AI that transformed code,

417
00:22:13,020 --> 00:22:15,900
check for errors, and tested output,

418
00:22:15,900 --> 00:22:18,120
reconciling every single table

419
00:22:18,120 --> 00:22:21,900
to our on-premise platform 100%.

420
00:22:21,900 --> 00:22:24,210
Every single row, column, and number

421
00:22:24,210 --> 00:22:25,803
had to be accounted for.

422
00:22:27,120 --> 00:22:30,210
But this was more than a
tech builder migration.

423
00:22:30,210 --> 00:22:33,270
It was also a major
change management effort.

424
00:22:33,270 --> 00:22:35,520
We onboarded federated data teams

425
00:22:35,520 --> 00:22:37,770
through 200 tailored sessions

426
00:22:37,770 --> 00:22:40,200
and trained over 1,000 engineers,

427
00:22:40,200 --> 00:22:42,990
embedding change, building capability,

428
00:22:42,990 --> 00:22:45,213
and driving sustained adoption.

429
00:22:46,350 --> 00:22:48,090
And throughout, we also worked closely

430
00:22:48,090 --> 00:22:50,520
with local and international regulators

431
00:22:50,520 --> 00:22:52,743
to ensure compliance and continuity.

432
00:22:53,700 --> 00:22:56,130
Now let's talk about the
second major stream of work,

433
00:22:56,130 --> 00:22:58,800
building the data marketplace.

434
00:22:58,800 --> 00:23:00,480
This was a fundamental shift

435
00:23:00,480 --> 00:23:02,520
in how we think about data ownership

436
00:23:02,520 --> 00:23:04,830
and access across the bank.

437
00:23:04,830 --> 00:23:07,633
Each line of business
was becoming a producer,

438
00:23:07,633 --> 00:23:11,010
a consumer, and many cases, both.

439
00:23:11,010 --> 00:23:13,890
We started by building
our technical foundations,

440
00:23:13,890 --> 00:23:17,730
a federated data mesh
platform designed for scale,

441
00:23:17,730 --> 00:23:20,880
governance, and decentralized ownership.

442
00:23:20,880 --> 00:23:23,400
As we transitioned from a monolithic

443
00:23:23,400 --> 00:23:27,000
to a composable platform,
we faced the challenge

444
00:23:27,000 --> 00:23:29,370
how to seamlessly connect hundreds of

445
00:23:29,370 --> 00:23:32,220
line of business owned AWS accounts

446
00:23:32,220 --> 00:23:35,850
while keeping the user
experience frictionless.

447
00:23:35,850 --> 00:23:39,060
So we implemented a abstraction layer,

448
00:23:39,060 --> 00:23:41,640
one that could unify access to data,

449
00:23:41,640 --> 00:23:44,670
offer flexibility in
compute and UI choices,

450
00:23:44,670 --> 00:23:49,050
and uphold our rigorous
governance standards.

451
00:23:49,050 --> 00:23:50,790
Praveen will bring that experience to life

452
00:23:50,790 --> 00:23:51,843
in a demo shortly.

453
00:23:53,070 --> 00:23:54,480
Once that was all in place,

454
00:23:54,480 --> 00:23:57,870
we implemented AWS DataZone
to enable discovery,

455
00:23:57,870 --> 00:24:01,500
access, and sharing
across the organization,

456
00:24:01,500 --> 00:24:03,900
bringing on early adopters.

457
00:24:03,900 --> 00:24:07,200
And with the launch of
SageMaker Unified Studio,

458
00:24:07,200 --> 00:24:08,520
earlier this year, we added in

459
00:24:08,520 --> 00:24:11,910
a single pane of glass
experience so everyone

460
00:24:11,910 --> 00:24:15,420
from analysts to executives
could see and use

461
00:24:15,420 --> 00:24:16,623
the data they needed.

462
00:24:17,700 --> 00:24:20,610
In completing this, we
aligned our engineers,

463
00:24:20,610 --> 00:24:24,840
onboarded producers and
consumers, to a shared framework,

464
00:24:24,840 --> 00:24:28,500
one that creates self-service
while maintaining governance

465
00:24:28,500 --> 00:24:31,230
and interoperability across the mesh.

466
00:24:31,230 --> 00:24:34,680
And now with the right
data in the right place,

467
00:24:34,680 --> 00:24:37,770
we're ready to scale with governed AI.

468
00:24:37,770 --> 00:24:40,260
Let's look at the benefits
of what this transformation

469
00:24:40,260 --> 00:24:42,153
is allowing us to do today.

470
00:24:44,070 --> 00:24:47,100
When we spoke to our
engineers and data scientists,

471
00:24:47,100 --> 00:24:50,010
they told us CommBank.data marketplace

472
00:24:50,010 --> 00:24:52,620
has transformed the way they work.

473
00:24:52,620 --> 00:24:54,750
Now they connect directly

474
00:24:54,750 --> 00:24:57,810
from their local VS code environment,

475
00:24:57,810 --> 00:25:02,370
no longer restricted to
SageMaker or remote platforms.

476
00:25:02,370 --> 00:25:04,260
This access streamlines workflows

477
00:25:04,260 --> 00:25:06,813
and speeds up experimentation.

478
00:25:08,070 --> 00:25:09,420
On the standout features

479
00:25:09,420 --> 00:25:12,330
is a single pane of glass experience.

480
00:25:12,330 --> 00:25:15,510
Instead of switching between
tools and interfaces,

481
00:25:15,510 --> 00:25:17,820
teams have a unified dashboard

482
00:25:17,820 --> 00:25:21,003
for data discovery,
analytics, and monitoring.

483
00:25:21,990 --> 00:25:24,723
The Spark UI is easily accessible,

484
00:25:25,950 --> 00:25:29,310
allowing real-time tracking
of query performance

485
00:25:29,310 --> 00:25:33,060
and quick identification of bottlenecks.

486
00:25:33,060 --> 00:25:35,400
Dedicated compute resources also means

487
00:25:35,400 --> 00:25:38,370
workflows run reliably and troubleshooting

488
00:25:38,370 --> 00:25:42,840
is far more simple if problems arise.

489
00:25:42,840 --> 00:25:47,190
The integration with
SageMaker was a game changer.

490
00:25:47,190 --> 00:25:50,790
Data labs outputs are now available

491
00:25:50,790 --> 00:25:53,070
directly within the platform,

492
00:25:53,070 --> 00:25:55,710
removing manual steps and making it easier

493
00:25:55,710 --> 00:26:00,390
to run advanced analytics and
machine learning workloads.

494
00:26:00,390 --> 00:26:03,600
These improvements have
created a unified environment

495
00:26:03,600 --> 00:26:05,940
that empowers our teams to experiment,

496
00:26:05,940 --> 00:26:09,033
deploy, and scale AI with ease.

497
00:26:09,930 --> 00:26:12,570
It helps us build a
culture of AI innovation

498
00:26:12,570 --> 00:26:15,120
where our people are closer to the data

499
00:26:15,120 --> 00:26:18,063
and scaling AI with confidence.

500
00:26:20,580 --> 00:26:25,580
So we've achieved a lot
in the past 18 months.

501
00:26:25,710 --> 00:26:30,300
We built a strategic mesh
ecosystem in AWS cloud.

502
00:26:30,300 --> 00:26:35,300
We migrated decades of investment
to that cloud ecosystem

503
00:26:35,310 --> 00:26:37,473
and onboarded data and engineers,

504
00:26:38,400 --> 00:26:41,130
data engineers and data scientists to AWS

505
00:26:41,130 --> 00:26:42,990
and innovative tooling.

506
00:26:42,990 --> 00:26:44,823
But what did we learn along the way?

507
00:26:45,930 --> 00:26:49,140
We could not scale with data and AI

508
00:26:49,140 --> 00:26:52,953
with centralized operating
model and monolithic platforms.

509
00:26:53,790 --> 00:26:55,440
We needed to train our people

510
00:26:55,440 --> 00:26:58,530
to understand the new
federated operating model

511
00:26:58,530 --> 00:27:00,300
and allow time for engineers

512
00:27:00,300 --> 00:27:04,323
and data scientists to become
certified in the new tooling.

513
00:27:06,210 --> 00:27:10,950
MVPs, steel threads, and iteration
are our path to discovery

514
00:27:10,950 --> 00:27:12,600
and in fact the fastest way

515
00:27:12,600 --> 00:27:14,913
to true value outcomes for our customers.

516
00:27:16,230 --> 00:27:18,660
And importantly, we needed to recognize

517
00:27:18,660 --> 00:27:21,540
that transformation is
always a learning curve,

518
00:27:21,540 --> 00:27:24,333
which means being
comfortable with the unknown.

519
00:27:25,500 --> 00:27:27,750
Looking ahead, we'll continue

520
00:27:27,750 --> 00:27:29,460
onboarding our lines of business

521
00:27:29,460 --> 00:27:33,093
and maturing with them,
expanding generative AI,

522
00:27:34,170 --> 00:27:39,090
Unified Studio including
Amazon Q and agentic AI

523
00:27:39,090 --> 00:27:43,260
and deepening lineage
explainability and observability.

524
00:27:43,260 --> 00:27:47,910
Next, Praveen will show you
this unified experience works

525
00:27:47,910 --> 00:27:50,370
and end-to-end SageMaker Unified Studio

526
00:27:50,370 --> 00:27:52,500
for the CommBank.data marketplace

527
00:27:52,500 --> 00:27:55,020
so you can see the
architecture come to life.

528
00:27:56,835 --> 00:27:59,835
(audience applauds)

529
00:28:03,480 --> 00:28:05,220
- Thank you, Terri.

530
00:28:05,220 --> 00:28:07,380
It's been a pleasure to collaborate

531
00:28:07,380 --> 00:28:10,020
and work alongside CommBank data team

532
00:28:10,020 --> 00:28:12,520
on this very exciting data
transformation program.

533
00:28:13,680 --> 00:28:17,580
I strongly believe that the
team has built a highly scalable

534
00:28:17,580 --> 00:28:20,910
and a future fit modern data platform

535
00:28:20,910 --> 00:28:24,480
that is going to help
accelerate time to insights,

536
00:28:24,480 --> 00:28:27,570
is going to increase
organizational agility,

537
00:28:27,570 --> 00:28:30,870
and at the same time meet
all of the strict governance

538
00:28:30,870 --> 00:28:32,433
and regulatory obligations.

539
00:28:34,290 --> 00:28:35,850
My name is Praveen Kumar.

540
00:28:35,850 --> 00:28:39,150
I'm a principal AI
solutions architect at AWS.

541
00:28:39,150 --> 00:28:41,220
And the next 20 minutes or so,

542
00:28:41,220 --> 00:28:44,340
I'm going to take you through
CommBank data platform,

543
00:28:44,340 --> 00:28:47,550
how we'll design, platform architecture,

544
00:28:47,550 --> 00:28:49,890
and I'll also have a
short demo to show you

545
00:28:49,890 --> 00:28:52,320
an end-to-end user experience

546
00:28:52,320 --> 00:28:54,360
in a multi-account environment.

547
00:28:54,360 --> 00:28:57,150
And that's how CommBank
data platform is set up.

548
00:28:57,150 --> 00:28:59,970
But before that, I would
like to acknowledge

549
00:28:59,970 --> 00:29:02,820
CommBank data team and
particularly Olatunde Baruwa

550
00:29:02,820 --> 00:29:05,790
who's a chief engineer at CommBank.

551
00:29:05,790 --> 00:29:07,830
They have been instrumental
in the implementation

552
00:29:07,830 --> 00:29:09,830
and the execution of this data platform.

553
00:29:13,260 --> 00:29:16,140
As Terri shared, the data
transformation program

554
00:29:16,140 --> 00:29:18,273
included two major streams of work.

555
00:29:19,140 --> 00:29:22,350
The first one was about migrating

556
00:29:22,350 --> 00:29:25,950
our on-premise Hadoop-based
data lake platform

557
00:29:25,950 --> 00:29:28,443
with over 10 petabytes of data to AWS.

558
00:29:30,150 --> 00:29:32,040
And the second program involved

559
00:29:32,040 --> 00:29:34,470
setting up an internal data marketplace

560
00:29:34,470 --> 00:29:38,710
that aligned with targeted
state federated operating model

561
00:29:38,710 --> 00:29:42,840
where each line of business,
there are 40 of them,

562
00:29:42,840 --> 00:29:46,440
will act as data producer
or data consumer or both.

563
00:29:46,440 --> 00:29:49,860
And the central team will
provide lightweight governance

564
00:29:49,860 --> 00:29:52,110
while facilitating the
marketplace ecosystem.

565
00:29:54,270 --> 00:29:56,520
So let's look at the
migration program first.

566
00:29:57,780 --> 00:30:00,270
What you see here is the high level design

567
00:30:00,270 --> 00:30:04,230
and the key components of
the on-premise platform

568
00:30:04,230 --> 00:30:05,823
before and after migration.

569
00:30:06,660 --> 00:30:09,813
On the left is the setup
before the migration.

570
00:30:11,520 --> 00:30:14,040
The on-premise platform was made up

571
00:30:14,040 --> 00:30:16,113
of two large scale Hadoop clusters.

572
00:30:18,030 --> 00:30:23,030
One of the cluster was used
to run 61,000 pipelines

573
00:30:24,180 --> 00:30:26,190
and then data and metadata was copied

574
00:30:26,190 --> 00:30:28,020
to the other Hadoop cluster

575
00:30:28,020 --> 00:30:29,520
to serve interactive workload.

576
00:30:31,230 --> 00:30:34,503
When this platform was moved to cloud,

577
00:30:35,430 --> 00:30:38,313
it was mainly done through
a lift and shift approach.

578
00:30:39,360 --> 00:30:41,820
So data was migrated to Amazon S3

579
00:30:41,820 --> 00:30:44,733
and compute is now powered by Amazon EMR.

580
00:30:46,530 --> 00:30:49,200
However, one of the key
benefit when migrating

581
00:30:49,200 --> 00:30:51,780
Hadoop-based data lake platform to cloud

582
00:30:51,780 --> 00:30:54,430
is that there is separation
of a storage and compute.

583
00:30:55,920 --> 00:30:58,890
Because all of the data is in Amazon S3,

584
00:30:58,890 --> 00:31:03,750
the team is able to
enable access of this data

585
00:31:03,750 --> 00:31:07,710
to variety of cloud native
analytics and ML engines

586
00:31:07,710 --> 00:31:09,903
to support diverse set of use cases.

587
00:31:12,060 --> 00:31:13,203
Next is federation.

588
00:31:15,180 --> 00:31:19,380
Previously, the team built,
managed, and supported

589
00:31:19,380 --> 00:31:23,073
on-premise data lake
platform that was monolithic.

590
00:31:24,240 --> 00:31:28,500
However, in the federated
setup, each line of business,

591
00:31:28,500 --> 00:31:32,463
there are 40 of them, have their
own dedicated AWS accounts.

592
00:31:33,330 --> 00:31:36,030
They use Amazon S3 to store their data

593
00:31:36,030 --> 00:31:38,310
in Apache Iceberg format.

594
00:31:38,310 --> 00:31:42,450
They built pipelines using
either EMR Serverless or Redshift

595
00:31:42,450 --> 00:31:44,580
or other third party engines.

596
00:31:44,580 --> 00:31:47,820
They use Glue to store technical metadata,

597
00:31:47,820 --> 00:31:50,433
lake formation for
policy stored, and so on.

598
00:31:51,750 --> 00:31:55,590
But more importantly,
LOBs are now responsible

599
00:31:55,590 --> 00:31:59,010
for building and managing
their own data assets,

600
00:31:59,010 --> 00:32:01,623
their own data pipelines,
their orchestration.

601
00:32:02,490 --> 00:32:05,490
They're responsible for the
data quality of these assets,

602
00:32:05,490 --> 00:32:08,493
SLAs, observability, and more.

603
00:32:11,130 --> 00:32:15,810
So as the platform evolved
from a monolithic unit

604
00:32:15,810 --> 00:32:19,293
to a distributed and composable construct,

605
00:32:20,430 --> 00:32:23,430
we needed to build an abstraction layer

606
00:32:23,430 --> 00:32:26,700
that would bring all
of these LOBs together.

607
00:32:26,700 --> 00:32:28,770
So there are two key requirements.

608
00:32:28,770 --> 00:32:30,420
The first one was to build

609
00:32:30,420 --> 00:32:33,030
an enterprise-wide business data catalog

610
00:32:33,030 --> 00:32:36,270
where data producers
can publish data assets

611
00:32:36,270 --> 00:32:40,350
and data products, and
data consumers can search,

612
00:32:40,350 --> 00:32:43,443
discover these assets, and
request access to these assets.

613
00:32:44,850 --> 00:32:48,150
The second key requirement was to provide

614
00:32:48,150 --> 00:32:52,200
an integrated builder
experience for all data users.

615
00:32:52,200 --> 00:32:56,610
Users such as data engineers,
data analysts, data scientists

616
00:32:56,610 --> 00:32:59,340
to use user interface of their choice.

617
00:32:59,340 --> 00:33:03,090
This could be notebooks,
query editors, visual ETL.

618
00:33:03,090 --> 00:33:05,640
And then we also be able to leverage

619
00:33:05,640 --> 00:33:07,260
a variety of compute engines

620
00:33:07,260 --> 00:33:09,060
that's optimized for their use case.

621
00:33:10,230 --> 00:33:12,990
And so to support these
two key requirements,

622
00:33:12,990 --> 00:33:15,663
the team onboarded Amazon
SageMaker Unified Studio.

623
00:33:17,250 --> 00:33:20,280
SageMaker Unified Studio
provides an integrated

624
00:33:20,280 --> 00:33:23,430
developer experience for all data users

625
00:33:23,430 --> 00:33:25,833
to build data and AI-driven applications.

626
00:33:27,210 --> 00:33:30,453
It has got governance built
in with SageMaker catalog.

627
00:33:31,770 --> 00:33:33,510
The teams were also able to leverage

628
00:33:33,510 --> 00:33:36,870
many of the construct within
SageMaker Unified Studio

629
00:33:36,870 --> 00:33:40,470
for broader governance.

630
00:33:40,470 --> 00:33:42,960
For example, they use domain units

631
00:33:42,960 --> 00:33:45,330
to replicate organizational hierarchy

632
00:33:45,330 --> 00:33:47,670
and implement governance policies.

633
00:33:47,670 --> 00:33:51,690
They used projects to isolate workloads

634
00:33:51,690 --> 00:33:53,913
and map to hundreds of AWS accounts,

635
00:33:55,050 --> 00:33:56,310
and they use the pops up model

636
00:33:56,310 --> 00:33:57,450
within SageMaker Unified Studio

637
00:33:57,450 --> 00:33:59,493
to integrate with third party engines.

638
00:34:01,710 --> 00:34:04,530
Now, earlier this year, we migrated

639
00:34:04,530 --> 00:34:07,383
the on-premise Hadoop based
data lake platform to AWS.

640
00:34:09,480 --> 00:34:11,553
This has almost all of CBA data.

641
00:34:12,420 --> 00:34:14,870
We are talking about tens
of thousands of tables.

642
00:34:15,780 --> 00:34:17,913
However, this is monolithic in nature.

643
00:34:19,560 --> 00:34:24,560
In parallel, we started onboarding
various lines of business

644
00:34:25,050 --> 00:34:27,840
to the internal data marketplace,

645
00:34:27,840 --> 00:34:31,320
and this platform is federated
and distributed in nature.

646
00:34:31,320 --> 00:34:34,120
So the next challenge was how
do we bring them together.

647
00:34:35,790 --> 00:34:37,353
So we did that in two steps.

648
00:34:38,670 --> 00:34:43,260
The first was to identify
the line of business owner

649
00:34:43,260 --> 00:34:48,260
for each table that has
been migrated to AWS now

650
00:34:48,450 --> 00:34:50,073
within the data marketplace.

651
00:34:51,630 --> 00:34:55,020
And so the idea was to provide
the necessary governance.

652
00:34:55,020 --> 00:34:57,630
For example, a business
owner is now responsible

653
00:34:57,630 --> 00:34:59,760
for approving the usage and access

654
00:34:59,760 --> 00:35:01,563
of that table or data asset.

655
00:35:03,000 --> 00:35:06,483
The second step was to enable
the technical integration.

656
00:35:07,650 --> 00:35:09,240
And this is going back
to my earlier point.

657
00:35:09,240 --> 00:35:12,063
Because all of the migrated
data is in Amazon S3,

658
00:35:13,080 --> 00:35:16,440
we built a lightweight ETL
job that replicates metadata

659
00:35:16,440 --> 00:35:18,753
from RDS Hive Metastore
to Glue Data Catalog.

660
00:35:19,830 --> 00:35:21,480
So you have all of the tables now

661
00:35:21,480 --> 00:35:23,730
appearing in Glue Data Catalog.

662
00:35:23,730 --> 00:35:26,940
And then we mapped each of those tables

663
00:35:26,940 --> 00:35:31,833
to the respective SageMaker
project under their LOB.

664
00:35:33,090 --> 00:35:34,800
Now since all of the data is available

665
00:35:34,800 --> 00:35:37,593
in the centralized SageMaker data catalog,

666
00:35:38,580 --> 00:35:42,150
data consumers can come
to a central place,

667
00:35:42,150 --> 00:35:43,740
which is the studio.

668
00:35:43,740 --> 00:35:47,070
They can browse, discover, request access.

669
00:35:47,070 --> 00:35:49,440
And once the access is approved,

670
00:35:49,440 --> 00:35:51,870
they can build their applications

671
00:35:51,870 --> 00:35:53,820
through a query in place architecture

672
00:35:53,820 --> 00:35:56,130
across hundreds of AWS accounts.

673
00:35:56,130 --> 00:35:58,020
So there is no data movement.

674
00:35:58,020 --> 00:36:01,740
Data can be living in many
of these AWS accounts,

675
00:36:01,740 --> 00:36:05,010
but we're able to use compute
from any of these accounts

676
00:36:05,010 --> 00:36:06,310
with the right governance.

677
00:36:08,790 --> 00:36:11,673
All right, so now it's time
to see all of this in action.

678
00:36:12,690 --> 00:36:14,580
The demo is partly inspired by

679
00:36:14,580 --> 00:36:16,680
how common data platform is set up.

680
00:36:16,680 --> 00:36:19,770
So you'll be using a
multi-account environment.

681
00:36:19,770 --> 00:36:23,880
The demo use case is building
a fraud detection model

682
00:36:23,880 --> 00:36:25,953
for a financial services organization.

683
00:36:27,600 --> 00:36:29,973
We'll have two user personas in this demo.

684
00:36:30,810 --> 00:36:32,343
We have got Samantha or Sam.

685
00:36:33,180 --> 00:36:34,710
She's a data engineer.

686
00:36:34,710 --> 00:36:36,900
She's part of retail banking team,

687
00:36:36,900 --> 00:36:38,550
so that's a LOB line of business.

688
00:36:39,540 --> 00:36:41,400
She's going to create a new table

689
00:36:41,400 --> 00:36:43,110
called customer_profile table

690
00:36:43,110 --> 00:36:46,140
by combining data from two existing table

691
00:36:46,140 --> 00:36:47,910
that her team owns.

692
00:36:47,910 --> 00:36:50,370
She's then able to,

693
00:36:50,370 --> 00:36:52,710
she's then going to enrich the metadata.

694
00:36:52,710 --> 00:36:54,750
So she's going to add a README,

695
00:36:54,750 --> 00:36:57,750
she's going to add data quality, lineage.

696
00:36:57,750 --> 00:36:59,130
And once the metadata is enriched,

697
00:36:59,130 --> 00:37:01,230
she's going to publish it to the catalog

698
00:37:01,230 --> 00:37:04,020
for others to discover and request access.

699
00:37:04,020 --> 00:37:07,470
She's also responsible
for approving access

700
00:37:07,470 --> 00:37:10,413
to the subscription
requests on LOB's behalf.

701
00:37:11,580 --> 00:37:14,580
The second user persona is Javier.

702
00:37:14,580 --> 00:37:17,343
He's a data scientist and
part of financial crime team.

703
00:37:18,420 --> 00:37:20,823
He's going to build the
fraud detection model.

704
00:37:21,690 --> 00:37:26,040
And to do that, he needs
access to the various datasets.

705
00:37:26,040 --> 00:37:27,240
So he'll go to the catalog

706
00:37:27,240 --> 00:37:30,210
where he can centrally
discover all of the assets

707
00:37:30,210 --> 00:37:33,543
and request access to this new
table that Sam has created.

708
00:37:34,650 --> 00:37:36,390
Once the access is approved,

709
00:37:36,390 --> 00:37:39,750
Javier is going to do
some interactive analysis

710
00:37:39,750 --> 00:37:44,163
and then build a ML model
using SageMaker AI API.

711
00:37:46,110 --> 00:37:47,313
Here is the setup.

712
00:37:48,210 --> 00:37:50,370
So this is the multi-account
environment setup.

713
00:37:50,370 --> 00:37:54,330
At the top in this demo, I
have used three AWS accounts.

714
00:37:54,330 --> 00:37:57,000
At the top is the shared services account.

715
00:37:57,000 --> 00:38:00,180
So this account is owned
by the central team

716
00:38:00,180 --> 00:38:03,180
and this is where
SageMaker domain is set up.

717
00:38:03,180 --> 00:38:08,180
And then SageMaker domain is
hooked to two AWS accounts.

718
00:38:08,820 --> 00:38:10,740
One is the data producer
account on the left

719
00:38:10,740 --> 00:38:13,340
and the other is data
consumer account on the right.

720
00:38:14,460 --> 00:38:16,920
So anything that Sam is using

721
00:38:16,920 --> 00:38:18,120
in terms of storage and compute,

722
00:38:18,120 --> 00:38:20,220
that's going to sit in
the producer account.

723
00:38:20,220 --> 00:38:22,680
When she's building a pipeline using Spark

724
00:38:22,680 --> 00:38:25,170
or when storing data in
S3, that's all going to be

725
00:38:25,170 --> 00:38:29,400
in that leftmost account,
which is the producer account.

726
00:38:29,400 --> 00:38:31,470
Anything that Javier is going to do,

727
00:38:31,470 --> 00:38:34,470
so essentially, you know,
you're running Athena query

728
00:38:34,470 --> 00:38:36,240
to do exploratory analysis

729
00:38:36,240 --> 00:38:39,030
or using SageMaker AI
API to train the model,

730
00:38:39,030 --> 00:38:42,393
that sits in the consumer
account on the right.

731
00:38:43,350 --> 00:38:47,010
This pattern is scalable to
hundreds of AWS accounts,

732
00:38:47,010 --> 00:38:48,840
and this provides you the best practices

733
00:38:48,840 --> 00:38:51,543
in terms of setting up
multi-account strategy.

734
00:38:52,683 --> 00:38:55,350
You get workload isolation,

735
00:38:55,350 --> 00:38:58,320
you can allocate costs to respective LOBs,

736
00:38:58,320 --> 00:39:00,660
and at the same time, you can implement

737
00:39:00,660 --> 00:39:02,310
the distinct governance boundary.

738
00:39:04,110 --> 00:39:04,943
All right.

739
00:39:19,170 --> 00:39:20,003
I'll hit play.

740
00:39:20,940 --> 00:39:25,940
So I'm in SageMaker Unified
Studio and I'm logged in as Sam.

741
00:39:26,160 --> 00:39:29,400
So this is the homepage of
SageMaker Unified Studio.

742
00:39:29,400 --> 00:39:32,550
And you can see at the top section

743
00:39:32,550 --> 00:39:34,950
you have things like discover catalog,

744
00:39:34,950 --> 00:39:36,480
you can play with the foundation model

745
00:39:36,480 --> 00:39:38,280
through generative AI playground.

746
00:39:38,280 --> 00:39:42,930
So let's, you know, as
one of the first action,

747
00:39:42,930 --> 00:39:44,223
let's browse catalog.

748
00:39:45,630 --> 00:39:47,490
So this is the catalog
view where you can discover

749
00:39:47,490 --> 00:39:49,080
all of the organizational asset.

750
00:39:49,080 --> 00:39:53,190
So at the top section,
you can type in a keyword,

751
00:39:53,190 --> 00:39:54,660
it does semantic search,

752
00:39:54,660 --> 00:39:56,850
and then it will return
the results, right?

753
00:39:56,850 --> 00:39:58,860
So for example, I have a search for

754
00:39:58,860 --> 00:40:00,750
a table called transaction, it showed up.

755
00:40:00,750 --> 00:40:02,580
At the bottom half of the page,

756
00:40:02,580 --> 00:40:05,580
what you see here is the
domain unit hierarchy.

757
00:40:05,580 --> 00:40:07,560
So this maps to your LOB hierarchy.

758
00:40:07,560 --> 00:40:09,414
In this case there are only two LOBs,

759
00:40:09,414 --> 00:40:11,700
RetailBanking and FinCrime.

760
00:40:11,700 --> 00:40:14,160
And then at the top is the project.

761
00:40:14,160 --> 00:40:18,210
So project is a concept
where you map a project

762
00:40:18,210 --> 00:40:21,570
to a use case which allows you to group

763
00:40:21,570 --> 00:40:24,210
compute and data that
a team has access to.

764
00:40:24,210 --> 00:40:26,035
So in this case I've got two projects,

765
00:40:26,035 --> 00:40:29,480
RetailBanking data team
project that is aligned

766
00:40:29,480 --> 00:40:31,118
to the retail banking team.

767
00:40:31,118 --> 00:40:32,640
And this is where Sam is going to work.

768
00:40:32,640 --> 00:40:34,740
And I've also got another project FinCrime

769
00:40:34,740 --> 00:40:36,780
which Javier has access to,

770
00:40:36,780 --> 00:40:39,480
and this is where, you
know, he's going to use

771
00:40:39,480 --> 00:40:41,030
that project to train ML model.

772
00:40:42,030 --> 00:40:45,540
So Sam is going to click on that project.

773
00:40:45,540 --> 00:40:48,153
This is the project overview homepage.

774
00:40:49,170 --> 00:40:52,413
We'll go through the menu
options on the left one by one,

775
00:40:53,370 --> 00:40:55,170
but at the top on this page,

776
00:40:55,170 --> 00:40:57,330
you can see the various project files.

777
00:40:57,330 --> 00:41:00,720
This could be your notebook
files, query files and so on.

778
00:41:00,720 --> 00:41:04,560
But important point to note
is this project role ARN

779
00:41:04,560 --> 00:41:07,890
on the right side, so,
which I have highlighted.

780
00:41:07,890 --> 00:41:12,120
And I can see that there
is an account number here.

781
00:41:12,120 --> 00:41:14,370
This is the data producer account, right?

782
00:41:14,370 --> 00:41:18,480
And this project role helps map
what compute and data access

783
00:41:18,480 --> 00:41:20,680
that the team has as a
part of this project.

784
00:41:21,780 --> 00:41:24,060
So in case of, you know, JVR,

785
00:41:24,060 --> 00:41:27,300
this will be a different
role and a different account.

786
00:41:27,300 --> 00:41:30,270
All right, so that's the
project overview homepage.

787
00:41:30,270 --> 00:41:35,270
Next we will go and click on
the data on the left hand side.

788
00:41:36,210 --> 00:41:37,800
And so this is all of the dataset

789
00:41:37,800 --> 00:41:40,170
that the team has access to.

790
00:41:40,170 --> 00:41:42,270
So, you know, any Glue
tables will show up.

791
00:41:42,270 --> 00:41:43,830
In this case there are three Glue tables

792
00:41:43,830 --> 00:41:45,663
that the team has already created.

793
00:41:47,010 --> 00:41:48,540
If you have access to Redshift data,

794
00:41:48,540 --> 00:41:51,570
that will show up here as
well as any S3 buckets.

795
00:41:51,570 --> 00:41:52,980
Then there is the Compute tab,

796
00:41:52,980 --> 00:41:55,370
which is where all of
the compute will show up.

797
00:41:55,370 --> 00:41:59,040
So in this case the team has
access to Redshift Compute

798
00:41:59,040 --> 00:42:00,780
as well as Glue Spark.

799
00:42:00,780 --> 00:42:03,240
And you have access to
other compute like Hyperport

800
00:42:03,240 --> 00:42:06,720
or MLflow Tracking Server,
all of that will appear here.

801
00:42:06,720 --> 00:42:10,170
And this Members is, you
know, all of your team members

802
00:42:10,170 --> 00:42:11,280
who are part of this project.

803
00:42:11,280 --> 00:42:12,990
So essentially part of
retail banking team,

804
00:42:12,990 --> 00:42:14,790
that will show up here.

805
00:42:14,790 --> 00:42:17,670
On the bottom half of the left hand side

806
00:42:17,670 --> 00:42:19,020
is the project catalog.

807
00:42:19,020 --> 00:42:22,230
So this is where you build your catalog.

808
00:42:22,230 --> 00:42:24,990
So you bring your table
from Glue Data Catalog

809
00:42:24,990 --> 00:42:27,300
to SageMaker catalog inventory space

810
00:42:27,300 --> 00:42:29,040
and this is where you enrich the metadata.

811
00:42:29,040 --> 00:42:32,550
So you can add glossary,
metadata forms, data quality,

812
00:42:32,550 --> 00:42:34,410
and I'll show you this in a while.

813
00:42:34,410 --> 00:42:38,250
But for example, this team
has already got access

814
00:42:38,250 --> 00:42:40,800
to three tables that they have enriched.

815
00:42:40,800 --> 00:42:42,300
And this is showing up as customer,

816
00:42:42,300 --> 00:42:44,050
customer account, and transactions.

817
00:42:45,780 --> 00:42:48,330
All right, so the next step from here is,

818
00:42:48,330 --> 00:42:52,050
let's say Sam wants to
build a new table, right?

819
00:42:52,050 --> 00:42:53,430
They already have access to two tables,

820
00:42:53,430 --> 00:42:55,290
customer and customer activity.

821
00:42:55,290 --> 00:42:56,940
Now she's wanting to build a new table

822
00:42:56,940 --> 00:42:58,680
called customer_profile.

823
00:42:58,680 --> 00:43:01,050
So she comes to the Jupyter Notebook.

824
00:43:01,050 --> 00:43:05,160
She sets some Glue Spark
config, like number of workers.

825
00:43:05,160 --> 00:43:07,920
Run some selected statement, you know,

826
00:43:07,920 --> 00:43:09,630
customer account and customers.

827
00:43:09,630 --> 00:43:13,440
And at the bottom, there's a
cell called create table as

828
00:43:13,440 --> 00:43:15,090
where essentially she combines data

829
00:43:15,090 --> 00:43:16,890
from customer and customer account table

830
00:43:16,890 --> 00:43:19,023
and creates a new customer_profile table.

831
00:43:20,280 --> 00:43:23,490
So once that cell runs, you'll
see on the left-hand side

832
00:43:23,490 --> 00:43:25,890
that we'll have fourth table appear

833
00:43:25,890 --> 00:43:27,303
called customer_profile.

834
00:43:29,160 --> 00:43:30,990
So that seller has executed

835
00:43:30,990 --> 00:43:32,740
and now I can see the fourth table.

836
00:43:33,630 --> 00:43:36,060
So I've created a new table
by combining information

837
00:43:36,060 --> 00:43:37,680
from other tables that my team owns.

838
00:43:37,680 --> 00:43:39,300
So the next step is I want to enrich

839
00:43:39,300 --> 00:43:41,280
the metadata of this table.

840
00:43:41,280 --> 00:43:43,230
So I come to the data source.

841
00:43:43,230 --> 00:43:45,990
So I'm already creating an data source

842
00:43:45,990 --> 00:43:47,370
that points to Glue Data Catalog

843
00:43:47,370 --> 00:43:50,913
and I'll bring the metadata
to the inventory space.

844
00:43:52,140 --> 00:43:54,540
So I have now run that data source,

845
00:43:54,540 --> 00:43:57,330
I can see the new table is appearing here.

846
00:43:57,330 --> 00:44:00,330
Now this is where you can
add things like README,

847
00:44:00,330 --> 00:44:04,080
you can look at, you
know, the schema details.

848
00:44:04,080 --> 00:44:06,660
So all of the columns will appear here.

849
00:44:06,660 --> 00:44:10,410
Data quality is blank, we'll
populate it in a while.

850
00:44:10,410 --> 00:44:12,750
And then it also shows you the lineage.

851
00:44:12,750 --> 00:44:16,770
And this lineage is, you know,

852
00:44:16,770 --> 00:44:19,560
as we're building the pipeline,

853
00:44:19,560 --> 00:44:22,590
Glue Spark is already integrated
with SageMaker catalog.

854
00:44:22,590 --> 00:44:24,900
So it's pulling all of
the lineage information

855
00:44:24,900 --> 00:44:28,800
when we are ingesting the
metadata of that table.

856
00:44:28,800 --> 00:44:30,090
So it shows you column level lineage

857
00:44:30,090 --> 00:44:32,610
and this is open lineage compatible.

858
00:44:32,610 --> 00:44:36,420
So from here, let's say
I want to add a README,

859
00:44:36,420 --> 00:44:38,250
which is essentially looking
at the schema details

860
00:44:38,250 --> 00:44:41,340
of this table and looking at
few records of this table,

861
00:44:41,340 --> 00:44:43,140
I want to generate a summary.

862
00:44:43,140 --> 00:44:45,600
Now you could do this
in a couple of phase,

863
00:44:45,600 --> 00:44:50,490
you could have your steward
manually doing this task

864
00:44:50,490 --> 00:44:52,563
or you can take advantage of agentic AI.

865
00:44:53,580 --> 00:44:57,240
So when you go to the Jupyter Lab,

866
00:44:57,240 --> 00:44:58,830
we have this agentic AI capability,

867
00:44:58,830 --> 00:45:01,500
which is Q CLI integrated.

868
00:45:01,500 --> 00:45:02,520
So I'm giving it a prompt,

869
00:45:02,520 --> 00:45:04,860
I'm saying that I have a
table named customer_profile

870
00:45:04,860 --> 00:45:06,540
in my Glue Data Catalog.

871
00:45:06,540 --> 00:45:08,280
Can you generate a concise summary

872
00:45:08,280 --> 00:45:11,190
of this dataset using schema details

873
00:45:11,190 --> 00:45:14,340
and also looking at 10
records within the table?

874
00:45:14,340 --> 00:45:16,950
And I then provide a helper
function which has details like

875
00:45:16,950 --> 00:45:20,250
using the permission
scoped to the project,

876
00:45:20,250 --> 00:45:23,730
which is the project role, as
well as the Athena work group

877
00:45:23,730 --> 00:45:24,993
assigned to that project.

878
00:45:26,190 --> 00:45:28,770
So I give that instruction.

879
00:45:28,770 --> 00:45:33,150
Now this is where the agent is
planning out the set of steps

880
00:45:33,150 --> 00:45:35,250
and then it's going to execute.

881
00:45:35,250 --> 00:45:39,480
It fires up, you know,
Glue APIs to understand

882
00:45:39,480 --> 00:45:42,420
if there's a table
which is similar looking

883
00:45:42,420 --> 00:45:45,780
and it's able to detect that table.

884
00:45:45,780 --> 00:45:49,170
And then it's going to
use Athena as an engine

885
00:45:49,170 --> 00:45:51,900
to run a select star statement.

886
00:45:51,900 --> 00:45:54,660
So this takes about a few seconds,

887
00:45:54,660 --> 00:45:59,430
but once it completes, it
will save the description,

888
00:45:59,430 --> 00:46:03,363
concise description into a
local file within the space.

889
00:46:05,040 --> 00:46:06,540
If you want to look at the details,

890
00:46:06,540 --> 00:46:07,800
what query it has generated,

891
00:46:07,800 --> 00:46:11,070
you can, you know, just
expand one of the dropdown

892
00:46:11,070 --> 00:46:14,613
and you can look at the query
that that agent has generated.

893
00:46:16,470 --> 00:46:19,300
So just in few seconds, you'll see that

894
00:46:20,280 --> 00:46:22,410
it has now created a directory.

895
00:46:22,410 --> 00:46:24,810
Now it's creating a file
and it's going to save.

896
00:46:26,100 --> 00:46:29,160
All right, so it has
already created the file

897
00:46:29,160 --> 00:46:31,440
and saved the concise
description based on the schema

898
00:46:31,440 --> 00:46:32,973
and the tender codes.

899
00:46:34,020 --> 00:46:37,770
Next I want the agent
to use this information

900
00:46:37,770 --> 00:46:42,133
that I just generated and I
want it to update the data asset

901
00:46:42,133 --> 00:46:44,820
within the SageMaker catalog
with this information.

902
00:46:44,820 --> 00:46:46,260
So update the README.

903
00:46:46,260 --> 00:46:50,370
So I'm just saying that read
the dataset description file

904
00:46:50,370 --> 00:46:53,910
generated above as part of
analysis of the specified table.

905
00:46:53,910 --> 00:46:56,340
Format the content and
update the README section

906
00:46:56,340 --> 00:46:58,590
of the data asset.

907
00:46:58,590 --> 00:47:02,520
And I provide helper
function with an example API.

908
00:47:02,520 --> 00:47:04,470
So then it's able to, you know,

909
00:47:04,470 --> 00:47:07,440
compile the necessities and structure,

910
00:47:07,440 --> 00:47:11,190
it self-correct itself in
case it's not able to post.

911
00:47:11,190 --> 00:47:13,200
And again, within few seconds,

912
00:47:13,200 --> 00:47:16,290
it's able to create an asset revision

913
00:47:16,290 --> 00:47:19,020
within SageMaker catalog for this asset.

914
00:47:19,020 --> 00:47:20,850
So it has completed that activity now.

915
00:47:20,850 --> 00:47:22,890
If I go here and refresh,

916
00:47:22,890 --> 00:47:27,890
I will see the README
section is populated.

917
00:47:28,050 --> 00:47:30,540
Now this README section is fairly accurate

918
00:47:30,540 --> 00:47:33,873
in terms of what that dataset is about.

919
00:47:35,220 --> 00:47:37,800
Next, so we have got, we have enriched

920
00:47:37,800 --> 00:47:40,470
the dataset with lineage, README.

921
00:47:40,470 --> 00:47:43,860
Next if you want to, let's
say, run data quality checks,

922
00:47:43,860 --> 00:47:45,750
that's another prompt you can run.

923
00:47:45,750 --> 00:47:49,320
I'm saying that I have a dataset

924
00:47:49,320 --> 00:47:52,650
and I want to run a data
quality for this table.

925
00:47:52,650 --> 00:47:54,720
I would like you to create
two data quality rules,

926
00:47:54,720 --> 00:47:57,570
check null values for each
column and email validation

927
00:47:57,570 --> 00:48:00,360
and use Glue Spark engine to do that.

928
00:48:00,360 --> 00:48:02,370
So again, it goes and plans,

929
00:48:02,370 --> 00:48:04,530
it uses the Glue interactive session

930
00:48:04,530 --> 00:48:06,270
in the context of the project.

931
00:48:06,270 --> 00:48:08,430
It changes the Spark SQL.

932
00:48:08,430 --> 00:48:11,343
And right now the data quality is null.

933
00:48:12,270 --> 00:48:16,323
But once this finishes running
the data quality checks,

934
00:48:17,460 --> 00:48:20,370
you'll see that data
quality will get populated.

935
00:48:20,370 --> 00:48:22,980
And this is where it's save, like,

936
00:48:22,980 --> 00:48:25,320
it has finished running
the data quality checks

937
00:48:25,320 --> 00:48:28,230
and it's saving the results into

938
00:48:28,230 --> 00:48:30,720
a file in the local directory.

939
00:48:30,720 --> 00:48:32,790
And so one last prompt time entering is,

940
00:48:32,790 --> 00:48:36,420
now take this information
that I just generated

941
00:48:36,420 --> 00:48:38,220
in terms of data quality results

942
00:48:38,220 --> 00:48:42,347
and populate the data quality section

943
00:48:43,290 --> 00:48:45,183
within the SageMaker catalog.

944
00:48:46,260 --> 00:48:48,570
And this is all running
in context of the project.

945
00:48:48,570 --> 00:48:51,090
So the scope boundary is that project role

946
00:48:51,090 --> 00:48:52,980
that I showed you.

947
00:48:52,980 --> 00:48:56,250
So this has completed updating
the data quality rules.

948
00:48:56,250 --> 00:48:59,910
And you can see here again
within minute or two,

949
00:48:59,910 --> 00:49:03,210
you have got all of the
data quality results

950
00:49:03,210 --> 00:49:05,360
appearing in the SageMaker catalog.

951
00:49:05,360 --> 00:49:08,370
So now Sam is happy with
all of the enrichment.

952
00:49:08,370 --> 00:49:11,490
So she has updated README,
lineage, and data quality.

953
00:49:11,490 --> 00:49:13,950
So she's going to publish this asset,

954
00:49:13,950 --> 00:49:17,220
which makes it discoverable
to anyone in the organization

955
00:49:17,220 --> 00:49:20,430
who has access to the
SageMaker Unified Studio.

956
00:49:20,430 --> 00:49:23,130
And this is that single
pane that we talked about.

957
00:49:23,130 --> 00:49:25,440
So this customer_profile
table is appearing now.

958
00:49:25,440 --> 00:49:28,080
I'm logged in as Javier
in a different browser.

959
00:49:28,080 --> 00:49:30,150
So you can see it's a different user.

960
00:49:30,150 --> 00:49:33,630
And this is the project overview homepage

961
00:49:33,630 --> 00:49:35,730
and the project selected here is FinCrime.

962
00:49:35,730 --> 00:49:37,290
So it's a different project.

963
00:49:37,290 --> 00:49:40,890
You'll see here the project
role is a different role

964
00:49:40,890 --> 00:49:43,890
and the account number is
that data consumer account

965
00:49:43,890 --> 00:49:46,110
that I talked about earlier.

966
00:49:46,110 --> 00:49:49,050
Javier and his team has access
to a different set of table.

967
00:49:49,050 --> 00:49:50,700
So in this case, he has got access

968
00:49:50,700 --> 00:49:53,403
to a table called transactions.

969
00:49:55,410 --> 00:49:59,250
And he needs more dataset to
be able to build his ML model.

970
00:49:59,250 --> 00:50:01,770
So he goes and types for any information

971
00:50:01,770 --> 00:50:03,510
related to customer_profile.

972
00:50:03,510 --> 00:50:07,770
He can see that there is a
table that has been created

973
00:50:07,770 --> 00:50:09,480
and that exists in the catalog.

974
00:50:09,480 --> 00:50:13,110
So he can analyze all of the
metadata, like column details,

975
00:50:13,110 --> 00:50:15,990
data quality, lineage.

976
00:50:15,990 --> 00:50:19,440
And if he's happy with
all of the information,

977
00:50:19,440 --> 00:50:21,150
so he can also looks at who created this,

978
00:50:21,150 --> 00:50:22,590
which team owns it.

979
00:50:22,590 --> 00:50:24,990
But if he's happy with
all of the information,

980
00:50:24,990 --> 00:50:26,943
he requests access to this table.

981
00:50:27,780 --> 00:50:29,670
So it creates a subscription request.

982
00:50:29,670 --> 00:50:34,670
And this request goes to Sam
who is the owner of the table.

983
00:50:36,750 --> 00:50:40,110
So Sam can see there is
a subscription request

984
00:50:40,110 --> 00:50:44,610
and she goes and reviews
the subscription request

985
00:50:44,610 --> 00:50:46,980
where she can see details
like who has created it,

986
00:50:46,980 --> 00:50:49,440
which team is it coming from,

987
00:50:49,440 --> 00:50:52,770
and if she's happy with, you
know, all of the details,

988
00:50:52,770 --> 00:50:55,260
she goes ahead and approves.

989
00:50:55,260 --> 00:50:58,740
At this point, SageMaker
takes care of the fulfillment.

990
00:50:58,740 --> 00:51:01,680
So even though the data is
located in a different account

991
00:51:01,680 --> 00:51:04,050
and compute is in a different account,

992
00:51:04,050 --> 00:51:08,070
it goes and execute the lake
formation API to, you know,

993
00:51:08,070 --> 00:51:10,980
do the necessary policy
so that, now Javier,

994
00:51:10,980 --> 00:51:15,980
once he goes and refreshes
his technical data catalog,

995
00:51:16,710 --> 00:51:19,263
he will see that the
new table will appear.

996
00:51:21,000 --> 00:51:23,580
So now he has access to
customer_profile table,

997
00:51:23,580 --> 00:51:25,560
he'll run a quick select star.

998
00:51:25,560 --> 00:51:27,990
So this query which he's using Athena

999
00:51:27,990 --> 00:51:30,003
is running in the data consumer account.

1000
00:51:31,260 --> 00:51:33,330
So he's happy with all of the results.

1001
00:51:33,330 --> 00:51:36,630
And so one final step is
to build the ML model.

1002
00:51:36,630 --> 00:51:38,820
So I pre-created a notebook.

1003
00:51:38,820 --> 00:51:41,280
So Javier goes to the Jupyter lab.

1004
00:51:41,280 --> 00:51:44,130
I pre-created a notebook
and I've also taken

1005
00:51:44,130 --> 00:51:47,220
the help of agentic AI to kind
of generate this notebook.

1006
00:51:47,220 --> 00:51:50,760
But he goes and, you
know, runs Athena Query

1007
00:51:50,760 --> 00:51:51,930
for feature engineering.

1008
00:51:51,930 --> 00:51:55,140
He pre-process the data,
he prepares the data,

1009
00:51:55,140 --> 00:51:57,180
he uploads the data to S3,

1010
00:51:57,180 --> 00:51:59,490
and then eventually he trains the model

1011
00:51:59,490 --> 00:52:02,490
using SageMaker AI API.

1012
00:52:02,490 --> 00:52:04,230
This takes about few minutes to run,

1013
00:52:04,230 --> 00:52:07,230
so we won't probably
wait till this finishes,

1014
00:52:07,230 --> 00:52:11,310
but this is kind of the last
step in the process, right?

1015
00:52:11,310 --> 00:52:13,560
So that's the end of the demo.

1016
00:52:13,560 --> 00:52:15,663
Let's head back to the presentation now.

1017
00:52:21,960 --> 00:52:23,490
All right.

1018
00:52:23,490 --> 00:52:26,550
Now before I wrap up,
I'd like to emphasize on

1019
00:52:26,550 --> 00:52:29,250
a few key points that we
covered in our session today.

1020
00:52:30,240 --> 00:52:34,023
First, building data and
AI culture takes time.

1021
00:52:35,010 --> 00:52:38,340
So it's crucial that
you have executed buy-in

1022
00:52:38,340 --> 00:52:39,690
and that you stay invested.

1023
00:52:41,460 --> 00:52:46,440
Second, as you embark on your
data transformation journey,

1024
00:52:46,440 --> 00:52:50,520
it's important to start
with steel thread use cases

1025
00:52:50,520 --> 00:52:54,150
because not only it provides
immediate business value,

1026
00:52:54,150 --> 00:52:57,543
but it also helps with faster
iteration and feedback loop.

1027
00:52:58,920 --> 00:53:02,103
Third, your data is your
unique differentiator.

1028
00:53:03,060 --> 00:53:06,060
And so it's very important

1029
00:53:06,060 --> 00:53:09,300
that you have a strong
data foundation first

1030
00:53:09,300 --> 00:53:10,920
in order to get the maximum value

1031
00:53:10,920 --> 00:53:12,993
from your analytics and AI initiatives.

1032
00:53:14,220 --> 00:53:17,073
And finally, as you saw
in case of CommBank,

1033
00:53:18,060 --> 00:53:19,860
you can leverage Amazon SageMaker

1034
00:53:19,860 --> 00:53:22,920
for your data transformation journey.

1035
00:53:22,920 --> 00:53:25,203
And we at AWS are here to help.

1036
00:53:26,850 --> 00:53:29,160
With that, I'd like to
wrap up our session today.

1037
00:53:29,160 --> 00:53:33,060
I'd like to thank Terri and
Raghu again for co-presenting

1038
00:53:33,060 --> 00:53:35,820
and each one of you for
attending our session today.

1039
00:53:35,820 --> 00:53:38,970
I really appreciate if you could
fill up the feedback survey

1040
00:53:38,970 --> 00:53:40,890
that should be on the mobile app,

1041
00:53:40,890 --> 00:53:44,791
and thank you and have a
great rest of the re:Invent.

1042
00:53:44,791 --> 00:53:46,884
(audience applauds)

