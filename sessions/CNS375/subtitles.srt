1
00:00:00,000 --> 00:00:04,020
- A few weeks ago I was baking
cookies with my daughter.

2
00:00:04,020 --> 00:00:05,670
This is what we are always doing.

3
00:00:06,630 --> 00:00:07,980
She's mixing the dough

4
00:00:07,980 --> 00:00:10,293
and I'm the one who is baking the cookies.

5
00:00:11,250 --> 00:00:15,330
I opened the oven, I found a tray.

6
00:00:15,330 --> 00:00:17,913
Half of the cookies were burned.

7
00:00:19,380 --> 00:00:20,727
We tried to figure out,

8
00:00:20,727 --> 00:00:24,600
but my daughter was looking
like this, her hand full

9
00:00:24,600 --> 00:00:28,167
of flour and she asking me,
"What went wrong, Baba?"

10
00:00:29,070 --> 00:00:30,990
To be honest with you, I don't know

11
00:00:30,990 --> 00:00:33,760
because the recipe didn't tell me

12
00:00:34,650 --> 00:00:36,540
the butter was too soft

13
00:00:36,540 --> 00:00:40,800
or even the oven didn't
mention, do we have

14
00:00:40,800 --> 00:00:43,653
half of the oven hotter
than the other half or not?

15
00:00:44,490 --> 00:00:48,810
This is what hits me
because if this is happening

16
00:00:48,810 --> 00:00:52,560
inside any manufacturing,
what will happen?

17
00:00:52,560 --> 00:00:55,080
You have the data, you have the sensors

18
00:00:55,080 --> 00:00:57,390
are doing a notifications for your system.

19
00:00:57,390 --> 00:00:58,977
You have a system is recording this.

20
00:00:58,977 --> 00:01:01,800
You have a machines that is reporting

21
00:01:01,800 --> 00:01:04,710
those kind of informations, but at the end

22
00:01:04,710 --> 00:01:07,230
they are always disconnected.

23
00:01:07,230 --> 00:01:09,780
This kind of silent moment.

24
00:01:09,780 --> 00:01:12,510
This is what we are trying to solve today.

25
00:01:12,510 --> 00:01:14,040
Today I'm going to show you

26
00:01:14,040 --> 00:01:18,990
how AWS Serverless
architecture along with GenAI

27
00:01:18,990 --> 00:01:22,570
can build a connected manufacturer

28
00:01:23,575 --> 00:01:25,200
integrated with an AI system

29
00:01:25,200 --> 00:01:29,670
to have a solid understanding
of your manufacturer.

30
00:01:29,670 --> 00:01:31,050
My name is Mohamed Salah,

31
00:01:31,050 --> 00:01:33,780
working as a solution architect at AWS,

32
00:01:33,780 --> 00:01:36,933
looking after public
sector in the Middle East.

33
00:01:39,960 --> 00:01:44,490
My job at AWS is helping
different customers

34
00:01:44,490 --> 00:01:46,980
solving similar problems.

35
00:01:46,980 --> 00:01:48,003
Picture this,

36
00:01:49,680 --> 00:01:50,800
same silent

37
00:01:52,102 --> 00:01:54,273
inside the factory.

38
00:01:55,290 --> 00:01:59,340
Machine stops, production line pause,

39
00:01:59,340 --> 00:02:02,010
and everyone starts running around.

40
00:02:02,010 --> 00:02:06,000
Every minute, your products

41
00:02:06,000 --> 00:02:08,370
are not made.

42
00:02:08,370 --> 00:02:10,230
Your orders got delayed

43
00:02:10,230 --> 00:02:11,943
and cost pile up.

44
00:02:12,870 --> 00:02:16,050
This silence, this helpless pause,

45
00:02:16,050 --> 00:02:18,960
what we are calling unplanned downtime.

46
00:02:18,960 --> 00:02:22,830
The top 500 manufacturers globally,

47
00:02:22,830 --> 00:02:24,490
this silence cost them

48
00:02:25,379 --> 00:02:29,670
1.4 trillion US dollar every year.

49
00:02:29,670 --> 00:02:33,333
They are losing around
11% of their revenues.

50
00:02:34,290 --> 00:02:38,400
To make it much more clear,
this is equivalent to the GDP

51
00:02:38,400 --> 00:02:39,663
of a nation like Spain.

52
00:02:41,490 --> 00:02:44,940
When we take a closer
look about this, we found

53
00:02:44,940 --> 00:02:47,070
it has three different issues.

54
00:02:47,070 --> 00:02:48,870
First, is a data silo.

55
00:02:48,870 --> 00:02:52,230
Your production line consists
of different machines,

56
00:02:52,230 --> 00:02:53,610
four or five different machines

57
00:02:53,610 --> 00:02:56,920
and each machines has its own

58
00:02:57,900 --> 00:03:02,700
data sources or different
data informations.

59
00:03:02,700 --> 00:03:04,410
You need to report those informations

60
00:03:04,410 --> 00:03:07,860
and those informations
are completely recorded

61
00:03:07,860 --> 00:03:09,480
in a separate database.

62
00:03:09,480 --> 00:03:12,300
It will end up, you have
five machines as part

63
00:03:12,300 --> 00:03:16,650
of your production line, each
machine has its own database

64
00:03:16,650 --> 00:03:18,870
but you don't have this
kind of a correlation

65
00:03:18,870 --> 00:03:22,113
to understand what happened end to end.

66
00:03:23,250 --> 00:03:26,553
Yes, the data are there
but it is disconnected.

67
00:03:27,480 --> 00:03:30,210
Second is a scale gap.
It's all about people.

68
00:03:30,210 --> 00:03:33,270
It's all about having in each factory,

69
00:03:33,270 --> 00:03:36,210
you will have this kind
of an expert and seniors,

70
00:03:36,210 --> 00:03:40,980
they can understand what
exactly happening on the ground.

71
00:03:40,980 --> 00:03:43,530
They can give you from
the sound of the mixer,

72
00:03:43,530 --> 00:03:46,920
they can tell you there
is an over-mixing here.

73
00:03:46,920 --> 00:03:50,190
From the oven, this oven
is having a problem.

74
00:03:50,190 --> 00:03:51,760
This is hotter

75
00:03:53,100 --> 00:03:55,413
in left side than the right side.

76
00:03:57,150 --> 00:03:58,950
This kind of knowledge,

77
00:03:58,950 --> 00:04:02,670
if those experts are not
there, this kind of knowledge

78
00:04:02,670 --> 00:04:07,500
will end up that you
are not able to operate

79
00:04:07,500 --> 00:04:09,510
your production line correctly

80
00:04:09,510 --> 00:04:14,040
because junior operators
will stay in the line,

81
00:04:14,040 --> 00:04:17,550
receiving warnings but they
will not be able to act.

82
00:04:17,550 --> 00:04:21,633
This means that the knowledge
are there but are not shared.

83
00:04:23,280 --> 00:04:27,850
Third one, which is when
this silence became visible

84
00:04:28,710 --> 00:04:33,630
because once this silence
happened the production line

85
00:04:33,630 --> 00:04:38,133
will get a product delayed and
the orders will get delayed.

86
00:04:38,970 --> 00:04:39,803
Why?

87
00:04:39,803 --> 00:04:44,113
Because you have a multiple
data sources at the end.

88
00:04:44,113 --> 00:04:47,040
At the same time, you don't
have this kind of an expertise

89
00:04:47,040 --> 00:04:49,923
that can analyze those data sources.

90
00:04:51,600 --> 00:04:54,090
We thought about how we can solve this.

91
00:04:54,090 --> 00:04:56,550
We started with a simple solution.

92
00:04:56,550 --> 00:04:59,370
Let's sort of think of a data link.

93
00:04:59,370 --> 00:05:02,430
As you can see here, we have
a multiple data sources.

94
00:05:02,430 --> 00:05:04,770
We have a telemetry data and alarms.

95
00:05:04,770 --> 00:05:08,730
We have an environmental
data and we have a document

96
00:05:08,730 --> 00:05:11,340
and standard operation procedures.

97
00:05:11,340 --> 00:05:14,280
By implementing those data sources

98
00:05:14,280 --> 00:05:16,170
and having this in one data lake,

99
00:05:16,170 --> 00:05:19,290
we found another problem,
which is the integrations.

100
00:05:19,290 --> 00:05:23,610
Each data source has its own integrations.

101
00:05:23,610 --> 00:05:26,820
As you can see, some of the data sources

102
00:05:26,820 --> 00:05:29,400
has a modern IoT integration,

103
00:05:29,400 --> 00:05:34,290
MQTT or (indistinct) or even
you have a legacy integrations

104
00:05:34,290 --> 00:05:36,453
for a document using your SFTP.

105
00:05:37,560 --> 00:05:40,700
What we thought about why not to implement

106
00:05:40,700 --> 00:05:44,490
AWS Garnet framework to have one single

107
00:05:44,490 --> 00:05:49,470
unified API to absorb this
different types of integrations

108
00:05:49,470 --> 00:05:54,180
and convert your data in a modeled data

109
00:05:54,180 --> 00:05:56,700
where you can have integration

110
00:05:56,700 --> 00:06:01,700
between your different IoT
devices in one graph database.

111
00:06:02,760 --> 00:06:05,940
This will build the relations
between different things.

112
00:06:05,940 --> 00:06:09,060
Once you got this data
lake, you will be able

113
00:06:09,060 --> 00:06:12,990
to have a different
consumers for this data lake.

114
00:06:12,990 --> 00:06:16,380
One of them is QuickSight at the dashboard

115
00:06:16,380 --> 00:06:21,380
and one of them building a
replica of your production line.

116
00:06:22,080 --> 00:06:25,233
This replica, what we are
calling a digital twin.

117
00:06:26,400 --> 00:06:28,230
Let's have a closer look,

118
00:06:28,230 --> 00:06:30,000
where you can have, on the left side,

119
00:06:30,000 --> 00:06:31,440
this is your production line.

120
00:06:31,440 --> 00:06:34,110
This is what exactly
happening on the ground.

121
00:06:34,110 --> 00:06:36,810
You understand what is the problem.

122
00:06:36,810 --> 00:06:38,527
As you can see, the cookie

123
00:06:38,527 --> 00:06:41,490
and inspector is telling
you cookie are cracked

124
00:06:41,490 --> 00:06:44,283
and the machine state is down since five.

125
00:06:45,390 --> 00:06:47,700
With this dashboard you will achieve

126
00:06:47,700 --> 00:06:52,530
what we are calling a
connected data points.

127
00:06:52,530 --> 00:06:55,653
At the same time, you will
break this kind of data silos.

128
00:06:57,540 --> 00:07:00,960
The important part is skill gap.

129
00:07:00,960 --> 00:07:03,150
Yes, you have this expert

130
00:07:03,150 --> 00:07:07,020
but you don't have this
kind of knowledge sharing.

131
00:07:07,020 --> 00:07:10,560
What we thought about why not to implement

132
00:07:10,560 --> 00:07:15,560
a GenAI application to be
deployed over Amazon Bedrock

133
00:07:15,870 --> 00:07:19,980
and to let the operator integrate
with this, ask a question

134
00:07:19,980 --> 00:07:22,110
and chat with your data.

135
00:07:22,110 --> 00:07:25,500
As you can see, we are
fine-tuning the model

136
00:07:25,500 --> 00:07:29,400
and we are importing this
model inside Amazon Bedrock

137
00:07:29,400 --> 00:07:34,400
to expose this kind of
text and voice application.

138
00:07:34,410 --> 00:07:38,850
At the same time, you are
getting real-time data

139
00:07:38,850 --> 00:07:43,850
streamed using the Garnet
framework to your Bedrock engine.

140
00:07:44,040 --> 00:07:47,793
This is will give you
bridging the skill gap.

141
00:07:52,230 --> 00:07:54,396
Second, which is very important

142
00:07:54,396 --> 00:07:57,090
for the different cases,
the automation needed.

143
00:07:57,090 --> 00:07:59,520
You need to have this
kind of an automation

144
00:07:59,520 --> 00:08:03,180
every five minutes, understanding
what exactly happening

145
00:08:03,180 --> 00:08:06,357
under the hood, know the streams

146
00:08:06,357 --> 00:08:09,510
and then getting this
kind of recommendations

147
00:08:09,510 --> 00:08:13,200
according to what happening
and what data captured

148
00:08:13,200 --> 00:08:15,420
from different data sources

149
00:08:15,420 --> 00:08:19,830
and give a notification to an
operator to act accordingly.

150
00:08:19,830 --> 00:08:24,630
What we thought about why not
to have a full automation AI

151
00:08:24,630 --> 00:08:26,538
to act, to call APIs

152
00:08:26,538 --> 00:08:28,688
with different machines
to act accordingly.

153
00:08:29,970 --> 00:08:32,583
This is on a very high level how it works.

154
00:08:33,900 --> 00:08:36,390
You have a cookie inspector
here after the former,

155
00:08:36,390 --> 00:08:40,350
after the tunnel and this
inspector is inspecting

156
00:08:40,350 --> 00:08:42,993
just in a small language model, VLM,

157
00:08:44,310 --> 00:08:46,980
inspecting either the cookie are cracked

158
00:08:46,980 --> 00:08:51,980
or cookies misshaped or
cookies has air pockets.

159
00:08:53,280 --> 00:08:55,770
Let's say we take in cracked cookies.

160
00:08:55,770 --> 00:08:59,770
You are going to invoke Bedrock
with a vision language model

161
00:09:01,320 --> 00:09:06,150
and Bedrock will ask to
get multiple data sources.

162
00:09:06,150 --> 00:09:09,030
Production line standard
operation procedure,

163
00:09:09,030 --> 00:09:11,460
you can get machine alarms and data,

164
00:09:11,460 --> 00:09:13,230
you can get the telemetry data

165
00:09:13,230 --> 00:09:16,290
and get even the machine manuals.

166
00:09:16,290 --> 00:09:19,950
Once you are getting this
data source, you will be able

167
00:09:19,950 --> 00:09:24,510
to first send the automated
notifications to your operator.

168
00:09:24,510 --> 00:09:27,240
At the same time, you can expose this text

169
00:09:27,240 --> 00:09:29,650
and voice chatting application

170
00:09:30,510 --> 00:09:33,453
and accordingly you can take actions.

171
00:09:34,290 --> 00:09:36,510
This is what will happen

172
00:09:36,510 --> 00:09:41,400
to solve the problem automatically
without having a senior

173
00:09:41,400 --> 00:09:43,470
on the ground and without having

174
00:09:43,470 --> 00:09:46,650
a fully integrated data points.

175
00:09:46,650 --> 00:09:49,803
Here, on high-level
architecture, how it work.

176
00:09:50,700 --> 00:09:53,583
Let me show you in details.

177
00:09:54,630 --> 00:09:57,570
This is where the equipment
measurement will fit

178
00:09:57,570 --> 00:10:00,810
inside AWS Garnet framework

179
00:10:00,810 --> 00:10:03,760
and for AWS IoT Core, you will

180
00:10:04,950 --> 00:10:05,860
send the data

181
00:10:06,840 --> 00:10:10,260
using this lambda function NSGI

182
00:10:10,260 --> 00:10:14,190
will change the data model of your input

183
00:10:14,190 --> 00:10:17,460
to a different standard data model.

184
00:10:17,460 --> 00:10:21,180
Accordingly, you will have
an SAQS to do a decoupling

185
00:10:21,180 --> 00:10:25,440
between your S3, which is
here acting as a data lake,

186
00:10:25,440 --> 00:10:28,533
to consolidate all your
data points in one place.

187
00:10:29,490 --> 00:10:34,490
Second part, which is important
here is actions needed.

188
00:10:34,950 --> 00:10:38,670
Is it possible to have actions to update

189
00:10:38,670 --> 00:10:41,160
actions in your production line,

190
00:10:41,160 --> 00:10:45,120
to update the cookie mixer
to reduce the temperature,

191
00:10:45,120 --> 00:10:48,090
reduce the torque or do different actions?

192
00:10:48,090 --> 00:10:52,623
This can be done using an API
gateway to do those actions.

193
00:10:55,770 --> 00:10:57,630
This is the foundational part.

194
00:10:57,630 --> 00:10:58,773
The data preparation.

195
00:10:59,670 --> 00:11:02,040
We are doing here a data preparation

196
00:11:02,040 --> 00:11:05,490
because we need the model itself

197
00:11:05,490 --> 00:11:10,380
to understand how to respond to queries.

198
00:11:10,380 --> 00:11:14,880
The response can be structured
in a very detailed way

199
00:11:14,880 --> 00:11:18,840
to guide the operator
how to solve the problem

200
00:11:18,840 --> 00:11:22,620
and instructions should be
compliant, at the same time,

201
00:11:22,620 --> 00:11:26,730
should be safe and should
categorize this kind of safety.

202
00:11:26,730 --> 00:11:31,050
That's why we thought about
why not to fine-tune the model.

203
00:11:31,050 --> 00:11:33,330
To achieve this kind of skills

204
00:11:33,330 --> 00:11:36,960
and this kind of tone
needed for your model.

205
00:11:36,960 --> 00:11:40,323
We started with having
a multiple documents

206
00:11:42,207 --> 00:11:44,130
and multiple data sources.

207
00:11:44,130 --> 00:11:47,280
We are converting this to...

208
00:11:47,280 --> 00:11:49,950
We are transferring this to S3

209
00:11:49,950 --> 00:11:52,200
and accordingly we having a step function.

210
00:11:52,200 --> 00:11:55,470
This step function will
act as a data pipeline.

211
00:11:55,470 --> 00:11:58,402
You are reading the document,
reading the PDF files,

212
00:11:58,402 --> 00:12:00,240
CSV files, text files,

213
00:12:00,240 --> 00:12:03,330
understanding what exactly
happening on the ground

214
00:12:03,330 --> 00:12:08,330
and then invoking a model to
generate a structured data.

215
00:12:09,780 --> 00:12:11,400
Why structured data is needed

216
00:12:11,400 --> 00:12:14,730
because if you are going
to fine-tune a model,

217
00:12:14,730 --> 00:12:17,310
you need a very structured data

218
00:12:17,310 --> 00:12:19,233
because if you are not
having a structured data,

219
00:12:19,233 --> 00:12:22,113
it's a kind of fine-tuning
hallucination machine.

220
00:12:23,820 --> 00:12:27,270
Accordingly, you will have an
output of a structured data

221
00:12:27,270 --> 00:12:30,273
and another output of a
structured data only for testing.

222
00:12:31,470 --> 00:12:35,100
This output will feed into a SageMaker AI

223
00:12:35,100 --> 00:12:37,650
foundation model operations

224
00:12:37,650 --> 00:12:41,400
where I will fine tune
the model using this,

225
00:12:41,400 --> 00:12:42,720
fine-tune the data

226
00:12:42,720 --> 00:12:46,713
and using what we are calling
instructed fine-tuning.

227
00:12:47,640 --> 00:12:52,200
Once I got this kind of artifacts

228
00:12:52,200 --> 00:12:55,950
or updated weights for my model,

229
00:12:55,950 --> 00:13:00,950
I can accordingly import the
model inside Amazon Bedrock.

230
00:13:02,280 --> 00:13:05,670
This step can be done in different ways.

231
00:13:05,670 --> 00:13:09,630
We prefer here to use a serverless
implementation by Bedrock

232
00:13:09,630 --> 00:13:12,813
and instead of deploying
this over a EC2 instance,

233
00:13:12,813 --> 00:13:16,380
this will end up, you have to
calculate the GP utilizations.

234
00:13:16,380 --> 00:13:19,890
You have to be aware
exactly what is happening

235
00:13:19,890 --> 00:13:22,770
under the hood to do a correct inferencing

236
00:13:22,770 --> 00:13:25,710
but just importing this over Bedrock

237
00:13:25,710 --> 00:13:30,710
will give you a unified API
to call a different versions

238
00:13:30,912 --> 00:13:32,793
of your imported model.

239
00:13:33,720 --> 00:13:34,740
Once you've got this

240
00:13:34,740 --> 00:13:39,030
and once you have this
Amazon Bedrock, we can go

241
00:13:39,030 --> 00:13:40,263
to the web application.

242
00:13:41,790 --> 00:13:44,130
The significance here of
using the web application

243
00:13:44,130 --> 00:13:48,300
is to expose this kind
of a chatting for a voice

244
00:13:48,300 --> 00:13:50,640
and data with your model.

245
00:13:50,640 --> 00:13:54,420
As you can see, we have
two different things here.

246
00:13:54,420 --> 00:13:57,600
First is the chatting
application with the React,

247
00:13:57,600 --> 00:14:00,870
the normal React over
S3 with lambda functions

248
00:14:00,870 --> 00:14:03,930
to invoke the model, here we are using

249
00:14:03,930 --> 00:14:06,420
something called Converse API.

250
00:14:06,420 --> 00:14:09,750
This Converse API is
giving you a flexibility

251
00:14:09,750 --> 00:14:13,950
and even the scalability to invoke

252
00:14:13,950 --> 00:14:16,860
any model versions from
your fine-tune model.

253
00:14:16,860 --> 00:14:20,100
Let's say you got a feedback,
thumbs up, about one

254
00:14:20,100 --> 00:14:22,710
of your models and this
model is doing great.

255
00:14:22,710 --> 00:14:25,440
However, you found an
issue in a production

256
00:14:25,440 --> 00:14:28,250
after 10 days, you can
roll back in a model

257
00:14:28,250 --> 00:14:33,250
by just changing the model
ID using the Converse API.

258
00:14:33,270 --> 00:14:37,140
The significance here is the Converse API

259
00:14:37,140 --> 00:14:40,440
remain the same signature of the API

260
00:14:40,440 --> 00:14:43,863
while changing a different model IDs.

261
00:14:45,390 --> 00:14:47,340
Second, which is the important part,

262
00:14:47,340 --> 00:14:49,410
is the notification part here.

263
00:14:49,410 --> 00:14:53,940
The notification here where
you can feed the model

264
00:14:53,940 --> 00:14:57,000
with the data captured every five minutes

265
00:14:57,000 --> 00:15:01,080
and then start digesting
those informations,

266
00:15:01,080 --> 00:15:03,420
comparing those informations

267
00:15:03,420 --> 00:15:06,330
with the fine-tune, the model, in order

268
00:15:06,330 --> 00:15:09,180
to give you a clear recommendations

269
00:15:09,180 --> 00:15:12,450
considering the compliance,
considering the severity,

270
00:15:12,450 --> 00:15:16,323
considering the needed steps
to be done on the ground.

271
00:15:17,190 --> 00:15:20,130
By doing this, you can have an SNS,

272
00:15:20,130 --> 00:15:24,303
SNS notifications to your
operator to notify them.

273
00:15:25,530 --> 00:15:29,400
You can act accordingly in this

274
00:15:29,400 --> 00:15:33,450
event in order to change
the needed actions.

275
00:15:33,450 --> 00:15:36,900
At the same time, you
can avail both options

276
00:15:36,900 --> 00:15:37,833
in the same place.

277
00:15:39,750 --> 00:15:42,270
This is on a very high
level the architecture

278
00:15:42,270 --> 00:15:46,890
where we started with
digesting the information

279
00:15:46,890 --> 00:15:49,410
here using a Garnet framework

280
00:15:49,410 --> 00:15:52,470
and then this is a foundational part

281
00:15:52,470 --> 00:15:55,050
where you are doing
here a data preparation

282
00:15:55,050 --> 00:15:57,270
and then fine-tuning the model

283
00:15:57,270 --> 00:16:00,760
and then you are having a
web application that connect

284
00:16:01,770 --> 00:16:04,803
with custom model
imported over the Bedrock.

285
00:16:07,500 --> 00:16:08,793
At this point of time,

286
00:16:11,826 --> 00:16:15,893
you have a AI application
that makes every single

287
00:16:17,949 --> 00:16:22,653
machine sensor and system are
speaking the same language.

288
00:16:24,870 --> 00:16:27,340
You are able to have a connected

289
00:16:29,220 --> 00:16:33,360
data points and connected
machines in one single place

290
00:16:33,360 --> 00:16:36,270
by having a Garnet
framework implementations

291
00:16:36,270 --> 00:16:41,270
to absorb this different types
of integrations in one place.

292
00:16:41,700 --> 00:16:46,140
Second, is you will be
able to have this kind

293
00:16:46,140 --> 00:16:49,170
of fine-tuning the model,

294
00:16:49,170 --> 00:16:52,300
importing the model inside Amazon Bedrock

295
00:16:53,140 --> 00:16:55,500
and giving you this kind of intelligency.

296
00:16:55,500 --> 00:16:57,630
Now, we understand you can explain

297
00:16:57,630 --> 00:17:00,870
what is happening on the ground,

298
00:17:00,870 --> 00:17:05,010
what changes and what will be done next

299
00:17:05,010 --> 00:17:06,960
and what is the recommendations needed.

300
00:17:08,820 --> 00:17:12,180
By implementing this, you are still having

301
00:17:12,180 --> 00:17:15,060
and you're still
generating much more data.

302
00:17:15,060 --> 00:17:17,460
This is what we are calling
a continuous learning

303
00:17:17,460 --> 00:17:19,230
and this is your gold.

304
00:17:19,230 --> 00:17:23,790
Once you have updated
informations, you still have room

305
00:17:23,790 --> 00:17:25,770
to improve your model

306
00:17:25,770 --> 00:17:27,960
and improve your accuracy of your model

307
00:17:27,960 --> 00:17:31,350
and getting much more
accurate information.

308
00:17:31,350 --> 00:17:34,950
This is what we are calling
a continuous learning.

309
00:17:34,950 --> 00:17:36,810
This is much-needed

310
00:17:36,810 --> 00:17:39,180
because as part of, you can see,

311
00:17:39,180 --> 00:17:41,700
as part of this implementation,

312
00:17:41,700 --> 00:17:45,230
there is a feedback implemented mechanism

313
00:17:45,230 --> 00:17:50,100
by the operator based on the
response came from the Bedrock.

314
00:17:50,100 --> 00:17:54,390
If this feedback sums up or
sums down, I can act accordingly

315
00:17:54,390 --> 00:17:58,650
and I can give updated, fine-tuned model

316
00:17:58,650 --> 00:18:00,273
based on this information.

317
00:18:01,830 --> 00:18:04,320
You can scan those QR codes.

318
00:18:04,320 --> 00:18:07,680
This is a Git triple
for the Garnet framework

319
00:18:07,680 --> 00:18:09,600
to have this kind of a conversion

320
00:18:09,600 --> 00:18:14,100
of your connected
devices, at the same time,

321
00:18:14,100 --> 00:18:18,240
the second QR code of how to implement

322
00:18:18,240 --> 00:18:19,167
a small language model

323
00:18:19,167 --> 00:18:22,203
and import the custom
model over the Bedrock.

324
00:18:23,106 --> 00:18:28,106
And last one, how to continue
updating your information,

325
00:18:28,800 --> 00:18:31,140
fine-tuning the model and the implementing

326
00:18:31,140 --> 00:18:34,263
machine-learning pipeline end to end.

327
00:18:36,090 --> 00:18:37,680
Now, it's your turn.

328
00:18:37,680 --> 00:18:41,550
You can start with a
small process, small data,

329
00:18:41,550 --> 00:18:43,620
you can understand
exactly what is happening

330
00:18:43,620 --> 00:18:44,760
under the ground.

331
00:18:44,760 --> 00:18:46,950
You can start with one production line,

332
00:18:46,950 --> 00:18:48,870
with multiple data source,

333
00:18:48,870 --> 00:18:53,870
implement those kind of
fine-tuning implementation,

334
00:18:55,470 --> 00:18:59,190
we did this full implementation
in just seven days.

335
00:18:59,190 --> 00:19:02,363
After implementing this
and importing the model,

336
00:19:02,363 --> 00:19:04,050
you will be able to gain more insights

337
00:19:04,050 --> 00:19:05,433
and enhance your model.

338
00:19:06,660 --> 00:19:07,493
Thank you.

