1
00:00:00,960 --> 00:00:03,720
- First of all, welcome to re:Invent 2025.

2
00:00:03,720 --> 00:00:06,180
I hope day one is going well for you.

3
00:00:06,180 --> 00:00:08,130
My name is Enrique Bastante.

4
00:00:08,130 --> 00:00:10,650
I am a customer solutions manager

5
00:00:10,650 --> 00:00:12,810
with AWS Strategic Accounts

6
00:00:12,810 --> 00:00:15,690
and I'm really excited
about this session today

7
00:00:15,690 --> 00:00:17,670
for a couple of reasons.

8
00:00:17,670 --> 00:00:20,100
First of all, as part of my role,

9
00:00:20,100 --> 00:00:22,530
I get to work closely with Capital One

10
00:00:22,530 --> 00:00:24,000
on some really cool stuff,

11
00:00:24,000 --> 00:00:26,900
including some of the things
that you'll hear about today.

12
00:00:28,110 --> 00:00:29,430
But also because resilience

13
00:00:29,430 --> 00:00:31,500
is actually one of my favorite topics

14
00:00:31,500 --> 00:00:32,650
when it comes to Cloud.

15
00:00:33,570 --> 00:00:35,700
It cuts across every industry,

16
00:00:35,700 --> 00:00:37,053
it's important to everyone,

17
00:00:37,950 --> 00:00:39,930
and you may or may not know this,

18
00:00:39,930 --> 00:00:40,763
but Capital One

19
00:00:40,763 --> 00:00:43,293
is actually really,
really good at resilience.

20
00:00:44,220 --> 00:00:46,920
They're often regarded across AWS

21
00:00:46,920 --> 00:00:48,960
as a world class organization

22
00:00:48,960 --> 00:00:51,270
when it comes to resilience practices.

23
00:00:51,270 --> 00:00:54,810
So I'm really excited for you
guys to hear about it today.

24
00:00:54,810 --> 00:00:56,700
I've got with me two leaders

25
00:00:56,700 --> 00:01:00,120
from their enterprise
platform organization

26
00:01:00,120 --> 00:01:01,290
and they're gonna talk to you about

27
00:01:01,290 --> 00:01:03,300
how Capital One goes about

28
00:01:03,300 --> 00:01:05,460
building high resilient

29
00:01:05,460 --> 00:01:08,640
platforms for business
critical applications

30
00:01:08,640 --> 00:01:10,023
that run a massive scale.

31
00:01:11,640 --> 00:01:13,230
They're gonna talk a little bit about

32
00:01:13,230 --> 00:01:15,210
how they got to where they today,

33
00:01:15,210 --> 00:01:17,610
some of the challenges
they faced early on,

34
00:01:17,610 --> 00:01:20,640
some of the decisions that
they made along the way,

35
00:01:20,640 --> 00:01:23,190
and some of the best
practices and lessons learned.

36
00:01:24,600 --> 00:01:25,440
But they're not gonna stop there.

37
00:01:25,440 --> 00:01:28,530
They're gonna talk to you also
about where they're headed.

38
00:01:28,530 --> 00:01:32,970
So some of the more
advanced resilience patterns

39
00:01:32,970 --> 00:01:34,590
that are implementing now

40
00:01:34,590 --> 00:01:36,360
to continue to build more resilience

41
00:01:36,360 --> 00:01:39,210
into these business critical applications

42
00:01:39,210 --> 00:01:40,043
because as you know,

43
00:01:40,043 --> 00:01:44,310
when it comes to the stuff
that runs your business,

44
00:01:44,310 --> 00:01:46,560
you always have to
continue to raise the bar.

45
00:01:47,640 --> 00:01:50,397
So welcome again, thank
you for joining us.

46
00:01:50,397 --> 00:01:51,390
And with that said,

47
00:01:51,390 --> 00:01:53,040
I'm gonna pass it over to Sobhan.

48
00:01:56,070 --> 00:01:59,100
- Thanks. Thanks for
setting the stage for us.

49
00:01:59,100 --> 00:01:59,943
First of all,

50
00:02:00,900 --> 00:02:02,760
good afternoon everyone.

51
00:02:02,760 --> 00:02:04,503
Thanks for joining us today.

52
00:02:05,610 --> 00:02:07,440
I'll start with a simple question.

53
00:02:07,440 --> 00:02:11,640
How many of you are using
any of Capital One apps

54
00:02:11,640 --> 00:02:12,960
like our shopping,

55
00:02:12,960 --> 00:02:15,393
banking, or any of our services?

56
00:02:17,838 --> 00:02:18,671
A lot of you.

57
00:02:19,680 --> 00:02:22,980
So our, all of the services apps

58
00:02:22,980 --> 00:02:26,403
are built on top of our modern platforms.

59
00:02:28,800 --> 00:02:32,280
So today, we are going
to focus on our journey

60
00:02:32,280 --> 00:02:33,730
as a platform company

61
00:02:34,620 --> 00:02:36,630
and what are the principles

62
00:02:36,630 --> 00:02:39,790
that we believe in building the platforms

63
00:02:41,250 --> 00:02:43,530
and our focus on reliability,

64
00:02:43,530 --> 00:02:46,110
the architectural patterns,

65
00:02:46,110 --> 00:02:48,900
and the best practices that we adopted

66
00:02:48,900 --> 00:02:50,523
to gain our customer trust.

67
00:02:52,890 --> 00:02:54,690
Platforms.

68
00:02:54,690 --> 00:02:57,600
The platforms are nothing but a digital

69
00:02:57,600 --> 00:02:58,620
Lego blocks

70
00:02:58,620 --> 00:03:02,910
that are leveraged to build
our customer capabilities,

71
00:03:02,910 --> 00:03:07,910
our customer functionalities.

72
00:03:07,920 --> 00:03:10,530
So which we can use

73
00:03:10,530 --> 00:03:12,990
to build similarly like a

74
00:03:12,990 --> 00:03:15,813
vibrant city with a Lego blocks.

75
00:03:17,130 --> 00:03:19,023
So before 2019,

76
00:03:21,000 --> 00:03:24,660
we used to have a duplicated capabilities

77
00:03:24,660 --> 00:03:26,520
across our lines of business.

78
00:03:26,520 --> 00:03:28,110
For example,

79
00:03:28,110 --> 00:03:30,750
our line of business like card,

80
00:03:30,750 --> 00:03:35,550
bank used to build the
payment capabilities

81
00:03:35,550 --> 00:03:38,970
or like transaction
processing capabilities

82
00:03:38,970 --> 00:03:41,523
as per to support their business needs.

83
00:03:42,840 --> 00:03:44,460
So we observed

84
00:03:44,460 --> 00:03:47,193
a non-standardized architectural patterns,

85
00:03:48,450 --> 00:03:50,460
a duplication of data.

86
00:03:50,460 --> 00:03:53,400
For example, if I'm a customer

87
00:03:53,400 --> 00:03:56,340
that have both a banking account

88
00:03:56,340 --> 00:03:58,740
and as well as a credit card account,

89
00:03:58,740 --> 00:04:00,450
my customer data is duplicated

90
00:04:00,450 --> 00:04:03,060
across these lines of businesses

91
00:04:03,060 --> 00:04:04,180
in a different format

92
00:04:05,280 --> 00:04:09,090
and we spent a lot of developer effort

93
00:04:09,090 --> 00:04:10,773
and the runtime engine cost.

94
00:04:12,218 --> 00:04:13,620
With that

95
00:04:13,620 --> 00:04:14,453
we incurred

96
00:04:15,450 --> 00:04:19,890
and we have an impact
on our speed to market,

97
00:04:19,890 --> 00:04:21,453
our business capabilities,

98
00:04:22,470 --> 00:04:25,893
and also scaling our platforms.

99
00:04:27,150 --> 00:04:30,243
And in turn, it resulted
to our customer trust.

100
00:04:32,190 --> 00:04:36,183
So in 2019, we declared ourself
as a platform organization.

101
00:04:37,050 --> 00:04:39,510
So we put a heavy investment

102
00:04:39,510 --> 00:04:43,800
on building the foundational
structures or platforms

103
00:04:43,800 --> 00:04:47,070
that are not just a reusable capabilities,

104
00:04:47,070 --> 00:04:50,430
but also that are scalable,

105
00:04:50,430 --> 00:04:51,263
reliable,

106
00:04:52,410 --> 00:04:53,943
that are trustworthy.

107
00:04:56,490 --> 00:04:59,310
So some of the platforms that we built

108
00:04:59,310 --> 00:05:01,110
throughout this journey,

109
00:05:01,110 --> 00:05:04,890
there are multiple
flavors of these platforms

110
00:05:04,890 --> 00:05:08,940
that can be leveraged by
both our internal customers

111
00:05:08,940 --> 00:05:10,830
like our developers,

112
00:05:10,830 --> 00:05:12,540
our LOB partners,

113
00:05:12,540 --> 00:05:16,050
and as well as our business
state business holders,

114
00:05:16,050 --> 00:05:19,410
and also our external
customers and the end users.

115
00:05:19,410 --> 00:05:24,000
Some of them are like
the developer platforms,

116
00:05:24,000 --> 00:05:25,930
like the CICD platform

117
00:05:27,309 --> 00:05:31,380
where we streamlined
our deployment processes

118
00:05:31,380 --> 00:05:32,940
with a lot of controls,

119
00:05:32,940 --> 00:05:35,700
guardrails embedded into it

120
00:05:35,700 --> 00:05:38,760
so that every engineering team

121
00:05:38,760 --> 00:05:41,400
or an engineer can benefit out of it

122
00:05:41,400 --> 00:05:44,190
and they don't need to reinvent the wheel.

123
00:05:44,190 --> 00:05:45,550
Similarly, we

124
00:05:46,410 --> 00:05:50,010
resolve the arbitrary uniqueness
across lines of business

125
00:05:50,010 --> 00:05:52,917
by building the core
business platforms like

126
00:05:52,917 --> 00:05:56,100
our transaction processing platform,

127
00:05:56,100 --> 00:05:57,753
our payment processing platform,

128
00:05:58,890 --> 00:06:02,610
to provide the support across
a seamless experience across

129
00:06:02,610 --> 00:06:03,993
all lines of businesses.

130
00:06:04,920 --> 00:06:07,620
And we have invested building like

131
00:06:07,620 --> 00:06:10,200
customer interacting platforms

132
00:06:10,200 --> 00:06:12,110
like identity platform,

133
00:06:12,110 --> 00:06:14,370
or a messaging platform

134
00:06:14,370 --> 00:06:17,943
that provides a seamless
customer experience.

135
00:06:19,710 --> 00:06:22,623
Irrespective of what type of
platforms that we build on,

136
00:06:23,640 --> 00:06:27,340
our objective is how we can scale as a

137
00:06:28,560 --> 00:06:30,210
organization.

138
00:06:30,210 --> 00:06:31,200
We can scale,

139
00:06:31,200 --> 00:06:33,150
these platforms can scale

140
00:06:33,150 --> 00:06:37,233
based upon the trust that
we get from our users.

141
00:06:39,750 --> 00:06:44,013
So to get that secure,
that foundational trust,

142
00:06:45,060 --> 00:06:47,940
we believe our platforms,

143
00:06:47,940 --> 00:06:50,520
our modern platform has to stand on

144
00:06:50,520 --> 00:06:52,923
seven foundational pillars.

145
00:06:54,390 --> 00:06:55,830
We are going to...

146
00:06:55,830 --> 00:06:58,680
All of these seven pillars are critical,

147
00:06:58,680 --> 00:07:01,860
but today we will be
focusing on our first pillar

148
00:07:01,860 --> 00:07:02,763
reliability.

149
00:07:03,690 --> 00:07:07,080
I'll take an example of
like if I'm a customer

150
00:07:07,080 --> 00:07:08,740
and trying to make a payment

151
00:07:09,600 --> 00:07:11,013
and my deadline is today,

152
00:07:12,450 --> 00:07:14,580
I use a Capital One app,

153
00:07:14,580 --> 00:07:17,520
my payment was not successful
on the first attempt.

154
00:07:17,520 --> 00:07:18,723
I retried it,

155
00:07:19,710 --> 00:07:22,413
it's still not gone through.

156
00:07:23,310 --> 00:07:24,143
As a customer,

157
00:07:24,143 --> 00:07:27,543
I'll look at my other
options to make that payment.

158
00:07:28,950 --> 00:07:30,300
So with that,

159
00:07:30,300 --> 00:07:31,890
I'll as a company,

160
00:07:31,890 --> 00:07:34,050
I'm losing my customer trust

161
00:07:34,050 --> 00:07:35,523
and also the business.

162
00:07:37,890 --> 00:07:40,140
So in this like journey,

163
00:07:40,140 --> 00:07:42,690
the availability of your platform

164
00:07:42,690 --> 00:07:45,483
and the resiliency are very important.

165
00:07:46,410 --> 00:07:49,380
So your availability
is going to be measured

166
00:07:49,380 --> 00:07:51,123
based upon your uptime.

167
00:07:52,290 --> 00:07:54,570
So uptime is a classic metrics

168
00:07:54,570 --> 00:07:56,010
that can help

169
00:07:56,010 --> 00:07:59,073
to define your service
is operational or not,

170
00:08:00,270 --> 00:08:02,370
and your success rate.

171
00:08:02,370 --> 00:08:05,730
Even though the ops
service is up and running,

172
00:08:05,730 --> 00:08:08,580
if the customer transactions are

173
00:08:08,580 --> 00:08:10,683
ending up within errors,

174
00:08:12,060 --> 00:08:14,823
that means they're not happy.

175
00:08:16,110 --> 00:08:20,100
So how can we measure the success rate?

176
00:08:20,100 --> 00:08:23,430
So we aggregate all the
requests that we receive

177
00:08:23,430 --> 00:08:25,230
as a platform

178
00:08:25,230 --> 00:08:29,130
and out of it how many
are actually errored out.

179
00:08:29,130 --> 00:08:30,840
So as an industry standard,

180
00:08:30,840 --> 00:08:33,513
we measure using those with nines.

181
00:08:34,860 --> 00:08:37,290
Resilience play a critical role.

182
00:08:37,290 --> 00:08:39,690
Even though we build, as you all know,

183
00:08:39,690 --> 00:08:42,000
we still face failures.

184
00:08:42,000 --> 00:08:44,280
While we are designing these platforms,

185
00:08:44,280 --> 00:08:46,200
how can we ensure

186
00:08:46,200 --> 00:08:50,370
these platforms can be
recovered from those

187
00:08:50,370 --> 00:08:51,510
failure points?

188
00:08:51,510 --> 00:08:53,103
Unexpected chaos.

189
00:08:54,780 --> 00:08:56,989
So the fall tolerance,

190
00:08:56,989 --> 00:09:00,010
when we design building a bridge

191
00:09:01,110 --> 00:09:04,710
with a structure redundant pillars,

192
00:09:04,710 --> 00:09:06,990
we always ensure if something goes wrong

193
00:09:06,990 --> 00:09:08,957
with one of those structure,

194
00:09:08,957 --> 00:09:11,730
the bridge is still up.

195
00:09:11,730 --> 00:09:14,760
Similarly, when we are
building a platform,

196
00:09:14,760 --> 00:09:16,680
we need to ensure

197
00:09:16,680 --> 00:09:19,350
if a component has failed,

198
00:09:19,350 --> 00:09:21,723
is our platform still serviceable?

199
00:09:23,190 --> 00:09:26,883
And we also measure our
meantime to recover.

200
00:09:27,810 --> 00:09:29,703
So when a failure happens,

201
00:09:30,900 --> 00:09:33,813
how quickly we are going
to identify the failure?

202
00:09:34,650 --> 00:09:37,410
How can we isolate from that failure?

203
00:09:37,410 --> 00:09:42,120
How quickly we can auto
resolve from that failure?

204
00:09:42,120 --> 00:09:43,623
These are very critical.

205
00:09:45,150 --> 00:09:46,113
Reliability.

206
00:09:47,610 --> 00:09:50,853
Reliability, I treat it
as a engine for trust.

207
00:09:51,870 --> 00:09:54,480
So along with availability

208
00:09:54,480 --> 00:09:56,760
and as well as resiliency,

209
00:09:56,760 --> 00:09:59,580
it is also very important

210
00:09:59,580 --> 00:10:03,120
the capabilities that are
provided by our platform

211
00:10:03,120 --> 00:10:05,520
are functioning as intended

212
00:10:05,520 --> 00:10:06,753
over a period of time.

213
00:10:07,620 --> 00:10:12,330
That is going to ensure the
trust with our platform,

214
00:10:12,330 --> 00:10:13,773
with our users.

215
00:10:15,150 --> 00:10:20,150
Our users, our customers are
going to provide their data,

216
00:10:20,640 --> 00:10:22,290
spend their time,

217
00:10:22,290 --> 00:10:23,913
rely on our systems.

218
00:10:24,840 --> 00:10:29,310
So with a lot of trust on these services,

219
00:10:29,310 --> 00:10:32,220
as a platform builders, owners,

220
00:10:32,220 --> 00:10:33,960
we need to ensure

221
00:10:33,960 --> 00:10:35,970
how we can make sure

222
00:10:35,970 --> 00:10:39,420
our capabilities are working as expected

223
00:10:39,420 --> 00:10:41,670
and also reliable,

224
00:10:41,670 --> 00:10:43,953
scalable, and trustworthy.

225
00:10:45,540 --> 00:10:48,960
To get more details on the how part,

226
00:10:48,960 --> 00:10:51,150
I'm going to hand it over to Arun

227
00:10:51,150 --> 00:10:54,900
who can get into the
details of our architectural

228
00:10:54,900 --> 00:10:57,150
practices and best practices

229
00:10:57,150 --> 00:10:59,553
that we adopted throughout this journey.

230
00:11:00,900 --> 00:11:01,960
- Thank you, Sobhan.

231
00:11:03,187 --> 00:11:05,127
Cool, nice.
(audience clapping)

232
00:11:05,127 --> 00:11:07,110
All the good things have to
have start like this, okay?

233
00:11:07,110 --> 00:11:09,600
Cool. So I have about 47 minutes,

234
00:11:09,600 --> 00:11:12,150
a lot of engineering content relating to

235
00:11:12,150 --> 00:11:13,530
reliability, engineering,

236
00:11:13,530 --> 00:11:15,900
and resilience engineering
on Capital One approach

237
00:11:15,900 --> 00:11:17,043
to accomplish those.

238
00:11:18,330 --> 00:11:20,880
So I'm going to dive deep.

239
00:11:20,880 --> 00:11:23,040
So there are two focus areas in my talk.

240
00:11:23,040 --> 00:11:24,360
So in my first focus area,

241
00:11:24,360 --> 00:11:27,000
I'm going to dive deep
into the architectural

242
00:11:27,000 --> 00:11:28,470
strategies that we follow

243
00:11:28,470 --> 00:11:31,140
and architectural pitfalls
we had in the past

244
00:11:31,140 --> 00:11:32,580
and our architectural journey.

245
00:11:32,580 --> 00:11:34,650
Basically, our architectural evolution

246
00:11:34,650 --> 00:11:37,620
towards the high
availability, high resiliency.

247
00:11:37,620 --> 00:11:39,720
And then, I will talk about
our serverless adoption,

248
00:11:39,720 --> 00:11:41,907
like why we are doing
serverless in Capital One

249
00:11:41,907 --> 00:11:43,860
and how it is improving

250
00:11:43,860 --> 00:11:46,920
the reliability aspects of
our system that we built.

251
00:11:46,920 --> 00:11:48,600
A lot of times, I tend to see,

252
00:11:48,600 --> 00:11:51,810
when we talk about
resiliency and reliability,

253
00:11:51,810 --> 00:11:53,700
we just stuck with the
architecture, right?

254
00:11:53,700 --> 00:11:56,130
So there are a lot of
non-architectural aspects

255
00:11:56,130 --> 00:11:58,380
that impacts your
reliability of the system.

256
00:11:58,380 --> 00:12:01,560
So I'm going to dive deep into
the different failure modes

257
00:12:01,560 --> 00:12:03,990
that could impact your platform services.

258
00:12:03,990 --> 00:12:05,490
And then, I will talk about

259
00:12:05,490 --> 00:12:06,870
the Zero DownTime Deployments

260
00:12:06,870 --> 00:12:08,735
using AWS CodeDeploy,

261
00:12:08,735 --> 00:12:10,080
CodeDeploy, excuse me.

262
00:12:10,080 --> 00:12:12,240
And then I will also use about,

263
00:12:12,240 --> 00:12:15,090
I will talk about the reliable
infrastructure building

264
00:12:15,090 --> 00:12:18,900
using the AWS CDKs,

265
00:12:18,900 --> 00:12:20,730
and then I will jump onto

266
00:12:20,730 --> 00:12:22,680
resiliency testing framework that we built

267
00:12:22,680 --> 00:12:24,780
using the AWS FIS,

268
00:12:24,780 --> 00:12:26,130
and then I will finally wrap it up

269
00:12:26,130 --> 00:12:27,780
with the observability standards.

270
00:12:29,670 --> 00:12:30,930
Resilient architecture, right?

271
00:12:30,930 --> 00:12:33,870
So architecture is the
foundational building blocks

272
00:12:33,870 --> 00:12:35,730
to build any system.

273
00:12:35,730 --> 00:12:37,350
So when I talk about architecture,

274
00:12:37,350 --> 00:12:39,210
I'm talking about the system architecture

275
00:12:39,210 --> 00:12:41,310
and as well as the
deployment architecture.

276
00:12:41,310 --> 00:12:43,260
So these are all the building blocks

277
00:12:43,260 --> 00:12:46,740
and resiliency and reliability
shouldn't be an afterthought.

278
00:12:46,740 --> 00:12:50,130
The moment we go build
something into the architecture,

279
00:12:50,130 --> 00:12:52,740
if we add more components
into the architecture,

280
00:12:52,740 --> 00:12:55,260
we need to bring the shift-left,

281
00:12:55,260 --> 00:12:56,550
thought processing about the reliability

282
00:12:56,550 --> 00:12:57,933
into the architecture.

283
00:12:58,980 --> 00:13:00,990
So let me start with
some of the anti-patterns

284
00:13:00,990 --> 00:13:02,280
that we had in the past

285
00:13:02,280 --> 00:13:03,990
in the pre-platform world

286
00:13:03,990 --> 00:13:04,950
because Sobhan talked about

287
00:13:04,950 --> 00:13:06,600
some of the non-standard architectures

288
00:13:06,600 --> 00:13:09,240
and the pitfalls that we had.

289
00:13:09,240 --> 00:13:10,380
Let's start with this.

290
00:13:10,380 --> 00:13:13,587
We call this as a "all-in-one approach".

291
00:13:14,640 --> 00:13:16,290
You can see a lot of
single point of failures

292
00:13:16,290 --> 00:13:17,123
in this approach, right?

293
00:13:17,123 --> 00:13:19,290
So we have all these platform capabilities

294
00:13:19,290 --> 00:13:21,330
packaged into a single monolith.

295
00:13:21,330 --> 00:13:23,700
It deploys into a single AWS region

296
00:13:23,700 --> 00:13:25,530
with a single availability zone

297
00:13:25,530 --> 00:13:26,820
with no autoscaling

298
00:13:26,820 --> 00:13:30,690
and no database replication, right?

299
00:13:30,690 --> 00:13:33,750
This pattern is a big no in Capital One.

300
00:13:33,750 --> 00:13:36,480
From the day one of Capital
One cloud migration journey,

301
00:13:36,480 --> 00:13:37,710
we made a clear decision

302
00:13:37,710 --> 00:13:40,080
that we are not going to
deploy in a single region

303
00:13:40,080 --> 00:13:42,240
or with many single point of failures.

304
00:13:42,240 --> 00:13:46,590
So we had always deployed our
services into two regions.

305
00:13:46,590 --> 00:13:47,423
Then you might be wondering

306
00:13:47,423 --> 00:13:49,650
why I'm talking about this now, right?

307
00:13:49,650 --> 00:13:54,650
We had a recent US East one
outage in AWS in October.

308
00:13:54,660 --> 00:13:57,240
All of our systems were able
to quickly recover from it

309
00:13:57,240 --> 00:13:59,340
by auto failover to the other region.

310
00:13:59,340 --> 00:14:01,530
But I have seen like some of the systems

311
00:14:01,530 --> 00:14:02,730
outside of Capital One

312
00:14:02,730 --> 00:14:05,010
had struggled during this outage, right?

313
00:14:05,010 --> 00:14:07,140
Because they were heavily
reliant on a single region

314
00:14:07,140 --> 00:14:10,710
with no other region to run their systems.

315
00:14:10,710 --> 00:14:11,790
This is a big no.

316
00:14:11,790 --> 00:14:13,080
If in 2026,

317
00:14:13,080 --> 00:14:15,930
if you're still thinking about
having a multiple regions,

318
00:14:15,930 --> 00:14:17,260
you are in this room because

319
00:14:17,260 --> 00:14:19,350
you think that resiliency is important.

320
00:14:19,350 --> 00:14:20,580
You cannot take your car

321
00:14:20,580 --> 00:14:23,070
to a road without an insurance, right?

322
00:14:23,070 --> 00:14:23,903
The same concept.

323
00:14:23,903 --> 00:14:25,680
You have to have multiple regions

324
00:14:25,680 --> 00:14:27,993
for higher resiliency
and higher reliability.

325
00:14:29,670 --> 00:14:31,470
So our wide single point of failures

326
00:14:32,670 --> 00:14:35,970
is one architectural bad pattern.

327
00:14:35,970 --> 00:14:38,850
The next one that we have seen in our

328
00:14:38,850 --> 00:14:40,410
pre-platform paradigm

329
00:14:40,410 --> 00:14:41,580
is having these

330
00:14:41,580 --> 00:14:46,110
multiple proliferation of microservices

331
00:14:46,110 --> 00:14:47,700
with zero fault tolerance.

332
00:14:47,700 --> 00:14:50,010
So the moment we talked about

333
00:14:50,010 --> 00:14:53,790
monolith move to a
microservices based approach,

334
00:14:53,790 --> 00:14:55,740
we tend to create a lot of microservices

335
00:14:55,740 --> 00:14:57,720
with a tightly coupled nature.

336
00:14:57,720 --> 00:14:59,370
So if one service goes down,

337
00:14:59,370 --> 00:15:01,200
it impacts the all the other services.

338
00:15:01,200 --> 00:15:03,360
So when we think about that microservices,

339
00:15:03,360 --> 00:15:04,770
think about in a way that

340
00:15:04,770 --> 00:15:07,380
every single service should be able to

341
00:15:07,380 --> 00:15:08,640
service capability

342
00:15:08,640 --> 00:15:10,680
and avoid cascading failures.

343
00:15:10,680 --> 00:15:11,700
So this example,

344
00:15:11,700 --> 00:15:13,770
if one service goes down here,

345
00:15:13,770 --> 00:15:14,910
the customer is impacted

346
00:15:14,910 --> 00:15:17,130
because all these services
are tightly coupled

347
00:15:17,130 --> 00:15:19,980
in order to provide a
capability to the customer.

348
00:15:19,980 --> 00:15:22,530
So this is an architectural anti-pattern

349
00:15:22,530 --> 00:15:24,360
for high resiliency,

350
00:15:24,360 --> 00:15:25,650
for a low resiliency.

351
00:15:25,650 --> 00:15:28,540
And then, we talk about another deployment

352
00:15:30,390 --> 00:15:31,920
pattern we had was,

353
00:15:31,920 --> 00:15:33,423
in the pre-platform paradigm,

354
00:15:34,350 --> 00:15:36,390
this is the deployment anti-pattern

355
00:15:36,390 --> 00:15:38,550
where we had a single cluster

356
00:15:38,550 --> 00:15:41,220
that was sharing the
same traffic patterns.

357
00:15:41,220 --> 00:15:43,530
So we had web and mobile customers

358
00:15:43,530 --> 00:15:45,510
using the realtime clusters

359
00:15:45,510 --> 00:15:46,860
and also at the same time,

360
00:15:46,860 --> 00:15:49,740
the batch load was hitting
our real time clusters.

361
00:15:49,740 --> 00:15:52,530
So this caused pressure into our systems,

362
00:15:52,530 --> 00:15:54,030
a lot of database timeouts,

363
00:15:54,030 --> 00:15:56,790
and it impacted the real
time users experience.

364
00:15:56,790 --> 00:15:59,700
So with this you cannot
reach a high availability.

365
00:15:59,700 --> 00:16:01,860
So this is an anti-pattern.

366
00:16:01,860 --> 00:16:03,780
All the system that we are building

367
00:16:03,780 --> 00:16:05,400
in the modern platform world,

368
00:16:05,400 --> 00:16:07,920
we have routed all the batch traffic

369
00:16:07,920 --> 00:16:09,210
into a separate cluster

370
00:16:09,210 --> 00:16:10,620
by using step functions

371
00:16:10,620 --> 00:16:14,163
or other vendor products
for a batch execution model.

372
00:16:16,410 --> 00:16:18,990
The other one that we had
in the pre-platform paradigm

373
00:16:18,990 --> 00:16:20,880
was having the single database

374
00:16:20,880 --> 00:16:21,930
that is used to share

375
00:16:21,930 --> 00:16:23,850
between the analytical loads

376
00:16:23,850 --> 00:16:25,380
on the real time customer loads.

377
00:16:25,380 --> 00:16:27,180
So whenever the analytical job runs,

378
00:16:27,180 --> 00:16:29,490
it goes and hacks the system resources.

379
00:16:29,490 --> 00:16:31,260
When it hacks the system resources,

380
00:16:31,260 --> 00:16:34,080
we started to getting a lot of
database timeouts on throttle

381
00:16:34,080 --> 00:16:36,330
and it impacted our user experience.

382
00:16:36,330 --> 00:16:37,980
So this is another anti-pattern

383
00:16:37,980 --> 00:16:40,263
that existed in our pre-platform world.

384
00:16:41,460 --> 00:16:42,293
Cool.

385
00:16:42,293 --> 00:16:44,280
So I talked about most
of the anti-patterns

386
00:16:44,280 --> 00:16:46,110
that impacted our high reliability.

387
00:16:46,110 --> 00:16:48,120
Let's talk about some of
the patterns that we follow

388
00:16:48,120 --> 00:16:49,170
and what are the

389
00:16:49,170 --> 00:16:51,423
enterprise architectural
standards we follow.

390
00:16:52,560 --> 00:16:54,330
When it comes to deployment,

391
00:16:54,330 --> 00:16:57,180
we all the time deploy into two regions.

392
00:16:57,180 --> 00:16:59,130
All of our CICD pipeline

393
00:16:59,130 --> 00:17:01,350
will deploy the same version of software

394
00:17:01,350 --> 00:17:03,360
into the two different regions

395
00:17:03,360 --> 00:17:05,790
and all of our services are scaled out

396
00:17:05,790 --> 00:17:07,380
so that they are anytime

397
00:17:07,380 --> 00:17:10,740
capable of taking the load in one region.

398
00:17:10,740 --> 00:17:14,073
And when it comes to the
architectural dependency,

399
00:17:14,940 --> 00:17:18,060
no single region should
have a tight coupling

400
00:17:18,060 --> 00:17:20,280
or any dependency with the cross region.

401
00:17:20,280 --> 00:17:23,370
So all these services should
have the regional affinity.

402
00:17:23,370 --> 00:17:26,340
Think of this when you failover

403
00:17:26,340 --> 00:17:27,600
to one region to the other region

404
00:17:27,600 --> 00:17:29,370
because your region one is impaired,

405
00:17:29,370 --> 00:17:32,280
but if you still have a dependency
with the impaired region,

406
00:17:32,280 --> 00:17:34,350
you are not solving the problem.

407
00:17:34,350 --> 00:17:37,200
So you're still defeating the
purpose of having two regions.

408
00:17:37,200 --> 00:17:38,670
So our guidance for you

409
00:17:38,670 --> 00:17:40,290
is all these region services

410
00:17:40,290 --> 00:17:41,550
should have the regional affinity

411
00:17:41,550 --> 00:17:43,173
and should operate in a silo.

412
00:17:44,790 --> 00:17:46,650
If you have any other dependencies,

413
00:17:46,650 --> 00:17:47,483
in our systems,

414
00:17:47,483 --> 00:17:49,020
we have other platform dependencies

415
00:17:49,020 --> 00:17:50,490
within Capital One systems.

416
00:17:50,490 --> 00:17:53,670
So we maintain the same
level of resiliency standards

417
00:17:53,670 --> 00:17:55,110
across all of our systems.

418
00:17:55,110 --> 00:17:58,230
So if I have a top-tier
resiliency standards,

419
00:17:58,230 --> 00:17:59,130
all my dependencies

420
00:17:59,130 --> 00:18:01,770
should also follow the
same resiliency standards.

421
00:18:01,770 --> 00:18:04,110
And when it comes to data consistency,

422
00:18:04,110 --> 00:18:05,520
we always tend to choose

423
00:18:05,520 --> 00:18:08,283
the databases that are auto replicable.

424
00:18:09,630 --> 00:18:12,960
When it comes to failure
recovery management,

425
00:18:12,960 --> 00:18:15,180
any single region failure

426
00:18:15,180 --> 00:18:18,570
should not cause impact to
our service capabilities

427
00:18:18,570 --> 00:18:20,790
and all of our services should be equipped

428
00:18:20,790 --> 00:18:22,230
with auto failover

429
00:18:22,230 --> 00:18:23,687
and all of our platform services

430
00:18:23,687 --> 00:18:27,240
have a very strict RTO and RPO defined.

431
00:18:27,240 --> 00:18:28,343
So the bottom line here is,

432
00:18:28,343 --> 00:18:31,260
when we are aiming for building
a high reliable system,

433
00:18:31,260 --> 00:18:32,430
we need to clearly define

434
00:18:32,430 --> 00:18:34,410
what are the goals we
are trying to achieve?

435
00:18:34,410 --> 00:18:35,280
Without goals,

436
00:18:35,280 --> 00:18:39,273
you cannot track your resiliency
or reliability aspirations.

437
00:18:40,830 --> 00:18:42,990
So let's move on to the
architectural patterns.

438
00:18:42,990 --> 00:18:45,330
Putting the words into a diagram,

439
00:18:45,330 --> 00:18:46,710
you can see

440
00:18:46,710 --> 00:18:48,860
we follow the domain-driven
design pattern.

441
00:18:50,160 --> 00:18:51,390
All of our modern platforms

442
00:18:51,390 --> 00:18:53,010
follow domain-driven design pattern

443
00:18:53,010 --> 00:18:55,690
because domain-driven design gives us

444
00:18:56,820 --> 00:18:58,110
high flexibility.

445
00:18:58,110 --> 00:18:59,940
This creates the modular services

446
00:18:59,940 --> 00:19:02,283
which are highly decoupled
and fault tolerant.

447
00:19:04,080 --> 00:19:06,060
To think of this in this way,

448
00:19:06,060 --> 00:19:08,310
think of this bigger domain

449
00:19:08,310 --> 00:19:12,600
and divide this bigger domain
into multiple sub domains.

450
00:19:12,600 --> 00:19:15,480
These sub domains have their own entities,

451
00:19:15,480 --> 00:19:16,470
aggregates,

452
00:19:16,470 --> 00:19:18,870
and their own database and layers

453
00:19:18,870 --> 00:19:21,810
and they heavily operate
on a bounded context.

454
00:19:21,810 --> 00:19:24,330
Now, think this into a platform model.

455
00:19:24,330 --> 00:19:25,500
You have a platform,

456
00:19:25,500 --> 00:19:27,720
this platform has multiple capabilities.

457
00:19:27,720 --> 00:19:30,480
Each capability has its one entities,

458
00:19:30,480 --> 00:19:32,820
aggregate models, and value objects

459
00:19:32,820 --> 00:19:34,950
and they do not depend on each other.

460
00:19:34,950 --> 00:19:38,100
So every capability runs
in a single bounded context

461
00:19:38,100 --> 00:19:40,413
with no dependency with
the other capability.

462
00:19:41,820 --> 00:19:43,194
With the banking example,

463
00:19:43,194 --> 00:19:46,620
this banking domain as a larger domain,

464
00:19:46,620 --> 00:19:50,040
you can divide this multiple
banking domain into sub domains

465
00:19:50,040 --> 00:19:52,080
like a transaction processing service,

466
00:19:52,080 --> 00:19:53,070
a reporting service,

467
00:19:53,070 --> 00:19:55,230
a customer management and account service.

468
00:19:55,230 --> 00:19:57,120
The bottom line here is these

469
00:19:57,120 --> 00:19:59,760
sub-domains are operated independently

470
00:19:59,760 --> 00:20:03,510
and every capability has its own SLAs.

471
00:20:03,510 --> 00:20:06,390
You are not targeting all these services

472
00:20:06,390 --> 00:20:08,490
to be five nine available or
three nine available, right?

473
00:20:08,490 --> 00:20:12,300
So you can set different uptime
SLAs and response time SLAs

474
00:20:12,300 --> 00:20:14,913
and domain-driven design is best for this.

475
00:20:16,770 --> 00:20:18,873
What are the other benefits that we have?

476
00:20:20,370 --> 00:20:22,740
These domain-driven designs are modular,

477
00:20:22,740 --> 00:20:24,750
which means those services
are easy to maintain,

478
00:20:24,750 --> 00:20:26,320
repair, and replace

479
00:20:27,690 --> 00:20:30,780
and they operate in a bounded context

480
00:20:30,780 --> 00:20:33,390
in a highly decoupled
manner, fault tolerant

481
00:20:33,390 --> 00:20:35,610
and they're independently scalable.

482
00:20:35,610 --> 00:20:38,040
And these service capabilities

483
00:20:38,040 --> 00:20:41,160
gives you the SLA flexibility.

484
00:20:41,160 --> 00:20:42,120
And not only that,

485
00:20:42,120 --> 00:20:43,090
our customers

486
00:20:44,130 --> 00:20:47,310
now pick and choose the capability
they want to connect to.

487
00:20:47,310 --> 00:20:48,210
For example,

488
00:20:48,210 --> 00:20:51,330
our finance will use a reporting module

489
00:20:51,330 --> 00:20:54,360
whereas our card and bank will use

490
00:20:54,360 --> 00:20:56,430
capabilities like transaction processing.

491
00:20:56,430 --> 00:20:59,070
So this offers the extreme flexibility.

492
00:20:59,070 --> 00:21:01,710
So now putting these words into a diagram.

493
00:21:01,710 --> 00:21:04,740
So this is the minimum requirement
to deploy in Capital One.

494
00:21:04,740 --> 00:21:06,270
Two regions,

495
00:21:06,270 --> 00:21:08,490
multiple availability zone,

496
00:21:08,490 --> 00:21:11,640
domain driven design with
bounded contact services,

497
00:21:11,640 --> 00:21:13,830
autoscaling enabled,

498
00:21:13,830 --> 00:21:17,940
and you have a database that's
auto replicate the data.

499
00:21:17,940 --> 00:21:21,120
And then we have this R53 record sets

500
00:21:21,120 --> 00:21:22,410
that has two types, right?

501
00:21:22,410 --> 00:21:24,330
One, we have a geo based location.

502
00:21:24,330 --> 00:21:26,670
So every request that
comes from the customer

503
00:21:26,670 --> 00:21:29,100
goes to the nearest data
center of the customer.

504
00:21:29,100 --> 00:21:31,620
And after that we also have
the failover record set.

505
00:21:31,620 --> 00:21:33,090
This failover record set

506
00:21:33,090 --> 00:21:35,670
is the mechanism for us
to do the auto failover.

507
00:21:35,670 --> 00:21:38,880
This failover record set has two routes.

508
00:21:38,880 --> 00:21:41,820
The primary route goes to the nearest

509
00:21:41,820 --> 00:21:42,990
region of the customer.

510
00:21:42,990 --> 00:21:44,740
The secondary route always

511
00:21:45,990 --> 00:21:47,940
take care of the failover mechanisms.

512
00:21:47,940 --> 00:21:51,450
It keeps an eye on the
other region's health.

513
00:21:51,450 --> 00:21:53,250
And the moment we find out

514
00:21:53,250 --> 00:21:56,280
that your primary region
is faulty, impaired,

515
00:21:56,280 --> 00:21:58,980
this auto failover will
kick in using the R53

516
00:21:58,980 --> 00:22:00,480
auto failover record set.

517
00:22:00,480 --> 00:22:02,880
All of that is done automatically.

518
00:22:02,880 --> 00:22:04,590
So this is the minimum deployment

519
00:22:04,590 --> 00:22:06,720
architecture requirement for us,

520
00:22:06,720 --> 00:22:09,090
but this is giving us
guaranteed three nine

521
00:22:09,090 --> 00:22:12,030
availability up to four nine availability.

522
00:22:12,030 --> 00:22:14,040
But there are some mission
critical applications

523
00:22:14,040 --> 00:22:15,870
that requires five nine availability

524
00:22:15,870 --> 00:22:18,110
and let's look into what
are the challenges we have

525
00:22:18,110 --> 00:22:19,563
in this architecture pattern.

526
00:22:22,320 --> 00:22:25,590
So now, imagine you have 10 customers

527
00:22:25,590 --> 00:22:27,180
and you have two regions

528
00:22:27,180 --> 00:22:28,860
and you have these
bounded contact services

529
00:22:28,860 --> 00:22:30,810
and all the good things I talked about.

530
00:22:34,530 --> 00:22:37,230
Let me introduce something
called poison pill request.

531
00:22:37,230 --> 00:22:40,023
Poison pill requests are type of requests

532
00:22:40,023 --> 00:22:43,890
that are capable of killing
your services every single time

533
00:22:43,890 --> 00:22:45,510
they get processed, right?

534
00:22:45,510 --> 00:22:47,340
It could cause a memory outage

535
00:22:47,340 --> 00:22:50,070
or it could cause some
saturation into your resources

536
00:22:50,070 --> 00:22:53,400
or it causes high database
timeouts or retry storms.

537
00:22:53,400 --> 00:22:54,510
But the bottom line here is

538
00:22:54,510 --> 00:22:57,690
every single time it causes an outage

539
00:22:57,690 --> 00:22:58,743
very consistently.

540
00:22:59,910 --> 00:23:01,800
Now, think this poison pill

541
00:23:01,800 --> 00:23:04,230
and apply into our architecture.

542
00:23:04,230 --> 00:23:07,080
Customer C2 is sending
a poison pill request

543
00:23:07,080 --> 00:23:08,670
and we have the router

544
00:23:08,670 --> 00:23:11,400
that sends the request to the region one,

545
00:23:11,400 --> 00:23:14,040
the region will have multiple task.

546
00:23:14,040 --> 00:23:18,660
So now this possible request
goes to the service task one,

547
00:23:18,660 --> 00:23:20,580
the service task one will crash.

548
00:23:20,580 --> 00:23:23,310
We have a ALB in front
of the service tasks.

549
00:23:23,310 --> 00:23:26,010
So the ALB operates in
a round robin manner.

550
00:23:26,010 --> 00:23:27,990
Every time when a
customer sends a request,

551
00:23:27,990 --> 00:23:29,820
it goes to the service task one crashes,

552
00:23:29,820 --> 00:23:30,960
service task two crashes,

553
00:23:30,960 --> 00:23:32,640
service three crashes.

554
00:23:32,640 --> 00:23:33,870
Now, guess what?

555
00:23:33,870 --> 00:23:35,910
Now we have our auto failover will kick in

556
00:23:35,910 --> 00:23:38,040
because our auto failover will think

557
00:23:38,040 --> 00:23:39,300
the region one is impaired

558
00:23:39,300 --> 00:23:41,730
because all the services
are getting crashed.

559
00:23:41,730 --> 00:23:44,310
We have our alerts based on
the unhealthy host count.

560
00:23:44,310 --> 00:23:45,900
on HTTP error codes.

561
00:23:45,900 --> 00:23:49,560
So now, the R53 will think
that our region is down

562
00:23:49,560 --> 00:23:52,920
so it'll automatically flip
the traffic to the region two.

563
00:23:52,920 --> 00:23:54,240
But now, what will happen?

564
00:23:54,240 --> 00:23:57,030
The customer is still sending
the poison pill request.

565
00:23:57,030 --> 00:23:59,580
Now, the request will
go to the region two.

566
00:23:59,580 --> 00:24:00,420
In the region two,

567
00:24:00,420 --> 00:24:03,240
it'll start killing the
task one after the other.

568
00:24:03,240 --> 00:24:06,510
So now, it is more bad than
the previous situation, right?

569
00:24:06,510 --> 00:24:08,430
So you are not only
impacting the region one,

570
00:24:08,430 --> 00:24:10,770
you are also now impacting the region two.

571
00:24:10,770 --> 00:24:13,380
Now, how are we solving this problem?

572
00:24:13,380 --> 00:24:16,140
We are creating a circuit breaker.

573
00:24:16,140 --> 00:24:18,120
Typically, with the
circuit breaker is used

574
00:24:18,120 --> 00:24:19,470
in the microservices world

575
00:24:19,470 --> 00:24:20,880
when we have a dependency,

576
00:24:20,880 --> 00:24:22,020
the dependency is struggling.

577
00:24:22,020 --> 00:24:24,420
We do not want to overwhelm the dependency

578
00:24:24,420 --> 00:24:25,770
so we'll wait for a minute.

579
00:24:26,880 --> 00:24:27,960
We are shifting left

580
00:24:27,960 --> 00:24:30,420
that circuit breaker
capability into our router.

581
00:24:30,420 --> 00:24:33,540
Our circuit breaker has
a customer level metrics.

582
00:24:33,540 --> 00:24:37,260
So our circuit breakers are enabled

583
00:24:37,260 --> 00:24:40,440
with observability that
tracks poison pill request

584
00:24:40,440 --> 00:24:41,550
from a customer level.

585
00:24:41,550 --> 00:24:43,770
The moment we find out that customer is

586
00:24:43,770 --> 00:24:45,480
sending a poison pill request,

587
00:24:45,480 --> 00:24:46,313
we'll open the circuit

588
00:24:46,313 --> 00:24:48,930
and we will stop taking the request.

589
00:24:48,930 --> 00:24:51,810
We also have this router
enabled with the rate limiters

590
00:24:51,810 --> 00:24:54,630
because sometimes our customer
used to overwhelm the systems

591
00:24:54,630 --> 00:24:56,223
and cause reliability issues.

592
00:24:57,150 --> 00:24:59,610
Even though the we have
these two capabilities,

593
00:24:59,610 --> 00:25:02,730
these are more reactive
capabilities, right?

594
00:25:02,730 --> 00:25:04,620
Because when we talk about five nines,

595
00:25:04,620 --> 00:25:06,510
your budget for error

596
00:25:06,510 --> 00:25:09,870
or downtime in a year is five minutes.

597
00:25:09,870 --> 00:25:10,890
When these things happen,

598
00:25:10,890 --> 00:25:15,180
you already breach that
reliability aspirations.

599
00:25:15,180 --> 00:25:17,160
So your reliability goals is down.

600
00:25:17,160 --> 00:25:18,900
This is reactive system.

601
00:25:18,900 --> 00:25:20,000
What are we doing now?

602
00:25:21,240 --> 00:25:22,073
Think of this.

603
00:25:22,073 --> 00:25:24,453
So we have been designing
sharding techniques.

604
00:25:25,320 --> 00:25:26,940
We went with this design one

605
00:25:26,940 --> 00:25:30,180
using the standard sharding pattern

606
00:25:30,180 --> 00:25:32,370
where we have these three shards,

607
00:25:32,370 --> 00:25:33,450
three customer groups,

608
00:25:33,450 --> 00:25:35,580
and we have a consistent hashing algorithm

609
00:25:35,580 --> 00:25:38,670
that always creates a
stickiness session, right?

610
00:25:38,670 --> 00:25:41,730
So customer group one always
get attached to the shard one,

611
00:25:41,730 --> 00:25:43,380
group two goes to shard two,

612
00:25:43,380 --> 00:25:45,180
group three goes to the shard three.

613
00:25:46,800 --> 00:25:51,330
Now, imagine customer C6 is
sending the poison pill request,

614
00:25:51,330 --> 00:25:52,800
it goes to the shard one

615
00:25:52,800 --> 00:25:55,230
and then it completely
takes down the shard one

616
00:25:55,230 --> 00:25:58,080
but your other customer
groups are not impacted

617
00:25:58,080 --> 00:26:00,630
because now your request
is going to the shard one.

618
00:26:00,630 --> 00:26:01,980
Is this good enough?

619
00:26:01,980 --> 00:26:03,090
This is not good enough for us

620
00:26:03,090 --> 00:26:06,150
because now imagine that we did
not have the circuit breaker

621
00:26:06,150 --> 00:26:09,420
or we did not have this rate limiters

622
00:26:09,420 --> 00:26:10,350
and things like that.

623
00:26:10,350 --> 00:26:12,690
We used to have the blast
radius of a hundred percent.

624
00:26:12,690 --> 00:26:15,000
Now, after this shard technique,

625
00:26:15,000 --> 00:26:17,400
now our blast series is reduced to 33%.

626
00:26:17,400 --> 00:26:19,020
Now, out of 10 customers,

627
00:26:19,020 --> 00:26:20,460
you are only impacting three customers

628
00:26:20,460 --> 00:26:23,640
because the other customers
are having the healthy shards.

629
00:26:23,640 --> 00:26:26,580
Is this good enough for us to
build a five nine services?

630
00:26:26,580 --> 00:26:27,600
Definitely not.

631
00:26:27,600 --> 00:26:28,950
So where we are heading towards

632
00:26:28,950 --> 00:26:31,263
is the shuffle sharding patterns.

633
00:26:32,220 --> 00:26:34,080
I'm pretty sure

634
00:26:34,080 --> 00:26:37,980
everyone in this room would've
played playing cards, right?

635
00:26:37,980 --> 00:26:40,140
So you have the deck of playing cards

636
00:26:40,140 --> 00:26:41,340
and then you shuffle the cards

637
00:26:41,340 --> 00:26:42,750
and you'll have multiple players

638
00:26:42,750 --> 00:26:44,550
and then you'll create the combinations

639
00:26:44,550 --> 00:26:46,500
and then you will deal five, seven,

640
00:26:46,500 --> 00:26:48,690
or three cards according to the game

641
00:26:48,690 --> 00:26:51,450
and then you will try to
create the combinations.

642
00:26:51,450 --> 00:26:54,240
The same concept is applied here.

643
00:26:54,240 --> 00:26:56,190
So imagine you have this deck of cards

644
00:26:56,190 --> 00:26:57,420
as the deck of shards.

645
00:26:57,420 --> 00:26:59,070
So you have now multiple shards.

646
00:26:59,070 --> 00:27:00,120
Now you have multiple customers.

647
00:27:00,120 --> 00:27:02,520
So you create the multiple
combination of shards

648
00:27:02,520 --> 00:27:03,353
and you will,

649
00:27:03,353 --> 00:27:05,010
every time you onboard a customer,

650
00:27:05,010 --> 00:27:06,600
you'll create a unique combination

651
00:27:06,600 --> 00:27:10,050
and you will attach a
particular shard to a customer.

652
00:27:10,050 --> 00:27:11,040
So in this diagram

653
00:27:11,040 --> 00:27:13,200
you can take a close look
into the customer one

654
00:27:13,200 --> 00:27:15,240
and customer three.

655
00:27:15,240 --> 00:27:17,700
They're having, in the middle block,

656
00:27:17,700 --> 00:27:19,863
they're having this same shard, shard 30.

657
00:27:20,790 --> 00:27:24,330
Now, think of this poison pill situation.

658
00:27:24,330 --> 00:27:27,000
Customer one is sending
a poison pill request,

659
00:27:27,000 --> 00:27:28,770
it'll only impact

660
00:27:28,770 --> 00:27:30,690
the shard that he belongs to shard one,

661
00:27:30,690 --> 00:27:32,520
shard 30, and shard 70.

662
00:27:32,520 --> 00:27:35,340
Even though customer three
shard 30 is impacted,

663
00:27:35,340 --> 00:27:39,210
the customer three has
other two healthy shards.

664
00:27:39,210 --> 00:27:41,400
So either the customer retries

665
00:27:41,400 --> 00:27:43,380
or internally when we retry,

666
00:27:43,380 --> 00:27:45,900
the customer's three's request
will go to the shard seven

667
00:27:45,900 --> 00:27:47,750
or shard 99.

668
00:27:47,750 --> 00:27:48,990
At the same time,

669
00:27:48,990 --> 00:27:51,150
if the shard 30 had a problem,

670
00:27:51,150 --> 00:27:54,300
if there is a database
problem or a shard problem,

671
00:27:54,300 --> 00:27:55,920
the customer one or customer three,

672
00:27:55,920 --> 00:27:57,630
either of them or not impacted

673
00:27:57,630 --> 00:28:00,660
because they have the other
healthy shards to use.

674
00:28:00,660 --> 00:28:01,493
This is the pattern

675
00:28:01,493 --> 00:28:03,720
that has been proven to
be very useful for us

676
00:28:03,720 --> 00:28:06,210
to achieve the five nines.

677
00:28:06,210 --> 00:28:08,760
How do we create this
multiple shard combinations?

678
00:28:08,760 --> 00:28:10,800
So that is this mathematical formula,

679
00:28:10,800 --> 00:28:12,900
binomial coefficient.

680
00:28:12,900 --> 00:28:14,970
You key in these different values.

681
00:28:14,970 --> 00:28:17,190
In my examples, I have
these hundred shards,

682
00:28:17,190 --> 00:28:18,540
a million customer

683
00:28:18,540 --> 00:28:21,540
and I'm trying to assign
three shards per customer.

684
00:28:21,540 --> 00:28:24,090
So with this mathematical formula

685
00:28:24,090 --> 00:28:29,090
I'm getting 161,700 unique combinations.

686
00:28:29,640 --> 00:28:32,220
These unique combinations of shards

687
00:28:32,220 --> 00:28:35,100
will be split across
the different customers,

688
00:28:35,100 --> 00:28:37,290
in this case a million customer.

689
00:28:37,290 --> 00:28:39,810
So the success of this model will be

690
00:28:39,810 --> 00:28:42,750
depends on how do you
reduce the overlapping.

691
00:28:42,750 --> 00:28:44,550
So with this formula,

692
00:28:44,550 --> 00:28:45,660
at any given point of time,

693
00:28:45,660 --> 00:28:48,510
seven customers will have the
same combination of shards.

694
00:28:49,350 --> 00:28:50,460
So now, think of this

695
00:28:50,460 --> 00:28:52,050
from the a hundred percent blast radius

696
00:28:52,050 --> 00:28:54,090
to the 33% of blast radius.

697
00:28:54,090 --> 00:28:58,170
We have come down to 0.0007
percentage of blast radius

698
00:28:58,170 --> 00:29:00,060
because of this shuffle
sharding technique.

699
00:29:00,060 --> 00:29:02,580
So this has been proven
to be very helpful for us

700
00:29:02,580 --> 00:29:03,900
to reach the five nine

701
00:29:03,900 --> 00:29:06,333
for some of the mission
critical applications.

702
00:29:08,310 --> 00:29:11,070
Where do we put this logic?

703
00:29:11,070 --> 00:29:12,990
So this is a sample python code

704
00:29:12,990 --> 00:29:15,753
to create the different
shard combinations.

705
00:29:16,770 --> 00:29:18,960
So you key in your number of customers,

706
00:29:18,960 --> 00:29:20,550
the number of nodes you want

707
00:29:20,550 --> 00:29:22,770
and the shard per customers you want.

708
00:29:22,770 --> 00:29:24,360
Then this code will output

709
00:29:24,360 --> 00:29:26,130
the number of different shard combinations

710
00:29:26,130 --> 00:29:26,963
that you want to create.

711
00:29:26,963 --> 00:29:27,796
In this example,

712
00:29:27,796 --> 00:29:30,627
you can see customer C1
is having S2, S4, and S5,

713
00:29:30,627 --> 00:29:35,627
and customer 100 is having S2, S4, and S5.

714
00:29:36,630 --> 00:29:38,700
Where does this logic sits?

715
00:29:38,700 --> 00:29:40,710
So we have this router.

716
00:29:40,710 --> 00:29:42,660
This router is where this logic sits.

717
00:29:42,660 --> 00:29:44,940
So every single time when
a customer is onboarded,

718
00:29:44,940 --> 00:29:46,950
this router will create the shuffle,

719
00:29:46,950 --> 00:29:49,440
the router will create the
combinations of the shuffle,

720
00:29:49,440 --> 00:29:51,840
and it'll attach it to the customer

721
00:29:51,840 --> 00:29:53,070
with the durable storage.

722
00:29:53,070 --> 00:29:54,870
So we have the database
attached to the router.

723
00:29:54,870 --> 00:29:56,880
So the router will create this combination

724
00:29:56,880 --> 00:29:58,260
and will create the stickiness

725
00:29:58,260 --> 00:30:00,660
with the customer and
the number of shards.

726
00:30:00,660 --> 00:30:03,210
And also before the shards,

727
00:30:03,210 --> 00:30:04,980
we also have the load balancer.

728
00:30:04,980 --> 00:30:06,960
The load balancer by nature, by default,

729
00:30:06,960 --> 00:30:08,700
is a round robin algorithm.

730
00:30:08,700 --> 00:30:11,310
So every single time a
customer sends a request,

731
00:30:11,310 --> 00:30:12,143
in this example,

732
00:30:12,143 --> 00:30:14,610
you can take a close
look into customer C1.

733
00:30:14,610 --> 00:30:17,130
Imagine C1 is sending a request, at first,

734
00:30:17,130 --> 00:30:19,243
with he has a shard S1, S2, and S6.

735
00:30:20,310 --> 00:30:21,560
The first time the router will know,

736
00:30:21,560 --> 00:30:24,360
oh, this is the first request
the customer is sending

737
00:30:24,360 --> 00:30:27,210
and the shard he belongs
to is S1, S2, and S6

738
00:30:27,210 --> 00:30:30,000
and I have the round robin algorithm

739
00:30:30,000 --> 00:30:31,020
built within me.

740
00:30:31,020 --> 00:30:33,000
So it'll set the header as S1.

741
00:30:33,000 --> 00:30:36,690
So now, this header will
send it to the load balancer.

742
00:30:36,690 --> 00:30:39,060
This load balancer based
on the header based routing

743
00:30:39,060 --> 00:30:40,383
will send it to the shard.

744
00:30:41,760 --> 00:30:43,770
And the second time when the
customer sent the request,

745
00:30:43,770 --> 00:30:46,935
the router will attach the header as S2

746
00:30:46,935 --> 00:30:48,305
and the request will go to the S2,

747
00:30:48,305 --> 00:30:49,650
on the third time, S6.

748
00:30:49,650 --> 00:30:52,230
It goes to the S6 and so on and so forth.

749
00:30:52,230 --> 00:30:53,490
When you think of shard,

750
00:30:53,490 --> 00:30:56,460
you can think the shard as target groups

751
00:30:56,460 --> 00:30:58,020
in ECS tasks, right?

752
00:30:58,020 --> 00:31:00,000
Every shard that I shown

753
00:31:00,000 --> 00:31:02,160
is a ECS target group.

754
00:31:02,160 --> 00:31:05,163
So they can be independently
scaled within that shard.

755
00:31:06,600 --> 00:31:09,180
There was another way of achieving this.

756
00:31:09,180 --> 00:31:10,950
We explored this way of achieving this

757
00:31:10,950 --> 00:31:12,570
by shift-left the skip

758
00:31:12,570 --> 00:31:14,790
the sharding mechanism into an SDK.

759
00:31:14,790 --> 00:31:17,970
So our platforms have an SDK

760
00:31:17,970 --> 00:31:19,690
where this code that I shown

761
00:31:21,300 --> 00:31:23,670
lies on this SDK.

762
00:31:23,670 --> 00:31:27,360
This SDK now knows the
back backend capability.

763
00:31:27,360 --> 00:31:29,190
Like what's the capacity of the backend?

764
00:31:29,190 --> 00:31:30,750
How many shots are running,

765
00:31:30,750 --> 00:31:32,847
and how many customers
are going to be onboarded,

766
00:31:32,847 --> 00:31:35,340
and how many shards per
customer I need to assign?

767
00:31:35,340 --> 00:31:37,710
So this SDK will create this combinations

768
00:31:37,710 --> 00:31:38,670
and create the headers

769
00:31:38,670 --> 00:31:39,900
and every single time

770
00:31:39,900 --> 00:31:41,073
it'll send the request
to the load balancer.

771
00:31:41,073 --> 00:31:42,150
With this approach,

772
00:31:42,150 --> 00:31:43,350
you avoid the router.

773
00:31:43,350 --> 00:31:45,090
But the biggest problem with this approach

774
00:31:45,090 --> 00:31:47,310
is that every single time
you make a code change

775
00:31:47,310 --> 00:31:50,100
or if you want to make a
version change on the SDK,

776
00:31:50,100 --> 00:31:52,110
you have to chase all your customers

777
00:31:52,110 --> 00:31:55,393
and have have them enforce them to

778
00:31:55,393 --> 00:31:58,230
take the latest version of the SDK.

779
00:31:58,230 --> 00:31:59,580
So for this reason,

780
00:31:59,580 --> 00:32:02,610
maintaining this SDK will
be a challenging aspect.

781
00:32:02,610 --> 00:32:04,320
So we are pretty much going to incline

782
00:32:04,320 --> 00:32:06,303
with the router approach.

783
00:32:08,130 --> 00:32:09,900
We talked a lot about architecture.

784
00:32:09,900 --> 00:32:11,940
What are the recommendations, right?

785
00:32:11,940 --> 00:32:13,410
So multiple regions,

786
00:32:13,410 --> 00:32:14,970
multiple availability zones,

787
00:32:14,970 --> 00:32:18,660
and use the domain-driven
design if applicable to you

788
00:32:18,660 --> 00:32:22,530
and use autoscaling our retrace drones

789
00:32:22,530 --> 00:32:25,410
and try to use fail fast timeouts

790
00:32:25,410 --> 00:32:27,300
because we had some reliability issues

791
00:32:27,300 --> 00:32:29,160
due to the high timeouts

792
00:32:29,160 --> 00:32:31,650
and use some of these sharding techniques

793
00:32:31,650 --> 00:32:34,350
that fits best for your use cases

794
00:32:34,350 --> 00:32:35,940
because managing the infrastructure

795
00:32:35,940 --> 00:32:38,460
for the sharding will
be a challenging for you

796
00:32:38,460 --> 00:32:40,530
and be aware of different failure modes.

797
00:32:40,530 --> 00:32:42,630
I'm gonna talk about
different failure modes

798
00:32:42,630 --> 00:32:44,310
in the later part of the session.

799
00:32:44,310 --> 00:32:46,470
And when we talk about five nines,

800
00:32:46,470 --> 00:32:47,670
we are not talking about

801
00:32:47,670 --> 00:32:50,010
all your platform capability
must be five nines, right?

802
00:32:50,010 --> 00:32:52,050
Because the only the
mission critical capability

803
00:32:52,050 --> 00:32:53,280
should be five nines.

804
00:32:53,280 --> 00:32:56,133
It is very hard to manage
a five nine service.

805
00:32:59,460 --> 00:33:00,750
Cool.

806
00:33:00,750 --> 00:33:03,180
We are also a serverless company.

807
00:33:03,180 --> 00:33:04,860
Why are we moving to the serverless?

808
00:33:04,860 --> 00:33:07,620
Because we had some critical loads

809
00:33:07,620 --> 00:33:10,890
that were running on EC2 instances

810
00:33:10,890 --> 00:33:12,540
and that caused some reliability issues.

811
00:33:12,540 --> 00:33:14,400
I will give you some couple of examples.

812
00:33:14,400 --> 00:33:16,020
So we had this docket containers

813
00:33:16,020 --> 00:33:17,700
running on the EC2 instances

814
00:33:17,700 --> 00:33:19,590
and whenever we want to scale out,

815
00:33:19,590 --> 00:33:21,840
we had few incidents like where the

816
00:33:21,840 --> 00:33:23,940
specific instance type was not available,

817
00:33:23,940 --> 00:33:25,950
the IPSR ran out,

818
00:33:25,950 --> 00:33:28,440
so we couldn't autoscale
in a timely manner.

819
00:33:28,440 --> 00:33:29,610
And also, whenever we autoscale,

820
00:33:29,610 --> 00:33:33,210
EC2 takes a lot of lead
time to come up, right?

821
00:33:33,210 --> 00:33:35,190
You need to bring out EC2,

822
00:33:35,190 --> 00:33:36,210
run the user scripts

823
00:33:36,210 --> 00:33:37,950
and then we bring the docker up.

824
00:33:37,950 --> 00:33:41,550
It takes a lot of time for
the EC2 based instances

825
00:33:41,550 --> 00:33:43,683
to come and take the request.

826
00:33:44,730 --> 00:33:45,600
Not only that,

827
00:33:45,600 --> 00:33:48,390
it also created some operational problems.

828
00:33:48,390 --> 00:33:52,350
So managing a fleet of thousands
of notes of EC2 instances

829
00:33:52,350 --> 00:33:55,260
and every single time we have
to patch them, manage them,

830
00:33:55,260 --> 00:33:57,630
it was a big human toil for us, right?

831
00:33:57,630 --> 00:33:59,700
And it also created
some reliability issues.

832
00:33:59,700 --> 00:34:02,413
When we tried to update EC2s,

833
00:34:02,413 --> 00:34:04,680
we we did some manual errors

834
00:34:04,680 --> 00:34:06,990
and they did cause some outages.

835
00:34:06,990 --> 00:34:10,083
That's the reason that
we moved to serverless.

836
00:34:10,083 --> 00:34:11,430
So with serverless,

837
00:34:11,430 --> 00:34:14,400
none of this manual
operations or anymore exist

838
00:34:14,400 --> 00:34:17,190
and we are able to like reliably scale out

839
00:34:17,190 --> 00:34:22,190
and all our operational overhead
has been taken care, right?

840
00:34:22,380 --> 00:34:24,120
Because if you are in Cloud,

841
00:34:24,120 --> 00:34:26,790
if you're still managing
the EC2 instances,

842
00:34:26,790 --> 00:34:28,410
you have to consider that, right?

843
00:34:28,410 --> 00:34:29,520
You have to reconsider that,

844
00:34:29,520 --> 00:34:33,180
try to adapt to a serverless technology

845
00:34:33,180 --> 00:34:35,520
and especially all of our critical,

846
00:34:35,520 --> 00:34:39,256
mission critical
platforms are running with

847
00:34:39,256 --> 00:34:41,103
ECS Fargate services.

848
00:34:42,930 --> 00:34:44,250
When you talk about AWS Lambda,

849
00:34:44,250 --> 00:34:45,390
I want to make a note of here

850
00:34:45,390 --> 00:34:46,950
a note out of it here, right?

851
00:34:46,950 --> 00:34:50,460
So AWS Lambdas is that
we use on a non-critical

852
00:34:50,460 --> 00:34:51,720
asynchronous path

853
00:34:51,720 --> 00:34:54,150
because in the past we had
some reliability issues.

854
00:34:54,150 --> 00:34:56,970
Make sure that whenever you
run the AWS Lambda functions,

855
00:34:56,970 --> 00:34:59,700
you create bulkhead patterns
between these functions

856
00:34:59,700 --> 00:35:03,503
by setting the maximum concurrency
limit to the AWS Lambdas

857
00:35:03,503 --> 00:35:06,060
because they have the
tendency to go and occupy

858
00:35:06,060 --> 00:35:08,850
the entire account
level concurrency limits

859
00:35:08,850 --> 00:35:10,530
if you're not careful about that.

860
00:35:10,530 --> 00:35:13,500
So that will cause some
reliability incidents to you.

861
00:35:13,500 --> 00:35:16,830
And these are all the
different serverless functions,

862
00:35:16,830 --> 00:35:19,230
serverless services that
we use in Capital One

863
00:35:19,230 --> 00:35:21,570
and we are greatly getting benefited.

864
00:35:21,570 --> 00:35:23,880
This has been a biggest
game changer for us.

865
00:35:23,880 --> 00:35:27,990
It's been operationally
we are really efficient

866
00:35:27,990 --> 00:35:29,493
with this adoption.

867
00:35:31,380 --> 00:35:32,930
Let's talk about failure modes.

868
00:35:34,260 --> 00:35:35,490
Failure modes.

869
00:35:35,490 --> 00:35:38,670
We constantly review our
different failure modes, right?

870
00:35:38,670 --> 00:35:41,070
So these are all the
failure modes we look into.

871
00:35:41,070 --> 00:35:43,380
Your Cloud profiler will fail you.

872
00:35:43,380 --> 00:35:45,900
Your internal platforms
dependency could fail you

873
00:35:45,900 --> 00:35:47,790
or external vendors could fail you.

874
00:35:47,790 --> 00:35:49,860
Our customers can send
a poison pill request

875
00:35:49,860 --> 00:35:51,750
and fail your systems

876
00:35:51,750 --> 00:35:54,390
or your own platform
engineers will cost some bucks

877
00:35:54,390 --> 00:35:56,550
and create a reliability problems.

878
00:35:56,550 --> 00:35:58,920
And the last one is the UnTrusted Code.

879
00:35:58,920 --> 00:36:00,150
This is a very special condition

880
00:36:00,150 --> 00:36:02,430
where we are building a platform

881
00:36:02,430 --> 00:36:05,340
where we take this business logic

882
00:36:05,340 --> 00:36:06,540
from our internal developers

883
00:36:06,540 --> 00:36:07,740
from a different business units.

884
00:36:07,740 --> 00:36:09,000
Like for instance,

885
00:36:09,000 --> 00:36:10,230
we have a card business unit

886
00:36:10,230 --> 00:36:11,550
and bank business unit.

887
00:36:11,550 --> 00:36:15,090
They will pack their
business logic in the code

888
00:36:15,090 --> 00:36:16,260
and then ship it to us

889
00:36:16,260 --> 00:36:18,510
and we will execute it in our platform.

890
00:36:18,510 --> 00:36:20,460
So this is not causing a security issue,

891
00:36:20,460 --> 00:36:22,410
this is causing a reliability issue.

892
00:36:22,410 --> 00:36:25,350
I'm gonna talk about
that a little bit here.

893
00:36:25,350 --> 00:36:27,180
We had this design

894
00:36:27,180 --> 00:36:28,320
in our version one

895
00:36:28,320 --> 00:36:32,490
where our services and this untrusted code

896
00:36:32,490 --> 00:36:35,730
were sharing the same VM resources, right?

897
00:36:35,730 --> 00:36:39,060
So what happens when you get a request,

898
00:36:39,060 --> 00:36:41,700
we'll call this untrusted code function.

899
00:36:41,700 --> 00:36:42,990
And in a separate thread,

900
00:36:42,990 --> 00:36:44,640
even though they run a,

901
00:36:44,640 --> 00:36:45,840
we run a separate thread,

902
00:36:45,840 --> 00:36:48,210
they share the underlying JVMs

903
00:36:48,210 --> 00:36:50,010
and underlying VMs and things like that.

904
00:36:50,010 --> 00:36:52,140
Now, imagine this untrusted code

905
00:36:52,140 --> 00:36:54,150
had something like a system dot exit

906
00:36:54,150 --> 00:36:55,980
or it has some wild through loop.

907
00:36:55,980 --> 00:36:58,110
The code is constantly running nonstop

908
00:36:58,110 --> 00:37:00,270
or it has a recursion it causing some

909
00:37:00,270 --> 00:37:01,800
stack workflow errors, right?

910
00:37:01,800 --> 00:37:03,150
There are possibilities

911
00:37:03,150 --> 00:37:05,940
either intentionally or unintentionally,

912
00:37:05,940 --> 00:37:08,070
our internal developers could bring

913
00:37:08,070 --> 00:37:10,530
reliability issues into
these platforms like this.

914
00:37:10,530 --> 00:37:12,540
So for this kind of platforms,

915
00:37:12,540 --> 00:37:15,450
we are moving towards the
"Sandbox Safety Model".

916
00:37:15,450 --> 00:37:17,400
So in the Sandbox Safety Model,

917
00:37:17,400 --> 00:37:19,590
we'll create multiple micro JVMs.

918
00:37:19,590 --> 00:37:21,930
So these multiple micro JVMs are

919
00:37:21,930 --> 00:37:23,880
heavily built with bounded context.

920
00:37:23,880 --> 00:37:28,880
So these service VMs are never
shared with this micro VMs.

921
00:37:29,100 --> 00:37:30,570
Our untrusted code

922
00:37:30,570 --> 00:37:33,750
will be allocated a very fixed capacity

923
00:37:33,750 --> 00:37:35,490
of let's say 10 mb

924
00:37:35,490 --> 00:37:39,270
and we set a very strict
timeline of execution

925
00:37:39,270 --> 00:37:40,290
like the timeouts

926
00:37:40,290 --> 00:37:42,270
for every single function execution

927
00:37:42,270 --> 00:37:46,170
and we'll strictly prohibit
the IO access spots.

928
00:37:46,170 --> 00:37:47,580
So this untrusted code

929
00:37:47,580 --> 00:37:51,000
can never access the
critical file system path

930
00:37:51,000 --> 00:37:52,320
and cause an outage.

931
00:37:52,320 --> 00:37:53,970
So if in case,

932
00:37:53,970 --> 00:37:55,590
if you're running a malicious code

933
00:37:55,590 --> 00:38:00,590
inside this untrusted block of sandbox,

934
00:38:00,780 --> 00:38:02,640
whatever happens between this sandbox

935
00:38:02,640 --> 00:38:04,410
will just stay within the sandbox.

936
00:38:04,410 --> 00:38:06,450
It'll never impact your service

937
00:38:06,450 --> 00:38:10,140
because you're not sharing
anything with the untrusted code.

938
00:38:10,140 --> 00:38:12,840
If you are building a
platform similar to this,

939
00:38:12,840 --> 00:38:15,720
I highly recommend you
to go and read about the

940
00:38:15,720 --> 00:38:17,193
sandbox safety approach.

941
00:38:19,050 --> 00:38:20,820
Cool. So that's a lot of non...

942
00:38:20,820 --> 00:38:23,910
A lot of architectural context

943
00:38:23,910 --> 00:38:27,123
and let's move on to some of
the non-architectural context.

944
00:38:27,960 --> 00:38:29,010
What are the challenges we had?

945
00:38:29,010 --> 00:38:31,440
So we talked about a lot of
infrastructure management

946
00:38:31,440 --> 00:38:32,400
and things like that.

947
00:38:32,400 --> 00:38:34,620
One of the biggest challenge we had was

948
00:38:34,620 --> 00:38:36,063
infrastructure management.

949
00:38:37,260 --> 00:38:38,093
Why?

950
00:38:38,093 --> 00:38:42,630
Because our infrastructure
as a generation one pipeline,

951
00:38:42,630 --> 00:38:45,630
our generation one pipeline
was heavily AML based.

952
00:38:45,630 --> 00:38:47,370
So there are multiple values,

953
00:38:47,370 --> 00:38:48,420
multiple environments,

954
00:38:48,420 --> 00:38:49,470
multiple copy paste

955
00:38:49,470 --> 00:38:52,530
and it was used to be
a lot of manual errors

956
00:38:52,530 --> 00:38:54,607
and it caused some reliability problems.

957
00:38:54,607 --> 00:38:57,060
To give you a couple of examples,

958
00:38:57,060 --> 00:38:58,920
we had some port misconfiguration.

959
00:38:58,920 --> 00:39:00,090
Instead of port 8080,

960
00:39:00,090 --> 00:39:02,250
someone did port 808,

961
00:39:02,250 --> 00:39:03,720
they missed the last digit, right?

962
00:39:03,720 --> 00:39:05,640
That caused a reliability instant.

963
00:39:05,640 --> 00:39:08,070
And our pipeline did not have

964
00:39:08,070 --> 00:39:09,810
drift detection capability.

965
00:39:09,810 --> 00:39:13,260
So we had one setup is
running in the Cloud

966
00:39:13,260 --> 00:39:15,420
but your repository is
completely different.

967
00:39:15,420 --> 00:39:17,820
So the next time when
you go and create it,

968
00:39:17,820 --> 00:39:18,870
the infrastructure,

969
00:39:18,870 --> 00:39:20,370
you'll be creating the infrastructure

970
00:39:20,370 --> 00:39:21,900
with the faulty configuration

971
00:39:21,900 --> 00:39:24,093
and that has caused a
reliability incident.

972
00:39:25,230 --> 00:39:26,133
What we are doing.

973
00:39:27,120 --> 00:39:30,150
We are taking quantum leap.

974
00:39:30,150 --> 00:39:32,160
We are trying to mitigate this with

975
00:39:32,160 --> 00:39:35,550
complete automation using AWS CDK

976
00:39:35,550 --> 00:39:36,780
because

977
00:39:36,780 --> 00:39:38,880
when we talk about reliability

978
00:39:38,880 --> 00:39:40,350
architecture is one thing

979
00:39:40,350 --> 00:39:41,850
and these non-architectural things

980
00:39:41,850 --> 00:39:44,070
that infrastructures and code releases

981
00:39:44,070 --> 00:39:46,353
have to support your reliability goals.

982
00:39:48,120 --> 00:39:49,620
So I talked about CDK,

983
00:39:49,620 --> 00:39:51,900
so now we are taking this quantum leap

984
00:39:51,900 --> 00:39:54,150
from infrastructure as configuration

985
00:39:54,150 --> 00:39:55,680
into infrastructure as code.

986
00:39:55,680 --> 00:39:58,323
We manage our complete
infrastructure using code.

987
00:39:59,220 --> 00:40:00,657
We use the AWS CDK for that.

988
00:40:00,657 --> 00:40:03,840
AWS CDK is the Cloud

989
00:40:03,840 --> 00:40:05,460
development kit.

990
00:40:05,460 --> 00:40:06,293
Now, think of this,

991
00:40:06,293 --> 00:40:08,387
you have some R53s,

992
00:40:08,387 --> 00:40:09,347
ALBs,

993
00:40:09,347 --> 00:40:11,790
and ECS containers and things like that.

994
00:40:11,790 --> 00:40:14,430
If I say all of this can be coded,

995
00:40:14,430 --> 00:40:17,310
instead of using a vendor specific,

996
00:40:17,310 --> 00:40:18,720
domain specific language

997
00:40:18,720 --> 00:40:20,550
or a AML file,

998
00:40:20,550 --> 00:40:21,900
all of this can be coded

999
00:40:21,900 --> 00:40:24,240
in an imperative programming language way

1000
00:40:24,240 --> 00:40:27,660
such as TypeScript, Java, Go, or Python,

1001
00:40:27,660 --> 00:40:28,920
things like that, right?

1002
00:40:28,920 --> 00:40:31,440
So it unblocks the capability.

1003
00:40:31,440 --> 00:40:33,750
This is an example of an AWS CDK code

1004
00:40:33,750 --> 00:40:35,285
where I'm trying to create

1005
00:40:35,285 --> 00:40:39,240
ECS fargate service behind an ALB.

1006
00:40:39,240 --> 00:40:40,650
And whenever you create this,

1007
00:40:40,650 --> 00:40:43,680
this underneath creates a
Cloud formation template

1008
00:40:43,680 --> 00:40:45,090
and the infrastructure is managed

1009
00:40:45,090 --> 00:40:46,800
using the Cloud formation templates

1010
00:40:46,800 --> 00:40:50,220
and it unblocks the capability
of writing test cases.

1011
00:40:50,220 --> 00:40:52,500
So the moment you write code,

1012
00:40:52,500 --> 00:40:55,680
you now can write code to
test your infrastructure.

1013
00:40:55,680 --> 00:40:57,840
So in this example, you can see

1014
00:40:57,840 --> 00:41:00,300
I'm trying to verify
whether I'm using the ALB

1015
00:41:00,300 --> 00:41:02,580
with the right SL certificate.

1016
00:41:02,580 --> 00:41:03,720
If not,

1017
00:41:03,720 --> 00:41:05,250
I'm not going to create this.

1018
00:41:05,250 --> 00:41:08,130
This test case will fail
in my developer mission

1019
00:41:08,130 --> 00:41:10,170
and as well as in the CICD pipeline.

1020
00:41:10,170 --> 00:41:12,030
So this is the shift-lift
testing capability.

1021
00:41:12,030 --> 00:41:14,070
It's the biggest game changer for us.

1022
00:41:14,070 --> 00:41:16,590
We are no longer creating
faulty infrastructures

1023
00:41:16,590 --> 00:41:17,913
or causing incidents.

1024
00:41:19,860 --> 00:41:23,670
Another example of how
do you find this bugs.

1025
00:41:23,670 --> 00:41:27,670
So let's imagine that you
are trying to create ECS

1026
00:41:28,800 --> 00:41:31,677
task with 256 CPU

1027
00:41:31,677 --> 00:41:34,560
and some memory that is
non-standard for you.

1028
00:41:34,560 --> 00:41:37,140
So this can be cut much earlier

1029
00:41:37,140 --> 00:41:40,470
because in the V1 of our CICD pipeline,

1030
00:41:40,470 --> 00:41:43,230
we used to create the
resources in the Cloud

1031
00:41:43,230 --> 00:41:44,610
and then later we figured out,

1032
00:41:44,610 --> 00:41:46,590
okay, we create it with
the faulty configuration

1033
00:41:46,590 --> 00:41:47,730
and then we'll go back,

1034
00:41:47,730 --> 00:41:49,170
destroy the resources,

1035
00:41:49,170 --> 00:41:51,600
and then we'll rerun, fix it, correct it.

1036
00:41:51,600 --> 00:41:53,970
So we'll do this back and forth of fixing

1037
00:41:53,970 --> 00:41:57,060
and making sure the infrastructure
we created was correct

1038
00:41:57,060 --> 00:41:59,430
and it also caused some reliability issues

1039
00:41:59,430 --> 00:42:01,830
and also it was too much impact

1040
00:42:01,830 --> 00:42:04,080
to our developer
productivity and Cloud cost.

1041
00:42:05,700 --> 00:42:08,430
We also run infrastructure rules as code

1042
00:42:08,430 --> 00:42:09,900
using the CDK Nag.

1043
00:42:09,900 --> 00:42:12,300
So we have multiple rules that we run

1044
00:42:12,300 --> 00:42:14,634
like naming standards,

1045
00:42:14,634 --> 00:42:17,520
the ALB style port configurations,

1046
00:42:17,520 --> 00:42:21,180
or do you have the right
SSL ports open, right?

1047
00:42:21,180 --> 00:42:25,080
Things like that are also
managed using CDK Nag.

1048
00:42:25,080 --> 00:42:28,290
So infrastructure as code
offers this capability for you

1049
00:42:28,290 --> 00:42:30,180
and all these rules we run

1050
00:42:30,180 --> 00:42:32,280
in the developer machine
and in your pipeline

1051
00:42:32,280 --> 00:42:34,833
before we get created in the Cloud.

1052
00:42:35,700 --> 00:42:36,630
Let's talk about some of the

1053
00:42:36,630 --> 00:42:37,950
release techniques that we have.

1054
00:42:37,950 --> 00:42:40,816
So whenever we do releases,

1055
00:42:40,816 --> 00:42:43,950
we never take down the services, right?

1056
00:42:43,950 --> 00:42:44,783
We should not.

1057
00:42:44,783 --> 00:42:46,440
So we do the releases

1058
00:42:46,440 --> 00:42:48,630
while the customers
are using the services.

1059
00:42:48,630 --> 00:42:52,200
Let me bring back this
shuffle charting diagram here.

1060
00:42:52,200 --> 00:42:54,060
Just now, imagine that
we have a new version,

1061
00:42:54,060 --> 00:42:55,560
version in the blue box,

1062
00:42:55,560 --> 00:42:57,180
version N plus one.

1063
00:42:57,180 --> 00:43:00,210
Now, this blue box is
ready to go to production.

1064
00:43:00,210 --> 00:43:03,090
Now, instead of applying to
all these different charts,

1065
00:43:03,090 --> 00:43:04,920
I will pick and choose which shard

1066
00:43:04,920 --> 00:43:07,710
and which particular task I
should go and update, right?

1067
00:43:07,710 --> 00:43:10,020
Now in this example, customer group one

1068
00:43:10,020 --> 00:43:12,450
I will release to this customer group one,

1069
00:43:12,450 --> 00:43:15,450
I will just drop the code
into a particular task

1070
00:43:15,450 --> 00:43:16,410
and then replace

1071
00:43:16,410 --> 00:43:18,210
and I will constantly monitor the

1072
00:43:18,210 --> 00:43:19,950
behavior of the new version.

1073
00:43:19,950 --> 00:43:21,270
And if I get confident

1074
00:43:21,270 --> 00:43:23,070
then I will automatically roll the

1075
00:43:23,070 --> 00:43:24,690
new version to the other shards

1076
00:43:24,690 --> 00:43:25,620
under the other shards

1077
00:43:25,620 --> 00:43:27,630
and other shards and on.

1078
00:43:27,630 --> 00:43:28,710
How are we doing this?

1079
00:43:28,710 --> 00:43:30,690
It's completely using the CodeDeploy.

1080
00:43:30,690 --> 00:43:33,960
So we do not have any
manual release process.

1081
00:43:33,960 --> 00:43:37,864
So ever since we moved to
the AWS Fargate services,

1082
00:43:37,864 --> 00:43:40,350
the serverless services,

1083
00:43:40,350 --> 00:43:41,794
we are using CodeDeploys.

1084
00:43:41,794 --> 00:43:44,100
So CodeDeploy with the app spec deployment

1085
00:43:44,100 --> 00:43:46,230
configuration files,

1086
00:43:46,230 --> 00:43:48,870
we will just just orchestrate our releases

1087
00:43:48,870 --> 00:43:51,360
so we can say what is our
roll forward strategy?

1088
00:43:51,360 --> 00:43:52,650
What is our rollback strategy?

1089
00:43:52,650 --> 00:43:53,490
And things like that.

1090
00:43:53,490 --> 00:43:55,890
And CodeDeploy takes care
of everything for you.

1091
00:43:57,120 --> 00:43:58,890
The biggest force multiplier

1092
00:43:58,890 --> 00:44:01,680
of CodeDeploy is the
deployment lifecycle hooks.

1093
00:44:01,680 --> 00:44:04,893
So I've given highlighted
three examples here.

1094
00:44:05,760 --> 00:44:06,720
Before install,

1095
00:44:06,720 --> 00:44:07,740
after install,

1096
00:44:07,740 --> 00:44:09,750
and before allow traffic.

1097
00:44:09,750 --> 00:44:13,380
Before you install a particular
version of the software,

1098
00:44:13,380 --> 00:44:15,240
make sure that this software

1099
00:44:15,240 --> 00:44:17,100
has all the right configurations.

1100
00:44:17,100 --> 00:44:19,290
And after you install,

1101
00:44:19,290 --> 00:44:21,540
before you service the customer traffic,

1102
00:44:21,540 --> 00:44:24,240
make sure the release is actually correct

1103
00:44:24,240 --> 00:44:27,180
by running some synthetic
functional test, right?

1104
00:44:27,180 --> 00:44:28,440
Using the synthetic data.

1105
00:44:28,440 --> 00:44:30,840
So we run some functional
test in production

1106
00:44:30,840 --> 00:44:32,400
using some synthetic data

1107
00:44:32,400 --> 00:44:34,830
and ensure that the release is good

1108
00:44:34,830 --> 00:44:37,590
and we are not going to
break any reliability,

1109
00:44:37,590 --> 00:44:39,990
and before we allow the customer traffic,

1110
00:44:39,990 --> 00:44:42,930
because some of our systems
had some cold start problem.

1111
00:44:42,930 --> 00:44:45,000
So we use this capability,

1112
00:44:45,000 --> 00:44:46,350
this lifecycle hook,

1113
00:44:46,350 --> 00:44:48,453
to warm up our containers.

1114
00:44:49,920 --> 00:44:52,170
So this is Lambda example

1115
00:44:52,170 --> 00:44:55,470
that listens to all these
different lifecycle events.

1116
00:44:55,470 --> 00:44:57,480
So for every lifecycle event

1117
00:44:57,480 --> 00:45:00,303
we will run a different
pre-validation steps.

1118
00:45:03,177 --> 00:45:04,860
Another great part

1119
00:45:04,860 --> 00:45:08,250
is that all of this is
maintained in AWS CDK

1120
00:45:08,250 --> 00:45:10,920
include not only creating
our infrastructures,

1121
00:45:10,920 --> 00:45:12,750
also doing the releases

1122
00:45:12,750 --> 00:45:15,330
is maintained in AWS CDK as code

1123
00:45:15,330 --> 00:45:17,883
so that we test out
everything before we create.

1124
00:45:20,520 --> 00:45:21,353
Like I said,

1125
00:45:21,353 --> 00:45:22,260
we use the CodeDeploy

1126
00:45:22,260 --> 00:45:24,030
to do the gradual rollout

1127
00:45:24,030 --> 00:45:25,470
and we have this warranty period

1128
00:45:25,470 --> 00:45:28,530
before we completely
route the customer traffic

1129
00:45:28,530 --> 00:45:30,690
to the new version,

1130
00:45:30,690 --> 00:45:34,530
we will use the CodeDeploys
linear gradual growth

1131
00:45:34,530 --> 00:45:36,870
for traffic routing.

1132
00:45:36,870 --> 00:45:38,700
This is in another check that we do

1133
00:45:38,700 --> 00:45:40,080
called readiness checks.

1134
00:45:40,080 --> 00:45:43,170
We had some reliability incidents where,

1135
00:45:43,170 --> 00:45:44,490
to give you an example,

1136
00:45:44,490 --> 00:45:46,170
we had introduced a new capability

1137
00:45:46,170 --> 00:45:48,270
that required a Dynamodb table access

1138
00:45:48,270 --> 00:45:49,830
and we had an IAM bug.

1139
00:45:49,830 --> 00:45:51,030
So what happened?

1140
00:45:51,030 --> 00:45:52,380
We opened the customer traffic

1141
00:45:52,380 --> 00:45:55,050
and then the customer use case was failed

1142
00:45:55,050 --> 00:45:57,300
because the IAM policy was not updated.

1143
00:45:57,300 --> 00:45:59,640
So that incident taught us a lesson.

1144
00:45:59,640 --> 00:46:01,260
We should shift-left the capability

1145
00:46:01,260 --> 00:46:03,390
of checking the readiness
on the application side.

1146
00:46:03,390 --> 00:46:04,680
So our applications,

1147
00:46:04,680 --> 00:46:05,940
before they even come up,

1148
00:46:05,940 --> 00:46:07,260
they do this,

1149
00:46:07,260 --> 00:46:11,010
they call this custom IAM
simulate policy endpoint

1150
00:46:11,010 --> 00:46:11,843
and they will think,

1151
00:46:11,843 --> 00:46:15,150
okay I have to access these many services,

1152
00:46:15,150 --> 00:46:17,700
do I have the right access

1153
00:46:17,700 --> 00:46:19,200
before I take the customer traffic,

1154
00:46:19,200 --> 00:46:20,730
before I say I'm healthy

1155
00:46:20,730 --> 00:46:22,560
and attach it to the ALB,

1156
00:46:22,560 --> 00:46:27,000
this application itself
will do several checks

1157
00:46:27,000 --> 00:46:28,710
before it takes the customer traffic.

1158
00:46:28,710 --> 00:46:30,660
And this is, again, a game changer for us

1159
00:46:30,660 --> 00:46:35,660
with that lot of reliability
issues related to code releases

1160
00:46:35,820 --> 00:46:37,323
have been gone.

1161
00:46:38,640 --> 00:46:41,520
We also have these automated failover

1162
00:46:41,520 --> 00:46:43,860
and also we have the automated failback.

1163
00:46:43,860 --> 00:46:45,450
So the moment we failover,

1164
00:46:45,450 --> 00:46:46,740
we will create an event.

1165
00:46:46,740 --> 00:46:49,340
That event will be going to the AWS Lambda

1166
00:46:49,340 --> 00:46:52,530
and the AWS Lambda will
listen to this failover event

1167
00:46:52,530 --> 00:46:54,030
and it'll kick in,

1168
00:46:54,030 --> 00:46:56,910
and then it'll start
sending a particular load

1169
00:46:56,910 --> 00:46:58,200
to the unhealthy region

1170
00:46:58,200 --> 00:47:01,380
and it'll ensure that the
unhealthy region has come back up.

1171
00:47:01,380 --> 00:47:03,300
Sometimes we use the synthetic load

1172
00:47:03,300 --> 00:47:05,640
and sometimes we'll open
up a little bit of traffic,

1173
00:47:05,640 --> 00:47:08,220
customer traffic to check
the other region is good.

1174
00:47:08,220 --> 00:47:09,960
So all of this is automated.

1175
00:47:09,960 --> 00:47:13,473
So none of this, either the
failover or failback, is manual.

1176
00:47:15,060 --> 00:47:17,160
I want to spend a minute here.

1177
00:47:17,160 --> 00:47:19,620
Some of our critical
mission critical platforms

1178
00:47:19,620 --> 00:47:22,620
requires zero RTO/RPO goals

1179
00:47:22,620 --> 00:47:25,953
and they also require
cross-regional consistencies.

1180
00:47:26,820 --> 00:47:27,653
Think of this,

1181
00:47:27,653 --> 00:47:30,840
you're a new customer with a $0 balance.

1182
00:47:30,840 --> 00:47:33,240
You're now trying to make a $1 deposit

1183
00:47:33,240 --> 00:47:35,700
and we have Dynamodb global table.

1184
00:47:35,700 --> 00:47:38,220
So your request goes to the region one,

1185
00:47:38,220 --> 00:47:40,380
your $1 deposit goes to the region one.

1186
00:47:40,380 --> 00:47:41,700
We successfully processed it

1187
00:47:41,700 --> 00:47:45,300
and we persisted the data in
the Dynamodb global table.

1188
00:47:45,300 --> 00:47:46,133
But the data,

1189
00:47:46,133 --> 00:47:48,600
in order for the data to
replicate to the other region,

1190
00:47:48,600 --> 00:47:50,700
there is no guaranteed SLA.

1191
00:47:50,700 --> 00:47:52,980
It'll be eventually available
on the other region, right?

1192
00:47:52,980 --> 00:47:54,420
It could take few milliseconds,

1193
00:47:54,420 --> 00:47:57,540
few seconds, or even few
days or hours we do not know,

1194
00:47:57,540 --> 00:47:59,040
but it'll be eventually available.

1195
00:47:59,040 --> 00:48:02,880
Now, think that you made a
$1 deposit to the region one

1196
00:48:02,880 --> 00:48:04,620
and we failed over to region two.

1197
00:48:04,620 --> 00:48:09,090
Now we are trying to make
another $1 payment or a deposit.

1198
00:48:09,090 --> 00:48:11,010
It goes to the region two.

1199
00:48:11,010 --> 00:48:13,200
But now after the second transaction,

1200
00:48:13,200 --> 00:48:14,850
your balance will be still $1

1201
00:48:14,850 --> 00:48:17,850
because the other transaction
you made on the region one

1202
00:48:17,850 --> 00:48:20,100
have not replicated to the region two.

1203
00:48:20,100 --> 00:48:21,150
This has been a problem

1204
00:48:21,150 --> 00:48:22,980
for some of our mission
critical platforms,

1205
00:48:22,980 --> 00:48:25,290
which requires cross-region consistency.

1206
00:48:25,290 --> 00:48:26,930
So what we are exploring here is the

1207
00:48:26,930 --> 00:48:29,910
Dynamodb multi-region strong consistency

1208
00:48:29,910 --> 00:48:32,940
in which every single time
you write it to the Dynamodb,

1209
00:48:32,940 --> 00:48:35,610
it'll durably write it to the other region

1210
00:48:35,610 --> 00:48:37,200
in a strongly consistent manner

1211
00:48:37,200 --> 00:48:38,610
so that you failover,

1212
00:48:38,610 --> 00:48:40,110
you go read on the other region,

1213
00:48:40,110 --> 00:48:41,973
your data is available.

1214
00:48:45,000 --> 00:48:48,240
Some of our learnings using
the Cold Start, right?

1215
00:48:48,240 --> 00:48:49,770
So there are a couple of things

1216
00:48:49,770 --> 00:48:51,360
that impacted our high availability

1217
00:48:51,360 --> 00:48:53,100
because I am talking about this five nine

1218
00:48:53,100 --> 00:48:55,020
because when you talk about five nines,

1219
00:48:55,020 --> 00:48:57,360
your downtime budget is very low,

1220
00:48:57,360 --> 00:48:58,950
your added budget is very low.

1221
00:48:58,950 --> 00:49:02,310
So there were two scenarios
where we were impacted.

1222
00:49:02,310 --> 00:49:05,040
Once is the cold start
immediately after the release

1223
00:49:05,040 --> 00:49:07,770
and the other one is when
there is a long lull period.

1224
00:49:07,770 --> 00:49:08,850
So customer comes,

1225
00:49:08,850 --> 00:49:10,380
uses your platform

1226
00:49:10,380 --> 00:49:11,490
and there is a lull period,

1227
00:49:11,490 --> 00:49:12,323
they'll come back,

1228
00:49:12,323 --> 00:49:13,530
they use the platform again.

1229
00:49:13,530 --> 00:49:16,050
So every single time when
this pattern is repeated,

1230
00:49:16,050 --> 00:49:18,330
you'll see a spike in the response time.

1231
00:49:18,330 --> 00:49:19,440
Why this is a problem?

1232
00:49:19,440 --> 00:49:22,320
Because we have set a
clear response time SLA

1233
00:49:22,320 --> 00:49:23,430
of 500 millisecond

1234
00:49:23,430 --> 00:49:26,460
and every single time we breach that SLA

1235
00:49:26,460 --> 00:49:28,680
we will count that as not available.

1236
00:49:28,680 --> 00:49:30,480
Even though the request was successful,

1237
00:49:30,480 --> 00:49:32,010
it breached the response time SLA,

1238
00:49:32,010 --> 00:49:34,530
hence it counted as not available.

1239
00:49:34,530 --> 00:49:36,450
How are we mitigating this problem?

1240
00:49:36,450 --> 00:49:39,210
We are mitigating this
problem using the CodeDeploy

1241
00:49:39,210 --> 00:49:40,200
deployment hooks.

1242
00:49:40,200 --> 00:49:41,940
So the moment we deploy

1243
00:49:41,940 --> 00:49:43,830
the CodeDeploy deployment hook will know

1244
00:49:43,830 --> 00:49:45,330
the new version is deployed,

1245
00:49:45,330 --> 00:49:49,140
let me go and use this pre
warm script that I have.

1246
00:49:49,140 --> 00:49:51,150
So with that we are able to eliminate

1247
00:49:51,150 --> 00:49:52,563
the cold start problems.

1248
00:49:53,840 --> 00:49:56,940
Resilience testing as code
is another bigger area

1249
00:49:56,940 --> 00:49:58,290
that we have been doing.

1250
00:49:58,290 --> 00:50:01,260
You can also call that
as a chaos engineering.

1251
00:50:01,260 --> 00:50:02,093
If you remember,

1252
00:50:02,093 --> 00:50:04,410
I talked about the
different failure modes.

1253
00:50:04,410 --> 00:50:07,050
So what is the outcome of
the different failure modes?

1254
00:50:07,050 --> 00:50:08,940
So we will decide

1255
00:50:08,940 --> 00:50:11,820
for every failure mode we'll
apply a "what if" scenario.

1256
00:50:11,820 --> 00:50:15,150
What if your Cloud providers
region one has gone down?

1257
00:50:15,150 --> 00:50:17,370
What if your Cloud providers
couple of availabilities

1258
00:50:17,370 --> 00:50:18,270
have gone down?

1259
00:50:18,270 --> 00:50:20,220
What if your platform had a bug?

1260
00:50:20,220 --> 00:50:22,710
So we will convert the
different failure modes

1261
00:50:22,710 --> 00:50:24,600
into several "what if" scenarios.

1262
00:50:24,600 --> 00:50:27,330
After we convert the
several "what if" scenarios,

1263
00:50:27,330 --> 00:50:29,910
we will try to reverse those scenarios

1264
00:50:29,910 --> 00:50:31,770
in a very controlled environment.

1265
00:50:31,770 --> 00:50:33,120
When we reverse those environment,

1266
00:50:33,120 --> 00:50:34,080
it helps in two ways.

1267
00:50:34,080 --> 00:50:36,540
One, it helps to check whether

1268
00:50:36,540 --> 00:50:38,700
our alerts are working properly,

1269
00:50:38,700 --> 00:50:40,530
our runbooks are up to date,

1270
00:50:40,530 --> 00:50:44,070
and it helps tremendously
our engineers to prepare

1271
00:50:44,070 --> 00:50:47,730
and be prepared when the
real things happen, right?

1272
00:50:47,730 --> 00:50:50,310
So this is where we are
getting a biggest benefit

1273
00:50:50,310 --> 00:50:53,910
because it constantly
trains your engineers.

1274
00:50:53,910 --> 00:50:54,953
How are we doing this?

1275
00:50:54,953 --> 00:50:57,540
There's several different
"what if" scenario.

1276
00:50:57,540 --> 00:51:02,540
We are creating an AWS FAS based SDK

1277
00:51:02,820 --> 00:51:05,010
where we will use this gherkin format

1278
00:51:05,010 --> 00:51:08,400
and then we will define
in a simple gherkin format

1279
00:51:08,400 --> 00:51:10,410
the different failure scenarios

1280
00:51:10,410 --> 00:51:12,330
and then our SDK will convert into

1281
00:51:12,330 --> 00:51:14,943
the failure scenario testing.

1282
00:51:16,830 --> 00:51:18,720
We also do game days.

1283
00:51:18,720 --> 00:51:22,440
In our game days we will
completely isolate traffic

1284
00:51:22,440 --> 00:51:23,910
to the one of the regions

1285
00:51:23,910 --> 00:51:26,670
and we will see whether
our systems are able

1286
00:51:26,670 --> 00:51:29,100
to independently operate
in a single region.

1287
00:51:29,100 --> 00:51:31,860
So I talked about regional affinity.

1288
00:51:31,860 --> 00:51:34,710
So how do we test the regional
affinity by doing this?

1289
00:51:34,710 --> 00:51:36,120
So we'll completely isolate

1290
00:51:36,120 --> 00:51:40,170
and we will ensure the systems
running in the single region

1291
00:51:40,170 --> 00:51:41,790
is able to like take up the load

1292
00:51:41,790 --> 00:51:43,980
and that is no cross dependency.

1293
00:51:43,980 --> 00:51:45,210
Resilience testing

1294
00:51:45,210 --> 00:51:48,693
is the way you test your
architecture's resiliency, right?

1295
00:51:50,970 --> 00:51:55,050
We also do several
observability-related standardization.

1296
00:51:55,050 --> 00:51:57,300
We standardize all our logging.

1297
00:51:57,300 --> 00:52:00,750
So all our loggings are
based on structured logging.

1298
00:52:00,750 --> 00:52:04,140
This helps to immediately
identify the problems

1299
00:52:04,140 --> 00:52:06,690
and reduce the mean time
to identify the issues.

1300
00:52:06,690 --> 00:52:09,000
And we also standardize
the several metrics,

1301
00:52:09,000 --> 00:52:10,290
metrics related to,

1302
00:52:10,290 --> 00:52:13,020
saturation metrics related to

1303
00:52:13,020 --> 00:52:15,270
availability and error
budget and things like that.

1304
00:52:15,270 --> 00:52:18,720
And all of our platforms
are enabled with tracing.

1305
00:52:18,720 --> 00:52:21,060
So we have a distributed tracing

1306
00:52:21,060 --> 00:52:23,460
and we also have a continuous profiling.

1307
00:52:23,460 --> 00:52:24,660
Every single time,

1308
00:52:24,660 --> 00:52:26,490
we will run a continuous profiling

1309
00:52:26,490 --> 00:52:28,860
on a different versions
of the same function

1310
00:52:28,860 --> 00:52:30,090
and we use the histograms

1311
00:52:30,090 --> 00:52:32,640
to compare the performance
of the same function

1312
00:52:32,640 --> 00:52:34,040
with the different versions.

1313
00:52:35,010 --> 00:52:37,260
And we also do the error
code standardization

1314
00:52:37,260 --> 00:52:40,410
because error code
standardization is very important

1315
00:52:40,410 --> 00:52:43,350
because some errors needs a failover.

1316
00:52:43,350 --> 00:52:46,200
Some errors needs a N minus one.

1317
00:52:46,200 --> 00:52:49,010
Some error codes might need you to retry.

1318
00:52:49,010 --> 00:52:51,660
So you cannot have a single error code

1319
00:52:51,660 --> 00:52:53,730
for all your resiliency requirements.

1320
00:52:53,730 --> 00:52:56,430
So standardizing the error
code is another important thing

1321
00:52:56,430 --> 00:52:58,740
that we follow in Capital One

1322
00:52:58,740 --> 00:53:01,620
and which constantly message these KPIs.

1323
00:53:01,620 --> 00:53:03,540
What are our meantime between the failure,

1324
00:53:03,540 --> 00:53:05,400
meantime to identify an issue,

1325
00:53:05,400 --> 00:53:09,120
or meantime to recover from
an issue and things like that.

1326
00:53:09,120 --> 00:53:12,180
And another important thing that we do,

1327
00:53:12,180 --> 00:53:14,670
we have been doing in this space

1328
00:53:14,670 --> 00:53:18,150
is moving away from static
alerts into dynamic alerts.

1329
00:53:18,150 --> 00:53:21,540
We had this problem where if
you look into the left side,

1330
00:53:21,540 --> 00:53:24,753
we had the static alerts for
every five minutes window.

1331
00:53:26,250 --> 00:53:27,513
This alert,

1332
00:53:28,410 --> 00:53:33,000
right now measures
account of error that is

1333
00:53:33,000 --> 00:53:34,320
greater than a hundred for five minutes.

1334
00:53:34,320 --> 00:53:36,750
Now, imagine you have
for every five minutes,

1335
00:53:36,750 --> 00:53:40,200
you have 99 requests
on all the 99 are fail.

1336
00:53:40,200 --> 00:53:41,250
If that happens,

1337
00:53:41,250 --> 00:53:44,100
this alert will never go off, right?

1338
00:53:44,100 --> 00:53:47,310
Because even though you're
a hundred percent failure,

1339
00:53:47,310 --> 00:53:50,100
this alert is based on a static threshold,

1340
00:53:50,100 --> 00:53:52,440
which is a count of error.

1341
00:53:52,440 --> 00:53:54,060
In this scenario, it'll never go off.

1342
00:53:54,060 --> 00:53:56,060
So what we are doing is like

1343
00:53:56,060 --> 00:53:58,740
we are moving to a more
dynamic based alerts

1344
00:53:58,740 --> 00:54:00,990
based on the different traffic patterns.

1345
00:54:00,990 --> 00:54:03,450
Our low traffic analysis,

1346
00:54:03,450 --> 00:54:04,800
medium traffic patterns,

1347
00:54:04,800 --> 00:54:06,420
and then high throughput patterns.

1348
00:54:06,420 --> 00:54:07,470
For different patterns,

1349
00:54:07,470 --> 00:54:10,980
we have a different dynamic
thresholds and error budgets.

1350
00:54:10,980 --> 00:54:11,813
With this,

1351
00:54:11,813 --> 00:54:13,800
there is no way of we
are missing any alerts.

1352
00:54:13,800 --> 00:54:14,730
In the static alerts,

1353
00:54:14,730 --> 00:54:16,230
we used to miss a lot of alerts

1354
00:54:16,230 --> 00:54:18,393
because we are tracking a static number.

1355
00:54:19,440 --> 00:54:21,270
And then we have this kind of a dashboard

1356
00:54:21,270 --> 00:54:22,680
in all our platforms

1357
00:54:22,680 --> 00:54:26,010
where we have multiple shelves
of five minute intervals.

1358
00:54:26,010 --> 00:54:28,770
These green boxes are where our thresholds

1359
00:54:28,770 --> 00:54:29,790
were not breached.

1360
00:54:29,790 --> 00:54:31,080
And white boxes is where

1361
00:54:31,080 --> 00:54:33,240
there were no requests
come to our platform

1362
00:54:33,240 --> 00:54:35,130
and they're considered as good.

1363
00:54:35,130 --> 00:54:38,640
And our red boxes is where
like we breached the threshold

1364
00:54:38,640 --> 00:54:41,910
and we constantly review this
dashboard in a every day,

1365
00:54:41,910 --> 00:54:43,233
every week, every month.

1366
00:54:45,210 --> 00:54:49,260
And another important aspect
about running reliable systems

1367
00:54:49,260 --> 00:54:50,190
and resilient systems

1368
00:54:50,190 --> 00:54:52,830
is using these bulkhead patterns.

1369
00:54:52,830 --> 00:54:55,440
We had an incident where our service task

1370
00:54:55,440 --> 00:54:56,910
and our observability tool

1371
00:54:56,910 --> 00:54:59,220
were both sharing the
same resources, right?

1372
00:54:59,220 --> 00:55:02,100
And our observability
backend was having a problem

1373
00:55:02,100 --> 00:55:05,370
and that was creating a back
pressure to our service task

1374
00:55:05,370 --> 00:55:08,250
and it was impacting the
throughput of the service task.

1375
00:55:08,250 --> 00:55:10,320
So that incident cost us,

1376
00:55:10,320 --> 00:55:12,690
taught us a lesson that we need to create

1377
00:55:12,690 --> 00:55:15,240
and create a bounded context
between these services

1378
00:55:15,240 --> 00:55:17,620
and these axillary things like

1379
00:55:18,546 --> 00:55:22,260
loggings and audits and
retries and things like that.

1380
00:55:22,260 --> 00:55:24,780
So we are moving away from

1381
00:55:24,780 --> 00:55:25,920
tightly couple services

1382
00:55:25,920 --> 00:55:29,250
into a bulkhead patterns for
all of our axillary services.

1383
00:55:29,250 --> 00:55:31,813
We use these sidecar patterns.

1384
00:55:31,813 --> 00:55:32,646
Guess what?

1385
00:55:32,646 --> 00:55:35,220
Because of these serverless adoption,

1386
00:55:35,220 --> 00:55:38,070
using this sidecar pattern
has become very easy for us.

1387
00:55:38,070 --> 00:55:40,260
Imagine if you're running on EC2 instance.

1388
00:55:40,260 --> 00:55:43,170
If any of these axillary
systems are having a pressure,

1389
00:55:43,170 --> 00:55:47,160
it'll completely occupy
your EC2 instances resources

1390
00:55:47,160 --> 00:55:49,290
and it'll impact your services.

1391
00:55:49,290 --> 00:55:50,790
With the serverless adoption,

1392
00:55:50,790 --> 00:55:54,990
we are able to create these
sidecar patterns really well.

1393
00:55:54,990 --> 00:55:56,690
Why are we doing all these things?

1394
00:55:57,690 --> 00:56:00,210
Like talked about in the beginning,

1395
00:56:00,210 --> 00:56:01,365
we all wanted to create the trust

1396
00:56:01,365 --> 00:56:02,198
between the customers, right?

1397
00:56:02,198 --> 00:56:04,650
So customers trust,
they're highly reliable,

1398
00:56:04,650 --> 00:56:07,320
highly resilient, always on,

1399
00:56:07,320 --> 00:56:08,610
always secure,

1400
00:56:08,610 --> 00:56:10,800
always scalable systems.

1401
00:56:10,800 --> 00:56:11,940
Not only that,

1402
00:56:11,940 --> 00:56:14,130
there are other Four Cs,

1403
00:56:14,130 --> 00:56:15,480
Customers trust,

1404
00:56:15,480 --> 00:56:17,190
and competitive edge,

1405
00:56:17,190 --> 00:56:19,410
The moment your systems are not reliable,

1406
00:56:19,410 --> 00:56:22,380
your customers will start
using the other systems

1407
00:56:22,380 --> 00:56:26,070
and for highly regulated
industries as such,

1408
00:56:26,070 --> 00:56:29,160
we have to be always
adhered to our compliance

1409
00:56:29,160 --> 00:56:30,900
and also the company's reputations.

1410
00:56:30,900 --> 00:56:31,860
These are very important

1411
00:56:31,860 --> 00:56:34,710
and that's why we take the
resiliency very seriously

1412
00:56:34,710 --> 00:56:37,020
and every single time
we make a code commit

1413
00:56:37,020 --> 00:56:38,820
and we will push it to the production,

1414
00:56:38,820 --> 00:56:41,820
please make sure that you are responsible

1415
00:56:41,820 --> 00:56:43,440
for the company's reputation

1416
00:56:43,440 --> 00:56:44,733
and the customer's trust.

1417
00:56:46,110 --> 00:56:48,720
That's pretty much what I got.

1418
00:56:48,720 --> 00:56:49,830
Thanks for tuning in

1419
00:56:49,830 --> 00:56:52,110
and I hope that you got
something out of this

1420
00:56:52,110 --> 00:56:53,520
and you will bring it back

1421
00:56:53,520 --> 00:56:57,120
and build reliable systems
in your organizations.

1422
00:56:57,120 --> 00:56:58,126
Thanks a lot.

1423
00:56:58,126 --> 00:57:01,293
(audience applauding)

