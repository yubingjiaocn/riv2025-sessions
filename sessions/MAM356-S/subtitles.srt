1
00:00:00,660 --> 00:00:01,590
- All right. Good morning everyone,

2
00:00:01,590 --> 00:00:05,670
and thanks for taking the time
to join our session today.

3
00:00:05,670 --> 00:00:06,750
I am Scott Warren,

4
00:00:06,750 --> 00:00:10,650
I run the America's Cloud
Center of Excellence

5
00:00:10,650 --> 00:00:12,240
for Capgemini,

6
00:00:12,240 --> 00:00:14,880
and I'm joined by Cedric Bordonne,

7
00:00:14,880 --> 00:00:17,730
who is a global technology
leader with AWS.

8
00:00:17,730 --> 00:00:20,520
And the project we're gonna
tell you about this morning,

9
00:00:20,520 --> 00:00:22,260
you know, Cedric and I both worked on

10
00:00:22,260 --> 00:00:24,450
and was jointly delivered for our customer

11
00:00:24,450 --> 00:00:26,610
by Capgemini and AWS.

12
00:00:26,610 --> 00:00:27,930
Sorry about the little
bit of the late start,

13
00:00:27,930 --> 00:00:29,550
we had some minor technical difficulties,

14
00:00:29,550 --> 00:00:32,640
but everything seems
to be working okay now.

15
00:00:32,640 --> 00:00:33,473
So, a quick run through

16
00:00:33,473 --> 00:00:35,850
of what we're gonna cover this morning,

17
00:00:35,850 --> 00:00:38,790
our customer that we built this AI POC for

18
00:00:38,790 --> 00:00:40,350
was called Fortive.

19
00:00:40,350 --> 00:00:42,030
So we're gonna spend a
few minutes explaining

20
00:00:42,030 --> 00:00:43,530
who Fortive is and what they do

21
00:00:43,530 --> 00:00:44,910
'cause I think it's really important

22
00:00:44,910 --> 00:00:46,440
to understand their business model,

23
00:00:46,440 --> 00:00:50,730
to understand what we were
trying to do with this POC.

24
00:00:50,730 --> 00:00:53,520
we're gonna do a deep dive
onto what we call Fort BRAIN.

25
00:00:53,520 --> 00:00:55,830
that is the actual project that we built.

26
00:00:55,830 --> 00:00:57,690
Talk a little bit about some
of the business outcomes

27
00:00:57,690 --> 00:00:59,370
we wanted to see out of this.

28
00:00:59,370 --> 00:01:02,040
Get into the AWS services and architecture

29
00:01:02,040 --> 00:01:04,470
that we used to build this product.

30
00:01:04,470 --> 00:01:08,520
Show you some real life footage
of the product in action,

31
00:01:08,520 --> 00:01:09,630
and then kind of walk through

32
00:01:09,630 --> 00:01:11,823
what's next for this project.

33
00:01:13,807 --> 00:01:15,120
So, jumping in, who is Fortive?

34
00:01:15,120 --> 00:01:19,230
So Fortive is a customer
of ours here at Capgemini.

35
00:01:19,230 --> 00:01:21,120
They're an industrial technology company

36
00:01:21,120 --> 00:01:23,820
headquartered in Everett, Washington.

37
00:01:23,820 --> 00:01:26,220
They have 18,000 global employees

38
00:01:26,220 --> 00:01:28,590
and operate in 50 countries.

39
00:01:28,590 --> 00:01:31,473
So really big company with
a big global footprint.

40
00:01:32,400 --> 00:01:35,100
And I think one of the key things here is

41
00:01:35,100 --> 00:01:38,130
Fortive does have an innovation
hub, they call the Fort.

42
00:01:38,130 --> 00:01:40,530
And that hub is where the idea
that you're gonna see today

43
00:01:40,530 --> 00:01:43,110
was kind of incubated and creative.

44
00:01:43,110 --> 00:01:45,780
For their main like lines
and areas of business,

45
00:01:45,780 --> 00:01:48,540
Fortive really focuses
on workplace safety,

46
00:01:48,540 --> 00:01:51,300
healthcare, environment safety,

47
00:01:51,300 --> 00:01:53,430
you know, servicing, frontline workers,

48
00:01:53,430 --> 00:01:54,750
scientists and patients

49
00:01:54,750 --> 00:01:56,900
in those 50 countries
all across the world.

50
00:01:58,800 --> 00:02:01,620
So really key to
understanding what we built

51
00:02:01,620 --> 00:02:03,720
with this BRAIN project

52
00:02:03,720 --> 00:02:06,570
is Fortive at the end of the
day is an operating company.

53
00:02:06,570 --> 00:02:08,670
So they've got all these
different companies

54
00:02:08,670 --> 00:02:12,300
that operate fairly
independently underneath Fortive,

55
00:02:12,300 --> 00:02:14,490
and you'll hear us reference
these operating companies

56
00:02:14,490 --> 00:02:16,680
throughout the project
as we go through today,

57
00:02:16,680 --> 00:02:19,620
and we'll use the term
OpCo to talk about them.

58
00:02:19,620 --> 00:02:22,320
But you can see that the
OpCos that Fortive runs today

59
00:02:22,320 --> 00:02:25,680
are really broken into
three main categories,

60
00:02:25,680 --> 00:02:27,840
intelligent operating solutions,

61
00:02:27,840 --> 00:02:31,050
so think of facilities
and asset management,

62
00:02:31,050 --> 00:02:34,650
you know, in office
buildings, construction sites,

63
00:02:34,650 --> 00:02:39,030
detection and safety, anything
around operating real estate.

64
00:02:39,030 --> 00:02:41,490
Precision technologies
is really focused on

65
00:02:41,490 --> 00:02:43,860
electronic and test measurements.

66
00:02:43,860 --> 00:02:45,870
There's one company here, Fluke,
that you may have heard of,

67
00:02:45,870 --> 00:02:47,130
that is really the focus here,

68
00:02:47,130 --> 00:02:48,390
and they work in the healthcare

69
00:02:48,390 --> 00:02:50,880
and all these all different industries.

70
00:02:50,880 --> 00:02:54,090
And then their final pillar
of OpCos that Fortive runs

71
00:02:54,090 --> 00:02:57,120
is specific to healthcare solutions.

72
00:02:57,120 --> 00:02:59,910
So think infection prevention.

73
00:02:59,910 --> 00:03:02,760
So during COVID this
is a really big thing,

74
00:03:02,760 --> 00:03:06,090
wearing your masks and
sanitizer and things like that.

75
00:03:06,090 --> 00:03:07,470
Surgical asset management

76
00:03:07,470 --> 00:03:08,940
kind of falling in line with what they do

77
00:03:08,940 --> 00:03:12,660
with facilities management, but
specific to surgery centers.

78
00:03:12,660 --> 00:03:15,900
And then end-to-end clinical
productivity solutions for,

79
00:03:15,900 --> 00:03:18,300
again, healthcare facilities.

80
00:03:18,300 --> 00:03:20,850
So as we go through
this, you'll kind of see

81
00:03:20,850 --> 00:03:22,530
how all of these companies come together

82
00:03:22,530 --> 00:03:25,383
within this AI POC that we
helped Fortive build out.

83
00:03:26,790 --> 00:03:29,100
So talking about BRAIN,

84
00:03:29,100 --> 00:03:30,870
so if we look across

85
00:03:30,870 --> 00:03:32,640
all of these different operating companies

86
00:03:32,640 --> 00:03:34,470
that Fortive operates,

87
00:03:34,470 --> 00:03:38,400
they all have different types
in different data stores.

88
00:03:38,400 --> 00:03:41,460
So think structured data,
you know, Postgres databases,

89
00:03:41,460 --> 00:03:44,640
SQL databases, Oracle databases,

90
00:03:44,640 --> 00:03:47,490
things like Snowflake
for data warehousing,

91
00:03:47,490 --> 00:03:49,020
each different OpCo could have

92
00:03:49,020 --> 00:03:51,210
all different kinds of structured data.

93
00:03:51,210 --> 00:03:53,220
Same when we get to unstructured data,

94
00:03:53,220 --> 00:03:56,040
so think of things like SharePoint, Jira,

95
00:03:56,040 --> 00:03:58,290
any other kind of document storage.

96
00:03:58,290 --> 00:04:02,700
And then the third type of
data in scope for this POC

97
00:04:02,700 --> 00:04:05,040
was software engineering data.

98
00:04:05,040 --> 00:04:06,510
So every one of the OpCos

99
00:04:06,510 --> 00:04:08,430
that you saw on that previous slide

100
00:04:08,430 --> 00:04:10,110
has their own IT department,

101
00:04:10,110 --> 00:04:11,610
you know, they may be using GitHub,

102
00:04:11,610 --> 00:04:13,560
they may be using an Atlassian product,

103
00:04:13,560 --> 00:04:15,540
you know, different code repositories

104
00:04:15,540 --> 00:04:17,763
across all of these different OpCos.

105
00:04:19,161 --> 00:04:20,490
And so what Fortive wanted to do

106
00:04:20,490 --> 00:04:23,160
with the first iteration of BRAIN was

107
00:04:23,160 --> 00:04:26,370
give all of these different
operating companies a,

108
00:04:26,370 --> 00:04:28,920
like seamless integrated platform

109
00:04:28,920 --> 00:04:30,204
where they could build chatbots

110
00:04:30,204 --> 00:04:34,080
for their end users, for
their non-technical staff,

111
00:04:34,080 --> 00:04:36,420
for their software engineering staff.

112
00:04:36,420 --> 00:04:37,890
But Fortive really wanted to do this

113
00:04:37,890 --> 00:04:40,710
in a way that was completely structured,

114
00:04:40,710 --> 00:04:43,560
secure, governed
properly, and standardized

115
00:04:43,560 --> 00:04:46,140
across all of the different
operating companies.

116
00:04:46,140 --> 00:04:48,660
So the key here is we didn't want OpCo one

117
00:04:48,660 --> 00:04:50,850
going building one chatbot,

118
00:04:50,850 --> 00:04:53,160
these guys building in another
platform, another platform,

119
00:04:53,160 --> 00:04:54,690
they really wanted to bring

120
00:04:54,690 --> 00:04:56,460
all of this into a centralized way

121
00:04:56,460 --> 00:04:57,537
for all of the operating companies,

122
00:04:57,537 --> 00:05:00,858
and so that is the idea
behind Fortive BRAIN.

123
00:05:00,858 --> 00:05:03,840
There was a previous
iteration of the BRAIN product

124
00:05:03,840 --> 00:05:05,430
and not on AWS,

125
00:05:05,430 --> 00:05:07,090
but one of the big blockers there is,

126
00:05:07,090 --> 00:05:09,510
is it was static versus dynamic.

127
00:05:09,510 --> 00:05:11,760
So if you think of building a chatbot

128
00:05:11,760 --> 00:05:13,020
against a structured database,

129
00:05:13,020 --> 00:05:15,153
something like SQL Server or Postgres,

130
00:05:16,170 --> 00:05:18,000
there's gonna be changes in that database,

131
00:05:18,000 --> 00:05:19,950
not just day-to-day with the actual data,

132
00:05:19,950 --> 00:05:22,260
but the structure of the database itself,

133
00:05:22,260 --> 00:05:24,990
the table structure, the
schema, things like that.

134
00:05:24,990 --> 00:05:27,780
So in the older versions
of the BRAIN product,

135
00:05:27,780 --> 00:05:30,480
anytime a change was made to
the structure of that database

136
00:05:30,480 --> 00:05:31,920
or the data source,

137
00:05:31,920 --> 00:05:33,780
we'd have to go and
remap the whole schema,

138
00:05:33,780 --> 00:05:36,900
train all the new data every
time a change was made.

139
00:05:36,900 --> 00:05:40,585
So really building this new
next generation BRAIN on AWS

140
00:05:40,585 --> 00:05:43,770
was completely focused on
how do we turn that dynamic.

141
00:05:43,770 --> 00:05:47,040
So if a change is made
in that OpCo database,

142
00:05:47,040 --> 00:05:50,880
the tool and the AWS platform
takes care of that on its own.

143
00:05:50,880 --> 00:05:51,973
And the actual OpCo,

144
00:05:51,973 --> 00:05:54,240
you know, building and
operating the chatbot

145
00:05:54,240 --> 00:05:56,482
doesn't have to worry about that.

146
00:05:56,482 --> 00:05:59,430
Really wanted to, again, standardize this,

147
00:05:59,430 --> 00:06:01,200
so each of these different OpCos

148
00:06:01,200 --> 00:06:03,630
has different IT departments,

149
00:06:03,630 --> 00:06:07,290
different levels of expertise,
different skill sets,

150
00:06:07,290 --> 00:06:10,449
so making this very user-friendly,

151
00:06:10,449 --> 00:06:13,080
as much auto-generated code as possible

152
00:06:13,080 --> 00:06:15,480
was really key to this project.

153
00:06:15,480 --> 00:06:17,370
And we really wanted to make this

154
00:06:17,370 --> 00:06:20,640
as close to a serverless
architecture as possible.

155
00:06:20,640 --> 00:06:23,850
So, you know, scalable,
on-demand, cost effective

156
00:06:23,850 --> 00:06:25,653
on how we built this on AWS.

157
00:06:28,230 --> 00:06:29,910
So some of the challenges we face

158
00:06:29,910 --> 00:06:31,950
as we started digging into this project

159
00:06:31,950 --> 00:06:35,190
and how we would architect
it on the AWS platform,

160
00:06:35,190 --> 00:06:37,860
you know, there wasn't
really a set standard

161
00:06:37,860 --> 00:06:39,240
on how to implement this type of thing

162
00:06:39,240 --> 00:06:42,270
across all of the
different OpCos on Fortive,

163
00:06:42,270 --> 00:06:43,250
they worked in silos,

164
00:06:43,250 --> 00:06:45,120
so we really had to build something,

165
00:06:45,120 --> 00:06:46,320
again, that was easy to use

166
00:06:46,320 --> 00:06:48,087
and these different
operators could come in

167
00:06:48,087 --> 00:06:50,040
and build it on their own.

168
00:06:50,040 --> 00:06:53,370
There was no common UI
for building any of this,

169
00:06:53,370 --> 00:06:56,209
so that was a key to the
project is to build out a UI

170
00:06:56,209 --> 00:06:58,320
where you could add new data sources

171
00:06:58,320 --> 00:07:00,420
on the fly very quickly.

172
00:07:00,420 --> 00:07:02,910
And then, like I mentioned earlier,

173
00:07:02,910 --> 00:07:04,410
and one of the keys to this project is,

174
00:07:04,410 --> 00:07:06,813
make it dynamic versus static.

175
00:07:08,520 --> 00:07:11,220
So some of the project
requirements we dug into,

176
00:07:11,220 --> 00:07:12,180
kind of like I mentioned,

177
00:07:12,180 --> 00:07:15,060
so a centralized platform,
governance security baked in

178
00:07:15,060 --> 00:07:17,953
so the different OpCos didn't
have to worry about it.

179
00:07:17,953 --> 00:07:19,710
We needed specific training

180
00:07:19,710 --> 00:07:21,810
for each of the OpCos data repositories,

181
00:07:21,810 --> 00:07:23,790
and we had to segregate that data as well.

182
00:07:23,790 --> 00:07:25,590
So, you know, company one's data

183
00:07:25,590 --> 00:07:28,920
couldn't be accessed by company
two, so on and so forth.

184
00:07:28,920 --> 00:07:32,280
We needed real-time or close
to real-time data access,

185
00:07:32,280 --> 00:07:34,620
and, you know, everything as a service

186
00:07:34,620 --> 00:07:36,723
or a standardized API economy.

187
00:07:37,800 --> 00:07:39,840
So I think this diagram does a good job

188
00:07:39,840 --> 00:07:41,910
of kind of showing conceptually

189
00:07:41,910 --> 00:07:44,193
what the BRAIN product
is and how it works.

190
00:07:45,030 --> 00:07:47,457
So on the left hand side are
actually where the chatbots

191
00:07:47,457 --> 00:07:50,370
and the different operating
companies would sit.

192
00:07:50,370 --> 00:07:52,260
So you can kind of see
the different applications

193
00:07:52,260 --> 00:07:53,160
and where those could live,

194
00:07:53,160 --> 00:07:56,280
so, you know, things
like Teams and Outlook.

195
00:07:56,280 --> 00:07:58,410
And then more importantly
and what we focused on

196
00:07:58,410 --> 00:08:01,440
for the first version
and the POC for this was

197
00:08:01,440 --> 00:08:04,110
streaming through Streamlit or JavaScript

198
00:08:04,110 --> 00:08:06,780
primarily into web applications.

199
00:08:06,780 --> 00:08:09,540
But there is, you know,
plans to expand this

200
00:08:09,540 --> 00:08:11,250
and have a lot more different consumers

201
00:08:11,250 --> 00:08:12,453
where chatbots can sit.

202
00:08:13,440 --> 00:08:16,560
On the right hand side is all
of the different types of data

203
00:08:16,560 --> 00:08:18,810
that we eventually want
to build into this product

204
00:08:18,810 --> 00:08:19,683
and platform.

205
00:08:20,520 --> 00:08:22,980
And so you can see these data structures

206
00:08:22,980 --> 00:08:25,170
sit within the OpCos IT's environments,

207
00:08:25,170 --> 00:08:27,488
and it can be lots of different
things from unstructured,

208
00:08:27,488 --> 00:08:32,040
you know, Jira S3 buckets,
SharePoint, Confluence,

209
00:08:32,040 --> 00:08:33,870
to the structured data
where we're looking at,

210
00:08:33,870 --> 00:08:38,340
you know, Amazon RDS, Oracle
databases, MySQL, Snowflake,

211
00:08:38,340 --> 00:08:40,590
and then the software engineering data,

212
00:08:40,590 --> 00:08:44,670
you know, GitLab, Bitbucket,
other Atlassian products.

213
00:08:44,670 --> 00:08:46,980
So the key here is we have to

214
00:08:46,980 --> 00:08:49,560
build this platform in a
way where it can ingest

215
00:08:49,560 --> 00:08:53,880
any of these data sources
within the OpCos IT environment.

216
00:08:53,880 --> 00:08:55,680
So we've really picked three to start

217
00:08:55,680 --> 00:08:57,300
for the first version in the POC

218
00:08:57,300 --> 00:09:00,090
that you'll see when we
dig into the architecture.

219
00:09:00,090 --> 00:09:03,090
So for unstructured data,
we chose SharePoint,

220
00:09:03,090 --> 00:09:05,940
for structured data we did Amazon RDS,

221
00:09:05,940 --> 00:09:10,920
and for the software
engineering data we did GitLab

222
00:09:10,920 --> 00:09:12,070
for the source control.

223
00:09:13,320 --> 00:09:15,000
And then in the middle you can see

224
00:09:15,000 --> 00:09:18,030
where the actual AWS
environment sits for this.

225
00:09:18,030 --> 00:09:23,030
And so this is the Fortive
BRAIN that we built out

226
00:09:23,280 --> 00:09:27,600
where the OpCo user can
input their data source

227
00:09:27,600 --> 00:09:30,000
that ingests the data into our Bedrock,

228
00:09:30,000 --> 00:09:31,680
trains against the data,

229
00:09:31,680 --> 00:09:35,580
and then allows the end user
application to access that

230
00:09:35,580 --> 00:09:36,543
via streaming.

231
00:09:40,800 --> 00:09:43,260
So as we began to build
and architect this,

232
00:09:43,260 --> 00:09:45,240
there was some business outcomes

233
00:09:45,240 --> 00:09:47,133
we really wanted to focus on Fortive.

234
00:09:48,180 --> 00:09:50,280
First I mentioned during the POC itself

235
00:09:50,280 --> 00:09:53,250
was the web interface
with chat functionality.

236
00:09:53,250 --> 00:09:57,630
So we wanted end users
within the operating company

237
00:09:57,630 --> 00:10:00,090
to be able to interact with this data

238
00:10:00,090 --> 00:10:02,703
via chatbot using natural language.

239
00:10:03,570 --> 00:10:06,090
We really had to make this
scalable and performant.

240
00:10:06,090 --> 00:10:09,290
So there couldn't be lag time
between accessing the data,

241
00:10:09,290 --> 00:10:11,400
it couldn't take hours to ingest the data

242
00:10:11,400 --> 00:10:13,383
into the AWS environment.

243
00:10:14,370 --> 00:10:16,710
And really, I think like
a lot of folks here,

244
00:10:16,710 --> 00:10:19,200
we had to prove that this all worked

245
00:10:19,200 --> 00:10:20,760
to get investment and funding

246
00:10:20,760 --> 00:10:22,380
and buy-in from executive leadership

247
00:10:22,380 --> 00:10:24,210
to continue on with the
rest of this project.

248
00:10:24,210 --> 00:10:27,270
So it was really important
to show quick wins,

249
00:10:27,270 --> 00:10:29,970
how we can do this in a very
rapid and scalable manner,

250
00:10:29,970 --> 00:10:33,540
give them that dynamic data
access that they wanted

251
00:10:33,540 --> 00:10:35,400
to be able to, you know,
prove out to leadership

252
00:10:35,400 --> 00:10:37,743
that we could move
forward with this project.

253
00:10:39,690 --> 00:10:41,880
Some of the guiding principles
we used on this project

254
00:10:41,880 --> 00:10:43,953
as we dug into development,

255
00:10:44,820 --> 00:10:46,590
first and foremost,
everything needed to be

256
00:10:46,590 --> 00:10:48,360
infrastructure as code.

257
00:10:48,360 --> 00:10:51,497
So the primary method we
used for that was the AWS CDK

258
00:10:52,388 --> 00:10:54,510
and Terraform for the infrastructure.

259
00:10:54,510 --> 00:10:57,060
So we wanted no access
to the AWS consoles,

260
00:10:57,060 --> 00:10:59,310
everything done
programmatically through code,

261
00:10:59,310 --> 00:11:01,680
through deployment pipelines.

262
00:11:01,680 --> 00:11:05,730
GitHub was our data
source, our code repository

263
00:11:05,730 --> 00:11:08,280
for all of the project that we built out.

264
00:11:08,280 --> 00:11:12,000
Data processing was done
using the AWS Glue service.

265
00:11:12,000 --> 00:11:15,000
All of the knowledge base
creation for ingesting the data in

266
00:11:15,000 --> 00:11:18,153
and running the learning was
Amazon Bedrock knowledge bases.

267
00:11:19,050 --> 00:11:21,570
We did agentic
implementation using Bedrock

268
00:11:21,570 --> 00:11:24,750
that you'll see a little
bit more in a few minutes.

269
00:11:24,750 --> 00:11:27,000
There was some existing front end

270
00:11:27,000 --> 00:11:29,697
for setting up new data sources
and building new chatbots,

271
00:11:29,697 --> 00:11:32,718
but we pretty greatly enhanced that.

272
00:11:32,718 --> 00:11:36,540
And we knew we needed basic
telemetry and UX metrics

273
00:11:36,540 --> 00:11:38,550
using some of the existing Fortive systems

274
00:11:38,550 --> 00:11:40,100
that existed out there already.

275
00:11:42,210 --> 00:11:44,550
All right, so with that, I'm
going to hand it over to Cedric

276
00:11:44,550 --> 00:11:47,820
who's gonna dive into how we
actually built this on AWS

277
00:11:47,820 --> 00:11:49,170
and which services we used.

278
00:11:50,190 --> 00:11:51,060
- Thank you Scott.

279
00:11:51,060 --> 00:11:52,680
Hi everybody.

280
00:11:52,680 --> 00:11:56,373
So before I dive into the architecture,

281
00:11:57,270 --> 00:11:59,430
I just want to give two or three words

282
00:11:59,430 --> 00:12:02,853
on what we set out for
this eight weeks POC.

283
00:12:04,080 --> 00:12:08,070
So our first objective was
really to prove to Fortive

284
00:12:08,070 --> 00:12:12,620
that we can interact with three
different data sources type,

285
00:12:12,620 --> 00:12:17,313
so structured, unstructured
and repositories.

286
00:12:19,260 --> 00:12:21,390
We had multiple requirements.

287
00:12:21,390 --> 00:12:24,990
So the first requirement is
that we wanted a familiar WebUI

288
00:12:24,990 --> 00:12:29,990
so that the Fortive users can
adopt the solution quickly.

289
00:12:35,460 --> 00:12:37,530
The second requirements

290
00:12:37,530 --> 00:12:41,670
was that we needed it to respond quickly.

291
00:12:41,670 --> 00:12:44,430
When you are interacting with humans,

292
00:12:44,430 --> 00:12:47,550
you need answers in
seconds, not in minutes.

293
00:12:47,550 --> 00:12:50,500
We wanted to keep it very conversational

294
00:12:51,510 --> 00:12:55,173
and not asking and coming
back a few minutes later,

295
00:12:57,660 --> 00:12:59,880
as you can see in the
architecture in the center,

296
00:12:59,880 --> 00:13:03,180
you don't see Amazon Bedrock AgentCore.

297
00:13:03,180 --> 00:13:05,910
So this is not an architectural decision,

298
00:13:05,910 --> 00:13:07,530
this is a delivery decision.

299
00:13:07,530 --> 00:13:09,610
So AgentCore went to GA

300
00:13:10,500 --> 00:13:14,340
at the very middle of the
development cycle of that POC.

301
00:13:14,340 --> 00:13:18,120
So we decided to stick
to what we were building

302
00:13:18,120 --> 00:13:21,120
and to move to AgentCore
in the future state.

303
00:13:21,120 --> 00:13:26,120
So with AI moving so fast
and innovating at that pace,

304
00:13:27,600 --> 00:13:30,420
we all should be ready to iterate.

305
00:13:30,420 --> 00:13:34,410
We cannot change every time
so build something, finish it,

306
00:13:34,410 --> 00:13:38,583
prove and iterate in the next version.

307
00:13:40,470 --> 00:13:43,660
So that being said, at the left hand side

308
00:13:44,907 --> 00:13:48,630
of the architecture, we
have the customer view.

309
00:13:48,630 --> 00:13:52,023
So the customer view connects to WebUI,

310
00:13:52,860 --> 00:13:57,300
the web application runs into a container

311
00:13:57,300 --> 00:13:59,460
that is hosted on Fargate.

312
00:13:59,460 --> 00:14:02,010
So we did the decision of going Fargate

313
00:14:02,010 --> 00:14:05,580
to have the advantage of
containers, the flexibility,

314
00:14:05,580 --> 00:14:08,700
without having to deal with
the underlying complexibility

315
00:14:08,700 --> 00:14:11,133
of managing all the
container infrastructures.

316
00:14:12,480 --> 00:14:16,890
It is published to the
end user using CloudFront

317
00:14:16,890 --> 00:14:21,420
and the web application
firewall for the security

318
00:14:21,420 --> 00:14:23,340
and the delivery content.

319
00:14:23,340 --> 00:14:24,870
From a backend perspective,

320
00:14:24,870 --> 00:14:29,040
we decided to use API gateway and Lambda.

321
00:14:29,040 --> 00:14:32,250
So to orchestrate the different requests

322
00:14:32,250 --> 00:14:36,540
and to add a bit of
context to those requests.

323
00:14:36,540 --> 00:14:41,460
Lambda passed the request to a first agent

324
00:14:41,460 --> 00:14:44,160
that is the SQL query agent,

325
00:14:44,160 --> 00:14:49,160
and that agent is responsible
of the prompt pre-processing.

326
00:14:50,100 --> 00:14:53,730
So this agent makes sense of the request,

327
00:14:53,730 --> 00:14:55,350
it adds some meaning to it,

328
00:14:55,350 --> 00:14:57,900
and is able to understand

329
00:14:57,900 --> 00:15:00,510
to which data source he needs to tap in

330
00:15:00,510 --> 00:15:03,120
to give an answer to the customer.

331
00:15:03,120 --> 00:15:08,120
So once it decided to which
data source it need to tap,

332
00:15:08,130 --> 00:15:10,200
it passed the request to a second agent

333
00:15:10,200 --> 00:15:13,410
that is in the action group agent.

334
00:15:13,410 --> 00:15:17,280
And those agents connect to
the different data sources.

335
00:15:17,280 --> 00:15:20,318
As you can see, we have
data sources as GitHub

336
00:15:20,318 --> 00:15:25,318
and PostgreSQL connected via an MCP server

337
00:15:25,320 --> 00:15:28,110
and we have a SharePoint
connected directly to Bedrock

338
00:15:28,110 --> 00:15:30,273
using a Bedrock knowledge base.

339
00:15:32,880 --> 00:15:37,880
So the whole session is
about static to dynamic.

340
00:15:38,130 --> 00:15:42,660
So what makes that solution
dynamic is really the MCP.

341
00:15:42,660 --> 00:15:47,010
So MCP stands for Model Context Protocol,

342
00:15:47,010 --> 00:15:52,010
and this is a standard for
connecting AI models to tools.

343
00:15:53,040 --> 00:15:56,190
So it's not only about connecting

344
00:15:56,190 --> 00:16:01,190
to a data source and
querying the data source,

345
00:16:01,230 --> 00:16:04,620
it's also about interacting
with that data source.

346
00:16:04,620 --> 00:16:06,288
So if you are a sales manager,

347
00:16:06,288 --> 00:16:10,500
and if you want to know
the results of yesterday,

348
00:16:10,500 --> 00:16:13,830
you don't want to to wait next week

349
00:16:13,830 --> 00:16:18,420
for a knowledge base to be
updated, indexed and everything.

350
00:16:18,420 --> 00:16:21,510
So with MCP, you are
targeting the live database

351
00:16:21,510 --> 00:16:26,510
and you can retrieve
the live data directly.

352
00:16:26,910 --> 00:16:31,910
So again, MCP is not only
about querying but it's also,

353
00:16:32,280 --> 00:16:35,700
it gives a whole set of
tools so you can interact

354
00:16:35,700 --> 00:16:40,260
with the external systems.

355
00:16:40,260 --> 00:16:43,580
So you can, for instance, start a workflow

356
00:16:43,580 --> 00:16:46,440
in an external tool,

357
00:16:46,440 --> 00:16:48,840
you can commit in Git,

358
00:16:48,840 --> 00:16:53,100
you can add a new role in PostgreSQL,

359
00:16:53,100 --> 00:16:54,690
so it's not only about querying,

360
00:16:54,690 --> 00:16:58,443
it's also about interacting
with the data source.

361
00:17:01,080 --> 00:17:03,840
For SharePoint, we decided to go with

362
00:17:03,840 --> 00:17:06,330
Amazon Bedrock knowledge base

363
00:17:06,330 --> 00:17:08,880
because here we have a lots of documents

364
00:17:08,880 --> 00:17:12,450
and there is no need
for it to be real-time.

365
00:17:12,450 --> 00:17:17,450
So you want to have a quick answer

366
00:17:17,850 --> 00:17:19,770
and to pass and to go through

367
00:17:19,770 --> 00:17:22,470
unstructured data and documents,

368
00:17:22,470 --> 00:17:25,920
we are using Amazon Bedrock knowledge base

369
00:17:25,920 --> 00:17:30,370
together with OpenSearch Vector database

370
00:17:32,029 --> 00:17:35,940
so that we can retrieve the data quickly

371
00:17:35,940 --> 00:17:38,403
and keep it short for the answer.

372
00:17:40,410 --> 00:17:41,810
And not only...

373
00:17:43,380 --> 00:17:46,770
so for SharePoint there is
a knowledge base connector,

374
00:17:46,770 --> 00:17:47,943
native connector,

375
00:17:49,170 --> 00:17:51,333
but due to the time
constraint of this POC,

376
00:17:52,200 --> 00:17:55,440
we decided to go with the
web crawler connector,

377
00:17:55,440 --> 00:17:58,410
which make the trick for the
POC but in the next iteration

378
00:17:58,410 --> 00:18:02,100
we are gonna go with the Amazon
SharePoint native connector.

379
00:18:02,100 --> 00:18:03,360
Why did we do that choice?

380
00:18:03,360 --> 00:18:06,240
Just because of a timeframe constraint.

381
00:18:06,240 --> 00:18:08,490
We had no time to go through

382
00:18:08,490 --> 00:18:12,453
all security requirements
and checks from the customer.

383
00:18:14,940 --> 00:18:16,953
So what does it look like?

384
00:18:18,990 --> 00:18:23,990
So as Scott mentioned,
Fortive has many companies,

385
00:18:24,360 --> 00:18:26,640
every OpCo has its own IT,

386
00:18:26,640 --> 00:18:29,100
every IT has its own maturity

387
00:18:29,100 --> 00:18:33,927
on how to handle AWS
infrastructure AI solutions.

388
00:18:36,450 --> 00:18:40,740
So we made it very simple for
them to add a new data source

389
00:18:40,740 --> 00:18:43,320
and then to interact with a new chatbot.

390
00:18:43,320 --> 00:18:47,880
So as you can see there
is a seven-step workflow

391
00:18:47,880 --> 00:18:52,293
toward any new data sources
connecting to a new MCP.

392
00:18:55,530 --> 00:18:58,800
Let's say you are the sales
manager I mentioned before

393
00:18:58,800 --> 00:19:03,800
and you want to know the
sales report from yesterday.

394
00:19:04,380 --> 00:19:07,170
So you just asked that simple
question to the chatbot,

395
00:19:07,170 --> 00:19:11,310
the chatbot answer to you with the data,

396
00:19:11,310 --> 00:19:13,140
it make an analysis of the data,

397
00:19:13,140 --> 00:19:15,150
it give you a quick analysis,

398
00:19:15,150 --> 00:19:19,740
and then it also gives you
the SQL query that it chooses

399
00:19:19,740 --> 00:19:22,233
through the database to get the data back.

400
00:19:23,130 --> 00:19:24,870
This is important because

401
00:19:24,870 --> 00:19:27,270
it helps you also refine your prompt.

402
00:19:27,270 --> 00:19:31,170
So if you want more
precise data using that,

403
00:19:31,170 --> 00:19:32,370
you can refine your prompt

404
00:19:32,370 --> 00:19:34,560
and you can ask a better
question to the chatbot

405
00:19:34,560 --> 00:19:37,413
so that you are closer to
the answer you're expecting.

406
00:19:42,000 --> 00:19:45,480
For GitHub, you are a software engineer

407
00:19:45,480 --> 00:19:49,500
and you want to know how
your code repository looks.

408
00:19:49,500 --> 00:19:52,020
So you can ask to list a code repository

409
00:19:52,020 --> 00:19:54,210
and to give you some recommendation.

410
00:19:54,210 --> 00:19:58,020
And the chatbot we've asked
will answer you with a branch,

411
00:19:58,020 --> 00:19:59,700
the structure of the branch,

412
00:19:59,700 --> 00:20:02,340
and if it looks good, if
you might change something,

413
00:20:02,340 --> 00:20:03,600
if you need to change something,

414
00:20:03,600 --> 00:20:06,200
if you need to add something
or to remove something.

415
00:20:08,773 --> 00:20:12,300
SharePoint chatbot, here
it's all around the company.

416
00:20:12,300 --> 00:20:14,850
If your HR, customer support,

417
00:20:14,850 --> 00:20:19,850
you wanna know if you have a
question on a product manual,

418
00:20:21,270 --> 00:20:22,980
a user manual product,

419
00:20:22,980 --> 00:20:25,380
you can just ask the
question to the chatbot

420
00:20:25,380 --> 00:20:28,560
and it'll not only give you
the link to the document,

421
00:20:28,560 --> 00:20:30,990
but it'll also give you the real answer

422
00:20:30,990 --> 00:20:34,833
that you are looking for
with the source it tapped in.

423
00:20:36,990 --> 00:20:39,293
That being said, I hand
over back to Scott.

424
00:20:40,221 --> 00:20:41,221
- Thank you.

425
00:20:44,880 --> 00:20:46,470
Yeah, thank you Cedric.

426
00:20:46,470 --> 00:20:49,680
So as you there we were
able to build out this POC

427
00:20:49,680 --> 00:20:52,590
like Cedric mentioned
in about eight weeks,

428
00:20:52,590 --> 00:20:55,470
where we had the three
different data source types,

429
00:20:55,470 --> 00:20:57,840
the OpCos could go in and
add those data sources

430
00:20:57,840 --> 00:21:00,360
through the UI that you saw.

431
00:21:00,360 --> 00:21:02,670
So I think it's a pretty powerful thing

432
00:21:02,670 --> 00:21:04,710
to be able to stand up this type of system

433
00:21:04,710 --> 00:21:06,210
supporting lots of different companies

434
00:21:06,210 --> 00:21:08,250
in an eight week timeframe.

435
00:21:08,250 --> 00:21:09,720
So I think we hit all the targets

436
00:21:09,720 --> 00:21:12,060
and achieved the goals we wanted there,

437
00:21:12,060 --> 00:21:14,340
but if we look at kind of
what the next steps are

438
00:21:14,340 --> 00:21:17,250
for this application and
how we wish to expand it,

439
00:21:17,250 --> 00:21:19,470
first and foremost, that
diagram I showed you

440
00:21:19,470 --> 00:21:20,303
a little bit earlier

441
00:21:20,303 --> 00:21:22,380
with all of the different
data source types,

442
00:21:22,380 --> 00:21:24,900
there was Oracle on there, and Jira,

443
00:21:24,900 --> 00:21:27,210
and Confluence, and many other things,

444
00:21:27,210 --> 00:21:28,410
so that's kind of the next step

445
00:21:28,410 --> 00:21:30,300
is adding more and more data source types

446
00:21:30,300 --> 00:21:32,220
that are standardized and used

447
00:21:32,220 --> 00:21:35,340
by the different operating
companies that you saw.

448
00:21:35,340 --> 00:21:37,860
And the MPC servers
that Cedric talked about

449
00:21:37,860 --> 00:21:38,940
are gonna be key for that

450
00:21:38,940 --> 00:21:42,093
in keeping this a dynamic
versus static type application.

451
00:21:44,610 --> 00:21:46,260
The next thing we're gonna add is

452
00:21:47,160 --> 00:21:52,160
really utilizing some of
the newer AWS services.

453
00:21:52,200 --> 00:21:55,290
That's one of the tricky
things about doing applications

454
00:21:55,290 --> 00:21:57,720
and pilots like these is,

455
00:21:57,720 --> 00:22:00,120
you saw that during the development

456
00:22:00,120 --> 00:22:01,440
of the eight weeks itself,

457
00:22:01,440 --> 00:22:04,860
there was a new AWS service
AgentCore that was released

458
00:22:04,860 --> 00:22:06,660
that would've made this
a little bit easier

459
00:22:06,660 --> 00:22:07,863
and a little bit faster,

460
00:22:08,730 --> 00:22:09,563
but at some point

461
00:22:09,563 --> 00:22:12,060
we had to kind of stick
to our architecture,

462
00:22:12,060 --> 00:22:14,940
and finish it, show the value.

463
00:22:14,940 --> 00:22:17,820
But really implementing things
like AgentCore in the future

464
00:22:17,820 --> 00:22:20,520
are gonna be key to
expanding this application,

465
00:22:20,520 --> 00:22:22,470
making it more functional,

466
00:22:22,470 --> 00:22:24,970
and providing more value
to Fortive down the road.

467
00:22:26,220 --> 00:22:30,870
And then lastly, if anyone's
familiar with AWS Kiro,

468
00:22:30,870 --> 00:22:34,860
this was actually announced back in July

469
00:22:34,860 --> 00:22:36,360
during the development of this

470
00:22:36,360 --> 00:22:38,370
and went just generally
available from AWS,

471
00:22:38,370 --> 00:22:40,680
I think a couple weeks ago now.

472
00:22:40,680 --> 00:22:43,740
And that's one thing we're
testing and planning for

473
00:22:43,740 --> 00:22:46,380
on the next version of this product is,

474
00:22:46,380 --> 00:22:49,230
how do we integrate Kiro into

475
00:22:49,230 --> 00:22:53,460
build this even faster and
make it even easier to use.

476
00:22:53,460 --> 00:22:55,080
So some things we've started testing

477
00:22:55,080 --> 00:22:56,820
and are doing right now

478
00:22:56,820 --> 00:23:01,620
is using Kiro in the end-to-end
development lifecycle.

479
00:23:01,620 --> 00:23:05,580
So the first thing we can
do is create a sample DB

480
00:23:05,580 --> 00:23:06,480
to help test this.

481
00:23:06,480 --> 00:23:10,290
So, you know, a very
simple human language query

482
00:23:10,290 --> 00:23:11,640
was sent to Kiro in this case

483
00:23:11,640 --> 00:23:13,980
to create us a sample database with,

484
00:23:13,980 --> 00:23:17,400
you know, users, products and orders.

485
00:23:17,400 --> 00:23:19,440
So with that very simple knowledge query,

486
00:23:19,440 --> 00:23:22,290
you can see that it creates the SQL

487
00:23:22,290 --> 00:23:24,945
to actually build out this
database in a matter of seconds.

488
00:23:24,945 --> 00:23:26,940
So now we don't have to go

489
00:23:26,940 --> 00:23:30,690
and manually populate a
database for testing or use,

490
00:23:30,690 --> 00:23:33,240
we can actually build this
out real-time with Kiro.

491
00:23:34,380 --> 00:23:36,960
Next we can create the MCP servers

492
00:23:36,960 --> 00:23:39,180
to actually query against that data

493
00:23:39,180 --> 00:23:41,760
and load that data into Bedrock.

494
00:23:41,760 --> 00:23:44,100
So you can see the very
simple query here too

495
00:23:44,100 --> 00:23:46,590
is create an MCP server and tools

496
00:23:46,590 --> 00:23:49,500
for connecting to the
database that we just created.

497
00:23:49,500 --> 00:23:53,880
And so about two minutes,
Kiro generated the code,

498
00:23:53,880 --> 00:23:57,720
executed it, and spun up
the MCP servers where,

499
00:23:57,720 --> 00:24:00,120
I think, during our
development effort this was a,

500
00:24:00,120 --> 00:24:02,820
you know, several week process to do this

501
00:24:02,820 --> 00:24:04,830
and something like Kiro can take that down

502
00:24:04,830 --> 00:24:05,880
to a matter of minutes.

503
00:24:05,880 --> 00:24:08,340
So this is, again, in
a way we can show value

504
00:24:08,340 --> 00:24:10,110
to our business stakeholders

505
00:24:10,110 --> 00:24:12,600
and bring value into the
next phase of this product

506
00:24:12,600 --> 00:24:15,450
where we can significantly
reduce the development time

507
00:24:15,450 --> 00:24:16,893
using a tool like this.

508
00:24:19,380 --> 00:24:23,550
Next thing we can use Kiro
to do is actually test

509
00:24:23,550 --> 00:24:26,250
that the whole Fort
BRAIN product is working

510
00:24:26,250 --> 00:24:28,470
as specified using MCP.

511
00:24:28,470 --> 00:24:30,690
And so in this we just ask it, you know,

512
00:24:30,690 --> 00:24:33,120
how many products do
I have in my database?

513
00:24:33,120 --> 00:24:36,360
And this is gonna go out
and create all of the code

514
00:24:36,360 --> 00:24:39,830
needed to send the natural language query

515
00:24:39,830 --> 00:24:43,230
to Bedrock using the MCP servers

516
00:24:43,230 --> 00:24:44,580
to show back that in this case

517
00:24:44,580 --> 00:24:48,630
the test database that was
created with seven products.

518
00:24:48,630 --> 00:24:51,180
So think of this as something
we can significantly use

519
00:24:51,180 --> 00:24:54,030
to reduce testing time.

520
00:24:54,030 --> 00:24:56,790
So rather than having somebody
from the QA team come in

521
00:24:56,790 --> 00:24:58,770
and build manual test scripts

522
00:24:58,770 --> 00:25:01,080
or build automated test scripts,

523
00:25:01,080 --> 00:25:02,730
we can use Kiro in this example

524
00:25:02,730 --> 00:25:05,100
to create all of the test code

525
00:25:05,100 --> 00:25:07,560
and then build that into our deployment

526
00:25:07,560 --> 00:25:09,093
and testing pipelines.

527
00:25:10,620 --> 00:25:13,590
And then finally we can actually

528
00:25:13,590 --> 00:25:16,560
expand upon that testing even further,

529
00:25:16,560 --> 00:25:18,510
where in this case we
can actually give Kiro

530
00:25:18,510 --> 00:25:20,190
a business question.

531
00:25:20,190 --> 00:25:23,220
So in this test we asked
who are my best customers?

532
00:25:23,220 --> 00:25:26,480
And it's actually gonna build
a fairly complex SQL query

533
00:25:26,480 --> 00:25:28,290
in this scenario,

534
00:25:28,290 --> 00:25:30,960
where we can look at multiple
fields within that database

535
00:25:30,960 --> 00:25:33,270
that was created and see
that Alice in this case

536
00:25:33,270 --> 00:25:34,720
spent the most money with us.

537
00:25:35,580 --> 00:25:37,417
So we really wanted to highlight this as,

538
00:25:37,417 --> 00:25:41,880
you know, a very new and
cool innovation from AWS

539
00:25:41,880 --> 00:25:45,333
that just very, very recently
became available to everybody.

540
00:25:46,470 --> 00:25:48,240
But you know, as you are building

541
00:25:48,240 --> 00:25:51,600
these types of applications
that are pretty complex on AWS,

542
00:25:51,600 --> 00:25:54,843
have lots of different services
and architectures in play,

543
00:25:55,980 --> 00:25:57,270
you know, this is really gonna be

544
00:25:57,270 --> 00:26:00,000
kind of a cornerstone of
the future of this product

545
00:26:00,000 --> 00:26:02,700
and how we go and build
it out in the future.

546
00:26:02,700 --> 00:26:05,550
And it can hopefully
provide us some real value

547
00:26:05,550 --> 00:26:06,630
and speed to market

548
00:26:06,630 --> 00:26:08,883
as we continue to
progress with the product.

549
00:26:10,410 --> 00:26:13,890
With that, that kind of
covers Fortive and Fort BRAIN.

550
00:26:13,890 --> 00:26:16,770
So we've left some time
for questions at the end,

551
00:26:16,770 --> 00:26:18,150
so just go ahead and raise your hand

552
00:26:18,150 --> 00:26:19,400
if anyone has a question.

