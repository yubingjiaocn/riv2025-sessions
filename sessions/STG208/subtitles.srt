1
00:00:04,380 --> 00:00:05,430
- Hello.

2
00:00:05,430 --> 00:00:07,110
Hi, good afternoon.

3
00:00:07,110 --> 00:00:12,110
Welcome to STG 208, maximize
the value of cold data

4
00:00:13,590 --> 00:00:16,800
with Amazon S3 Glacier storage classes.

5
00:00:16,800 --> 00:00:19,500
My name is Gayla, I'm
very excited to be here

6
00:00:19,500 --> 00:00:21,660
and I'm really excited to see you all

7
00:00:21,660 --> 00:00:23,670
in the afternoon after lunch.

8
00:00:23,670 --> 00:00:26,070
So, give yourself a pat on the back

9
00:00:26,070 --> 00:00:28,290
for making it to your session.

10
00:00:28,290 --> 00:00:29,850
Just by a show of hands,

11
00:00:29,850 --> 00:00:32,973
how many folks are already using Glacier?

12
00:00:34,290 --> 00:00:36,720
Okay, we got a few of y'all out there.

13
00:00:36,720 --> 00:00:38,163
Good to know, good to know.

14
00:00:40,200 --> 00:00:44,880
We built this session for
people who are new to Glacier,

15
00:00:44,880 --> 00:00:47,670
for some folks who may be
occasionally use Glacier

16
00:00:47,670 --> 00:00:50,340
or familiar, and we even have some stuff

17
00:00:50,340 --> 00:00:53,460
for those archiving
storage pros out there.

18
00:00:53,460 --> 00:00:56,013
So, got a good session for you.

19
00:00:57,600 --> 00:01:01,860
You can think of this
session in three parts.

20
00:01:01,860 --> 00:01:04,830
In the first part, for
those new to Glacier,

21
00:01:04,830 --> 00:01:08,850
I'm gonna briefly cover why
your cold data is important.

22
00:01:08,850 --> 00:01:12,930
I'm gonna go over the S3
Glacier storage classes.

23
00:01:12,930 --> 00:01:16,680
I'm gonna talk to you about
getting your data into Glacier,

24
00:01:16,680 --> 00:01:18,303
getting your data out of Glacier.

25
00:01:18,303 --> 00:01:20,340
Then I'm gonna hand it over to Nitish,

26
00:01:20,340 --> 00:01:24,030
who's gonna cover some new S3 storage

27
00:01:24,030 --> 00:01:25,950
archive-related features.

28
00:01:25,950 --> 00:01:29,583
And then we'll have a
quick demo for the pros.

29
00:01:32,443 --> 00:01:34,890
All right, let's get started.

30
00:01:34,890 --> 00:01:39,183
S3 has hundreds of trillions
of objects currently,

31
00:01:40,020 --> 00:01:44,730
and one estimate out
there says that 70 to 80%

32
00:01:44,730 --> 00:01:49,410
of cold storage of data
everywhere is cold.

33
00:01:49,410 --> 00:01:53,850
What we mean by cold means it's
data that's rarely accessed,

34
00:01:53,850 --> 00:01:58,850
it's sometimes stored for
months, years, and even decades.

35
00:02:01,110 --> 00:02:02,760
And that's hard to ignore

36
00:02:02,760 --> 00:02:06,690
because that amount of data
is growing every single day.

37
00:02:06,690 --> 00:02:10,050
But what's interesting is
that the value of that data

38
00:02:10,050 --> 00:02:11,073
is increasing.

39
00:02:15,750 --> 00:02:17,040
Across industries,

40
00:02:17,040 --> 00:02:19,563
we're seeing something
very remarkable happen.

41
00:02:20,850 --> 00:02:25,080
Cold data isn't just store
and forgotten anymore,

42
00:02:25,080 --> 00:02:27,873
it's now become a catalyst for innovation.

43
00:02:30,480 --> 00:02:34,080
It's emerging as a huge
differentiating factor

44
00:02:34,080 --> 00:02:36,300
for a lot of businesses,

45
00:02:36,300 --> 00:02:40,440
and our customers are unlocking
all different types of ways

46
00:02:40,440 --> 00:02:42,270
to use what was months,

47
00:02:42,270 --> 00:02:44,790
what used to be considered dormant data

48
00:02:44,790 --> 00:02:47,463
and turning it into
actionable intelligence.

49
00:02:48,360 --> 00:02:50,670
So think bigger than storage.

50
00:02:50,670 --> 00:02:52,080
Let's say you have a whole bunch

51
00:02:52,080 --> 00:02:56,580
of historical handwritten
records and you wanna build,

52
00:02:56,580 --> 00:02:58,350
use it to train machine learning

53
00:02:58,350 --> 00:03:02,283
to recognize handwritten
data for those archives.

54
00:03:03,240 --> 00:03:08,010
You might wanna derive new
insights for archived analytics.

55
00:03:08,010 --> 00:03:10,590
Maybe you have a bunch of
financial or stock data

56
00:03:10,590 --> 00:03:13,290
that you want to use to
build new application.

57
00:03:13,290 --> 00:03:14,790
The opportunities are endless,

58
00:03:14,790 --> 00:03:17,640
and our customers are
every day identifying

59
00:03:17,640 --> 00:03:21,570
new opportunities for
this archive content.

60
00:03:21,570 --> 00:03:25,800
And it's about turning
every byte of cold data

61
00:03:25,800 --> 00:03:27,990
into an opportunity.

62
00:03:27,990 --> 00:03:30,030
And that's why Nitish and I are excited

63
00:03:30,030 --> 00:03:32,730
to go over S3 Glacier with you all.

64
00:03:32,730 --> 00:03:34,890
We're both on the service team

65
00:03:34,890 --> 00:03:37,710
and this is hugely important to us.

66
00:03:37,710 --> 00:03:39,600
So let's dive a little bit deep

67
00:03:39,600 --> 00:03:43,773
into what the Glacier storage classes are.

68
00:03:46,320 --> 00:03:48,000
Now, if you've been around S3,

69
00:03:48,000 --> 00:03:51,210
you've probably seen a
slide like this before.

70
00:03:51,210 --> 00:03:52,560
But for the new folks,

71
00:03:52,560 --> 00:03:54,783
let's talk about these storage classes.

72
00:03:55,710 --> 00:04:00,710
Imagine a continuum where you're
balancing two key factors.

73
00:04:00,720 --> 00:04:03,933
It's your access speed
and your cost efficiency.

74
00:04:05,340 --> 00:04:10,140
On your left, we have our
frequently access data,

75
00:04:10,140 --> 00:04:13,470
so it's ready for you in your
applications in milliseconds,

76
00:04:13,470 --> 00:04:14,703
but it's at a premium.

77
00:04:15,750 --> 00:04:17,580
And as we move right,

78
00:04:17,580 --> 00:04:20,223
we're entering into
progressively cooler territory.

79
00:04:23,370 --> 00:04:25,860
This is where your access
becomes less frequent,

80
00:04:25,860 --> 00:04:29,640
but your storage costs are
going to drop significantly.

81
00:04:29,640 --> 00:04:32,400
So let's break this
down a little bit more.

82
00:04:32,400 --> 00:04:37,050
With S3 Standard, you're
getting that millisecond data

83
00:04:37,050 --> 00:04:38,790
you need for your active, I'm sorry,

84
00:04:38,790 --> 00:04:41,790
that millisecond access
for your active data.

85
00:04:41,790 --> 00:04:45,180
And moving through S3
Standard and S3 One Zone

86
00:04:45,180 --> 00:04:46,260
and frequent access,

87
00:04:46,260 --> 00:04:49,650
you're already gonna start
seeing some cost savings.

88
00:04:49,650 --> 00:04:51,840
Then as your data continues to cool,

89
00:04:51,840 --> 00:04:53,640
you can move it through Glacier,

90
00:04:53,640 --> 00:04:56,613
and this is where the real
magic for cold data happens.

91
00:04:57,480 --> 00:05:00,090
You have Glacier Instant Retrieval,

92
00:05:00,090 --> 00:05:02,880
it's for that data that
you need in milliseconds

93
00:05:02,880 --> 00:05:05,010
that's still archived.

94
00:05:05,010 --> 00:05:07,680
You have your Glacier Flexible Retrieval

95
00:05:07,680 --> 00:05:09,990
for data that you need within minutes.

96
00:05:09,990 --> 00:05:12,270
And then you have Glacier Deep Archive,

97
00:05:12,270 --> 00:05:17,163
which is our lowest cost
storage in the cloud.

98
00:05:18,420 --> 00:05:19,983
And here's the brilliant part.

99
00:05:21,030 --> 00:05:23,280
As your data naturally cools

100
00:05:23,280 --> 00:05:25,590
and as access patterns decrease,

101
00:05:25,590 --> 00:05:28,500
you can progressively
move through these tiers

102
00:05:28,500 --> 00:05:31,530
and continuously optimize
your storage costs

103
00:05:31,530 --> 00:05:35,073
without sacrificing the ability
to retrieve when needed.

104
00:05:35,910 --> 00:05:37,020
And you may ask though,

105
00:05:37,020 --> 00:05:39,213
which storage class should you choose?

106
00:05:41,460 --> 00:05:45,510
Think of this as finding the
right combination of cost

107
00:05:45,510 --> 00:05:47,313
and retrieval speed.

108
00:05:48,180 --> 00:05:49,740
For Glacier Instant Retrieval,

109
00:05:49,740 --> 00:05:51,660
this is your go-to for data that's cold

110
00:05:51,660 --> 00:05:53,430
but you need very quickly.

111
00:05:53,430 --> 00:05:56,850
So, some examples of that
could be medical records

112
00:05:56,850 --> 00:05:58,440
that are rarely accessed,

113
00:05:58,440 --> 00:06:01,470
but when you need them,
you need them right away.

114
00:06:01,470 --> 00:06:05,070
Or if any of you are in
the broadcast media space,

115
00:06:05,070 --> 00:06:08,070
this could be archival cliffs
that have a tight deadline

116
00:06:08,070 --> 00:06:10,083
that you have to pull in for a project.

117
00:06:10,920 --> 00:06:13,380
This tier is also popular with those

118
00:06:13,380 --> 00:06:16,470
who have compliance requirements

119
00:06:16,470 --> 00:06:19,173
that require instant access as well.

120
00:06:20,970 --> 00:06:24,270
Next, you have Glacier Flexible Retrieval.

121
00:06:24,270 --> 00:06:25,950
And with Glacier Flexible Retrieval,

122
00:06:25,950 --> 00:06:27,630
you could get something really cool,

123
00:06:27,630 --> 00:06:30,150
which is free bulk retrievals.

124
00:06:30,150 --> 00:06:34,060
This is for large scale data and analytics

125
00:06:35,160 --> 00:06:37,143
where your timing is flexible,

126
00:06:38,100 --> 00:06:40,350
backup archives where you have some time

127
00:06:40,350 --> 00:06:42,600
and you can plan your retrievals,

128
00:06:42,600 --> 00:06:44,430
and also historical records

129
00:06:44,430 --> 00:06:47,043
that may need occasional bulk processing.

130
00:06:48,540 --> 00:06:51,480
And our last tier, Glacier Deep Archive

131
00:06:51,480 --> 00:06:55,440
where I mentioned before, is
our lowest cost storage option.

132
00:06:55,440 --> 00:06:58,650
This is for your long-term retention

133
00:06:58,650 --> 00:07:01,020
at the lowest possible cost.

134
00:07:01,020 --> 00:07:03,900
It's storage for data
you hope you never need,

135
00:07:03,900 --> 00:07:05,493
but you must legally keep.

136
00:07:07,080 --> 00:07:10,650
And this, typically you're
measuring storage archive

137
00:07:10,650 --> 00:07:12,363
in years and decades.

138
00:07:13,350 --> 00:07:16,653
But how do you get data
into these storage classes?

139
00:07:18,960 --> 00:07:21,873
For this, we have S3 lifecycle policies.

140
00:07:23,520 --> 00:07:26,890
Lifecycle policies allow
you to transition your data

141
00:07:27,900 --> 00:07:31,470
into infrequent access or
archive storage classes over time

142
00:07:31,470 --> 00:07:33,360
as your data cools off,

143
00:07:33,360 --> 00:07:35,370
and it can automatically delete that data

144
00:07:35,370 --> 00:07:36,873
at the end of its life.

145
00:07:37,890 --> 00:07:40,320
This allows you to
automatically control costs

146
00:07:40,320 --> 00:07:42,990
based on known access patterns.

147
00:07:42,990 --> 00:07:45,570
For example, let's say you have an object

148
00:07:45,570 --> 00:07:47,550
that you created on day zero,

149
00:07:47,550 --> 00:07:51,390
that's accessed very frequently
in the first 90 days.

150
00:07:51,390 --> 00:07:54,990
And then after that 90
days, it's rarely accessed.

151
00:07:54,990 --> 00:07:57,270
You can create a lifecycle policy

152
00:07:57,270 --> 00:07:59,460
that automatically transitions your object

153
00:07:59,460 --> 00:08:03,720
to Glacier Instant Retrieval
after that 90 days.

154
00:08:03,720 --> 00:08:05,910
And if your application from then

155
00:08:05,910 --> 00:08:07,470
needs to access that object,

156
00:08:07,470 --> 00:08:11,310
it's available to it in
milliseconds and there's no problem,

157
00:08:11,310 --> 00:08:12,243
it'll work great.

158
00:08:13,110 --> 00:08:18,110
But let's say after 180 days
access is extremely rare

159
00:08:18,210 --> 00:08:21,063
and we want to further
optimize our storage costs.

160
00:08:22,980 --> 00:08:25,740
Our lifecycle policies can handle that too

161
00:08:25,740 --> 00:08:28,683
by transitioning the object
to Glacier Deep Archive.

162
00:08:32,340 --> 00:08:33,540
And then after that,

163
00:08:33,540 --> 00:08:35,700
let's say you have a
compliance requirement,

164
00:08:35,700 --> 00:08:38,040
maybe you're in the financial industry

165
00:08:38,040 --> 00:08:40,590
and you have a requirement
of keeping data around

166
00:08:40,590 --> 00:08:42,390
for seven to 10 years.

167
00:08:42,390 --> 00:08:43,920
You can build a lifecycle policy

168
00:08:43,920 --> 00:08:45,780
that will automatically delete that data

169
00:08:45,780 --> 00:08:46,983
at the end of that time.

170
00:08:53,820 --> 00:08:57,120
And we provide you with
the number of filters

171
00:08:57,120 --> 00:08:59,670
that you can apply to
your lifecycle policy,

172
00:08:59,670 --> 00:09:03,570
which determine which
objects that it will affect.

173
00:09:03,570 --> 00:09:07,200
You can apply them to an
entire bucket if you need to,

174
00:09:07,200 --> 00:09:11,130
but you can also apply
them to specific prefixes

175
00:09:11,130 --> 00:09:13,893
or objects that match certain object tags.

176
00:09:14,850 --> 00:09:17,400
You can filter by object
size, which is really great

177
00:09:17,400 --> 00:09:19,473
for archiving because you typically want,

178
00:09:20,460 --> 00:09:23,730
you typically want to avoid
sending a bunch of small objects

179
00:09:23,730 --> 00:09:28,410
into archive, and you can
also filter by the number

180
00:09:28,410 --> 00:09:30,600
of versions an object has.

181
00:09:30,600 --> 00:09:32,910
So, if you want to avoid creating

182
00:09:32,910 --> 00:09:35,850
a bunch of unneeded versions,
you can create a policy

183
00:09:35,850 --> 00:09:37,710
that maybe creates one or two

184
00:09:37,710 --> 00:09:39,603
and will delete everything else.

185
00:09:41,550 --> 00:09:44,830
But let's say you don't
know your access patterns

186
00:09:45,750 --> 00:09:48,210
or they're very unpredictable.

187
00:09:48,210 --> 00:09:51,030
We have a storage class for that as well,

188
00:09:51,030 --> 00:09:53,493
and that's S3 Intelligent-Tiering.

189
00:09:55,170 --> 00:09:58,410
Sometimes access patterns
truly are just unpredictable

190
00:09:58,410 --> 00:09:59,940
or unknown.

191
00:09:59,940 --> 00:10:01,683
And S3 Intelligent-Tiering,

192
00:10:03,990 --> 00:10:04,880
excuse me.

193
00:10:04,880 --> 00:10:07,320
S3 Intelligent-Tiering is a storage class

194
00:10:07,320 --> 00:10:09,330
designed for customers
who want to optimize

195
00:10:09,330 --> 00:10:11,910
their storage costs automatically

196
00:10:11,910 --> 00:10:14,400
when data access patterns change.

197
00:10:14,400 --> 00:10:17,880
And it does that without
any performance impact,

198
00:10:17,880 --> 00:10:22,563
operational overhead or
lifecycle fees or retrieval fees.

199
00:10:26,760 --> 00:10:30,090
Intelligent-Tiering is a
cloud object storage class

200
00:10:30,090 --> 00:10:32,580
that delivers those automatic savings

201
00:10:32,580 --> 00:10:35,100
by moving your data between the tiers.

202
00:10:35,100 --> 00:10:38,640
There's five tiers, three
of them are synchronous,

203
00:10:38,640 --> 00:10:40,650
and you get those automatically.

204
00:10:40,650 --> 00:10:42,750
They're available as soon as you sign up

205
00:10:42,750 --> 00:10:45,933
and you have to opt in to
the two asynchronous tiers.

206
00:10:47,910 --> 00:10:51,360
After you opt in,
Intelligent-Tiering, excuse me,

207
00:10:51,360 --> 00:10:54,663
Intelligent-Tiering will choose
the correct tier for you.

208
00:10:57,480 --> 00:11:00,753
So far, we've talked about why
your cold data is important.

209
00:11:01,770 --> 00:11:05,190
We've talked to you about how
to choose your storage class,

210
00:11:05,190 --> 00:11:07,800
how to get your data into a storage tier,

211
00:11:07,800 --> 00:11:08,633
and how to get,

212
00:11:08,633 --> 00:11:11,760
now we're gonna talk about
how to get your data out.

213
00:11:11,760 --> 00:11:15,000
You've optimized all your storage costs,

214
00:11:15,000 --> 00:11:18,600
so what happens when your
data is suddenly hot again?

215
00:11:18,600 --> 00:11:21,303
Let's talk about restoration options.

216
00:11:23,340 --> 00:11:25,860
The first question you might ask

217
00:11:25,860 --> 00:11:28,263
is why are our customers restoring data?

218
00:11:29,790 --> 00:11:31,050
Great question.

219
00:11:31,050 --> 00:11:34,443
Let's look at some new and
exciting emerging patterns.

220
00:11:35,430 --> 00:11:40,110
So you're sitting on a gold
mine of archived media files.

221
00:11:40,110 --> 00:11:42,630
We're seeing our
customers breathe new life

222
00:11:42,630 --> 00:11:44,640
into their historical content,

223
00:11:44,640 --> 00:11:47,820
transforming decades of
old footage into fresh,

224
00:11:47,820 --> 00:11:51,120
compelling content for today's audiences.

225
00:11:51,120 --> 00:11:54,180
And when it comes to
backup and compliance,

226
00:11:54,180 --> 00:11:58,023
it's not just about checking
regulatory boxes anymore.

227
00:11:58,023 --> 00:12:00,900
Organizations are finding strategic value

228
00:12:00,900 --> 00:12:02,550
in their historical records

229
00:12:02,550 --> 00:12:05,970
and using them to track long-term patterns

230
00:12:05,970 --> 00:12:07,893
and inform their future decisions.

231
00:12:09,450 --> 00:12:12,240
But here's where it's
getting really exciting.

232
00:12:12,240 --> 00:12:17,240
Cold data is rocket fuel
for machine learning models.

233
00:12:17,430 --> 00:12:21,060
So think about it, you have
decades of historical data

234
00:12:21,060 --> 00:12:23,880
training models to spot patterns

235
00:12:23,880 --> 00:12:25,623
we never could have seen before.

236
00:12:26,460 --> 00:12:28,860
Your archive data isn't just history,

237
00:12:28,860 --> 00:12:30,783
it's your competitive advantage.

238
00:12:32,370 --> 00:12:35,040
For example, let's say you are a company

239
00:12:35,040 --> 00:12:38,100
that's working on
self-driving car technology.

240
00:12:38,100 --> 00:12:40,470
Perhaps you need to train your model

241
00:12:40,470 --> 00:12:42,870
and you wanna restore all your content

242
00:12:42,870 --> 00:12:46,650
related to cars taking left turns.

243
00:12:46,650 --> 00:12:48,423
You can do that with Glacier.

244
00:12:49,410 --> 00:12:51,660
We are seeing a brilliant trend

245
00:12:51,660 --> 00:12:54,450
where companies are
generating rich metadata

246
00:12:54,450 --> 00:12:55,950
from their archive content,

247
00:12:55,950 --> 00:12:59,490
making vast data lakes
searchable and actionable

248
00:12:59,490 --> 00:13:01,173
in ways that they weren't before.

249
00:13:04,110 --> 00:13:07,680
So now let's dive deeper into
how customers are accessing

250
00:13:07,680 --> 00:13:09,633
their stored data in Glacier.

251
00:13:13,440 --> 00:13:17,400
First with Glacier Instant
Retrieval, it's pretty easy.

252
00:13:17,400 --> 00:13:22,400
It's the same GET REQUEST that
you would use for Standard

253
00:13:22,560 --> 00:13:26,820
and Intelligent-Tiering
and S3 Infrequent Access.

254
00:13:26,820 --> 00:13:28,920
You get the millisecond access,

255
00:13:28,920 --> 00:13:32,973
and the trade off is higher
retrieval and API charges.

256
00:13:34,650 --> 00:13:38,310
Customers often mention with
Glacier Instant Retrieval,

257
00:13:38,310 --> 00:13:40,590
it's amazing for them because
now they're able to save

258
00:13:40,590 --> 00:13:43,350
on their storage costs without
having to make any changes

259
00:13:43,350 --> 00:13:45,510
to their applications.

260
00:13:45,510 --> 00:13:47,880
However, if you have
retrievals that can withstand

261
00:13:47,880 --> 00:13:51,513
some more times from
minutes to hours to days,

262
00:13:53,100 --> 00:13:55,710
we can pull your data out of
Glacier Flexible Retrieval,

263
00:13:55,710 --> 00:13:57,573
but that's a bit different.

264
00:13:59,940 --> 00:14:02,370
You can lower your cost of storage

265
00:14:02,370 --> 00:14:04,830
while also reducing the cost to retrieve

266
00:14:04,830 --> 00:14:08,790
even large amounts of data
with Glacier Flexible Retrieval

267
00:14:08,790 --> 00:14:10,353
and Glacier Deep Archive.

268
00:14:11,250 --> 00:14:12,720
But with these storage classes,

269
00:14:12,720 --> 00:14:17,160
they require a request
before data can be accessed,

270
00:14:17,160 --> 00:14:21,420
and once restored, you
can call a GET REQUEST

271
00:14:21,420 --> 00:14:24,213
on the same object key to get the object.

272
00:14:30,390 --> 00:14:33,960
So, for retrievals from these
Glacier S3 storage classes,

273
00:14:33,960 --> 00:14:37,290
Glacier Flexible Retrieval
and Glacier Deep Archive,

274
00:14:37,290 --> 00:14:39,603
you generally have three steps involved.

275
00:14:40,500 --> 00:14:43,170
First, you have to initiate a request.

276
00:14:43,170 --> 00:14:47,100
Second, you have to check that
the restore has completed.

277
00:14:47,100 --> 00:14:49,230
And finally, you access your data.

278
00:14:49,230 --> 00:14:51,280
And I'm gonna take you through each step.

279
00:14:54,750 --> 00:14:58,470
When you restore large
volumes of archived data

280
00:14:58,470 --> 00:15:01,650
consisting of hundreds
and thousands and millions

281
00:15:01,650 --> 00:15:05,310
and even billions of objects,
one factor to account for

282
00:15:05,310 --> 00:15:08,280
is the time it will take
to submit all the requests

283
00:15:08,280 --> 00:15:09,113
to Glacier.

284
00:15:10,380 --> 00:15:12,120
Glacier supports a request rate

285
00:15:12,120 --> 00:15:14,223
at a thousand transactions per second.

286
00:15:15,390 --> 00:15:18,570
This TPS limit automatically
applies to all standard

287
00:15:18,570 --> 00:15:20,580
and bulk retrievals requests

288
00:15:20,580 --> 00:15:24,123
from S3 Glacier Flexible Retrieval
and Glacier Deep Archive.

289
00:15:25,080 --> 00:15:28,710
So let's take an example, I did do math.

290
00:15:28,710 --> 00:15:32,850
A thousand TPS, you can submit
10 million restore requests

291
00:15:32,850 --> 00:15:34,950
in under three hours.

292
00:15:34,950 --> 00:15:38,190
And using Standard Restore
from Glacier Flexible Retrieval

293
00:15:38,190 --> 00:15:41,550
for example, you can complete all restores

294
00:15:41,550 --> 00:15:43,320
in about six hours.

295
00:15:43,320 --> 00:15:45,390
So that's, includes the three hours

296
00:15:45,390 --> 00:15:47,823
to submit the restore requests,

297
00:15:48,810 --> 00:15:52,563
and then three to five hours
to complete the restores.

298
00:15:54,390 --> 00:15:57,540
And to ensure you get the
highest restore performance,

299
00:15:57,540 --> 00:15:59,730
you can also rely on Batch Operations,

300
00:15:59,730 --> 00:16:01,890
which I'll talk about in a bit,

301
00:16:01,890 --> 00:16:05,130
and that'll help you maximize your TPS

302
00:16:05,130 --> 00:16:07,053
and get automatic retries.

303
00:16:11,970 --> 00:16:14,400
Now if you remember our second step,

304
00:16:14,400 --> 00:16:18,573
that was monitoring the request
status for Glacier Restores.

305
00:16:19,860 --> 00:16:23,730
For these restores, S3
will create an event

306
00:16:23,730 --> 00:16:26,133
for restore initiation and completion.

307
00:16:27,120 --> 00:16:29,820
And you can publish these
events to Amazon EventBridge

308
00:16:30,960 --> 00:16:35,280
and you can configure that
to fan out to an SNS topic

309
00:16:35,280 --> 00:16:37,050
or an SQS queue.

310
00:16:37,050 --> 00:16:40,380
You can even have it
trigger a Lambda function

311
00:16:40,380 --> 00:16:41,850
if you need it.

312
00:16:41,850 --> 00:16:44,850
You can also configure
restores completion events

313
00:16:44,850 --> 00:16:48,363
within EventBridge to send
events to an SNS topic,

314
00:16:49,740 --> 00:16:53,013
which your application can then
follow up and subscribe to.

315
00:16:54,960 --> 00:16:58,200
This will allow your application
to automatically proceed

316
00:16:58,200 --> 00:17:00,930
to the next step such as a GET or a COPY

317
00:17:00,930 --> 00:17:02,763
as soon as the object is restored.

318
00:17:04,530 --> 00:17:06,933
And now we get to our final step,

319
00:17:08,730 --> 00:17:10,263
accessing your restored data.

320
00:17:11,310 --> 00:17:13,860
So once an object is
restored from Glacier,

321
00:17:13,860 --> 00:17:15,840
your application can
access it like you would

322
00:17:15,840 --> 00:17:17,520
any other object.

323
00:17:17,520 --> 00:17:22,020
It's an S3 Standard now,
you can perform a GET

324
00:17:22,020 --> 00:17:23,880
or access it any other way

325
00:17:23,880 --> 00:17:26,970
that you would access an
object in a synchronous tier.

326
00:17:26,970 --> 00:17:29,520
However, a tip, so remember this,

327
00:17:29,520 --> 00:17:33,063
the restored data is a
temporary copy of the object.

328
00:17:35,190 --> 00:17:36,390
To move it out of Glacier,

329
00:17:36,390 --> 00:17:40,200
You can either do a copy
and place over the same key

330
00:17:40,200 --> 00:17:42,603
or you can copy it to a new bucket.

331
00:17:43,770 --> 00:17:45,720
If you're copying over
the same key though,

332
00:17:45,720 --> 00:17:48,660
just keep an eye out for
adding an extra version

333
00:17:48,660 --> 00:17:51,783
in case don't wanna keep an
additional Glacier version.

334
00:17:55,950 --> 00:17:59,913
Now, I did briefly
mention Batch Operations.

335
00:18:01,920 --> 00:18:04,923
The Glacier kind of works
like a freight train,

336
00:18:05,850 --> 00:18:07,500
and if you know the batch,

337
00:18:07,500 --> 00:18:10,380
we can do some optimizations on our side

338
00:18:10,380 --> 00:18:12,780
to make things run smoother.

339
00:18:12,780 --> 00:18:15,390
If you're getting less than
a thousand transactions

340
00:18:15,390 --> 00:18:18,900
per second, that means the
request will take longer

341
00:18:18,900 --> 00:18:20,520
to submit.

342
00:18:20,520 --> 00:18:24,923
I've seen examples of
customers request around 25 TPS

343
00:18:26,100 --> 00:18:29,580
and essentially increasing
their total restore time by 40x,

344
00:18:29,580 --> 00:18:31,620
which is suboptimal.

345
00:18:31,620 --> 00:18:34,020
But the good news is that
you don't have to optimize

346
00:18:34,020 --> 00:18:36,450
your software or multithreading,

347
00:18:36,450 --> 00:18:38,460
you can use Batch Operations

348
00:18:38,460 --> 00:18:41,853
and that can dramatically
improve your restore experience.

349
00:18:44,490 --> 00:18:47,010
Instead of fine tuning your
software, as I mentioned,

350
00:18:47,010 --> 00:18:48,480
you can use Batch Operations

351
00:18:48,480 --> 00:18:51,873
to automatically maximize your
restore request per second.

352
00:18:53,640 --> 00:18:56,730
It can also do automatic retries

353
00:18:56,730 --> 00:18:59,250
to handle any error failures

354
00:18:59,250 --> 00:19:02,040
and with the completion report
that will signal any issues

355
00:19:02,040 --> 00:19:03,033
with your job.

356
00:19:05,520 --> 00:19:08,340
And then you create a manifest,

357
00:19:08,340 --> 00:19:10,713
a list of keys you want to restore,

358
00:19:11,970 --> 00:19:14,070
and submit to Batch Operations,

359
00:19:14,070 --> 00:19:17,640
each manifest will be
associated to a single job

360
00:19:17,640 --> 00:19:21,663
and additional jobs from there
will then split up your TPS.

361
00:19:24,150 --> 00:19:29,150
All right, so we've covered, again,

362
00:19:30,150 --> 00:19:32,370
the importance of your cold data

363
00:19:32,370 --> 00:19:36,720
and how it can be an innovation
for your applications.

364
00:19:36,720 --> 00:19:38,430
I talked to you about your storage classes

365
00:19:38,430 --> 00:19:40,110
and how to pick the best one.

366
00:19:40,110 --> 00:19:42,870
Talked to you about getting
your data into Glacier,

367
00:19:42,870 --> 00:19:45,900
and I talked to you about
getting your data out of Glacier.

368
00:19:45,900 --> 00:19:48,180
Now Nitish is gonna come up

369
00:19:48,180 --> 00:19:52,080
and talk to you about some
new archive-related features.

370
00:19:52,080 --> 00:19:52,913
Thank you.

371
00:19:54,789 --> 00:19:56,160
(indistinct)

372
00:19:56,160 --> 00:19:56,993
Whoa, I didn't.

373
00:20:01,500 --> 00:20:02,790
- Thank you, Gayla.

374
00:20:02,790 --> 00:20:05,460
Now you have seen S3 storage offering

375
00:20:05,460 --> 00:20:07,560
and the pathways of getting the data in

376
00:20:07,560 --> 00:20:09,273
and out of S3 Glacier.

377
00:20:10,170 --> 00:20:11,880
Let me build on that foundation

378
00:20:11,880 --> 00:20:13,890
and introduce two new capabilities

379
00:20:13,890 --> 00:20:16,080
that we have recently added.

380
00:20:16,080 --> 00:20:18,933
Each specifically designed
for archive workloads.

381
00:20:19,950 --> 00:20:23,490
We actually, we are going to
talk about three capabilities,

382
00:20:23,490 --> 00:20:25,650
so there is one that will be like coming

383
00:20:25,650 --> 00:20:28,380
in Matt Garman's review keynote tomorrow.

384
00:20:28,380 --> 00:20:30,090
So stay tuned.

385
00:20:30,090 --> 00:20:31,830
So the first thing that
we are going to talk about

386
00:20:31,830 --> 00:20:36,030
is checking the integrity of
your archived data at rest.

387
00:20:36,030 --> 00:20:38,010
And the second thing that
we are going to talk about

388
00:20:38,010 --> 00:20:41,460
is using S3 Metadata for quick discovery

389
00:20:41,460 --> 00:20:42,903
of your archive content.

390
00:20:43,950 --> 00:20:46,260
For each of these capabilities,

391
00:20:46,260 --> 00:20:48,270
I'll address three questions.

392
00:20:48,270 --> 00:20:52,020
First, what problems are we solving here?

393
00:20:52,020 --> 00:20:55,413
How does it work and what
does it mean for you?

394
00:20:57,690 --> 00:20:59,670
Let's start with the first feature,

395
00:20:59,670 --> 00:21:02,583
new compute checksum
operation in Amazon S3.

396
00:21:04,350 --> 00:21:06,543
Why this capability is so vital?

397
00:21:07,380 --> 00:21:09,480
Because customers across domains

398
00:21:09,480 --> 00:21:12,150
such as media and
entertainment, life sciences,

399
00:21:12,150 --> 00:21:15,450
crime and justice, and
preservation institutions

400
00:21:15,450 --> 00:21:17,700
perform periodic data integrity check

401
00:21:17,700 --> 00:21:20,700
to make sure that the data is intact.

402
00:21:20,700 --> 00:21:23,610
This could be the master
copy of an iconic movie

403
00:21:23,610 --> 00:21:27,930
or it can be a historical artifact

404
00:21:27,930 --> 00:21:30,120
like the Constitution
of the United States,

405
00:21:30,120 --> 00:21:33,300
or this is something that is
required by the compliance team

406
00:21:33,300 --> 00:21:38,300
or a proof that the evidence
has not been tampered with.

407
00:21:38,400 --> 00:21:41,970
In all these cases, verifying archive data

408
00:21:41,970 --> 00:21:44,430
is an industry standard.

409
00:21:44,430 --> 00:21:48,720
Our customers asked us
to provide tools in S3

410
00:21:48,720 --> 00:21:52,413
to help them do this, and that's
what exactly we have built.

411
00:21:53,850 --> 00:21:55,830
Before I dive deeper into this capability,

412
00:21:55,830 --> 00:21:57,240
one thing that I want to highlight

413
00:21:57,240 --> 00:21:59,730
is that everything that
we are talking about,

414
00:21:59,730 --> 00:22:03,630
checksum validation here
is completely optional.

415
00:22:03,630 --> 00:22:07,710
S3 performs billions of
checksum operations every second

416
00:22:07,710 --> 00:22:11,700
to make sure every date,
like every byte in transit

417
00:22:11,700 --> 00:22:14,190
and addressed is intact.

418
00:22:14,190 --> 00:22:17,340
But we also know that our
customers are already doing that

419
00:22:17,340 --> 00:22:20,550
and we can provide a better
way of performing these checks.

420
00:22:20,550 --> 00:22:23,343
So that was the intention
behind this feature.

421
00:22:24,600 --> 00:22:26,160
But first, let's cover the basics.

422
00:22:26,160 --> 00:22:28,680
What is a checksum that
we are computing here?

423
00:22:28,680 --> 00:22:31,713
A checksum is a distal
fingerprint of an object.

424
00:22:32,559 --> 00:22:34,170
In this example, we have three objects,

425
00:22:34,170 --> 00:22:36,960
we are using the checksum algorithm CRC32,

426
00:22:36,960 --> 00:22:39,240
and we have this unique
alpha pneumatic value

427
00:22:39,240 --> 00:22:40,920
for each object.

428
00:22:40,920 --> 00:22:42,450
Even a single bit flip

429
00:22:42,450 --> 00:22:44,763
will result in a different checksum value.

430
00:22:45,660 --> 00:22:47,850
Customers store original checksum value

431
00:22:47,850 --> 00:22:50,160
on their media asset management systems

432
00:22:50,160 --> 00:22:53,400
or in a checksum repository
as a source of growth.

433
00:22:53,400 --> 00:22:56,040
And later it can be six months, two days,

434
00:22:56,040 --> 00:22:59,130
few seconds after the
upload or after 10 years,

435
00:22:59,130 --> 00:23:02,610
they come back and calculate
a fresh checksum of the object

436
00:23:02,610 --> 00:23:05,550
that is in S3 and compare
that against the checksum

437
00:23:05,550 --> 00:23:07,203
that is stored in their systems.

438
00:23:08,070 --> 00:23:13,070
It's a way for them to prove
that the data remains intact.

439
00:23:16,080 --> 00:23:19,920
S3 already provides a range
of capabilities during upload.

440
00:23:19,920 --> 00:23:21,990
So when you upload an object to S3,

441
00:23:21,990 --> 00:23:24,600
you can specify the checksum
algorithm you want to use

442
00:23:24,600 --> 00:23:27,570
as well as you can provide a
pre-calculated checksum value,

443
00:23:27,570 --> 00:23:29,280
which is optional.

444
00:23:29,280 --> 00:23:32,100
You have support for six
checksum value algorithms,

445
00:23:32,100 --> 00:23:37,100
CRC32, 32C, CRC64, MD5, SHA-1 and SHA-256.

446
00:23:39,960 --> 00:23:44,340
For every upload, S3 calculate
CRC64 on the client side

447
00:23:44,340 --> 00:23:45,780
and on the server side

448
00:23:45,780 --> 00:23:48,540
to provide you end-to-end data integrity,

449
00:23:48,540 --> 00:23:52,080
and only on the match
the request succeeds.

450
00:23:52,080 --> 00:23:55,380
This works well for data in transit.

451
00:23:55,380 --> 00:23:58,380
Now let's see how customers
perform data verification

452
00:23:58,380 --> 00:24:00,423
for already stored data in S3.

453
00:24:02,340 --> 00:24:05,910
Until now, verifying
checksum for data at rest

454
00:24:05,910 --> 00:24:07,350
required two steps.

455
00:24:07,350 --> 00:24:11,580
First, you download the
object which takes the,

456
00:24:11,580 --> 00:24:13,710
which will have a compute call,

457
00:24:13,710 --> 00:24:16,080
sorry, a bandwidth cost
or data transfer fee,

458
00:24:16,080 --> 00:24:19,140
as well as some time,
it will take some time.

459
00:24:19,140 --> 00:24:21,630
And then you spin up an easy to instance

460
00:24:21,630 --> 00:24:24,430
or use your own infra to
calculate the checksum locally.

461
00:24:25,320 --> 00:24:28,470
That means more compute costs, more time,

462
00:24:28,470 --> 00:24:30,063
and added complexity.

463
00:24:31,080 --> 00:24:35,550
For large archives, this
process can be cost prohibitive

464
00:24:35,550 --> 00:24:38,310
or it will, it can be time-consuming.

465
00:24:38,310 --> 00:24:41,280
We needed to eliminate the
step of downloading the object

466
00:24:41,280 --> 00:24:42,930
and then calculating the checksum.

467
00:24:42,930 --> 00:24:45,420
We wanted to come up
with an innovative way

468
00:24:45,420 --> 00:24:48,000
to do an in-place read of the data

469
00:24:48,000 --> 00:24:50,163
and calculate a fresh checksum.

470
00:24:51,240 --> 00:24:52,830
And that's what we have done

471
00:24:52,830 --> 00:24:55,290
with Compute Checksum Operation.

472
00:24:55,290 --> 00:24:58,860
This is a new capability
in S3 Batch Operations

473
00:24:58,860 --> 00:25:01,410
and it provides you a new
way to verify the content

474
00:25:01,410 --> 00:25:05,283
of your dataset stored in
Glacier or any storage class.

475
00:25:06,720 --> 00:25:09,600
You can efficiently
verify billions of objects

476
00:25:09,600 --> 00:25:12,183
and automatically generate
a data integrity report.

477
00:25:13,260 --> 00:25:16,440
You can use that to prove
that your data remains intact

478
00:25:16,440 --> 00:25:17,880
over time.

479
00:25:17,880 --> 00:25:20,820
This capability works
with any object in S3,

480
00:25:20,820 --> 00:25:24,090
regardless of the storage
class or the object size.

481
00:25:24,090 --> 00:25:26,940
Now whether you're verifying your data

482
00:25:26,940 --> 00:25:30,540
for compliance reasons,
for digital preservation

483
00:25:30,540 --> 00:25:34,620
or accuracy checks before
feeding the data into the model,

484
00:25:34,620 --> 00:25:36,660
you can reduce the time, cost,

485
00:25:36,660 --> 00:25:39,843
and the effort associated in that process.

486
00:25:40,710 --> 00:25:44,220
And because it is built
in S3 Batch Operations,

487
00:25:44,220 --> 00:25:47,190
you get automatic retries on failures

488
00:25:47,190 --> 00:25:49,260
and detailed completion report in the end,

489
00:25:49,260 --> 00:25:51,783
which you can use as the
data integrity report.

490
00:25:53,310 --> 00:25:54,903
Let me show you how it works.

491
00:25:57,270 --> 00:26:00,660
Creating a compute checksum job is simple.

492
00:26:00,660 --> 00:26:02,370
It has three components.

493
00:26:02,370 --> 00:26:04,740
First, you provide an object list,

494
00:26:04,740 --> 00:26:07,953
also known as manifest
in S3 Batch Operations.

495
00:26:08,790 --> 00:26:11,190
You can create a curated list

496
00:26:11,190 --> 00:26:13,290
and then submit that as the CSV file,

497
00:26:13,290 --> 00:26:16,200
or you can use S3 Batch Operations

498
00:26:16,200 --> 00:26:18,810
automatic manifest generation service.

499
00:26:18,810 --> 00:26:20,820
You can also use inventory report

500
00:26:20,820 --> 00:26:22,890
and just feed that in as the manifest,

501
00:26:22,890 --> 00:26:24,663
Batch Operation supports that too.

502
00:26:26,040 --> 00:26:28,530
Second, you choose the checksum algorithm.

503
00:26:28,530 --> 00:26:29,970
We already talked about the algorithms

504
00:26:29,970 --> 00:26:32,520
that are supported in S3,
so the story is consistent,

505
00:26:32,520 --> 00:26:35,070
everything that is supported on upload

506
00:26:35,070 --> 00:26:37,320
is supported with Compute
Checksum Operation.

507
00:26:38,250 --> 00:26:41,070
The algorithm that you pick
really depends on your business

508
00:26:41,070 --> 00:26:42,990
or compliance use case.

509
00:26:42,990 --> 00:26:45,960
So for example, if you want
something that is secure

510
00:26:45,960 --> 00:26:49,110
and more compliant to regulatory needs,

511
00:26:49,110 --> 00:26:53,760
you can go with Secure Hash
Algorithm like SHA-1 or SHA-256.

512
00:26:53,760 --> 00:26:55,530
And if you need performance

513
00:26:55,530 --> 00:26:57,930
and don't care much about
the compliance requirements

514
00:26:57,930 --> 00:27:02,407
and all, you can go ahead
with CRC, CRC64, 32 or 32C.

515
00:27:03,540 --> 00:27:06,633
And the third thing that you
specify is checksum type.

516
00:27:07,710 --> 00:27:09,960
Checksum type, you can
provide a full object

517
00:27:09,960 --> 00:27:12,930
or a composite checksum.

518
00:27:12,930 --> 00:27:14,520
So if you're in media supply chain

519
00:27:14,520 --> 00:27:18,030
or you're dealing with
third party providers

520
00:27:18,030 --> 00:27:20,490
where you're providing your content

521
00:27:20,490 --> 00:27:23,220
and you need to provide
a full object checksum

522
00:27:23,220 --> 00:27:25,170
so that everybody's
talking the same language,

523
00:27:25,170 --> 00:27:27,180
the chain of custody is maintained.

524
00:27:27,180 --> 00:27:29,280
In those scenarios, you
can use the checksum type

525
00:27:29,280 --> 00:27:30,510
as full object.

526
00:27:30,510 --> 00:27:33,930
Whereas you don't have any
search needs, it's all internal,

527
00:27:33,930 --> 00:27:36,420
your team knows what we are talking about,

528
00:27:36,420 --> 00:27:38,310
you are dealing with large objects,

529
00:27:38,310 --> 00:27:40,620
you want to perform
parallel checksum operation,

530
00:27:40,620 --> 00:27:42,270
in that case, continue

531
00:27:42,270 --> 00:27:44,320
or go ahead with composite checksum type.

532
00:27:46,620 --> 00:27:49,590
And of course you'll need to
provide the IAM permissions

533
00:27:49,590 --> 00:27:52,440
to S3 Batch Operations so
that it can read the bytes

534
00:27:52,440 --> 00:27:54,840
and write the completion report.

535
00:27:54,840 --> 00:27:57,540
That's it, these are the
three inputs that you need

536
00:27:57,540 --> 00:28:00,390
and the permissions, and S3
Batch Ops handles the rest.

537
00:28:00,390 --> 00:28:04,200
It will read the object, it
will compute the checksum

538
00:28:04,200 --> 00:28:06,753
and provide you a nice
integrity report in the end.

539
00:28:08,070 --> 00:28:11,280
And this is how the completion
report would look like.

540
00:28:11,280 --> 00:28:15,930
It will have fields like bucket,
key, version_id, ErrorCode,

541
00:28:15,930 --> 00:28:17,550
and ResultMessage.

542
00:28:18,840 --> 00:28:21,540
Once the job is complete, you can use this

543
00:28:21,540 --> 00:28:23,340
to validate with the
checksum that are stored

544
00:28:23,340 --> 00:28:25,320
on your media asset management system

545
00:28:25,320 --> 00:28:26,583
or checksum repository.

546
00:28:28,320 --> 00:28:30,540
You can also use, let's say JSON parsing

547
00:28:30,540 --> 00:28:32,550
and extract the checksum and the values

548
00:28:32,550 --> 00:28:34,170
and convert this into a nice table,

549
00:28:34,170 --> 00:28:36,630
the result message fields
that are available,

550
00:28:36,630 --> 00:28:38,133
and use that for validation.

551
00:28:39,210 --> 00:28:42,090
Additionally, you can use Lambda function

552
00:28:42,090 --> 00:28:45,690
to automate this job,
let's say every six months

553
00:28:45,690 --> 00:28:48,030
or one year if that is the need.

554
00:28:48,030 --> 00:28:52,530
The best part is that you do
not incur any fee for restore

555
00:28:52,530 --> 00:28:53,970
or retrieval.

556
00:28:53,970 --> 00:28:58,080
Your data that is in
Intelligent-Tearing, it won't warm up,

557
00:28:58,080 --> 00:28:59,460
and if it's in Glacier,

558
00:28:59,460 --> 00:29:02,110
you don't have to restore
those objects from Glacier.

559
00:29:03,330 --> 00:29:08,330
You pay a single fee of
0.004 per GB or $4 per TB

560
00:29:09,330 --> 00:29:10,830
to process the data.

561
00:29:10,830 --> 00:29:14,433
And that is consistent
across all storage classes.

562
00:29:15,510 --> 00:29:17,610
Now let's move on to the next capability

563
00:29:17,610 --> 00:29:22,610
that would be helpful,
and that is S3 Metadata.

564
00:29:23,010 --> 00:29:27,420
Now this capability, this is
something that we launched

565
00:29:27,420 --> 00:29:28,773
during re:Invent last year,

566
00:29:29,790 --> 00:29:32,670
and we have made some
improvements to S3 Metadata.

567
00:29:32,670 --> 00:29:35,610
S3 Metadata automatically
extracts metadata

568
00:29:35,610 --> 00:29:38,880
from your objects and
makes it available to you

569
00:29:38,880 --> 00:29:42,660
to generate valuable insights
using simple SQL queries

570
00:29:42,660 --> 00:29:44,700
or natural language.

571
00:29:44,700 --> 00:29:47,160
We believe that this
will fundamentally change

572
00:29:47,160 --> 00:29:50,760
how customers manage and
extract value out of their cold

573
00:29:50,760 --> 00:29:51,873
or archive data.

574
00:29:52,890 --> 00:29:55,240
Let me explain the problem
we are solving here.

575
00:29:57,210 --> 00:30:01,140
Here is an example, we hear
this from our customers

576
00:30:01,140 --> 00:30:03,870
very often, that hey, we have a bucket

577
00:30:03,870 --> 00:30:06,930
with millions of objects or files in that,

578
00:30:06,930 --> 00:30:08,430
that they have archived.

579
00:30:08,430 --> 00:30:10,710
They have tagged those
objects, so in this case,

580
00:30:10,710 --> 00:30:14,130
we have tagged all the objects
either as Project Odin,

581
00:30:14,130 --> 00:30:18,660
Loki or Thor, and then
few of them are untagged.

582
00:30:18,660 --> 00:30:22,200
Some are in Glacier and
few are in S3 Standard.

583
00:30:22,200 --> 00:30:24,240
And the customer's ops
team come and ask like,

584
00:30:24,240 --> 00:30:27,270
hey, we want to move all
the data for Project Odin

585
00:30:27,270 --> 00:30:29,700
that is in Glacier to Standard.

586
00:30:29,700 --> 00:30:32,370
How much data are we talking about?

587
00:30:32,370 --> 00:30:36,120
Today, there are multiple
options to answer that question.

588
00:30:36,120 --> 00:30:39,330
You can paginate the
API requests like list

589
00:30:39,330 --> 00:30:42,750
and get object tags and scan the storage

590
00:30:42,750 --> 00:30:46,950
and calculate the storage
across storage classes.

591
00:30:46,950 --> 00:30:51,950
Or you can use the S3 inventory
report, which comes like,

592
00:30:52,680 --> 00:30:55,140
refreshes daily or every 48 hours.

593
00:30:55,140 --> 00:30:58,650
And then you can use that
to answer this question.

594
00:30:58,650 --> 00:31:01,650
But both options at scale
either delay in time

595
00:31:01,650 --> 00:31:04,353
or are not easy to perform.

596
00:31:05,220 --> 00:31:07,503
But we saw an opportunity
to make it faster.

597
00:31:08,370 --> 00:31:10,860
And what if we could answer
this question in seconds,

598
00:31:10,860 --> 00:31:12,930
we're writing a single query?

599
00:31:12,930 --> 00:31:16,383
And that's exactly what we
try to do with S3 Metadata.

600
00:31:18,270 --> 00:31:22,110
It is a fully managed service
that automatically captures

601
00:31:22,110 --> 00:31:25,260
all your object metadata
tags, object storage class,

602
00:31:25,260 --> 00:31:28,080
object size, everything that
you get from the head object

603
00:31:28,080 --> 00:31:30,993
and store that into a
queryable Apache Iceberg table.

604
00:31:32,490 --> 00:31:36,270
No APIs to call, no inventory
reports to wait for,

605
00:31:36,270 --> 00:31:38,820
just instant SQL queries.

606
00:31:38,820 --> 00:31:42,600
We launch S3 Metadata
at re:Invent last year

607
00:31:42,600 --> 00:31:45,150
with starting with journal table.

608
00:31:45,150 --> 00:31:48,450
The journal table captures
every change to your bucket

609
00:31:48,450 --> 00:31:49,833
in near real time.

610
00:31:50,880 --> 00:31:54,480
Puts, deletes, metadata
updates, tag changes,

611
00:31:54,480 --> 00:31:57,390
all of them are captured in journal table.

612
00:31:57,390 --> 00:32:01,140
It is your complete change
of logs of what's happening

613
00:32:01,140 --> 00:32:03,153
in your bucket right now.

614
00:32:04,710 --> 00:32:07,830
In July this year, we
added a new capability,

615
00:32:07,830 --> 00:32:09,543
the live inventory table.

616
00:32:10,680 --> 00:32:15,360
It shows the current state of
every object in your bucket.

617
00:32:15,360 --> 00:32:18,420
It backfills all your existing
data when you enable it

618
00:32:18,420 --> 00:32:20,313
and it refreshes every hour.

619
00:32:21,690 --> 00:32:25,260
Both tables are read-only
and fully managed by AWS,

620
00:32:25,260 --> 00:32:28,590
and you can think of them
as your authoritative system

621
00:32:28,590 --> 00:32:31,173
of record for everything in your bucket.

622
00:32:33,840 --> 00:32:37,500
Now, here is where it
gets really powerful.

623
00:32:37,500 --> 00:32:41,100
You can query these metadata in two ways.

624
00:32:41,100 --> 00:32:44,640
First, through standard SQL
queries using Athena, Redshift,

625
00:32:44,640 --> 00:32:46,230
SageMaker Unified Studio,

626
00:32:46,230 --> 00:32:49,770
any analytics tool that
supports Iceberg tables.

627
00:32:49,770 --> 00:32:54,090
And second, through natural
language using Amazon Q, Kiro,

628
00:32:54,090 --> 00:32:57,660
or any agent that you're using
with supports MCP server.

629
00:32:57,660 --> 00:32:59,763
Here are a few examples that we tried.

630
00:33:01,080 --> 00:33:03,810
First, to learn about storage usage,

631
00:33:03,810 --> 00:33:06,510
the question that we ask ourself,

632
00:33:06,510 --> 00:33:09,270
how much data is in
Glacier for each project?

633
00:33:09,270 --> 00:33:11,250
Now you can simply write a SQL query

634
00:33:11,250 --> 00:33:14,100
and it will give you the snapshot of that.

635
00:33:14,100 --> 00:33:17,100
Second, you can also
identify all the objects

636
00:33:17,100 --> 00:33:20,103
that were untagged so that you
can classify them properly.

637
00:33:21,120 --> 00:33:23,520
And lastly, you can
also use it for auditing

638
00:33:23,520 --> 00:33:26,640
the journal table to track
what data was deleted,

639
00:33:26,640 --> 00:33:29,460
who deleted it, when delete,
like when was it deleted

640
00:33:29,460 --> 00:33:31,260
and from where?

641
00:33:31,260 --> 00:33:34,680
And just recently, we
added MCP server support

642
00:33:34,680 --> 00:33:37,260
for S3 tables, which means you can connect

643
00:33:37,260 --> 00:33:40,950
your AI assistants like
Claude, custom agents, Kiro,

644
00:33:40,950 --> 00:33:43,020
directly to your metadata tables,

645
00:33:43,020 --> 00:33:45,513
and interact with them in plain English.

646
00:33:46,650 --> 00:33:50,760
That is really powerful because
it democratizes the access

647
00:33:50,760 --> 00:33:53,670
or insights generation
from your archive data,

648
00:33:53,670 --> 00:33:56,190
whether like you don't
have to be a data engineer,

649
00:33:56,190 --> 00:33:59,520
data scientist, like anyone
from your team like finance,

650
00:33:59,520 --> 00:34:03,420
ops, compliance team, they
can just write their questions

651
00:34:03,420 --> 00:34:06,840
in plain English and MCP server
will convert those questions

652
00:34:06,840 --> 00:34:09,150
or the agent can convert
those questions to queries

653
00:34:09,150 --> 00:34:13,380
and extract the insight
out of the archive data

654
00:34:13,380 --> 00:34:15,900
and provide them as something
that is like a summary

655
00:34:15,900 --> 00:34:16,983
or easy to consume.

656
00:34:19,710 --> 00:34:22,740
And now let's move on to the
next section, which is a demo.

657
00:34:22,740 --> 00:34:27,740
So I'll quickly try to figure
out the logistics here,

658
00:34:27,810 --> 00:34:29,073
so gimme a second.

659
00:34:48,539 --> 00:34:53,460
So for a moment, let's assume
that we are space tech startup

660
00:34:53,460 --> 00:34:56,010
and we are building an
autonomous space vehicle

661
00:34:56,010 --> 00:35:00,630
and we are using the images
available from NASA and videos

662
00:35:00,630 --> 00:35:03,480
to build a space simulation model

663
00:35:03,480 --> 00:35:05,130
so that we can train how the vehicle,

664
00:35:05,130 --> 00:35:07,023
how to navigate through obstacles.

665
00:35:08,580 --> 00:35:10,980
These images are valuable
for training the data,

666
00:35:10,980 --> 00:35:14,070
but we need to verify the
integrity of these objects

667
00:35:14,070 --> 00:35:18,010
before putting them or feeding
them into those expensive GPU

668
00:35:18,870 --> 00:35:20,790
compute thing that we need.

669
00:35:20,790 --> 00:35:24,060
So for that, we are going to
focus on three things today.

670
00:35:24,060 --> 00:35:25,980
We are going to use the inventory table

671
00:35:25,980 --> 00:35:29,340
to find all the objects that
are related to Project Odin,

672
00:35:29,340 --> 00:35:31,470
which is our code name for our,

673
00:35:31,470 --> 00:35:34,530
like autonomous space vehicle project.

674
00:35:34,530 --> 00:35:37,380
Then we'll calculate the
checksum of these objects,

675
00:35:37,380 --> 00:35:41,130
and then we will use Kiro to
compare the completion report

676
00:35:41,130 --> 00:35:44,313
with the original checksums
that are added as tags.

677
00:35:56,340 --> 00:35:57,770
So...

678
00:36:04,170 --> 00:36:05,640
Okay.

679
00:36:05,640 --> 00:36:07,233
So here what we are doing.

680
00:36:09,300 --> 00:36:11,943
One second, this is still not working.

681
00:36:18,630 --> 00:36:23,630
Okay, so we'll go to this bucket
re:Invent storage 208 demo,

682
00:36:23,880 --> 00:36:27,120
where we have hundreds
of images and videos

683
00:36:27,120 --> 00:36:29,373
that we have downloaded from NASA website,

684
00:36:30,390 --> 00:36:33,663
and they're stored across
different storage classes.

685
00:36:35,190 --> 00:36:38,130
Few of them are in
Glacier Instant Retrieval,

686
00:36:38,130 --> 00:36:40,230
few of them are in Standard.

687
00:36:40,230 --> 00:36:42,360
We'll open one of them

688
00:36:42,360 --> 00:36:44,850
and just look at the object properties.

689
00:36:44,850 --> 00:36:48,000
We'll scroll down, we have
tags that is attached to it,

690
00:36:48,000 --> 00:36:52,020
so SHA-256 checksum
algorithm that was computed,

691
00:36:52,020 --> 00:36:54,240
the time when this object was created.

692
00:36:54,240 --> 00:36:56,100
The checksum timestamp.

693
00:36:56,100 --> 00:36:58,170
Project name as Odin.

694
00:36:58,170 --> 00:37:02,910
And baseline checksum or the
original checksum hex code.

695
00:37:02,910 --> 00:37:05,790
So all these are added to the object

696
00:37:05,790 --> 00:37:09,240
and we have added that to most
of the objects and kept them,

697
00:37:09,240 --> 00:37:12,060
few of them untagged intentionally.

698
00:37:12,060 --> 00:37:15,270
Now, our plan is to use inventory table,

699
00:37:15,270 --> 00:37:18,720
live inventory table,
to query all the objects

700
00:37:18,720 --> 00:37:22,590
that are associated to Project Odin,

701
00:37:22,590 --> 00:37:23,550
and create a list of that

702
00:37:23,550 --> 00:37:25,470
that we can feed into Batch Operations.

703
00:37:25,470 --> 00:37:27,870
So what we will do, we'll go to query

704
00:37:27,870 --> 00:37:29,580
and we'll use live inventory table,

705
00:37:29,580 --> 00:37:34,580
use SageMaker Unified Studio
to query our inventory table.

706
00:37:36,930 --> 00:37:41,910
And we need to make sure
that we tweak the query

707
00:37:41,910 --> 00:37:44,880
in a way that can be used
with Batch Operations.

708
00:37:44,880 --> 00:37:45,713
So...

709
00:37:49,080 --> 00:37:52,503
Yeah, so we need bucket and key,

710
00:37:55,830 --> 00:38:00,830
and then we need to add a WHERE
clause where we are talking,

711
00:38:02,010 --> 00:38:04,350
we are adding the WHERE
clause where object tags

712
00:38:04,350 --> 00:38:06,393
have project name as Odin.

713
00:38:09,810 --> 00:38:11,373
And then we'll run this query.

714
00:38:29,970 --> 00:38:34,970
So, we have 25 files that are
associated to Project Odin.

715
00:38:35,100 --> 00:38:37,083
Now let's download the CSV,

716
00:38:38,310 --> 00:38:40,800
and let's see the values once

717
00:38:40,800 --> 00:38:43,260
before fitting it into
S3 Batch Operations.

718
00:38:43,260 --> 00:38:45,480
So we need to remove the first row

719
00:38:45,480 --> 00:38:48,450
so that it matches the
format that is supported

720
00:38:48,450 --> 00:38:52,773
in Batch Operation intake
process, and then save this file.

721
00:38:55,380 --> 00:38:58,720
We'll save this as odin_manifest_v2.

722
00:39:20,790 --> 00:39:24,270
Now let's go back to our
general purpose bucket,

723
00:39:24,270 --> 00:39:26,223
S3 bucket that we had.

724
00:39:28,230 --> 00:39:31,320
And we have created one more bucket there

725
00:39:31,320 --> 00:39:33,543
for S3 Batch Operations jobs.

726
00:39:40,590 --> 00:39:44,670
So here we have two buckets,
one for storing the manifest

727
00:39:44,670 --> 00:39:48,570
and another one for providing
us the destination location

728
00:39:48,570 --> 00:39:51,060
to get the results from Batch Operations.

729
00:39:51,060 --> 00:39:55,773
So we'll upload the
odin_manifest_v2 to this folder,

730
00:39:59,550 --> 00:40:04,550
then we go back and look at.

731
00:40:04,890 --> 00:40:06,440
Okay, I think we are good here.

732
00:40:13,530 --> 00:40:17,370
So this is our, this will
be our result location

733
00:40:17,370 --> 00:40:18,510
where we'll be storing the result

734
00:40:18,510 --> 00:40:21,510
from compute checksum job operation.

735
00:40:21,510 --> 00:40:23,440
Now we'll create a new job

736
00:40:26,220 --> 00:40:29,820
and we will use the existing
manifest that we just created,

737
00:40:29,820 --> 00:40:34,820
which is stored in our
batch_manifest folder, v2.

738
00:40:39,450 --> 00:40:40,863
Oops, CSV.

739
00:40:46,601 --> 00:40:48,510
We will select compute checksum operation,

740
00:40:48,510 --> 00:40:49,830
that is the new one.

741
00:40:49,830 --> 00:40:53,880
Checksum type as full
object, and SHA-256 algo.

742
00:40:55,380 --> 00:40:57,930
This is important, we need to
acknowledge that this report

743
00:40:57,930 --> 00:40:59,550
can be accessed by the bucket order

744
00:40:59,550 --> 00:41:02,430
because the completion report
will have checksum value

745
00:41:02,430 --> 00:41:05,100
which is off the plain text data.

746
00:41:05,100 --> 00:41:08,193
So that's important to acknowledge.

747
00:41:10,290 --> 00:41:12,723
Then we provide the destination location.

748
00:41:13,920 --> 00:41:18,920
And now we will just add the
permissions for IAM role.

749
00:41:20,310 --> 00:41:25,050
We have created S3 batch
checksum role for this,

750
00:41:25,050 --> 00:41:26,823
and we'll submit the job.

751
00:41:34,320 --> 00:41:35,793
We will run this job,

752
00:41:47,160 --> 00:41:50,550
and we will wait for it to complete,

753
00:41:50,550 --> 00:41:52,680
which can take few seconds.

754
00:41:52,680 --> 00:41:54,840
Meanwhile, just good to known info,

755
00:41:54,840 --> 00:41:58,320
we have added automatic
manifest generation capability,

756
00:41:58,320 --> 00:42:01,260
which was available through
Batch Operations API

757
00:42:01,260 --> 00:42:02,250
to console as well.

758
00:42:02,250 --> 00:42:04,950
So you can use that
for generating manifest

759
00:42:04,950 --> 00:42:06,550
if you're trying it by yourself.

760
00:42:08,130 --> 00:42:09,693
Should be done any moment.

761
00:42:20,890 --> 00:42:23,373
Okay, so the job is complete.

762
00:42:24,840 --> 00:42:26,880
Let's go back to our folder

763
00:42:26,880 --> 00:42:29,433
that we created for batch results.

764
00:42:31,470 --> 00:42:33,213
F2e9.

765
00:42:35,490 --> 00:42:36,993
We'll go to the results.

766
00:42:39,540 --> 00:42:44,247
And let's open it to see how
does the output looks like

767
00:42:45,210 --> 00:42:46,503
in the completion report.

768
00:42:51,540 --> 00:42:53,610
So this is how you get
as the result message,

769
00:42:53,610 --> 00:42:55,623
this column is a result message,

770
00:42:56,490 --> 00:43:01,490
and it has the checksum algorithm SHA-256.

771
00:43:03,720 --> 00:43:05,850
The checksum type, full object.

772
00:43:05,850 --> 00:43:09,603
The checksum value, both
in Base64 and hex code.

773
00:43:14,020 --> 00:43:18,210
Okay, after this, we will
use Kiro to compare this

774
00:43:18,210 --> 00:43:20,763
with the values that are stored on,

775
00:43:21,720 --> 00:43:24,453
with the objects tags as tags.

776
00:43:26,040 --> 00:43:28,263
So, we are using the Kiro CLI,

777
00:43:31,200 --> 00:43:36,200
and we want to specify that
validate project Odin checksums

778
00:43:37,020 --> 00:43:40,080
using the output of S3
Batch Operations job,

779
00:43:40,080 --> 00:43:45,080
and then provide the link
or S3 UI for that CSV file

780
00:43:47,400 --> 00:43:49,383
that we just got from Batch Ops.

781
00:44:10,560 --> 00:44:12,393
It has created a Python script,

782
00:44:14,430 --> 00:44:19,260
and all the 25 objects that
were related to Project Odin,

783
00:44:19,260 --> 00:44:23,070
it validated that they match the checksum

784
00:44:23,070 --> 00:44:25,320
that are stored as tags.

785
00:44:25,320 --> 00:44:28,410
And with that, we conclude the demo.

786
00:44:28,410 --> 00:44:32,400
And I want to summarize our
session with few key takeaways

787
00:44:32,400 --> 00:44:34,530
that you can keep in mind.

788
00:44:34,530 --> 00:44:38,070
So the first is, with advancement in AI,

789
00:44:38,070 --> 00:44:41,013
archived data can be the
differentiating factor.

790
00:44:42,000 --> 00:44:45,210
And S3 provides you
multiple storage options

791
00:44:45,210 --> 00:44:47,430
curated for your specific business needs

792
00:44:47,430 --> 00:44:51,030
in terms of access pattern
or storage time or the cost.

793
00:44:51,030 --> 00:44:55,080
And finally, we are adding
new capabilities in S3,

794
00:44:55,080 --> 00:44:59,070
such as compute checksum
operation or S3 Metadata

795
00:44:59,070 --> 00:45:01,890
that can help you easily
manage archived data

796
00:45:01,890 --> 00:45:05,010
at the same time, extract
more value out of them.

797
00:45:05,010 --> 00:45:07,350
And with that, we would like
to conclude this session.

798
00:45:07,350 --> 00:45:08,820
Thank you so much.

799
00:45:08,820 --> 00:45:10,260
Yeah, please complete the session.

800
00:45:10,260 --> 00:45:13,643
Thank you.
(audience applauding)

