1
00:00:00,390 --> 00:00:01,223
- Okay.

2
00:00:01,223 --> 00:00:02,327
Hello and welcome, everyone.

3
00:00:02,327 --> 00:00:05,400
My name is Marc, I'm an
engineer on the DSQL team,

4
00:00:05,400 --> 00:00:07,020
and this is DAT439.

5
00:00:07,020 --> 00:00:09,570
This is a deep dive
into Amazon Aurora DSQL

6
00:00:09,570 --> 00:00:11,490
and its architecture.

7
00:00:11,490 --> 00:00:13,110
Last year, my colleague Marc Brooker,

8
00:00:13,110 --> 00:00:15,282
gave a really fantastic talk

9
00:00:15,282 --> 00:00:17,430
with the same title as this one

10
00:00:17,430 --> 00:00:19,710
where we gave a broad
overview of the service

11
00:00:19,710 --> 00:00:21,660
and its capabilities.

12
00:00:21,660 --> 00:00:24,600
There's a lot going on
under the cover of DSQL,

13
00:00:24,600 --> 00:00:27,420
and so this year we have a
series of deep dive talks

14
00:00:27,420 --> 00:00:28,770
where we're gonna go much deeper

15
00:00:28,770 --> 00:00:31,143
into specific areas of the architecture.

16
00:00:32,370 --> 00:00:33,840
In this talk, we're gonna be talking about

17
00:00:33,840 --> 00:00:36,060
how DSQL manages connections?

18
00:00:36,060 --> 00:00:38,910
What the architecture of the
query processor looks like,

19
00:00:38,910 --> 00:00:40,410
and the architecture of the session

20
00:00:40,410 --> 00:00:42,780
routing layer in front of it looks like.

21
00:00:42,780 --> 00:00:44,579
Hope you enjoy the talk.

22
00:00:44,579 --> 00:00:49,193
But first, let's do a quick
recap of what Aurora DSQL is.

23
00:00:49,193 --> 00:00:51,570
DSQL is a distributed implementation

24
00:00:51,570 --> 00:00:53,340
of a relational database.

25
00:00:53,340 --> 00:00:55,920
Relational databases are
awesome. They're older than I am.

26
00:00:55,920 --> 00:00:58,410
You can run complex queries,
you can involve your schema,

27
00:00:58,410 --> 00:01:00,600
add indexes on the fly.

28
00:01:00,600 --> 00:01:03,330
Meanwhile, we have services like DynamoDB.

29
00:01:03,330 --> 00:01:04,890
We've been figuring out how to build

30
00:01:04,890 --> 00:01:06,420
and run distributed services

31
00:01:06,420 --> 00:01:09,330
for quite a long time at this point.

32
00:01:09,330 --> 00:01:12,390
And the really cool thing
about architectures like Dynamo

33
00:01:12,390 --> 00:01:14,460
is that they don't have
a single point of failure

34
00:01:14,460 --> 00:01:18,150
that they can scale horizontally
rather than vertically.

35
00:01:18,150 --> 00:01:20,430
DSQL is based on Postgres.

36
00:01:20,430 --> 00:01:22,680
We're running Postgres under the covers,

37
00:01:22,680 --> 00:01:24,480
and it's a database that has been designed

38
00:01:24,480 --> 00:01:26,373
for running transactional workloads.

39
00:01:27,450 --> 00:01:29,580
And so our customers have been asking us

40
00:01:29,580 --> 00:01:31,710
to bridge these two worlds,

41
00:01:31,710 --> 00:01:33,090
the world of relational,

42
00:01:33,090 --> 00:01:35,100
the world of distributor database,

43
00:01:35,100 --> 00:01:37,590
and at the same time, our
customers have been asking us

44
00:01:37,590 --> 00:01:39,690
for a fully serverless database,

45
00:01:39,690 --> 00:01:41,610
one where there are no
server to provision,

46
00:01:41,610 --> 00:01:43,350
manage or patch,

47
00:01:43,350 --> 00:01:45,810
a service where you can pay per use,

48
00:01:45,810 --> 00:01:49,323
a service that scales up
but also all the way down.

49
00:01:51,030 --> 00:01:53,880
This is what it looks like
to create a DSQL cluster.

50
00:01:53,880 --> 00:01:56,280
I'm really proud of this page.

51
00:01:56,280 --> 00:01:58,980
This is something that we intend
to keep as simple as it is,

52
00:01:58,980 --> 00:02:00,810
there's no scroll bar here.

53
00:02:00,810 --> 00:02:03,794
You can click Create Cluster,
and in just five seconds,

54
00:02:03,794 --> 00:02:06,900
that's something we recently released.

55
00:02:06,900 --> 00:02:09,210
You can have a cluster ready to go.

56
00:02:09,210 --> 00:02:11,250
There's nothing on this page

57
00:02:11,250 --> 00:02:13,410
that you have to decide upfront, right?

58
00:02:13,410 --> 00:02:16,710
There is no maintenance windows,
there's no VPC settings.

59
00:02:16,710 --> 00:02:19,200
Everything on here is
something you can change later.

60
00:02:19,200 --> 00:02:21,000
Tags, deletion protection.

61
00:02:21,000 --> 00:02:22,950
You can re-encrypt your cluster

62
00:02:22,950 --> 00:02:24,900
with the different key later,

63
00:02:24,900 --> 00:02:28,260
you can toggle the resource
based policies later.

64
00:02:28,260 --> 00:02:30,060
So if you want, you can just click create

65
00:02:30,060 --> 00:02:31,110
and off you're going.

66
00:02:31,980 --> 00:02:33,930
Here's another feature our
team released recently.

67
00:02:33,930 --> 00:02:35,490
This is the query editor.

68
00:02:35,490 --> 00:02:36,357
What's really cool about this,

69
00:02:36,357 --> 00:02:39,180
is this is a Postgres application

70
00:02:39,180 --> 00:02:40,350
running in your web browser,

71
00:02:40,350 --> 00:02:42,843
speaking web sockets directly to DSQL.

72
00:02:43,680 --> 00:02:46,140
Because DSQL is based on Postgres,

73
00:02:46,140 --> 00:02:48,240
our expectation is that you can pick up

74
00:02:48,240 --> 00:02:50,850
the vast majority of
Postgres based libraries,

75
00:02:50,850 --> 00:02:54,270
whether that's libpq based or
JDBC or anything like that,

76
00:02:54,270 --> 00:02:55,773
and connect to DSQL.

77
00:02:57,450 --> 00:02:58,860
So how does this all work?

78
00:02:58,860 --> 00:03:00,990
We have application, maybe that's a shell

79
00:03:00,990 --> 00:03:02,730
or something that you built on Lambda.

80
00:03:02,730 --> 00:03:04,170
It's connecting to DSQL,

81
00:03:04,170 --> 00:03:06,750
and we're sending SQL
statements to a component

82
00:03:06,750 --> 00:03:08,460
called the Query Processor.

83
00:03:08,460 --> 00:03:10,680
This is gonna be the
star of our show today.

84
00:03:10,680 --> 00:03:14,580
And the QP is receiving
the Postgres wire protocol.

85
00:03:14,580 --> 00:03:17,220
It's receiving SQL from your applications,

86
00:03:17,220 --> 00:03:20,160
and it's interpreting and
running those queries.

87
00:03:20,160 --> 00:03:22,740
The second key component is the Journal.

88
00:03:22,740 --> 00:03:25,740
The journal is our
distributed transaction log.

89
00:03:25,740 --> 00:03:28,890
It's a core building block at AWS.

90
00:03:28,890 --> 00:03:31,230
It powers services like S3

91
00:03:31,230 --> 00:03:34,380
and the journal is where we
get our durability guarantee.

92
00:03:34,380 --> 00:03:37,620
In single region, when the
journal accepts a transaction,

93
00:03:37,620 --> 00:03:40,440
it's writing it to at least
two availability zones.

94
00:03:40,440 --> 00:03:42,480
And in a multi-region configuration,

95
00:03:42,480 --> 00:03:44,310
we're getting our transactions replicated

96
00:03:44,310 --> 00:03:46,950
to at least one other AWS region.

97
00:03:46,950 --> 00:03:49,200
And the third key component is Storage.

98
00:03:49,200 --> 00:03:52,770
Storage is a service that our
team built from the ground up

99
00:03:52,770 --> 00:03:56,580
designed to meet the performance
needs that we had of DSQL.

100
00:03:56,580 --> 00:03:58,530
And the way storage fits into this picture

101
00:03:58,530 --> 00:04:00,390
is that it connects to the journal.

102
00:04:00,390 --> 00:04:02,880
It learns about transactions
that you've committed

103
00:04:02,880 --> 00:04:05,130
and it's taken those transactions

104
00:04:05,130 --> 00:04:06,750
and updating its local view of the world

105
00:04:06,750 --> 00:04:08,880
so that we can run queries.

106
00:04:08,880 --> 00:04:12,480
And so, when we send a SQL
Select statement to DSQL,

107
00:04:12,480 --> 00:04:17,130
it's the QPs job to parse,
plan and execute that query

108
00:04:17,130 --> 00:04:18,600
and turn that into a series

109
00:04:18,600 --> 00:04:21,543
of low level read
operations against storage.

110
00:04:22,500 --> 00:04:24,150
Meanwhile, if you're running any DML,

111
00:04:24,150 --> 00:04:26,490
like an insert statement,

112
00:04:26,490 --> 00:04:28,860
any rows that you insert into DSQL

113
00:04:28,860 --> 00:04:31,350
are gonna reside in the
memory of the query processor.

114
00:04:31,350 --> 00:04:33,690
Nothing's been written to the disc.

115
00:04:33,690 --> 00:04:35,520
If you run an update statement,

116
00:04:35,520 --> 00:04:37,440
the QP is first gonna turn that into

117
00:04:37,440 --> 00:04:39,030
a series of select statements to go

118
00:04:39,030 --> 00:04:41,580
and read the existing
values of those rows,

119
00:04:41,580 --> 00:04:43,650
and then apply anything
in the update statement

120
00:04:43,650 --> 00:04:46,590
like a set or, you know,
increment might balance by one

121
00:04:46,590 --> 00:04:49,233
to produce new versions
of those rows in memory.

122
00:04:50,490 --> 00:04:52,140
And so as you run your transaction,

123
00:04:52,140 --> 00:04:55,650
the QP is doing all of this
work, ephemerally in memory.

124
00:04:55,650 --> 00:04:57,990
And if your connection breaks

125
00:04:57,990 --> 00:05:00,690
or if you type rollback,
that data's simply gone.

126
00:05:00,690 --> 00:05:02,970
Meanwhile, if you commit your transaction,

127
00:05:02,970 --> 00:05:06,343
the QP is gonna send that
transaction over the network.

128
00:05:06,343 --> 00:05:08,264
Eventually, it'll be
written to the journal

129
00:05:08,264 --> 00:05:10,920
again to at least two availability zones

130
00:05:10,920 --> 00:05:13,220
and this is the point
of commit in our system.

131
00:05:14,610 --> 00:05:17,040
Storage, again, is
subscribed to the journal.

132
00:05:17,040 --> 00:05:18,810
It's learning about these transactions

133
00:05:18,810 --> 00:05:20,560
and it's keeping itself up to date.

134
00:05:23,160 --> 00:05:26,190
Now, you may be wondering
what about consistency?

135
00:05:26,190 --> 00:05:28,080
DSQL is strongly consistent.

136
00:05:28,080 --> 00:05:29,340
And the way we do this is through

137
00:05:29,340 --> 00:05:31,533
a time-based synchronization protocol.

138
00:05:32,580 --> 00:05:34,440
When your transaction starts,

139
00:05:34,440 --> 00:05:36,750
the Query Processor is doing two things.

140
00:05:36,750 --> 00:05:39,390
First, it's using the
EC2 Time Sync Service

141
00:05:39,390 --> 00:05:41,790
which is based on GPS satellite clocks

142
00:05:41,790 --> 00:05:44,460
to get a really accurate,
microsecond accurate reading

143
00:05:44,460 --> 00:05:46,920
of what the current time is.

144
00:05:46,920 --> 00:05:50,760
The second thing it does is
use the AWS ClockBound Service

145
00:05:50,760 --> 00:05:54,090
to correct even that very
accurate time measurement

146
00:05:54,090 --> 00:05:55,980
for any potential errors.

147
00:05:55,980 --> 00:05:57,480
And by doing this, we get called

148
00:05:57,480 --> 00:05:59,280
a linear risible timestamps,

149
00:05:59,280 --> 00:06:01,365
which is to say it's a
value that's guaranteed

150
00:06:01,365 --> 00:06:03,783
to be in the future of
any commit timestamps.

151
00:06:04,950 --> 00:06:08,010
The QP is going to include that timestamp

152
00:06:08,010 --> 00:06:09,933
in request to storage.

153
00:06:10,800 --> 00:06:12,840
And by doing this, we
guarantee that storage

154
00:06:12,840 --> 00:06:15,000
has always seen the latest data.

155
00:06:15,000 --> 00:06:17,730
Storage implements multi-version
concurrency control.

156
00:06:17,730 --> 00:06:19,590
And so we'll return results

157
00:06:19,590 --> 00:06:23,010
for precisely the timestamp
that we asked for,

158
00:06:23,010 --> 00:06:25,020
which means that as we run our transaction

159
00:06:25,020 --> 00:06:28,080
and many seconds may pass while
we're going back and forth,

160
00:06:28,080 --> 00:06:30,123
we see a frozen moment in time.

161
00:06:32,340 --> 00:06:34,230
So because DSQL is strongly consistent,

162
00:06:34,230 --> 00:06:36,840
this means that we can open
additional connections.

163
00:06:36,840 --> 00:06:37,980
These different connections

164
00:06:37,980 --> 00:06:40,020
can run on different query processes

165
00:06:40,020 --> 00:06:41,880
on different physical hosts

166
00:06:41,880 --> 00:06:44,640
in different EC2 availability zones,

167
00:06:44,640 --> 00:06:45,960
or in the case of multi-region,

168
00:06:45,960 --> 00:06:47,910
even in different AWS regions

169
00:06:47,910 --> 00:06:50,673
and still get strongly consistent results.

170
00:06:52,140 --> 00:06:54,240
The SQL is also active-active,

171
00:06:54,240 --> 00:06:56,550
which means that any of
these query processes

172
00:06:56,550 --> 00:06:58,800
can also update data.

173
00:06:58,800 --> 00:07:01,020
And the way they do
that is by coordinating

174
00:07:01,020 --> 00:07:02,610
with a service called the Adjudicator,

175
00:07:02,610 --> 00:07:04,650
which sits in front of the journal.

176
00:07:04,650 --> 00:07:07,560
The Adjudicator implements a
concurrency control technique

177
00:07:07,560 --> 00:07:10,110
known as optimistic concurrency control.

178
00:07:10,110 --> 00:07:12,810
And the idea behind this
is that we take that bundle

179
00:07:12,810 --> 00:07:15,300
of everything we wanted
to do in our transaction,

180
00:07:15,300 --> 00:07:16,950
and the adjudicator's going to go look

181
00:07:16,950 --> 00:07:19,710
at other commits that
have been made recently,

182
00:07:19,710 --> 00:07:21,993
and check if there are any conflicts.

183
00:07:24,270 --> 00:07:27,090
We are not gonna go deep into OCC today.

184
00:07:27,090 --> 00:07:30,399
Again, in the talk last
year with the same title,

185
00:07:30,399 --> 00:07:32,400
we went into much more detail,

186
00:07:32,400 --> 00:07:34,680
and yesterday we gave
a 500 level talk about

187
00:07:34,680 --> 00:07:36,060
Optimistic Concurrency Control

188
00:07:36,060 --> 00:07:37,460
if you'd like to learn more.

189
00:07:39,570 --> 00:07:42,990
DSQL implements hands-free
automatic scaling

190
00:07:42,990 --> 00:07:44,340
through a number of techniques,

191
00:07:44,340 --> 00:07:46,770
it does this automatically
and out of the box.

192
00:07:46,770 --> 00:07:48,030
So on this picture,

193
00:07:48,030 --> 00:07:49,620
any number of things might have happened.

194
00:07:49,620 --> 00:07:52,200
Let's pretend there's
rescaling happening here.

195
00:07:52,200 --> 00:07:55,080
DSQL is continuously backing up your data,

196
00:07:55,080 --> 00:07:56,940
and as we open more of these connections

197
00:07:56,940 --> 00:07:59,910
that are able to drive more and
more traffic to the service,

198
00:07:59,910 --> 00:08:01,813
DSQL will use those recent snapshots

199
00:08:01,813 --> 00:08:04,590
to create clones of storage.

200
00:08:04,590 --> 00:08:07,590
Those clones will go and
connect into the journal.

201
00:08:07,590 --> 00:08:09,510
They will learn about recent transactions,

202
00:08:09,510 --> 00:08:11,070
they'll keep themselves up to date,

203
00:08:11,070 --> 00:08:12,300
and then the query processes

204
00:08:12,300 --> 00:08:16,170
will intelligently load
balance across those replicas.

205
00:08:16,170 --> 00:08:18,150
Another thing this picture might represent

206
00:08:18,150 --> 00:08:19,830
is splitting for size, right?

207
00:08:19,830 --> 00:08:22,830
Like as we put more data into the service,

208
00:08:22,830 --> 00:08:24,630
these replicas are gonna become bigger.

209
00:08:24,630 --> 00:08:27,900
And so DSQL will try and keep
them of a manageable size.

210
00:08:27,900 --> 00:08:28,830
And the reason we do that

211
00:08:28,830 --> 00:08:31,320
is because if we keep the data sets small,

212
00:08:31,320 --> 00:08:32,880
we can improve performance,

213
00:08:32,880 --> 00:08:35,130
and in the case that one
of our replicas fails,

214
00:08:35,130 --> 00:08:37,380
we can bring a replacement
online very quickly

215
00:08:37,380 --> 00:08:39,830
because it doesn't have
too much data to restore.

216
00:08:41,340 --> 00:08:44,310
DSQL can also scale out the journal layer,

217
00:08:44,310 --> 00:08:47,820
and it does that through automatic
and transparent sharding.

218
00:08:47,820 --> 00:08:50,250
And we'll start to send
some of your updates

219
00:08:50,250 --> 00:08:52,290
to some adjudicators in some journals

220
00:08:52,290 --> 00:08:55,410
and some of your updates
to other adjudicators.

221
00:08:55,410 --> 00:08:57,060
For both the read and write scaling,

222
00:08:57,060 --> 00:08:59,202
DSQL does this very proactively,

223
00:08:59,202 --> 00:09:01,620
so it's monitoring an increase in load,

224
00:09:01,620 --> 00:09:05,580
and it implements these scaling
techniques transparently.

225
00:09:05,580 --> 00:09:06,690
And it actually doesn't matter

226
00:09:06,690 --> 00:09:08,970
what the sharding strategy it picks is,

227
00:09:08,970 --> 00:09:12,633
because DSQLs fully capable
of doing cross-shot commits.

228
00:09:13,560 --> 00:09:15,630
Okay, that's our crash
course out of the way.

229
00:09:15,630 --> 00:09:17,730
We're gonna now start to
speak a little bit about

230
00:09:17,730 --> 00:09:20,040
how connections work in DSQL.

231
00:09:20,040 --> 00:09:21,210
Before we do that,

232
00:09:21,210 --> 00:09:22,590
let's set the stage and talk about

233
00:09:22,590 --> 00:09:24,563
how connections would
work in vanilla Postgres.

234
00:09:24,563 --> 00:09:27,060
This is Postgres that you
may have downloaded off

235
00:09:27,060 --> 00:09:29,040
the internet installed in your laptop,

236
00:09:29,040 --> 00:09:32,070
or Postgres that you're getting on EC2

237
00:09:32,070 --> 00:09:34,233
maybe through RDS or Aurora.

238
00:09:35,340 --> 00:09:37,860
So our application is gonna connect in,

239
00:09:37,860 --> 00:09:40,020
it's gonna do some kind
of credential exchange.

240
00:09:40,020 --> 00:09:42,540
Postgres has many
mechanisms for doing this.

241
00:09:42,540 --> 00:09:45,570
Let's say we are doing a user
password credential exchange.

242
00:09:45,570 --> 00:09:48,120
The server is gonna look at our password,

243
00:09:48,120 --> 00:09:49,440
it's sorted in the database,

244
00:09:49,440 --> 00:09:51,810
it's gonna make sure that
it matches what we ask for.

245
00:09:51,810 --> 00:09:54,810
And once it's authenticated
and authorized our connection,

246
00:09:54,810 --> 00:09:56,610
the server is going to fork itself.

247
00:09:56,610 --> 00:09:58,770
This is the Unix fork system call

248
00:09:58,770 --> 00:10:00,960
and so we're gonna get a dedicated process

249
00:10:00,960 --> 00:10:02,013
for our connection.

250
00:10:02,970 --> 00:10:05,640
And as we open more connections
either from the same host

251
00:10:05,640 --> 00:10:07,020
or from a different host,

252
00:10:07,020 --> 00:10:09,390
we're gonna land up with
more of these sessions.

253
00:10:09,390 --> 00:10:11,430
And each of our connections,
each of these sessions

254
00:10:11,430 --> 00:10:14,100
is getting its own dedicated process.

255
00:10:14,100 --> 00:10:16,560
And because of this, each one
of these connections consumes

256
00:10:16,560 --> 00:10:18,330
some number of resources on the service,

257
00:10:18,330 --> 00:10:20,340
even if you're not doing anything.

258
00:10:20,340 --> 00:10:22,590
And so, at some point we may
need to get a bigger host

259
00:10:22,590 --> 00:10:24,423
simply to have more connections.

260
00:10:25,977 --> 00:10:27,150
And so what we're gonna do now

261
00:10:27,150 --> 00:10:28,766
is go through a little bit of a journey

262
00:10:28,766 --> 00:10:30,992
that many of you may have
been on several times

263
00:10:30,992 --> 00:10:34,530
as you've built applications
against the relational database

264
00:10:34,530 --> 00:10:37,170
and started to scale or solve problems

265
00:10:37,170 --> 00:10:42,170
like how to manage heat, or
availability, or failover.

266
00:10:43,130 --> 00:10:45,600
And so we have our application,
it has a single customer,

267
00:10:45,600 --> 00:10:47,820
we've just gotten started.

268
00:10:47,820 --> 00:10:50,160
And the first question that
we may have to answer is,

269
00:10:50,160 --> 00:10:52,470
how do we secure this endpoint?

270
00:10:52,470 --> 00:10:57,470
Postgres is about a million
lines of sea, it's 40 years old.

271
00:10:57,570 --> 00:11:00,060
And it's a very performance
sensitive code, right?

272
00:11:00,060 --> 00:11:01,770
There's tons of optimizations.

273
00:11:01,770 --> 00:11:04,530
And so just by running this
service on the internet,

274
00:11:04,530 --> 00:11:07,507
you have to run the risk of
some kind of security breach.

275
00:11:07,507 --> 00:11:09,510
It may not even be in Postgres itself,

276
00:11:09,510 --> 00:11:12,120
it may be a security issue
with the operating system

277
00:11:12,120 --> 00:11:13,050
or through libraries like OpenSSL

278
00:11:13,050 --> 00:11:15,480
that Postgres is linked to.

279
00:11:15,480 --> 00:11:17,970
And even if Postgres itself is secure,

280
00:11:17,970 --> 00:11:20,010
we may have something
like a password, right?

281
00:11:20,010 --> 00:11:21,270
Maybe you put it on a sticky note

282
00:11:21,270 --> 00:11:22,661
on your machine, please don't do that.

283
00:11:22,661 --> 00:11:24,930
Best practice here is
you can do some sort of

284
00:11:24,930 --> 00:11:27,450
a password rotation
through Secrets Manager,

285
00:11:27,450 --> 00:11:29,550
but still you have a fundamental property

286
00:11:29,550 --> 00:11:31,110
that if somebody leaks that password,

287
00:11:31,110 --> 00:11:32,760
they may have access to your database.

288
00:11:32,760 --> 00:11:35,707
And so this is a common
reason for customers to,

289
00:11:35,707 --> 00:11:39,810
you know, run their database
in something like a VPC,

290
00:11:39,810 --> 00:11:42,241
which can really complicate
the lives of developers

291
00:11:42,241 --> 00:11:45,003
trying to connect to and
work with the database.

292
00:11:46,290 --> 00:11:48,090
Now, hopefully we have
more than one customer,

293
00:11:48,090 --> 00:11:50,070
as that other customer starts to connect

294
00:11:50,070 --> 00:11:51,360
and use our service.

295
00:11:51,360 --> 00:11:53,220
We can't just have one connection open

296
00:11:53,220 --> 00:11:54,630
to the database, right?

297
00:11:54,630 --> 00:11:56,730
And so we're gonna need at
least one more connection

298
00:11:56,730 --> 00:11:58,413
so we can do work in parallel.

299
00:11:59,340 --> 00:12:01,710
Now, you don't want to be
opening that connection

300
00:12:01,710 --> 00:12:04,230
as that customer arrives.

301
00:12:04,230 --> 00:12:06,450
Opening a connection in
Postgres can take some time,

302
00:12:06,450 --> 00:12:08,490
sometimes as much as a second,

303
00:12:08,490 --> 00:12:10,050
because we have to do a
bunch of things, right?

304
00:12:10,050 --> 00:12:12,210
We have to establish a TCP connection,

305
00:12:12,210 --> 00:12:13,783
we have to set up TLS,

306
00:12:13,783 --> 00:12:16,770
we have to do multiple round
trips to do handshaking,

307
00:12:16,770 --> 00:12:18,360
we have to do credential exchange,

308
00:12:18,360 --> 00:12:20,220
we have to fork that backend process,

309
00:12:20,220 --> 00:12:21,990
that backend process has to get ready.

310
00:12:21,990 --> 00:12:24,750
And then finally, our
application can prepare any data,

311
00:12:24,750 --> 00:12:27,420
like prepared statements
before it can get going.

312
00:12:27,420 --> 00:12:28,500
And so to solve this problem,

313
00:12:28,500 --> 00:12:31,560
many customers will create
client side connection pools

314
00:12:31,560 --> 00:12:33,654
that ahead of time, open
a bunch of connections.

315
00:12:33,654 --> 00:12:36,990
We move all of that slow
stuff off the critical path.

316
00:12:36,990 --> 00:12:38,940
And so when one of our
customers comes along

317
00:12:38,940 --> 00:12:40,260
with an API request,

318
00:12:40,260 --> 00:12:43,710
we can just pop a connection
out of the pool and get going.

319
00:12:43,710 --> 00:12:44,970
Which leads us to a question, right?

320
00:12:44,970 --> 00:12:46,560
How big should this pool be?

321
00:12:46,560 --> 00:12:49,770
If our application is running
at 10 transactions per second

322
00:12:49,770 --> 00:12:51,330
and you look on your monitoring graph

323
00:12:51,330 --> 00:12:53,130
and it's just sitting at 10,

324
00:12:53,130 --> 00:12:54,810
well, that probably doesn't mean

325
00:12:54,810 --> 00:12:56,580
that you can use a pool size of 10, right?

326
00:12:56,580 --> 00:12:58,740
Because if we zoom into any minute,

327
00:12:58,740 --> 00:13:00,180
right down to the second,

328
00:13:00,180 --> 00:13:03,090
we may see spikes in concurrency

329
00:13:03,090 --> 00:13:05,010
because our customers
may not always be doing

330
00:13:05,010 --> 00:13:06,900
work at a steady rate.

331
00:13:06,900 --> 00:13:08,790
This is a problem known
as peak to average,

332
00:13:08,790 --> 00:13:11,190
and it may lead you to
require a much bigger pool

333
00:13:11,190 --> 00:13:12,600
than your steady state rate.

334
00:13:12,600 --> 00:13:15,393
Let's pretend it's about 110x increase.

335
00:13:16,320 --> 00:13:19,290
This is a formula from the
RDS documentation that says,

336
00:13:19,290 --> 00:13:21,780
for every gigabyte of
memory that an instance has,

337
00:13:21,780 --> 00:13:23,967
you can have roughly 10 connections.

338
00:13:23,967 --> 00:13:26,820
And so if we need a hundred
connections for our application,

339
00:13:26,820 --> 00:13:29,010
that means we can use the
smallest instance type

340
00:13:29,010 --> 00:13:30,213
that micro over there.

341
00:13:31,560 --> 00:13:33,090
But, of course, we don't want to run

342
00:13:33,090 --> 00:13:34,590
one copy of our application, right?

343
00:13:34,590 --> 00:13:36,330
We want to be able to do deployments,

344
00:13:36,330 --> 00:13:37,800
we want to be able to survive the loss

345
00:13:37,800 --> 00:13:39,420
of an availability zone.

346
00:13:39,420 --> 00:13:41,370
And so if we are running in a typical

347
00:13:41,370 --> 00:13:43,923
three replica scenario,

348
00:13:45,240 --> 00:13:48,180
you may think that we can set
that pool size to 33 only,

349
00:13:48,180 --> 00:13:49,500
but it actually doesn't work like that.

350
00:13:49,500 --> 00:13:51,120
Because again, if we lose a host,

351
00:13:51,120 --> 00:13:53,550
then the surviving hosts
need to be able to survive

352
00:13:53,550 --> 00:13:55,053
to handle those spikes.

353
00:13:56,190 --> 00:13:58,500
And so now we need 300 connections.

354
00:13:58,500 --> 00:14:00,990
We can no longer use a micro.

355
00:14:00,990 --> 00:14:02,310
This is really common pattern

356
00:14:02,310 --> 00:14:04,260
that customers have to
increase their instance size

357
00:14:04,260 --> 00:14:05,850
just to get more connections.

358
00:14:05,850 --> 00:14:08,040
But you'll notice that the
second part of this formula,

359
00:14:08,040 --> 00:14:10,450
that 5,000 says that even if you're using

360
00:14:10,450 --> 00:14:14,460
an M84XL with 64 gigs of memory,

361
00:14:14,460 --> 00:14:18,840
or the beast of a machine, the
48XL with 768 gigs of memory,

362
00:14:18,840 --> 00:14:21,840
you can still only have 5,000 connections.

363
00:14:21,840 --> 00:14:22,830
To solve this problem,

364
00:14:22,830 --> 00:14:25,860
it's very common to deploy
something like PgBouncer

365
00:14:25,860 --> 00:14:29,640
or if you're an RDS customer,
then you can use RDS Proxy.

366
00:14:29,640 --> 00:14:30,473
And the way this works

367
00:14:30,473 --> 00:14:32,520
is that you're connect into the proxy,

368
00:14:32,520 --> 00:14:34,770
and the proxy is gonna
hold those 300 connections

369
00:14:34,770 --> 00:14:36,210
and only use the hundred

370
00:14:36,210 --> 00:14:37,800
that you actually need on the backend,

371
00:14:37,800 --> 00:14:40,470
which is gonna save resources
and improve performance.

372
00:14:40,470 --> 00:14:42,180
This can be a really great solution

373
00:14:42,180 --> 00:14:43,740
in managing this complexity,

374
00:14:43,740 --> 00:14:45,690
but it's just one more
thing for you to worry about

375
00:14:45,690 --> 00:14:47,463
to configure and to pay for.

376
00:14:49,170 --> 00:14:50,220
Here's another challenge.

377
00:14:50,220 --> 00:14:51,960
What host are we connecting to, right?

378
00:14:51,960 --> 00:14:53,100
We have a single writer,

379
00:14:53,100 --> 00:14:55,260
a single place that we can do mutations,

380
00:14:55,260 --> 00:14:58,200
a single place where we can
get strongly consistent reads.

381
00:14:58,200 --> 00:15:01,710
And if it becomes unavailable
or you can't contact it,

382
00:15:01,710 --> 00:15:03,870
then things are go about to go bad.

383
00:15:03,870 --> 00:15:05,850
You have to somehow fence this machine off

384
00:15:05,850 --> 00:15:07,800
so it can no longer do writes.

385
00:15:07,800 --> 00:15:10,950
We have to find a standby,
hopefully we have one ready,

386
00:15:10,950 --> 00:15:12,840
or we have to restore from backup.

387
00:15:12,840 --> 00:15:14,850
We have to kick the primary out of DNS,

388
00:15:14,850 --> 00:15:17,010
we have to bring the standby in.

389
00:15:17,010 --> 00:15:19,140
We have to make sure that
our application is ready

390
00:15:19,140 --> 00:15:20,913
to reconnect to that machine.

391
00:15:22,080 --> 00:15:23,370
And even if you do all of this right,

392
00:15:23,370 --> 00:15:26,370
this can still lead to
minutes or hours of downtime.

393
00:15:26,370 --> 00:15:28,380
And if you're using all
of the best practices,

394
00:15:28,380 --> 00:15:29,820
really you shouldn't expect less

395
00:15:29,820 --> 00:15:32,523
than 30 seconds of downtime.

396
00:15:34,350 --> 00:15:37,140
Here's another challenge, scaling, right?

397
00:15:37,140 --> 00:15:38,640
If we need to scale out reads,

398
00:15:38,640 --> 00:15:40,500
we're gonna add read replicas,

399
00:15:40,500 --> 00:15:42,750
we're gonna need to
configure reader endpoints.

400
00:15:42,750 --> 00:15:44,340
We're gonna have to teach our application,

401
00:15:44,340 --> 00:15:47,638
which of our APIs can
tolerate eventual consistency

402
00:15:47,638 --> 00:15:50,520
in our read-only and then
start to do some sort of

403
00:15:50,520 --> 00:15:52,590
traffic splitting in the application.

404
00:15:52,590 --> 00:15:53,700
Some of these read replicas

405
00:15:53,700 --> 00:15:55,650
may be further behind than others.

406
00:15:55,650 --> 00:15:58,620
So our load balancing is not
simply distributing the load,

407
00:15:58,620 --> 00:16:01,233
it's also excluding hosts
that we shouldn't talk to.

408
00:16:02,130 --> 00:16:05,340
And even if you get all of that right,

409
00:16:05,340 --> 00:16:07,740
you're still left with
eventual consistency,

410
00:16:07,740 --> 00:16:09,724
which is very difficult to reason about

411
00:16:09,724 --> 00:16:11,973
and build correct applications against.

412
00:16:13,463 --> 00:16:15,090
And so, as we were thinking about

413
00:16:15,090 --> 00:16:17,100
what we wanted to build with DSQL,

414
00:16:17,100 --> 00:16:18,270
we were talking to customers

415
00:16:18,270 --> 00:16:19,770
to understand these pain points.

416
00:16:19,770 --> 00:16:22,620
These are the kinds of
things that kept coming up.

417
00:16:22,620 --> 00:16:24,900
And if you really think about it,

418
00:16:24,900 --> 00:16:26,425
many of these problems are caused

419
00:16:26,425 --> 00:16:29,280
by the single fundamental property

420
00:16:29,280 --> 00:16:32,070
that we have this writer,
a single point of success.

421
00:16:32,070 --> 00:16:34,140
It needs to be there, it
needs to be available,

422
00:16:34,140 --> 00:16:36,570
it needs to be scaled,
it needs to be secure,

423
00:16:36,570 --> 00:16:38,460
and it needs to have enough capacity

424
00:16:38,460 --> 00:16:40,533
to do the work that you need out of it.

425
00:16:41,760 --> 00:16:45,510
Meanwhile, with DSQL, we have
an active-active architecture.

426
00:16:45,510 --> 00:16:47,607
In DSQL, any of these query processes

427
00:16:47,607 --> 00:16:49,860
can accept reads or writes,

428
00:16:49,860 --> 00:16:53,163
and all of our reads are
gonna be strongly consistent.

429
00:16:54,240 --> 00:16:55,950
And so this was the start of our journey

430
00:16:55,950 --> 00:16:57,750
as we were building DSQL.

431
00:16:57,750 --> 00:16:59,910
How can we take this architecture,

432
00:16:59,910 --> 00:17:02,460
put something like a load
balance in front of it,

433
00:17:02,460 --> 00:17:04,282
fully encapsulate all of the problems

434
00:17:04,282 --> 00:17:06,150
that we've just described,

435
00:17:06,150 --> 00:17:10,113
and offer customers a just
much simpler experience.

436
00:17:10,950 --> 00:17:13,470
And so this is our vision, our North star,

437
00:17:13,470 --> 00:17:15,960
our customer's gonna come
in with their application

438
00:17:15,960 --> 00:17:18,300
and there is simply gonna
be given a query processor

439
00:17:18,300 --> 00:17:21,050
that they can use for the
duration of their connection.

440
00:17:24,060 --> 00:17:26,040
Okay, so we are thinking
about how to build this,

441
00:17:26,040 --> 00:17:27,030
and we are working backwards.

442
00:17:27,030 --> 00:17:29,310
And the first question that we have is,

443
00:17:29,310 --> 00:17:31,260
we have this picture in mind,

444
00:17:31,260 --> 00:17:35,070
but do we have one of
these pictures per cluster?

445
00:17:35,070 --> 00:17:36,960
When you go to the DSQL API

446
00:17:36,960 --> 00:17:38,940
and you click that create cluster button,

447
00:17:38,940 --> 00:17:40,620
do you get a dedicated load balancer

448
00:17:40,620 --> 00:17:43,203
with your own query
processes running behind it.

449
00:17:44,940 --> 00:17:46,740
Which leads up to the following question,

450
00:17:46,740 --> 00:17:49,653
which is how many QP should
be behind this load balancer?

451
00:17:50,670 --> 00:17:54,093
Now, if we had a dedicated
endpoint per cluster,

452
00:17:55,320 --> 00:17:57,540
this question becomes very
difficult to answer, right.

453
00:17:57,540 --> 00:18:00,697
We would require our customers
to give us some kind of hint.

454
00:18:00,697 --> 00:18:02,310
"Hey, I need a hundred connections.

455
00:18:02,310 --> 00:18:03,930
I need 200 connections."

456
00:18:03,930 --> 00:18:07,200
And so, now each of these QPs,

457
00:18:07,200 --> 00:18:08,640
they're just a Unix process, right?

458
00:18:08,640 --> 00:18:10,710
That we are forking to accept more work.

459
00:18:10,710 --> 00:18:12,270
And so our minimum unit would be

460
00:18:12,270 --> 00:18:14,820
whatever we can pack onto
the smallest instance.

461
00:18:14,820 --> 00:18:17,910
Let's say the smallest
instance can run 50 QPs.

462
00:18:17,910 --> 00:18:21,270
That would mean that if
you created a DSQL cluster

463
00:18:21,270 --> 00:18:23,130
and you went to that little box

464
00:18:23,130 --> 00:18:24,750
where you can hit plus or minus,

465
00:18:24,750 --> 00:18:27,990
the fewest connections
you could offer is 50,

466
00:18:27,990 --> 00:18:29,640
but as we saw before, that's not enough

467
00:18:29,640 --> 00:18:31,260
because what if that machine died, right.

468
00:18:31,260 --> 00:18:34,770
We want at least redundant
capacity in other zones.

469
00:18:34,770 --> 00:18:36,180
We would also not be able to offer you

470
00:18:36,180 --> 00:18:41,040
51 connections, right, it
would be 50, or 100, or 150.

471
00:18:41,040 --> 00:18:44,130
And then we want to think
about patching, right.

472
00:18:44,130 --> 00:18:46,233
How do we do deployments to these hosts?

473
00:18:47,190 --> 00:18:49,590
And so it very quickly became clear to us

474
00:18:49,590 --> 00:18:50,850
that we should not do this.

475
00:18:50,850 --> 00:18:52,620
Instead, what we should work towards

476
00:18:52,620 --> 00:18:55,260
is a single shared endpoint,

477
00:18:55,260 --> 00:18:57,600
an endpoint that any cluster can use

478
00:18:57,600 --> 00:18:59,040
and an endpoint that simply has

479
00:18:59,040 --> 00:19:01,260
as many query processes behind it

480
00:19:01,260 --> 00:19:03,180
that are needed for any of our customers

481
00:19:03,180 --> 00:19:04,353
at any point in time.

482
00:19:05,550 --> 00:19:06,900
And if we're gonna build this endpoint,

483
00:19:06,900 --> 00:19:10,020
we need to play well with
the Postgres ecosystem

484
00:19:10,020 --> 00:19:12,960
because if DSQL was simply an AWS Service,

485
00:19:12,960 --> 00:19:15,270
we could design whatever API we wanted

486
00:19:15,270 --> 00:19:17,280
such as, "Hey, customer,

487
00:19:17,280 --> 00:19:19,140
you need to tell us which
cluster you're using

488
00:19:19,140 --> 00:19:20,790
when you run a transaction."

489
00:19:20,790 --> 00:19:23,280
But because we speak in
the Postgres wire protocol,

490
00:19:23,280 --> 00:19:24,840
we need to fit within the bounds

491
00:19:24,840 --> 00:19:26,820
of what that protocol could offer.

492
00:19:26,820 --> 00:19:29,070
And so as we were looking
around for our options,

493
00:19:29,070 --> 00:19:30,720
we very quickly settled

494
00:19:30,720 --> 00:19:33,969
on using a feature of TLS called SNI.

495
00:19:33,969 --> 00:19:36,390
And the way this works is when
you create a DSQL cluster,

496
00:19:36,390 --> 00:19:37,890
we generate a random identifier

497
00:19:37,890 --> 00:19:40,290
that's that colored bit YEA,

498
00:19:40,290 --> 00:19:43,890
and that cluster is gonna
get its own endpoint

499
00:19:43,890 --> 00:19:46,920
using a feature of Route
53 called an alias record.

500
00:19:46,920 --> 00:19:49,020
So it looks like it's its own a record,

501
00:19:49,020 --> 00:19:51,180
but in fact, it's actually just a pointer

502
00:19:51,180 --> 00:19:53,133
to our shared endpoint.

503
00:19:54,150 --> 00:19:56,280
And the way this works
is that when your client

504
00:19:56,280 --> 00:19:58,380
is establishes a connection

505
00:19:58,380 --> 00:20:00,960
and starts doing that TLS handshake,

506
00:20:00,960 --> 00:20:02,160
the first thing it's gonna do

507
00:20:02,160 --> 00:20:04,050
is send a message called the Client Hello.

508
00:20:04,050 --> 00:20:05,370
And if you see that blue bit,

509
00:20:05,370 --> 00:20:06,870
it includes the name of the server

510
00:20:06,870 --> 00:20:08,580
that it's trying to connect to.

511
00:20:08,580 --> 00:20:09,660
And then on the server side,

512
00:20:09,660 --> 00:20:11,640
we can take a look at that Client Hello,

513
00:20:11,640 --> 00:20:13,050
we can strip that name out,

514
00:20:13,050 --> 00:20:15,450
and now we know which
cluster's trying to be...

515
00:20:15,450 --> 00:20:17,283
Which cluster you're connecting to.

516
00:20:19,500 --> 00:20:21,510
So this feature server name indication

517
00:20:21,510 --> 00:20:24,707
is a TLS feature it's
not a Postgres feature,

518
00:20:24,707 --> 00:20:27,963
but it's been integrated into
Postgres since version 14.

519
00:20:28,837 --> 00:20:30,600
(person coughs)

520
00:20:30,600 --> 00:20:33,905
Okay, so now we have the
shared endpoint. We have SNI.

521
00:20:33,905 --> 00:20:35,165
And what we're gonna do

522
00:20:35,165 --> 00:20:37,794
is that we're gonna run this
big fleet of EC2 instances.

523
00:20:37,794 --> 00:20:39,510
It's the job of the service team

524
00:20:39,510 --> 00:20:41,610
to make sure that those instances

525
00:20:41,610 --> 00:20:45,150
are sufficient for all
of our capacity needs,

526
00:20:45,150 --> 00:20:47,370
and that we're gonna be taking on the job

527
00:20:47,370 --> 00:20:49,620
of making sure that they're up to date.

528
00:20:49,620 --> 00:20:51,330
And so if our application comes in,

529
00:20:51,330 --> 00:20:53,370
we could have the low
balance to do something

530
00:20:53,370 --> 00:20:56,100
like round robin and just pick out a QP,

531
00:20:56,100 --> 00:20:57,840
so here we go, one, two, three,

532
00:20:57,840 --> 00:21:00,060
we're now connected to
all three of the hosts.

533
00:21:00,060 --> 00:21:00,893
And now look at the top.

534
00:21:00,893 --> 00:21:03,270
We are gonna have another
customer come along

535
00:21:03,270 --> 00:21:05,430
this fuab red cluster.

536
00:21:05,430 --> 00:21:06,480
They're gonna come in,

537
00:21:06,480 --> 00:21:09,030
they're gonna get a connection
on the same host as us,

538
00:21:09,030 --> 00:21:10,020
all's well.

539
00:21:10,020 --> 00:21:12,543
Except, oh no, they're a baddie.

540
00:21:14,400 --> 00:21:17,070
They've found a security
vulnerability in Postgres, right.

541
00:21:17,070 --> 00:21:19,320
And they fully intend to exploit that.

542
00:21:19,320 --> 00:21:20,640
So they're connecting in,

543
00:21:20,640 --> 00:21:23,070
they're running some kind
of Bobby Drop Tables query,

544
00:21:23,070 --> 00:21:25,200
and they're gonna break out of Postgres

545
00:21:25,200 --> 00:21:27,180
and now they have access
to the environment

546
00:21:27,180 --> 00:21:28,590
that they've broken out into.

547
00:21:28,590 --> 00:21:30,690
And because they're just a process running

548
00:21:30,690 --> 00:21:34,110
on the same machine as our orange cluster,

549
00:21:34,110 --> 00:21:36,660
they're gonna have access
to everything on that host,

550
00:21:36,660 --> 00:21:38,640
and they could potentially
read or write data

551
00:21:38,640 --> 00:21:40,110
from other customers.

552
00:21:40,110 --> 00:21:42,210
So this is a big deal.

553
00:21:42,210 --> 00:21:45,450
And at AWS, the bar
for this kind of design

554
00:21:45,450 --> 00:21:47,670
is not process level isolation,

555
00:21:47,670 --> 00:21:49,050
it's not containers,

556
00:21:49,050 --> 00:21:51,033
it's hardware level virtualization.

557
00:21:51,870 --> 00:21:53,610
Now, fortunately, this is the same problem

558
00:21:53,610 --> 00:21:56,550
that our friends over in
Lambda faced many years ago,

559
00:21:56,550 --> 00:21:57,900
which they solved by building

560
00:21:57,900 --> 00:22:00,610
a hardware level virtualization

561
00:22:01,860 --> 00:22:05,610
library known as Firecracker,
which gives us a microVM.

562
00:22:05,610 --> 00:22:07,800
And by microVM, we mean
it's almost exactly

563
00:22:07,800 --> 00:22:09,690
the same thing as an EC2 instance,

564
00:22:09,690 --> 00:22:10,889
but it's designed for machines

565
00:22:10,889 --> 00:22:13,353
that use much fewer resources.

566
00:22:14,220 --> 00:22:17,490
Now, VMs launch in milliseconds
and have minimal overhead,

567
00:22:17,490 --> 00:22:21,030
but even Firecracker outta the
box wasn't quite good enough

568
00:22:21,030 --> 00:22:23,730
for the performance we wanted
to offer our customers.

569
00:22:23,730 --> 00:22:25,470
And so what we do is
we have a little agent

570
00:22:25,470 --> 00:22:26,910
running on our hosts,

571
00:22:26,910 --> 00:22:29,610
and these agents are
going to create these VMs

572
00:22:29,610 --> 00:22:31,500
ahead of time into a Warm Pool.

573
00:22:31,500 --> 00:22:33,120
So here we are filling up a Warm Pool.

574
00:22:33,120 --> 00:22:34,620
I've shown six on the slide,

575
00:22:34,620 --> 00:22:36,957
but in reality, we are
packing hundreds of these VMs

576
00:22:36,957 --> 00:22:39,360
onto a single machine.

577
00:22:39,360 --> 00:22:41,943
And then when our application
is coming to connect

578
00:22:41,943 --> 00:22:44,640
and it's been assigned this host,

579
00:22:44,640 --> 00:22:49,170
we can simply take one of
these QPs out of the pool,

580
00:22:49,170 --> 00:22:51,900
and then we connect directly into that QP

581
00:22:51,900 --> 00:22:52,950
and then in the background,

582
00:22:52,950 --> 00:22:55,002
the agents is gonna go
fill up the warm pool.

583
00:22:55,002 --> 00:22:57,000
And because this is an
in-memory operation,

584
00:22:57,000 --> 00:22:59,201
it's essentially popping
an item from a linked list

585
00:22:59,201 --> 00:23:01,920
and establishing a TCP connection,

586
00:23:01,920 --> 00:23:04,533
this happens really fast
and with minimal overhead.

587
00:23:08,010 --> 00:23:10,800
However, there is a really

588
00:23:10,800 --> 00:23:12,780
complicated economic challenge here.

589
00:23:12,780 --> 00:23:14,490
And so in order to explain this to you,

590
00:23:14,490 --> 00:23:17,520
let's look more closely
at how connections work

591
00:23:17,520 --> 00:23:19,050
in standard Postgres.

592
00:23:19,050 --> 00:23:21,510
So applications connected into the server,

593
00:23:21,510 --> 00:23:22,800
and you can see that that server

594
00:23:22,800 --> 00:23:24,750
is using some amount of memory, right?

595
00:23:24,750 --> 00:23:28,740
The dotted box represents
memory available to the system.

596
00:23:28,740 --> 00:23:31,470
Each of these blocks is
something like a page of memory,

597
00:23:31,470 --> 00:23:33,030
and you can see that our servers

598
00:23:33,030 --> 00:23:34,980
using many hundreds of megabytes of memory

599
00:23:34,980 --> 00:23:35,930
just to be running.

600
00:23:36,990 --> 00:23:40,410
Now, when we fork one of
these backend processes,

601
00:23:40,410 --> 00:23:44,280
even though that new process
has essentially the same amount

602
00:23:44,280 --> 00:23:46,050
of memory requirements to run,

603
00:23:46,050 --> 00:23:48,060
the operating system is gonna be cunning,

604
00:23:48,060 --> 00:23:49,800
and it's gonna use pointers

605
00:23:49,800 --> 00:23:51,840
to say that actually
this underlying memory

606
00:23:51,840 --> 00:23:53,850
that I cloned from has not been modified.

607
00:23:53,850 --> 00:23:56,040
And so that both of these processes

608
00:23:56,040 --> 00:23:59,067
can share the same physical pages.

609
00:23:59,067 --> 00:24:00,840
And so as we open more connections,

610
00:24:00,840 --> 00:24:02,490
we can keep doing this trick,

611
00:24:02,490 --> 00:24:04,560
and this is gonna significantly reduce

612
00:24:04,560 --> 00:24:07,110
the overhead of running
these additional processes.

613
00:24:08,010 --> 00:24:09,690
Now, each of these connections, of course,

614
00:24:09,690 --> 00:24:11,790
needs its own memory, right.

615
00:24:11,790 --> 00:24:13,170
Pay attention to the orange one.

616
00:24:13,170 --> 00:24:14,940
It's running some kind of query.

617
00:24:14,940 --> 00:24:17,700
And so it's claiming new memory pages

618
00:24:17,700 --> 00:24:19,740
as it's doing something
like loading all your data

619
00:24:19,740 --> 00:24:21,180
into memories that are consulted

620
00:24:21,180 --> 00:24:23,230
or run some sort of aggregation function.

621
00:24:28,080 --> 00:24:30,480
Meanwhile, over in our Firecracker world,

622
00:24:30,480 --> 00:24:33,240
we have something that
looks very different.

623
00:24:33,240 --> 00:24:35,790
Here's a VM we've just
launched it into the pool

624
00:24:35,790 --> 00:24:37,920
and it's running a full operating system,

625
00:24:37,920 --> 00:24:39,570
a full copy of Linux.

626
00:24:39,570 --> 00:24:41,160
It's running our Postgres server

627
00:24:41,160 --> 00:24:43,263
that's gonna handle just one connection,

628
00:24:44,100 --> 00:24:46,290
and it's also got a bunch
of additional services

629
00:24:46,290 --> 00:24:48,570
that our team installed into that Sandbox.

630
00:24:48,570 --> 00:24:50,700
So we can do things like monitor,

631
00:24:50,700 --> 00:24:52,380
manage system lifecycle,

632
00:24:52,380 --> 00:24:54,183
get logs out of the Sandbox.

633
00:24:55,110 --> 00:24:56,250
And if you add all of this up,

634
00:24:56,250 --> 00:24:57,990
we are looking at many
hundreds of megabytes.

635
00:24:57,990 --> 00:25:00,150
Just pretend it's 700 megabytes.

636
00:25:00,150 --> 00:25:01,980
And if we do this a couple of times,

637
00:25:01,980 --> 00:25:03,830
we're almost at three gigs of memory.

638
00:25:05,700 --> 00:25:07,113
So what do we do about this?

639
00:25:08,070 --> 00:25:10,980
A Firecracker has a
really cool trick here,

640
00:25:10,980 --> 00:25:13,800
which was again, developed by
our friends over in Lambda,

641
00:25:13,800 --> 00:25:16,590
which we've blogged about
extensively called SnapStart.

642
00:25:16,590 --> 00:25:18,690
And the way this works is
that when our host comes up

643
00:25:18,690 --> 00:25:21,990
for the very first time, it's
gonna create one of these VMs.

644
00:25:21,990 --> 00:25:23,760
I call it the seed
because we're gonna use it

645
00:25:23,760 --> 00:25:25,800
to grow new VMs.

646
00:25:25,800 --> 00:25:27,060
And when the seed comes up,

647
00:25:27,060 --> 00:25:28,680
we do everything that you might expect.

648
00:25:28,680 --> 00:25:30,200
We launch Linux, we start Postgres,

649
00:25:30,200 --> 00:25:31,920
we make sure everything's healthy,

650
00:25:31,920 --> 00:25:33,330
and we get right to the point

651
00:25:33,330 --> 00:25:35,850
where we're ready to accept
our first connection.

652
00:25:35,850 --> 00:25:38,760
And then we talk to our
agent, we say, "We're ready."

653
00:25:38,760 --> 00:25:41,670
What the agent's gonna do
is it's gonna pause that VM

654
00:25:41,670 --> 00:25:44,730
and it's gonna take all of
them, all of those memory pages,

655
00:25:44,730 --> 00:25:47,250
and write them to a file
called the MEM file.

656
00:25:47,250 --> 00:25:49,440
It's gonna be persistent on disc.

657
00:25:49,440 --> 00:25:51,990
And then at this point,
our seed has done its job.

658
00:25:51,990 --> 00:25:53,550
We can kill it.

659
00:25:53,550 --> 00:25:54,600
Now that we have our seed,

660
00:25:54,600 --> 00:25:59,010
we can start new clones from
that seed, we can grow new VMs.

661
00:25:59,010 --> 00:26:01,980
And these VMs have the
exact same memory layout.

662
00:26:01,980 --> 00:26:03,780
In fact, they're identical copies.

663
00:26:03,780 --> 00:26:06,270
They think they have the same IP address.

664
00:26:06,270 --> 00:26:10,170
They think that time is
what the original seed had.

665
00:26:10,170 --> 00:26:11,640
They think they have the same Mac address.

666
00:26:11,640 --> 00:26:13,233
They're identical in every way.

667
00:26:14,460 --> 00:26:16,080
Except just like with fork,

668
00:26:16,080 --> 00:26:19,890
because we've memory mapped
in this file on disc.

669
00:26:19,890 --> 00:26:21,390
The contents of this VM

670
00:26:21,390 --> 00:26:25,350
are actually just pointers
to the original snapshot.

671
00:26:25,350 --> 00:26:27,300
So far we haven't saved any memory,

672
00:26:27,300 --> 00:26:30,840
but as we start to launch additional VMs

673
00:26:30,840 --> 00:26:32,160
from that same snapshot,

674
00:26:32,160 --> 00:26:36,390
we get to use that same
pointer chasing technique.

675
00:26:36,390 --> 00:26:37,800
And just like before,

676
00:26:37,800 --> 00:26:40,740
as one of our VM starts to do unique work,

677
00:26:40,740 --> 00:26:42,420
it can claim additional memory

678
00:26:42,420 --> 00:26:44,793
and we are only paying
for that unique memory.

679
00:26:47,670 --> 00:26:48,503
Okay.

680
00:26:49,940 --> 00:26:53,970
Instead of just doing round
robin across these VMs,

681
00:26:53,970 --> 00:26:56,136
we have a service that runs
behind the load balance

682
00:26:56,136 --> 00:26:58,800
called the Relay service.

683
00:26:58,800 --> 00:27:00,630
And when your application connects in

684
00:27:00,630 --> 00:27:02,790
and it starts doing that TLS handshake,

685
00:27:02,790 --> 00:27:05,460
it is the relay that it's talking to.

686
00:27:05,460 --> 00:27:08,580
Relay is a service that we built
from the ground up in Rust,

687
00:27:08,580 --> 00:27:10,439
and it uses the s2n library

688
00:27:10,439 --> 00:27:12,641
so that we can just have assurance

689
00:27:12,641 --> 00:27:15,870
about the security of the service.

690
00:27:15,870 --> 00:27:20,733
And it uses s2ns capability
to parse that SNI value.

691
00:27:22,350 --> 00:27:24,330
Now, the next thing your
application is gonna do

692
00:27:24,330 --> 00:27:27,840
is send over what's called
an authentication token,

693
00:27:27,840 --> 00:27:30,570
which I just wanna take
a moment to explain.

694
00:27:30,570 --> 00:27:33,480
Typically, when you're using
an AWS service like S3,

695
00:27:33,480 --> 00:27:35,640
let's say we're doing
a get object request,

696
00:27:35,640 --> 00:27:37,200
that is an HDP request, right.

697
00:27:37,200 --> 00:27:39,150
We have a bunch of
headers, we have a body,

698
00:27:39,150 --> 00:27:41,640
and your SDK is going to sign that request

699
00:27:41,640 --> 00:27:44,610
using your AWS credentials
to produce a new header

700
00:27:44,610 --> 00:27:46,923
using the Signature
version four algorithm.

701
00:27:47,910 --> 00:27:51,450
And this signature is used
to prevent any tampering.

702
00:27:51,450 --> 00:27:53,850
So if somebody was able
to intercept that request

703
00:27:53,850 --> 00:27:55,140
and try and change the bucket

704
00:27:55,140 --> 00:27:57,480
or the object key that you
were trying to download,

705
00:27:57,480 --> 00:27:59,730
then the signature would no longer match.

706
00:27:59,730 --> 00:28:00,760
The other thing the signature does

707
00:28:00,760 --> 00:28:04,890
is it allows the S3 service to
understand who the caller is,

708
00:28:04,890 --> 00:28:08,343
so we can do any
authorization enforcement.

709
00:28:09,390 --> 00:28:13,350
Now, S3 is a really cool
feature called pre-signed URLs,

710
00:28:13,350 --> 00:28:15,510
where instead of actually
signing that request

711
00:28:15,510 --> 00:28:16,800
and sending it over the network,

712
00:28:16,800 --> 00:28:19,260
you can just take a frozen
version of that request,

713
00:28:19,260 --> 00:28:20,550
it looks like a URL,

714
00:28:20,550 --> 00:28:23,220
it has the signature as a query parameter,

715
00:28:23,220 --> 00:28:25,890
and you can share that URL
with one of your friends

716
00:28:25,890 --> 00:28:28,470
and they will be able
to just go to that URL

717
00:28:28,470 --> 00:28:30,399
without an AWS SDK and download the object

718
00:28:30,399 --> 00:28:32,490
that you gave them permission to.

719
00:28:32,490 --> 00:28:34,350
You're not giving them your credentials.

720
00:28:34,350 --> 00:28:35,970
They can't write data to your bucket,

721
00:28:35,970 --> 00:28:38,787
and they can only download
precisely what you asked them to.

722
00:28:38,787 --> 00:28:40,530
The other thing you can do

723
00:28:40,530 --> 00:28:43,620
with SIG V4 request is put an expiry in

724
00:28:43,620 --> 00:28:46,050
for requests that are sent by your SDK,

725
00:28:46,050 --> 00:28:48,810
they typically are expiring
something like five minutes,

726
00:28:48,810 --> 00:28:50,670
but with an S3 pre-sign URL,

727
00:28:50,670 --> 00:28:53,700
you can customize that
value for up to a week.

728
00:28:53,700 --> 00:28:56,910
So this is the underlying
machinery that DSQL uses.

729
00:28:56,910 --> 00:28:58,440
It's actually the same machinery

730
00:28:58,440 --> 00:29:01,860
that I am authentication and RDS uses.

731
00:29:01,860 --> 00:29:04,110
And so what your
application is going to do

732
00:29:04,110 --> 00:29:06,450
is use the DSQL SDK to generate one

733
00:29:06,450 --> 00:29:08,460
of these authentication tokens.

734
00:29:08,460 --> 00:29:09,450
This is very fast.

735
00:29:09,450 --> 00:29:11,310
It takes us a few nanoseconds

736
00:29:11,310 --> 00:29:14,430
because this is something that
AWS has invested in heavily.

737
00:29:14,430 --> 00:29:16,650
If you're talking to S3 or DynamoDB,

738
00:29:16,650 --> 00:29:19,470
every single request you
make is being signed.

739
00:29:19,470 --> 00:29:21,210
But what we are doing
here is just generating

740
00:29:21,210 --> 00:29:23,280
one of these tokens periodically

741
00:29:23,280 --> 00:29:24,963
or once per connection request.

742
00:29:25,800 --> 00:29:27,750
And this allows QSQL to integrate

743
00:29:27,750 --> 00:29:31,650
with the IAM Control plane,
sorry, the IAM Data plane.

744
00:29:31,650 --> 00:29:35,100
And it's gonna do some
fetching of your keys

745
00:29:35,100 --> 00:29:36,150
for your account,

746
00:29:36,150 --> 00:29:38,460
fetching of your policies and your tags.

747
00:29:38,460 --> 00:29:40,350
And it does this very efficiently

748
00:29:40,350 --> 00:29:44,471
so that relay can authenticate
and authorize your requests

749
00:29:44,471 --> 00:29:47,493
in many times locally using its cache.

750
00:29:49,500 --> 00:29:52,170
At this point, we have
established your AWS identity.

751
00:29:52,170 --> 00:29:53,658
We're able to enforce any policies.

752
00:29:53,658 --> 00:29:55,831
For example, you can say,

753
00:29:55,831 --> 00:29:58,170
users in this group or users in this role

754
00:29:58,170 --> 00:30:01,380
can only access clusters
tagged in certain ways.

755
00:30:01,380 --> 00:30:04,440
And if you've used
resource-based policies on DSQL,

756
00:30:04,440 --> 00:30:06,900
this is a really great place
to enforce things like,

757
00:30:06,900 --> 00:30:09,570
I must be connecting
from certain IP addresses

758
00:30:09,570 --> 00:30:10,893
or through private link.

759
00:30:12,594 --> 00:30:15,243
After writing login
attempts at CloudTrail,

760
00:30:15,243 --> 00:30:18,600
DSQL has established the
identity of your user,

761
00:30:18,600 --> 00:30:21,063
and it'll talk to a placement service.

762
00:30:22,020 --> 00:30:23,390
Now, the job of the placement service

763
00:30:23,390 --> 00:30:24,810
is to keep track of our fleet,

764
00:30:24,810 --> 00:30:26,970
we'll go into that more in a few minutes,

765
00:30:26,970 --> 00:30:29,059
and suggest one of these query processes

766
00:30:29,059 --> 00:30:31,710
for your application to connect to.

767
00:30:31,710 --> 00:30:34,350
So we've picked a host, we've picked a QP,

768
00:30:34,350 --> 00:30:37,140
and then the relay is simply going to send

769
00:30:37,140 --> 00:30:39,423
your connection over to the QP.

770
00:30:40,560 --> 00:30:42,480
At this point, the relay
doesn't really have

771
00:30:42,480 --> 00:30:44,220
anything interesting to do.

772
00:30:44,220 --> 00:30:45,660
You see, the relay is a service

773
00:30:45,660 --> 00:30:46,740
that we built from the ground up.

774
00:30:46,740 --> 00:30:48,270
It's written in Rust.

775
00:30:48,270 --> 00:30:51,150
And those first few
messages, the TLS handshake,

776
00:30:51,150 --> 00:30:54,960
the Postgres authentication messages,

777
00:30:54,960 --> 00:30:57,960
this requires complex protocol parsing,

778
00:30:57,960 --> 00:31:02,941
the Postgres protocol
can lead you to making

779
00:31:02,941 --> 00:31:05,100
some very silly security mistakes.

780
00:31:05,100 --> 00:31:07,140
For example, messages have a length,

781
00:31:07,140 --> 00:31:08,430
and if you get that length wrong

782
00:31:08,430 --> 00:31:09,810
when you're assigning buffers,

783
00:31:09,810 --> 00:31:12,210
you can have all sorts of
overflow and underflow attacks

784
00:31:12,210 --> 00:31:14,130
or arbitrary code execution.

785
00:31:14,130 --> 00:31:16,380
And so the way we approached this in relay

786
00:31:16,380 --> 00:31:18,840
is that we designed it to process

787
00:31:18,840 --> 00:31:20,850
as few packets as possible.

788
00:31:20,850 --> 00:31:24,210
And when we implemented
the protocol parsing code

789
00:31:24,210 --> 00:31:26,575
for those packets, we
were extremely careful

790
00:31:26,575 --> 00:31:28,890
and worked closely with our security teams

791
00:31:28,890 --> 00:31:31,260
to make sure that we
had done this correctly.

792
00:31:31,260 --> 00:31:33,090
But now that we've done that work,

793
00:31:33,090 --> 00:31:35,250
relay really has nothing
interesting to do.

794
00:31:35,250 --> 00:31:37,290
Everything else that's gonna
happen on this connection

795
00:31:37,290 --> 00:31:39,990
should be handled by Postgres.

796
00:31:39,990 --> 00:31:43,080
And so what we do is
we move the TLS session

797
00:31:43,080 --> 00:31:44,820
into the Sandbox.

798
00:31:44,820 --> 00:31:46,710
Now this is super cool
and the way it works

799
00:31:46,710 --> 00:31:49,440
is that a TLS session is a
layer seven concept, right.

800
00:31:49,440 --> 00:31:52,530
It's happening right at
the top of the OSI model.

801
00:31:52,530 --> 00:31:56,280
And we take all of the
ephemeral encryption keys

802
00:31:56,280 --> 00:31:58,170
that are used for that session,

803
00:31:58,170 --> 00:31:59,970
and we package them over the network

804
00:31:59,970 --> 00:32:03,450
and we securely send them
into the query processor.

805
00:32:03,450 --> 00:32:06,540
At this point, we are gonna use a function

806
00:32:06,540 --> 00:32:08,910
that's zeroes out the memory in relay.

807
00:32:08,910 --> 00:32:12,360
And so Relay can no longer
decode anything else

808
00:32:12,360 --> 00:32:13,960
that happens on this connection.

809
00:32:14,910 --> 00:32:16,500
And so if our application sends

810
00:32:16,500 --> 00:32:18,540
any data into the query processor,

811
00:32:18,540 --> 00:32:19,950
it's gonna take that data,

812
00:32:19,950 --> 00:32:22,110
it's gonna send it over a TLS connection,

813
00:32:22,110 --> 00:32:23,430
it's getting encrypted,

814
00:32:23,430 --> 00:32:26,760
it's gonna flow encrypted
through the network load balancer

815
00:32:26,760 --> 00:32:27,593
through the relay,

816
00:32:27,593 --> 00:32:29,880
and it's gonna arrive at the QP,

817
00:32:29,880 --> 00:32:33,243
which now has the keys
necessary to decrypt our data.

818
00:32:37,710 --> 00:32:39,260
Okay, let's talk about Pooling.

819
00:32:40,680 --> 00:32:43,260
When our application connects
into one of these hosts,

820
00:32:43,260 --> 00:32:46,170
instead of simply taking one of these QPs

821
00:32:46,170 --> 00:32:48,690
out of the warm pool and that's it,

822
00:32:48,690 --> 00:32:52,203
what we instead do is we move
it into a per cluster pool,

823
00:32:53,070 --> 00:32:55,770
which is gonna hold
capacity just for that host.

824
00:32:55,770 --> 00:32:56,940
And the reason we do that is,

825
00:32:56,940 --> 00:32:59,430
watch what happens when we
open the second connection.

826
00:32:59,430 --> 00:33:02,700
We have not dragged an additional
QP out of the warm pool.

827
00:33:02,700 --> 00:33:04,530
Now, I forgot to fill the
warm pool on the slide,

828
00:33:04,530 --> 00:33:07,863
but keep in mind that the agent
is continuously doing that.

829
00:33:09,120 --> 00:33:11,340
And so now that first
connection we opened,

830
00:33:11,340 --> 00:33:12,843
it started to flash.

831
00:33:13,950 --> 00:33:16,020
What's gonna happen is
it's gonna be able to use

832
00:33:16,020 --> 00:33:19,710
one of these QPs directly
from the pool, right.

833
00:33:19,710 --> 00:33:21,180
But we have two connections open.

834
00:33:21,180 --> 00:33:24,240
What happens if that other
connection becomes busy?

835
00:33:24,240 --> 00:33:26,460
Well, you don't have to
do anything about this.

836
00:33:26,460 --> 00:33:29,310
DSQL is going to
automatically take another QP

837
00:33:29,310 --> 00:33:30,390
out of the warm pool.

838
00:33:30,390 --> 00:33:33,540
Again, these query processes
are already running.

839
00:33:33,540 --> 00:33:35,970
And so this happens extremely quickly

840
00:33:35,970 --> 00:33:38,673
with a very minimal performance impact.

841
00:33:40,080 --> 00:33:41,460
And the reason we do this is

842
00:33:41,460 --> 00:33:42,990
because think back to those problems

843
00:33:42,990 --> 00:33:44,460
that our customers were telling about

844
00:33:44,460 --> 00:33:45,420
right in the beginning.

845
00:33:45,420 --> 00:33:49,110
Like as customers want to scale
their application fleet out,

846
00:33:49,110 --> 00:33:51,990
they want to deal with this
peak to average problem.

847
00:33:51,990 --> 00:33:54,180
And so typically open
many more connections

848
00:33:54,180 --> 00:33:56,430
than they actively intend to use,

849
00:33:56,430 --> 00:33:59,220
and we didn't want DSQL customers

850
00:33:59,220 --> 00:34:03,150
to have to worry about
anything like running PgBouncer

851
00:34:03,150 --> 00:34:04,800
or running RDS proxy

852
00:34:04,800 --> 00:34:07,290
because that would just add
another hop in the network,

853
00:34:07,290 --> 00:34:10,260
that would be something
else you'd have to pay for,

854
00:34:10,260 --> 00:34:11,640
you would have to run many of them

855
00:34:11,640 --> 00:34:13,680
to great get high availability.

856
00:34:13,680 --> 00:34:15,480
And it would be have to be something

857
00:34:15,480 --> 00:34:18,990
that you scale in addition
to everything else in DSQL.

858
00:34:18,990 --> 00:34:20,880
And so by doing it like this,

859
00:34:20,880 --> 00:34:22,980
we've built pooling into the service

860
00:34:22,980 --> 00:34:25,730
and there's simply no need
for you to worry about that.

861
00:34:26,970 --> 00:34:29,250
Now, that placement service
I mentioned earlier,

862
00:34:29,250 --> 00:34:33,090
what it's doing is continuously
patrolling the fleet.

863
00:34:33,090 --> 00:34:35,900
So at a very high frequency,
each of our placement services

864
00:34:35,900 --> 00:34:37,980
is connecting into these agents,

865
00:34:37,980 --> 00:34:39,367
and it's asking it questions,

866
00:34:39,367 --> 00:34:40,740
"How busy are you?

867
00:34:40,740 --> 00:34:42,630
How many connections do you have?

868
00:34:42,630 --> 00:34:44,040
How many of these are being used?

869
00:34:44,040 --> 00:34:45,240
How much memory do you have?

870
00:34:45,240 --> 00:34:47,370
What is the CPU load?

871
00:34:47,370 --> 00:34:49,830
For these specific clusters,

872
00:34:49,830 --> 00:34:51,990
how many Sandboxes do you have?

873
00:34:51,990 --> 00:34:53,640
How many of them are being used?"

874
00:34:53,640 --> 00:34:56,100
And it's doing this all the time.

875
00:34:56,100 --> 00:34:58,440
The placement service is
another service we built

876
00:34:58,440 --> 00:34:59,850
from the ground up in Rust

877
00:34:59,850 --> 00:35:02,010
and it does all of this in memory.

878
00:35:02,010 --> 00:35:04,830
And so when the relay
asks the placement service

879
00:35:04,830 --> 00:35:06,690
where it should place a new connection,

880
00:35:06,690 --> 00:35:08,520
the placement service
can do this very quickly

881
00:35:08,520 --> 00:35:12,003
because it has very current
information sitting in memory.

882
00:35:13,950 --> 00:35:15,570
And because of this, I'm not
sure if you can read that,

883
00:35:15,570 --> 00:35:18,210
that bottom line, this
is from our coder's page.

884
00:35:18,210 --> 00:35:20,250
By default, a DSQL cluster

885
00:35:20,250 --> 00:35:22,440
has a limit of 10,000 connections,

886
00:35:22,440 --> 00:35:24,846
which is twice what you
get out of the biggest

887
00:35:24,846 --> 00:35:26,790
RDS instance size.

888
00:35:26,790 --> 00:35:28,290
And if you want, you can configure it.

889
00:35:28,290 --> 00:35:29,850
We have customers who are running

890
00:35:29,850 --> 00:35:32,000
with many, many more
connections than this.

891
00:35:35,157 --> 00:35:37,170
Okay, so we are opening connections.

892
00:35:37,170 --> 00:35:38,910
Here, we have three open,

893
00:35:38,910 --> 00:35:42,120
and it turns out one of our
metal instances has failed,

894
00:35:42,120 --> 00:35:44,100
or maybe one of our relays has failed.

895
00:35:44,100 --> 00:35:45,600
What's gonna happen?

896
00:35:45,600 --> 00:35:48,249
Instead of experiencing a
full application outage,

897
00:35:48,249 --> 00:35:53,249
we're gonna experience a one
third drop in connectivity.

898
00:35:53,490 --> 00:35:55,530
Two of our connections
are still good to go.

899
00:35:55,530 --> 00:35:57,510
And what our application needs to do

900
00:35:57,510 --> 00:35:59,730
is open a third connection again,

901
00:35:59,730 --> 00:36:02,640
it's gonna flow through the NLB,

902
00:36:02,640 --> 00:36:04,530
it's gonna find a healthy relay,

903
00:36:04,530 --> 00:36:06,810
and it's gonna talk to a
healthy placement service

904
00:36:06,810 --> 00:36:09,063
and get us a new query processor.

905
00:36:10,080 --> 00:36:14,100
Now, we can't tell your
application to reconnect, right.

906
00:36:14,100 --> 00:36:17,100
There are many kinds of failure
that can happen within DSQL,

907
00:36:17,100 --> 00:36:20,520
and for almost all of
these kinds of failures,

908
00:36:20,520 --> 00:36:24,060
DSQL is gonna take care of
it automatically for you,

909
00:36:24,060 --> 00:36:26,850
but we can't have your
application reconnect.

910
00:36:26,850 --> 00:36:30,450
And so, what we decided is to put a limit

911
00:36:30,450 --> 00:36:32,940
of one hour on connection age.

912
00:36:32,940 --> 00:36:34,110
And so after an hour,

913
00:36:34,110 --> 00:36:36,510
which is gonna be really
early on into your experience

914
00:36:36,510 --> 00:36:38,670
building applications with DSQL,

915
00:36:38,670 --> 00:36:40,473
either you hang up or we hang up.

916
00:36:41,790 --> 00:36:43,980
Now, don't worry, we're not
gonna close the connection

917
00:36:43,980 --> 00:36:45,000
while you're using it,

918
00:36:45,000 --> 00:36:47,370
like if you happen to start a transaction,

919
00:36:47,370 --> 00:36:49,800
you know, at 59 minutes and 59 seconds,

920
00:36:49,800 --> 00:36:51,630
we'll actually give you
some breathing room.

921
00:36:51,630 --> 00:36:53,850
We have some smart things
going on like jitters,

922
00:36:53,850 --> 00:36:56,520
so we don't close all your
connections at the same time.

923
00:36:56,520 --> 00:36:58,140
But the best practice here

924
00:36:58,140 --> 00:37:00,330
is to use a client site
connection pooling library,

925
00:37:00,330 --> 00:37:01,740
like we spoke about earlier.

926
00:37:01,740 --> 00:37:04,530
You want to take all that
slow and expensive TLS set up,

927
00:37:04,530 --> 00:37:06,660
credential exchange placement,

928
00:37:06,660 --> 00:37:08,610
move that off the critical path.

929
00:37:08,610 --> 00:37:11,700
That's the best practice
anyways, so while you're at it,

930
00:37:11,700 --> 00:37:13,530
configure that client site pooling library

931
00:37:13,530 --> 00:37:15,480
to have a maximum age of one hour.

932
00:37:15,480 --> 00:37:17,400
If you want, you can do health checks.

933
00:37:17,400 --> 00:37:18,233
And that's gonna mean

934
00:37:18,233 --> 00:37:20,310
that when this kind of failure happens,

935
00:37:20,310 --> 00:37:23,040
probably what your
application is going to do

936
00:37:23,040 --> 00:37:26,190
is simply retry locally
against your connection pool

937
00:37:26,190 --> 00:37:27,690
and find a healthy connection.

938
00:37:30,780 --> 00:37:34,380
Now, Postgres is a fairly chatty protocol.

939
00:37:34,380 --> 00:37:35,670
I think when it was designed,

940
00:37:35,670 --> 00:37:38,610
probably the client and the
server were on the same machine.

941
00:37:38,610 --> 00:37:39,960
But even so, when you're running

942
00:37:39,960 --> 00:37:41,580
these interactive transactions

943
00:37:41,580 --> 00:37:45,870
where we're sending selects
from your client to the server

944
00:37:45,870 --> 00:37:48,540
getting results back in the application,

945
00:37:48,540 --> 00:37:50,940
then your application is
able to look at those results

946
00:37:50,940 --> 00:37:53,040
and run complex business logic

947
00:37:53,040 --> 00:37:54,690
and off we go back and forth,

948
00:37:54,690 --> 00:37:57,897
every time we go across the
network, we're spending time.

949
00:37:57,897 --> 00:37:59,880
And so what we wanted to do in DSQL

950
00:37:59,880 --> 00:38:02,343
is give you the best possible performance.

951
00:38:03,330 --> 00:38:06,370
And the way we do this is
with the Route 53 feature

952
00:38:07,350 --> 00:38:10,020
where your EC2's nitro card is gonna send

953
00:38:10,020 --> 00:38:13,140
some additional information
as part of the DNS request,

954
00:38:13,140 --> 00:38:15,990
which is gonna tag it
with the availability zone

955
00:38:15,990 --> 00:38:17,610
that that server is in.

956
00:38:17,610 --> 00:38:20,048
And so when you connect
to DSQL, by default,

957
00:38:20,048 --> 00:38:23,250
your application is going
to go through an NLB host

958
00:38:23,250 --> 00:38:26,730
in the same zone through to a
relay host in the same zone,

959
00:38:26,730 --> 00:38:29,400
which is gonna talk to a
placement host in the same zone,

960
00:38:29,400 --> 00:38:31,650
which is gonna give you
a QP in the same zone,

961
00:38:31,650 --> 00:38:33,900
which is gonna talk to
storage in the same zone.

962
00:38:33,900 --> 00:38:34,733
And by doing this,

963
00:38:34,733 --> 00:38:37,680
we're giving you the
lowest possible latency.

964
00:38:37,680 --> 00:38:39,870
And so as we open additional connections,

965
00:38:39,870 --> 00:38:42,180
we're taking advantage of DSQLs

966
00:38:42,180 --> 00:38:44,550
active-active strong consistency.

967
00:38:44,550 --> 00:38:47,280
But in the event that one of our AZs

968
00:38:47,280 --> 00:38:49,530
are completely unavailable,

969
00:38:49,530 --> 00:38:52,380
the load balance is gonna
fall back to a healthy zone.

970
00:38:52,380 --> 00:38:54,330
This is gonna give you a connection

971
00:38:54,330 --> 00:38:55,590
with slightly higher latency,

972
00:38:55,590 --> 00:38:57,483
but the system will remain available.

973
00:38:58,920 --> 00:39:00,700
Any internal failures, for example here,

974
00:39:00,700 --> 00:39:03,720
one of our storage
replicas in AZ3 has failed,

975
00:39:03,720 --> 00:39:06,240
is gonna be automatically
handled by the service

976
00:39:06,240 --> 00:39:08,880
by picking a healthy
replica in the same zone.

977
00:39:08,880 --> 00:39:10,380
And in the event that
there are none of those,

978
00:39:10,380 --> 00:39:11,583
it'll also go across AZ.

979
00:39:13,650 --> 00:39:15,720
So this is a really cool
feature of Route 53.

980
00:39:15,720 --> 00:39:16,553
You know, you can use it

981
00:39:16,553 --> 00:39:18,300
in your own applications if you want.

982
00:39:18,300 --> 00:39:20,370
And there's something that
we are very happy about

983
00:39:20,370 --> 00:39:23,490
because even though DSQL being
a distributed architecture

984
00:39:23,490 --> 00:39:26,880
would appear to have
more hops in this case,

985
00:39:26,880 --> 00:39:29,130
there are actually cases
where connecting to DSQL

986
00:39:29,130 --> 00:39:33,420
can be faster than connecting
to a single writer endpoint

987
00:39:33,420 --> 00:39:35,970
because we're able to give
you a QP in the same zone

988
00:39:35,970 --> 00:39:37,440
no matter where the primary is,

989
00:39:37,440 --> 00:39:39,273
because in DSQL there is no primary.

990
00:39:42,120 --> 00:39:44,729
Okay, let's talk about pricing.

991
00:39:44,729 --> 00:39:48,390
DSQL is designed for
activity-based pricing.

992
00:39:48,390 --> 00:39:50,610
So if you're familiar with DynamoDB,

993
00:39:50,610 --> 00:39:53,670
where as you send, put
item, and get item requests,

994
00:39:53,670 --> 00:39:55,500
you're paying for right capacity units,

995
00:39:55,500 --> 00:39:57,480
you're paying for re-capacity units

996
00:39:57,480 --> 00:39:58,830
based on the number of bytes

997
00:39:58,830 --> 00:40:00,390
that you're putting into the system,

998
00:40:00,390 --> 00:40:02,310
based on the number of
bytes that you're visiting

999
00:40:02,310 --> 00:40:03,840
when you do those reads.

1000
00:40:03,840 --> 00:40:06,180
DSQL is a very similar concept,

1001
00:40:06,180 --> 00:40:08,490
but we are actually gonna take all usage

1002
00:40:08,490 --> 00:40:09,960
and talk about it in what we call

1003
00:40:09,960 --> 00:40:12,633
a Distributed Processing Unit or a DPU.

1004
00:40:14,327 --> 00:40:18,270
At the end of the month, you'll
see a DPU on your AWS bill.

1005
00:40:18,270 --> 00:40:20,184
But actually, if you go
into the usage metrics

1006
00:40:20,184 --> 00:40:22,470
on your DSQL clusters page,

1007
00:40:22,470 --> 00:40:25,260
you'll see that we break
out these DPU usages

1008
00:40:25,260 --> 00:40:26,093
in several ways.

1009
00:40:26,093 --> 00:40:28,110
So let me explain that quickly.

1010
00:40:28,110 --> 00:40:29,967
The QP is running Postgres,

1011
00:40:29,967 --> 00:40:32,250
and it's kind of like a
Lambda function, right.

1012
00:40:32,250 --> 00:40:35,520
In the sense that you can
ask this QP to do work,

1013
00:40:35,520 --> 00:40:37,260
you can have it do Fibonacci if you want.

1014
00:40:37,260 --> 00:40:40,050
It doesn't actually have
to do any reads or writes.

1015
00:40:40,050 --> 00:40:43,500
And so as you consume time
on these query processes,

1016
00:40:43,500 --> 00:40:47,280
we're gonna capture that time
through a compute DPU metric,

1017
00:40:47,280 --> 00:40:48,750
which is essentially the seconds

1018
00:40:48,750 --> 00:40:51,750
that you were active on this
QP, so it's per second billing.

1019
00:40:52,860 --> 00:40:56,160
When the QP does reads over
the network to storage,

1020
00:40:56,160 --> 00:40:58,320
we're gonna capture the reads that you do

1021
00:40:58,320 --> 00:41:00,210
in the same way that we
would do in DynamoDB.

1022
00:41:00,210 --> 00:41:02,940
We are looking at the
bytes that you're visiting.

1023
00:41:02,940 --> 00:41:06,720
Of course, if you're scanning
a table, right, with a filter,

1024
00:41:06,720 --> 00:41:08,840
you may visit many bytes in the table,

1025
00:41:08,840 --> 00:41:11,340
but the return bytes
may be smaller, right.

1026
00:41:11,340 --> 00:41:13,350
So think about how many
bytes you're visiting

1027
00:41:13,350 --> 00:41:16,920
as part of your cost optimization.

1028
00:41:16,920 --> 00:41:17,940
And then on the right path,

1029
00:41:17,940 --> 00:41:20,670
what we're doing is we are
looking at that commit payload,

1030
00:41:20,670 --> 00:41:21,900
and we are looking at how many bytes

1031
00:41:21,900 --> 00:41:23,220
you're putting into the system

1032
00:41:23,220 --> 00:41:26,010
and recording those is read DPUs.

1033
00:41:26,010 --> 00:41:28,140
And all of these are activity based,

1034
00:41:28,140 --> 00:41:31,330
which means that as your
application scales up,

1035
00:41:31,330 --> 00:41:35,010
you're simply gonna
spending more on compute,

1036
00:41:35,010 --> 00:41:36,990
writes and reads, and this is
how you should think about it,

1037
00:41:36,990 --> 00:41:38,340
you shouldn't be thinking about

1038
00:41:38,340 --> 00:41:39,630
how big should my machine be?

1039
00:41:39,630 --> 00:41:41,160
You should simply be thinking about

1040
00:41:41,160 --> 00:41:43,263
how much am I spending on DSQL?

1041
00:41:44,190 --> 00:41:46,520
Of course, when you put
data into the system,

1042
00:41:46,520 --> 00:41:49,290
we have to store it, right?

1043
00:41:49,290 --> 00:41:50,580
We have to keep it there

1044
00:41:50,580 --> 00:41:53,820
and ready to go when you
connect in and run a query.

1045
00:41:53,820 --> 00:41:56,010
And this is just gonna be
measured in gigabyte hours,

1046
00:41:56,010 --> 00:41:58,863
just like you would expect
from S3 or DynamoDB.

1047
00:42:00,930 --> 00:42:03,750
Now, DSQL can obviously
scale for you, right?

1048
00:42:03,750 --> 00:42:06,990
We may be adding multiple
replicas behind the scenes.

1049
00:42:06,990 --> 00:42:08,430
If you're running a very high throughput,

1050
00:42:08,430 --> 00:42:11,250
you may have 10, 15, or 100 replicas.

1051
00:42:11,250 --> 00:42:12,660
With DSQL, you're only paying

1052
00:42:12,660 --> 00:42:14,970
for a single copy of your data, right?

1053
00:42:14,970 --> 00:42:17,130
So don't think about
the number of replicas,

1054
00:42:17,130 --> 00:42:19,740
don't think about those continuous
backups that we're doing.

1055
00:42:19,740 --> 00:42:22,140
Just think about what
is the number of bytes

1056
00:42:22,140 --> 00:42:23,640
that I've put into the system.

1057
00:42:24,840 --> 00:42:27,300
And because of everything
we've covered in this talk,

1058
00:42:27,300 --> 00:42:29,370
if your application
goes to sleep at night,

1059
00:42:29,370 --> 00:42:32,130
because you don't have any
customers using your application,

1060
00:42:32,130 --> 00:42:33,840
you can simply close connections.

1061
00:42:33,840 --> 00:42:35,220
You don't even have to close connections.

1062
00:42:35,220 --> 00:42:37,230
You can leave them running if you want.

1063
00:42:37,230 --> 00:42:38,610
The important piece is that

1064
00:42:38,610 --> 00:42:40,860
you're not sending us any SQL, right.

1065
00:42:40,860 --> 00:42:42,060
If you're not sending us any SQL,

1066
00:42:42,060 --> 00:42:44,220
you're not spending any time on the QPs,

1067
00:42:44,220 --> 00:42:46,380
you're not spending any
of their compute DPU,

1068
00:42:46,380 --> 00:42:48,480
you're not doing any reads,
you're not doing any writes.

1069
00:42:48,480 --> 00:42:51,480
And so the only ongoing cost for DSQL

1070
00:42:51,480 --> 00:42:54,303
in this kind of scenario
is that storage amount.

1071
00:42:57,570 --> 00:43:00,540
When morning comes and you
want to get going again,

1072
00:43:00,540 --> 00:43:02,223
just open new connections,

1073
00:43:03,150 --> 00:43:05,100
everything's there, it's ready to go.

1074
00:43:05,100 --> 00:43:07,140
There's really no difference in DSQL

1075
00:43:07,140 --> 00:43:09,960
between going from 99
connections to a hundred,

1076
00:43:09,960 --> 00:43:11,760
compared to going from zero connection

1077
00:43:11,760 --> 00:43:13,530
to that first connection, right?

1078
00:43:13,530 --> 00:43:16,500
The way we think about this
is you're just grabbing a QP.

1079
00:43:16,500 --> 00:43:18,600
QPS are always there. They're ready to go.

1080
00:43:19,680 --> 00:43:22,623
And so this is gonna
really simplify your lives.

1081
00:43:25,290 --> 00:43:28,200
Okay, so wrapping up.

1082
00:43:28,200 --> 00:43:31,200
Connections in DSQL.
Connections are secure.

1083
00:43:31,200 --> 00:43:32,580
In today's talk, we spoke about

1084
00:43:32,580 --> 00:43:36,761
how every connection in DSQL
is running in its own microVM,

1085
00:43:36,761 --> 00:43:40,260
using best in class hardware
level virtualization

1086
00:43:40,260 --> 00:43:43,890
and end-to-end encryption
between your application in DSQL.

1087
00:43:43,890 --> 00:43:46,680
Even members of the DSQL service team

1088
00:43:46,680 --> 00:43:49,650
cannot run something like
TCP dump and see your data.

1089
00:43:49,650 --> 00:43:51,200
There is end-to-end encryption.

1090
00:43:52,140 --> 00:43:54,510
It's secure out of the box.

1091
00:43:54,510 --> 00:43:56,580
We actually encourage
you to just run against

1092
00:43:56,580 --> 00:43:58,016
our public endpoint.

1093
00:43:58,016 --> 00:44:00,300
Take advantage of those tokens

1094
00:44:00,300 --> 00:44:02,670
because you don't have to
worry about credential leaks,

1095
00:44:02,670 --> 00:44:04,140
keep them short-lived.

1096
00:44:04,140 --> 00:44:05,190
But if you want,

1097
00:44:05,190 --> 00:44:06,540
you can set a private link

1098
00:44:06,540 --> 00:44:08,400
and configure resource-based policies

1099
00:44:08,400 --> 00:44:10,713
to lock down those
connections to your VPC.

1100
00:44:11,760 --> 00:44:14,250
Connections in DSQL are scalable.

1101
00:44:14,250 --> 00:44:16,050
It's the job of the
service team to make sure

1102
00:44:16,050 --> 00:44:18,480
that there's enough capacity
out there for every customer,

1103
00:44:18,480 --> 00:44:20,250
even if they're all spike.

1104
00:44:20,250 --> 00:44:22,980
You can use as many connections
as you need or as few,

1105
00:44:22,980 --> 00:44:23,910
and you're only gonna pay

1106
00:44:23,910 --> 00:44:26,250
for what you're doing
on those connections.

1107
00:44:26,250 --> 00:44:27,750
So if you have a hundred connections

1108
00:44:27,750 --> 00:44:29,430
and you're only using one connection,

1109
00:44:29,430 --> 00:44:32,380
you're only paying for the
activity on that one connection.

1110
00:44:33,750 --> 00:44:35,190
Connections in DSQL are designed

1111
00:44:35,190 --> 00:44:36,330
to be as simple as possible.

1112
00:44:36,330 --> 00:44:37,170
There's no patching,

1113
00:44:37,170 --> 00:44:39,360
there are no maintenance windows in DSQL.

1114
00:44:39,360 --> 00:44:42,450
There's no need to run
PgBounce or RDS proxy.

1115
00:44:42,450 --> 00:44:44,730
And there's no single point of
success in the system, right?

1116
00:44:44,730 --> 00:44:47,616
This is a system that has
no single point of failure.

1117
00:44:47,616 --> 00:44:49,380
It's strongly consistent.

1118
00:44:49,380 --> 00:44:51,660
And this is really gonna simplify your job

1119
00:44:51,660 --> 00:44:53,073
as an application developer.

1120
00:44:53,910 --> 00:44:56,670
And then connections are
snappy, they're fast.

1121
00:44:56,670 --> 00:44:58,290
Think about everything
we spoke about today.

1122
00:44:58,290 --> 00:44:59,610
We have AZ local routing.

1123
00:44:59,610 --> 00:45:03,270
We are going to a very fast
service written in Rust.

1124
00:45:03,270 --> 00:45:05,070
We're doing placement out of memory.

1125
00:45:06,000 --> 00:45:09,183
And we are grabbing a
connection out of a warm pool.

1126
00:45:11,400 --> 00:45:14,370
Once you've got that connection
open, connections are fast.

1127
00:45:14,370 --> 00:45:16,890
You're basically speaking
directly into that QP

1128
00:45:16,890 --> 00:45:19,650
that's ideally running
in the same AZ as you.

1129
00:45:19,650 --> 00:45:21,780
And the service has been designed

1130
00:45:21,780 --> 00:45:24,030
to handle mass reconnect storms.

1131
00:45:24,030 --> 00:45:25,650
So if all of your
connections die at there,

1132
00:45:25,650 --> 00:45:27,180
some kind of networking event,

1133
00:45:27,180 --> 00:45:30,540
we have extremely generous
default threatening rules,

1134
00:45:30,540 --> 00:45:32,100
this will allow you to get back online

1135
00:45:32,100 --> 00:45:33,303
as quickly as possible.

1136
00:45:34,740 --> 00:45:36,480
And with that said, that's
the end of the talk.

1137
00:45:36,480 --> 00:45:38,220
Thank you so much for attending.

1138
00:45:38,220 --> 00:45:39,053
My name is Marc,

1139
00:45:39,053 --> 00:45:40,680
I'd love to chat to you after the talk

1140
00:45:40,680 --> 00:45:42,090
if you have any questions.

1141
00:45:42,090 --> 00:45:44,040
Thank you for attending.
Have a great re:Invent.

1142
00:45:44,040 --> 00:45:46,893
And please do complete the
session survey in the mobile.

