1
00:00:04,770 --> 00:00:05,940
- Hello everyone.

2
00:00:05,940 --> 00:00:08,340
Welcome, good afternoon.

3
00:00:08,340 --> 00:00:11,700
Hope everybody was able
to have like a good lunch

4
00:00:11,700 --> 00:00:12,810
and be able to enjoy.

5
00:00:12,810 --> 00:00:14,430
Hope I don't make you too

6
00:00:14,430 --> 00:00:16,503
sleepy in this session here, okay.

7
00:00:18,450 --> 00:00:20,730
One of the interesting
things just to break the ice

8
00:00:20,730 --> 00:00:23,400
and get everybody like a
little bit more engaged,

9
00:00:23,400 --> 00:00:24,960
who's here is the first time

10
00:00:24,960 --> 00:00:28,560
or like maybe less than three times in the

11
00:00:28,560 --> 00:00:30,363
re:Invent or first time?

12
00:00:31,650 --> 00:00:33,810
Wow, quite a bit of people here.

13
00:00:33,810 --> 00:00:34,920
That is amazing.

14
00:00:34,920 --> 00:00:37,923
Who is maybe more than
10 times in re:Invent?

15
00:00:40,170 --> 00:00:42,420
Nobody, really?

16
00:00:42,420 --> 00:00:44,220
Why I'm asking this, is because actually

17
00:00:44,220 --> 00:00:46,170
a customer asked me yesterday

18
00:00:46,170 --> 00:00:48,390
and I was starting reflecting

19
00:00:48,390 --> 00:00:52,680
and this is my 12th time
in re:Invent, right?

20
00:00:52,680 --> 00:00:56,097
I remember when Trend
came from the first time,

21
00:00:56,097 --> 00:00:59,220
and the re:Invent was the
first cybersecurity vendor.

22
00:00:59,220 --> 00:01:00,960
Being participating in the re:Invent

23
00:01:00,960 --> 00:01:03,270
and being like, seeing the evolution

24
00:01:03,270 --> 00:01:06,090
how much like effort and like everything

25
00:01:06,090 --> 00:01:07,410
that is being built in the re:Invent

26
00:01:07,410 --> 00:01:09,150
is super like impressive.

27
00:01:09,150 --> 00:01:12,000
Thank you for helping me to understand

28
00:01:12,000 --> 00:01:13,500
if it's your first time or not,

29
00:01:13,500 --> 00:01:18,500
but today we are going to
dive into the reality of like

30
00:01:18,930 --> 00:01:22,410
how like AI, it is
becoming one of the most

31
00:01:22,410 --> 00:01:25,350
innovative environments
and most innovative

32
00:01:25,350 --> 00:01:27,930
like driving for organizations,

33
00:01:27,930 --> 00:01:31,230
but at the same time
how AI is transforming

34
00:01:31,230 --> 00:01:34,320
and becoming the new
battleground for attackers.

35
00:01:34,320 --> 00:01:36,510
Like how they are taking an advantage

36
00:01:36,510 --> 00:01:38,730
on this new era, right?

37
00:01:38,730 --> 00:01:40,140
As we are evolving,

38
00:01:40,140 --> 00:01:43,200
creating more AI
infrastructure applications,

39
00:01:43,200 --> 00:01:45,870
utilizing the cloud native services,

40
00:01:45,870 --> 00:01:49,860
how is your new strategy for
protecting those environments?

41
00:01:49,860 --> 00:01:51,330
That is one of the most important

42
00:01:51,330 --> 00:01:53,910
things that we wanted to discuss today.

43
00:01:53,910 --> 00:01:55,260
And when we go here,

44
00:01:55,260 --> 00:01:57,630
starting thinking about like

45
00:01:57,630 --> 00:01:59,550
this is your mission briefing

46
00:01:59,550 --> 00:02:01,620
for a modern AI security

47
00:02:01,620 --> 00:02:05,940
and how you can start strategizing

48
00:02:05,940 --> 00:02:08,070
AI in a different way.

49
00:02:08,070 --> 00:02:10,320
A lot of people sometimes they think no,

50
00:02:10,320 --> 00:02:12,840
AI is just like adding the (indistinct)

51
00:02:12,840 --> 00:02:16,830
or just like using the model
safety in the AI application

52
00:02:16,830 --> 00:02:18,570
and it's actually not true.

53
00:02:18,570 --> 00:02:20,070
There's multiple layers that you can

54
00:02:20,070 --> 00:02:21,630
think about protecting when you are

55
00:02:21,630 --> 00:02:23,373
building those AI applications.

56
00:02:24,990 --> 00:02:28,470
When you think about like
this new battlefield, right,

57
00:02:28,470 --> 00:02:30,150
AI is accelerating innovation

58
00:02:30,150 --> 00:02:31,620
but at the same time it's

59
00:02:31,620 --> 00:02:34,530
expanding the attack surface, right?

60
00:02:34,530 --> 00:02:37,800
More like environments are being using AI,

61
00:02:37,800 --> 00:02:39,660
at the same time you are expanding

62
00:02:39,660 --> 00:02:41,790
how attackers can take advantage

63
00:02:41,790 --> 00:02:44,610
or getting in, in your infrastructure.

64
00:02:44,610 --> 00:02:48,030
Every new AI like a workload
that you're building,

65
00:02:48,030 --> 00:02:50,430
becomes a new vector for attackers

66
00:02:50,430 --> 00:02:54,090
if you don't take the
protection layer needed, right?

67
00:02:54,090 --> 00:02:57,030
Modern security, it is
about embracing innovation

68
00:02:57,030 --> 00:03:01,320
while like you are staying
ahead of like this adversaries

69
00:03:01,320 --> 00:03:05,370
and who is like basically trying
to find different approach

70
00:03:05,370 --> 00:03:07,870
and different ways to get
in, in your environment.

71
00:03:09,030 --> 00:03:12,660
The new battleground, it is
not just about infrastructure.

72
00:03:12,660 --> 00:03:14,130
For many, many years we think

73
00:03:14,130 --> 00:03:15,840
about the cloud infrastructure,

74
00:03:15,840 --> 00:03:19,590
but now it's about the data, the models,

75
00:03:19,590 --> 00:03:23,040
the AI pipelines, all that
combination right now.

76
00:03:23,040 --> 00:03:25,290
It is really important for the life cycle

77
00:03:25,290 --> 00:03:27,630
when you are building the AI applications.

78
00:03:27,630 --> 00:03:30,300
And if you don't think
in a new approach of like

79
00:03:30,300 --> 00:03:33,010
how you're protecting, any new like

80
00:03:34,680 --> 00:03:36,495
attack vector that you create

81
00:03:36,495 --> 00:03:40,650
in one of those points here
are very high valuable.

82
00:03:40,650 --> 00:03:41,640
Why is that?

83
00:03:41,640 --> 00:03:45,000
Because once you affect
one of the infrastructure,

84
00:03:45,000 --> 00:03:48,540
it can affect the entire
ecosystem of your AI application.

85
00:03:48,540 --> 00:03:51,420
Okay, and we are going to
like, in the deeper details

86
00:03:51,420 --> 00:03:53,850
of like how attackers can be manipulating

87
00:03:53,850 --> 00:03:56,493
those AI applications and
taking advantage of that.

88
00:03:57,510 --> 00:04:00,270
When you think about like how adversaries

89
00:04:00,270 --> 00:04:02,430
are exploiting AI applications,

90
00:04:02,430 --> 00:04:05,670
you can think like as a simple flywheel.

91
00:04:05,670 --> 00:04:10,140
The data, the models, AI
pipelines, and run time.

92
00:04:10,140 --> 00:04:14,460
Attackers treat your AI
as their weapon, right?

93
00:04:14,460 --> 00:04:16,680
Poisoning data, stealing models,

94
00:04:16,680 --> 00:04:20,580
and invading run time
or manipulating inputs.

95
00:04:20,580 --> 00:04:25,170
AI is super powerful but that
power also can be hijacked

96
00:04:25,170 --> 00:04:29,280
and be utilized for like bad
actors for stealing your data,

97
00:04:29,280 --> 00:04:31,890
doing data breach of your information,

98
00:04:31,890 --> 00:04:33,960
stealing your intellectual property

99
00:04:33,960 --> 00:04:35,610
that your organization have been spending

100
00:04:35,610 --> 00:04:38,253
so much time and so
much money investing on.

101
00:04:39,390 --> 00:04:42,300
The important thing here
too in this flywheel,

102
00:04:42,300 --> 00:04:43,860
it is your user.

103
00:04:43,860 --> 00:04:45,360
And when I talk about your user,

104
00:04:45,360 --> 00:04:47,280
there's two major points.

105
00:04:47,280 --> 00:04:49,350
User, it is the user that you are

106
00:04:49,350 --> 00:04:51,060
having from your enterprise,

107
00:04:51,060 --> 00:04:54,090
your employees utilizing AI.

108
00:04:54,090 --> 00:04:56,100
Like everybody here, right,

109
00:04:56,100 --> 00:04:59,250
Is using in a way generative AI

110
00:04:59,250 --> 00:05:02,703
for like Chat GPT, Gemini,

111
00:05:04,170 --> 00:05:07,080
some kind of like Bedrock applications

112
00:05:07,080 --> 00:05:09,150
that you created for your customers.

113
00:05:09,150 --> 00:05:11,310
Like there is some level
of like generative AI

114
00:05:11,310 --> 00:05:13,320
being used in the applications.

115
00:05:13,320 --> 00:05:17,220
How you are monitoring any
risks related with those users,

116
00:05:17,220 --> 00:05:18,690
that is important.

117
00:05:18,690 --> 00:05:20,400
The second part of the users,

118
00:05:20,400 --> 00:05:23,820
it is when you build those AI applications

119
00:05:23,820 --> 00:05:27,390
there is like multiple users
that you don't have control.

120
00:05:27,390 --> 00:05:29,580
You don't know if they
are malicious or not.

121
00:05:29,580 --> 00:05:31,740
How you are making sure
every single prompt

122
00:05:31,740 --> 00:05:32,880
that they're interacting with

123
00:05:32,880 --> 00:05:35,640
your AI application are safety.

124
00:05:35,640 --> 00:05:39,330
And how you also, sometimes
these specific prompts

125
00:05:39,330 --> 00:05:41,310
are not like direct attacks.

126
00:05:41,310 --> 00:05:43,650
They're not a prompt
injection that we call.

127
00:05:43,650 --> 00:05:45,990
They're indirect attacks.

128
00:05:45,990 --> 00:05:47,490
What that means, it is, okay,

129
00:05:47,490 --> 00:05:49,590
when you request your AI application

130
00:05:49,590 --> 00:05:53,700
that maybe will search in a
different like search data

131
00:05:53,700 --> 00:05:56,850
that will have like a cross site scripting

132
00:05:56,850 --> 00:05:58,800
that was created by attacker

133
00:05:58,800 --> 00:06:01,160
and when processed by your AI application

134
00:06:01,160 --> 00:06:03,630
it will create a data breach.

135
00:06:03,630 --> 00:06:04,980
People don't think about that

136
00:06:04,980 --> 00:06:07,320
but the input and the output

137
00:06:07,320 --> 00:06:09,450
needs to be validated always

138
00:06:09,450 --> 00:06:12,390
as a additional layer protection, okay?

139
00:06:12,390 --> 00:06:15,720
That's why users critical in this main

140
00:06:15,720 --> 00:06:18,303
like flywheel here when
you were talking about it.

141
00:06:19,620 --> 00:06:21,450
Data poisoning, right?

142
00:06:21,450 --> 00:06:23,310
A lot of people talk about data poisoning

143
00:06:23,310 --> 00:06:26,010
and it's one of the most damaging attacks.

144
00:06:26,010 --> 00:06:28,740
By injecting bad data into training sets

145
00:06:28,740 --> 00:06:30,300
or fine tuning process,

146
00:06:30,300 --> 00:06:33,780
this can manipulate how
your models are going to act

147
00:06:33,780 --> 00:06:35,940
or basically creating the response

148
00:06:35,940 --> 00:06:39,090
for your users, like
employees as I mentioned,

149
00:06:39,090 --> 00:06:42,450
and your user as a
customers as one example.

150
00:06:42,450 --> 00:06:44,070
Making sure every single data

151
00:06:44,070 --> 00:06:46,470
that you are using in the data sets

152
00:06:46,470 --> 00:06:49,260
are being like safe and they don't have

153
00:06:49,260 --> 00:06:51,480
like any kind of like malicious content.

154
00:06:51,480 --> 00:06:53,280
It is critical for every single

155
00:06:53,280 --> 00:06:55,380
AI models that you're training

156
00:06:55,380 --> 00:06:56,880
but also the database vector

157
00:06:56,880 --> 00:06:58,473
that you are creating, okay?

158
00:06:59,430 --> 00:07:00,690
Model theft.

159
00:07:00,690 --> 00:07:03,420
This is important because
a lot of people forget

160
00:07:03,420 --> 00:07:07,560
that like models can be
actually stolen in some cases

161
00:07:07,560 --> 00:07:10,950
and redeployed in infrastructure.

162
00:07:10,950 --> 00:07:13,500
One interesting thing
about like this session,

163
00:07:13,500 --> 00:07:17,850
it is the name of this
session was based actually

164
00:07:17,850 --> 00:07:21,120
in a thread research
that was made by one of

165
00:07:21,120 --> 00:07:22,950
the Trend Micro Thread Research,

166
00:07:22,950 --> 00:07:26,373
Alfredo Oliveira and David Fisher.

167
00:07:27,450 --> 00:07:28,680
They find out like,

168
00:07:28,680 --> 00:07:32,070
that like specifically models can be

169
00:07:32,070 --> 00:07:34,710
utilized as a way for you to export

170
00:07:34,710 --> 00:07:36,540
the model from containers.

171
00:07:36,540 --> 00:07:40,020
Manipulated the way how
they were being trained

172
00:07:40,020 --> 00:07:42,360
and redeployed again to the container,

173
00:07:42,360 --> 00:07:44,910
and the application,
it is actually changing

174
00:07:44,910 --> 00:07:47,400
the outcome of the responses, right?

175
00:07:47,400 --> 00:07:49,650
That's why when you
think about model theft,

176
00:07:49,650 --> 00:07:51,600
it is not just like about manipulating,

177
00:07:51,600 --> 00:07:53,040
but also attackers right now,

178
00:07:53,040 --> 00:07:54,780
they're trying to get your models

179
00:07:54,780 --> 00:07:58,020
to resold in the actual bad market, right?

180
00:07:58,020 --> 00:07:59,700
It's intellectual property.

181
00:07:59,700 --> 00:08:01,560
A lot of customers here,

182
00:08:01,560 --> 00:08:04,290
they not just use the regular data

183
00:08:04,290 --> 00:08:07,233
but also sensitive data as
part of the training, okay?

184
00:08:08,070 --> 00:08:09,690
They're kind of like what we call

185
00:08:09,690 --> 00:08:12,870
the new crown jewels for
organizations, right?

186
00:08:12,870 --> 00:08:16,800
AI pipelines, there's multiple
points in the AI pipelines.

187
00:08:16,800 --> 00:08:19,620
There is like the poisoning that can be

188
00:08:19,620 --> 00:08:22,620
in the model build when you are adding

189
00:08:22,620 --> 00:08:25,260
or you when you are
creating that pipeline,

190
00:08:25,260 --> 00:08:28,170
there is the inject back doors

191
00:08:28,170 --> 00:08:30,720
that can happen through the supply chains

192
00:08:30,720 --> 00:08:32,220
from like open source libraries

193
00:08:32,220 --> 00:08:34,020
that you maybe you'll be using,

194
00:08:34,020 --> 00:08:37,200
or maybe for like affecting
developer machines

195
00:08:37,200 --> 00:08:38,910
that are interacting and adding

196
00:08:38,910 --> 00:08:41,130
back doors to your applications.

197
00:08:41,130 --> 00:08:43,440
There is a way to manipulate

198
00:08:43,440 --> 00:08:46,590
and alter the outputs
from those applications.

199
00:08:46,590 --> 00:08:48,390
Prompting injection for sure

200
00:08:48,390 --> 00:08:50,490
and inference attacks are the most

201
00:08:50,490 --> 00:08:52,860
common for like exploitation.

202
00:08:52,860 --> 00:08:55,740
And shadow AIs are becoming
one of the biggest problems.

203
00:08:55,740 --> 00:08:58,500
And when I say about shadow AI, it is,

204
00:08:58,500 --> 00:09:00,210
do you have a full visibility

205
00:09:00,210 --> 00:09:03,363
of like all the AIs that your
organizations are using today?

206
00:09:04,410 --> 00:09:06,540
A lot of companies they don't, right?

207
00:09:06,540 --> 00:09:09,690
And it's complex because
every single person,

208
00:09:09,690 --> 00:09:10,770
every single developer,

209
00:09:10,770 --> 00:09:12,690
every single cloud architect,

210
00:09:12,690 --> 00:09:15,000
in some ways they are
using different tools.

211
00:09:15,000 --> 00:09:16,890
And having that full visibility

212
00:09:16,890 --> 00:09:20,460
for like everything that
is like AI technologies,

213
00:09:20,460 --> 00:09:23,580
it is important for you
to minimize that shadow AI

214
00:09:23,580 --> 00:09:27,063
problems that you may have
in your organization, okay?

215
00:09:28,830 --> 00:09:29,940
I don't know if you saw

216
00:09:29,940 --> 00:09:31,500
or you're monitoring a lot of Reddit.

217
00:09:31,500 --> 00:09:32,973
Who reads Reddit here?

218
00:09:34,740 --> 00:09:36,120
Okay, half of the room.

219
00:09:36,120 --> 00:09:37,650
That's good.

220
00:09:37,650 --> 00:09:39,810
I was reading this one and catch my eye

221
00:09:39,810 --> 00:09:41,040
and said like I need to use this

222
00:09:41,040 --> 00:09:43,260
in my slides this week, right?

223
00:09:43,260 --> 00:09:44,730
This was like an organization

224
00:09:44,730 --> 00:09:47,220
that was sharing their challenge

225
00:09:47,220 --> 00:09:50,700
about like why they fail an AI project.

226
00:09:50,700 --> 00:09:53,700
And I love and highlighted
this specific spot here

227
00:09:53,700 --> 00:09:54,667
where they're saying,

228
00:09:54,667 --> 00:09:58,320
"Our mistake was relying
on model safety alone

229
00:09:58,320 --> 00:10:02,220
and not like in runtime guardrails."

230
00:10:02,220 --> 00:10:03,750
Why highlight about this?

231
00:10:03,750 --> 00:10:06,180
Because model safety,

232
00:10:06,180 --> 00:10:09,840
it was created for one specific outcome,

233
00:10:09,840 --> 00:10:11,040
for every single model.

234
00:10:11,040 --> 00:10:15,330
And every single model has your
own way to do model safety.

235
00:10:15,330 --> 00:10:17,130
But every single organization

236
00:10:17,130 --> 00:10:18,570
they have your own guardrails

237
00:10:18,570 --> 00:10:21,600
that they should be
created on top of that.

238
00:10:21,600 --> 00:10:23,400
You are not just going to be trusting

239
00:10:23,400 --> 00:10:25,500
in the model safety from like

240
00:10:25,500 --> 00:10:27,750
every single model that there is outside,

241
00:10:27,750 --> 00:10:30,870
because you have no
validation in a lot of cases.

242
00:10:30,870 --> 00:10:33,030
There is some, specifically companies

243
00:10:33,030 --> 00:10:35,040
that do the testings and all of that,

244
00:10:35,040 --> 00:10:37,380
but having your own
guardrails that you can,

245
00:10:37,380 --> 00:10:39,810
making sure which kind
of protection you have

246
00:10:39,810 --> 00:10:41,940
for every single AI agent,

247
00:10:41,940 --> 00:10:44,850
agentic architecture in AI applications,

248
00:10:44,850 --> 00:10:47,520
it is extremely important, okay?

249
00:10:47,520 --> 00:10:51,090
Please, making sure you have that.

250
00:10:51,090 --> 00:10:53,250
Don't get the same mistake.

251
00:10:53,250 --> 00:10:54,600
Why did I bring about this?

252
00:10:54,600 --> 00:10:56,880
Because I think sharing, it is important.

253
00:10:56,880 --> 00:10:59,820
Bringing that as a highlight
for people to remember

254
00:10:59,820 --> 00:11:00,840
when they are building that,

255
00:11:00,840 --> 00:11:03,573
it is extremely important for all of us.

256
00:11:06,300 --> 00:11:09,120
Traditional defenses are very reactive.

257
00:11:09,120 --> 00:11:10,260
We know of this, right?

258
00:11:10,260 --> 00:11:12,180
For many, many years we have been using

259
00:11:12,180 --> 00:11:14,460
specific technologies
that are being the same

260
00:11:14,460 --> 00:11:16,950
for like protecting workloads,

261
00:11:16,950 --> 00:11:18,690
protecting serverless,

262
00:11:18,690 --> 00:11:21,840
or protecting like
containers in some cases.

263
00:11:21,840 --> 00:11:24,240
AI threats evolve too quickly.

264
00:11:24,240 --> 00:11:26,670
Like what was AI six months ago,

265
00:11:26,670 --> 00:11:29,490
it is completely different than right now.

266
00:11:29,490 --> 00:11:32,370
Model drifts, pipeline updates constantly,

267
00:11:32,370 --> 00:11:34,380
agents making a huge amount

268
00:11:34,380 --> 00:11:36,240
of like automation decisions

269
00:11:36,240 --> 00:11:38,460
more and more with agentic approach.

270
00:11:38,460 --> 00:11:40,350
It is extremely important for us

271
00:11:40,350 --> 00:11:42,330
to starting rethinking not in like

272
00:11:42,330 --> 00:11:44,850
a static approach of protection

273
00:11:44,850 --> 00:11:47,820
but in a more proactive approach of like

274
00:11:47,820 --> 00:11:51,930
how we are protecting our
own agentic infrastructure

275
00:11:51,930 --> 00:11:54,540
in AI applications, okay?

276
00:11:54,540 --> 00:11:57,690
You need to be adaptive,
you need to be proactive,

277
00:11:57,690 --> 00:12:00,810
and you need to have some
kind of like AI awareness

278
00:12:00,810 --> 00:12:04,320
that the AI can learn through the process

279
00:12:04,320 --> 00:12:07,920
and understand your risk prioritization

280
00:12:07,920 --> 00:12:10,500
for you to mitigate it soon as possible

281
00:12:10,500 --> 00:12:12,093
before going into production.

282
00:12:14,400 --> 00:12:17,490
I created like a little
lineup of like this session

283
00:12:17,490 --> 00:12:19,650
with like the different layers

284
00:12:19,650 --> 00:12:22,380
that we should be reflecting about, right?

285
00:12:22,380 --> 00:12:25,920
And let's begin at the
foundation here, data.

286
00:12:25,920 --> 00:12:29,790
Most AI risks originated
actually in the data, right?

287
00:12:29,790 --> 00:12:34,020
AI systems often process
massive amounts of data

288
00:12:34,020 --> 00:12:37,650
and any blind spots how
to data can be stored,

289
00:12:37,650 --> 00:12:41,430
shared, and classify becomes
a direct attack vector

290
00:12:41,430 --> 00:12:43,443
for like specific attacks.

291
00:12:44,550 --> 00:12:48,450
This visibility here, it
is a portion of your data

292
00:12:48,450 --> 00:12:51,300
that is considered classified data, right?

293
00:12:51,300 --> 00:12:54,135
You know what is that data,

294
00:12:54,135 --> 00:12:56,640
you know the information usually,

295
00:12:56,640 --> 00:12:59,670
but that's like just a surface, right?

296
00:12:59,670 --> 00:13:02,340
Majority of the data that we actually have

297
00:13:02,340 --> 00:13:04,950
are under like that surface,

298
00:13:04,950 --> 00:13:07,950
or like unknown data are unclassified,

299
00:13:07,950 --> 00:13:10,110
are like unprotected data

300
00:13:10,110 --> 00:13:13,110
that a lot of times
people forget about that.

301
00:13:13,110 --> 00:13:14,970
They use as part of the training,

302
00:13:14,970 --> 00:13:17,340
they use as the database factory,

303
00:13:17,340 --> 00:13:19,680
they use as the defined creation

304
00:13:19,680 --> 00:13:21,390
for your AI applications,

305
00:13:21,390 --> 00:13:24,990
but they actually forgot
to check and validate it

306
00:13:24,990 --> 00:13:26,940
if there is any sensitive information

307
00:13:26,940 --> 00:13:29,400
before the actual utilization of that.

308
00:13:29,400 --> 00:13:33,510
And this is where things can
be a little bit tricky here.

309
00:13:33,510 --> 00:13:35,940
Like understanding that data

310
00:13:35,940 --> 00:13:39,030
and understanding if they're sensitive.

311
00:13:39,030 --> 00:13:42,540
And you can, for sure you can
use for your AI applications

312
00:13:42,540 --> 00:13:46,320
but now, okay, my AI
application is utilizing

313
00:13:46,320 --> 00:13:49,140
one specific set of like
sensitive information.

314
00:13:49,140 --> 00:13:51,540
The level of protection
on that application

315
00:13:51,540 --> 00:13:54,900
needs to be way higher because the risk

316
00:13:54,900 --> 00:13:57,540
of data exfiltration is extremely high

317
00:13:57,540 --> 00:14:00,363
and risky for the organization, okay?

318
00:14:01,500 --> 00:14:04,740
When you talk about data
leakage, it can happen anywhere.

319
00:14:04,740 --> 00:14:07,560
During training, inference logs,

320
00:14:07,560 --> 00:14:09,180
through APIs communication,

321
00:14:09,180 --> 00:14:11,100
even through models outputs, right?

322
00:14:11,100 --> 00:14:14,070
The interaction that you
have with the models.

323
00:14:14,070 --> 00:14:17,970
Attacks exploits this weak points

324
00:14:17,970 --> 00:14:20,340
to extract any kind of like sensitive

325
00:14:20,340 --> 00:14:22,200
information that they may find.

326
00:14:22,200 --> 00:14:25,080
PII data, information about passports,

327
00:14:25,080 --> 00:14:28,560
people, emails, account
information, and so on.

328
00:14:28,560 --> 00:14:30,270
Depending the level of like

329
00:14:30,270 --> 00:14:33,150
what is your organization
providing outside, right?

330
00:14:33,150 --> 00:14:34,650
And they can do some level of like

331
00:14:34,650 --> 00:14:35,840
reverse engineering depending

332
00:14:35,840 --> 00:14:37,490
on the information that they get.

333
00:14:38,760 --> 00:14:41,340
Our response here and the way how you

334
00:14:41,340 --> 00:14:42,720
should be thinking, it is like

335
00:14:42,720 --> 00:14:44,700
how can you create a data posture

336
00:14:44,700 --> 00:14:46,410
security management, right?

337
00:14:46,410 --> 00:14:50,190
How you can start in monitoring
every single data set

338
00:14:50,190 --> 00:14:51,930
before the actual training,

339
00:14:51,930 --> 00:14:54,960
before the actual utilization
by your AI applications

340
00:14:54,960 --> 00:14:57,600
and understanding which
ones are sensitive,

341
00:14:57,600 --> 00:14:59,610
which ones have like intellectual property

342
00:14:59,610 --> 00:15:00,840
from your organization,

343
00:15:00,840 --> 00:15:04,260
and which ones are like
actually no problem, right?

344
00:15:04,260 --> 00:15:06,720
It's a public information
that can be utilized

345
00:15:06,720 --> 00:15:09,480
and if you have any attack
on this application,

346
00:15:09,480 --> 00:15:11,400
even it's like compromising the

347
00:15:11,400 --> 00:15:12,960
image from your organization,

348
00:15:12,960 --> 00:15:15,090
they're not compromising the data

349
00:15:15,090 --> 00:15:16,200
that is the most important

350
00:15:16,200 --> 00:15:18,273
and the crowd use for a lot of companies.

351
00:15:19,875 --> 00:15:22,110
Okay, the next one.

352
00:15:22,110 --> 00:15:23,880
We move to the stack up and we

353
00:15:23,880 --> 00:15:27,750
talk about like AI microservices
like supply chains.

354
00:15:27,750 --> 00:15:30,690
AI components depend on like countless

355
00:15:30,690 --> 00:15:32,520
number of like libraries.

356
00:15:32,520 --> 00:15:35,160
Open source applications

357
00:15:35,160 --> 00:15:38,070
that is being utilized outside containers

358
00:15:38,070 --> 00:15:39,960
and external services, right?

359
00:15:39,960 --> 00:15:43,260
Each of like has an indirect way

360
00:15:43,260 --> 00:15:46,290
for infiltration by attackers, right?

361
00:15:46,290 --> 00:15:49,230
More and more we see supply chain attacks

362
00:15:49,230 --> 00:15:50,460
in the organizations.

363
00:15:50,460 --> 00:15:52,230
And that's why it's
really, really important

364
00:15:52,230 --> 00:15:53,940
for all of us to make sure,

365
00:15:53,940 --> 00:15:56,070
because a single vulnerability,

366
00:15:56,070 --> 00:15:59,430
a single dependency
shoe in your application

367
00:15:59,430 --> 00:16:01,470
can be a domino effect,

368
00:16:01,470 --> 00:16:04,140
can be starting affecting
the entire application

369
00:16:04,140 --> 00:16:07,020
and the way how you were
building the security strategy

370
00:16:07,020 --> 00:16:09,423
for that like environment or workload.

371
00:16:11,940 --> 00:16:15,480
When a new component is compromised,

372
00:16:15,480 --> 00:16:18,090
the entire life cycle will be affected.

373
00:16:18,090 --> 00:16:19,710
And not just like the runtime

374
00:16:19,710 --> 00:16:21,690
but also any new application

375
00:16:21,690 --> 00:16:24,210
that is being utilizing the foundation

376
00:16:24,210 --> 00:16:28,020
that's used for other applications, right?

377
00:16:28,020 --> 00:16:31,500
That's why it's important to
have a continuously monitoring

378
00:16:31,500 --> 00:16:34,830
for any vulnerability in the pipeline,

379
00:16:34,830 --> 00:16:36,120
in the supply chain,

380
00:16:36,120 --> 00:16:40,170
secrets that can be forgotten
inside the container image,

381
00:16:40,170 --> 00:16:43,323
mowers that can be embedded
by attackers, and so on.

382
00:16:44,160 --> 00:16:46,320
And when you talk about
like attack vectors, right,

383
00:16:46,320 --> 00:16:48,990
there's three major ones
that I actually got here.

384
00:16:48,990 --> 00:16:50,490
Dependency poisoning,

385
00:16:50,490 --> 00:16:52,860
this is probably one of the most common

386
00:16:52,860 --> 00:16:55,200
when you are using a lot of
the open source libraries

387
00:16:55,200 --> 00:16:57,300
and if you are not validating those.

388
00:16:57,300 --> 00:16:59,850
Unverified image, that is very,

389
00:16:59,850 --> 00:17:01,470
very important for all of us.

390
00:17:01,470 --> 00:17:04,050
I know we talk about
shift to lab security,

391
00:17:04,050 --> 00:17:06,330
we talk about container image scanning,

392
00:17:06,330 --> 00:17:09,630
it's still a lot of companies missing that

393
00:17:09,630 --> 00:17:12,810
basic or foundation layer protection.

394
00:17:12,810 --> 00:17:16,740
Very, very important for
all of you to make sure.

395
00:17:16,740 --> 00:17:18,090
And when we talk about like

396
00:17:18,090 --> 00:17:20,610
it's not just looking for vulnerabilities

397
00:17:20,610 --> 00:17:23,310
but also looking for secrets.

398
00:17:23,310 --> 00:17:25,530
Recently we have been
finding more and more

399
00:17:25,530 --> 00:17:28,650
container image that are public facing

400
00:17:28,650 --> 00:17:30,810
that in some ways, there is a way for you

401
00:17:30,810 --> 00:17:33,990
to access secrets inside
the container image

402
00:17:33,990 --> 00:17:38,023
that can compromise your
cloud infrastructure.

403
00:17:38,023 --> 00:17:39,600
This could be AWS credentials,

404
00:17:39,600 --> 00:17:41,490
it could be applications credentials

405
00:17:41,490 --> 00:17:44,250
that are like plain text,

406
00:17:44,250 --> 00:17:47,340
forgotten inside actual containers, right?

407
00:17:47,340 --> 00:17:50,520
Really important for you to
make sure you are validating

408
00:17:50,520 --> 00:17:53,700
and verifying and maybe
creating like a signature

409
00:17:53,700 --> 00:17:54,960
for every single image.

410
00:17:54,960 --> 00:17:56,370
This way if there is any kind of

411
00:17:56,370 --> 00:17:58,770
like a compromised supply chain,

412
00:17:58,770 --> 00:18:01,170
the image needs to be validated by your

413
00:18:01,170 --> 00:18:03,900
security controls that you have it.

414
00:18:03,900 --> 00:18:05,730
And pipeline compromise, right?

415
00:18:05,730 --> 00:18:08,760
Adversary is trying to affect the systems,

416
00:18:08,760 --> 00:18:10,920
not just like the regular from like

417
00:18:10,920 --> 00:18:13,230
a coder post storage to the CI/CD pipeline

418
00:18:13,230 --> 00:18:15,180
but from the developer machines.

419
00:18:15,180 --> 00:18:17,490
More attacks are being
affected the developers

420
00:18:17,490 --> 00:18:21,420
that can drive malicious code
to the actual supply chain

421
00:18:21,420 --> 00:18:22,893
that needs to be validated.

422
00:18:25,170 --> 00:18:26,220
When you talk about this,

423
00:18:26,220 --> 00:18:29,970
we are talking about like
CI/CD pipeline verification,

424
00:18:29,970 --> 00:18:33,210
we are talking about container
and code integrity checking

425
00:18:33,210 --> 00:18:38,010
for vulnerabilities, secrets,
mowers, misconfiguration,

426
00:18:38,010 --> 00:18:40,080
and all that information being part

427
00:18:40,080 --> 00:18:42,840
of like your application visibility.

428
00:18:42,840 --> 00:18:45,180
Whatever you deploy in production,

429
00:18:45,180 --> 00:18:48,450
how can you have a full
visibility from the shift to left

430
00:18:48,450 --> 00:18:50,463
to the run time, okay?

431
00:18:52,170 --> 00:18:53,730
Model and AI agents,

432
00:18:53,730 --> 00:18:56,940
and when like things can
go like rogue, right?

433
00:18:56,940 --> 00:19:00,150
Models and agents can be,

434
00:19:00,150 --> 00:19:02,520
can become like attack vector themselves.

435
00:19:02,520 --> 00:19:04,230
Once they are compromised,

436
00:19:04,230 --> 00:19:06,840
they can basically
interact with your systems

437
00:19:06,840 --> 00:19:09,180
and affect in the way how
they are being manipulated

438
00:19:09,180 --> 00:19:12,330
or getting information that
can be exfiltrated outside.

439
00:19:12,330 --> 00:19:14,580
And can be basically affecting data

440
00:19:14,580 --> 00:19:17,703
and be harmful for the
infrastructure that you build, right?

441
00:19:19,170 --> 00:19:20,550
Models can be poisoning,

442
00:19:20,550 --> 00:19:22,800
manipulated, or misled through

443
00:19:22,800 --> 00:19:26,340
subtle adversaries, right, inputs.

444
00:19:26,340 --> 00:19:29,340
AI agents which can act autonomously

445
00:19:29,340 --> 00:19:32,250
introduce the entire
new class of like risks

446
00:19:32,250 --> 00:19:34,740
that can affect the entire infrastructure

447
00:19:34,740 --> 00:19:37,800
when you are building
the agentic approach.

448
00:19:37,800 --> 00:19:39,900
When you're talking about
like agentic approach,

449
00:19:39,900 --> 00:19:42,540
if you have an AI agent that is going

450
00:19:42,540 --> 00:19:45,600
outside their like scope, right,

451
00:19:45,600 --> 00:19:48,510
this is actually interesting conversation

452
00:19:48,510 --> 00:19:50,850
because yesterday I was talking with like

453
00:19:50,850 --> 00:19:54,270
one of the biggest software
organizations in the globe.

454
00:19:54,270 --> 00:19:55,590
And one of the biggest challenges

455
00:19:55,590 --> 00:19:57,060
that they're facing right now,

456
00:19:57,060 --> 00:19:59,010
they are creating more and more agents

457
00:19:59,010 --> 00:20:01,470
as part of their agentic approach

458
00:20:01,470 --> 00:20:05,670
but they don't have a way to
control the scope of the agent.

459
00:20:05,670 --> 00:20:08,010
If the agent is starting acting in

460
00:20:08,010 --> 00:20:10,380
a different approach as they created,

461
00:20:10,380 --> 00:20:12,390
they don't have a way to remediate it.

462
00:20:12,390 --> 00:20:15,660
They don't have a way to mitigate and like

463
00:20:15,660 --> 00:20:17,790
contain that agent,

464
00:20:17,790 --> 00:20:19,740
should not affect other parts

465
00:20:19,740 --> 00:20:21,930
of the infrastructure, okay?

466
00:20:21,930 --> 00:20:23,460
That's really, really important

467
00:20:23,460 --> 00:20:24,750
and it's very associated with

468
00:20:24,750 --> 00:20:28,083
the unauthorized actions on those agents.

469
00:20:29,490 --> 00:20:32,670
Key attacks vectors includes
malicious inputs here,

470
00:20:32,670 --> 00:20:35,880
woke agent behavior, manipulate outputs.

471
00:20:35,880 --> 00:20:38,010
Attacks try to steer the models

472
00:20:38,010 --> 00:20:41,010
into unsafe actions, right?

473
00:20:41,010 --> 00:20:43,200
Or bypass the safety components.

474
00:20:43,200 --> 00:20:46,650
That's why I was talking
about like the model safety.

475
00:20:46,650 --> 00:20:49,890
Every attacker have access to your models

476
00:20:49,890 --> 00:20:51,360
that is like open source

477
00:20:51,360 --> 00:20:53,220
and they understand really, really well

478
00:20:53,220 --> 00:20:56,700
how the model safety works in each model.

479
00:20:56,700 --> 00:20:59,460
But the guard rails that you created,

480
00:20:59,460 --> 00:21:01,620
they are customized for you.

481
00:21:01,620 --> 00:21:03,090
You understand the challenge

482
00:21:03,090 --> 00:21:04,950
and the risks in your AI applications.

483
00:21:04,950 --> 00:21:08,730
That's why it's important for
you to edit on that, right?

484
00:21:08,730 --> 00:21:10,380
And when you talk about
like the guardrails,

485
00:21:10,380 --> 00:21:11,823
there is two layers here.

486
00:21:12,720 --> 00:21:15,960
Because model, it is very dedicated

487
00:21:15,960 --> 00:21:18,450
and mathematical way to decision.

488
00:21:18,450 --> 00:21:20,970
Like you can ask the same question

489
00:21:20,970 --> 00:21:22,830
100 times to the models,

490
00:21:22,830 --> 00:21:26,790
the answers and outputs in
some cases will be different.

491
00:21:26,790 --> 00:21:29,133
You all tested this already, right?

492
00:21:31,560 --> 00:21:33,810
The point here is

493
00:21:33,810 --> 00:21:36,510
when you are testing securing the models,

494
00:21:36,510 --> 00:21:38,520
you cannot test just once.

495
00:21:38,520 --> 00:21:42,030
You need to keep like
continuously testing the models

496
00:21:42,030 --> 00:21:45,540
to make sure maybe that
attack that didn't work today,

497
00:21:45,540 --> 00:21:47,550
maybe it'll work tomorrow

498
00:21:47,550 --> 00:21:48,900
because you changed something.

499
00:21:48,900 --> 00:21:51,270
You're retraining with
like a different dataset

500
00:21:51,270 --> 00:21:54,510
and the actual decision making
of the model are changing

501
00:21:54,510 --> 00:21:57,360
and give information that
they should not, right?

502
00:21:57,360 --> 00:21:59,970
That is why it's really very
important for you to make sure

503
00:21:59,970 --> 00:22:02,850
that interaction and
continuously validation

504
00:22:02,850 --> 00:22:05,670
when you are building those applications.

505
00:22:05,670 --> 00:22:07,860
And one of the interesting
things that we did this week,

506
00:22:07,860 --> 00:22:11,940
it is a launching of like a new
tag that we call AI Scanner.

507
00:22:11,940 --> 00:22:13,980
What that means, it is, okay,

508
00:22:13,980 --> 00:22:18,330
you can point it out, AI
application, the API endpoint,

509
00:22:18,330 --> 00:22:20,640
we are going to scan that AI application

510
00:22:20,640 --> 00:22:24,870
for sensitive data disclosure,
for prompt injections,

511
00:22:24,870 --> 00:22:26,610
system prompt leakage,

512
00:22:26,610 --> 00:22:28,590
malicious code generation,

513
00:22:28,590 --> 00:22:30,510
agent two definitions leakage,

514
00:22:30,510 --> 00:22:32,797
and many other things associated with the

515
00:22:32,797 --> 00:22:36,810
OWASP top 10 for LLMs and generative AI.

516
00:22:36,810 --> 00:22:39,270
Those informations and those risks

517
00:22:39,270 --> 00:22:42,450
are being utilized by our
systems in the backend,

518
00:22:42,450 --> 00:22:44,700
what we call LLM judge.

519
00:22:44,700 --> 00:22:48,510
Our LLM judge, it is embedded
as part of the guardrails.

520
00:22:48,510 --> 00:22:52,020
This way, whatever risks we detect there,

521
00:22:52,020 --> 00:22:54,660
it can be automatically protected.

522
00:22:54,660 --> 00:22:57,570
I like to say for people who knows Trend,

523
00:22:57,570 --> 00:22:59,670
this is like the new virtual patching

524
00:22:59,670 --> 00:23:01,650
for AI applications, right?

525
00:23:01,650 --> 00:23:03,630
We have a virtual patching for protective

526
00:23:03,630 --> 00:23:05,550
vulnerabilities for many, many years.

527
00:23:05,550 --> 00:23:06,960
This is a new approach of like

528
00:23:06,960 --> 00:23:09,750
how we are protecting the application,

529
00:23:09,750 --> 00:23:11,760
the AI applications in real time

530
00:23:11,760 --> 00:23:16,230
as we identify those
risks right away, okay?

531
00:23:16,230 --> 00:23:19,530
This basically extends of like how you are

532
00:23:19,530 --> 00:23:23,880
creating a prevention mechanism

533
00:23:23,880 --> 00:23:25,500
for every single OWASP Top 10

534
00:23:25,500 --> 00:23:29,373
being analyzed by your
scanner as one example.

535
00:23:31,020 --> 00:23:34,470
Infrastructure and Under Siege, right?

536
00:23:34,470 --> 00:23:37,080
AI infrastructure, it
is extremely important

537
00:23:37,080 --> 00:23:39,540
and attackers are trying
to take advantage.

538
00:23:39,540 --> 00:23:42,368
It could be because the
resources exhaustions,

539
00:23:42,368 --> 00:23:44,850
it can be by the utilization of the GPUs,

540
00:23:44,850 --> 00:23:47,490
it can be by the cloud
services infrastructure

541
00:23:47,490 --> 00:23:50,220
that they're trying to
take advantage for mining,

542
00:23:50,220 --> 00:23:53,553
or crypto mining for
their own specific gains.

543
00:23:55,620 --> 00:23:56,790
This visual here,

544
00:23:56,790 --> 00:23:59,790
it's a frontline infrastructure, right?

545
00:23:59,790 --> 00:24:02,100
When you think about like
how you're protecting

546
00:24:02,100 --> 00:24:04,497
like your castle of like you when you

547
00:24:04,497 --> 00:24:06,780
are building AI applications, right?

548
00:24:06,780 --> 00:24:09,720
Compute, privilege, access,

549
00:24:09,720 --> 00:24:12,600
access coverage from the
different like AI applications

550
00:24:12,600 --> 00:24:14,880
or containers or microservices.

551
00:24:14,880 --> 00:24:17,970
All that highlights a huge impact

552
00:24:17,970 --> 00:24:20,883
that can affect the AI applications.

553
00:24:21,900 --> 00:24:23,460
But when you think about this,

554
00:24:23,460 --> 00:24:25,410
usually a lot of customers are

555
00:24:25,410 --> 00:24:27,660
concerned about like how attackers

556
00:24:27,660 --> 00:24:29,970
are not being compromised
in my infrastructure

557
00:24:29,970 --> 00:24:32,100
for resources exhaustion,

558
00:24:32,100 --> 00:24:35,730
for increasing the cost
of the GPUs utilization

559
00:24:35,730 --> 00:24:38,430
that happens in some cases
when they have access

560
00:24:38,430 --> 00:24:41,430
to your infrastructure, and
also data leakage, right?

561
00:24:41,430 --> 00:24:43,770
Those are the three most critical things

562
00:24:43,770 --> 00:24:46,440
that I usually hear from customers.

563
00:24:46,440 --> 00:24:48,780
When infrastructure controls are weak,

564
00:24:48,780 --> 00:24:52,200
attackers can hijack
workloads and drain resources

565
00:24:52,200 --> 00:24:54,450
and take advantage of your cloud accounts.

566
00:24:54,450 --> 00:24:57,030
There are companies that went bankrupt

567
00:24:57,030 --> 00:25:00,150
because this kind of like
attacks back in the day, right?

568
00:25:00,150 --> 00:25:02,790
I hope no more, as more people are aware

569
00:25:02,790 --> 00:25:04,140
about the security elements

570
00:25:04,140 --> 00:25:06,960
that they should be adding
for every single account.

571
00:25:06,960 --> 00:25:08,880
But it's really, really
important for all of us

572
00:25:08,880 --> 00:25:10,710
to remember that security needs

573
00:25:10,710 --> 00:25:12,510
to be created from the scratch.

574
00:25:12,510 --> 00:25:14,250
From the moment that you are building

575
00:25:14,250 --> 00:25:16,623
your AI infrastructure, okay?

576
00:25:17,670 --> 00:25:19,590
When you think about like those different

577
00:25:19,590 --> 00:25:22,110
like attack vectors on the infrastructure,

578
00:25:22,110 --> 00:25:25,410
a lot of times are related
with the privilege escalation,

579
00:25:25,410 --> 00:25:26,970
cloud misconfigurations,

580
00:25:26,970 --> 00:25:28,860
and vulnerabilities that can be affected

581
00:25:28,860 --> 00:25:32,100
from outside to inside the organizations.

582
00:25:32,100 --> 00:25:33,810
A lot of them, they're focusing

583
00:25:33,810 --> 00:25:36,120
like a deny of services, right?

584
00:25:36,120 --> 00:25:38,970
Basically stopping your
systems to work, right,

585
00:25:38,970 --> 00:25:41,250
and people won't be able to access.

586
00:25:41,250 --> 00:25:43,440
In a lot of cases also as I mentioned,

587
00:25:43,440 --> 00:25:44,760
the cost increase,

588
00:25:44,760 --> 00:25:46,740
that can create like a massive problem

589
00:25:46,740 --> 00:25:48,713
for some organizations, okay?

590
00:25:50,190 --> 00:25:51,900
Infrastructure it is important

591
00:25:51,900 --> 00:25:53,250
because you need to think about like

592
00:25:53,250 --> 00:25:55,170
the AI security posture management,

593
00:25:55,170 --> 00:25:56,790
how you are monitoring for every

594
00:25:56,790 --> 00:26:00,060
potential misconfiguration
in Bedrock, SageMaker,

595
00:26:00,060 --> 00:26:01,380
or any other infrastructure

596
00:26:01,380 --> 00:26:03,690
that you're using from the cloud provider.

597
00:26:03,690 --> 00:26:08,010
But also how you tie in that
with the new compliances,

598
00:26:08,010 --> 00:26:10,260
like it could be

599
00:26:10,260 --> 00:26:12,270
the EU act from Europe,

600
00:26:12,270 --> 00:26:16,320
it can be like a new
systems from the CSA in US,

601
00:26:16,320 --> 00:26:18,810
it can be from OWASP Top 10

602
00:26:18,810 --> 00:26:20,760
that like it is being very vocal

603
00:26:20,760 --> 00:26:22,350
like how you should be looking

604
00:26:22,350 --> 00:26:25,200
for those risks involving AI applications.

605
00:26:25,200 --> 00:26:26,910
All those elements are tie in

606
00:26:26,910 --> 00:26:29,460
with the AI secure posture management.

607
00:26:29,460 --> 00:26:32,880
The other point, it is AI
models, resources, controls.

608
00:26:32,880 --> 00:26:35,700
When you are having those infrastructures,

609
00:26:35,700 --> 00:26:38,190
they usually generate logs, right?

610
00:26:38,190 --> 00:26:40,190
Majority of the logs are being basically

611
00:26:41,040 --> 00:26:43,800
collected through like a
VPC flow logs in some cases

612
00:26:43,800 --> 00:26:45,210
and the cloud trails.

613
00:26:45,210 --> 00:26:46,980
What we do in these cases,

614
00:26:46,980 --> 00:26:49,170
it is collecting that information

615
00:26:49,170 --> 00:26:52,320
and apply tread intelligence
on top of that, right?

616
00:26:52,320 --> 00:26:55,050
If we see any suspicious activities

617
00:26:55,050 --> 00:26:58,020
happening with like a
SageMaker or Bedrock,

618
00:26:58,020 --> 00:27:01,080
you can basically create
a cloud detection response

619
00:27:01,080 --> 00:27:04,113
to mitigate those
attacks right away, okay?

620
00:27:05,370 --> 00:27:06,630
Network.

621
00:27:06,630 --> 00:27:08,730
As more and more organizations

622
00:27:08,730 --> 00:27:10,170
are building the AI applications

623
00:27:10,170 --> 00:27:11,340
and making that available,

624
00:27:11,340 --> 00:27:14,310
there is a new path,
right, for manipulation.

625
00:27:14,310 --> 00:27:16,680
In the beginning, AI applications was like

626
00:27:16,680 --> 00:27:19,260
basically focused of
like the internal usage.

627
00:27:19,260 --> 00:27:22,410
Now more and more companies
are putting that outside

628
00:27:22,410 --> 00:27:25,830
and it's very, very important
for us to validate it

629
00:27:25,830 --> 00:27:28,530
because in some organizations

630
00:27:28,530 --> 00:27:30,540
they have a hybrid infrastructure.

631
00:27:30,540 --> 00:27:32,460
I was actually right before here

632
00:27:32,460 --> 00:27:34,620
talking with an Israel customer

633
00:27:34,620 --> 00:27:36,780
where they have like a full dedicated

634
00:27:36,780 --> 00:27:39,210
data centers for some of the applications,

635
00:27:39,210 --> 00:27:40,980
but they're also creating some level

636
00:27:40,980 --> 00:27:42,540
of like a cloud infrastructure

637
00:27:42,540 --> 00:27:44,310
and they want to connect with each other.

638
00:27:44,310 --> 00:27:46,560
Hybrid, it is a very common element

639
00:27:46,560 --> 00:27:48,810
for a lot of companies that we see today.

640
00:27:48,810 --> 00:27:50,880
But how they can have the same level

641
00:27:50,880 --> 00:27:53,910
of protection for like
the APIs communications

642
00:27:53,910 --> 00:27:55,590
but also potential vulnerabilities

643
00:27:55,590 --> 00:27:59,760
that can exist on those
infrastructure, okay?

644
00:27:59,760 --> 00:28:01,920
Attackers use vulnerabilities

645
00:28:01,920 --> 00:28:05,010
that are like fighting very often.

646
00:28:05,010 --> 00:28:07,920
One of the things that Trend
tried to help on this way,

647
00:28:07,920 --> 00:28:12,750
it is through what we call
Zero Day Initiative, ZDI, okay?

648
00:28:12,750 --> 00:28:15,760
We are the biggest bug
boundary for enterprise

649
00:28:17,550 --> 00:28:19,740
vulnerabilities in the market, right?

650
00:28:19,740 --> 00:28:22,170
Last year we found 73% of the

651
00:28:22,170 --> 00:28:24,030
total vulnerabilities in the market

652
00:28:24,030 --> 00:28:26,070
and our goal is like how we can bring

653
00:28:26,070 --> 00:28:28,140
that information about the vulnerabilities

654
00:28:28,140 --> 00:28:31,440
in the different like
AI stack applications

655
00:28:31,440 --> 00:28:33,930
for you all be aware of like the issues

656
00:28:33,930 --> 00:28:37,440
and the risks involving
AI technologies, okay?

657
00:28:37,440 --> 00:28:40,950
It can be MF flow, it can be MCP servers,

658
00:28:40,950 --> 00:28:44,250
it can be many different
like parts of the AI stack

659
00:28:44,250 --> 00:28:47,460
that you are using that
sometimes people forget

660
00:28:47,460 --> 00:28:51,240
that potential vulnerabilities can exist.

661
00:28:51,240 --> 00:28:54,810
And part of this, we did
like a recently launch

662
00:28:54,810 --> 00:28:57,660
with AWS on the year, AWS Firewall.

663
00:28:57,660 --> 00:29:00,990
Where we can edit our IDS and IPS rules

664
00:29:00,990 --> 00:29:03,150
on top of the AWS Firewall

665
00:29:03,150 --> 00:29:05,700
for you to protect against those exploits,

666
00:29:05,700 --> 00:29:06,870
those vulnerabilities.

667
00:29:06,870 --> 00:29:11,400
It's additional layer in
clouded like native approach

668
00:29:11,400 --> 00:29:14,883
with AWS to enhance
those network protection.

669
00:29:16,110 --> 00:29:18,780
When you talk about like
network attack vectors, right,

670
00:29:18,780 --> 00:29:21,150
we are talking about
like an API interception

671
00:29:21,150 --> 00:29:23,760
that are very important lateral movements

672
00:29:23,760 --> 00:29:26,130
that can be moving from
one container to another

673
00:29:26,130 --> 00:29:30,150
or from one VPC to another VPC.

674
00:29:30,150 --> 00:29:31,590
Data exfiltration,

675
00:29:31,590 --> 00:29:34,380
when the attackers find
the critical assets

676
00:29:34,380 --> 00:29:36,000
or the critical data they're trying

677
00:29:36,000 --> 00:29:39,210
to exfiltrate it to outside, right?

678
00:29:39,210 --> 00:29:41,700
And having that like layer visibility

679
00:29:41,700 --> 00:29:45,210
or that like layer of like a risk

680
00:29:45,210 --> 00:29:47,280
with like a different network sensor,

681
00:29:47,280 --> 00:29:48,540
it is really important.

682
00:29:48,540 --> 00:29:51,540
Plus the IDS and IPS visibility.

683
00:29:51,540 --> 00:29:53,100
If there is any exploits coming

684
00:29:53,100 --> 00:29:56,640
to your cloud infrastructure,
how do you see that?

685
00:29:56,640 --> 00:29:59,010
How can you not just like detect,

686
00:29:59,010 --> 00:30:01,230
but how you can actually prevent

687
00:30:01,230 --> 00:30:04,320
and block those exploits right away?

688
00:30:04,320 --> 00:30:08,010
Drafting traffic inspection
is really important.

689
00:30:08,010 --> 00:30:09,720
Not just for the regular traffic

690
00:30:09,720 --> 00:30:13,683
but also the APIs that the
people forget about this.

691
00:30:14,599 --> 00:30:16,950
I was checking some data recently

692
00:30:16,950 --> 00:30:19,590
and I was actually super skeptical

693
00:30:19,590 --> 00:30:22,920
and concerned because a lot of customers,

694
00:30:22,920 --> 00:30:25,650
they are forgetting about
adding a layer of like

695
00:30:25,650 --> 00:30:28,650
authentication authorization
in API gateways

696
00:30:28,650 --> 00:30:30,240
and API end points.

697
00:30:30,240 --> 00:30:35,240
40% of the total APIs that
we are monitoring today,

698
00:30:35,610 --> 00:30:37,710
they are not having any level

699
00:30:37,710 --> 00:30:40,710
of like authorization and verification.

700
00:30:40,710 --> 00:30:42,180
This is super concerning.

701
00:30:42,180 --> 00:30:43,770
What that means, it is

702
00:30:43,770 --> 00:30:47,970
if those API endpoints are
connected with critical data,

703
00:30:47,970 --> 00:30:49,740
they have a freedom.

704
00:30:49,740 --> 00:30:53,880
It's kind of like a free highway
to get access to that data.

705
00:30:53,880 --> 00:30:57,240
Please make sure every single
time when you have an API

706
00:30:57,240 --> 00:31:00,360
open to the internet,
they are being validated.

707
00:31:00,360 --> 00:31:02,730
They have some level of authentication

708
00:31:02,730 --> 00:31:05,220
as a minimal layer protection.

709
00:31:05,220 --> 00:31:06,780
Plus if you can,

710
00:31:06,780 --> 00:31:09,270
add like some layer of like IDS and IPS

711
00:31:09,270 --> 00:31:11,223
to protect that too, okay?

712
00:31:12,810 --> 00:31:15,270
Users: The Human Factor,

713
00:31:15,270 --> 00:31:17,970
it's been the biggest problem for probably

714
00:31:17,970 --> 00:31:20,760
since the internet has
been created, right?

715
00:31:20,760 --> 00:31:22,410
Every single organization,

716
00:31:22,410 --> 00:31:24,720
we have that user that can click

717
00:31:24,720 --> 00:31:28,320
and do all the things
that the security teams

718
00:31:28,320 --> 00:31:31,080
or the security awareness
tell them to not do it,

719
00:31:31,080 --> 00:31:33,000
they actually go against, right?

720
00:31:33,000 --> 00:31:36,660
Even if we do the training
awareness and all that.

721
00:31:36,660 --> 00:31:39,780
But how you can make sure the human error

722
00:31:39,780 --> 00:31:43,380
it is not giving like
access to infrastructure?

723
00:31:43,380 --> 00:31:45,990
How do you make sure
every single developer

724
00:31:45,990 --> 00:31:49,260
are aware about like the risks involving

725
00:31:49,260 --> 00:31:52,440
of like things that they're
accessing from outside?

726
00:31:52,440 --> 00:31:54,240
It is super, super important

727
00:31:54,240 --> 00:31:56,430
to understand the access control,

728
00:31:56,430 --> 00:31:58,680
the permissions that the users, they have,

729
00:31:58,680 --> 00:32:00,990
phishing, social engineering attacks

730
00:32:00,990 --> 00:32:02,970
that happens every single day.

731
00:32:02,970 --> 00:32:05,550
Those awareness, it is super important

732
00:32:05,550 --> 00:32:07,110
for people to starting paying

733
00:32:07,110 --> 00:32:09,720
attention what they're doing, right?

734
00:32:09,720 --> 00:32:11,940
More and more scams are being created.

735
00:32:11,940 --> 00:32:15,210
With AI right now, it's been crazy.

736
00:32:15,210 --> 00:32:17,550
Deepfake attacks and more are coming

737
00:32:17,550 --> 00:32:20,103
as a big problem When
you think about human.

738
00:32:21,480 --> 00:32:23,580
One single

739
00:32:23,580 --> 00:32:26,970
click or one single malicious library

740
00:32:26,970 --> 00:32:29,940
can open the door for like the attacker

741
00:32:29,940 --> 00:32:32,160
inside your organization.

742
00:32:32,160 --> 00:32:36,180
That's how critical it is
a human in our companies.

743
00:32:36,180 --> 00:32:39,480
If they do a mistake,
it can be open the door

744
00:32:39,480 --> 00:32:41,730
and if you don't have a validation after,

745
00:32:41,730 --> 00:32:43,590
they'll be able to take advantage

746
00:32:43,590 --> 00:32:46,080
of like going lateral movement,

747
00:32:46,080 --> 00:32:48,180
detecting sensitive information,

748
00:32:48,180 --> 00:32:51,363
and do couple other things that
can compromise your company.

749
00:32:52,350 --> 00:32:54,330
Misconfiguration is a good,

750
00:32:54,330 --> 00:32:56,490
it's a problem sometimes, right?

751
00:32:56,490 --> 00:32:59,700
Overexposed access for specific users

752
00:32:59,700 --> 00:33:04,350
and deepfake manipulation
are increasing heavily.

753
00:33:04,350 --> 00:33:06,090
Mainly like the deepfake for sure

754
00:33:06,090 --> 00:33:10,230
with the usage of like
AI on those technologies.

755
00:33:10,230 --> 00:33:13,020
Having ways that you can
monitoring, for example,

756
00:33:13,020 --> 00:33:16,350
developers today are
creating AI applications

757
00:33:16,350 --> 00:33:18,450
in their own desktops because they're

758
00:33:18,450 --> 00:33:21,540
getting more GPUs and all, and so on.

759
00:33:21,540 --> 00:33:24,420
Having ways that you're
monitoring those AI applications

760
00:33:24,420 --> 00:33:27,210
and making sure attackers
are not manipulating

761
00:33:27,210 --> 00:33:31,050
the configuration file and those
applications are important.

762
00:33:31,050 --> 00:33:33,900
Having, like it's
important for you to have

763
00:33:33,900 --> 00:33:37,470
this kind of layer
protection for local AI apps.

764
00:33:37,470 --> 00:33:41,070
The second one, having a way to detect

765
00:33:41,070 --> 00:33:43,800
and protect against deepfakes.

766
00:33:43,800 --> 00:33:45,180
Okay, that's important.

767
00:33:45,180 --> 00:33:47,250
And phishing attacks for sure.

768
00:33:47,250 --> 00:33:49,470
Every single organization
should have already

769
00:33:49,470 --> 00:33:51,920
because it's increasing
every single day with AI.

770
00:33:53,670 --> 00:33:55,800
Okay, as we wrap up here,

771
00:33:55,800 --> 00:33:57,840
the mission is, I think
it's very clear, right?

772
00:33:57,840 --> 00:33:59,703
We create like a little,

773
00:34:00,690 --> 00:34:04,950
a flow of like coming from
the data to the users.

774
00:34:04,950 --> 00:34:07,800
The last point in every
single organization.

775
00:34:07,800 --> 00:34:12,510
Securing AI requires a
connecting every single layer

776
00:34:12,510 --> 00:34:14,460
from the data to the users

777
00:34:14,460 --> 00:34:18,660
with a full visibility
and proactive controls.

778
00:34:18,660 --> 00:34:19,890
When you think about this,

779
00:34:19,890 --> 00:34:22,800
it is like not just adding
one layer protection,

780
00:34:22,800 --> 00:34:26,040
AI security only works when data, models,

781
00:34:26,040 --> 00:34:28,680
pipelines, and runtime are unified

782
00:34:28,680 --> 00:34:31,140
under a single security lens.

783
00:34:31,140 --> 00:34:32,250
Why do I say this?

784
00:34:32,250 --> 00:34:36,390
It is, sometimes if you have
like a disparate technologies

785
00:34:36,390 --> 00:34:39,570
for data, for AI pipelines, or for models,

786
00:34:39,570 --> 00:34:41,310
the problem in this case it is

787
00:34:41,310 --> 00:34:43,500
you are not correlating those telemetries,

788
00:34:43,500 --> 00:34:46,560
you are not correlating the findings

789
00:34:46,560 --> 00:34:48,480
that can help us to create like

790
00:34:48,480 --> 00:34:50,730
a mitigation detection response.

791
00:34:50,730 --> 00:34:54,603
Super important for all of
you to make sure those points.

792
00:34:55,920 --> 00:34:58,200
As I mentioned this session here

793
00:34:58,200 --> 00:35:00,540
was actually based in one
of the thread research

794
00:35:00,540 --> 00:35:01,980
that we found globally.

795
00:35:01,980 --> 00:35:04,867
It's called, this article was called

796
00:35:04,867 --> 00:35:07,890
"Silent Sabotage: Weaponizing AI Models

797
00:35:07,890 --> 00:35:09,750
in Exposed Containers".

798
00:35:09,750 --> 00:35:12,480
This case, a malicious actor was able

799
00:35:12,480 --> 00:35:15,483
through a exposed container
that was public facing,

800
00:35:17,550 --> 00:35:19,710
extract the model, right?

801
00:35:19,710 --> 00:35:23,460
Get the model, manipulated
the data of the model,

802
00:35:23,460 --> 00:35:26,850
redeploy or kind of
like reimport that model

803
00:35:26,850 --> 00:35:29,730
back to the container
that's being exposed,

804
00:35:29,730 --> 00:35:32,550
and compromising the way the predictions

805
00:35:32,550 --> 00:35:35,010
are being solved, right?

806
00:35:35,010 --> 00:35:36,150
Why I mention about this?

807
00:35:36,150 --> 00:35:37,710
Because there are technologies,

808
00:35:37,710 --> 00:35:40,110
like container secure
technologies in runtime

809
00:35:40,110 --> 00:35:42,630
that you can have this
kind of like monitoring.

810
00:35:42,630 --> 00:35:44,780
Like one of the rules
that I was mentioning here

811
00:35:44,780 --> 00:35:46,540
is like any kind of like

812
00:35:48,390 --> 00:35:51,150
external connectivity with the containers

813
00:35:51,150 --> 00:35:53,820
that are trying to extract files

814
00:35:53,820 --> 00:35:56,850
or that they're trying to
manipulate files in the container.

815
00:35:56,850 --> 00:35:59,760
Container should be mutable, right?

816
00:35:59,760 --> 00:36:00,990
Like should be not changing.

817
00:36:00,990 --> 00:36:03,930
If you are changing, you
should be changing the pipeline

818
00:36:03,930 --> 00:36:06,480
before you push a production, right?

819
00:36:06,480 --> 00:36:09,870
But this is really interesting
article for you all

820
00:36:09,870 --> 00:36:12,060
to see it and understand a little bit more

821
00:36:12,060 --> 00:36:14,160
where this session came from

822
00:36:14,160 --> 00:36:15,420
and why we are talking about

823
00:36:15,420 --> 00:36:17,973
this layers protection here, okay?

824
00:36:19,468 --> 00:36:22,743
One of the interesting
things we talk about,

825
00:36:24,000 --> 00:36:26,760
this approach of like layer protection

826
00:36:26,760 --> 00:36:29,680
came from my actually
last session in re:Invent

827
00:36:30,570 --> 00:36:34,440
where I created what we call
AI security blueprint, okay?

828
00:36:34,440 --> 00:36:36,660
And that AI security blueprint,

829
00:36:36,660 --> 00:36:40,770
we actually not just wanted
to create as a paper,

830
00:36:40,770 --> 00:36:43,140
as an informative information for you,

831
00:36:43,140 --> 00:36:45,540
but we bring that to our platform.

832
00:36:45,540 --> 00:36:48,750
We brought like every
single layer on that one

833
00:36:48,750 --> 00:36:53,010
to the developers, deployment,
runtime, AI workloads,

834
00:36:53,010 --> 00:36:55,410
AI applications, AI data,

835
00:36:55,410 --> 00:36:59,220
and as we identify every single specific

836
00:36:59,220 --> 00:37:02,070
piece of AI infrastructure
in your environment,

837
00:37:02,070 --> 00:37:03,720
we also map it if you have

838
00:37:03,720 --> 00:37:06,270
the layer protection enabled or not.

839
00:37:06,270 --> 00:37:08,130
The idea here is like helping you

840
00:37:08,130 --> 00:37:11,910
to identify if every single
layer protection that we have,

841
00:37:11,910 --> 00:37:13,500
you are being able to protect

842
00:37:13,500 --> 00:37:17,343
and prevent from malicious
actors on that, okay?

843
00:37:18,810 --> 00:37:21,390
When you talk about like
those layers protection,

844
00:37:21,390 --> 00:37:25,080
what I did here for customers
to help, is okay, data.

845
00:37:25,080 --> 00:37:26,970
What are the security challenge?

846
00:37:26,970 --> 00:37:29,370
What are the security controls

847
00:37:29,370 --> 00:37:30,840
that exist in the market?

848
00:37:30,840 --> 00:37:34,380
And how our platform can
help with some technologies.

849
00:37:34,380 --> 00:37:36,780
Microservices, when you're
talking about microservices,

850
00:37:36,780 --> 00:37:38,610
it's not just Kubernetes.

851
00:37:38,610 --> 00:37:40,740
It could be NIMs, NVIDIA NIMs, right?

852
00:37:40,740 --> 00:37:42,840
Where you are creating those containers

853
00:37:42,840 --> 00:37:46,500
that are being utilized
by NVIDIA technologies.

854
00:37:46,500 --> 00:37:50,280
Models, agents, how you are
protecting that interaction

855
00:37:50,280 --> 00:37:52,470
as you are building those AI applications

856
00:37:52,470 --> 00:37:54,750
and retraining those models.

857
00:37:54,750 --> 00:37:57,360
Infrastructure, it could be a AWS

858
00:37:57,360 --> 00:37:59,490
or any other cloud provider plus

859
00:37:59,490 --> 00:38:02,670
any VD infrastructure if
you have some of those.

860
00:38:02,670 --> 00:38:05,820
How you are applying AI
secure posture management,

861
00:38:05,820 --> 00:38:09,480
API risk visibility
for unauthorized risks,

862
00:38:09,480 --> 00:38:11,640
AI detection response as I mentioned,

863
00:38:11,640 --> 00:38:14,010
when you collected those
informations from Bedrock

864
00:38:14,010 --> 00:38:17,850
and SageMaker, you are able
to apply detection models

865
00:38:17,850 --> 00:38:21,600
on top of that to identify
any suspicious activities,

866
00:38:21,600 --> 00:38:23,550
Any AI application
security that I mentioned

867
00:38:23,550 --> 00:38:26,343
about the AI scanner
and the AI guard, okay?

868
00:38:27,540 --> 00:38:29,040
The two ones here,

869
00:38:29,040 --> 00:38:31,203
because I know people were taking photos,

870
00:38:32,130 --> 00:38:33,750
was like the network layer

871
00:38:33,750 --> 00:38:36,810
that's related with
the network IDS and IPS

872
00:38:36,810 --> 00:38:39,810
where we can prevent and protect against

873
00:38:39,810 --> 00:38:42,750
the exploitation when you
have those applications

874
00:38:42,750 --> 00:38:46,200
or those AI stacks, how
we talk that, right?

875
00:38:46,200 --> 00:38:47,670
And the last one is the user,

876
00:38:47,670 --> 00:38:49,890
how you can basically protect on those.

877
00:38:49,890 --> 00:38:52,960
There's a comprehensive paper on this

878
00:38:54,150 --> 00:38:58,290
AI security blueprint that I
go in every single detail here

879
00:38:58,290 --> 00:39:00,840
and explain more information.

880
00:39:00,840 --> 00:39:04,080
So like why you should be
protecting the use cases

881
00:39:04,080 --> 00:39:06,240
and the level of like the layer protection

882
00:39:06,240 --> 00:39:07,560
that you should be thinking when you are

883
00:39:07,560 --> 00:39:10,083
creating those strategies, okay?

884
00:39:12,831 --> 00:39:16,080
Okay, preparing for like the next wave.

885
00:39:16,080 --> 00:39:18,120
AI-born attacks are being

886
00:39:18,120 --> 00:39:20,460
accelerated every single day.

887
00:39:20,460 --> 00:39:23,370
To stay resilient, we
must anticipate a threat.

888
00:39:23,370 --> 00:39:25,080
We need to think in a different way

889
00:39:25,080 --> 00:39:27,600
how we are protecting
against those threats.

890
00:39:27,600 --> 00:39:31,050
Automated defense and
evolve infrastructure

891
00:39:31,050 --> 00:39:33,990
as fast as the adversaries
are evolving their attacks

892
00:39:33,990 --> 00:39:36,900
are extremely important
for us today, okay?

893
00:39:36,900 --> 00:39:38,580
Proactive AI security,

894
00:39:38,580 --> 00:39:40,680
it is something that
you should be thinking

895
00:39:40,680 --> 00:39:44,550
for like this new adaptive
way for you protecting

896
00:39:44,550 --> 00:39:46,650
not just your AI applications

897
00:39:46,650 --> 00:39:48,030
but every single application

898
00:39:48,030 --> 00:39:49,620
that you have in the organization.

899
00:39:49,620 --> 00:39:51,510
The velocity, how we are innovating

900
00:39:51,510 --> 00:39:53,310
in every single company today,

901
00:39:53,310 --> 00:39:55,800
it is unbelievable, right?

902
00:39:55,800 --> 00:39:57,630
We release new applications,

903
00:39:57,630 --> 00:40:01,410
we release new versions
every couple of minutes

904
00:40:01,410 --> 00:40:03,003
in those companies here today.

905
00:40:04,350 --> 00:40:07,170
Okay, we are doing some pretty cool stuff.

906
00:40:07,170 --> 00:40:10,890
I dunno if you saw what we
call a Trend Vision One Vault.

907
00:40:10,890 --> 00:40:14,040
People in the United
States can win $10,000

908
00:40:14,040 --> 00:40:15,690
if they crack the code.

909
00:40:15,690 --> 00:40:17,920
This session is giving one of the codes

910
00:40:19,167 --> 00:40:22,350
for you to have the way to test it

911
00:40:22,350 --> 00:40:24,090
and trying to crack the code.

912
00:40:24,090 --> 00:40:26,070
The number three, it is the number

913
00:40:26,070 --> 00:40:28,590
that I'm giving from the session.

914
00:40:28,590 --> 00:40:31,370
I hope some of you will win, okay.

915
00:40:31,370 --> 00:40:34,380
We're playing this since Black Hat

916
00:40:34,380 --> 00:40:36,150
and nobody was able to crack

917
00:40:36,150 --> 00:40:38,460
and I hope somebody this time in re:Invent

918
00:40:38,460 --> 00:40:42,600
will be able to crack the
code for the $10,000, okay.

919
00:40:42,600 --> 00:40:44,820
Without more, I want to
say thank you so much

920
00:40:44,820 --> 00:40:47,250
for joining me today in this session.

921
00:40:47,250 --> 00:40:49,020
Really appreciated your attention.

922
00:40:49,020 --> 00:40:51,600
And if there is any question,

923
00:40:51,600 --> 00:40:54,120
I'm open because there
is actually time here

924
00:40:54,120 --> 00:40:56,040
that I can answer some of the questions.

925
00:40:56,040 --> 00:40:57,660
But thank you so much, everyone.

926
00:40:57,660 --> 00:40:59,013
Really appreciate it.

927
00:40:59,013 --> 00:41:02,013
(audience applauds)

