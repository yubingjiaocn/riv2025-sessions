1
00:00:00,180 --> 00:00:02,640
- Hey, guys, welcome. Good afternoon.

2
00:00:02,640 --> 00:00:04,410
This is really awesome.

3
00:00:04,410 --> 00:00:07,710
We are super excited
with this customer panel.

4
00:00:07,710 --> 00:00:10,680
We have an exciting session for you.

5
00:00:10,680 --> 00:00:12,340
If I can advance the slide

6
00:00:13,410 --> 00:00:16,690
and it's very (indistinct)
to what we have been

7
00:00:17,850 --> 00:00:19,530
listening in at the keynote.

8
00:00:19,530 --> 00:00:20,760
How many of you were able

9
00:00:20,760 --> 00:00:23,280
to watch Matt's keynote this morning?

10
00:00:23,280 --> 00:00:26,430
So you, oh wow. This more than I thought.

11
00:00:26,430 --> 00:00:28,950
So you heard about AI infrastructure

12
00:00:28,950 --> 00:00:31,590
and how we are optimizing,
you heard about AI factory,

13
00:00:31,590 --> 00:00:34,233
there was a Trainium
3 launch, it's now GA.

14
00:00:35,910 --> 00:00:39,750
He talked about UltraClusters
and UltraServers.

15
00:00:39,750 --> 00:00:41,760
So we have a distinguished
panel of speakers.

16
00:00:41,760 --> 00:00:45,090
We have Andrea Klein Director
of Engineering at Arm.

17
00:00:45,090 --> 00:00:47,010
We have Henri Dwyer,

18
00:00:47,010 --> 00:00:49,560
Senior Director of
Engineering at Genentech

19
00:00:49,560 --> 00:00:53,640
and we have Shaunak Godbole who
is a field CTO at Fireworks.

20
00:00:53,640 --> 00:00:57,150
And what we really like you
to take away from this session

21
00:00:57,150 --> 00:00:59,850
is when you go back
home you should be armed

22
00:00:59,850 --> 00:01:03,360
with some tips and tricks and mechanisms

23
00:01:03,360 --> 00:01:06,210
in how you can scale your AI applications

24
00:01:06,210 --> 00:01:10,230
using AWS's infrastructure and our stack.

25
00:01:10,230 --> 00:01:13,863
And so we have a framework
that seems to work.

26
00:01:15,090 --> 00:01:17,670
The agenda is really that
I'll set up the framework.

27
00:01:17,670 --> 00:01:21,030
We think about scaling
AI in terms of scaling up

28
00:01:21,030 --> 00:01:24,180
in terms of TFLOPS and memory

29
00:01:24,180 --> 00:01:25,380
and memory bandwidth.

30
00:01:25,380 --> 00:01:28,710
Scaling out in terms of
increasing the number of GPUs

31
00:01:28,710 --> 00:01:32,700
and servers and
UltraClusters and clusters.

32
00:01:32,700 --> 00:01:34,590
And then scaling across
the inference stack.

33
00:01:34,590 --> 00:01:38,040
And each of the speakers
will actually index on that.

34
00:01:38,040 --> 00:01:41,610
Andrea will talk about her
experiences scaling out.

35
00:01:41,610 --> 00:01:45,750
She'll talk about fault
detection and observability tools

36
00:01:45,750 --> 00:01:49,110
and how, you know, what
you need to pay attention

37
00:01:49,110 --> 00:01:51,900
to when you scale out to thousands of GPUs

38
00:01:51,900 --> 00:01:53,430
and thousands of clusters.

39
00:01:53,430 --> 00:01:55,170
Henri will talk about scaling up.

40
00:01:55,170 --> 00:01:57,150
He has a very interesting application

41
00:01:57,150 --> 00:01:58,950
around molecular dynamics.

42
00:01:58,950 --> 00:02:02,580
So a lot of LLMs but also
a lot of spatial coding.

43
00:02:02,580 --> 00:02:05,040
And he'll end with agentic AI.

44
00:02:05,040 --> 00:02:06,990
Genentech is actually using agentic AI

45
00:02:06,990 --> 00:02:09,270
for what they call lab in the loop.

46
00:02:09,270 --> 00:02:11,520
So this will be a very
interesting thing to hear from.

47
00:02:11,520 --> 00:02:15,150
And that's actually a good
segue to Shaunak's session

48
00:02:15,150 --> 00:02:17,520
where he's going to talk
about the principles

49
00:02:17,520 --> 00:02:19,380
of building agentic AI

50
00:02:19,380 --> 00:02:23,100
and how they have been able
to optimize across each layer

51
00:02:23,100 --> 00:02:25,860
of the stack at the hardware
layer, model, frameworks,

52
00:02:25,860 --> 00:02:27,990
and runtime layer to get what,

53
00:02:27,990 --> 00:02:29,940
10 plus trillion tokens per day.

54
00:02:29,940 --> 00:02:33,990
So he'll talk about more on that.

55
00:02:33,990 --> 00:02:35,670
I'll let them introduce themselves.

56
00:02:35,670 --> 00:02:38,460
Fireworks also has a booth so Shaunak

57
00:02:38,460 --> 00:02:39,610
will have more details.

58
00:02:40,560 --> 00:02:42,120
Alright, so without further ado.

59
00:02:42,120 --> 00:02:45,150
So when I think of scaling
up and scaling out,

60
00:02:45,150 --> 00:02:47,890
there are two workloads that come up

61
00:02:49,200 --> 00:02:53,610
front and center and I
spend most of my days

62
00:02:53,610 --> 00:02:55,380
working with accelerate compute

63
00:02:55,380 --> 00:03:00,240
and understanding customer's
needs around AI and around HPC.

64
00:03:00,240 --> 00:03:02,730
So AI as we think of frontier models,

65
00:03:02,730 --> 00:03:04,110
a hundred billion plus models,

66
00:03:04,110 --> 00:03:05,940
they of course need a lot of compute,

67
00:03:05,940 --> 00:03:07,080
but even smaller models

68
00:03:07,080 --> 00:03:09,930
that are more network latency sensitive

69
00:03:09,930 --> 00:03:13,710
like video and image models,
they require scaling up

70
00:03:13,710 --> 00:03:16,800
and scaling out in terms of inferencing,

71
00:03:16,800 --> 00:03:18,810
especially around aggregated inferencing

72
00:03:18,810 --> 00:03:20,580
if you're serving batch mode inferencing

73
00:03:20,580 --> 00:03:23,610
but also real time where
latency is critical,

74
00:03:23,610 --> 00:03:24,930
this becomes important.

75
00:03:24,930 --> 00:03:27,810
And the reason it becomes
important is these models

76
00:03:27,810 --> 00:03:32,700
are processing huge, massive
amounts petabytes of data

77
00:03:32,700 --> 00:03:36,090
and doing massive amounts
of parallel computing

78
00:03:36,090 --> 00:03:37,620
and scientific calculations.

79
00:03:37,620 --> 00:03:41,790
So that's why scaling on
infrastructure becomes important

80
00:03:41,790 --> 00:03:45,720
and where AWS spends, like our team spends

81
00:03:45,720 --> 00:03:48,330
every single day talking to customers.

82
00:03:48,330 --> 00:03:50,580
And that reminds me, I
forgot to introduce myself.

83
00:03:50,580 --> 00:03:54,150
So I'm Principal Specialist

84
00:03:54,150 --> 00:03:56,400
in our GenAI accelerated compute team.

85
00:03:56,400 --> 00:03:59,430
So everything to do with
accelerated compute.

86
00:03:59,430 --> 00:04:01,780
One of the hardware layers
that I'll talk about

87
00:04:03,150 --> 00:04:04,170
comes through our team,

88
00:04:04,170 --> 00:04:05,670
Trainium and Inferentia of course,

89
00:04:05,670 --> 00:04:08,310
but NVIDIA GPUs and other GPUs,

90
00:04:08,310 --> 00:04:10,653
that's what we deal with day in day out.

91
00:04:12,210 --> 00:04:17,190
And then HPC also has
some interesting quirks.

92
00:04:17,190 --> 00:04:19,350
They have both tightly coupled

93
00:04:19,350 --> 00:04:20,670
and loosely coupled workloads.

94
00:04:20,670 --> 00:04:23,640
And I've had the fortune of
working in a (indistinct),

95
00:04:23,640 --> 00:04:27,720
Henri will talk about his
protein folding use case.

96
00:04:27,720 --> 00:04:32,430
So these use cases also
lead to large amount

97
00:04:32,430 --> 00:04:33,903
of scale requirements.

98
00:04:35,190 --> 00:04:38,940
And so if I dive deep
into one of the workloads,

99
00:04:38,940 --> 00:04:40,020
if you think of training,

100
00:04:40,020 --> 00:04:43,140
these are typically episodic workloads.

101
00:04:43,140 --> 00:04:45,600
They take a few weeks to a few months,

102
00:04:45,600 --> 00:04:46,950
but they're tightly coupled.

103
00:04:46,950 --> 00:04:50,370
Often when they ask for GPUs from us,

104
00:04:50,370 --> 00:04:51,840
they ask for a single AZ.

105
00:04:51,840 --> 00:04:54,360
In fact they ask for a single spine.

106
00:04:54,360 --> 00:04:57,510
And one of the reasons why
we introduced UltraServers

107
00:04:57,510 --> 00:04:59,550
that you heard at the keynote

108
00:04:59,550 --> 00:05:02,610
is because even a single
spine was not enough.

109
00:05:02,610 --> 00:05:06,063
So we needed to create a high
performance domain for them.

110
00:05:07,530 --> 00:05:10,680
But customization is a
little bit different.

111
00:05:10,680 --> 00:05:13,830
These runs are short but very frequent.

112
00:05:13,830 --> 00:05:16,590
So the runs could run anywhere
from one to four weeks

113
00:05:16,590 --> 00:05:17,700
and you could have anywhere

114
00:05:17,700 --> 00:05:20,730
between four and 10 cycles a year.

115
00:05:20,730 --> 00:05:23,790
And there you often could
use heterogeneous compute.

116
00:05:23,790 --> 00:05:27,840
So you might have P series
GPUs and G series with CPUs.

117
00:05:27,840 --> 00:05:31,440
So you had to think about how
to scale up using containers.

118
00:05:31,440 --> 00:05:34,050
Andrea will have a word
or two of about that

119
00:05:34,050 --> 00:05:36,594
and how to use tools like Karpenter

120
00:05:36,594 --> 00:05:38,050
and EKS to sort of scale up

121
00:05:39,414 --> 00:05:41,460
a heterogeneous compute cluster.

122
00:05:41,460 --> 00:05:44,580
And inference is completely different.

123
00:05:44,580 --> 00:05:46,320
It's often spiky and sporadic,

124
00:05:46,320 --> 00:05:49,920
even batch mode inference
goes on for a few hours,

125
00:05:49,920 --> 00:05:52,560
let's say document
processing or summarization.

126
00:05:52,560 --> 00:05:55,620
And these are often
loosely coupled workloads,

127
00:05:55,620 --> 00:05:57,990
but depending on whether
it's real time batch mode,

128
00:05:57,990 --> 00:06:00,290
you could have different
latency requirements.

129
00:06:01,170 --> 00:06:05,130
So if you think of a mock example

130
00:06:05,130 --> 00:06:07,980
of a chatbot, this is a
very simplistic example

131
00:06:07,980 --> 00:06:11,310
of a chatbot producing one
output token at a time.

132
00:06:11,310 --> 00:06:14,190
Of course now chatbots are
much more sophisticated,

133
00:06:14,190 --> 00:06:16,380
but by and large input token lens

134
00:06:16,380 --> 00:06:19,200
still happens to be higher.

135
00:06:19,200 --> 00:06:21,120
So the initial portion,
the encoding portion

136
00:06:21,120 --> 00:06:24,300
or the prefill portion is compute-bound.

137
00:06:24,300 --> 00:06:28,170
And the intuition there is
you load the hyperparameters

138
00:06:28,170 --> 00:06:29,760
from the HBM memory,

139
00:06:29,760 --> 00:06:33,060
but depending on the length of the prompt,

140
00:06:33,060 --> 00:06:37,230
you could essentially be using
the entire prompt computation

141
00:06:37,230 --> 00:06:38,520
using those hyperparameters.

142
00:06:38,520 --> 00:06:42,930
So you're essentially amortizing
the one time load operation

143
00:06:42,930 --> 00:06:45,063
across a large context length.

144
00:06:46,590 --> 00:06:49,500
But now if you go to the decoding portion,

145
00:06:49,500 --> 00:06:50,460
it's the same intuition.

146
00:06:50,460 --> 00:06:54,330
You load the hyperparameters
again from HBM memory,

147
00:06:54,330 --> 00:06:57,690
but now you're processing
output one token at a time.

148
00:06:57,690 --> 00:07:00,600
So you're amortizing that load operation

149
00:07:00,600 --> 00:07:02,430
across one token at a time.

150
00:07:02,430 --> 00:07:05,160
Of course it's simplistic now
you can have many more tokens

151
00:07:05,160 --> 00:07:07,020
at a time and in fact
Shaunak will talk about

152
00:07:07,020 --> 00:07:09,453
how they optimize the runtime layer.

153
00:07:13,110 --> 00:07:14,280
And we are not done.

154
00:07:14,280 --> 00:07:18,540
So think of memory capacity
as inference of course

155
00:07:18,540 --> 00:07:21,240
some of the bigger models like Llama 4

156
00:07:21,240 --> 00:07:25,350
and DeepSeek require at
least a P5E, if not P5EN,

157
00:07:25,350 --> 00:07:26,823
which is our H200s.

158
00:07:27,750 --> 00:07:29,970
So memory capacity becomes important.

159
00:07:29,970 --> 00:07:33,180
But even for training, if you
think of pipeline parallelism,

160
00:07:33,180 --> 00:07:37,890
if you're trying to compress
a model into a single GPU

161
00:07:37,890 --> 00:07:39,000
and the memory is not enough,

162
00:07:39,000 --> 00:07:41,100
you start getting into
pipeline parallelism

163
00:07:41,100 --> 00:07:43,710
and the more pipeline
parallelism you have,

164
00:07:43,710 --> 00:07:45,750
the less efficient memory usage you have.

165
00:07:45,750 --> 00:07:49,260
So that starts hurting
performance as well.

166
00:07:49,260 --> 00:07:51,000
But we're not done, some of the models,

167
00:07:51,000 --> 00:07:54,060
especially mixture of
expert models, MOA models,

168
00:07:54,060 --> 00:07:57,570
mixture of agents, some
of the multimodal models,

169
00:07:57,570 --> 00:07:59,160
video processing of course,

170
00:07:59,160 --> 00:08:03,750
some of the large frontier
models, they are network bound.

171
00:08:03,750 --> 00:08:05,940
So we introduced a couple of years ago,

172
00:08:05,940 --> 00:08:08,160
10p10u networking

173
00:08:08,160 --> 00:08:09,393
and as a name suggests,

174
00:08:11,970 --> 00:08:15,570
our goal is to deliver tens of petabytes

175
00:08:15,570 --> 00:08:17,520
of non-blocking bandwidth

176
00:08:17,520 --> 00:08:20,160
across tens of thousands of servers

177
00:08:20,160 --> 00:08:21,720
in under 10 microseconds.

178
00:08:21,720 --> 00:08:24,723
So this is something we
use UltraClusters for.

179
00:08:26,626 --> 00:08:28,620
I keep pressing the wrong button.

180
00:08:28,620 --> 00:08:32,610
UltraClusters for, and we will
talk about that in a second.

181
00:08:32,610 --> 00:08:34,760
So this is the framework
I want to present.

182
00:08:35,700 --> 00:08:39,360
When we think of scaling
on infrastructure,

183
00:08:39,360 --> 00:08:42,600
we can either scale out so you
can add more and more servers

184
00:08:42,600 --> 00:08:47,280
and clusters and UltraClusters,
which is Andreas topic.

185
00:08:47,280 --> 00:08:50,520
Or you can scale up which is
if you have smaller models

186
00:08:50,520 --> 00:08:53,880
or less than 10 billion parameter models.

187
00:08:53,880 --> 00:08:55,110
But even if you have bigger models

188
00:08:55,110 --> 00:08:57,600
and you want to kind of
homogenize your procurement

189
00:08:57,600 --> 00:09:00,423
and you have a certain cluster
that you're purchasing,

190
00:09:01,530 --> 00:09:04,740
which is Andre's domain, you can scale up.

191
00:09:04,740 --> 00:09:07,980
So you can now go from
A100s to H100s to H200s

192
00:09:07,980 --> 00:09:11,460
and now be Blackwells.

193
00:09:11,460 --> 00:09:14,280
Or you can scale across
which is the inference stack.

194
00:09:14,280 --> 00:09:16,770
And that depends on what
kind of models you have.

195
00:09:16,770 --> 00:09:19,740
Is it distributed high performance
inferencing on the right

196
00:09:19,740 --> 00:09:23,043
or single node GPUs on the left?

197
00:09:26,460 --> 00:09:28,230
So I mentioned UltraClusters.

198
00:09:28,230 --> 00:09:31,773
So this is in fact today when
Trainium 3 was announced,

199
00:09:32,993 --> 00:09:35,610
we also announced UltraCluster 3.0.

200
00:09:35,610 --> 00:09:40,610
Even UltraCluster 2.0
delivers 16x the bandwidth.

201
00:09:40,710 --> 00:09:44,831
This is our way of managing
tens of thousands of GPUs,

202
00:09:44,831 --> 00:09:49,110
or Trainium accelerators,
in one high speed domain.

203
00:09:49,110 --> 00:09:52,690
So this becomes really important
when you are scaling out

204
00:09:53,970 --> 00:09:57,330
to hundreds or even thousands of GPUs.

205
00:09:57,330 --> 00:10:00,633
We have another paradigm
called UltraServer.

206
00:10:01,500 --> 00:10:03,540
Last year you heard how we were able

207
00:10:03,540 --> 00:10:06,810
to use UltraServer in a
single high-speed domain

208
00:10:06,810 --> 00:10:11,810
of either 72 GPUs or 64
Trainium accelerators.

209
00:10:12,720 --> 00:10:14,760
And this morning Matt
announced Trainium 3,

210
00:10:14,760 --> 00:10:19,530
which is our way of
packing 144 Trainium chips

211
00:10:19,530 --> 00:10:21,600
in one single high speed domain

212
00:10:21,600 --> 00:10:24,783
that gives you five times
the performance per megawatt.

213
00:10:27,570 --> 00:10:28,740
In terms of scaling up,

214
00:10:28,740 --> 00:10:31,830
we have had a 15 year plus
partnership with NVIDIA.

215
00:10:31,830 --> 00:10:34,620
We have been launching
instances almost every year

216
00:10:34,620 --> 00:10:37,080
and over the last six months
the pace has increased

217
00:10:37,080 --> 00:10:38,190
as you can see.

218
00:10:38,190 --> 00:10:41,823
A couple of weeks ago we
announced a B300 base P6,

219
00:10:43,170 --> 00:10:45,960
which delivers a whopping
6.4 terabytes per second

220
00:10:45,960 --> 00:10:49,680
networking speed, 270
gigabytes of memory per GPU.

221
00:10:49,680 --> 00:10:53,250
So this is the biggest
baddest GPU that we have.

222
00:10:53,250 --> 00:10:56,160
But you can read the
specs on others as well.

223
00:10:56,160 --> 00:10:57,903
We have GB200 based P6e.

224
00:10:58,770 --> 00:11:01,060
A number of you should be using

225
00:11:02,400 --> 00:11:04,053
Hopper-based instances as well.

226
00:11:05,580 --> 00:11:09,390
And finally the stack,

227
00:11:09,390 --> 00:11:11,070
we have the hardware layer,

228
00:11:11,070 --> 00:11:15,240
which is what our team
primarily focuses on at AWS.

229
00:11:15,240 --> 00:11:17,580
But on top of the hardware layer

230
00:11:17,580 --> 00:11:20,550
we have many other abstraction layers,

231
00:11:20,550 --> 00:11:23,250
right from security hypervisor using Nitro

232
00:11:23,250 --> 00:11:26,100
to storage, to EFA networking

233
00:11:26,100 --> 00:11:28,593
and then manage services like SageMaker.

234
00:11:29,520 --> 00:11:32,130
So this is kind of the
framework I wanted to set

235
00:11:32,130 --> 00:11:35,460
and now each speaker will
speak about their experiences

236
00:11:35,460 --> 00:11:37,170
on kind of optimizing this

237
00:11:37,170 --> 00:11:39,510
and in fact that this would be
the best part of the session

238
00:11:39,510 --> 00:11:42,393
where you can hear from them.

239
00:11:43,590 --> 00:11:45,000
We'll have Q and A.

240
00:11:45,000 --> 00:11:47,250
We would love to devote

241
00:11:47,250 --> 00:11:51,120
as much time as possible for the Q and A.

242
00:11:51,120 --> 00:11:53,580
If you can hold onto
the questions till then,

243
00:11:53,580 --> 00:11:55,350
it'll help us go through
the narrative first

244
00:11:55,350 --> 00:11:58,830
and then we are open session after that

245
00:11:58,830 --> 00:12:01,683
and we'll pass the mics
around for your questions.

246
00:12:03,720 --> 00:12:06,120
Next up is Andrea, she's a Director at Arm

247
00:12:06,120 --> 00:12:08,070
and she'll speak about her experiences.

248
00:12:09,049 --> 00:12:10,440
- Great, okay. Hi, my name is Andrea.

249
00:12:10,440 --> 00:12:13,530
I'm here from Arm where
I deal with some GPUs

250
00:12:13,530 --> 00:12:15,480
and I have prior experience dealing

251
00:12:15,480 --> 00:12:18,453
with a lot more GPUs at once.

252
00:12:22,470 --> 00:12:24,630
The following advice is
really targeted at someone

253
00:12:24,630 --> 00:12:27,660
who's dealing with a large number of GPUs

254
00:12:27,660 --> 00:12:29,430
for the first time.

255
00:12:29,430 --> 00:12:31,170
And so maybe you've had a few servers,

256
00:12:31,170 --> 00:12:32,190
you've run a few experiments,

257
00:12:32,190 --> 00:12:34,500
but a thousand GPUs kind
of sounds like a lot

258
00:12:34,500 --> 00:12:36,993
and you don't really know what to do.

259
00:12:37,890 --> 00:12:40,080
And if you have not really budgeted

260
00:12:40,080 --> 00:12:41,310
for more than a few machines,

261
00:12:41,310 --> 00:12:43,620
most of this you do not
need to worry about.

262
00:12:43,620 --> 00:12:45,750
If on the other hand you
have raised like let's say

263
00:12:45,750 --> 00:12:49,020
one to 10 to 100 billion
dollars seed round

264
00:12:49,020 --> 00:12:50,880
and you're making a pre-training cluster

265
00:12:50,880 --> 00:12:54,240
and this kind of thing also
this advice is not for you.

266
00:12:54,240 --> 00:12:57,213
I'm gonna have a lot of
different suggestions for you.

267
00:12:58,470 --> 00:13:00,510
But the basic idea is that
there are certain problems

268
00:13:00,510 --> 00:13:03,390
that emerge predictably at scale for GPUs

269
00:13:03,390 --> 00:13:05,070
and a thousand is about the line

270
00:13:05,070 --> 00:13:06,780
where you cannot ignore them anymore.

271
00:13:06,780 --> 00:13:09,270
And somewhere between 1,000
and 10,000 you will have

272
00:13:09,270 --> 00:13:12,300
all of these problems and then
exciting new problems emerge

273
00:13:12,300 --> 00:13:14,730
beyond that that we're going to ignore.

274
00:13:14,730 --> 00:13:15,990
But do not worry, we are gonna solve

275
00:13:15,990 --> 00:13:17,580
all of these problems in about 10 minutes.

276
00:13:17,580 --> 00:13:19,630
And so just pay attention if this is you.

277
00:13:21,870 --> 00:13:24,720
So for my sake, let's assume
you are my target audience

278
00:13:24,720 --> 00:13:27,300
and you have money, the
money is not the problem.

279
00:13:27,300 --> 00:13:29,970
You need to buy some GPUs
and you need to manage them.

280
00:13:29,970 --> 00:13:30,900
And so this is great.

281
00:13:30,900 --> 00:13:33,153
Congratulations, you now have a GPU farm,

282
00:13:34,440 --> 00:13:36,690
but you do need to do
some planning in advance

283
00:13:36,690 --> 00:13:39,340
and think about how you're
gonna handle these things.

284
00:13:40,980 --> 00:13:43,410
And it's a little bit
harder than it looks,

285
00:13:43,410 --> 00:13:45,600
especially if you've dealt
with a few machines before

286
00:13:45,600 --> 00:13:47,310
and you think I know how to run GPUs

287
00:13:47,310 --> 00:13:49,350
and everything is fine.

288
00:13:49,350 --> 00:13:51,120
I think there are a few things
that are really important

289
00:13:51,120 --> 00:13:52,820
to pay attention to at this scale.

290
00:13:53,850 --> 00:13:55,590
The first thing is
you're adding more GPUs.

291
00:13:55,590 --> 00:13:58,860
And so problems that happen
with GPUs happen more often,

292
00:13:58,860 --> 00:14:02,100
but you're probably also adding
more jobs and larger jobs.

293
00:14:02,100 --> 00:14:05,100
Larger jobs are big jobs
that use more GPUs at once.

294
00:14:05,100 --> 00:14:08,400
And so when you have bigger jobs

295
00:14:08,400 --> 00:14:11,280
and longer running jobs,
any problem is going

296
00:14:11,280 --> 00:14:12,420
to occur more often.

297
00:14:12,420 --> 00:14:14,790
It's more likely that your
workload gets disrupted

298
00:14:14,790 --> 00:14:17,790
and fails and you need to be
very good at spotting this

299
00:14:17,790 --> 00:14:19,620
and recovering from it.

300
00:14:19,620 --> 00:14:21,900
And so at this scale you've
probably got a few different

301
00:14:21,900 --> 00:14:23,040
use cases going on.

302
00:14:23,040 --> 00:14:24,780
You may have multiple tenants

303
00:14:24,780 --> 00:14:27,300
or customers who are sharing the capacity.

304
00:14:27,300 --> 00:14:29,010
You may have different types of jobs,

305
00:14:29,010 --> 00:14:30,870
maybe some are training
and some are inference

306
00:14:30,870 --> 00:14:32,610
and some are doing something else.

307
00:14:32,610 --> 00:14:34,200
And you think you've bought a lot of GPUs,

308
00:14:34,200 --> 00:14:35,100
but if you ask your users,

309
00:14:35,100 --> 00:14:36,630
you have not really bought that many.

310
00:14:36,630 --> 00:14:38,730
And so very quickly all
of the workloads pile up

311
00:14:38,730 --> 00:14:40,560
and are kind of demanding GPUs all at once

312
00:14:40,560 --> 00:14:42,510
and you get yourself into some trouble.

313
00:14:43,752 --> 00:14:45,450
The second thing is you're probably

314
00:14:45,450 --> 00:14:47,400
with this type of machine
and this type of scale

315
00:14:47,400 --> 00:14:49,710
doing some kind of distributed workloads,

316
00:14:49,710 --> 00:14:51,090
probably distributed training,

317
00:14:51,090 --> 00:14:53,940
maybe even distributed
or multi-node inference

318
00:14:53,940 --> 00:14:56,310
since you kind of have to
pay attention to networking

319
00:14:56,310 --> 00:14:57,420
and you have to pay attention

320
00:14:57,420 --> 00:15:00,630
to distributed job
optimization and failures.

321
00:15:00,630 --> 00:15:01,860
And finally, as I mentioned,

322
00:15:01,860 --> 00:15:03,810
everything fails more often at scale

323
00:15:03,810 --> 00:15:06,420
where failures become constant failures

324
00:15:06,420 --> 00:15:08,103
and you have to worry about this.

325
00:15:10,920 --> 00:15:12,930
And so don't lose all hope.

326
00:15:12,930 --> 00:15:14,040
It's not that bad.

327
00:15:14,040 --> 00:15:15,390
There are established ways of dealing

328
00:15:15,390 --> 00:15:17,133
with these machines at this scale.

329
00:15:18,120 --> 00:15:20,250
But I think before I talk about what to do

330
00:15:20,250 --> 00:15:24,270
with your machines, I'm going
to consider the possibility

331
00:15:24,270 --> 00:15:26,490
that you have not yet made
any purchasing decisions.

332
00:15:26,490 --> 00:15:29,310
Maybe you just have money
but you don't have GPUs yet.

333
00:15:29,310 --> 00:15:31,410
And if you're in that scenario

334
00:15:31,410 --> 00:15:33,180
and you're seeing many nice slides

335
00:15:33,180 --> 00:15:35,400
with many, many different types of GPUs

336
00:15:35,400 --> 00:15:37,560
and accelerated instances,
it can get overwhelming

337
00:15:37,560 --> 00:15:39,180
trying to decide what to pick.

338
00:15:39,180 --> 00:15:40,770
But I think there are a
couple of rules of thumb

339
00:15:40,770 --> 00:15:42,510
that are helpful.

340
00:15:42,510 --> 00:15:45,060
One thing if it's not obvious is to try

341
00:15:45,060 --> 00:15:47,670
and get all of your machines in one place.

342
00:15:47,670 --> 00:15:49,620
And this is a little bit hard to do,

343
00:15:49,620 --> 00:15:51,420
especially with constrained availability.

344
00:15:51,420 --> 00:15:54,330
And maybe someone can offer
you a few on one coast

345
00:15:54,330 --> 00:15:56,040
right away and a few in another place

346
00:15:56,040 --> 00:15:57,030
and a few in another place

347
00:15:57,030 --> 00:15:59,310
and that's fine, that's okay.

348
00:15:59,310 --> 00:16:01,650
But then you're gonna do a
lot more work managing them

349
00:16:01,650 --> 00:16:03,270
and your utilization is probably going

350
00:16:03,270 --> 00:16:06,090
to be a lot lower if you
have a fragmented system.

351
00:16:06,090 --> 00:16:09,870
And so given the option,
especially for newer GPUs,

352
00:16:09,870 --> 00:16:12,690
I recommend either at the beginning

353
00:16:12,690 --> 00:16:14,970
or kind of pre-negotiated over time

354
00:16:14,970 --> 00:16:18,420
to consolidate your capacity
into a single location.

355
00:16:18,420 --> 00:16:21,360
And increasingly for modern
GPUs is doesn't not just mean

356
00:16:21,360 --> 00:16:23,670
one region or one zone

357
00:16:23,670 --> 00:16:25,230
which you can run with one control plane.

358
00:16:25,230 --> 00:16:27,780
It means one physical UltraCluster built

359
00:16:27,780 --> 00:16:29,730
or equivalently one networking spine.

360
00:16:29,730 --> 00:16:31,320
And so try and get your machines

361
00:16:31,320 --> 00:16:33,540
of one type in the same physical cluster

362
00:16:33,540 --> 00:16:37,773
with very fast interconnect
kind of all in one place.

363
00:16:38,910 --> 00:16:40,710
We may discuss this later,

364
00:16:40,710 --> 00:16:43,920
but you have a broad
selection of instance types

365
00:16:43,920 --> 00:16:46,710
and it can be tempting to
hyper optimize your machines

366
00:16:46,710 --> 00:16:48,990
to your workloads because
maybe you have many different

367
00:16:48,990 --> 00:16:50,307
workloads and you benchmark them all

368
00:16:50,307 --> 00:16:52,290
and one is faster on one machine

369
00:16:52,290 --> 00:16:54,240
and one is faster on another machine,

370
00:16:54,240 --> 00:16:56,310
but you run a bit the
same fragmentation risk

371
00:16:56,310 --> 00:16:57,990
trying to get greedy and get capacity

372
00:16:57,990 --> 00:16:59,640
wherever you can get it right away.

373
00:16:59,640 --> 00:17:04,500
If you try and get a
super optimized machine

374
00:17:04,500 --> 00:17:07,110
for each workload, you may
not have consistent traffic

375
00:17:07,110 --> 00:17:09,930
to keep all your machines
busy all the time.

376
00:17:09,930 --> 00:17:13,110
And so I tend to toward what
I call workhorse machines,

377
00:17:13,110 --> 00:17:14,880
machines that are fast enough

378
00:17:14,880 --> 00:17:16,980
and you have a lot of the same thing

379
00:17:16,980 --> 00:17:18,720
that they can handle most of your traffic

380
00:17:18,720 --> 00:17:21,480
and remain consistently busy.

381
00:17:21,480 --> 00:17:23,040
And so if you manage this,

382
00:17:23,040 --> 00:17:26,130
you make a choice, maybe one
or two accelerated instances,

383
00:17:26,130 --> 00:17:28,440
and all in one place,
there are a lot of ways

384
00:17:28,440 --> 00:17:29,820
to manage capacity,

385
00:17:29,820 --> 00:17:31,830
but I think it's a fairly
safe recommendation

386
00:17:31,830 --> 00:17:33,600
that at this scale you
should think about running

387
00:17:33,600 --> 00:17:37,230
your own clusters and that you
should do so with Kubernetes.

388
00:17:37,230 --> 00:17:39,420
I think it's also fairly safe to say

389
00:17:39,420 --> 00:17:43,215
that for the majority of use
cases there's no major downside

390
00:17:43,215 --> 00:17:46,710
to using a managed
Kubernetes service like EKS.

391
00:17:46,710 --> 00:17:48,120
I think there are exceptions to that

392
00:17:48,120 --> 00:17:50,640
but you kind of have to
a reason not to use it

393
00:17:50,640 --> 00:17:52,953
as opposed to a reason to justify it.

394
00:17:54,300 --> 00:17:57,210
And so this will simplify
your cluster management a lot

395
00:17:57,210 --> 00:17:59,580
and I think there are some good practices

396
00:17:59,580 --> 00:18:00,930
about how to set up your cluster

397
00:18:00,930 --> 00:18:04,740
that we'll look at in a moment
with an example diagram.

398
00:18:04,740 --> 00:18:06,990
But you should kind of
standardize your cluster.

399
00:18:06,990 --> 00:18:09,840
All your machines should be
on the same versions of things

400
00:18:09,840 --> 00:18:11,940
so all your jobs can
run on all your machines

401
00:18:11,940 --> 00:18:15,090
and tend to have the same
dependencies and same failures.

402
00:18:15,090 --> 00:18:18,990
And you should do something
to deal with GPU failures,

403
00:18:18,990 --> 00:18:20,790
which we'll talk about more as well.

404
00:18:23,340 --> 00:18:27,420
So here's the diagram I promised
and it's just an example.

405
00:18:27,420 --> 00:18:29,100
You may have a slightly different setup,

406
00:18:29,100 --> 00:18:30,060
but there are a couple of things

407
00:18:30,060 --> 00:18:31,800
that I want to draw your attention to.

408
00:18:31,800 --> 00:18:33,330
The first thing is that you have somehow

409
00:18:33,330 --> 00:18:35,130
got your machines all in one place.

410
00:18:35,130 --> 00:18:37,380
And so there is only one
account in this diagram,

411
00:18:37,380 --> 00:18:38,730
there's only one cluster

412
00:18:38,730 --> 00:18:42,390
and all the machines are in one
cluster, which is very nice.

413
00:18:42,390 --> 00:18:43,620
The second thing to notice

414
00:18:43,620 --> 00:18:45,840
is that there's a mysterious scheduling

415
00:18:45,840 --> 00:18:49,290
or orchestration component
implied in the cluster

416
00:18:49,290 --> 00:18:52,380
other than the built-in
Kubernetes scheduler.

417
00:18:52,380 --> 00:18:54,240
And again, there's a lot of selection

418
00:18:54,240 --> 00:18:56,640
and we'll look at some
options for this in a moment.

419
00:18:56,640 --> 00:18:58,230
But especially for batch computing,

420
00:18:58,230 --> 00:19:00,480
I think it's broadly
accepted that Kubernetes

421
00:19:00,480 --> 00:19:03,270
is not really going to have
all the scheduling properties

422
00:19:03,270 --> 00:19:04,890
that you want for AI workloads

423
00:19:04,890 --> 00:19:06,660
and you're probably
going to end up layering

424
00:19:06,660 --> 00:19:10,020
something on top to manage
the way your workloads

425
00:19:10,020 --> 00:19:11,523
are scheduled to machines.

426
00:19:12,540 --> 00:19:15,360
The third thing to
notice is the node pools.

427
00:19:15,360 --> 00:19:16,650
This is deliberately vague.

428
00:19:16,650 --> 00:19:18,240
It could be autoscaling groups,

429
00:19:18,240 --> 00:19:21,000
it could be managed node
groups of some kind,

430
00:19:21,000 --> 00:19:22,320
but they're divided up.

431
00:19:22,320 --> 00:19:25,590
The machines are not all
in one giant pool somehow.

432
00:19:25,590 --> 00:19:28,140
So probably in Kubernetes
they've got different labels

433
00:19:28,140 --> 00:19:30,510
or taints and workloads
are kind of pointed

434
00:19:30,510 --> 00:19:32,910
at one node pool or another.

435
00:19:32,910 --> 00:19:35,010
There's some CPU only capacity in there.

436
00:19:35,010 --> 00:19:36,420
Don't forget to budget for that.

437
00:19:36,420 --> 00:19:37,980
You will probably need it.

438
00:19:37,980 --> 00:19:39,810
There's a couple different types of GPU,

439
00:19:39,810 --> 00:19:41,130
they're in different node pools

440
00:19:41,130 --> 00:19:43,410
with maybe different driver versions.

441
00:19:43,410 --> 00:19:44,820
And you can see

442
00:19:44,820 --> 00:19:47,730
that in this scenario there's
been more than one GPU

443
00:19:47,730 --> 00:19:49,860
procured for the batch use case.

444
00:19:49,860 --> 00:19:51,540
There's some P5s and P6Cs

445
00:19:51,540 --> 00:19:52,830
and those are in different node pools

446
00:19:52,830 --> 00:19:54,150
and kept logically separate.

447
00:19:54,150 --> 00:19:55,500
Workloads are either going to one

448
00:19:55,500 --> 00:19:57,120
or they're going to the other.

449
00:19:57,120 --> 00:19:59,400
And if you do end up combining some

450
00:19:59,400 --> 00:20:01,500
kind of realtime inference use case

451
00:20:01,500 --> 00:20:03,360
with offline batch processing,

452
00:20:03,360 --> 00:20:06,240
which there can also be reasons not to do,

453
00:20:06,240 --> 00:20:08,250
but if you do do that,
I think at least have

454
00:20:08,250 --> 00:20:11,490
some kind of logical barrier
between the realtime capacity

455
00:20:11,490 --> 00:20:13,413
and the offline processing capacity.

456
00:20:14,550 --> 00:20:16,320
And finally, I'm not
gonna make a sales pitch

457
00:20:16,320 --> 00:20:18,750
for any particular storage technology,

458
00:20:18,750 --> 00:20:20,220
but planning out your storage hierarchy

459
00:20:20,220 --> 00:20:21,480
is gonna be a really important part

460
00:20:21,480 --> 00:20:23,520
of planning your cluster.

461
00:20:23,520 --> 00:20:26,280
It does help that the newer
GPU instances increasingly

462
00:20:26,280 --> 00:20:30,660
come with a lot of built-in
SSD in the form of NVME,

463
00:20:30,660 --> 00:20:33,720
especially P5 and definitely P6e.

464
00:20:33,720 --> 00:20:36,000
And so that is your first bet

465
00:20:36,000 --> 00:20:39,420
and best bet for storage
during things like training.

466
00:20:39,420 --> 00:20:41,250
It's very fast and it is very free

467
00:20:41,250 --> 00:20:43,140
and it is built into the machines.

468
00:20:43,140 --> 00:20:46,260
But inevitably that is not a
durable place to store anything

469
00:20:46,260 --> 00:20:47,880
that you care about long term.

470
00:20:47,880 --> 00:20:51,270
And so probably there's some
other storage in the picture

471
00:20:51,270 --> 00:20:54,180
that's keeping longer term
records of training data

472
00:20:54,180 --> 00:20:57,180
and checkpoints or data that
your workloads are processing

473
00:20:57,180 --> 00:20:59,370
while they run in the cluster.

474
00:20:59,370 --> 00:21:01,950
And so finally it is just
a bit implied by this

475
00:21:01,950 --> 00:21:04,860
that you need to budget
for things other than GPUs.

476
00:21:04,860 --> 00:21:07,500
So I pointed out there's some
CPU only machines in there,

477
00:21:07,500 --> 00:21:08,730
so do not forget these.

478
00:21:08,730 --> 00:21:11,850
There's some storage, do not
forget to budget for storage.

479
00:21:11,850 --> 00:21:13,920
And then finally these
things talk to each other.

480
00:21:13,920 --> 00:21:16,650
And so there is definitely
some networking traffic

481
00:21:16,650 --> 00:21:19,860
likely to be happening here
unless all of your data

482
00:21:19,860 --> 00:21:21,960
is inside the UltraCluster never leaving,

483
00:21:21,960 --> 00:21:24,060
probably there's some data flow in and out

484
00:21:24,060 --> 00:21:27,003
and so don't forget to plan
for and budget for networking.

485
00:21:30,450 --> 00:21:32,580
So I mentioned scheduling
and orchestration.

486
00:21:32,580 --> 00:21:34,710
You probably need to make some choices

487
00:21:34,710 --> 00:21:36,270
beyond just turning up Kubernetes

488
00:21:36,270 --> 00:21:37,770
and deciding that you're done.

489
00:21:38,850 --> 00:21:43,380
For batch jobs, there's a
longer list of quality tools

490
00:21:43,380 --> 00:21:44,213
than I can list.

491
00:21:44,213 --> 00:21:47,010
And so I don't want to sell
you on any particular one

492
00:21:47,010 --> 00:21:48,330
but I think anyone who tells you

493
00:21:48,330 --> 00:21:50,220
that there is a perfect scheduler

494
00:21:50,220 --> 00:21:52,563
for all workloads is a
little bit suspicious.

495
00:21:53,400 --> 00:21:57,240
You should really evaluate a
range for what you're doing.

496
00:21:57,240 --> 00:21:59,460
And if you're looking at
hosting serving workloads,

497
00:21:59,460 --> 00:22:01,890
you're probably looking at
a different set of tools

498
00:22:01,890 --> 00:22:05,250
than someone who's doing
mostly batch processing.

499
00:22:05,250 --> 00:22:08,460
And so when you're thinking
through the options,

500
00:22:08,460 --> 00:22:09,293
especially at this scale,

501
00:22:09,293 --> 00:22:12,060
1,000 GPUs is not as much as it sounds.

502
00:22:12,060 --> 00:22:15,300
And if the size of your
large jobs is anywhere

503
00:22:15,300 --> 00:22:17,880
within an order of magnitude
of the size of your cluster,

504
00:22:17,880 --> 00:22:18,990
if you do not have something

505
00:22:18,990 --> 00:22:21,570
that does gang scheduling
on top of Kubernetes,

506
00:22:21,570 --> 00:22:23,430
your jobs are gonna deadlock very fast.

507
00:22:23,430 --> 00:22:25,260
And you'll have two or
three that are half started

508
00:22:25,260 --> 00:22:28,020
and nothing goes and the
whole system locks up.

509
00:22:28,020 --> 00:22:30,510
And so you definitely cannot
ignore this at this scale

510
00:22:30,510 --> 00:22:31,923
for this type of workload.

511
00:22:33,570 --> 00:22:34,680
In addition to scheduling,

512
00:22:34,680 --> 00:22:37,170
you need to worry a little
bit about resource management.

513
00:22:37,170 --> 00:22:39,390
You are definitely going to
have a range of workloads

514
00:22:39,390 --> 00:22:42,150
and probably a range of tenants
fighting over this capacity.

515
00:22:42,150 --> 00:22:44,760
And all you can really do is
have some kind of a policy

516
00:22:44,760 --> 00:22:46,320
that everyone has agreed to

517
00:22:46,320 --> 00:22:48,600
that the system enforces on their behalf.

518
00:22:48,600 --> 00:22:50,610
And whatever tool that ends up making

519
00:22:50,610 --> 00:22:52,410
your scheduling decisions probably also

520
00:22:52,410 --> 00:22:53,243
has something to say

521
00:22:53,243 --> 00:22:55,050
about your resource management decisions.

522
00:22:55,050 --> 00:22:56,880
And so you need to look at your interest

523
00:22:56,880 --> 00:22:59,970
in both of those properties
when you choose that tool.

524
00:22:59,970 --> 00:23:02,940
And then finally, most of
these things are fairly solid

525
00:23:02,940 --> 00:23:06,000
but you should look not just in principle

526
00:23:06,000 --> 00:23:06,990
how those scheduling works

527
00:23:06,990 --> 00:23:08,970
and what promises it makes in the docks,

528
00:23:08,970 --> 00:23:10,530
but also make sure that it can handle

529
00:23:10,530 --> 00:23:11,970
the load in the traffic

530
00:23:11,970 --> 00:23:14,610
that you expect in the
system when it's live.

531
00:23:14,610 --> 00:23:16,620
'Cause the scheduling will
be a single point of failure

532
00:23:16,620 --> 00:23:18,090
for your system.

533
00:23:18,090 --> 00:23:21,077
And then I will give a
special shout out to GB200.

534
00:23:21,077 --> 00:23:24,210
And so if you're dealing
with any rack scale capacity

535
00:23:24,210 --> 00:23:27,420
like GB200 or upcoming GB300,

536
00:23:27,420 --> 00:23:31,560
at this point you really cannot
avoid dealing with topology

537
00:23:31,560 --> 00:23:32,760
in your choice of scheduler.

538
00:23:32,760 --> 00:23:36,000
Most likely some workloads
have some constraints,

539
00:23:36,000 --> 00:23:38,760
maybe multi-node workload
needs to go to the same rack

540
00:23:38,760 --> 00:23:41,700
or big training workload needs
to use several whole racks

541
00:23:41,700 --> 00:23:44,430
in training as you also need

542
00:23:44,430 --> 00:23:46,080
to choose a scheduling component

543
00:23:46,080 --> 00:23:48,033
that can express that the way you want.

544
00:23:51,180 --> 00:23:54,270
Finally, observability, if
you have spent all this money

545
00:23:54,270 --> 00:23:56,040
and bought all these GPUs, you better know

546
00:23:56,040 --> 00:23:57,780
what is going on with them.

547
00:23:57,780 --> 00:23:59,550
And this is partially for your own sanity

548
00:23:59,550 --> 00:24:01,590
so you can tell if the system is working.

549
00:24:01,590 --> 00:24:03,240
Is partially for your users sanity

550
00:24:03,240 --> 00:24:04,680
so they can tell if it's working

551
00:24:04,680 --> 00:24:06,870
and they can also tell how busy it is.

552
00:24:06,870 --> 00:24:09,300
And finally it is also
for what I will refer to

553
00:24:09,300 --> 00:24:11,460
as the powers that be people

554
00:24:11,460 --> 00:24:14,010
who maybe have a financial
interest in what you are doing

555
00:24:14,010 --> 00:24:16,170
with all the money and all of the GPUs.

556
00:24:16,170 --> 00:24:17,550
And by the time they come asking

557
00:24:17,550 --> 00:24:18,870
what your utilization looks like,

558
00:24:18,870 --> 00:24:21,000
ideally you should already know.

559
00:24:21,000 --> 00:24:24,360
And so these are kind of made up diagrams

560
00:24:24,360 --> 00:24:26,550
just to give a sense of things.

561
00:24:26,550 --> 00:24:27,660
But there are a couple of things I think

562
00:24:27,660 --> 00:24:29,670
you should really monitor in real time

563
00:24:29,670 --> 00:24:31,020
and for long periods of time

564
00:24:31,020 --> 00:24:33,150
so that you always have this information.

565
00:24:33,150 --> 00:24:35,100
You should know how much
of your capacity appears

566
00:24:35,100 --> 00:24:38,850
to be working and you should
know how much of it is used

567
00:24:38,850 --> 00:24:40,500
by end user workloads,

568
00:24:40,500 --> 00:24:43,080
what kind of scheduling
utilization you're getting.

569
00:24:43,080 --> 00:24:44,580
And then depending on your use case

570
00:24:44,580 --> 00:24:47,700
you should have some notion
of live traffic to the system.

571
00:24:47,700 --> 00:24:49,830
The graph shown is for
something like a batch system

572
00:24:49,830 --> 00:24:52,050
that has queue jobs and running jobs.

573
00:24:52,050 --> 00:24:54,450
And this also is very
entertaining to people

574
00:24:54,450 --> 00:24:56,070
who wanna know why their job is queued

575
00:24:56,070 --> 00:24:57,843
and whether the queue is very long.

576
00:24:59,250 --> 00:25:00,720
Over longer periods of time,

577
00:25:00,720 --> 00:25:02,580
not necessarily in real time,

578
00:25:02,580 --> 00:25:05,250
but maybe on a daily or weekly basis,

579
00:25:05,250 --> 00:25:07,710
I really recommended the
scale having some notion

580
00:25:07,710 --> 00:25:10,050
of how responsible your
end users are being.

581
00:25:10,050 --> 00:25:12,900
And so some notion of workload efficiency

582
00:25:12,900 --> 00:25:15,600
that catches if workloads
are requesting GPUs

583
00:25:15,600 --> 00:25:16,650
that they never use

584
00:25:16,650 --> 00:25:19,050
or running with a really
low average utilization

585
00:25:19,050 --> 00:25:22,440
or have really wonky
resource configurations

586
00:25:22,440 --> 00:25:23,970
is a great idea.

587
00:25:23,970 --> 00:25:26,460
We'll let you catch
these things out quicker

588
00:25:26,460 --> 00:25:28,620
before they tank your overall metrics.

589
00:25:28,620 --> 00:25:31,440
And then finally you should
know how the computing time

590
00:25:31,440 --> 00:25:34,260
or the cost in the system is being spent

591
00:25:34,260 --> 00:25:36,570
between your tenants or your use cases

592
00:25:36,570 --> 00:25:40,143
and have some kind of
monitoring or show backs.

593
00:25:42,960 --> 00:25:44,910
And with this I'm gonna
pass it off to Henri,

594
00:25:44,910 --> 00:25:47,010
who's going to talk about
his lived experience

595
00:25:47,010 --> 00:25:47,883
in this domain.

596
00:25:50,370 --> 00:25:53,453
(audience clapping)

597
00:26:01,530 --> 00:26:02,940
- Hi, my name's Henri

598
00:26:02,940 --> 00:26:07,560
and I wish I had heard
Andrea's talk a few years ago

599
00:26:07,560 --> 00:26:09,750
because I was the target audience.

600
00:26:09,750 --> 00:26:12,390
So what I'm gonna do is
walk you through some of our

601
00:26:12,390 --> 00:26:15,660
sort of learned experience
scaling AI compute

602
00:26:15,660 --> 00:26:17,550
for lab in the loop.

603
00:26:17,550 --> 00:26:20,040
And I'll start

604
00:26:20,040 --> 00:26:23,340
by giving you a brief overview
of what I'm talking about.

605
00:26:23,340 --> 00:26:27,720
So I work in AI for drug discovery

606
00:26:27,720 --> 00:26:30,720
and what we do is make
machine learning models

607
00:26:30,720 --> 00:26:33,330
to design molecules.

608
00:26:33,330 --> 00:26:35,940
And when we talk about lab
in the loop, what you mean

609
00:26:35,940 --> 00:26:40,440
is essentially you start
with some experimental data

610
00:26:40,440 --> 00:26:44,070
and train machine learning
models on it, use those models

611
00:26:44,070 --> 00:26:48,000
to come up with new potential
molecules in our case

612
00:26:48,000 --> 00:26:51,180
and then make those run
experiments, get new data

613
00:26:51,180 --> 00:26:53,190
that hopefully feeds back into the models

614
00:26:53,190 --> 00:26:57,660
and eventually by iterating
you reach the level of quality

615
00:26:57,660 --> 00:26:59,220
of your molecules that you're hoping for.

616
00:26:59,220 --> 00:27:01,230
Or at least that's the dream.

617
00:27:01,230 --> 00:27:04,930
And on the right, this
is just an example of

618
00:27:05,940 --> 00:27:09,150
one class of molecules that we
worked on called antibodies.

619
00:27:09,150 --> 00:27:11,460
We were optimizing across expression,

620
00:27:11,460 --> 00:27:13,770
how easy they are to make in Affinity

621
00:27:13,770 --> 00:27:16,380
and sort of you can see
over subsequent rounds

622
00:27:16,380 --> 00:27:19,170
we're kind of pushing the
Pareto frontier forward,

623
00:27:19,170 --> 00:27:22,980
which is what you're hoping
to do in this kind of process.

624
00:27:22,980 --> 00:27:27,980
And so in order for this to work,

625
00:27:28,200 --> 00:27:29,790
one of the things that you need

626
00:27:29,790 --> 00:27:31,590
is good machine learning models.

627
00:27:31,590 --> 00:27:34,950
And nowadays in order to
have state-of-the-art models,

628
00:27:34,950 --> 00:27:39,950
what that means is training
large models on multiple nodes.

629
00:27:40,380 --> 00:27:45,380
And in particular I'll focus
on one of the kinds of models

630
00:27:45,540 --> 00:27:48,540
that we've been working
on which are these LLMs

631
00:27:48,540 --> 00:27:52,440
in the like 70 billion parameter range.

632
00:27:52,440 --> 00:27:55,020
I guess those still count as large

633
00:27:55,020 --> 00:27:57,510
and very briefly on our infrastructure.

634
00:27:57,510 --> 00:28:01,920
So we have our GPUs on parallel cluster,

635
00:28:01,920 --> 00:28:03,810
which is one of the schedulers

636
00:28:03,810 --> 00:28:05,070
that Andrea was talking about.

637
00:28:05,070 --> 00:28:08,160
And most of our long-term
data storage is an S3.

638
00:28:08,160 --> 00:28:13,160
And so by scaling up our
model size and compute,

639
00:28:13,620 --> 00:28:16,830
we ran into all of the
following challenges.

640
00:28:16,830 --> 00:28:19,770
The first one are hardware faults.

641
00:28:19,770 --> 00:28:23,640
When you're working on one
machine, one node, a few GPUs,

642
00:28:23,640 --> 00:28:25,050
typically you don't run into this

643
00:28:25,050 --> 00:28:27,540
but at scale they're inevitable.

644
00:28:27,540 --> 00:28:29,580
I think we ran into these

645
00:28:29,580 --> 00:28:33,600
as soon as we were scaling
it across tens of nodes.

646
00:28:33,600 --> 00:28:35,823
And these are typically seen as,

647
00:28:37,350 --> 00:28:41,583
you know, the training job
crashing with unclear reasons.

648
00:28:42,540 --> 00:28:46,440
And so in order to sort of solve this,

649
00:28:46,440 --> 00:28:49,350
what we did was test the nodes.

650
00:28:49,350 --> 00:28:51,300
There are a variety of
different ways you can do this.

651
00:28:51,300 --> 00:28:54,660
In the end we sort of run a
very simple PyTorch script

652
00:28:54,660 --> 00:28:56,400
on a new node, make sure that it's able

653
00:28:56,400 --> 00:28:59,160
to use the GPUs, load data on the GPU

654
00:28:59,160 --> 00:29:02,553
and run simple calculations
and then remove bad ones.

655
00:29:04,290 --> 00:29:06,210
A trick here that took us a little while

656
00:29:06,210 --> 00:29:07,410
to figure out is just

657
00:29:07,410 --> 00:29:08,790
because you evict a node,

658
00:29:08,790 --> 00:29:10,770
you might actually just get it back,

659
00:29:10,770 --> 00:29:12,720
especially when you start worrying

660
00:29:12,720 --> 00:29:15,540
about challenge number
two, which is networking.

661
00:29:15,540 --> 00:29:20,280
So with networking these
are typically really hard

662
00:29:20,280 --> 00:29:22,680
to debug errors.

663
00:29:22,680 --> 00:29:26,440
You don't often have clear reasons why

664
00:29:27,990 --> 00:29:30,930
some networking error happens
or can see it in the logs.

665
00:29:30,930 --> 00:29:32,730
We usually saw these as Nickel timeouts

666
00:29:32,730 --> 00:29:35,460
and what we found was really helpful

667
00:29:35,460 --> 00:29:37,380
is really try to avoid hops.

668
00:29:37,380 --> 00:29:40,980
So if you can avoid cross zone traffic

669
00:29:40,980 --> 00:29:44,130
or even cross spine networking

670
00:29:44,130 --> 00:29:47,820
you can get much better stability

671
00:29:47,820 --> 00:29:49,923
and also better performance.

672
00:29:51,330 --> 00:29:54,000
But in one spine there's
only so much capacity,

673
00:29:54,000 --> 00:29:56,370
that's why this issue of
getting back the same node

674
00:29:56,370 --> 00:29:57,203
can happen.

675
00:29:58,620 --> 00:30:01,710
And the final challenge is
around the model implementation

676
00:30:01,710 --> 00:30:03,000
and this is something

677
00:30:03,000 --> 00:30:05,163
that's really fully under your control.

678
00:30:09,210 --> 00:30:12,000
One strategy that really
helped us was carefully testing

679
00:30:12,000 --> 00:30:13,320
the various libraries used.

680
00:30:13,320 --> 00:30:18,240
So CUDA your EFA kernel
if you're using, you know,

681
00:30:18,240 --> 00:30:21,990
PE instances as well as, you know,

682
00:30:21,990 --> 00:30:24,210
Nickel, OFI, plugins and things like that.

683
00:30:24,210 --> 00:30:25,830
All of these are really brittle

684
00:30:25,830 --> 00:30:28,050
and making sure that you
test all of the versions

685
00:30:28,050 --> 00:30:29,280
that you're gonna run together

686
00:30:29,280 --> 00:30:32,280
and sort of upgrade them
altogether will lead

687
00:30:32,280 --> 00:30:35,310
to tremendous improvements
in your stability.

688
00:30:35,310 --> 00:30:37,260
On the data loading side,

689
00:30:37,260 --> 00:30:40,470
the common challenge is really
feeding data fast enough

690
00:30:40,470 --> 00:30:43,620
to the GPUs so that you can
get that good utilization

691
00:30:43,620 --> 00:30:46,440
and this is sort of the
most common source of bugs

692
00:30:46,440 --> 00:30:48,810
that I see when machine
learning scientists come to me

693
00:30:48,810 --> 00:30:51,033
with bad GPU efficiency.

694
00:30:52,020 --> 00:30:54,060
This is also sort of a Python problem

695
00:30:54,060 --> 00:30:57,960
because in order to load
data quickly you need

696
00:30:57,960 --> 00:31:00,660
to have multiple threads,
multiple processes,

697
00:31:00,660 --> 00:31:02,940
this is notoriously finicky in Python.

698
00:31:02,940 --> 00:31:06,510
And spending a lot of time
stress testing your data loaders

699
00:31:06,510 --> 00:31:07,803
is really useful.

700
00:31:10,500 --> 00:31:11,820
On the checkpointing side,

701
00:31:11,820 --> 00:31:14,397
no matter what you do there
will be failures for your models

702
00:31:14,397 --> 00:31:17,160
and so being able to checkpoint

703
00:31:17,160 --> 00:31:20,070
and resume is critical

704
00:31:20,070 --> 00:31:23,220
and that will really help you save

705
00:31:23,220 --> 00:31:24,393
sort of wasted compute.

706
00:31:25,770 --> 00:31:29,190
And lastly we found that as you

707
00:31:29,190 --> 00:31:33,330
start training larger models
moving away from abstractions

708
00:31:33,330 --> 00:31:35,790
and libraries that sort of wrap PyTorch,

709
00:31:35,790 --> 00:31:38,340
to sort of native
PyTorch helped us improve

710
00:31:38,340 --> 00:31:40,890
the efficiency and the stability.

711
00:31:40,890 --> 00:31:43,383
And so putting these in practice,

712
00:31:46,080 --> 00:31:48,690
here's some charts of our GPU utilization.

713
00:31:48,690 --> 00:31:51,330
So if you don't have the pleasure

714
00:31:51,330 --> 00:31:54,423
in your day-to-day job of staring
at GPU utilization charts,

715
00:31:55,290 --> 00:31:56,550
I feel sorry for you, they're pretty fun.

716
00:31:56,550 --> 00:31:59,140
But also what you're
looking at here essentially

717
00:32:00,232 --> 00:32:03,873
on the Y axis is the percent average.

718
00:32:05,348 --> 00:32:06,780
GPU utilization...

719
00:32:06,780 --> 00:32:08,910
Is everyone okay there?

720
00:32:08,910 --> 00:32:10,480
Average GP utilization percent

721
00:32:11,400 --> 00:32:14,340
for a LLM training job.

722
00:32:14,340 --> 00:32:18,540
And you can see a couple issues
with the one on the left.

723
00:32:18,540 --> 00:32:21,690
The first one is sort
of low GP utilization.

724
00:32:21,690 --> 00:32:25,470
We're around mid 60 to a mid 60%.

725
00:32:25,470 --> 00:32:27,900
That means you know if you
are running on 100 GPUs

726
00:32:27,900 --> 00:32:32,900
sort of 40 or 35 of them are
not being utilized on average.

727
00:32:34,860 --> 00:32:36,360
The other issue we see here are these

728
00:32:36,360 --> 00:32:40,230
sort of pauses every
couple tens of minutes

729
00:32:40,230 --> 00:32:42,510
where the GPU utilization drops to zero

730
00:32:42,510 --> 00:32:45,570
and this is often a symptom
of sort of checkpointing

731
00:32:45,570 --> 00:32:48,510
where everything is
stopping, stop the world,

732
00:32:48,510 --> 00:32:52,413
you're writing to disc all of
the weights and then resuming.

733
00:32:54,540 --> 00:32:56,340
And the other thing we can see here is

734
00:32:56,340 --> 00:32:59,250
sort of relatively short training runs

735
00:32:59,250 --> 00:33:01,050
on the order of several hours.

736
00:33:01,050 --> 00:33:03,300
This is because we had
some stability issues.

737
00:33:04,200 --> 00:33:07,390
And so by implementing
some of the improvements

738
00:33:08,700 --> 00:33:10,290
and addressing some of
the challenges I mentioned

739
00:33:10,290 --> 00:33:12,870
in the previous slide,
distributed checkpointing,

740
00:33:12,870 --> 00:33:14,820
really investing in our data loading,

741
00:33:14,820 --> 00:33:17,490
and dropping down to fully
sharded data parallel,

742
00:33:17,490 --> 00:33:22,490
we're able to really bring up
our GP utilization to sort of

743
00:33:22,590 --> 00:33:26,280
where you want to be, you know mid 90%.

744
00:33:26,280 --> 00:33:28,560
This is the chart I wanna
show to the powers that be

745
00:33:28,560 --> 00:33:31,510
how good we are at using our
GPUs and not the previous one.

746
00:33:33,060 --> 00:33:37,590
Of course using your GPUs is not enough,

747
00:33:37,590 --> 00:33:40,500
it's really important to use them in ways

748
00:33:40,500 --> 00:33:42,120
that are effective.

749
00:33:42,120 --> 00:33:47,120
And so for us we run
varying types of compute

750
00:33:47,250 --> 00:33:49,440
but I showed here two major ones.

751
00:33:49,440 --> 00:33:52,680
So on the left it's
molecular dynamic simulation,

752
00:33:52,680 --> 00:33:57,420
how long it takes to generate
a one nanosecond trajectory.

753
00:33:57,420 --> 00:34:01,410
And here less is better so
you want to take less time

754
00:34:01,410 --> 00:34:03,630
to generate that one nanosecond.

755
00:34:03,630 --> 00:34:05,460
And what you can see
it's kind of interesting,

756
00:34:05,460 --> 00:34:09,030
an L40S takes about as long as a B200

757
00:34:09,030 --> 00:34:11,673
to run this compute, this is just one GPU.

758
00:34:12,810 --> 00:34:16,170
Which, you know, L40S is relatively old

759
00:34:16,170 --> 00:34:18,960
or very old I guess in GPU generation time

760
00:34:18,960 --> 00:34:21,630
it's three years old versus the B200

761
00:34:21,630 --> 00:34:23,313
which is pretty recent.

762
00:34:25,050 --> 00:34:27,270
And so that means, you know, if you have

763
00:34:27,270 --> 00:34:30,032
a lot of specific kinds of workloads

764
00:34:30,032 --> 00:34:31,380
you should really benchmark them

765
00:34:31,380 --> 00:34:34,560
to see if it's worth considering
having dedicated compute

766
00:34:34,560 --> 00:34:37,710
and tuning the architecture for that.

767
00:34:37,710 --> 00:34:40,860
On the LLM training side, this is sort of

768
00:34:40,860 --> 00:34:45,150
what you see in all of
the releases of new chips.

769
00:34:45,150 --> 00:34:48,090
We saw very similar results

770
00:34:48,090 --> 00:34:49,830
where you get more tokens per second

771
00:34:49,830 --> 00:34:52,323
which is good on newer GPUs.

772
00:34:56,940 --> 00:35:00,240
Yeah so tying it all together, you know,

773
00:35:00,240 --> 00:35:03,010
thanks to some of our efforts in

774
00:35:05,610 --> 00:35:08,550
optimizing our LLM training,

775
00:35:08,550 --> 00:35:10,800
really now we're working on

776
00:35:10,800 --> 00:35:13,920
having intention aware
agents that are trained

777
00:35:13,920 --> 00:35:15,450
on all of our internal data

778
00:35:15,450 --> 00:35:17,880
so they understand drug discovery,

779
00:35:17,880 --> 00:35:19,590
they understand our models

780
00:35:19,590 --> 00:35:23,820
and what we're working towards
is having these agents,

781
00:35:23,820 --> 00:35:27,510
you know, be able to
translate some requirements.

782
00:35:27,510 --> 00:35:30,300
So in this case the example
I'm giving you is this phase

783
00:35:30,300 --> 00:35:32,430
in drug discovery that's
called lead optimization

784
00:35:32,430 --> 00:35:33,450
where you have a molecule

785
00:35:33,450 --> 00:35:36,210
and you're trying to
improve certain properties.

786
00:35:36,210 --> 00:35:38,977
And so the idea is you ask your agent,

787
00:35:38,977 --> 00:35:41,760
"Can you improve these properties?"

788
00:35:41,760 --> 00:35:45,630
And the agent can translate
that into a workflow,

789
00:35:45,630 --> 00:35:48,150
you know, run these generative
models that we have,

790
00:35:48,150 --> 00:35:50,970
these property prediction models
that we have that are able

791
00:35:50,970 --> 00:35:55,110
to identify specific aspects
of the molecule and then rank

792
00:35:55,110 --> 00:35:58,650
and filter those, maybe run
through a few loops of that

793
00:35:58,650 --> 00:36:02,130
and ultimately then we can
send that for synthesis

794
00:36:02,130 --> 00:36:04,440
to the wet lab and look at the predicted

795
00:36:04,440 --> 00:36:06,240
and real output and sort of compare.

796
00:36:06,240 --> 00:36:09,840
And so in order for this
agentic loop to work,

797
00:36:09,840 --> 00:36:12,660
what you need is to have
all of your models deployed.

798
00:36:12,660 --> 00:36:16,380
So we use Kubernetes to run those

799
00:36:16,380 --> 00:36:19,440
and sort of some agentic
framework to tie it together.

800
00:36:19,440 --> 00:36:22,020
And that's how we've really been working

801
00:36:22,020 --> 00:36:24,570
on speeding drug development process

802
00:36:24,570 --> 00:36:27,840
and trying to improve the
success rate of our R and D.

803
00:36:27,840 --> 00:36:32,490
Yeah and with that I'll
hand it off to Shaunak

804
00:36:32,490 --> 00:36:35,073
for more information about agents.

805
00:36:36,818 --> 00:36:39,901
(audience clapping)

806
00:36:45,330 --> 00:36:46,367
- Thank you, Henri.

807
00:36:47,250 --> 00:36:49,740
So as Henri was describing,

808
00:36:49,740 --> 00:36:54,740
the most successful agents that are built

809
00:36:54,900 --> 00:36:57,630
are built using models

810
00:36:57,630 --> 00:37:01,680
that are trained on the data
that your enterprises have.

811
00:37:01,680 --> 00:37:03,870
And that's what I'm going
to talk a little bit about.

812
00:37:03,870 --> 00:37:08,040
We call it artificial
autonomous intelligence,

813
00:37:08,040 --> 00:37:11,250
which basically essentially
means you own your data

814
00:37:11,250 --> 00:37:14,130
so you should essentially own your models

815
00:37:14,130 --> 00:37:16,702
and own your deployments as well.

816
00:37:16,702 --> 00:37:20,430
I'm Shaunak, I'm the
field CTO at Fireworks

817
00:37:20,430 --> 00:37:23,610
we specialize in inferencing

818
00:37:23,610 --> 00:37:28,413
as well as training for
a lot of open models.

819
00:37:29,490 --> 00:37:34,170
So I think 2025 is the year of agents

820
00:37:34,170 --> 00:37:36,393
I should say or the year of agents.

821
00:37:37,530 --> 00:37:39,840
I think almost all of you must have used

822
00:37:39,840 --> 00:37:44,650
one of these agents, specifically
Cursor or Sourcegraph.

823
00:37:44,650 --> 00:37:46,650
They're all are in like the coding space,

824
00:37:46,650 --> 00:37:47,730
you have a bunch of them

825
00:37:47,730 --> 00:37:50,280
in like the document processing space.

826
00:37:50,280 --> 00:37:53,160
You're seeing a lot of
like enterprises show up

827
00:37:53,160 --> 00:37:57,000
that are doing legal processing,
healthcare processing,

828
00:37:57,000 --> 00:37:59,400
FSNI and those types of stuff.

829
00:37:59,400 --> 00:38:02,850
Everybody's building one
or these types of agents.

830
00:38:02,850 --> 00:38:06,720
And so the agents on the
top are some of the ones

831
00:38:06,720 --> 00:38:09,510
that have like really picked up this year

832
00:38:09,510 --> 00:38:11,340
but we feel and we have observed

833
00:38:11,340 --> 00:38:13,200
that there are many, many more

834
00:38:13,200 --> 00:38:16,050
that are actually being built out

835
00:38:16,050 --> 00:38:18,510
and we feel that these
are the agents that will

836
00:38:18,510 --> 00:38:22,053
sort of be more pervasive
this year as well.

837
00:38:24,270 --> 00:38:26,880
So when you think about like agents,

838
00:38:26,880 --> 00:38:31,880
I think there are two main
principles that we think about

839
00:38:32,340 --> 00:38:35,310
and both of these principles
are based on the fact

840
00:38:35,310 --> 00:38:40,290
that your data is the most
precious commodity that you have

841
00:38:40,290 --> 00:38:43,860
and the models that are
generally available do not align

842
00:38:43,860 --> 00:38:47,160
with the data that exists
in your enterprises.

843
00:38:47,160 --> 00:38:50,190
Especially because they have
been trained on public data

844
00:38:50,190 --> 00:38:52,080
or they have been trained on data

845
00:38:52,080 --> 00:38:56,190
that is synthetically
generated by a few companies.

846
00:38:56,190 --> 00:38:59,730
And so what we have realized
is the first principle

847
00:38:59,730 --> 00:39:03,360
is don't treat your model as a commodity.

848
00:39:03,360 --> 00:39:04,830
Your model is your product.

849
00:39:04,830 --> 00:39:08,850
It needs to be built using data flywheel.

850
00:39:08,850 --> 00:39:10,950
I think Henri gave a great example of

851
00:39:10,950 --> 00:39:15,630
how they built out their
models by iteratively training

852
00:39:15,630 --> 00:39:18,660
and are like constantly
improving the model

853
00:39:18,660 --> 00:39:20,703
using the data flywheel that they have.

854
00:39:21,600 --> 00:39:24,630
And then the second one
is, in the same lines,

855
00:39:24,630 --> 00:39:26,880
your model is your IP, right?

856
00:39:26,880 --> 00:39:30,750
Your model and your product
needs to work together

857
00:39:30,750 --> 00:39:34,590
to provide your users with
the best user experience

858
00:39:34,590 --> 00:39:36,600
that they can have.

859
00:39:36,600 --> 00:39:40,980
So the overall principles,
your model and your product

860
00:39:40,980 --> 00:39:43,800
should coexist and your model is your IP.

861
00:39:43,800 --> 00:39:45,513
Do not treat it as a commodity.

862
00:39:48,150 --> 00:39:51,720
Now when you think about
models, when you think about

863
00:39:51,720 --> 00:39:53,730
this infrastructure,
when you think about like

864
00:39:53,730 --> 00:39:57,090
where LLMs are, there are so many choices.

865
00:39:57,090 --> 00:39:58,560
So I'll give you a little bit of overview

866
00:39:58,560 --> 00:40:00,330
over the next couple of slides

867
00:40:00,330 --> 00:40:03,330
about the different types
of choices you can make,

868
00:40:03,330 --> 00:40:05,580
whether they are regards to models

869
00:40:05,580 --> 00:40:07,880
or whether they are
regards to infrastructure.

870
00:40:09,030 --> 00:40:12,330
So models now come in
different types of modalities.

871
00:40:12,330 --> 00:40:16,440
You have text, you have image,
you have image generation,

872
00:40:16,440 --> 00:40:21,030
you have video generation,
you have embeddings et cetera.

873
00:40:21,030 --> 00:40:23,550
You have different types
of model providers.

874
00:40:23,550 --> 00:40:26,790
All of them have like
different expertise, right?

875
00:40:26,790 --> 00:40:31,790
So you have model, sorry, you
have Amazon, you have Meta,

876
00:40:32,070 --> 00:40:35,820
you have DeepSeek, you have
Qwen, Qwen's from Alibaba

877
00:40:35,820 --> 00:40:39,480
and they have all been
showing signs of catching up

878
00:40:39,480 --> 00:40:42,120
with the proprietary

879
00:40:42,120 --> 00:40:45,960
or different tier models
that you see today.

880
00:40:45,960 --> 00:40:48,423
And of course like there
are lots of model sizes,

881
00:40:49,470 --> 00:40:52,140
I'm specifically talking here about LLMs

882
00:40:52,140 --> 00:40:53,610
or large language models.

883
00:40:53,610 --> 00:40:56,750
So they range anywhere from
1B, even smaller for some

884
00:40:56,750 --> 00:40:59,820
of the embedding models,
all the way to 1 trillion,

885
00:40:59,820 --> 00:41:00,840
which is the latest model

886
00:41:00,840 --> 00:41:03,750
that you saw from this
company called Kimi,

887
00:41:03,750 --> 00:41:06,213
which is roughly a
trillion parameters long.

888
00:41:07,750 --> 00:41:09,660
After you choose the model,

889
00:41:09,660 --> 00:41:11,610
the next one is model alignment

890
00:41:11,610 --> 00:41:13,350
based on the data that you have.

891
00:41:13,350 --> 00:41:15,573
Again, many, many choices here.

892
00:41:16,440 --> 00:41:18,540
The first one is supervise fine tuning

893
00:41:18,540 --> 00:41:21,030
where you give your models
a few set of examples

894
00:41:21,030 --> 00:41:22,680
to sort of align the model.

895
00:41:22,680 --> 00:41:25,530
Then you have DPO where
you give it positive

896
00:41:25,530 --> 00:41:26,880
and negative samples.

897
00:41:26,880 --> 00:41:28,770
And then you have
reinforcement fine tuning

898
00:41:28,770 --> 00:41:31,170
where you have a reward
function that tries

899
00:41:31,170 --> 00:41:34,980
to align your model based
on the data that you have.

900
00:41:34,980 --> 00:41:37,140
The last one is sort of the one

901
00:41:37,140 --> 00:41:39,900
that now almost all the frontier labs

902
00:41:39,900 --> 00:41:43,050
and these open model
companies actually use

903
00:41:43,050 --> 00:41:46,500
to improve the quality of the
model because it's cheaper

904
00:41:46,500 --> 00:41:48,780
and it gives you like really good results.

905
00:41:48,780 --> 00:41:50,700
And then the last one is

906
00:41:50,700 --> 00:41:54,060
talking a little bit about
serving those models.

907
00:41:54,060 --> 00:41:57,120
So we talk about like the
inference infrastructure.

908
00:41:57,120 --> 00:41:59,250
There are lots of trade offs here.

909
00:41:59,250 --> 00:42:01,020
Typically you have three things,

910
00:42:01,020 --> 00:42:03,420
your inference can be better,

911
00:42:03,420 --> 00:42:05,880
it can be faster, it can be cheaper.

912
00:42:05,880 --> 00:42:08,890
And most of the companies
have like one choice

913
00:42:09,750 --> 00:42:11,070
or two choices.

914
00:42:11,070 --> 00:42:14,880
And we can also talk about
a little bit after this

915
00:42:14,880 --> 00:42:16,530
that can you actually make trade off

916
00:42:16,530 --> 00:42:17,973
on all the three choices?

917
00:42:19,350 --> 00:42:22,380
Then you have a security
constraint, right?

918
00:42:22,380 --> 00:42:25,530
Whether this model and
deployments should be SaaS based,

919
00:42:25,530 --> 00:42:28,050
should they run in your own private PPC?

920
00:42:28,050 --> 00:42:29,730
Should they run in on-prem?

921
00:42:29,730 --> 00:42:32,580
Do you have legal approvals to use models?

922
00:42:32,580 --> 00:42:34,920
Are these models properly
penetrated, tested,

923
00:42:34,920 --> 00:42:36,093
and so on and so forth.

924
00:42:38,040 --> 00:42:41,430
So with that, let's move to the next one

925
00:42:41,430 --> 00:42:42,990
which is I'll talk a little bit about like

926
00:42:42,990 --> 00:42:46,770
the inference infrastructure
Andrea covered

927
00:42:46,770 --> 00:42:48,960
that you have for
inference infrastructure.

928
00:42:48,960 --> 00:42:53,850
You have different types of
serving stack for bad jobs,

929
00:42:53,850 --> 00:42:57,780
different types of serving
stack for real time jobs.

930
00:42:57,780 --> 00:43:00,660
And so the next few slides
we'll talk a little bit

931
00:43:00,660 --> 00:43:04,920
about when you have your own deployments,

932
00:43:04,920 --> 00:43:07,710
how do you think about
which things to use?

933
00:43:07,710 --> 00:43:09,870
So the first one is pick the hardware.

934
00:43:09,870 --> 00:43:12,330
There are so many of them, right?

935
00:43:12,330 --> 00:43:16,770
There's accelerated compute,
you have AWS, you have NVIDIA

936
00:43:16,770 --> 00:43:20,820
providing you GPUs, you
have what we call here

937
00:43:20,820 --> 00:43:24,270
Ampere series, Hopper
series, Blackwell series.

938
00:43:24,270 --> 00:43:27,303
And as of today Trainium
has Trainium 3 as well.

939
00:43:28,907 --> 00:43:30,420
All of these hardwares have like

940
00:43:30,420 --> 00:43:32,370
different trade offs to make.

941
00:43:32,370 --> 00:43:36,063
I think Aniruddha covered a
lot of them as part of a stock.

942
00:43:36,982 --> 00:43:38,760
Some have huge amounts of FLOPS,

943
00:43:38,760 --> 00:43:41,910
some have lots of memory per GPU.

944
00:43:41,910 --> 00:43:46,910
Some have really good inter
GPU connects et cetera.

945
00:43:48,630 --> 00:43:50,490
So once you sort of pick the hardware,

946
00:43:50,490 --> 00:43:52,230
you pick the hardware provider

947
00:43:52,230 --> 00:43:53,850
and then you have to start thinking about

948
00:43:53,850 --> 00:43:55,503
optimizing your inference engine.

949
00:43:57,060 --> 00:44:00,360
You typically have few
choices that have listed out,

950
00:44:00,360 --> 00:44:04,260
these are the most common
ones, but you have many more.

951
00:44:04,260 --> 00:44:06,720
You typically think about model charting.

952
00:44:06,720 --> 00:44:08,070
If your model is very large

953
00:44:08,070 --> 00:44:09,417
it might not fit on a single GPU.

954
00:44:09,417 --> 00:44:12,397
You have to shard it across different GPU.

955
00:44:12,397 --> 00:44:14,520
You have disaggregated serving.

956
00:44:14,520 --> 00:44:19,140
This is where you separate out
the two phases of generation,

957
00:44:19,140 --> 00:44:23,313
which is the prefill
and the generation step.

958
00:44:24,210 --> 00:44:27,810
This type of techniques
are very, very common

959
00:44:27,810 --> 00:44:30,120
especially in agent TKI systems

960
00:44:30,120 --> 00:44:32,850
where your inputs are extremely long

961
00:44:32,850 --> 00:44:37,113
but your outputs are structured
and very short JSON outputs.

962
00:44:38,580 --> 00:44:42,270
Then you think about which
inference kernel to use.

963
00:44:42,270 --> 00:44:43,800
There are many of them.

964
00:44:43,800 --> 00:44:46,650
I think TRT-LLM has done a great job

965
00:44:46,650 --> 00:44:48,990
or NVIDA has done a great
job of like providing

966
00:44:48,990 --> 00:44:50,703
a bunch of like inference kernels.

967
00:44:51,599 --> 00:44:55,200
vLLM, SGLang have all created

968
00:44:55,200 --> 00:44:57,600
like Flash Infer, Flash
Attention, et cetera,

969
00:44:57,600 --> 00:44:59,250
which are like doing really well.

970
00:45:00,120 --> 00:45:02,400
Then you think about cross serving.

971
00:45:02,400 --> 00:45:05,130
So what if your model is so large

972
00:45:05,130 --> 00:45:08,520
that it doesn't fit on
a single GPU machine?

973
00:45:08,520 --> 00:45:10,530
Then you actually have
to shard the model across

974
00:45:10,530 --> 00:45:12,270
like different types of hosts.

975
00:45:12,270 --> 00:45:14,610
When you do that, you need like really

976
00:45:14,610 --> 00:45:16,740
good communication to happen.

977
00:45:16,740 --> 00:45:20,790
So Amazon here does a great
job of providing us with EFA

978
00:45:20,790 --> 00:45:23,760
where the speed of transfer

979
00:45:23,760 --> 00:45:26,070
of KV cache from like
one machine to another

980
00:45:26,070 --> 00:45:27,330
is like really, really quick.

981
00:45:27,330 --> 00:45:29,970
And that essentially helps
with faster generation,

982
00:45:29,970 --> 00:45:32,220
faster throughputs.

983
00:45:32,220 --> 00:45:35,340
Speculative decoding is one
of the most commonly used

984
00:45:35,340 --> 00:45:40,340
techniques that allows you to
train a very, very small model

985
00:45:41,010 --> 00:45:42,060
that just speculates

986
00:45:42,060 --> 00:45:45,120
and the larger model
instead of generating,

987
00:45:45,120 --> 00:45:48,270
just realizes or just
checks whether the token

988
00:45:48,270 --> 00:45:50,470
that's generated is
actually correct or not.

989
00:45:51,660 --> 00:45:54,990
So once you've finalized the
inferencing engine options,

990
00:45:54,990 --> 00:45:58,440
you then have to move to serving stack.

991
00:45:58,440 --> 00:46:03,180
Andrea already covered a lot
of this in her Kubernetes talk

992
00:46:03,180 --> 00:46:05,340
around like horizontal scaling,

993
00:46:05,340 --> 00:46:07,050
but I'll talk a little bit more deeply

994
00:46:07,050 --> 00:46:10,290
about specifics to inference.

995
00:46:10,290 --> 00:46:11,850
So you have prompt caching,

996
00:46:11,850 --> 00:46:14,160
which is one of the most
common techniques now

997
00:46:14,160 --> 00:46:16,650
that's used if you have
like the same system prompts

998
00:46:16,650 --> 00:46:21,060
that get used, what kind of system

999
00:46:21,060 --> 00:46:24,150
that is provided by like the
inference engine, et cetera.

1000
00:46:24,150 --> 00:46:25,770
So you can use like prompt caching.

1001
00:46:25,770 --> 00:46:27,363
Second one is session affinity.

1002
00:46:28,230 --> 00:46:32,340
Very, very useful if you
have coding types of agents

1003
00:46:32,340 --> 00:46:34,080
that you're building or that you're using

1004
00:46:34,080 --> 00:46:37,593
so that the same request
goes to the same machine.

1005
00:46:38,970 --> 00:46:42,450
In these types of cases,
the reliability of machines

1006
00:46:42,450 --> 00:46:44,430
is extremely, extremely important.

1007
00:46:44,430 --> 00:46:47,010
And so using something like EKS

1008
00:46:47,010 --> 00:46:48,423
that's actually managed.

1009
00:46:49,667 --> 00:46:53,190
Using Amazon, which is extremely reliable

1010
00:46:53,190 --> 00:46:55,410
and extremely available is super important

1011
00:46:55,410 --> 00:46:59,040
because then the failures
around session affinity

1012
00:46:59,040 --> 00:47:03,090
or your prompt dropping
reduces significantly.

1013
00:47:03,090 --> 00:47:05,670
Then you have request
failover, load shedding,

1014
00:47:05,670 --> 00:47:08,070
these are typically the cases when you

1015
00:47:08,070 --> 00:47:12,370
sort of get like spikes
of traffic or you have

1016
00:47:13,380 --> 00:47:15,930
or you want to fail over
to like another region.

1017
00:47:15,930 --> 00:47:19,290
So once you have picked
out your serving stack,

1018
00:47:19,290 --> 00:47:23,850
you then have to move to
global secure serving, right?

1019
00:47:23,850 --> 00:47:26,610
So Andrea talked a
little bit about what if,

1020
00:47:26,610 --> 00:47:30,480
try to get your GPUs in one
region as much as possible,

1021
00:47:30,480 --> 00:47:32,520
but sometimes it is not possible

1022
00:47:32,520 --> 00:47:35,130
or sometimes you might need

1023
00:47:35,130 --> 00:47:36,510
to have cross region traffic

1024
00:47:36,510 --> 00:47:38,790
or you might need to have regionality

1025
00:47:38,790 --> 00:47:41,880
and so you use global
secure serving in that case.

1026
00:47:41,880 --> 00:47:45,390
You need to have some
sort of global scheduling.

1027
00:47:45,390 --> 00:47:50,390
You need to have private
PPCs for secure serving.

1028
00:47:50,520 --> 00:47:53,340
You might even want to do
things like Private-Link

1029
00:47:53,340 --> 00:47:56,100
so that your data never
leaves your VPC boundary.

1030
00:47:56,100 --> 00:47:57,510
And all of these are

1031
00:47:57,510 --> 00:48:01,680
provided by Amazon's ELB systems

1032
00:48:01,680 --> 00:48:03,840
or through Amazon's Private-Link.

1033
00:48:03,840 --> 00:48:08,070
And that's where we are
seeing a lot of prevalence

1034
00:48:08,070 --> 00:48:11,103
of secure and compliant infrastructure.

1035
00:48:13,620 --> 00:48:15,630
So that was a very high level of like

1036
00:48:15,630 --> 00:48:17,790
what the inference stacks are.

1037
00:48:17,790 --> 00:48:20,880
So this is how we actually
built our inference stack,

1038
00:48:20,880 --> 00:48:22,593
taking all of these into account.

1039
00:48:23,850 --> 00:48:27,540
And so today Fireworks serves about,

1040
00:48:27,540 --> 00:48:30,960
I think this is a little old,
maybe like a few weeks old,

1041
00:48:30,960 --> 00:48:35,070
about 150,000 requests per second.

1042
00:48:35,070 --> 00:48:39,720
We process more than 13
trillion tokens a day.

1043
00:48:39,720 --> 00:48:43,710
We have bunch of models
across different modalities

1044
00:48:43,710 --> 00:48:47,010
and we host on private secure cloud.

1045
00:48:47,010 --> 00:48:48,720
And we have a booth.

1046
00:48:48,720 --> 00:48:50,223
Our booth number is 1588.

1047
00:48:51,210 --> 00:48:53,160
So please come and talk to us

1048
00:48:53,160 --> 00:48:55,620
if you're interested in
learning about any of this.

1049
00:48:55,620 --> 00:48:56,453
Thank you.

1050
00:48:57,384 --> 00:49:00,120
(audience clapping)

1051
00:49:00,120 --> 00:49:02,340
- [Aniruddha] Please, please
do prepare a lot of questions.

1052
00:49:02,340 --> 00:49:06,660
We really hope you guys ask
the best questions of all.

1053
00:49:06,660 --> 00:49:09,270
Please do fill out a survey from your app.

1054
00:49:09,270 --> 00:49:12,360
This really helps us raise
the bar at future events

1055
00:49:12,360 --> 00:49:14,910
and the more number of
people sign up the survey,

1056
00:49:14,910 --> 00:49:19,910
the better data we get in terms
of figuring out what worked,

1057
00:49:20,400 --> 00:49:23,190
what didn't work, and
what needs to improve.

1058
00:49:23,190 --> 00:49:27,543
So any questions, any burning
questions on how to scale AI?

