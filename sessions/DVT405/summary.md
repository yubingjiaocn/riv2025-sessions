# AWS re:Invent 2025 编码代理演进总结

## 会议概述

本次技术分享由AWS的两位首席科学家Joavanni Zapella和Loranka Lo主讲，他们专注于Amazon CodeWhisperer（Kiro）的开发工作，特别是自主代理的研究。演讲回顾了过去两年中编码代理的演进历程，从简单的代码补全工具发展到能够自主完成复杂编程任务的智能代理系统。

演讲者详细介绍了他们团队在构建编码代理过程中的技术迭代，从最初基于RAG（检索增强生成）的简单架构，逐步演进到固定工作流、灵活的代理循环、支持代码执行的Logos代理，最终发展为具有监督者-子代理架构的Hong系统。每一次架构升级都带来了显著的性能提升，在SWBench基准测试中的表现从最初的1-2%提升到最终的55%以上。

演讲的核心目标不是教授如何快速实现一个代理，而是分享实战经验和架构演进的思路，为开发者提供构建自己代理系统的灵感和参考。演讲者还强调了三个关键经验：针对特定用例优化、构建可靠系统、以及保持架构的可演进性。

## 详细时间线与关键要点

### 开场介绍 (0:00-2:30)
- **演讲者介绍**：Joavanni Zapella和Loranka Lo，均为AWS首席科学家，专注于CodeWhisperer和自主代理开发
- **演讲目标**：回顾编码代理从代码补全到自主架构的演进历程

### 编码工具的历史演进 (2:30-5:00)
- **早期IDE**：基础语法高亮，简单的自动补全功能，需要手动记忆语法细节
- **10年后的进步**：强大的IDE能够补全变量名、方法名等，但仍主要是加速输入而非代替编码
- **现代代理**：如Kiro CLI，能够根据自然语言描述自主与文件系统交互并完成代码修改

### 代理的两大类型 (5:00-8:00)
- **同步体验代理**：作为开发者的交互式伴侣，加速任务完成（如Kiro CLI）
- **异步体验代理**：开发者可以委托任务，代理自主工作直到完成目标
- **工作模式对比**：同步代理加速顺序工作流，异步代理支持任务并行化和委托

### 任务结构分析 (8:00-10:00)
- **任务定义阶段**：开发者需要明确定义任务目标（同步）
- **自主执行阶段**：代理独立工作并创建Pull Request（异步）
- **代码审查阶段**：开发者审查代理生成的代码（同步）
- **迭代改进阶段**：代理根据反馈自主迭代优化（异步）

### SWBench基准测试介绍 (10:00-12:30)
- **基准测试来源**：从GitHub真实问题中提取，包含人类开发者的解决方案和单元测试
- **评估方法**：移除人类编写的代码，让代理重新实现，通过单元测试验证正确性
- **局限性**：单元测试可能无法覆盖所有情况，但与代码质量高度相关

### 第一阶段：RAG架构的挑战 (12:30-15:00)
- **初始方案**：基于RAG的检索器，检索相关代码文件并传递给LLM生成修改
- **主要问题**：
  - 难以生成正确的补丁
  - 测试通过率低
  - 检索器召回率仅50%，意味着只能找到一半需要修改的文件
- **根本挑战**：从简单基准（单个函数）转向真实代码库（数千个文件）

### 第二阶段：固定工作流架构 (15:00-18:00)
- **架构设计**：
  1. 高召回率检索大量潜在相关文件
  2. 扩展文件内容（类名、方法名等）
  3. 第二轮检索过滤不相关文件
  4. 选择需要修改的代码块并重写
  5. 生成最终diff
- **性能提升**：仅使用4次LLM调用，性能提升约10倍，登顶SWBench排行榜
- **局限性**：工作流固定，缺乏灵活性，无法适应不同类型的任务

### 第三阶段：TextCode代理 (18:00-22:00)
- **核心改进**：
  - 创建专门的工具集和环境，支持代理式交互
  - 引入代理循环（agentic loop），LLM持续迭代并选择下一步工具
  - 提供工作空间（workspace）存储关键信息
- **工具设计原则**：
  - 人类工具是视觉化的，代理工具需要基于文本
  - 输出简洁明了，避免无用信息干扰
  - 保持任务相关信息始终可用
- **性能飞跃**：
  - SWBench Full从13%提升到19%
  - SWBench Verified从25%提升到38%，首次登顶验证集排行榜

### 灵活性验证：文档生成案例 (22:00-24:00)
- **实验设计**：使用相同的工具、工作空间和模型，仅修改指令专注于README文件生成
- **对比结果**：通用代理架构的表现匹配甚至超越专门优化的文档生成工作流
- **意义**：证明了灵活架构能够支持多种用例而无需重新设计

### 代码语义理解的瓶颈 (24:00-26:30)
- **核心问题**：模型难以理解代码的语义含义
- **人类vs模型**：人类在脑海中"执行"代码理解其含义，而LLM只看到文本
- **自我纠错困难**：即使有反馈，LLM也难以有效改进错误代码
- **测试生成挑战**：早期模型无法有效生成测试用例

### 第四阶段：Logos代理与代码执行 (26:30-30:00)
- **关键突破**：模型测试生成能力提升，使代码执行成为可能
- **架构创新**：
  - 沙箱环境安全执行代码和测试
  - 代码编写代理在隔离环境中工作
  - 并行运行多个代理实例（如3个）
  - 回归验证组件运行单元测试检测问题
  - LLM算法选择最佳补丁
- **工程挑战**：
  - 需要安全隔离的执行环境
  - 支持大规模并发执行（数千个代理）
  - 防止随机模型行为造成安全风险

### Logos性能提升 (30:00-32:00)
- **单次生成性能**：从38.8%提升到51%
- **多补丁选择**：生成3个补丁并选择最佳，再提升4个百分点至55%
- **扩展能力**：支持生成10个、25个甚至更多补丁，通过测试时计算资源提升性能
- **核心优势**：能够评估同一问题的多个解决方案并选择最优

### 推理能力的引入 (32:00-34:00)
- **模型进化**：新一代LLM的"思考"和"推理"能力显著提升
- **专用工具**：创建专门工具让代理充分利用推理能力
- **应用场景**：特别适合需要深度思考的任务，如bug调试
- **性能提升**：推理工具带来约4-5个百分点的性能提升

### 长任务与上下文挑战 (34:00-36:00)
- **问题识别**：
  - 上下文窗口限制
  - 任务灵活性仍不足
  - 无效步骤占用上下文（如10次bug复现尝试，只有最后一次成功）
- **隔离需求**：不同类型的子任务（如代码修改vs配置部署）需要独立处理，避免相互干扰

### 第五阶段：Hong监督者-子代理架构 (36:00-42:00)
- **架构设计**：
  - **监督者代理**：完整工具集，能读取文件、创建计划、管理子代理
  - **子代理**：类似"超级编辑器"，接收指令完成子目标
- **实际案例：Flask网站投票功能**
  - 任务：为餐厅详情网站添加密码保护的投票路由
  - 监督者探索代码库（使用find、ls、grep等命令行工具）
  - 识别相关模板和文件
  - 生成详细计划：
    1. 在app.py添加认证和/votes路由
    2. 创建vote.html模板
    3. 创建登录页面HTML
  - 为子代理生成详细指令（非用户提供，而是监督者生成）
  - 子代理执行具体实现
- **最终成果**：完整的密码保护投票页面

### 架构演进总结 (42:00-44:00)
- **RAG阶段**：检索器 + 1次LLM调用
- **固定工作流**：4次LLM调用，性能提升10倍
- **TextCode代理**：引入代理循环，模型选择下一步行动，更好的工具和环境
- **Logos代理**：代码执行能力，多补丁生成与选择，推理工具
- **Hong架构**：监督者-子代理模式，支持长时间运行和复杂任务

### 经验教训一：针对用例优化 (44:00-47:00)
- **设计选择**：
  - **重复性/明确任务**：使用简单工作流+LLM，兼顾LLM能力和确定性工作流的可预测性与速度
  - **交互式任务**：优先考虑低延迟设计，控制工具使用频率和速度
  - **复杂自主任务**：最大化代理能力，支持创建子代理、丰富工具箱、验证和反馈机制
- **指标设计**：创建真实反映客户体验的评估指标

### SWBench的局限性分析 (47:00-50:00)
- **任务复杂度差距**：
  - 基准测试：通常修改1-2个文件
  - 实际客户需求：需要修改3-10个文件
- **问题描述差异**：
  - 基准测试：超过1000字符的详细描述，期望明确
  - 实际使用：少于100字符的简短模糊描述，需要逐步细化
- **影响**：过度依赖基准测试会误判代理实际表现

### PolyBench新基准测试 (50:00-52:00)
- **改进特性**：
  - 支持4种编程语言（vs SWBench的单一语言）
  - 更难的任务，包含验证子集确保可解性
  - 更多任务类型：bug修复、新功能、代码重构等
- **丰富指标**：
  - 不仅评估任务完成度
  - 评估文件检索准确性
  - 评估函数定位正确性
  - 评估检索效率（避免过度检索）

### 经验教训二：构建可靠系统 (52:00-56:00)
- **随机性挑战**：
  - LLM是随机的，相同任务产生不同结果
  - 多样性是优势（支持多解决方案生成），但增加评估难度
  - 需要大量测试和统计学方法验证改进
- **测试复杂度**：
  - 复杂任务运行时间长（30分钟-1小时）
  - 需要数千次测试 × 每天多次运行
  - 要求极其稳定的系统基础设施
- **轨迹复杂性**：
  - 代理执行路径包含数十到数百个步骤
  - 故障归因困难：模型会自动寻找工具失败的替代方案
  - 无法通过人工检查理解，需要专门的可观测性工具

### 技术栈介绍 (56:00-58:00)
- **Strand Agent SDK**：
  - Apache 2.0开源Python SDK
  - 现已支持TypeScript
  - 已在生产环境大规模部署
  - 提供构建强大代理的工具和资源
- **Amazon Bedrock Agent Core**：
  - 提供构建、部署和观测代理的全套工具
  - 支持获取代理行为指标
  - 支持轨迹查看和分析
  - 帮助理解和改进代理性能

### 经验教训三：保持演进能力 (58:00-61:00)
- **客户使用的不可预测性**：
  - 需要观测实际使用模式
  - 理解代理在真实场景中的表现
  - 随时准备调整架构以改进用户体验
- **新工具的影响**：
  - 沙箱环境和代码执行能力从根本上改变了架构设计
  - 从静态读写工作流转向动态执行架构
- **模型快速迭代**：
  - 每几周甚至几天就有新基础模型发布
  - 新模型带来新能力，使某些架构组件过时
  - 例如：专用代码导航工具曾是关键，现在模型直接使用bash等标准工具更有效
- **小模型的崛起**：
  - 更小、更快的模型变得具有竞争力
  - 更低成本和延迟开启新应用场景
  - 支持设备端运行和更简单任务的代理化

### 总结 (61:00-62:00)
- 感谢听众参与
- 强调持续演进和适应变化的重要性