# AWS re:Invent 2025 会议总结：Pinterest 如何利用 Apache Iceberg 扩展 PB 级数据湖

## 会议概述

本次会议由 AWS 首席解决方案架构师 Sachin Holler 和 Pinterest 首席工程师 Ashi Singh 共同主讲，重点介绍了 Pinterest 如何利用 Apache Iceberg 表格式在 Amazon S3 上管理和扩展其 PB 级数据湖，为 6 亿月活跃用户提供服务。

Pinterest 面临着巨大的数据规模挑战：500 PB 的数据湖、10 万张表、超过 2 万个 Spark 节点和 1000 多个 Trino 节点，每天运行约 40 万个计算作业。传统的 Hive 表格式已无法满足新兴用例的需求，特别是在用户数据删除、表采样和特征回填等场景中遇到了严重的性能和成本问题。从 2022 年开始构建，2023 年正式投产后，Pinterest 已将 15,000 张表（约 200 PB 数据）迁移到 Iceberg，表数量年增长率超过 300%。

通过采用 Iceberg 及其快照隔离、分桶优化和存储分区连接等特性，Pinterest 实现了显著的业务成果：用户数据删除能力提升 10 倍、存储成本降低 30%、计算成本降低 30%、可靠性提升 90%，以及机器学习特征开发速度提升 90 倍。会议还分享了在 Amazon S3 上大规模运行 Iceberg 的关键经验，包括基于用户代理的访问控制、利用 S3 请求日志和清单报告进行监控，以及通过哈希路径解决限流问题。

## 详细时间线

### 开场介绍 (0:00-1:30)
- **0:00** - Sachin Holler（AWS 首席解决方案架构师）开场，介绍会议主题
- **0:30** - 介绍演讲嘉宾 Ashi Singh（Pinterest 首席工程师）
- **1:00** - 会议聚焦于 Pinterest 如何为 6 亿用户提供服务，管理 PB 级数据湖

### Pinterest 规模介绍 (1:30-3:30)
- **1:30** - Ashi Singh 接手演讲，介绍会议议程
- **2:00** - Pinterest 业务规模：6 亿月活跃用户，每周保存 15 亿个 Pin，超过 100 亿个看板
- **2:30** - 数据规模：500 PB 数据湖（Amazon S3），10 万张表（Hive 和 Iceberg）
- **3:00** - 计算资源：超过 2 万个 Spark 节点，1000 多个 Trino 节点，每天运行 40 万个计算作业

### 表格式演进历程 (3:30-5:00)
- **3:30** - Pinterest 表格式演进与行业趋势一致
- **4:00** - 2019 年前：Hive 表是事实标准的数据湖表格式
- **4:20** - 2020 年：行业和 Pinterest 开始意识到 Hive 无法满足新用例需求
- **4:40** - 行业出现替代方案：Hoodie、Delta Lake、Apache Iceberg
- **4:50** - 2022 年：Pinterest 开始构建 Iceberg 解决方案（经过两年的评估和审批）

### Iceberg 采用现状 (5:00-6:30)
- **5:00** - 2023 年：首个用例（用户数据删除）投入生产
- **5:30** - 当前规模：约 15,000 张 Iceberg 表，200 PB 数据
- **6:00** - 增长数据：表数量年增长率超过 300%，数据量增长较慢（通过更好的压缩实现）
- **6:20** - 支持的引擎：Trino、Spark、Flink、Python、MapReduce 框架

### 用例一：用户数据删除 (6:30-11:00)
- **6:30** - 用户数据删除是首个也是关键用例
- **7:00** - Hive 方案的问题：需要重写整个表，成本高、耗时长、可靠性差
- **7:30** - 并发读写问题：文件变更导致下游作业失败
- **8:00** - Iceberg 初步方案：仅重写包含待删除用户数据的文件，利用快照隔离
- **8:30** - 遇到的问题：批量删除时用户数据分散在所有文件中，成本仍然很高
- **9:00** - 优化方案：按删除键排序数据，减少需要重写的文件数量
- **9:30** - 进一步优化：仅对未排序的文件进行排序（需要修改 Iceberg 和 Spark）
- **10:30** - 最终成果：删除能力提升 10 倍，存储成本降低 30%，计算成本降低 30%，可靠性提升 90%

### 用例二：表采样 (11:00-13:30)
- **11:00** - 表采样用于数据探索，但需要可重现性
- **11:30** - 挑战：多表连接时需要确保采样数据的键一致性
- **12:00** - 解决方案：基于分桶的采样（Bucket-based sampling）
- **12:30** - 实现方式：对 Iceberg 表进行分桶，按桶进行采样而非随机采样
- **13:00** - 优势：保证多表采样的键一致性和可重现性
- **13:20** - 成果：开发效率提升 90%，与全表扫描的偏差小于 1%

### 用例三：特征回填 (13:30-18:00)
- **13:30** - Pinterest 使命：激励用户策划他们热爱的生活
- **14:00** - 依赖先进的推荐和广告模型，基于数十 PB 数据和数月的用户行为日志训练
- **14:30** - 模型使用广泛的特征：Pin、用户、广告主和会话级特征
- **15:00** - 传统方法：前向日志记录（Forward logging），需等待 3-6 个月数据积累，在生产环境中实验
- **15:30** - 特征回填方法：反事实计算历史特征值并与生产数据连接
- **16:00** - 优势：无需等待数据积累，隔离生产和实验环境
- **16:30** - 挑战：PB 级数据连接的 Shuffle 成本极高
- **17:00** - 解决方案：使用 Iceberg 分桶连接（Bucket join / Storage partition join）避免昂贵的 Shuffle
- **17:30** - 成果：大型连接节省 65% 成本，特征开发速度提升 90 倍
- **17:50** - 技术实现：在 Trino 上添加支持，从 Spark 3.5 回溯到 3.2，支持 Ray 直接读取分桶

### S3 运维经验一：基于用户代理的访问控制 (18:00-20:30)
- **18:00** - 挑战：200 PB 数据无法进行完整的数据重写迁移
- **18:30** - 解决方案：就地迁移（In-place migration）- 在 Hive 表上构建快照后原子性切换到 Iceberg
- **19:00** - 风险：Hive 中可以直接写入分区文件，但 Iceberg 需要通过快照管理
- **19:30** - 防护措施：使用基于用户代理的访问控制防止意外访问
- **20:00** - 实现方式：修改所有 Iceberg 客户端和目录在用户代理中添加标识信息，在 S3 桶级别设置策略
- **20:20** - 目的：防止意外访问而非恶意访问

### S3 运维经验二：S3 请求日志和清单报告 (20:30-22:30)
- **20:30** - Iceberg 特性：每次写入生成新快照，保留旧数据以支持并发读取
- **21:00** - 问题：容易产生孤立文件（Orphan files）或未引用文件
- **21:30** - 解决方案：使用 S3 清单报告识别孤立文件
- **22:00** - 方法：对比 S3 清单与 Iceberg 元数据，找出未引用文件并删除
- **22:20** - 另一用途：使用 S3 请求日志识别非预期的访问模式，发现未按预期方式使用 Iceberg 的用户

### S3 运维经验三：限流优化 (22:30-25:30)
- **22:30** - 限流问题：Iceberg 承诺解决此问题，但就地迁移保留了旧的数据组织方式
- **23:00** - Hive 路径结构问题：表名/分区名/分区值的长字符串缺乏熵值
- **23:30** - S3 建议：在对象路径中尽早引入高熵值
- **24:00** - 问题表现：大型数据集并发写入时遇到 503 错误，浪费计算资源和时间
- **24:30** - 解决方案：迁移完成后修改对象路径决策方式，尽早引入熵值
- **25:00** - 最新进展：使用 Amazon 贡献的 20 位哈希路径写入对象
- **25:20** - 成果：在前 10 大数据集的 66% 上部署后，503 错误投诉完全消失

### 结束 (25:30-26:00)
- **25:30** - Ashi Singh 总结发言
- **25:50** - 感谢听众参与