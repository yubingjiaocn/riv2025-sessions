1
00:00:01,440 --> 00:00:02,850
- Hi, good morning.

2
00:00:02,850 --> 00:00:04,053
How's everyone today?

3
00:00:05,580 --> 00:00:07,020
Good?

4
00:00:07,020 --> 00:00:08,190
Excellent.

5
00:00:08,190 --> 00:00:12,000
So first of all, thank you
so much for joining us today.

6
00:00:12,000 --> 00:00:13,410
Before we start the session,

7
00:00:13,410 --> 00:00:15,670
think about that you are a chef

8
00:00:16,860 --> 00:00:19,920
and you want to prepare
a dish for your friend

9
00:00:19,920 --> 00:00:21,220
because it's his birthday.

10
00:00:22,260 --> 00:00:25,500
As a chef, you have the end goal in mind,

11
00:00:25,500 --> 00:00:28,710
which is the dish that your friend likes.

12
00:00:28,710 --> 00:00:30,690
But through the course of preparation,

13
00:00:30,690 --> 00:00:33,390
there are so many steps involved.

14
00:00:33,390 --> 00:00:37,710
You have to get good-quality
food, the spices,

15
00:00:37,710 --> 00:00:39,750
the temperature needs to be right,

16
00:00:39,750 --> 00:00:43,140
and you have to put the
ingredients in the right order.

17
00:00:43,140 --> 00:00:46,650
And finally, there is a
dish that your friend likes.

18
00:00:46,650 --> 00:00:48,690
So when we think about data engineering,

19
00:00:48,690 --> 00:00:50,670
it's nothing different.

20
00:00:50,670 --> 00:00:54,570
You have the end goal in
mind to build a scalable,

21
00:00:54,570 --> 00:00:58,860
reliable data pipeline with quality

22
00:00:58,860 --> 00:01:01,770
that serves your business use case

23
00:01:01,770 --> 00:01:04,503
so you can make the
decision at the right time.

24
00:01:05,670 --> 00:01:09,900
But with data pipeline, there
are so many steps involved.

25
00:01:09,900 --> 00:01:14,310
You need to catalog the data,
you need to profile the data,

26
00:01:14,310 --> 00:01:16,110
and then you have to build the jobs,

27
00:01:16,110 --> 00:01:18,000
you have to orchestrate it,

28
00:01:18,000 --> 00:01:20,220
you have to deploy it in production,

29
00:01:20,220 --> 00:01:22,020
and then provide it to the end users

30
00:01:22,020 --> 00:01:24,630
for reporting or to serve them.*

31
00:01:24,630 --> 00:01:27,750
But what if through each of those steps,

32
00:01:27,750 --> 00:01:32,220
there is an agent which works
as a pair programmer for you.

33
00:01:32,220 --> 00:01:34,290
So this is our session today,

34
00:01:34,290 --> 00:01:37,650
to walk you through
agentic data engineering

35
00:01:37,650 --> 00:01:41,670
using AWS analytical MCP servers.

36
00:01:41,670 --> 00:01:42,840
I'm Harshida Patel,

37
00:01:42,840 --> 00:01:46,890
a principal specialist
solution architect from AWS.

38
00:01:46,890 --> 00:01:49,620
And joining me today is Ram Nottath,

39
00:01:49,620 --> 00:01:53,133
who is a principal solutions
architect with data and AI.

40
00:01:54,360 --> 00:01:55,810
Okay, thank you so much, Ram.

41
00:01:59,670 --> 00:02:02,730
So this is our journey for this session.

42
00:02:02,730 --> 00:02:06,030
We'll start off with the
data engineer's pain points

43
00:02:06,030 --> 00:02:08,670
and the challenges they run into,

44
00:02:08,670 --> 00:02:12,840
followed by using agentic
AI as the solution,

45
00:02:12,840 --> 00:02:15,630
with model context protocol.

46
00:02:15,630 --> 00:02:19,830
Then we will do a demo, share
with you best practices,

47
00:02:19,830 --> 00:02:23,460
and how you can deploy
this in production at scale

48
00:02:23,460 --> 00:02:26,643
and share resources that
you can walk away with.

49
00:02:28,050 --> 00:02:31,080
So let's get started with meet AnyCompany,

50
00:02:31,080 --> 00:02:33,390
which is a fictitious retailer.

51
00:02:33,390 --> 00:02:35,490
They want to transform their business

52
00:02:35,490 --> 00:02:38,040
by transforming their customer experience

53
00:02:38,040 --> 00:02:40,620
by tapping into the data.

54
00:02:40,620 --> 00:02:43,830
They have a very clear
vision and a mission,

55
00:02:43,830 --> 00:02:48,720
that they want to put data
to work and make an impact.

56
00:02:48,720 --> 00:02:51,960
So they start off with
a solid data strategy,

57
00:02:51,960 --> 00:02:54,300
where data is the foundation.

58
00:02:54,300 --> 00:02:57,240
They want to build scalable,
analytical pipelines

59
00:02:57,240 --> 00:03:01,140
with quality to serve
the growing use cases.

60
00:03:01,140 --> 00:03:05,130
But their main goal is
to provide this insight

61
00:03:05,130 --> 00:03:08,880
into the hands of the
decision-maker at the right time

62
00:03:08,880 --> 00:03:11,703
so they can make a data-driven decision.

63
00:03:12,750 --> 00:03:15,480
So this is their vision and their mission.

64
00:03:15,480 --> 00:03:20,250
But when we check the reality,
it's completely different.

65
00:03:20,250 --> 00:03:24,607
A marketing team reaches out
to the data engineers and say,

66
00:03:24,607 --> 00:03:26,670
"This is a new promotional analysis

67
00:03:26,670 --> 00:03:29,880
that we want to build
and get insight into.

68
00:03:29,880 --> 00:03:32,640
Can you build a data pipeline for this?"

69
00:03:32,640 --> 00:03:35,520
They say it's going to
take two to three weeks.

70
00:03:35,520 --> 00:03:38,370
The operational team wants
to get near real-time insight

71
00:03:38,370 --> 00:03:40,230
into the inventory.

72
00:03:40,230 --> 00:03:42,007
The data engineer shares,

73
00:03:42,007 --> 00:03:46,320
"Our data pipeline is already
lagging three to five days."

74
00:03:46,320 --> 00:03:50,310
The executives want to automate
the Monday morning report

75
00:03:50,310 --> 00:03:53,343
so they can make the
decisions at the right time,

76
00:03:54,180 --> 00:03:55,780
but it's still a manual refresh.

77
00:03:57,660 --> 00:04:00,720
So this is impacting the marketing team,

78
00:04:00,720 --> 00:04:03,090
the operations teams, the executives.

79
00:04:03,090 --> 00:04:05,670
So the bottom line, it's
impacting the revenue

80
00:04:05,670 --> 00:04:08,043
because the decisions
cannot be made on time.

81
00:04:09,450 --> 00:04:12,907
When you further deep dive into
and ask the data engineers,

82
00:04:12,907 --> 00:04:15,420
"What is the cause for this productivity,

83
00:04:15,420 --> 00:04:18,240
or what is the cause for this delays?"

84
00:04:18,240 --> 00:04:21,090
And I would ask, like,
love to ask you a question.

85
00:04:21,090 --> 00:04:25,020
Does any of this resonate
with any of you, right?

86
00:04:25,020 --> 00:04:28,110
Context switching
starting with number one.

87
00:04:28,110 --> 00:04:31,290
So when you get the data
engineers get the requirement,

88
00:04:31,290 --> 00:04:35,250
they want to embed a new
feature within their product.

89
00:04:35,250 --> 00:04:37,410
So they search through the code,

90
00:04:37,410 --> 00:04:41,250
they try to understand the
code, there is no documentation.

91
00:04:41,250 --> 00:04:42,780
Once they understand the requirement,

92
00:04:42,780 --> 00:04:47,010
they go and do web search to
figure out the best practices

93
00:04:47,010 --> 00:04:48,240
to write the code,

94
00:04:48,240 --> 00:04:52,320
figure out how this particular
feature in AWS really works.

95
00:04:52,320 --> 00:04:54,060
Then they start developing the code,

96
00:04:54,060 --> 00:04:56,130
writing things from scratch.

97
00:04:56,130 --> 00:05:00,540
And while they are writing and
focusing on the development,

98
00:05:00,540 --> 00:05:03,360
there is a data analyst
which sends an alert

99
00:05:03,360 --> 00:05:05,610
that the key performance
indicators for today

100
00:05:05,610 --> 00:05:07,440
are completely off.

101
00:05:07,440 --> 00:05:12,030
So they start debugging, they
start looking at the data,

102
00:05:12,030 --> 00:05:15,520
and they finally figure
out after hours of effort

103
00:05:16,470 --> 00:05:20,070
that there is a source data quality issue.

104
00:05:20,070 --> 00:05:22,320
So we missed an entry in the dimension.

105
00:05:22,320 --> 00:05:24,330
So when you're joining to the fact,

106
00:05:24,330 --> 00:05:26,700
it's not able to relay

107
00:05:26,700 --> 00:05:30,510
the key performance indicator metrics.

108
00:05:30,510 --> 00:05:35,133
So again, data quality is more
reactive, it's not proactive.

109
00:05:36,270 --> 00:05:37,170
They go back.

110
00:05:37,170 --> 00:05:38,370
Once the data is fixed,

111
00:05:38,370 --> 00:05:40,950
they go back to do the development.

112
00:05:40,950 --> 00:05:42,420
But then there is an alert

113
00:05:42,420 --> 00:05:45,750
that the data pipeline today is very slow.

114
00:05:45,750 --> 00:05:47,610
It's not performing.

115
00:05:47,610 --> 00:05:49,920
So again, the data engineer
switches the focus,

116
00:05:49,920 --> 00:05:52,440
figures out what the
performance issues are,

117
00:05:52,440 --> 00:05:54,630
and then tries to go to the web search.

118
00:05:54,630 --> 00:05:56,700
How do you optimize this pipeline?

119
00:05:56,700 --> 00:06:00,390
And this is where the time gets lost.

120
00:06:00,390 --> 00:06:02,430
So all of these things are piling up.

121
00:06:02,430 --> 00:06:05,100
Then one fine day, Sarah Chen,

122
00:06:05,100 --> 00:06:08,220
who is the CTO of data
engineering, sends an email

123
00:06:08,220 --> 00:06:11,610
that we are missing to provide

124
00:06:11,610 --> 00:06:14,490
the data insights to the marketing team,

125
00:06:14,490 --> 00:06:18,270
the operations team, as
well as the executive team.

126
00:06:18,270 --> 00:06:22,380
Sarah heard from one of her
friend at another retailer

127
00:06:22,380 --> 00:06:25,800
that they are able to
see productivity gain

128
00:06:25,800 --> 00:06:28,230
while building the data pipeline

129
00:06:28,230 --> 00:06:31,470
using data engineers using agentic AI.

130
00:06:31,470 --> 00:06:35,580
So Sarah asked the team whether
we continue to do something

131
00:06:35,580 --> 00:06:39,330
which is not working, or
radically shift the direction.

132
00:06:39,330 --> 00:06:43,020
So let's work backwards
from what the ask is

133
00:06:43,020 --> 00:06:44,583
and see what is possible.

134
00:06:45,420 --> 00:06:49,560
Let's use this use case of a
traditional data engineering

135
00:06:49,560 --> 00:06:52,173
versus with agentic data engineering.

136
00:06:53,160 --> 00:06:56,850
So let's go back that the
developer or the data engineer

137
00:06:56,850 --> 00:06:58,173
got the requirement,

138
00:06:59,070 --> 00:07:04,070
find the retail customer
dataset, process it,

139
00:07:04,380 --> 00:07:07,590
check the demographics, validate it,

140
00:07:07,590 --> 00:07:10,263
and upload it to a data
warehouse for reporting.

141
00:07:11,700 --> 00:07:13,533
So, what does a data engineer do?

142
00:07:14,700 --> 00:07:17,820
If the dataset is cataloged,
they would go to the catalog,

143
00:07:17,820 --> 00:07:19,140
they would search the location,

144
00:07:19,140 --> 00:07:22,050
they will be able to able
to find out the column,

145
00:07:22,050 --> 00:07:24,030
the corresponding data type.

146
00:07:24,030 --> 00:07:25,290
Once they have identified,

147
00:07:25,290 --> 00:07:28,320
they start writing the code from scratch,

148
00:07:28,320 --> 00:07:30,330
they deploy it, they run it.

149
00:07:30,330 --> 00:07:32,550
If there is an error, they fix it.

150
00:07:32,550 --> 00:07:35,610
Then they write a SQL
to validate that dataset

151
00:07:35,610 --> 00:07:38,460
just to look at, are there null values?

152
00:07:38,460 --> 00:07:41,070
Are there 50 states in the dataset?

153
00:07:41,070 --> 00:07:42,540
Then they write a Python code

154
00:07:42,540 --> 00:07:44,433
to load this into a data warehouse.

155
00:07:45,360 --> 00:07:49,770
Versus with agentic data engineering,

156
00:07:49,770 --> 00:07:52,260
what if for each of the step,

157
00:07:52,260 --> 00:07:55,800
instead of writing and typing
and going through the console,

158
00:07:55,800 --> 00:07:59,460
it's converted into a natural language.

159
00:07:59,460 --> 00:08:03,750
So first, find the
retail customer dataset.

160
00:08:03,750 --> 00:08:08,270
Once you find it, create
a Glue job, deploy it.

161
00:08:09,390 --> 00:08:12,900
If there are errors, you fix it.

162
00:08:12,900 --> 00:08:14,910
You validate the dataset.

163
00:08:14,910 --> 00:08:18,210
And then you finally load
it to a data warehouse.

164
00:08:18,210 --> 00:08:21,990
So the manual process of clicking through,

165
00:08:21,990 --> 00:08:25,350
converting that through
agentic data engineering

166
00:08:25,350 --> 00:08:28,410
by asking questions in
the natural language,

167
00:08:28,410 --> 00:08:32,610
and have the agent do
the work on your behalf.

168
00:08:32,610 --> 00:08:34,860
So, how is this possible?

169
00:08:34,860 --> 00:08:38,220
This is where the agentic
loop comes into play,

170
00:08:38,220 --> 00:08:39,480
where you have an agent

171
00:08:39,480 --> 00:08:41,590
where we exactly ask the same question

172
00:08:42,510 --> 00:08:46,110
for the retail customer dataset,
process it, validate it,

173
00:08:46,110 --> 00:08:48,540
and upload it to a data warehouse.

174
00:08:48,540 --> 00:08:50,790
This is your question.

175
00:08:50,790 --> 00:08:54,870
Your question becomes
the goal for the agent

176
00:08:54,870 --> 00:08:57,360
It has cognitive insight.

177
00:08:57,360 --> 00:09:00,240
So as a data engineer, once
we get the requirement,

178
00:09:00,240 --> 00:09:03,600
we try to understand
it, we try to plan it,

179
00:09:03,600 --> 00:09:05,400
we try to take action.

180
00:09:05,400 --> 00:09:09,480
Then once the action is performed,
we try to make decisions.

181
00:09:09,480 --> 00:09:12,150
So all of this is the reasoning.

182
00:09:12,150 --> 00:09:16,170
And while the large language
model takes your request,

183
00:09:16,170 --> 00:09:20,850
parses out into multiple steps,
it also needs information.

184
00:09:20,850 --> 00:09:22,957
So it's able to identify,

185
00:09:22,957 --> 00:09:26,850
"I need more data to get more context

186
00:09:26,850 --> 00:09:30,780
to figure out what my next
course of action should be."

187
00:09:30,780 --> 00:09:34,680
So the agents have access to the tools

188
00:09:34,680 --> 00:09:37,680
where they can execute, not only a code,

189
00:09:37,680 --> 00:09:40,140
but also retrieve more information.

190
00:09:40,140 --> 00:09:45,140
And this context goes back to the LLM,

191
00:09:45,240 --> 00:09:47,610
where it's able to see, get,

192
00:09:47,610 --> 00:09:50,820
and perceive from the
new information it got,

193
00:09:50,820 --> 00:09:54,060
and then figures out what
next step it has to take.

194
00:09:54,060 --> 00:09:57,450
So this loop of reasoning,

195
00:09:57,450 --> 00:10:01,500
as well as taking the
action till your goal is met

196
00:10:01,500 --> 00:10:03,693
is what is the agentic loop.

197
00:10:05,850 --> 00:10:10,200
Now let's dive into the building
blocks with the agentic AI.

198
00:10:10,200 --> 00:10:14,250
So agent is in the
center, agent has memory,

199
00:10:14,250 --> 00:10:16,710
it has short term and long-term memory.

200
00:10:16,710 --> 00:10:21,240
The short term memory is for
in the context of your session,

201
00:10:21,240 --> 00:10:24,240
the long-term memory,
think of it as episodic,

202
00:10:24,240 --> 00:10:27,510
what happened in the past,
like your personal diary.

203
00:10:27,510 --> 00:10:29,850
It can also have semantic memory,

204
00:10:29,850 --> 00:10:32,460
where it has the context of your domain

205
00:10:32,460 --> 00:10:34,650
or domain-level knowledge.

206
00:10:34,650 --> 00:10:39,180
The agents has an arm where
it has access to tools.

207
00:10:39,180 --> 00:10:42,540
And the tools are
something that you control,

208
00:10:42,540 --> 00:10:45,180
whether you want to have
a function for the agent

209
00:10:45,180 --> 00:10:49,200
to access your data, your AWS services,

210
00:10:49,200 --> 00:10:52,290
and this could also be external services.

211
00:10:52,290 --> 00:10:56,100
So tools is the action
that the agent can take.

212
00:10:56,100 --> 00:10:57,930
It's calling a function.

213
00:10:57,930 --> 00:11:01,590
The function can be retrieve
the data, run a code,

214
00:11:01,590 --> 00:11:02,940
or invoke an API.

215
00:11:02,940 --> 00:11:04,950
So that's the action.

216
00:11:04,950 --> 00:11:06,930
The agent has brain.

217
00:11:06,930 --> 00:11:08,640
So we talked about reasoning,

218
00:11:08,640 --> 00:11:12,150
the next step is it
identifies multiple steps

219
00:11:12,150 --> 00:11:16,260
that it needs to take to
solve a complex problem.

220
00:11:16,260 --> 00:11:19,260
It takes a step, it gets the insight,

221
00:11:19,260 --> 00:11:21,300
and the results from the tool.

222
00:11:21,300 --> 00:11:25,770
Then it reflects what should
be the next course of action.

223
00:11:25,770 --> 00:11:29,070
So the planning, reflection,
chain of thoughts,

224
00:11:29,070 --> 00:11:31,170
as well as self critiquing.

225
00:11:31,170 --> 00:11:33,210
The main thing is action.

226
00:11:33,210 --> 00:11:35,640
And this is also what you control.

227
00:11:35,640 --> 00:11:39,330
Whether you want all the
actions to be on autopilot,

228
00:11:39,330 --> 00:11:43,650
or you want the human in the
loop for some of the actions.

229
00:11:43,650 --> 00:11:46,410
So an agent, along with the memory,

230
00:11:46,410 --> 00:11:48,240
the planning, the reasoning,

231
00:11:48,240 --> 00:11:51,930
along with the tool is what it automates

232
00:11:51,930 --> 00:11:56,790
and independently to perform
the task that you provide,

233
00:11:56,790 --> 00:11:58,890
which is the goal for the agent.

234
00:11:58,890 --> 00:12:01,500
And for an agent, give it a persona.

235
00:12:01,500 --> 00:12:04,020
So for our talk,

236
00:12:04,020 --> 00:12:06,960
we will give it the
persona of a data engineer.

237
00:12:06,960 --> 00:12:09,480
The goal is to build the data pipeline

238
00:12:09,480 --> 00:12:13,110
and provide the instructions precisely

239
00:12:13,110 --> 00:12:15,753
so it can think better.

240
00:12:17,250 --> 00:12:19,440
Now, let's summarize all of this.

241
00:12:19,440 --> 00:12:21,510
So we talk about agentic loop,

242
00:12:21,510 --> 00:12:23,610
we talked about the building blocks,

243
00:12:23,610 --> 00:12:27,810
but now, let's put a simple
example of a workflow.

244
00:12:27,810 --> 00:12:29,520
So you as a data engineer,

245
00:12:29,520 --> 00:12:32,220
you're asking a question
to build the pipeline

246
00:12:32,220 --> 00:12:33,930
in natural language,

247
00:12:33,930 --> 00:12:37,470
your interaction is with
the agentic assistant.

248
00:12:37,470 --> 00:12:39,690
And then the agentic assistant

249
00:12:39,690 --> 00:12:44,690
is already integrating with
the tools your data sources.

250
00:12:45,030 --> 00:12:47,580
So along with this question,

251
00:12:47,580 --> 00:12:52,410
the tool descriptions are sent
to the large language model.

252
00:12:52,410 --> 00:12:53,940
The large language model,

253
00:12:53,940 --> 00:12:56,250
as it has pared out your complex question

254
00:12:56,250 --> 00:12:57,960
into multiple steps,

255
00:12:57,960 --> 00:13:01,620
it decides whether it
wants a specific tool

256
00:13:01,620 --> 00:13:03,570
or to do a function call.

257
00:13:03,570 --> 00:13:07,110
So it makes the request back
to the agentic assistant

258
00:13:07,110 --> 00:13:10,830
along with so that it can get the context.

259
00:13:10,830 --> 00:13:15,150
The agentic assistant invokes the function

260
00:13:15,150 --> 00:13:19,560
to retrieve the information
from your data source.

261
00:13:19,560 --> 00:13:20,850
It gets the results,

262
00:13:20,850 --> 00:13:24,873
and this results are sent back
to the large language model.

263
00:13:25,980 --> 00:13:28,950
Now, large language model
gets that information

264
00:13:28,950 --> 00:13:30,120
of the results.

265
00:13:30,120 --> 00:13:31,980
Context, it's gonna reflect.

266
00:13:31,980 --> 00:13:33,240
It's gonna figure out

267
00:13:33,240 --> 00:13:36,630
what is going to be the
next course of action.

268
00:13:36,630 --> 00:13:39,180
In our use case, we started off

269
00:13:39,180 --> 00:13:42,043
with retail data customer
dataset, which is on S3.

270
00:13:42,960 --> 00:13:44,707
So it's going to request,

271
00:13:44,707 --> 00:13:47,400
"Find me the list of,

272
00:13:47,400 --> 00:13:50,940
find me the location of
retail customer dataset."

273
00:13:50,940 --> 00:13:54,420
The tool it's going to
call whether it's list S3

274
00:13:54,420 --> 00:13:56,910
or whether if they are using DataZone,

275
00:13:56,910 --> 00:13:59,133
it will invoke any API for DataZone.

276
00:14:00,600 --> 00:14:05,010
The agent calls their tool,
performs that function,

277
00:14:05,010 --> 00:14:06,510
the results are returned,

278
00:14:06,510 --> 00:14:07,710
and that information that,

279
00:14:07,710 --> 00:14:10,260
hey, this is the location
of the S3 dataset

280
00:14:10,260 --> 00:14:12,480
is sent back to large language model.

281
00:14:12,480 --> 00:14:16,830
But the next step, we
need to create a Glue job.

282
00:14:16,830 --> 00:14:18,990
So the large language model

283
00:14:18,990 --> 00:14:22,170
is going to call another set of function.

284
00:14:22,170 --> 00:14:25,740
So this entire loop is dynamic in nature.

285
00:14:25,740 --> 00:14:29,040
It's going to, this loop
is going to continue until,

286
00:14:29,040 --> 00:14:33,480
unless it has all the
information to meet your goal

287
00:14:33,480 --> 00:14:37,263
and provide a final comprehensive
response to the end user.

288
00:14:39,330 --> 00:14:41,820
So we saw that we have agents,

289
00:14:41,820 --> 00:14:45,120
but the agents just with large
language model is not enough.

290
00:14:45,120 --> 00:14:46,650
It needs to have the context,

291
00:14:46,650 --> 00:14:48,480
it needs to have the information,

292
00:14:48,480 --> 00:14:50,220
it needs to perform an action.

293
00:14:50,220 --> 00:14:52,920
So the integration of the AI agents

294
00:14:52,920 --> 00:14:57,543
with the tools and the data
sources becomes important.

295
00:14:59,220 --> 00:15:01,560
So why is this little bit challenging

296
00:15:01,560 --> 00:15:05,910
to integrate the data sources
and the tool with an AI agent?

297
00:15:05,910 --> 00:15:07,440
Let's consider this use case.

298
00:15:07,440 --> 00:15:09,270
You have an AI agent.

299
00:15:09,270 --> 00:15:13,140
When you need to talk to a
data warehouse, it uses SQL.

300
00:15:13,140 --> 00:15:16,710
When it needs to talk
to, say, a Glue catalog

301
00:15:16,710 --> 00:15:20,520
or it needs to do data
processing, it's going to use API.

302
00:15:20,520 --> 00:15:22,890
What if there are specification changes?

303
00:15:22,890 --> 00:15:25,110
Then they have to update
those specification

304
00:15:25,110 --> 00:15:26,820
to adhere to the API.

305
00:15:26,820 --> 00:15:30,390
What if they need to add
another tool or remove a tool?

306
00:15:30,390 --> 00:15:32,820
And this might be simple
if you have one agent.

307
00:15:32,820 --> 00:15:36,240
But when you start creating
N number of agents,

308
00:15:36,240 --> 00:15:40,440
think about the tight coupling
and the integration problem.

309
00:15:40,440 --> 00:15:42,540
And this is all custom integration.

310
00:15:42,540 --> 00:15:46,260
This is N times M custom
integration problem.

311
00:15:46,260 --> 00:15:48,120
So, how do we simplify this?

312
00:15:48,120 --> 00:15:49,950
How do we make it seamless

313
00:15:49,950 --> 00:15:53,640
for the AI agents to discover
what are the data sources

314
00:15:53,640 --> 00:15:55,890
and what are the tools
which are available?

315
00:15:55,890 --> 00:16:00,210
This is where Model Context
Protocol comes into play.

316
00:16:00,210 --> 00:16:02,370
It's a universal language.

317
00:16:02,370 --> 00:16:05,970
So in this example,
you want to add a tool,

318
00:16:05,970 --> 00:16:07,080
you add a function.

319
00:16:07,080 --> 00:16:08,640
If you want to remove a function,

320
00:16:08,640 --> 00:16:11,280
you remove the function from MCP.

321
00:16:11,280 --> 00:16:12,750
If there is an API change,

322
00:16:12,750 --> 00:16:15,360
you make that change in one single place.

323
00:16:15,360 --> 00:16:17,400
Then you can integrate one agent,

324
00:16:17,400 --> 00:16:19,830
or you can integrate N number of agents.

325
00:16:19,830 --> 00:16:24,093
So this makes the integration
standardized integration.

326
00:16:26,550 --> 00:16:28,920
So what is Model Context Protocol?

327
00:16:28,920 --> 00:16:33,480
It's USB-C for agentic AI applications.

328
00:16:33,480 --> 00:16:36,633
And this was invented or
developed by Anthropic.

329
00:16:39,990 --> 00:16:42,603
So let's look at the architecture for MCP.

330
00:16:43,770 --> 00:16:48,060
There is MCP host, the one
with which you are integrating.

331
00:16:48,060 --> 00:16:49,620
There is an application.

332
00:16:49,620 --> 00:16:51,990
You can use an application of your choice.

333
00:16:51,990 --> 00:16:55,740
It could be Amazon SageMaker
Unified Studio, Kiro,

334
00:16:55,740 --> 00:16:57,060
Claude Desktop.

335
00:16:57,060 --> 00:17:00,840
And those application
needs to be MCP-compliant.

336
00:17:00,840 --> 00:17:02,850
So there is the client side,

337
00:17:02,850 --> 00:17:05,430
and these clients are integrating

338
00:17:05,430 --> 00:17:09,510
or communicating with the MCP servers.

339
00:17:09,510 --> 00:17:12,540
And this integration, from
the client to the server,

340
00:17:12,540 --> 00:17:15,290
is known as the transport
layer or communication model.

341
00:17:16,140 --> 00:17:18,750
There are two deployments of MCP servers.

342
00:17:18,750 --> 00:17:22,170
When the MCP server is locally deployed,

343
00:17:22,170 --> 00:17:25,830
where it's using the compute
of where your host is,

344
00:17:25,830 --> 00:17:29,250
that is the local deployment of MCP server

345
00:17:29,250 --> 00:17:33,120
and the communication it uses
is standard input output.

346
00:17:33,120 --> 00:17:37,650
If the MCP server is remote,
it's going to use HTTPS.

347
00:17:37,650 --> 00:17:39,780
Now, the server is the one

348
00:17:39,780 --> 00:17:44,070
that is integrating with
your data sources, your data,

349
00:17:44,070 --> 00:17:46,050
which is in your control.

350
00:17:46,050 --> 00:17:49,830
So the way the MCP server
integrates with the sources,

351
00:17:49,830 --> 00:17:51,420
it could be SQL, it could be API,

352
00:17:51,420 --> 00:17:52,920
it's completely independent,

353
00:17:52,920 --> 00:17:55,530
depending upon the
services that it provide.

354
00:17:55,530 --> 00:17:58,290
So let's take over here as an example.

355
00:17:58,290 --> 00:18:00,299
Say you're working with Kiro,

356
00:18:00,299 --> 00:18:01,676
I'm gonna use Kiro

357
00:18:01,676 --> 00:18:03,690
because it's been announced even yesterday

358
00:18:03,690 --> 00:18:04,953
with a lot of features.

359
00:18:05,910 --> 00:18:08,760
You ask your question, find
the same question, right?

360
00:18:08,760 --> 00:18:11,310
We are gonna repeat throughout the talk

361
00:18:11,310 --> 00:18:13,710
is find me the retail customer dataset,

362
00:18:13,710 --> 00:18:15,270
process it, validate it,

363
00:18:15,270 --> 00:18:17,850
and then load it into the data warehouse.

364
00:18:17,850 --> 00:18:20,124
The large language model, think about it.

365
00:18:20,124 --> 00:18:23,250
It's going to break it
down into multiple steps.

366
00:18:23,250 --> 00:18:25,620
For the first step, it's going to...

367
00:18:25,620 --> 00:18:28,770
And then, because of this
integration of MCP client,

368
00:18:28,770 --> 00:18:33,000
it also has the information
of the tools that it can use.

369
00:18:33,000 --> 00:18:37,650
So the first step is find the
dataset retail customer data.

370
00:18:37,650 --> 00:18:40,680
The tool it wants to use is list S3,

371
00:18:40,680 --> 00:18:42,210
or if there is a data catalog,

372
00:18:42,210 --> 00:18:44,910
go through and search
through the data catalog.

373
00:18:44,910 --> 00:18:48,780
So the client sends the
request to the server.

374
00:18:48,780 --> 00:18:52,230
The server is the one that
is running the function

375
00:18:52,230 --> 00:18:54,210
to integrate with your service.

376
00:18:54,210 --> 00:18:57,540
Then the response is sent back
from the server to the client

377
00:18:57,540 --> 00:19:01,260
and to the large language
model and reaching the context

378
00:19:01,260 --> 00:19:03,510
and it figure out next step.

379
00:19:03,510 --> 00:19:06,630
So what are the components
on the server side?

380
00:19:06,630 --> 00:19:08,040
There are three components.

381
00:19:08,040 --> 00:19:10,920
Tools, consider it, not consider,

382
00:19:10,920 --> 00:19:15,150
it is the functions that
the client can call.

383
00:19:15,150 --> 00:19:18,690
For example, create a Glue
crawler, or create a Glue job.

384
00:19:18,690 --> 00:19:21,870
For example, resources are read-only,

385
00:19:21,870 --> 00:19:24,360
get me the list of all the Glue tables.

386
00:19:24,360 --> 00:19:25,560
And then you have prompts,

387
00:19:25,560 --> 00:19:30,243
which are blueprints or
structured prompt templates.

388
00:19:32,340 --> 00:19:35,553
If you wanna take a picture,
I'll go a slide back.

389
00:19:37,230 --> 00:19:38,063
Okay.

390
00:19:39,300 --> 00:19:43,380
So now we talked about how a agentic AI,

391
00:19:43,380 --> 00:19:46,020
which is the reasoning
and the action in the loop

392
00:19:46,020 --> 00:19:47,520
and the integration with the tools

393
00:19:47,520 --> 00:19:51,420
going to help work independently

394
00:19:51,420 --> 00:19:54,513
to serve a data engineer
to perform the tasks.

395
00:19:56,340 --> 00:19:57,990
The MCP servers,

396
00:19:57,990 --> 00:20:01,770
when you have an analytical tech stack

397
00:20:01,770 --> 00:20:04,260
where you could be
using multiple services,

398
00:20:04,260 --> 00:20:09,090
AWS Glue or Amazon EMR or Amazon
Athena or Amazon Redshift,

399
00:20:09,090 --> 00:20:13,950
this MCP servers, AWS
analytical MCP servers,

400
00:20:13,950 --> 00:20:16,680
think about the first problem
that the developers had,

401
00:20:16,680 --> 00:20:18,420
contact switching.

402
00:20:18,420 --> 00:20:21,600
This MCP servers covers multiple services.

403
00:20:21,600 --> 00:20:23,130
The second is getting the latest

404
00:20:23,130 --> 00:20:25,380
and the greatest
information on the features,

405
00:20:25,380 --> 00:20:26,910
the best practices.

406
00:20:26,910 --> 00:20:30,630
So when the data engineer ask
a question, "Build me a code."

407
00:20:30,630 --> 00:20:32,820
That code will be with quality

408
00:20:32,820 --> 00:20:35,880
because it is aware of the best practices.

409
00:20:35,880 --> 00:20:37,950
When you're trying to build a workflow,

410
00:20:37,950 --> 00:20:39,480
process the Glue job,

411
00:20:39,480 --> 00:20:42,090
and then load this data
into a data warehouse,

412
00:20:42,090 --> 00:20:45,900
it's like workflow orchestration.

413
00:20:45,900 --> 00:20:48,000
So it has the domain knowledge

414
00:20:48,000 --> 00:20:50,283
of the AWS services to work with.

415
00:20:52,050 --> 00:20:54,250
So the MCP servers with AWS

416
00:20:55,410 --> 00:21:00,410
is when you're using Amazon
Glue, Amazon AWS Glue,

417
00:21:01,020 --> 00:21:04,440
or Amazon EMR for your data processing,

418
00:21:04,440 --> 00:21:07,680
or Amazon Athena for interactive query,

419
00:21:07,680 --> 00:21:11,250
there is data processing MCP server.

420
00:21:11,250 --> 00:21:14,857
And the questions a data
engineer you can ask,

421
00:21:14,857 --> 00:21:16,740
"Create a Glue crawler."

422
00:21:16,740 --> 00:21:20,850
And these are only few list
of questions in the list.

423
00:21:20,850 --> 00:21:23,490
We'll also share with you a QR code,

424
00:21:23,490 --> 00:21:26,940
where you can find the
entire list of MCP servers,

425
00:21:26,940 --> 00:21:29,223
along with the tools that it supports.

426
00:21:30,420 --> 00:21:35,420
The LLM is the one which
decides which tool to invoke.

427
00:21:35,730 --> 00:21:37,380
Say, you call Glue crawler.

428
00:21:37,380 --> 00:21:40,740
It knows that there is a
function associated with Glue,

429
00:21:40,740 --> 00:21:42,360
it's going to call that.

430
00:21:42,360 --> 00:21:45,847
But when you start asking
questions on your dataset,

431
00:21:45,847 --> 00:21:49,680
"From the sales data,
identify top 10 products,"

432
00:21:49,680 --> 00:21:52,620
the LLM is aware that,
hey, this is a query,

433
00:21:52,620 --> 00:21:54,720
so it's going to invoke the function,

434
00:21:54,720 --> 00:21:57,033
which is associated with Amazon Athena.

435
00:21:57,930 --> 00:22:00,420
So again, the best practices

436
00:22:00,420 --> 00:22:04,020
integrated with all of this workflow.

437
00:22:04,020 --> 00:22:05,460
When you're using Amazon Redshift,

438
00:22:05,460 --> 00:22:08,100
which is a data warehousing service,

439
00:22:08,100 --> 00:22:09,870
there is Redshift MCP server,

440
00:22:09,870 --> 00:22:12,600
and this Redshift MCP server is read-only.

441
00:22:12,600 --> 00:22:14,010
It provides you the information.

442
00:22:14,010 --> 00:22:17,850
You can say, "List all the
endpoints in my account."

443
00:22:17,850 --> 00:22:19,770
Or if you have two end points

444
00:22:19,770 --> 00:22:24,210
and you want to query
across those two endpoints

445
00:22:24,210 --> 00:22:27,510
on customer information, you
have the ability to do so,

446
00:22:27,510 --> 00:22:29,070
but it's read-only.

447
00:22:29,070 --> 00:22:31,396
If you are using DataZone,

448
00:22:31,396 --> 00:22:34,590
DataZone MCP server, where
you can list and subscribe.

449
00:22:34,590 --> 00:22:38,373
When you're using Amazon
managed streaming for Apache,

450
00:22:39,510 --> 00:22:44,510
think about an admin persona
who wants to create a cluster,

451
00:22:45,210 --> 00:22:47,790
who wants to set the retention log,

452
00:22:47,790 --> 00:22:51,150
who wants to get insight
into the resource monitoring,

453
00:22:51,150 --> 00:22:56,150
so the MCP server for MSK is
more from an admin persona.

454
00:22:56,640 --> 00:23:00,450
When you are using OpenSearch,
the OpenSearch MCP server,

455
00:23:00,450 --> 00:23:01,650
you can list indices,

456
00:23:01,650 --> 00:23:05,490
but you can also ask
questions on your dataset.

457
00:23:05,490 --> 00:23:09,810
Again, this is only a small
list of the MCP servers

458
00:23:09,810 --> 00:23:11,730
and the corresponding
natural language questions

459
00:23:11,730 --> 00:23:13,230
you can ask.

460
00:23:13,230 --> 00:23:17,040
So now, we walk through the theory part,

461
00:23:17,040 --> 00:23:19,620
let's now dive deep into the action

462
00:23:19,620 --> 00:23:21,570
and how to make it possible.

463
00:23:21,570 --> 00:23:23,280
I'd like to hand over to Ram,

464
00:23:23,280 --> 00:23:25,473
but thank you so much for joining us.

465
00:23:26,610 --> 00:23:29,610
(audience clapping)

466
00:23:31,830 --> 00:23:34,530
- Thank you, Harshida,
for that amazing overview

467
00:23:34,530 --> 00:23:39,255
of the agentic loop and the capabilities

468
00:23:39,255 --> 00:23:41,340
of the various analytics MCP servers.

469
00:23:41,340 --> 00:23:42,990
Just a quick check, am I audible?

470
00:23:42,990 --> 00:23:43,823
All good?

471
00:23:43,823 --> 00:23:44,656
Okay.

472
00:23:44,656 --> 00:23:45,489
All right.

473
00:23:45,489 --> 00:23:47,280
So as a solutions architect,

474
00:23:47,280 --> 00:23:50,550
I do help with multiple customers

475
00:23:50,550 --> 00:23:53,610
to implement data and AI solutions.

476
00:23:53,610 --> 00:23:57,600
When I talk to them, there is
one thing that is consistent,

477
00:23:57,600 --> 00:24:01,290
one requirement that is consistent
across all the customers,

478
00:24:01,290 --> 00:24:03,540
which is the need for data engineering.

479
00:24:03,540 --> 00:24:06,810
And I'm pretty sure we all are
here because of that, right?

480
00:24:06,810 --> 00:24:09,810
Now, let's see the AnyCompany Retail

481
00:24:09,810 --> 00:24:10,680
that we were talking about

482
00:24:10,680 --> 00:24:12,180
is not any different either, right?

483
00:24:12,180 --> 00:24:13,800
So they have the same challenge.

484
00:24:13,800 --> 00:24:14,633
Now, let's take a look

485
00:24:14,633 --> 00:24:17,970
at how we can implement that solution,

486
00:24:17,970 --> 00:24:21,243
which will help you to do this
data engineering much faster.

487
00:24:23,010 --> 00:24:26,160
Here is an architecture
that can help you with that.

488
00:24:26,160 --> 00:24:27,900
This can actually give you

489
00:24:27,900 --> 00:24:30,598
a state-of-the-art data engineering

490
00:24:30,598 --> 00:24:32,190
with agentic capabilities

491
00:24:32,190 --> 00:24:34,740
that Harshida was mentioning earlier.

492
00:24:34,740 --> 00:24:37,260
Now, here, if you look at,

493
00:24:37,260 --> 00:24:40,800
I can see the various MCP
servers coming together

494
00:24:40,800 --> 00:24:44,970
to help the task that a data
engineer would like to do.

495
00:24:44,970 --> 00:24:47,490
There are multiple tasks,
whether it's governance,

496
00:24:47,490 --> 00:24:49,870
whether it's data processing,
whether it's SQL query,

497
00:24:49,870 --> 00:24:51,180
there are multiple things

498
00:24:51,180 --> 00:24:53,760
that a data engineer would want to do.

499
00:24:53,760 --> 00:24:56,310
So these MCP servers come together

500
00:24:56,310 --> 00:24:59,430
to help the data engineer
with those tasks.

501
00:24:59,430 --> 00:25:01,830
Now, let's take a look
a bit more in detail.

502
00:25:01,830 --> 00:25:03,330
Let's start from the left.

503
00:25:03,330 --> 00:25:07,320
So here, the user is
interacting with the MCP host,

504
00:25:07,320 --> 00:25:09,450
the one that Harshida
was mentioning earlier,

505
00:25:09,450 --> 00:25:12,000
and that MCP host could be
any tool of your choice.

506
00:25:12,000 --> 00:25:14,880
It could be SageMaker
Unified Studio or Kiro

507
00:25:14,880 --> 00:25:19,880
or Claude desktop, any of
those MCP-compliant MCP host.

508
00:25:20,700 --> 00:25:22,740
Now once you have an MCP host,

509
00:25:22,740 --> 00:25:25,237
the next thing that you
do is you configure,

510
00:25:25,237 --> 00:25:28,230
whatever MCP servers that
you want to work with.

511
00:25:28,230 --> 00:25:29,520
When you configure that,

512
00:25:29,520 --> 00:25:34,350
it makes a connection between
the host and the server

513
00:25:34,350 --> 00:25:36,630
using the MCP client.

514
00:25:36,630 --> 00:25:41,630
Now, the intelligence part of
this whole agent lies below,

515
00:25:42,060 --> 00:25:44,250
which is the large language model.

516
00:25:44,250 --> 00:25:45,270
And in this case,

517
00:25:45,270 --> 00:25:48,540
we are using Amazon Bedrock's
large language models.

518
00:25:48,540 --> 00:25:50,947
Now, when the user ask a question,

519
00:25:50,947 --> 00:25:53,640
"Hey, this is a task that I want to do,"

520
00:25:53,640 --> 00:25:57,600
LLM interprets what is
the user looking for.

521
00:25:57,600 --> 00:25:59,580
And LLM also understands,

522
00:25:59,580 --> 00:26:02,100
through MCP host and the client,

523
00:26:02,100 --> 00:26:04,590
it also understands that
these are all the tools

524
00:26:04,590 --> 00:26:06,420
that I have at my disposal

525
00:26:06,420 --> 00:26:10,830
and how can I best deliver
the result to the user.

526
00:26:10,830 --> 00:26:14,250
Now, if you remember what
Harshida was mentioning earlier,

527
00:26:14,250 --> 00:26:17,400
there's always a loop, or
it's an iterative process

528
00:26:17,400 --> 00:26:20,730
because we may not get the
answer right at the first one

529
00:26:20,730 --> 00:26:23,340
because there are multiple
steps involved in the task.

530
00:26:23,340 --> 00:26:26,970
So what LLM does is it
uses the first tool,

531
00:26:26,970 --> 00:26:30,690
get the data or do the
first task, get the result,

532
00:26:30,690 --> 00:26:33,900
and then identify what is
the next tool which can solve

533
00:26:33,900 --> 00:26:37,200
or continue solving the
problem that user has asked.

534
00:26:37,200 --> 00:26:38,940
And that iterative process

535
00:26:38,940 --> 00:26:41,310
finally takes you to the final result

536
00:26:41,310 --> 00:26:43,050
what you want to get to.

537
00:26:43,050 --> 00:26:48,050
Now, let's take a look at how
we can configure MCP server

538
00:26:49,860 --> 00:26:51,450
in some of the tools

539
00:26:51,450 --> 00:26:55,620
so that you can get going
with some of these activities.

540
00:26:55,620 --> 00:26:57,390
So here is a screenshot

541
00:26:57,390 --> 00:27:01,323
of how you would do this in
SageMaker Unified Studio.

542
00:27:02,730 --> 00:27:05,010
If you're familiar with
SageMaker Unified Studio,

543
00:27:05,010 --> 00:27:07,140
this is the notebook experience there.

544
00:27:07,140 --> 00:27:10,410
On the left-hand side, you
will see the Amazon Q icon.

545
00:27:10,410 --> 00:27:13,770
And once you click on that,
there is a tool icon on right,

546
00:27:13,770 --> 00:27:16,770
the top of the right corner.

547
00:27:16,770 --> 00:27:19,080
Once you click on that,
that will give you a pop-up

548
00:27:19,080 --> 00:27:22,080
to give the details about the MCP servers

549
00:27:22,080 --> 00:27:23,550
that you wanna configure.

550
00:27:23,550 --> 00:27:24,390
All what you need to do

551
00:27:24,390 --> 00:27:26,550
is provide the name of the MCP server

552
00:27:26,550 --> 00:27:29,250
and couple of configurations,
and you're done.

553
00:27:29,250 --> 00:27:31,530
Now, in this particular case,

554
00:27:31,530 --> 00:27:34,740
I'm configuring data
processing MCP server.

555
00:27:34,740 --> 00:27:36,840
With this simple step,

556
00:27:36,840 --> 00:27:38,190
now you have the capability

557
00:27:38,190 --> 00:27:41,670
to interact with your various
data processing services

558
00:27:41,670 --> 00:27:42,783
in natural language.

559
00:27:44,280 --> 00:27:46,560
Now, let's take a look at how
you would do the same thing

560
00:27:46,560 --> 00:27:49,470
in case of another tool, Kiro.

561
00:27:49,470 --> 00:27:51,390
So it's not any different,

562
00:27:51,390 --> 00:27:55,470
just that in case of Kiro,
there's a mcp.json file.

563
00:27:55,470 --> 00:27:57,690
All what you would do is you have a block

564
00:27:57,690 --> 00:28:01,590
of JSON configuration that you need to add

565
00:28:01,590 --> 00:28:04,110
for each MCP server
that you wanna bring in.

566
00:28:04,110 --> 00:28:06,210
So you might be bringing
in Redshift MCP server

567
00:28:06,210 --> 00:28:09,540
or DataZone MCP server or
data processing MCP server.

568
00:28:09,540 --> 00:28:11,760
For each one of those MCP server,

569
00:28:11,760 --> 00:28:14,460
you'll have a JSON block
that you need to add.

570
00:28:14,460 --> 00:28:17,280
And you might see some
of the auto-approved list

571
00:28:17,280 --> 00:28:18,113
and things like that.

572
00:28:18,113 --> 00:28:19,890
We will cover that later in the session.

573
00:28:19,890 --> 00:28:23,250
But this is how easy that you can can go

574
00:28:23,250 --> 00:28:28,020
and configure the MCP in your
MCP host client that you have,

575
00:28:28,020 --> 00:28:30,030
MCP host application that you have.

576
00:28:30,030 --> 00:28:32,820
Now, let's get a bit more specific

577
00:28:32,820 --> 00:28:35,703
into the task that we have at
hand for AnyCompany Retail.

578
00:28:36,600 --> 00:28:41,340
Let's meet Alex, the
data engineering agent.

579
00:28:41,340 --> 00:28:44,190
Now, Harshida mentioned
the task that we have

580
00:28:44,190 --> 00:28:45,350
for AnyCompany.

581
00:28:46,800 --> 00:28:49,800
We wanna start with,
we want the help Alex.

582
00:28:49,800 --> 00:28:52,290
We wanna start with data discovery.

583
00:28:52,290 --> 00:28:56,700
And we wanna make sure
that Alex is able to go

584
00:28:56,700 --> 00:28:58,850
and find out where is the customer data,

585
00:28:58,850 --> 00:29:03,850
the customer behavior and
customer dimension data files.

586
00:29:04,230 --> 00:29:07,863
And then, we want Alex to
go and create an ETL job.

587
00:29:08,880 --> 00:29:10,170
And in that job,

588
00:29:10,170 --> 00:29:12,270
the activity that we want Alex to perform

589
00:29:12,270 --> 00:29:16,380
is join the two datasets
based on customer ID

590
00:29:16,380 --> 00:29:18,813
and aggregate the data based on state.

591
00:29:19,890 --> 00:29:23,610
And once that is done, of
course, we created the job.

592
00:29:23,610 --> 00:29:26,130
We want Alex to go and run that job.

593
00:29:26,130 --> 00:29:27,990
And it's not just about running,

594
00:29:27,990 --> 00:29:30,810
we want Alex to help us
with monitoring the job

595
00:29:30,810 --> 00:29:33,543
and fixing issues, if there
is anything coming up.

596
00:29:34,440 --> 00:29:36,483
And then we wanna catalog the output.

597
00:29:37,830 --> 00:29:40,230
After cataloging, we want
to do some data validation,

598
00:29:40,230 --> 00:29:41,610
because at the end of the day,

599
00:29:41,610 --> 00:29:43,950
anything that we do need to be validated.

600
00:29:43,950 --> 00:29:46,080
So we need to do some data validation.

601
00:29:46,080 --> 00:29:47,550
And once everything is done,

602
00:29:47,550 --> 00:29:49,380
if you remember the marketing team,

603
00:29:49,380 --> 00:29:52,110
the execs are really looking for the data

604
00:29:52,110 --> 00:29:53,490
in their data warehouse.

605
00:29:53,490 --> 00:29:56,370
So we want the data to be
loaded to data warehouse.

606
00:29:56,370 --> 00:30:00,120
So this is what the data engineering team

607
00:30:00,120 --> 00:30:02,363
at AnyCompany Retail has at hand.

608
00:30:02,363 --> 00:30:03,573
This is a task.

609
00:30:04,530 --> 00:30:05,943
So, let's take a look.

610
00:30:07,020 --> 00:30:08,430
As a first step,

611
00:30:08,430 --> 00:30:12,900
we'll take a look at how
to configure the MCP server

612
00:30:12,900 --> 00:30:14,130
in one of the tools, like Kiro.

613
00:30:14,130 --> 00:30:18,420
So I'm in Kiro, and this is
the lighter theme of Kiro.

614
00:30:18,420 --> 00:30:20,640
Typically, you might have
seen the darker theme there.

615
00:30:20,640 --> 00:30:23,220
So here, I have configured
multiple MCP servers,

616
00:30:23,220 --> 00:30:25,890
data processing, Redshift,

617
00:30:25,890 --> 00:30:28,200
and you'll see the various
tools that are listed.

618
00:30:28,200 --> 00:30:30,600
And you can also see
that other MCP servers

619
00:30:30,600 --> 00:30:31,920
that I've configured,

620
00:30:31,920 --> 00:30:34,570
you can either enable or
disable using a right click.

621
00:30:35,490 --> 00:30:39,630
Now, once you click on that
little icon icon over there,

622
00:30:39,630 --> 00:30:42,420
that's what will open up that JSON file.

623
00:30:42,420 --> 00:30:44,970
So it's a matter of just
adding that little block

624
00:30:44,970 --> 00:30:46,410
of JSON configuration,

625
00:30:46,410 --> 00:30:48,996
so that you can add any MCP server

626
00:30:48,996 --> 00:30:49,829
that you want to bring in.

627
00:30:49,829 --> 00:30:52,440
So that's the data processing MCP server.

628
00:30:52,440 --> 00:30:55,590
And here, you have S3 MCP server.

629
00:30:55,590 --> 00:30:57,000
Here's a Redshift one.

630
00:30:57,000 --> 00:30:58,920
So these are MCP servers
that I've configured.

631
00:30:58,920 --> 00:31:01,830
It's a matter of adding
that particular block.

632
00:31:01,830 --> 00:31:03,750
Once you're done, that's it.

633
00:31:03,750 --> 00:31:05,640
That's all the configuration
that you need to do.

634
00:31:05,640 --> 00:31:06,990
You are good to go

635
00:31:06,990 --> 00:31:09,753
and work with these services
using natural language.

636
00:31:12,030 --> 00:31:16,020
Now, this is the prompt
that we are giving to Alex,

637
00:31:16,020 --> 00:31:16,853
and let's not worry

638
00:31:16,853 --> 00:31:19,350
about all the things
that are written there.

639
00:31:19,350 --> 00:31:23,130
But the major things are, hey, Alex,

640
00:31:23,130 --> 00:31:28,130
you are a data engineering
agent and we need your help

641
00:31:28,680 --> 00:31:32,040
in identifying the dataset
and processing the data.

642
00:31:32,040 --> 00:31:35,220
But one thing here, human
and loop is important.

643
00:31:35,220 --> 00:31:37,590
So after every step,

644
00:31:37,590 --> 00:31:40,740
come and ask me if whatever was done

645
00:31:40,740 --> 00:31:42,630
in the previous step looks fine

646
00:31:42,630 --> 00:31:44,370
so that you can move forward.

647
00:31:44,370 --> 00:31:47,340
So that's a confirmation
that I'm expecting.

648
00:31:47,340 --> 00:31:50,970
So we are asking Alex
to come back and ask,

649
00:31:50,970 --> 00:31:53,040
and then move forward with the six steps

650
00:31:53,040 --> 00:31:54,810
that we just looked at.

651
00:31:54,810 --> 00:31:57,570
I want to go ahead and find the data,

652
00:31:57,570 --> 00:31:59,460
then create a Glue job.

653
00:31:59,460 --> 00:32:02,040
And there, we are becoming
very specific this time.

654
00:32:02,040 --> 00:32:04,350
We want to join based on customer_id,

655
00:32:04,350 --> 00:32:06,630
and I want to get the sum of page views

656
00:32:06,630 --> 00:32:10,233
and total purchase amount and
land the dataset in Parquet.

657
00:32:11,910 --> 00:32:14,850
And, of course, aggregation
is based on the state, right?

658
00:32:14,850 --> 00:32:19,850
And then run the job, fix any errors,

659
00:32:20,550 --> 00:32:22,350
and monitor the job.

660
00:32:22,350 --> 00:32:24,810
Then go and create a crawler
so that we can catalog them.

661
00:32:24,810 --> 00:32:27,540
We can, well, put that
in the same step itself,

662
00:32:27,540 --> 00:32:31,170
in the job creation itself
or job execution itself.

663
00:32:31,170 --> 00:32:33,300
Now, next one is validating the data

664
00:32:33,300 --> 00:32:34,920
using running the queries.

665
00:32:34,920 --> 00:32:35,940
And then finally,

666
00:32:35,940 --> 00:32:38,910
this time we want Alex
to create a notebook

667
00:32:38,910 --> 00:32:42,000
so that we can use that to
load the data into Redshift.

668
00:32:42,000 --> 00:32:43,290
There's a task at hand.

669
00:32:43,290 --> 00:32:46,470
Now, let's move to step one.

670
00:32:46,470 --> 00:32:50,160
So here, we are asking
Alex, "Hey, can you please,"

671
00:32:50,160 --> 00:32:52,590
these are all the steps
that are involved, right?

672
00:32:52,590 --> 00:32:55,140
So, "Hey, you are a
data engineering agent.

673
00:32:55,140 --> 00:32:56,070
These are all the six steps

674
00:32:56,070 --> 00:32:58,620
that we want you to go ahead and run.

675
00:32:58,620 --> 00:33:01,887
But make sure that come and
ask me after each step."

676
00:33:02,760 --> 00:33:04,350
Now, let's take a look.

677
00:33:04,350 --> 00:33:06,540
You can see that the very
first thing that it is doing

678
00:33:06,540 --> 00:33:10,143
is it's picking up an MCP
tool to list S3 buckets.

679
00:33:11,130 --> 00:33:15,810
And it is going and analyzing
the data that is there in S3

680
00:33:15,810 --> 00:33:18,780
because, again, in this particular case,

681
00:33:18,780 --> 00:33:19,860
we have specifically said

682
00:33:19,860 --> 00:33:23,370
that, hey, go and get the
data that is sitting in S3.

683
00:33:23,370 --> 00:33:25,028
Now, in your case, the
data might be sitting in,

684
00:33:25,028 --> 00:33:28,410
it could be in Redshift, it
could be in other places,

685
00:33:28,410 --> 00:33:30,630
or the data might be
cataloged in a data catalog.

686
00:33:30,630 --> 00:33:34,440
So you can use a respective
MCP server's help

687
00:33:34,440 --> 00:33:37,680
to go and scan for the
data, find where it is,

688
00:33:37,680 --> 00:33:39,870
and get that specific data.

689
00:33:39,870 --> 00:33:41,550
So it is able to find the data,

690
00:33:41,550 --> 00:33:43,860
but it is still trying to do something.

691
00:33:43,860 --> 00:33:44,880
You know why?

692
00:33:44,880 --> 00:33:47,220
Because we have asked in the next step,

693
00:33:47,220 --> 00:33:50,640
you need to go and join this
datasets based on customer_id

694
00:33:50,640 --> 00:33:53,190
and go and do the sum of page views

695
00:33:53,190 --> 00:33:54,750
and amount and things like that.

696
00:33:54,750 --> 00:33:57,480
So even though it is
able to find the dataset,

697
00:33:57,480 --> 00:33:59,347
it is still going and verifying,

698
00:33:59,347 --> 00:34:02,310
"Hey, do I have those fields in there?

699
00:34:02,310 --> 00:34:04,380
Because if I don't have those fields,

700
00:34:04,380 --> 00:34:06,120
I cannot really move on to the next step."

701
00:34:06,120 --> 00:34:07,620
So it is thinking ahead

702
00:34:07,620 --> 00:34:09,600
and making sure that those
fields are available.

703
00:34:09,600 --> 00:34:12,930
So it is able to find the
customer_id, customer_name, city,

704
00:34:12,930 --> 00:34:14,610
state, all of those details.

705
00:34:14,610 --> 00:34:17,700
So there you go, step 1 is complete.

706
00:34:17,700 --> 00:34:20,550
So you have the datasets, there's word is,

707
00:34:20,550 --> 00:34:23,790
and then you have the customer_id,
product_id, page views,

708
00:34:23,790 --> 00:34:25,200
all of those details.

709
00:34:25,200 --> 00:34:26,250
All good.

710
00:34:26,250 --> 00:34:30,780
Hey, I'm done with step 1,
can I proceed to step 2?

711
00:34:30,780 --> 00:34:33,900
So, cool, data discovery is done.

712
00:34:33,900 --> 00:34:35,100
Let's move to step 2.

713
00:34:35,100 --> 00:34:37,407
Let's say, "Yes, proceed to step 2."

714
00:34:39,720 --> 00:34:40,553
Okay.

715
00:34:40,553 --> 00:34:41,490
So this time,

716
00:34:41,490 --> 00:34:45,540
the expectation is Alex will
help us creating a Glue job.

717
00:34:45,540 --> 00:34:47,700
Now, to create a Glue job,

718
00:34:47,700 --> 00:34:51,150
first and foremost, if you
look at the first activity

719
00:34:51,150 --> 00:34:54,780
that it is trying to do is
using another MCP tool there

720
00:34:54,780 --> 00:34:56,013
to get the roles.

721
00:34:56,880 --> 00:34:59,160
What that really means is Alex knows

722
00:34:59,160 --> 00:35:03,480
that if it has to create a
job, it needs to understand

723
00:35:03,480 --> 00:35:06,360
that what is a role that
should be used with that job?

724
00:35:06,360 --> 00:35:08,280
Because the role should have access

725
00:35:08,280 --> 00:35:09,960
to the right resources, right?

726
00:35:09,960 --> 00:35:12,060
So it should have access
to the S3 datasets,

727
00:35:12,060 --> 00:35:15,300
it should have access to the
Glue job, Glue resources,

728
00:35:15,300 --> 00:35:16,260
all of those things.

729
00:35:16,260 --> 00:35:18,450
So it is able to get all the data.

730
00:35:18,450 --> 00:35:23,450
And it went ahead and
created a job, a .py file.

731
00:35:24,090 --> 00:35:25,890
So you can see the .py file.

732
00:35:25,890 --> 00:35:27,540
In a real-world scenario,

733
00:35:27,540 --> 00:35:30,600
we should ideally validate
some of these things

734
00:35:30,600 --> 00:35:32,550
before allowing it to run.

735
00:35:32,550 --> 00:35:35,550
So let's assume that
Alex is fine here, right?

736
00:35:35,550 --> 00:35:39,210
Now, it is asking, "Hey, can
I go and upload this to S3?"

737
00:35:39,210 --> 00:35:42,577
And we will talk about
why it is asking us,

738
00:35:42,577 --> 00:35:43,770
"Hey, can I go and upload this?"

739
00:35:43,770 --> 00:35:45,540
Some of them, it went ahead and did that.

740
00:35:45,540 --> 00:35:48,337
But some of the tools
it is coming and asking,

741
00:35:48,337 --> 00:35:49,860
"Hey, can I do this," right?

742
00:35:49,860 --> 00:35:52,500
We'll talk about that a little bit later.

743
00:35:52,500 --> 00:35:54,180
So here, you can see that, yes,

744
00:35:54,180 --> 00:35:58,200
it is using another operation,
which is create-job.

745
00:35:58,200 --> 00:36:00,540
So it made and created the job.

746
00:36:00,540 --> 00:36:01,373
All good.

747
00:36:01,373 --> 00:36:03,330
So it looks like the job
creation is complete,

748
00:36:03,330 --> 00:36:04,590
step 2 is done.

749
00:36:04,590 --> 00:36:06,150
So we have the...

750
00:36:06,150 --> 00:36:09,540
It's making sure that there
is a get-job run, get-job,

751
00:36:09,540 --> 00:36:12,150
to see whether the job
creation when successful.

752
00:36:12,150 --> 00:36:12,983
All good.

753
00:36:12,983 --> 00:36:14,580
So we have the job ready.

754
00:36:14,580 --> 00:36:16,950
Now, we can move on to step 3.

755
00:36:16,950 --> 00:36:21,950
So looks like job creation
is done, let's get to step 3.

756
00:36:22,410 --> 00:36:25,140
So this time, what we are hoping for

757
00:36:25,140 --> 00:36:30,000
is Alex will go and run
this job and monitor it

758
00:36:30,000 --> 00:36:32,550
and fix any error if
there is anything, right?

759
00:36:32,550 --> 00:36:35,490
So this time, it called another tool,

760
00:36:35,490 --> 00:36:39,180
which is manage AWS Glue jobs.

761
00:36:39,180 --> 00:36:41,580
And this time, it is
starting the job here.

762
00:36:41,580 --> 00:36:45,090
So again, think about the
way we would work, right?

763
00:36:45,090 --> 00:36:47,370
So it is thinking ahead

764
00:36:47,370 --> 00:36:50,670
and it is also going
that step-by-step process

765
00:36:50,670 --> 00:36:53,190
that we as data engineers would've done

766
00:36:53,190 --> 00:36:58,020
and giving us the assistance
of what we want to do.

767
00:36:58,020 --> 00:37:00,450
So since it started running the job,

768
00:37:00,450 --> 00:37:02,760
let's get to the Glue console

769
00:37:02,760 --> 00:37:04,470
just to see how things are happening.

770
00:37:04,470 --> 00:37:06,540
So let's go to Glue console.

771
00:37:06,540 --> 00:37:08,970
ETL and runs.

772
00:37:08,970 --> 00:37:12,180
So we can see that the
job is running now, right?

773
00:37:12,180 --> 00:37:15,300
So looks like it's happening.

774
00:37:15,300 --> 00:37:16,133
And if you remember,

775
00:37:16,133 --> 00:37:18,210
what we have told us, it's
not just about running the job

776
00:37:18,210 --> 00:37:19,770
and leaving it out there.

777
00:37:19,770 --> 00:37:22,740
As a data engineer, the agent,

778
00:37:22,740 --> 00:37:25,020
you want to go ahead and monitor it,

779
00:37:25,020 --> 00:37:29,670
so you can see that it is
trying to monitor the job.

780
00:37:29,670 --> 00:37:31,849
And it found that there's
a sleep time of...

781
00:37:31,849 --> 00:37:35,130
It's still running, so it put
a sleep time of 30 seconds.

782
00:37:35,130 --> 00:37:36,130
Now, think about it.

783
00:37:37,064 --> 00:37:40,440
As an end user, what you
may want to do is see,

784
00:37:40,440 --> 00:37:44,043
you may know that, hey,
this job may take one hour.

785
00:37:45,000 --> 00:37:46,080
You don't want the agent to go

786
00:37:46,080 --> 00:37:49,050
and monitor it every 30 seconds, right?

787
00:37:49,050 --> 00:37:50,220
So that is something,

788
00:37:50,220 --> 00:37:53,100
that is where human in the
loop is, again, important here.

789
00:37:53,100 --> 00:37:57,090
So you can guide the agent
what to do, what not to do.

790
00:37:57,090 --> 00:37:57,923
So you can say that,

791
00:37:57,923 --> 00:38:00,240
"Hey, go and monitor the
job after five minutes."

792
00:38:00,240 --> 00:38:03,870
But this is a smaller dataset,
so we let that happen.

793
00:38:03,870 --> 00:38:06,510
So, okay, go ahead and
check every 30 seconds.

794
00:38:06,510 --> 00:38:07,590
So that's okay.

795
00:38:07,590 --> 00:38:10,770
Now we can see that,
hey, the job was done,

796
00:38:10,770 --> 00:38:13,140
but it is able to, if you remember,

797
00:38:13,140 --> 00:38:15,690
we told that, hey, go and
check if there are any errors

798
00:38:15,690 --> 00:38:16,980
and fix it, right?

799
00:38:16,980 --> 00:38:19,230
So it found some error in the logs,

800
00:38:19,230 --> 00:38:20,707
and it is going and checking,

801
00:38:20,707 --> 00:38:23,100
"Hey, is there anything concerning?"

802
00:38:23,100 --> 00:38:24,450
So it went and checked,

803
00:38:24,450 --> 00:38:28,530
and found that, no, there
was an error that was fixed,

804
00:38:28,530 --> 00:38:29,880
so it went, moved forward.

805
00:38:29,880 --> 00:38:32,167
So it is again going back and validating,

806
00:38:32,167 --> 00:38:33,660
"Hey, is everything good?"

807
00:38:33,660 --> 00:38:36,000
So it did a list job Once again,

808
00:38:36,000 --> 00:38:37,650
just list job run once again

809
00:38:37,650 --> 00:38:39,360
to make sure that everything is fine.

810
00:38:39,360 --> 00:38:44,280
Looks like it succeeded and
it took around 83 seconds.

811
00:38:44,280 --> 00:38:45,480
And this is what is the result.

812
00:38:45,480 --> 00:38:48,630
So we have the output
saved in a Parquet format.

813
00:38:48,630 --> 00:38:49,500
Perfect.

814
00:38:49,500 --> 00:38:53,430
So the task force data
aggregation is done.

815
00:38:53,430 --> 00:38:55,590
Now, let's move on to our next one,

816
00:38:55,590 --> 00:38:59,280
which is cataloging it, right?

817
00:38:59,280 --> 00:39:00,780
So this time,

818
00:39:00,780 --> 00:39:04,650
we want Alex to help us with
cataloging this dataset.

819
00:39:04,650 --> 00:39:05,857
So for that, we are asking,

820
00:39:05,857 --> 00:39:09,720
"Hey, Alex, go and create
a crawler and catalog it."

821
00:39:09,720 --> 00:39:12,390
So this time, it is using the MCP tool,

822
00:39:12,390 --> 00:39:14,310
manage-aws-glue-databases,

823
00:39:14,310 --> 00:39:16,590
to find out which Glue database

824
00:39:16,590 --> 00:39:19,380
it can catalog that metadata.

825
00:39:19,380 --> 00:39:21,930
And once it finds that metadata,

826
00:39:21,930 --> 00:39:23,480
the next thing that it is doing

827
00:39:24,716 --> 00:39:28,560
is manage Glue crawler
and creating a crawler.

828
00:39:28,560 --> 00:39:31,380
So here, you can see that it
went and created a crawler,

829
00:39:31,380 --> 00:39:32,820
and it even started that.

830
00:39:32,820 --> 00:39:35,640
So think about how fast
it is going, right?

831
00:39:35,640 --> 00:39:37,890
So we would've clicked
through multiple screens,

832
00:39:37,890 --> 00:39:40,500
created the crawler, started the run.

833
00:39:40,500 --> 00:39:43,680
But by the time we explain
what it is able to do,

834
00:39:43,680 --> 00:39:44,640
you can see that in action.

835
00:39:44,640 --> 00:39:47,880
You can already see that
the crawler is running.

836
00:39:47,880 --> 00:39:51,570
So that is, that is what is
a productivity improvement

837
00:39:51,570 --> 00:39:54,060
that we can gain from these agents.

838
00:39:54,060 --> 00:39:57,360
So here, you can see that
the crawler is running.

839
00:39:57,360 --> 00:39:59,220
Now, if you remember,

840
00:39:59,220 --> 00:40:02,010
we told in the previous step that, hey,

841
00:40:02,010 --> 00:40:04,140
monitor my job, right?

842
00:40:04,140 --> 00:40:05,790
We did not specify anything

843
00:40:05,790 --> 00:40:07,920
when it comes to the Glue crawler.

844
00:40:07,920 --> 00:40:09,870
We do not specify
anything about monitoring.

845
00:40:09,870 --> 00:40:12,120
But it understands that whether it's a job

846
00:40:12,120 --> 00:40:14,190
that is running for data processing

847
00:40:14,190 --> 00:40:17,160
or a crawler that is
running for cataloging,

848
00:40:17,160 --> 00:40:18,150
it's all the same.

849
00:40:18,150 --> 00:40:19,680
The intent is same, right?

850
00:40:19,680 --> 00:40:21,960
So it is trying to monitor by itself

851
00:40:21,960 --> 00:40:24,510
and giving us an idea,
"Hey, is it running fine?"

852
00:40:24,510 --> 00:40:26,970
So you can see that there
are some sleep commands

853
00:40:26,970 --> 00:40:28,920
and all of those things
that it is putting,

854
00:40:28,920 --> 00:40:32,250
and it is validating that,
"Hey, is it running?"

855
00:40:32,250 --> 00:40:33,540
So you can see that now

856
00:40:33,540 --> 00:40:35,610
the crawler is in the stopping state.

857
00:40:35,610 --> 00:40:37,350
So it's validating again to see,

858
00:40:37,350 --> 00:40:40,200
another 15 seconds to
see if it got executed,

859
00:40:40,200 --> 00:40:41,730
if it is complete.

860
00:40:41,730 --> 00:40:45,900
So think about the time
that we would've spent

861
00:40:45,900 --> 00:40:47,520
in a lot of these activities.

862
00:40:47,520 --> 00:40:51,960
And what if you are going
into an important meeting,

863
00:40:51,960 --> 00:40:53,430
starting this pipeline,

864
00:40:53,430 --> 00:40:56,010
coming back and seeing the results, right?

865
00:40:56,010 --> 00:40:57,900
So a lot of these things can be done

866
00:40:57,900 --> 00:40:59,220
by the time you come back.

867
00:40:59,220 --> 00:41:02,943
So here, you can see that
yes, the task is complete.

868
00:41:03,870 --> 00:41:08,190
So you can see the new
crawler that got created

869
00:41:08,190 --> 00:41:10,140
and this is the database

870
00:41:10,140 --> 00:41:12,813
and that's a table that's cataloged.

871
00:41:13,830 --> 00:41:14,663
That's great.

872
00:41:14,663 --> 00:41:16,020
So let's move on.

873
00:41:16,020 --> 00:41:18,180
So we have our data cataloging done,

874
00:41:18,180 --> 00:41:19,800
next comes the validation.

875
00:41:19,800 --> 00:41:24,600
So we are requesting Alex,
"Hey, Alex, can you help us?"

876
00:41:24,600 --> 00:41:27,517
Let's, okay.

877
00:41:27,517 --> 00:41:29,850
"Alex, can you help us
validate this data?"

878
00:41:29,850 --> 00:41:30,683
And this time,

879
00:41:30,683 --> 00:41:33,960
we are just saying, "Validate
by running queries."

880
00:41:33,960 --> 00:41:38,960
We did not say what query,
which server to use,

881
00:41:39,240 --> 00:41:40,980
but this is where the intelligence

882
00:41:40,980 --> 00:41:42,330
comes into picture, right?

883
00:41:42,330 --> 00:41:45,870
So the LLM has an idea which...

884
00:41:45,870 --> 00:41:47,460
There are multiple tools
that are available.

885
00:41:47,460 --> 00:41:48,510
We have configured Redshift,

886
00:41:48,510 --> 00:41:50,970
we have configured data
processing MCP servers.

887
00:41:50,970 --> 00:41:52,890
So all of these tools are available.

888
00:41:52,890 --> 00:41:54,697
Now, LLM is able to decide that,

889
00:41:54,697 --> 00:41:58,470
"Hey, for me to go and
do this quick validation

890
00:41:58,470 --> 00:42:00,360
for the data that is sitting in S3,

891
00:42:00,360 --> 00:42:02,880
Athena seems to be a better choice."

892
00:42:02,880 --> 00:42:07,880
So it used that Athena query
MCP tool in data processing

893
00:42:08,220 --> 00:42:10,830
and went ahead and validated the data.

894
00:42:10,830 --> 00:42:14,250
Now, as a data engineer, if
we were asked to validate,

895
00:42:14,250 --> 00:42:15,960
would we just go with one query

896
00:42:15,960 --> 00:42:18,630
or we would validate
that from various angles?

897
00:42:18,630 --> 00:42:20,400
We would validate that
from multiple angles

898
00:42:20,400 --> 00:42:23,250
to make sure that our data
is data is good to go, right?

899
00:42:23,250 --> 00:42:25,950
That's exactly what Alex is also doing.

900
00:42:25,950 --> 00:42:29,730
So it is trying to validate
the data from multiple angles

901
00:42:29,730 --> 00:42:33,982
so that we don't have any
challenges in the data

902
00:42:33,982 --> 00:42:36,300
that is getting created for the next step.

903
00:42:36,300 --> 00:42:39,300
So looks like all the
validations are done.

904
00:42:39,300 --> 00:42:42,300
And it gives a good
amount of stats as well.

905
00:42:42,300 --> 00:42:45,840
So it has various the states.

906
00:42:45,840 --> 00:42:48,750
And it is also making sure
that all 50 states are,

907
00:42:48,750 --> 00:42:50,213
the data from all 50 states are there.

908
00:42:50,213 --> 00:42:52,260
This is what is the total page views,

909
00:42:52,260 --> 00:42:53,580
this is what is a total amount.

910
00:42:53,580 --> 00:42:58,170
So it gives a quick
validation of what can be done

911
00:42:58,170 --> 00:43:00,660
and making sure that the data is fine.

912
00:43:00,660 --> 00:43:01,493
Cool.

913
00:43:01,493 --> 00:43:03,540
So let's move on to the next one.

914
00:43:03,540 --> 00:43:05,220
So data validation is complete.

915
00:43:05,220 --> 00:43:07,470
Our final step is to go and load this data

916
00:43:07,470 --> 00:43:09,483
into our Redshift.

917
00:43:10,920 --> 00:43:12,727
For that, we are asking Alex,

918
00:43:12,727 --> 00:43:15,840
"Hey, can you go and do
load this to Redshift,

919
00:43:15,840 --> 00:43:19,020
or create a notebook to
load this to Redshift?"

920
00:43:19,020 --> 00:43:22,140
So here, you see that
at this point in time,

921
00:43:22,140 --> 00:43:24,900
it immediately use a Redshift MCP server.

922
00:43:24,900 --> 00:43:28,200
Until now, it has been
using other MCP servers.

923
00:43:28,200 --> 00:43:29,033
The moment we said,

924
00:43:29,033 --> 00:43:31,410
"Hey, I want to load
this data into Redshift,

925
00:43:31,410 --> 00:43:34,680
it used a Redshift MCP
server to list the clusters."

926
00:43:34,680 --> 00:43:36,300
So it is identifying which cluster

927
00:43:36,300 --> 00:43:39,510
because our prompt says go and load this

928
00:43:39,510 --> 00:43:41,670
to analytics Redshift cluster.

929
00:43:41,670 --> 00:43:43,380
So it identified the cluster,

930
00:43:43,380 --> 00:43:45,120
and it is creating a notebook

931
00:43:45,120 --> 00:43:47,640
with all the various
steps that are involved.

932
00:43:47,640 --> 00:43:50,460
And you'll be able to see the notebook

933
00:43:50,460 --> 00:43:54,480
that is getting created right
away in the Kiro IDE itself.

934
00:43:54,480 --> 00:43:58,050
Now, think about the real
challenge that we have.

935
00:43:58,050 --> 00:44:00,630
I think when Harshida was
mentioning about the company

936
00:44:00,630 --> 00:44:03,210
that the challenge that any company had,

937
00:44:03,210 --> 00:44:05,970
one of the challenges
were documentation, right?

938
00:44:05,970 --> 00:44:06,900
And we all face,

939
00:44:06,900 --> 00:44:08,250
we all love to code,

940
00:44:08,250 --> 00:44:10,410
we all love to build the data pipelines,

941
00:44:10,410 --> 00:44:13,680
but we all struggle a bit
to document it well, right?

942
00:44:13,680 --> 00:44:15,570
And that's a challenge that we all have.

943
00:44:15,570 --> 00:44:17,310
And that becomes even more challenging

944
00:44:17,310 --> 00:44:20,430
when we acquire a set of data pipelines

945
00:44:20,430 --> 00:44:23,460
or we transition over to another team

946
00:44:23,460 --> 00:44:25,830
and having to know what is being done.

947
00:44:25,830 --> 00:44:27,420
So if you look at this one,

948
00:44:27,420 --> 00:44:31,740
you see a very detailed notebook

949
00:44:31,740 --> 00:44:34,590
that has a step-by-step
process on how to load,

950
00:44:34,590 --> 00:44:36,720
how to make the connection,
what is a step to load,

951
00:44:36,720 --> 00:44:38,820
validate the data, all of those details,

952
00:44:38,820 --> 00:44:40,500
and some cleanup at the end.

953
00:44:40,500 --> 00:44:41,610
And on the left-hand side,

954
00:44:41,610 --> 00:44:44,280
you can also see a README file,

955
00:44:44,280 --> 00:44:48,420
so that it, by defaults,
create a documentation for you

956
00:44:48,420 --> 00:44:50,370
so that that can be passed on

957
00:44:50,370 --> 00:44:53,250
to any other team member who
is coming into your team.

958
00:44:53,250 --> 00:44:56,580
So, it's creating that the README file

959
00:44:56,580 --> 00:44:57,630
at this point in time.

960
00:44:57,630 --> 00:45:00,690
So that is how you can use these agents

961
00:45:00,690 --> 00:45:03,690
in your day-to-day work when
you do data engineering.

962
00:45:03,690 --> 00:45:06,843
Now, with all of these things,

963
00:45:08,040 --> 00:45:09,630
so we can say that, all right,

964
00:45:09,630 --> 00:45:13,410
so Alex is able to help us
with each one of the activities

965
00:45:13,410 --> 00:45:14,913
that we have asked Alex to do.

966
00:45:15,960 --> 00:45:20,880
Now, in this whole process,
AnyCompany has learned,

967
00:45:20,880 --> 00:45:22,890
had some good learnings as well, right?

968
00:45:22,890 --> 00:45:24,030
So let's talk about that.

969
00:45:24,030 --> 00:45:25,680
Let's talk about some of those things.

970
00:45:25,680 --> 00:45:27,210
One thing that you may
want to keep in mind

971
00:45:27,210 --> 00:45:31,020
is we talked about
configuring multiple services,

972
00:45:31,020 --> 00:45:33,960
multiple servers, MCP servers, right?

973
00:45:33,960 --> 00:45:38,400
Now, each one of these MCP
servers come with various tools.

974
00:45:38,400 --> 00:45:43,020
And it is the LLM, which is
deciding which tool to use

975
00:45:43,020 --> 00:45:47,370
for a particular task that
user is asking the agent to do.

976
00:45:47,370 --> 00:45:51,510
Now, how will the LLM
decide which tool to pick?

977
00:45:51,510 --> 00:45:53,640
It scans through the various tools

978
00:45:53,640 --> 00:45:55,740
and identify what is the best one to pick.

979
00:45:56,610 --> 00:45:59,670
It is always good to, if
you are a data engineer,

980
00:45:59,670 --> 00:46:02,280
you have the control to configure, enable,

981
00:46:02,280 --> 00:46:04,350
or disable the various MCP servers.

982
00:46:04,350 --> 00:46:09,350
It is strongly recommended
to work with the MCP servers

983
00:46:09,720 --> 00:46:11,190
or the number of tools

984
00:46:11,190 --> 00:46:13,980
that you feel that would be more relevant.

985
00:46:13,980 --> 00:46:16,500
Because if you have hundreds
and thousands of tools

986
00:46:16,500 --> 00:46:17,403
that are listed,

987
00:46:18,360 --> 00:46:20,550
your LLM will have to scan through that

988
00:46:20,550 --> 00:46:22,800
and figure out what is
the best one to use.

989
00:46:22,800 --> 00:46:26,940
So you are putting a little
bit more work on the LLM.

990
00:46:26,940 --> 00:46:29,160
So you may want to, as a data engineer,

991
00:46:29,160 --> 00:46:30,360
you might have a good idea.

992
00:46:30,360 --> 00:46:32,310
These are all the MP
servers that I want to use.

993
00:46:32,310 --> 00:46:35,160
So stick to the tools
that you want to use.

994
00:46:35,160 --> 00:46:36,240
That's one thing.

995
00:46:36,240 --> 00:46:39,030
And it's pretty easy to do
that in each one of these IDEs.

996
00:46:39,030 --> 00:46:41,130
For example, SageMaker Unified Studio,

997
00:46:41,130 --> 00:46:45,960
you can either add a MCP
server with a couple of clicks.

998
00:46:45,960 --> 00:46:48,960
In case of Kiro, we saw
that it's a right click

999
00:46:48,960 --> 00:46:49,860
and enabled disabled.

1000
00:46:49,860 --> 00:46:51,240
You can go and do that.

1001
00:46:51,240 --> 00:46:53,280
OpenSearch has a very
interesting way of doing that.

1002
00:46:53,280 --> 00:46:55,410
There's a tool filter
that you can provide.

1003
00:46:55,410 --> 00:46:58,350
You can say that, "Hey, I
want to have MCP servers

1004
00:46:58,350 --> 00:47:00,690
loaded only for this particular category,

1005
00:47:00,690 --> 00:47:03,090
only or this specific task."

1006
00:47:03,090 --> 00:47:06,150
So you can be very specific
in the list of tools

1007
00:47:06,150 --> 00:47:07,290
that are getting loaded.

1008
00:47:07,290 --> 00:47:10,170
So something that you
may want to keep in mind.

1009
00:47:10,170 --> 00:47:12,690
The next one is on the
protection of your data

1010
00:47:12,690 --> 00:47:14,100
and infrastructure, right?

1011
00:47:14,100 --> 00:47:17,280
So each one of the MCP
servers has some good levers

1012
00:47:17,280 --> 00:47:19,320
that you must be aware of.

1013
00:47:19,320 --> 00:47:21,420
And that's something
that we highly recommend

1014
00:47:21,420 --> 00:47:22,830
you all to be aware of.

1015
00:47:22,830 --> 00:47:26,820
For example, data processing
MCP server, by default,

1016
00:47:26,820 --> 00:47:29,610
it doesn't allow you to go and
create some of the resources

1017
00:47:29,610 --> 00:47:31,740
or write the data back
and things like that.

1018
00:47:31,740 --> 00:47:33,030
It is designed like that.

1019
00:47:33,030 --> 00:47:36,180
So you can go ahead and put allow write

1020
00:47:36,180 --> 00:47:38,130
to make sure that your MCP server

1021
00:47:38,130 --> 00:47:40,440
is able to do those activities.

1022
00:47:40,440 --> 00:47:42,600
Whereas in case of Redshift MCP server,

1023
00:47:42,600 --> 00:47:44,040
it's a read-only mode.

1024
00:47:44,040 --> 00:47:47,250
So it is intended to consume
the data from Redshift

1025
00:47:47,250 --> 00:47:48,420
at this point in time.

1026
00:47:48,420 --> 00:47:49,950
And same is the case with OpenSearch,

1027
00:47:49,950 --> 00:47:52,563
it is intended to consume
the data from OpenSearch.

1028
00:47:53,640 --> 00:47:56,403
When it comes to MSK MCP server,

1029
00:47:57,360 --> 00:47:58,890
the persona is slightly different

1030
00:47:58,890 --> 00:48:02,070
because that's a control plane
more than the data plane.

1031
00:48:02,070 --> 00:48:05,490
The MSK MCP server is
more for a control plane.

1032
00:48:05,490 --> 00:48:06,960
It's for an admin persona.

1033
00:48:06,960 --> 00:48:09,120
For example, creating a cluster,

1034
00:48:09,120 --> 00:48:11,430
or changing the cluster
configuration, and things like that.

1035
00:48:11,430 --> 00:48:14,724
That's why the modifications
are, by default, on

1036
00:48:14,724 --> 00:48:16,650
in case of MSK cluster.

1037
00:48:16,650 --> 00:48:19,230
But you have an option
to go and turn that off.

1038
00:48:19,230 --> 00:48:22,500
So please be aware, for each
one of these MCP server,

1039
00:48:22,500 --> 00:48:25,170
there are some good
levers that you can use

1040
00:48:25,170 --> 00:48:26,970
to protect your data and infrastructure.

1041
00:48:26,970 --> 00:48:28,320
So please make use of that.

1042
00:48:29,471 --> 00:48:31,380
And the next one is trusting the tools.

1043
00:48:31,380 --> 00:48:33,450
And this is what I was mentioning earlier.

1044
00:48:33,450 --> 00:48:36,900
So you might have noticed
that in some cases,

1045
00:48:36,900 --> 00:48:39,630
the agent was asking, "Hey,
can I go and run this one?"

1046
00:48:39,630 --> 00:48:43,770
In some cases, it went ahead
and run, executed those tools.

1047
00:48:43,770 --> 00:48:45,090
Why did that happen?

1048
00:48:45,090 --> 00:48:49,290
That is because I have
trusted some of the tools.

1049
00:48:49,290 --> 00:48:52,923
For example, let's say,
I want to run a query.

1050
00:48:53,970 --> 00:48:57,494
If it is going against
my production database,

1051
00:48:57,494 --> 00:48:59,340
I wanna see that first.

1052
00:48:59,340 --> 00:49:00,690
I wanna see that first.

1053
00:49:00,690 --> 00:49:03,900
Or if I'm uploading
something to my S3 bucket,

1054
00:49:03,900 --> 00:49:05,910
I wanna see what is getting uploaded.

1055
00:49:05,910 --> 00:49:09,090
So it depends on what
is your comfort level,

1056
00:49:09,090 --> 00:49:11,730
which environment you
are working on, right?

1057
00:49:11,730 --> 00:49:16,590
So depending on what are the
tools that you feel comfortable

1058
00:49:16,590 --> 00:49:17,490
that it can go and,

1059
00:49:17,490 --> 00:49:19,770
that agent can directly go and run

1060
00:49:19,770 --> 00:49:21,990
versus something that you want

1061
00:49:21,990 --> 00:49:23,850
a confirmation from your side,

1062
00:49:23,850 --> 00:49:26,790
you can configure those
in these various tools.

1063
00:49:26,790 --> 00:49:28,950
The left-hand side is
SageMaker Unified Studio.

1064
00:49:28,950 --> 00:49:32,190
So you can say that, "Hey,
for this particular tool,

1065
00:49:32,190 --> 00:49:34,650
ask me, or always allow."

1066
00:49:34,650 --> 00:49:37,890
In case of Kiro, that's where
that block comes into picture,

1067
00:49:37,890 --> 00:49:41,594
where you can put a list
of auto-approve tools,

1068
00:49:41,594 --> 00:49:45,510
so that it'll automatically be executed

1069
00:49:45,510 --> 00:49:48,153
when if the LLM decides
to make use of that tool.

1070
00:49:49,770 --> 00:49:52,950
Now, with all of that, no matter what,

1071
00:49:52,950 --> 00:49:54,420
human and loop is important.

1072
00:49:54,420 --> 00:49:57,330
We cannot emphasize this enough, right?

1073
00:49:57,330 --> 00:50:00,870
So please understand that this is,

1074
00:50:00,870 --> 00:50:03,270
the agent is there to help,

1075
00:50:03,270 --> 00:50:07,110
and it's a consider agent
as a peer programmer, right?

1076
00:50:07,110 --> 00:50:10,020
The human in the loop is important,

1077
00:50:10,020 --> 00:50:11,940
whether it's read versus write decisions

1078
00:50:11,940 --> 00:50:13,770
or what tool is being used.

1079
00:50:13,770 --> 00:50:16,530
Or even the code that
it is getting generated

1080
00:50:16,530 --> 00:50:19,140
or the query that it is getting generated,

1081
00:50:19,140 --> 00:50:20,580
you may want to validate that.

1082
00:50:20,580 --> 00:50:24,840
And also there are MCP
servers do have options

1083
00:50:24,840 --> 00:50:26,400
to prevent some of these runaway queries

1084
00:50:26,400 --> 00:50:27,233
and things like that.

1085
00:50:27,233 --> 00:50:29,340
But human in the loop is
important there as well.

1086
00:50:29,340 --> 00:50:30,740
So please keep that in mind.

1087
00:50:31,890 --> 00:50:36,660
With all that said, we have
looked at all of these pillars

1088
00:50:36,660 --> 00:50:39,660
where we see that contact
switching, data quality,

1089
00:50:39,660 --> 00:50:43,110
error handling, resource
management, best practices.

1090
00:50:43,110 --> 00:50:46,860
The pillars that any company
data engineers mentioned,

1091
00:50:46,860 --> 00:50:49,230
the pillars of loss of productivity,

1092
00:50:49,230 --> 00:50:51,390
that's been very well-addressed.

1093
00:50:51,390 --> 00:50:53,910
We are not saying that all
of these things will be gone,

1094
00:50:53,910 --> 00:50:56,220
but it'll be improved,
or it'll be reduced.

1095
00:50:56,220 --> 00:50:59,370
These productive productivity
losses will be reduced

1096
00:50:59,370 --> 00:51:01,320
with agentic capabilities.

1097
00:51:01,320 --> 00:51:04,500
Now, that decision,

1098
00:51:04,500 --> 00:51:06,780
here are some of the
results from AnyCompany.

1099
00:51:06,780 --> 00:51:09,780
So it is saving a lot of time for sure.

1100
00:51:09,780 --> 00:51:11,040
It is productive.

1101
00:51:11,040 --> 00:51:13,290
And it is easy to use,

1102
00:51:13,290 --> 00:51:15,780
very simple options to configure, right?

1103
00:51:15,780 --> 00:51:17,700
And at the end of the
day, yes, it is safe.

1104
00:51:17,700 --> 00:51:21,240
There are good number of
flavors to keep everything safe.

1105
00:51:21,240 --> 00:51:25,500
Now, yes, it's really good
to have a nice POCN pilot.

1106
00:51:25,500 --> 00:51:27,033
How will we scale it?

1107
00:51:27,870 --> 00:51:28,890
In case of a data engineering,

1108
00:51:28,890 --> 00:51:31,140
it's not really about
taking it to production,

1109
00:51:31,140 --> 00:51:32,670
rather it is about scaling it

1110
00:51:32,670 --> 00:51:36,360
and making sure that there's
a scalable option out there.

1111
00:51:36,360 --> 00:51:39,150
How will you deploy these, right?

1112
00:51:39,150 --> 00:51:41,700
So if you have...

1113
00:51:41,700 --> 00:51:44,340
If you're just starting with
agentic data engineering,

1114
00:51:44,340 --> 00:51:47,070
I would strongly recommend you to get

1115
00:51:47,070 --> 00:51:49,680
to SageMaker Unified Studio notebooks

1116
00:51:49,680 --> 00:51:52,680
and try out the new
features that was released,

1117
00:51:52,680 --> 00:51:54,450
which is a Data Agent.

1118
00:51:54,450 --> 00:51:58,200
So here, you have an
agentic AI assistance agent

1119
00:51:58,200 --> 00:52:01,440
that is inbuilt in
SageMaker Unified Studio,

1120
00:52:01,440 --> 00:52:03,000
which can discover the data,

1121
00:52:03,000 --> 00:52:06,000
which can create the various
steps that is involved

1122
00:52:06,000 --> 00:52:08,100
for the activities that you want to do

1123
00:52:08,100 --> 00:52:10,080
to generate the SQLs or Python code,

1124
00:52:10,080 --> 00:52:12,570
or troubleshooting any of the error

1125
00:52:12,570 --> 00:52:13,560
that you are getting into.

1126
00:52:13,560 --> 00:52:17,100
Pretty much whatever we just
saw what Alex was doing,

1127
00:52:17,100 --> 00:52:19,530
that's already in built in
SageMaker Unified Studio.

1128
00:52:19,530 --> 00:52:21,390
So please try that out.

1129
00:52:21,390 --> 00:52:23,430
Now, there are a lot of customers

1130
00:52:23,430 --> 00:52:27,210
who would want to have a
lot more control and lever.

1131
00:52:27,210 --> 00:52:29,880
For example, hey, I have
a third-party service

1132
00:52:29,880 --> 00:52:32,820
or third-party data source,
and how do I connect to that?

1133
00:52:32,820 --> 00:52:35,640
That is where this architecture
that we looked at earlier

1134
00:52:35,640 --> 00:52:36,720
comes into picture.

1135
00:52:36,720 --> 00:52:40,470
So it's not just about the MCP
servers that are listed here.

1136
00:52:40,470 --> 00:52:42,900
Depending on the various data sources

1137
00:52:42,900 --> 00:52:43,860
that you want to connect,

1138
00:52:43,860 --> 00:52:46,230
you can bring any of these MCP servers.

1139
00:52:46,230 --> 00:52:48,540
Now, if you have, let's
say, a 10-member team,

1140
00:52:48,540 --> 00:52:53,540
it is very much okay to have
each one of your developers

1141
00:52:54,000 --> 00:52:56,910
have the respective, let's say,
all of them are using Kiro,

1142
00:52:56,910 --> 00:53:00,300
them going and configuring that mcp.json

1143
00:53:00,300 --> 00:53:02,340
and working with these MCP servers.

1144
00:53:02,340 --> 00:53:04,260
It could be the same set of MCP servers,

1145
00:53:04,260 --> 00:53:06,120
it could be different set of MCP servers.

1146
00:53:06,120 --> 00:53:07,080
That's perfectly fine.

1147
00:53:07,080 --> 00:53:09,210
That's a widely accepted pattern.

1148
00:53:09,210 --> 00:53:11,700
Now, there are cases

1149
00:53:11,700 --> 00:53:15,270
where you may want to
have an agent deployed

1150
00:53:15,270 --> 00:53:17,490
and roll it out to the whole organization.

1151
00:53:17,490 --> 00:53:18,930
How would you do that?

1152
00:53:18,930 --> 00:53:20,820
That's where AgentCore,

1153
00:53:20,820 --> 00:53:22,770
Bedrock AgentCore comes into picture.

1154
00:53:22,770 --> 00:53:23,603
And in this session,

1155
00:53:23,603 --> 00:53:27,660
we will not be doing a deep
dive on AgentCore per se.

1156
00:53:27,660 --> 00:53:29,760
But just to give you an example,

1157
00:53:29,760 --> 00:53:31,770
or give you a little bit more details.

1158
00:53:31,770 --> 00:53:36,770
So here, Bedrock AgentCore
has multiple components to it.

1159
00:53:36,780 --> 00:53:40,260
The Runtime helps you to deploy an agent.

1160
00:53:40,260 --> 00:53:43,050
And the one on the left-hand side,

1161
00:53:43,050 --> 00:53:44,640
which is the AgentCore Memory,

1162
00:53:44,640 --> 00:53:46,590
helps you to keep track
of the conversations

1163
00:53:46,590 --> 00:53:47,460
that you're having.

1164
00:53:47,460 --> 00:53:49,440
So that's what is AgentCore Memory does.

1165
00:53:49,440 --> 00:53:52,260
Similarly, if you are
working with an agent,

1166
00:53:52,260 --> 00:53:54,870
we may want to know
everything that is happening.

1167
00:53:54,870 --> 00:53:56,430
So traceability is important.

1168
00:53:56,430 --> 00:53:57,660
So that's where the Observability

1169
00:53:57,660 --> 00:53:59,261
comes into the picture, right?

1170
00:53:59,261 --> 00:54:02,070
AgentCore Observability
will give you those details.

1171
00:54:02,070 --> 00:54:04,110
And AgentCore Identity will help you

1172
00:54:04,110 --> 00:54:05,733
to do the user authentication.

1173
00:54:06,630 --> 00:54:09,810
Now, Gateway, the crux of it,

1174
00:54:09,810 --> 00:54:12,690
is we talked about the
various MCP servers.

1175
00:54:12,690 --> 00:54:14,250
Now you have an option to go ahead

1176
00:54:14,250 --> 00:54:17,370
and configure various MCP
servers using Gateway,

1177
00:54:17,370 --> 00:54:22,260
so that when the LLM gets
a request from the user,

1178
00:54:22,260 --> 00:54:24,810
it can go to Gateway, find out,

1179
00:54:24,810 --> 00:54:26,820
out of the whole list of tools,

1180
00:54:26,820 --> 00:54:29,610
what is the tool that it wants to use

1181
00:54:29,610 --> 00:54:31,350
for the next best action.

1182
00:54:31,350 --> 00:54:32,640
So, this is an architecture.

1183
00:54:32,640 --> 00:54:34,410
And you can see other tools

1184
00:54:34,410 --> 00:54:36,480
that are getting added
to the Gateway as well.

1185
00:54:36,480 --> 00:54:38,400
So you may have some custom tools

1186
00:54:38,400 --> 00:54:40,290
that you have built in your organization,

1187
00:54:40,290 --> 00:54:41,670
which you wanna bring in as well.

1188
00:54:41,670 --> 00:54:44,670
So AgentCore Gateway helps
you to do that as well.

1189
00:54:44,670 --> 00:54:47,040
So this is an architecture
that you can use

1190
00:54:47,040 --> 00:54:48,210
if you want to scale it out.

1191
00:54:48,210 --> 00:54:50,100
Or if you have a very specific agent,

1192
00:54:50,100 --> 00:54:51,240
let's say, data analysis agent

1193
00:54:51,240 --> 00:54:54,600
or data engineering agent that
you want to bring or deploy

1194
00:54:54,600 --> 00:54:57,450
and roll it out to your
entire organization.

1195
00:54:57,450 --> 00:55:02,450
Now, with all that said,
going back to Sarah, our CTO,

1196
00:55:03,900 --> 00:55:08,730
she feels that, yes,
everything looks good.

1197
00:55:08,730 --> 00:55:10,110
The pilot went very well.

1198
00:55:10,110 --> 00:55:11,460
The results are good.

1199
00:55:11,460 --> 00:55:12,840
And at the same time,

1200
00:55:12,840 --> 00:55:17,070
she clearly see a path forward
to deploy this at scale.

1201
00:55:17,070 --> 00:55:19,380
And the recommendation was to move forward

1202
00:55:19,380 --> 00:55:22,260
and fully roll out the agentic capability

1203
00:55:22,260 --> 00:55:23,850
to all the work streams.

1204
00:55:23,850 --> 00:55:25,110
Now, we really hope

1205
00:55:25,110 --> 00:55:27,870
that you'll be able to
take these learnings

1206
00:55:27,870 --> 00:55:29,880
and implement some of these things

1207
00:55:29,880 --> 00:55:32,520
in your day-to-day activities
in your organization.

1208
00:55:32,520 --> 00:55:34,950
And we want to leave
you with some resources.

1209
00:55:34,950 --> 00:55:36,500
Here are some of the resources.

1210
00:55:37,410 --> 00:55:40,350
The first one is on the
list of MCP servers.

1211
00:55:40,350 --> 00:55:43,110
We have a couple of
blogs on data processing

1212
00:55:43,110 --> 00:55:45,090
and Redshift MCP blogs.

1213
00:55:45,090 --> 00:55:49,230
And the one on the bottom,
the left, is the data agent,

1214
00:55:49,230 --> 00:55:53,400
the SageMaker Unified Studio AI agent.

1215
00:55:53,400 --> 00:55:56,403
And we have a QR code for
the AgentCore as well.

1216
00:55:57,330 --> 00:55:58,163
Cool.

1217
00:55:58,163 --> 00:55:59,490
And at Amazon,

1218
00:55:59,490 --> 00:56:03,240
we strongly feel that we
should always keep learning

1219
00:56:03,240 --> 00:56:04,710
and be curious.

1220
00:56:04,710 --> 00:56:08,073
And here is a way to
learn more about agents.

1221
00:56:08,910 --> 00:56:11,853
Hope you'll be able to learn
more about agents as well.

1222
00:56:12,960 --> 00:56:15,450
And thank you.

1223
00:56:15,450 --> 00:56:20,190
And please, share what you
thought about the session.

1224
00:56:20,190 --> 00:56:21,780
Please fill in your survey

1225
00:56:21,780 --> 00:56:23,460
because we really care about that.

1226
00:56:23,460 --> 00:56:25,560
And we'll be around to talk to you

1227
00:56:25,560 --> 00:56:28,410
on any questions that you may have,

1228
00:56:28,410 --> 00:56:29,400
because as a silent session,

1229
00:56:29,400 --> 00:56:31,500
we may not be able to
take the live questions,

1230
00:56:31,500 --> 00:56:33,303
but we'll be around to talk to you.

1231
00:56:35,705 --> 00:56:36,755
All right, thank you.

