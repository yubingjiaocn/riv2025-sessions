1
00:00:06,110 --> 00:00:07,809
Today I'm standing.

2
00:00:09,890 --> 00:00:12,349
I live in a city that vibrates with life.

3
00:00:14,589 --> 00:00:15,569
A city of art,

4
00:00:16,318 --> 00:00:18,548
history, science, and projects.

5
00:00:23,379 --> 00:00:24,318
Projects.

6
00:00:24,899 --> 00:00:26,298
I have projects.

7
00:00:33,789 --> 00:00:35,789
To turn them into reality, I need to

8
00:00:35,789 --> 00:00:36,490
get out

9
00:00:36,829 --> 00:00:38,450
and immerse myself in the city.

10
00:00:41,090 --> 00:00:43,048
I need to see, sense,

11
00:00:43,368 --> 00:00:44,609
breathe, connect.

12
00:00:48,950 --> 00:00:50,709
That's what changed my life.

13
00:00:53,270 --> 00:00:55,649
Being able to stand or sit at will,

14
00:00:56,270 --> 00:00:57,069
to stride.

15
00:00:57,978 --> 00:00:58,908
Blend with people.

16
00:01:00,450 --> 00:01:01,899
Wander with my friends.

17
00:01:02,759 --> 00:01:03,978
And talk with them

18
00:01:04,278 --> 00:01:04,838
at eye level.

19
00:01:15,698 --> 00:01:16,599
3 years back,

20
00:01:16,859 --> 00:01:19,019
I met Nicola, who offered, how

21
00:01:19,019 --> 00:01:21,439
would you like to stand up and go for a walk?

22
00:01:23,418 --> 00:01:25,219
At first, I did not believe him.

23
00:01:26,969 --> 00:01:27,510
But

24
00:01:27,859 --> 00:01:30,219
today, I'm standing.

25
00:01:54,739 --> 00:01:55,418
Wow,

26
00:01:55,939 --> 00:01:58,079
what an incredibly inspiring example

27
00:01:58,079 --> 00:01:59,379
of physical AI.

28
00:02:00,079 --> 00:02:02,689
Hi everyone and welcome to AM 117S,

29
00:02:02,769 --> 00:02:04,808
which is all about building physical

30
00:02:04,808 --> 00:02:05,388
AI

31
00:02:05,930 --> 00:02:07,888
with Nvidia on AWS.

32
00:02:08,679 --> 00:02:10,838
I'm Sean Kirby and I'm a principal cloud

33
00:02:10,838 --> 00:02:13,069
architect in our AWS professional

34
00:02:13,069 --> 00:02:14,419
services organization,

35
00:02:14,800 --> 00:02:17,038
and I'm joined today by my colleagues Ali

36
00:02:17,038 --> 00:02:18,778
Shahroni from Nvidia

37
00:02:19,240 --> 00:02:22,169
and Abhishek Shrivasta also from AWS.

38
00:02:22,719 --> 00:02:24,800
We have a fun-filled and packed agenda for

39
00:02:24,800 --> 00:02:25,580
you here today.

40
00:02:25,838 --> 00:02:28,139
I wanted to start off with just a quick survey

41
00:02:28,319 --> 00:02:30,599
show of hands, how many of you already have

42
00:02:30,599 --> 00:02:32,979
physical AI programs underway.

43
00:02:33,758 --> 00:02:34,479
Any folks?

44
00:02:34,879 --> 00:02:36,490
OK, I see a couple of hands there.

45
00:02:36,889 --> 00:02:39,008
Another show of hands, how many people are

46
00:02:39,008 --> 00:02:41,129
considering launching into a physical

47
00:02:41,129 --> 00:02:43,368
AI program in the next 6 months

48
00:02:43,368 --> 00:02:43,969
or so?

49
00:02:45,250 --> 00:02:46,750
OK, some more hands. Well,

50
00:02:47,050 --> 00:02:49,139
we've got something for everyone here today. We're

51
00:02:49,139 --> 00:02:51,258
going to start off with a survey of physical

52
00:02:51,258 --> 00:02:53,538
AI, what it is, and the tremendous

53
00:02:53,538 --> 00:02:54,159
potential

54
00:02:54,419 --> 00:02:56,419
that it offers. We're then going to talk

55
00:02:56,419 --> 00:02:58,599
about what we view as the five key

56
00:02:58,599 --> 00:02:59,399
pillars

57
00:02:59,699 --> 00:03:01,699
for accelerating your physical

58
00:03:01,699 --> 00:03:03,699
AI programs, how to get started and

59
00:03:03,699 --> 00:03:05,778
how to meet some of the challenges that you may be

60
00:03:05,778 --> 00:03:06,379
facing.

61
00:03:07,159 --> 00:03:09,569
We'll talk about a number of proven technologies

62
00:03:09,569 --> 00:03:11,758
from Nvidia on AWS

63
00:03:11,758 --> 00:03:13,618
that will help you with your programs.

64
00:03:14,118 --> 00:03:15,139
We'll then go into

65
00:03:15,639 --> 00:03:16,939
a particular example

66
00:03:17,439 --> 00:03:19,439
where we've used these very architecture

67
00:03:19,439 --> 00:03:21,679
patterns and these technologies to

68
00:03:21,679 --> 00:03:24,219
create a humanoid stockkeeping robot

69
00:03:24,599 --> 00:03:26,679
and we'll include many of the best practices and

70
00:03:26,679 --> 00:03:27,699
lessons learned

71
00:03:27,960 --> 00:03:29,558
that we've gathered along the way.

72
00:03:31,879 --> 00:03:33,199
So first of all,

73
00:03:33,558 --> 00:03:35,199
what is physical AI?

74
00:03:36,360 --> 00:03:38,479
Well, put simply, physical AI

75
00:03:38,479 --> 00:03:41,240
is the embodiment of artificial intelligence

76
00:03:41,240 --> 00:03:43,558
in any physical object or smart

77
00:03:43,558 --> 00:03:45,929
object. Probably most of us

78
00:03:45,929 --> 00:03:48,088
think of robots when we think of physical

79
00:03:48,088 --> 00:03:50,330
AI, and that's certainly one of the greatest

80
00:03:50,330 --> 00:03:50,929
examples.

81
00:03:51,210 --> 00:03:51,729
However,

82
00:03:52,050 --> 00:03:54,270
physical AI extends far beyond

83
00:03:54,270 --> 00:03:56,569
robots to anything in the physical

84
00:03:56,569 --> 00:03:58,679
world, from smart machines

85
00:03:58,679 --> 00:04:01,008
to assembly lines in a factory

86
00:04:01,210 --> 00:04:03,288
to even a whole smart factory or

87
00:04:03,288 --> 00:04:05,528
even a smart municipality

88
00:04:05,528 --> 00:04:08,129
with smart buildings and connected infrastructure

89
00:04:08,129 --> 00:04:10,169
and vehicles. All of these are

90
00:04:10,169 --> 00:04:12,088
part of physical AI systems.

91
00:04:12,929 --> 00:04:15,129
And at the heart of physical AI

92
00:04:15,569 --> 00:04:17,850
is a world foundation model very

93
00:04:17,850 --> 00:04:20,428
similar to the models that have given rise

94
00:04:20,649 --> 00:04:23,009
to so much of the popular generative AI

95
00:04:23,009 --> 00:04:23,588
today.

96
00:04:24,048 --> 00:04:25,548
However, these models

97
00:04:25,850 --> 00:04:28,048
usually incorporate a vast array

98
00:04:28,048 --> 00:04:30,769
of different types of input. They're very multimodal.

99
00:04:30,850 --> 00:04:32,069
So in addition to

100
00:04:32,369 --> 00:04:34,569
some language instruction inputs and maybe

101
00:04:34,569 --> 00:04:36,559
some video or photos for context.

102
00:04:36,879 --> 00:04:38,959
They can take all kinds of other physical

103
00:04:38,959 --> 00:04:39,488
data,

104
00:04:39,829 --> 00:04:41,379
temperature, humidity,

105
00:04:41,720 --> 00:04:43,899
force feedback, sensor data, etc.

106
00:04:44,358 --> 00:04:46,439
and they combine all of that into

107
00:04:46,439 --> 00:04:48,519
an output that then instructs the

108
00:04:48,519 --> 00:04:49,519
physical object

109
00:04:50,199 --> 00:04:52,798
how to behave and to perform sophisticated

110
00:04:52,798 --> 00:04:53,379
behaviors.

111
00:04:53,829 --> 00:04:55,829
The nice thing about these models is that they

112
00:04:55,829 --> 00:04:57,988
incorporate a lot of knowledge about

113
00:04:57,988 --> 00:04:59,290
the physical world

114
00:04:59,548 --> 00:05:01,588
and they generalize very well so

115
00:05:01,588 --> 00:05:03,790
that you can fine tune them to

116
00:05:03,790 --> 00:05:05,869
your particular domains and expect them

117
00:05:05,869 --> 00:05:07,809
to do a variety of tasks.

118
00:05:08,269 --> 00:05:09,928
They're also very good at

119
00:05:10,540 --> 00:05:12,730
adapting quickly to different embodiments,

120
00:05:12,790 --> 00:05:15,259
so different types of humanoids, for example,

121
00:05:15,670 --> 00:05:17,189
doesn't take nearly as much training as

122
00:05:18,459 --> 00:05:20,959
has been invested in the initial world Foundation

123
00:05:20,959 --> 00:05:21,470
model.

124
00:05:22,028 --> 00:05:24,259
And we're going to talk about some examples like

125
00:05:24,480 --> 00:05:26,480
Nvidia, Isaac Groot, for example, and

126
00:05:26,480 --> 00:05:28,439
how to run that in the cloud.

127
00:05:29,389 --> 00:05:30,009
So

128
00:05:30,509 --> 00:05:32,750
what is the impact of all this physical

129
00:05:32,750 --> 00:05:33,488
AI?

130
00:05:33,939 --> 00:05:36,069
Well, one of the key industries I

131
00:05:36,069 --> 00:05:38,149
think that comes to mind when we think about physical

132
00:05:38,149 --> 00:05:38,750
AI

133
00:05:39,028 --> 00:05:40,309
is the manufacturing

134
00:05:40,629 --> 00:05:42,939
and the associated supply chain and logistics,

135
00:05:43,028 --> 00:05:45,129
making and delivering physical goods

136
00:05:45,428 --> 00:05:47,470
throughout the world. There are over

137
00:05:47,470 --> 00:05:48,689
10 million factories,

138
00:05:49,069 --> 00:05:50,949
200,000 warehouses,

139
00:05:51,350 --> 00:05:53,548
1.5 million cars and trucks

140
00:05:53,548 --> 00:05:55,629
that are all part of this complex

141
00:05:55,629 --> 00:05:57,778
industry. And every one of them

142
00:05:57,899 --> 00:05:59,059
stands to benefit

143
00:05:59,559 --> 00:06:00,920
from physical AI.

144
00:06:02,899 --> 00:06:04,899
So the potential is enormous in that

145
00:06:04,899 --> 00:06:07,579
industry, but it extends to all industries

146
00:06:07,579 --> 00:06:10,019
including healthcare, agriculture,

147
00:06:10,528 --> 00:06:12,619
retail, and the public sector and

148
00:06:12,619 --> 00:06:14,959
beyond. Nvidia

149
00:06:14,959 --> 00:06:17,059
CEO Jensen Wong has

150
00:06:17,278 --> 00:06:19,939
stated that 65%

151
00:06:20,160 --> 00:06:21,649
of the global GDP

152
00:06:22,250 --> 00:06:23,338
across industries

153
00:06:23,838 --> 00:06:25,819
can be improved by physical AI.

154
00:06:26,778 --> 00:06:29,939
This is a staggering more than $50

155
00:06:29,939 --> 00:06:31,838
trillion of economic impact.

156
00:06:33,889 --> 00:06:35,928
But physical AI isn't just

157
00:06:35,928 --> 00:06:37,798
some pipe dream out in the future.

158
00:06:38,119 --> 00:06:40,129
Physical AI is here

159
00:06:40,129 --> 00:06:42,338
today. In fact, at Amazon

160
00:06:42,338 --> 00:06:44,778
we're using physical AI in many of our robots

161
00:06:44,778 --> 00:06:47,238
in our warehouses and distribution centers.

162
00:06:47,899 --> 00:06:50,379
Pictured here is Vulcan. This is our first robot

163
00:06:50,379 --> 00:06:51,649
with a sense of touch,

164
00:06:51,939 --> 00:06:54,100
and Vulcan was born out of the desire

165
00:06:54,100 --> 00:06:56,278
to help our stockkeeping workers

166
00:06:56,278 --> 00:06:58,410
to reach both the highest bins on

167
00:06:58,410 --> 00:07:00,399
a rack and the lowest bins as well.

168
00:07:01,100 --> 00:07:03,819
And in order to do so, it needs a multi-sensory,

169
00:07:03,899 --> 00:07:06,290
multimodal input into these foundation

170
00:07:06,290 --> 00:07:07,959
models. It needs to be able to see

171
00:07:08,420 --> 00:07:10,399
where there's space in an existing bin.

172
00:07:10,920 --> 00:07:13,040
And it needs to be able to have

173
00:07:13,040 --> 00:07:15,199
some feedback as it's handling all

174
00:07:15,199 --> 00:07:17,399
different kinds of objects, different shapes

175
00:07:17,399 --> 00:07:18,910
and sizes and textures.

176
00:07:19,199 --> 00:07:21,278
And so we've incorporated a force feedback

177
00:07:21,278 --> 00:07:23,358
mechanism to give it that sense of

178
00:07:23,358 --> 00:07:25,639
touch. All this multimodal data

179
00:07:25,639 --> 00:07:27,738
goes into a physical AI model

180
00:07:28,000 --> 00:07:30,040
that was trained with many of the patterns we're going to

181
00:07:30,040 --> 00:07:31,000
show you here today.

182
00:07:32,048 --> 00:07:34,298
And so now in operation it's

183
00:07:34,298 --> 00:07:36,379
saving our employees up to an hour and

184
00:07:36,379 --> 00:07:37,278
a half a day,

185
00:07:37,699 --> 00:07:39,939
not having to go up and down a ladder and

186
00:07:39,939 --> 00:07:42,079
also making their jobs a lot safer

187
00:07:42,220 --> 00:07:44,329
and more enjoyable as they don't have to bend over

188
00:07:44,329 --> 00:07:46,569
all the time putting a lot of undue stress

189
00:07:46,569 --> 00:07:47,639
on the lower back.

190
00:07:49,108 --> 00:07:51,269
Well, that's not the only place that we're

191
00:07:51,269 --> 00:07:53,459
using physical AI. Another robot

192
00:07:53,459 --> 00:07:55,088
that we use is part of our

193
00:07:55,470 --> 00:07:57,670
Zero Touch manufacturing initiative,

194
00:07:58,108 --> 00:08:00,009
and we're using this robot arm

195
00:08:00,459 --> 00:08:02,588
for the product inspection and auditing

196
00:08:02,588 --> 00:08:04,970
stage for all of our Amazon products.

197
00:08:05,428 --> 00:08:07,548
And of course we're always coming out with new

198
00:08:07,548 --> 00:08:08,410
products, and so

199
00:08:08,790 --> 00:08:10,790
we need to be able to introduce those

200
00:08:10,790 --> 00:08:12,769
products into this zero touch station

201
00:08:13,170 --> 00:08:15,290
without bringing production to a halt.

202
00:08:15,980 --> 00:08:18,660
And so we need to train these robots

203
00:08:18,660 --> 00:08:20,660
to handle these new products in

204
00:08:20,660 --> 00:08:21,678
simulation,

205
00:08:21,980 --> 00:08:24,100
and this is part of our simulation

206
00:08:24,100 --> 00:08:26,259
first initiative, which is a very

207
00:08:26,259 --> 00:08:28,298
important part of the 5 pillars that

208
00:08:28,298 --> 00:08:30,720
we're going to be talking about to really help accelerate

209
00:08:31,139 --> 00:08:32,070
physical AI.

210
00:08:32,369 --> 00:08:34,719
What we do in this case, for example, is we

211
00:08:34,960 --> 00:08:37,019
can bring in a CAD design for

212
00:08:37,019 --> 00:08:39,259
the packaging and for the new product

213
00:08:39,259 --> 00:08:41,788
itself. And then using Nvidia

214
00:08:41,788 --> 00:08:43,869
Cosmos on AWS

215
00:08:43,869 --> 00:08:45,768
batch, we're able to generate

216
00:08:46,337 --> 00:08:48,729
what we do is we generate 50,000

217
00:08:48,827 --> 00:08:50,489
variations of that

218
00:08:50,788 --> 00:08:51,508
CAD

219
00:08:52,668 --> 00:08:55,229
specification so that we can train the robot

220
00:08:55,229 --> 00:08:57,308
through simulation to handle that

221
00:08:57,308 --> 00:08:58,129
new product

222
00:08:58,587 --> 00:09:00,989
and with a zero shot deployment,

223
00:09:01,188 --> 00:09:03,308
that robot is able to handle the new product and

224
00:09:03,308 --> 00:09:06,087
manipulate it as you see here in the animation

225
00:09:06,467 --> 00:09:07,808
without bringing the line down.

226
00:09:08,269 --> 00:09:10,509
That's not the only place we're using AI

227
00:09:10,509 --> 00:09:12,750
here with this robot. We actually also

228
00:09:12,750 --> 00:09:15,168
bring in all of the product specs

229
00:09:15,469 --> 00:09:16,129
and using

230
00:09:17,000 --> 00:09:19,308
Amazon Bedrock behind the scenes

231
00:09:19,308 --> 00:09:21,428
with some large language models,

232
00:09:21,469 --> 00:09:23,570
we interpret those specifications

233
00:09:23,710 --> 00:09:25,808
to understand what kinds of testing

234
00:09:25,950 --> 00:09:28,070
do we need to do on this product, what really

235
00:09:28,070 --> 00:09:30,210
constitutes the necessary

236
00:09:30,210 --> 00:09:32,609
auditing, and so all of that is automated

237
00:09:32,830 --> 00:09:35,029
as we introduce these new products into a

238
00:09:35,029 --> 00:09:37,308
true zero-touch manufacturing

239
00:09:37,308 --> 00:09:37,950
initiative.

240
00:09:39,859 --> 00:09:42,479
Well, what's required fundamentally

241
00:09:42,479 --> 00:09:43,200
to

242
00:09:43,580 --> 00:09:45,469
achieve this kind of physical AI?

243
00:09:46,408 --> 00:09:48,788
We believe there are 5 pillars, and the first

244
00:09:49,090 --> 00:09:49,769
is data.

245
00:09:51,090 --> 00:09:53,590
Training this kind of sophisticated behavior

246
00:09:54,250 --> 00:09:56,450
requires an enormous volume of very

247
00:09:56,450 --> 00:09:58,500
high quality and realistic data.

248
00:09:59,210 --> 00:10:01,450
Some of this data is generated today by

249
00:10:01,450 --> 00:10:04,070
humans through teleoperation and examples

250
00:10:04,330 --> 00:10:06,349
that robots can be trained to emulate,

251
00:10:06,918 --> 00:10:08,389
but the vast majority

252
00:10:08,649 --> 00:10:10,808
will be generated synthetically

253
00:10:10,808 --> 00:10:13,009
in the future because it's just impractical.

254
00:10:14,340 --> 00:10:16,590
The next pillar is around training. It's taking

255
00:10:16,590 --> 00:10:18,639
all of that data, and there are many

256
00:10:18,639 --> 00:10:20,250
applications for training from

257
00:10:20,908 --> 00:10:22,908
fine tuning a world foundation model

258
00:10:22,908 --> 00:10:25,250
to your particular industry or domain

259
00:10:25,629 --> 00:10:26,168
to

260
00:10:26,519 --> 00:10:29,269
training a model through imitation learning or reinforcement

261
00:10:29,269 --> 00:10:31,269
learning and training a policy for

262
00:10:31,269 --> 00:10:34,038
robots to execute particular

263
00:10:34,038 --> 00:10:36,090
advanced and sophisticated tasks.

264
00:10:37,808 --> 00:10:39,859
The next pillar is simulation, and this is

265
00:10:39,859 --> 00:10:42,080
again part of our sim first approach.

266
00:10:43,029 --> 00:10:43,580
We

267
00:10:43,960 --> 00:10:46,080
test these trained models extensively

268
00:10:46,080 --> 00:10:48,440
in a simulated environment before deploying

269
00:10:48,440 --> 00:10:50,940
to the real world. This is both a lot safer

270
00:10:51,158 --> 00:10:53,129
and less time consuming

271
00:10:53,558 --> 00:10:55,558
and less costly as well because we

272
00:10:55,558 --> 00:10:56,979
don't have accidents with

273
00:10:57,440 --> 00:10:59,479
robots and the expensive equipment that

274
00:10:59,479 --> 00:11:01,200
they're ultimately deployed onto.

275
00:11:03,048 --> 00:11:05,210
The fourth pillar is what we call sim

276
00:11:05,210 --> 00:11:07,229
to real, and this is where we have

277
00:11:07,529 --> 00:11:09,609
gone through a cycle of simulation and

278
00:11:09,609 --> 00:11:11,690
training and gotten to a good result. Now

279
00:11:11,690 --> 00:11:12,590
we need to really

280
00:11:12,928 --> 00:11:14,928
test this in the real world. This is

281
00:11:14,928 --> 00:11:17,308
literally where the rubber meets the road,

282
00:11:17,668 --> 00:11:19,788
and as you're developing

283
00:11:20,090 --> 00:11:22,609
more and more sophisticated physical AI,

284
00:11:22,889 --> 00:11:25,048
you're going to be deploying this to more and more different

285
00:11:25,048 --> 00:11:27,340
types of objects, maybe in a manufacturing

286
00:11:27,340 --> 00:11:29,570
setting, different types of machines.

287
00:11:30,460 --> 00:11:32,519
In different locations

288
00:11:32,519 --> 00:11:34,619
around the world and everyone may have a

289
00:11:34,619 --> 00:11:36,269
different version of the model,

290
00:11:36,700 --> 00:11:37,359
a different.

291
00:11:38,029 --> 00:11:38,629
Uh,

292
00:11:39,070 --> 00:11:41,428
type of device and so forth and so managing

293
00:11:41,428 --> 00:11:42,048
your fleet

294
00:11:42,710 --> 00:11:44,989
of physical AI smart devices

295
00:11:44,989 --> 00:11:47,269
becomes a challenge, and we have some technologies

296
00:11:47,269 --> 00:11:49,830
including uh AWS IOT

297
00:11:49,830 --> 00:11:52,330
Greengrass to help you manage this complexity.

298
00:11:53,308 --> 00:11:55,509
The 5th and final pillar is agentic

299
00:11:55,509 --> 00:11:56,469
orchestration,

300
00:11:57,029 --> 00:11:59,090
and this is where we bring together

301
00:11:59,229 --> 00:12:01,340
modular building blocks of physical

302
00:12:01,340 --> 00:12:01,950
AI.

303
00:12:02,820 --> 00:12:04,940
Not only does this provide good separation

304
00:12:04,940 --> 00:12:05,899
of concerns.

305
00:12:06,418 --> 00:12:08,509
But it also provides you with reusable

306
00:12:08,509 --> 00:12:10,590
elements and frankly it's

307
00:12:10,690 --> 00:12:12,389
an essential element to achieve

308
00:12:12,808 --> 00:12:14,849
long running and very sophisticated

309
00:12:14,849 --> 00:12:16,889
behaviors like fulfilling an

310
00:12:16,889 --> 00:12:19,009
order all the way from order placement

311
00:12:19,009 --> 00:12:21,048
to the shipment of that order and all of

312
00:12:21,048 --> 00:12:23,619
the picking and packing and logistical

313
00:12:23,619 --> 00:12:25,529
operations that need to take place in between.

314
00:12:25,849 --> 00:12:28,129
You're going to see how we've used this pillar

315
00:12:28,129 --> 00:12:30,210
in the example of the humanoid

316
00:12:30,408 --> 00:12:32,548
that we built a little bit later on in this talk.

317
00:12:33,450 --> 00:12:35,609
So these 5 pillars are by no

318
00:12:35,609 --> 00:12:38,070
means meant to be part of a waterfall

319
00:12:38,239 --> 00:12:40,450
cascade approach. Once you've been through all the 5,

320
00:12:40,529 --> 00:12:42,869
you're good to go, move on to the next problem.

321
00:12:43,330 --> 00:12:45,399
Instead, they form part of a virtuous

322
00:12:45,399 --> 00:12:46,070
cycle,

323
00:12:46,609 --> 00:12:48,729
a highly iterative process in what we

324
00:12:48,729 --> 00:12:50,989
call the physical AI flywheel.

325
00:12:51,288 --> 00:12:52,629
You start off with your data

326
00:12:53,090 --> 00:12:55,090
and you do some training and then you test

327
00:12:55,090 --> 00:12:56,918
that trained model and simulation,

328
00:12:57,288 --> 00:12:59,450
deploy it to the real world, and then you gain

329
00:12:59,450 --> 00:13:00,989
some feedback from that.

330
00:13:01,729 --> 00:13:04,239
And additional data based on that

331
00:13:04,239 --> 00:13:06,369
real world testing that you can then feed

332
00:13:06,369 --> 00:13:08,668
back into further training and so forth

333
00:13:08,969 --> 00:13:11,009
and so we go around and around the

334
00:13:11,009 --> 00:13:13,288
flywheel loop until we achieve the

335
00:13:13,288 --> 00:13:15,349
level of sophistication and quality

336
00:13:15,570 --> 00:13:17,418
for physical AI that's required.

337
00:13:18,090 --> 00:13:20,808
Now that said, there are still plenty of challenges.

338
00:13:21,239 --> 00:13:23,369
That I'm sure many of you may be thinking about

339
00:13:23,369 --> 00:13:24,590
or are already facing

340
00:13:24,928 --> 00:13:27,369
in this, and we're gonna show you how these Nvidia

341
00:13:27,369 --> 00:13:28,229
technologies

342
00:13:28,710 --> 00:13:31,168
on AWS can help you meet these challenges.

343
00:13:31,450 --> 00:13:33,590
The first is the high volume of data.

344
00:13:33,989 --> 00:13:36,070
In the humanoid example, just the

345
00:13:36,070 --> 00:13:37,168
simple task of

346
00:13:37,548 --> 00:13:39,658
picking a bag of potato chips and putting it

347
00:13:39,658 --> 00:13:40,529
in a box

348
00:13:40,928 --> 00:13:43,139
requires at least 800

349
00:13:43,389 --> 00:13:45,609
examples in order to be able to learn

350
00:13:45,609 --> 00:13:47,250
that through imitation learning.

351
00:13:47,548 --> 00:13:49,840
More sophisticated behaviors could take thousands

352
00:13:49,840 --> 00:13:52,070
or even millions of examples, and

353
00:13:52,070 --> 00:13:54,070
the only way to really achieve that is

354
00:13:54,070 --> 00:13:55,649
through synthetic data generation.

355
00:13:56,879 --> 00:13:59,389
Another challenge is simulation accuracy,

356
00:13:59,719 --> 00:14:01,960
and this really helps to close the sim

357
00:14:01,960 --> 00:14:03,038
to real gap.

358
00:14:03,399 --> 00:14:05,840
You need not only a realistic environment

359
00:14:05,840 --> 00:14:07,960
with photorealistic surroundings and so

360
00:14:07,960 --> 00:14:09,960
forth, but very realistic

361
00:14:09,960 --> 00:14:10,750
physics,

362
00:14:11,119 --> 00:14:13,389
which is a complex topic, and that's something

363
00:14:13,389 --> 00:14:15,440
Nvidia together with AWS can

364
00:14:15,440 --> 00:14:16,058
help you

365
00:14:16,440 --> 00:14:18,649
achieve. Now

366
00:14:18,649 --> 00:14:20,808
all these tools are very sophisticated. They've

367
00:14:20,808 --> 00:14:23,090
got a lot of ingenuity built in and

368
00:14:23,090 --> 00:14:25,129
to really get the most out of them, you really have

369
00:14:25,129 --> 00:14:27,210
to know how to deploy them

370
00:14:27,210 --> 00:14:28,619
most effectively. So

371
00:14:28,969 --> 00:14:31,149
there's some complexity in the setup of this,

372
00:14:31,408 --> 00:14:33,609
and we're going to show you how through things like

373
00:14:33,609 --> 00:14:34,330
Nvidia

374
00:14:34,599 --> 00:14:37,029
NIMs, for example, we can prepackage

375
00:14:37,250 --> 00:14:39,250
that into a distribution that

376
00:14:39,250 --> 00:14:41,330
you can spin up and get working quickly.

377
00:14:42,178 --> 00:14:44,038
Shortcutting all of that complexity.

378
00:14:44,908 --> 00:14:47,109
And that also leads to increased

379
00:14:47,109 --> 00:14:49,548
repeatability and collaboration. You might be facing

380
00:14:49,548 --> 00:14:51,750
some challenges with different teams working

381
00:14:51,750 --> 00:14:53,798
around the world in different standards and so

382
00:14:53,798 --> 00:14:55,869
forth. This is a little bit of a wild west

383
00:14:56,190 --> 00:14:58,298
right now in the physical AI world, but

384
00:14:58,298 --> 00:15:00,389
we'll show you how to standardize this so that you

385
00:15:00,389 --> 00:15:02,869
can share and collaborate more easily.

386
00:15:03,840 --> 00:15:06,279
Then there's time to results. These models

387
00:15:06,279 --> 00:15:08,320
are pretty sophisticated and they

388
00:15:08,320 --> 00:15:10,178
take a very long time to train

389
00:15:10,668 --> 00:15:11,798
on a single instance.

390
00:15:12,239 --> 00:15:15,460
And so you really need to think about multi-GPU

391
00:15:15,719 --> 00:15:17,719
and multi-node parallelization,

392
00:15:18,119 --> 00:15:20,139
and that can be a complex topic.

393
00:15:20,519 --> 00:15:22,639
But here again we're bringing some

394
00:15:22,639 --> 00:15:24,639
proven patterns. We're going to go into more

395
00:15:24,639 --> 00:15:26,759
detail with you here as to how you can get

396
00:15:26,759 --> 00:15:28,099
your multi-node

397
00:15:28,359 --> 00:15:30,399
simulation, your multi-node training

398
00:15:30,399 --> 00:15:32,219
jobs up and running very quickly.

399
00:15:33,190 --> 00:15:35,308
And then of course cost effectiveness trying to

400
00:15:35,308 --> 00:15:37,019
do this on a fleet

401
00:15:37,440 --> 00:15:39,678
of instances yourself

402
00:15:39,678 --> 00:15:41,678
on premise can be very costly and

403
00:15:41,678 --> 00:15:43,678
probably not fully utilized unless you have

404
00:15:43,678 --> 00:15:45,719
a truly massive physical AI

405
00:15:45,719 --> 00:15:47,719
program. So doing this in the cloud gives

406
00:15:47,719 --> 00:15:49,940
you all the benefits of elasticity

407
00:15:50,359 --> 00:15:52,359
and cost effectiveness. And then we talked

408
00:15:52,359 --> 00:15:53,619
about fleet management,

409
00:15:54,239 --> 00:15:56,460
last but not least here, as you begin to

410
00:15:56,678 --> 00:15:58,759
deploy to more and more smart devices, you

411
00:15:58,759 --> 00:16:00,418
need a single pane of glass

412
00:16:00,678 --> 00:16:02,479
to manage these very effectively.

413
00:16:03,769 --> 00:16:04,308
So

414
00:16:05,408 --> 00:16:07,529
Those are the challenges we're going to

415
00:16:07,658 --> 00:16:10,099
be diving deep into each of these pillars

416
00:16:10,099 --> 00:16:12,178
with various solutions for these. You can

417
00:16:12,178 --> 00:16:13,288
see some of them here,

418
00:16:13,580 --> 00:16:15,779
but I want to make sure our colleagues have enough

419
00:16:15,779 --> 00:16:17,779
time to go into the details

420
00:16:17,779 --> 00:16:20,158
and also share this example with you. So

421
00:16:20,658 --> 00:16:21,739
without further ado.

422
00:16:22,690 --> 00:16:24,190
I'm going to pass to

423
00:16:24,450 --> 00:16:26,629
my colleague Ali to take you into

424
00:16:26,808 --> 00:16:29,168
a deep dive in these incredible Nvidia

425
00:16:29,168 --> 00:16:30,009
technologies.

426
00:16:33,969 --> 00:16:34,658
Thank you very much.

427
00:16:39,129 --> 00:16:39,729
Hi all,

428
00:16:40,250 --> 00:16:42,529
uh, I'm Ali Sharoni. I'm a developer relations

429
00:16:42,529 --> 00:16:44,570
manager at Nvidia focusing on the

430
00:16:44,570 --> 00:16:46,729
robotics ecosystem at AWS.

431
00:16:47,979 --> 00:16:50,149
I hope you all agree that we have entered

432
00:16:50,149 --> 00:16:51,969
the era of physical AI.

433
00:16:53,889 --> 00:16:56,009
Autonomous vehicles and

434
00:16:56,048 --> 00:16:57,808
robots have been around for decades,

435
00:16:58,168 --> 00:17:00,418
but something fundamental has recently shifted.

436
00:17:01,210 --> 00:17:03,509
And that's moving from arrays

437
00:17:03,509 --> 00:17:04,390
of

438
00:17:05,930 --> 00:17:07,068
sophisticated

439
00:17:07,449 --> 00:17:09,769
specialized models into a unified

440
00:17:09,769 --> 00:17:10,309
model

441
00:17:10,568 --> 00:17:12,348
that performs end to end

442
00:17:12,809 --> 00:17:13,969
understanding of physical AI.

443
00:17:14,969 --> 00:17:17,059
And one key enabler for that is

444
00:17:17,059 --> 00:17:19,098
what we call the 3 computer

445
00:17:19,098 --> 00:17:19,838
framework.

446
00:17:20,299 --> 00:17:21,838
Let me walk you through this.

447
00:17:22,420 --> 00:17:24,608
So first we need to train a robotic

448
00:17:24,608 --> 00:17:25,598
foundation model

449
00:17:25,858 --> 00:17:27,630
on the cloud using, for example,

450
00:17:27,979 --> 00:17:29,779
the P instances on AWS

451
00:17:30,180 --> 00:17:32,479
that would enable us to develop

452
00:17:32,699 --> 00:17:34,809
large foundation models for robotics

453
00:17:35,338 --> 00:17:37,380
and and use massive scale

454
00:17:37,380 --> 00:17:38,759
data sets to train those.

455
00:17:39,430 --> 00:17:41,459
And then the other computer that we need is for

456
00:17:41,459 --> 00:17:43,549
simulation and synthesis of data that is

457
00:17:43,549 --> 00:17:45,509
used to train those models,

458
00:17:45,828 --> 00:17:46,568
and that's what

459
00:17:47,239 --> 00:17:48,640
You can use like

460
00:17:49,279 --> 00:17:51,338
other like sophisticated

461
00:17:52,559 --> 00:17:54,979
RTX Pro servers on AWS to

462
00:17:55,160 --> 00:17:57,140
synthesize data and train

463
00:17:57,559 --> 00:17:59,400
models and test those on the cloud.

464
00:18:00,078 --> 00:18:02,338
And then the last computer is actually the brain

465
00:18:02,338 --> 00:18:04,338
of the robot that is deployed on the edge,

466
00:18:04,640 --> 00:18:07,098
and for that you can use Ajax computers

467
00:18:07,358 --> 00:18:09,559
and deploy the train models that are optimized

468
00:18:09,559 --> 00:18:11,479
for edge deployment on the cloud.

469
00:18:13,358 --> 00:18:15,670
And those would process real sensory

470
00:18:15,670 --> 00:18:16,739
data on the

471
00:18:17,170 --> 00:18:19,439
uh on the on the edge in real time

472
00:18:19,439 --> 00:18:21,578
and perform actions uh

473
00:18:22,318 --> 00:18:22,890
respectively,

474
00:18:23,338 --> 00:18:25,299
right. So,

475
00:18:26,150 --> 00:18:28,289
But there is a significant data gap when it comes

476
00:18:28,289 --> 00:18:29,338
to physical AI.

477
00:18:29,949 --> 00:18:32,299
Unlike large language models that can

478
00:18:32,299 --> 00:18:33,019
leverage

479
00:18:33,479 --> 00:18:34,219
internet scale data,

480
00:18:35,519 --> 00:18:37,959
robotic systems face severe data scarcity.

481
00:18:39,318 --> 00:18:41,500
We need thousands of human demonstrations, as

482
00:18:41,500 --> 00:18:42,380
Sean mentioned,

483
00:18:42,838 --> 00:18:44,880
to train a simple manipulation

484
00:18:44,880 --> 00:18:46,880
task, and that does not scale

485
00:18:46,880 --> 00:18:47,858
and it's not,

486
00:18:49,509 --> 00:18:50,380
it's not feasible.

487
00:18:52,118 --> 00:18:54,279
So what we can use is instead

488
00:18:54,279 --> 00:18:55,108
to use a

489
00:18:56,439 --> 00:18:57,500
handful of real data

490
00:18:58,140 --> 00:19:00,219
and to augment those using synthetic data

491
00:19:00,640 --> 00:19:02,939
to generate diverse and

492
00:19:03,400 --> 00:19:04,059
um

493
00:19:04,479 --> 00:19:06,519
and edge cases for, for robotic

494
00:19:06,519 --> 00:19:09,019
training. So

495
00:19:09,019 --> 00:19:10,400
you can use total operation

496
00:19:10,739 --> 00:19:12,868
to collect the initial seed of training

497
00:19:12,868 --> 00:19:13,519
data using

498
00:19:14,828 --> 00:19:16,229
manual demonstrations,

499
00:19:16,578 --> 00:19:18,670
and then you can use Isaac SIM

500
00:19:19,049 --> 00:19:20,479
and Isaac Lab to

501
00:19:21,219 --> 00:19:23,779
generate variations of those using

502
00:19:23,779 --> 00:19:24,858
realistic scenarios,

503
00:19:25,219 --> 00:19:26,848
realistic trajectories,

504
00:19:27,180 --> 00:19:28,818
and add variations of the scene.

505
00:19:32,239 --> 00:19:34,799
Guru Guruth mimic from Isaac

506
00:19:34,799 --> 00:19:36,838
family of models can actually be used

507
00:19:36,838 --> 00:19:37,500
to

508
00:19:38,380 --> 00:19:40,390
create those trajectories for you, and Isaac's

509
00:19:40,390 --> 00:19:42,358
team would add the variations to the scene.

510
00:19:44,219 --> 00:19:46,338
So what we are delivering here is

511
00:19:46,338 --> 00:19:48,519
quite simple. We are using

512
00:19:48,519 --> 00:19:49,479
real world data.

513
00:19:50,269 --> 00:19:50,789
And

514
00:19:51,318 --> 00:19:53,618
augmenting those by adding

515
00:19:53,959 --> 00:19:55,719
domain randomization into the scene.

516
00:19:58,059 --> 00:19:59,680
So let's dive deeper into this.

517
00:20:00,608 --> 00:20:01,279
This is

518
00:20:01,818 --> 00:20:03,920
the data that you need for training real

519
00:20:04,259 --> 00:20:05,640
realistic physical AI

520
00:20:06,410 --> 00:20:08,640
applications. At the top we have real world data

521
00:20:09,140 --> 00:20:10,279
that is high quality

522
00:20:10,759 --> 00:20:13,180
but it's hard to capture and it's not scalable.

523
00:20:14,239 --> 00:20:16,059
But then you have synthetic data

524
00:20:16,640 --> 00:20:18,920
that is not as high quality as

525
00:20:18,920 --> 00:20:19,670
real data

526
00:20:20,279 --> 00:20:22,799
because it doesn't capture a lot of the nuances

527
00:20:22,799 --> 00:20:25,140
of physical interactions and dynamics,

528
00:20:25,719 --> 00:20:27,519
uh, but it's very scalable.

529
00:20:29,348 --> 00:20:31,699
But at the bottom you have

530
00:20:31,699 --> 00:20:33,289
in level data

531
00:20:34,549 --> 00:20:36,900
which is not really usable for robotics

532
00:20:36,900 --> 00:20:37,809
applications. For example,

533
00:20:38,108 --> 00:20:40,479
if you have a video of someone picking up an object,

534
00:20:40,828 --> 00:20:42,868
it doesn't give you the tactile feedback or

535
00:20:42,868 --> 00:20:44,209
the force dynamics

536
00:20:44,509 --> 00:20:46,469
or the mechanical

537
00:20:47,390 --> 00:20:49,348
interactions that you need to train a robot.

538
00:20:50,799 --> 00:20:53,039
So we need a hybrid approach here

539
00:20:53,039 --> 00:20:54,140
that would take a small

540
00:20:54,559 --> 00:20:56,098
amount of real world data

541
00:20:56,400 --> 00:20:58,598
and would amplify it and diversify

542
00:20:58,598 --> 00:21:00,699
it using domain randomization

543
00:21:00,828 --> 00:21:02,519
and real physics simulation.

544
00:21:05,670 --> 00:21:07,910
Right, so you can leverage Cosmos

545
00:21:07,910 --> 00:21:10,029
Family Awards Foundation models to bridge

546
00:21:10,029 --> 00:21:11,130
that data gap.

547
00:21:11,828 --> 00:21:13,930
Uh, this is an open source set of models,

548
00:21:13,949 --> 00:21:15,449
uh, from Nvidia

549
00:21:15,799 --> 00:21:17,890
that is available on AWS and, uh,

550
00:21:17,900 --> 00:21:18,779
and on GitHub,

551
00:21:19,509 --> 00:21:20,410
and, uh.

552
00:21:21,009 --> 00:21:23,328
In those family of models we have Cosmos Predict

553
00:21:23,328 --> 00:21:25,529
that allows you to generate up to 30 seconds

554
00:21:25,529 --> 00:21:26,309
of video

555
00:21:27,449 --> 00:21:29,348
that would describe what happens next.

556
00:21:30,299 --> 00:21:32,338
And we also have cosmos transfer that

557
00:21:32,338 --> 00:21:34,279
would allow you to generate photorealistic

558
00:21:34,848 --> 00:21:37,880
scenarios by varying lighting

559
00:21:37,900 --> 00:21:39,640
conditions, background,

560
00:21:40,739 --> 00:21:43,180
the environment, and and the weather, etc.

561
00:21:44,019 --> 00:21:46,299
And we also have Cosmos Reason, which is a visual

562
00:21:46,299 --> 00:21:47,000
language model

563
00:21:47,539 --> 00:21:49,680
that is trained for physical interactions

564
00:21:49,890 --> 00:21:51,939
and it can support human-like thinking.

565
00:21:54,049 --> 00:21:56,588
Here is an example of leveraging

566
00:21:57,209 --> 00:21:59,489
cosmos transfer in combination with

567
00:21:59,489 --> 00:22:01,588
Isaac Simon and Nvidia Omniverse.

568
00:22:01,930 --> 00:22:04,289
Nvidia Omniverse provides you the physical accuracy

569
00:22:04,289 --> 00:22:05,199
of the real world,

570
00:22:05,959 --> 00:22:08,449
the physics part of it, the simulation of the robot

571
00:22:08,449 --> 00:22:10,750
and the 3D assets and the interaction

572
00:22:11,689 --> 00:22:12,969
that happens between those.

573
00:22:13,779 --> 00:22:15,930
But then you can leverage cosmos

574
00:22:15,930 --> 00:22:16,920
cosmos transfer to

575
00:22:17,848 --> 00:22:20,250
inject diversity and domain randomization

576
00:22:20,250 --> 00:22:22,489
into the scene and create very realistic

577
00:22:22,489 --> 00:22:24,650
scenes that would help you amplify your

578
00:22:24,650 --> 00:22:27,170
training data set for your robotics

579
00:22:27,170 --> 00:22:32,318
applications. Another

580
00:22:32,318 --> 00:22:34,660
way to create synthetic data

581
00:22:35,199 --> 00:22:36,699
is to use dreams.

582
00:22:38,459 --> 00:22:40,660
This is Group Dreams from AI that would

583
00:22:40,660 --> 00:22:42,699
allow you to use a single image and a

584
00:22:42,699 --> 00:22:43,338
text prompt.

585
00:22:44,019 --> 00:22:46,130
To generate robotics trajectories

586
00:22:46,769 --> 00:22:49,029
that would be then used to

587
00:22:49,049 --> 00:22:50,949
train your physical AI models.

588
00:22:52,900 --> 00:22:54,838
So you can just provide a simple text prompt.

589
00:22:55,640 --> 00:22:56,670
And

590
00:22:57,289 --> 00:22:58,309
use Cosmos

591
00:22:58,608 --> 00:22:59,269
predict

592
00:22:59,848 --> 00:23:00,828
to create

593
00:23:01,529 --> 00:23:03,539
a future scenario based on

594
00:23:03,539 --> 00:23:05,880
based on your text prompt and single image input,

595
00:23:06,500 --> 00:23:07,750
and then use Cosmos

596
00:23:08,068 --> 00:23:10,140
reason to label the data

597
00:23:10,140 --> 00:23:12,328
and create the trajectories that you need and validate

598
00:23:12,328 --> 00:23:13,509
validate those as well.

599
00:23:14,979 --> 00:23:17,039
Right, so let's take a look at

600
00:23:17,039 --> 00:23:19,439
the end to end pipeline for

601
00:23:19,660 --> 00:23:22,019
a ste-first approach for physical AI

602
00:23:22,019 --> 00:23:24,108
development. So on

603
00:23:24,108 --> 00:23:26,140
the left you have your training data,

604
00:23:26,309 --> 00:23:27,328
and that can be

605
00:23:28,309 --> 00:23:30,338
text, image, videos, teleop

606
00:23:30,338 --> 00:23:32,009
data, or simulation data.

607
00:23:32,650 --> 00:23:35,049
And then you have at the top you have your

608
00:23:35,049 --> 00:23:36,910
foundation model training pipeline where

609
00:23:37,209 --> 00:23:39,250
you have a model architecture and

610
00:23:39,250 --> 00:23:41,439
you train those and validate it and optimize

611
00:23:41,439 --> 00:23:42,250
it for deployment.

612
00:23:43,059 --> 00:23:44,559
And that's your perception model.

613
00:23:45,108 --> 00:23:47,229
But for robotics application of physical AI,

614
00:23:47,430 --> 00:23:48,328
you also need,

615
00:23:50,098 --> 00:23:52,449
you need the robot to interact with the physical world. So

616
00:23:52,709 --> 00:23:54,868
just the perception is not good enough.

617
00:23:55,390 --> 00:23:57,719
So at the bottom, that's what you have robot learning,

618
00:23:57,910 --> 00:24:00,348
and that's where you train the model in

619
00:24:00,348 --> 00:24:01,410
simulation

620
00:24:01,719 --> 00:24:02,680
using

621
00:24:03,108 --> 00:24:05,299
the perception model as well as your synthetic training

622
00:24:05,299 --> 00:24:07,868
data to train the policies

623
00:24:07,868 --> 00:24:09,910
that would allow the robot to interact with the physical

624
00:24:09,910 --> 00:24:12,380
world. But that's not a single

625
00:24:12,380 --> 00:24:14,660
shot training. This actually

626
00:24:14,660 --> 00:24:16,588
works as a virtual cycle where

627
00:24:16,969 --> 00:24:19,699
the VLA would then use the demonstrations

628
00:24:19,699 --> 00:24:21,799
from the robot to improve itself

629
00:24:22,019 --> 00:24:23,519
and then it's fed back to the loop,

630
00:24:23,779 --> 00:24:25,680
and that's how the robot

631
00:24:26,618 --> 00:24:29,118
would be trained and the VLA would be trained together

632
00:24:29,380 --> 00:24:31,098
to meet the KPIs for deployment.

633
00:24:31,989 --> 00:24:34,108
And finally, you can also test and validate all of

634
00:24:34,108 --> 00:24:35,479
that in simulation as well.

635
00:24:35,920 --> 00:24:38,049
So you can have your, your test data as

636
00:24:38,049 --> 00:24:39,309
well in simulation

637
00:24:39,709 --> 00:24:41,739
and as a fleet, not a

638
00:24:41,739 --> 00:24:42,250
single unit,

639
00:24:42,509 --> 00:24:43,789
and test the whole pipeline.

640
00:24:44,459 --> 00:24:45,299
So the whole thing,

641
00:24:45,660 --> 00:24:48,009
the amazing thing about this is that you do

642
00:24:48,009 --> 00:24:49,160
the entire robot

643
00:24:50,259 --> 00:24:52,318
training and the perception training

644
00:24:52,420 --> 00:24:53,519
in simulation without

645
00:24:53,779 --> 00:24:55,858
doing anything in the real world that would save you a

646
00:24:55,858 --> 00:24:58,039
lot of time and money in terms

647
00:24:58,039 --> 00:25:00,439
of prototyping and testing

648
00:25:00,858 --> 00:25:01,779
on real hardware.

649
00:25:03,509 --> 00:25:05,719
Right, so just double clicking on the

650
00:25:05,719 --> 00:25:07,719
learning part and the training parts, you

651
00:25:07,719 --> 00:25:09,828
can, uh, for the robot for the robotics

652
00:25:09,828 --> 00:25:10,390
part of it,

653
00:25:10,799 --> 00:25:11,650
you can uh

654
00:25:11,959 --> 00:25:13,420
use different types of data

655
00:25:14,160 --> 00:25:15,539
together with Isaac Lab

656
00:25:16,118 --> 00:25:18,318
for their enforcement learning or imitation

657
00:25:18,318 --> 00:25:20,338
learning. And uh

658
00:25:21,289 --> 00:25:23,328
And then you can use Isaacs and Cosmos for

659
00:25:23,328 --> 00:25:25,529
synthetic data generation and software

660
00:25:25,529 --> 00:25:26,449
testing in the loop.

661
00:25:27,088 --> 00:25:29,088
And while that cycle is

662
00:25:29,088 --> 00:25:31,289
done and your model is trained, you can then deploy

663
00:25:31,289 --> 00:25:33,650
that to the edge on jets on tour

664
00:25:34,489 --> 00:25:37,568
for various applications like humanoid manipulation

665
00:25:37,930 --> 00:25:39,969
or autonomous mobile robots.

666
00:25:41,880 --> 00:25:44,199
For humans, humanoids specifically, you

667
00:25:44,199 --> 00:25:46,509
can leverage the generalized humanoid robot

668
00:25:46,509 --> 00:25:49,578
model from Nvidia Groot N 1.5,

669
00:25:50,039 --> 00:25:52,118
that is an open source model and is available

670
00:25:52,118 --> 00:25:53,680
on hugging face and on AWS

671
00:25:54,279 --> 00:25:56,559
that would enable a two-system architecture

672
00:25:56,559 --> 00:25:58,828
for thinking fast and slow. The System

673
00:25:58,828 --> 00:26:01,199
2 is for reasoning and

674
00:26:01,199 --> 00:26:03,838
understanding, and System One is for fast thinking

675
00:26:04,799 --> 00:26:06,140
and fast reactions.

676
00:26:07,420 --> 00:26:09,549
And it also supports cross embodiments.

677
00:26:12,130 --> 00:26:14,130
Once you have generated all your

678
00:26:14,130 --> 00:26:15,949
synthetic data

679
00:26:16,729 --> 00:26:18,809
and trained your robot policies

680
00:26:18,809 --> 00:26:19,920
and perception models,

681
00:26:20,250 --> 00:26:22,509
you, you're now ready to deploy

682
00:26:24,009 --> 00:26:25,170
the robot to the edge,

683
00:26:25,489 --> 00:26:26,259
and for that

684
00:26:26,729 --> 00:26:28,828
you're going to need also a very performant

685
00:26:28,969 --> 00:26:29,910
ultra high

686
00:26:30,650 --> 00:26:32,729
computer to be able to do

687
00:26:32,969 --> 00:26:34,969
interactions with the real world and fleet management.

688
00:26:36,140 --> 00:26:38,420
So Thor is a good candidate for that. And

689
00:26:38,420 --> 00:26:39,368
here are some of the

690
00:26:40,709 --> 00:26:42,670
early customers that we have engaged with.

691
00:26:43,039 --> 00:26:45,539
So it can be used for human aid robots,

692
00:26:45,618 --> 00:26:46,680
for multimodal

693
00:26:46,979 --> 00:26:49,779
AI and perception interactions.

694
00:26:50,180 --> 00:26:52,250
You can use it for visual agents, which allows

695
00:26:52,250 --> 00:26:52,959
you to

696
00:26:53,338 --> 00:26:56,539
generate custom alerts and video summarizations

697
00:26:56,618 --> 00:26:58,400
in warehouse environments, for example.

698
00:26:59,719 --> 00:27:02,479
For mobile medical robotics and instrumentation

699
00:27:02,479 --> 00:27:04,259
where you need real-time

700
00:27:04,680 --> 00:27:06,838
streaming and inference,

701
00:27:07,239 --> 00:27:09,479
Thor, it provides great compute

702
00:27:09,479 --> 00:27:11,059
power for that with over

703
00:27:11,348 --> 00:27:12,838
2000 tops of compute.

704
00:27:13,789 --> 00:27:16,000
And also for autonomous robots and

705
00:27:16,000 --> 00:27:18,000
machines where you need simultaneous localization

706
00:27:18,000 --> 00:27:18,939
and mapping

707
00:27:19,239 --> 00:27:21,519
or navigation for agriculture, for example,

708
00:27:21,880 --> 00:27:22,660
Tor is a great

709
00:27:24,160 --> 00:27:24,939
computer for those

710
00:27:25,279 --> 00:27:27,459
and for high performance computing

711
00:27:27,640 --> 00:27:29,660
and inference where you need high bandwidth

712
00:27:29,660 --> 00:27:32,000
and high quality images, again, Tor

713
00:27:32,000 --> 00:27:34,078
has great audio throughput for

714
00:27:34,078 --> 00:27:34,959
those applications.

715
00:27:35,608 --> 00:27:37,650
And with that, I'm going to pass it to my colleague

716
00:27:37,650 --> 00:27:39,289
Abhi Shaikh who is going to talk to you,

717
00:27:39,618 --> 00:27:40,799
talk you through

718
00:27:41,098 --> 00:27:43,098
some great use cases for

719
00:27:43,098 --> 00:27:44,380
humanoid robotics. Thank you.

720
00:27:46,088 --> 00:27:46,828
Thanks, Ollie.

721
00:27:49,739 --> 00:27:51,779
Good morning everybody. I hope you enjoyed

722
00:27:51,779 --> 00:27:52,838
the session thus far.

723
00:27:53,420 --> 00:27:55,500
I'm Abhishek Shrivastov, principal solutions

724
00:27:55,500 --> 00:27:57,000
architect AWS.

725
00:27:57,858 --> 00:27:59,559
For the remaining of the presentation,

726
00:27:59,818 --> 00:28:02,299
I'm going to go through the exciting

727
00:28:02,618 --> 00:28:04,818
collaboration between Nvidia and

728
00:28:04,818 --> 00:28:05,779
AWS

729
00:28:06,059 --> 00:28:07,900
in advancing the physical AI.

730
00:28:09,049 --> 00:28:10,519
But before I go deeper,

731
00:28:11,199 --> 00:28:13,689
I would like to bring the same slide

732
00:28:14,049 --> 00:28:14,769
once again.

733
00:28:15,729 --> 00:28:16,818
This is what

734
00:28:17,199 --> 00:28:19,279
a complete ecosystem

735
00:28:19,279 --> 00:28:20,140
looks like.

736
00:28:20,789 --> 00:28:22,920
5 pillars of physical AI.

737
00:28:23,789 --> 00:28:24,519
Data,

738
00:28:25,029 --> 00:28:25,670
training,

739
00:28:25,949 --> 00:28:26,930
simulation,

740
00:28:27,229 --> 00:28:28,160
semurial,

741
00:28:28,549 --> 00:28:29,969
and agentic orchestration.

742
00:28:30,818 --> 00:28:32,598
And across each of them,

743
00:28:33,068 --> 00:28:34,890
Nvidia and AWS

744
00:28:35,309 --> 00:28:37,430
is offering a unified

745
00:28:37,430 --> 00:28:38,088
solution

746
00:28:38,509 --> 00:28:39,769
for our customers.

747
00:28:40,750 --> 00:28:42,368
Keep this pillars in mind.

748
00:28:43,189 --> 00:28:44,769
As I move forward,

749
00:28:45,199 --> 00:28:47,299
I'll cover the integration architecture,

750
00:28:47,640 --> 00:28:49,799
as well as the making of a

751
00:28:49,799 --> 00:28:51,160
humanoid robot

752
00:28:51,640 --> 00:28:53,160
for a supply chain operation.

753
00:28:54,769 --> 00:28:56,670
So let's start with a data pillar,

754
00:28:57,209 --> 00:28:59,410
the foundation of physical AI.

755
00:29:00,920 --> 00:29:04,239
Training a humanoid require humanoid robot.

756
00:29:05,259 --> 00:29:07,318
Requires a massive amount

757
00:29:07,779 --> 00:29:09,160
of synthetic data,

758
00:29:09,578 --> 00:29:10,598
and as we learn,

759
00:29:10,979 --> 00:29:13,239
capturing the data from a real world

760
00:29:13,618 --> 00:29:15,689
is really time and

761
00:29:15,689 --> 00:29:17,219
a resource intensive.

762
00:29:18,348 --> 00:29:20,549
This is where Nvidia Cosmo's World

763
00:29:20,549 --> 00:29:22,689
Foundation model on AWS

764
00:29:23,150 --> 00:29:24,670
can help our customers.

765
00:29:25,630 --> 00:29:26,650
So when we are making

766
00:29:26,989 --> 00:29:29,380
a humanoid robot for our demo,

767
00:29:29,828 --> 00:29:32,068
we followed a three-step

768
00:29:32,068 --> 00:29:34,328
process. In

769
00:29:34,328 --> 00:29:35,140
step one,

770
00:29:35,650 --> 00:29:38,358
we captured 800

771
00:29:38,368 --> 00:29:39,430
episodes of

772
00:29:39,729 --> 00:29:41,410
human tele-operated data.

773
00:29:42,299 --> 00:29:44,529
Performing a pick and place

774
00:29:44,529 --> 00:29:46,239
operation in the real world

775
00:29:46,660 --> 00:29:48,338
for a single environment.

776
00:29:50,039 --> 00:29:52,078
This is the ground truth, and

777
00:29:52,078 --> 00:29:53,880
it sets the baseline

778
00:29:54,358 --> 00:29:56,479
of how a successful operation

779
00:29:56,479 --> 00:29:58,160
looks like in a real world.

780
00:29:59,239 --> 00:30:00,789
We then use that data,

781
00:30:01,368 --> 00:30:03,029
the human tele-operated data,

782
00:30:04,108 --> 00:30:06,170
to train Cosmos Predict World

783
00:30:06,170 --> 00:30:08,250
Foundation model on AWS

784
00:30:08,250 --> 00:30:09,529
batch at scale.

785
00:30:11,049 --> 00:30:13,130
Using this fine-tuned Cosmos

786
00:30:13,130 --> 00:30:14,880
Predict World Foundation model,

787
00:30:15,209 --> 00:30:17,549
we augmented the data sets

788
00:30:17,809 --> 00:30:20,088
to create a future world

789
00:30:20,088 --> 00:30:22,789
state. Of the videos.

790
00:30:23,618 --> 00:30:25,818
And that completes

791
00:30:25,828 --> 00:30:27,949
how a new synthetic data looks

792
00:30:27,949 --> 00:30:29,410
like in the physical world.

793
00:30:31,098 --> 00:30:32,439
In the 2nd step,

794
00:30:32,818 --> 00:30:34,828
we augmented the dataset

795
00:30:35,459 --> 00:30:37,618
using cosmos transfer to

796
00:30:37,618 --> 00:30:38,959
create diversity

797
00:30:39,219 --> 00:30:40,279
such as material

798
00:30:40,660 --> 00:30:42,078
and lighting conditions.

799
00:30:43,189 --> 00:30:45,390
We then use cosmos transfer

800
00:30:45,390 --> 00:30:47,009
in the final step

801
00:30:47,390 --> 00:30:48,650
to evaluate

802
00:30:48,989 --> 00:30:50,088
and filter

803
00:30:50,390 --> 00:30:52,469
the synthetic data that is generated.

804
00:30:53,559 --> 00:30:54,719
Just so you know,

805
00:30:55,199 --> 00:30:57,239
with the 16 hours of

806
00:30:57,239 --> 00:30:58,500
human demonstration,

807
00:30:58,880 --> 00:31:01,759
we have created more than 1000 scenarios

808
00:31:01,880 --> 00:31:03,019
of synthetic data

809
00:31:03,598 --> 00:31:05,640
that would have taken for us almost

810
00:31:05,640 --> 00:31:06,739
4 months

811
00:31:07,039 --> 00:31:09,078
to train the humanoid robot

812
00:31:09,078 --> 00:31:10,779
for the supply chain operations.

813
00:31:12,250 --> 00:31:14,390
So let's go deeper and learn about

814
00:31:14,689 --> 00:31:16,910
the deployment architecture

815
00:31:17,410 --> 00:31:18,529
in AWS.

816
00:31:20,049 --> 00:31:20,959
We have chosen

817
00:31:21,259 --> 00:31:22,759
Amazon EKS.

818
00:31:23,709 --> 00:31:24,818
To deploy,

819
00:31:25,150 --> 00:31:27,309
which is a fully managed Kuberniity service

820
00:31:27,309 --> 00:31:29,959
to deploy Cosmos Predict

821
00:31:30,660 --> 00:31:33,039
foundation model on AWS.

822
00:31:34,539 --> 00:31:37,059
EKS scales across multiple

823
00:31:37,059 --> 00:31:37,660
node

824
00:31:37,979 --> 00:31:39,939
based on the inference volume

825
00:31:41,160 --> 00:31:43,660
while still managing the control plane availability.

826
00:31:44,789 --> 00:31:47,400
One of the key benefits of this architecture

827
00:31:47,618 --> 00:31:50,059
is total cost of ownership optimization,

828
00:31:50,140 --> 00:31:51,739
the TCO optimization.

829
00:31:52,858 --> 00:31:54,719
NIMMS containers are

830
00:31:55,439 --> 00:31:57,578
pre pre-optimized for

831
00:31:57,578 --> 00:31:59,318
high inference volumes,

832
00:31:59,979 --> 00:32:01,479
whereas EKS

833
00:32:01,739 --> 00:32:03,838
offers a dynamic scaling

834
00:32:04,019 --> 00:32:06,598
and pay as you go model that is required

835
00:32:06,858 --> 00:32:09,779
for the optimum compute utilization

836
00:32:09,939 --> 00:32:12,299
and reduce overall cost of training.

837
00:32:13,769 --> 00:32:16,088
For our human demonstration, we have used

838
00:32:16,088 --> 00:32:18,809
P54X Large EC2

839
00:32:18,809 --> 00:32:20,930
instance, each of them powered

840
00:32:20,930 --> 00:32:23,328
by Nvidia H100 tensor

841
00:32:23,328 --> 00:32:24,068
core GPU

842
00:32:24,969 --> 00:32:27,328
with 256

843
00:32:27,328 --> 00:32:29,170
GBytes of system memory

844
00:32:29,500 --> 00:32:31,729
and 80 gigabyte of

845
00:32:31,729 --> 00:32:33,469
GPU memory, just sufficient

846
00:32:34,250 --> 00:32:36,449
to run Cosmos Predict on

847
00:32:36,449 --> 00:32:37,180
AWS.

848
00:32:38,959 --> 00:32:41,358
We are leveraging Amazon Elastic

849
00:32:41,358 --> 00:32:43,118
File System EFS

850
00:32:43,640 --> 00:32:46,160
to store the generated

851
00:32:46,160 --> 00:32:48,640
synthetic data out of Cosmos

852
00:32:48,640 --> 00:32:51,068
model. And cloud watch

853
00:32:51,068 --> 00:32:53,118
to monitor the cluster

854
00:32:53,118 --> 00:32:55,118
performance as well as the job

855
00:32:55,118 --> 00:32:56,559
inference attributes.

856
00:32:57,828 --> 00:33:00,189
If you'd like to go deeper into learning

857
00:33:00,189 --> 00:33:01,259
how to deploy

858
00:33:01,549 --> 00:33:04,209
Nvidia Cosmos models on AWS,

859
00:33:04,549 --> 00:33:06,729
I would highly encourage you to check out

860
00:33:06,828 --> 00:33:08,588
our detailed blog post.

861
00:33:10,750 --> 00:33:12,979
Let's dive into the training pillar

862
00:33:13,318 --> 00:33:14,459
of physical AI.

863
00:33:15,680 --> 00:33:16,380
This is where

864
00:33:16,759 --> 00:33:18,699
you teach the policy

865
00:33:19,160 --> 00:33:21,078
to interact with the physical world.

866
00:33:22,670 --> 00:33:24,759
There are 2 key approaches

867
00:33:25,108 --> 00:33:27,430
when it comes to training a robot.

868
00:33:28,390 --> 00:33:29,689
Reinforcement learning

869
00:33:30,229 --> 00:33:31,348
and imitation learning.

870
00:33:32,469 --> 00:33:34,930
Reinforcement learning is like training

871
00:33:35,390 --> 00:33:36,650
by examples,

872
00:33:37,029 --> 00:33:39,509
sorry, training by trial and errors.

873
00:33:40,930 --> 00:33:43,328
Imagine a humanoid robot walking

874
00:33:43,328 --> 00:33:44,989
in a complex environment.

875
00:33:46,900 --> 00:33:47,858
Each time

876
00:33:48,420 --> 00:33:50,539
The robot takes a stable step

877
00:33:50,539 --> 00:33:51,118
forward.

878
00:33:52,229 --> 00:33:54,269
You give a positive reward, and

879
00:33:54,269 --> 00:33:55,459
when it stumbles,

880
00:33:55,828 --> 00:33:57,549
you provide a negative feedback.

881
00:33:58,979 --> 00:34:01,219
This approach is extremely

882
00:34:01,219 --> 00:34:03,299
important when it comes to introducing an

883
00:34:03,299 --> 00:34:04,910
adaptive behavior

884
00:34:05,299 --> 00:34:07,618
within a robot like locomotion policy.

885
00:34:09,059 --> 00:34:09,869
On the other hand,

886
00:34:10,289 --> 00:34:11,628
the imitation learning

887
00:34:11,889 --> 00:34:14,010
is like teaching by an example.

888
00:34:15,090 --> 00:34:17,260
So as a human, you demonstrate

889
00:34:17,610 --> 00:34:18,789
an example

890
00:34:19,329 --> 00:34:21,449
and the robot is going to observe

891
00:34:21,449 --> 00:34:23,489
the demonstration and imitate

892
00:34:23,489 --> 00:34:24,250
the movement.

893
00:34:25,750 --> 00:34:27,760
This type, the imitation learning,

894
00:34:28,059 --> 00:34:30,320
is very effective when it comes

895
00:34:30,320 --> 00:34:30,840
to

896
00:34:31,179 --> 00:34:33,250
performing a manipulative task like

897
00:34:33,250 --> 00:34:34,719
pick and place operations.

898
00:34:35,677 --> 00:34:37,838
But in practice, when you are building a

899
00:34:37,838 --> 00:34:39,579
complex physical AI system,

900
00:34:39,998 --> 00:34:41,998
we normally use a combination of

901
00:34:41,998 --> 00:34:44,079
both reinforcement learning

902
00:34:44,309 --> 00:34:45,998
as well as imitation learning.

903
00:34:47,760 --> 00:34:49,260
Now let's explore the

904
00:34:49,599 --> 00:34:50,659
architecture

905
00:34:51,119 --> 00:34:52,099
for training

906
00:34:52,360 --> 00:34:54,139
pipeline on AWS.

907
00:34:54,599 --> 00:34:56,659
We have made this really simple for

908
00:34:56,659 --> 00:34:58,188
our, for our customer.

909
00:34:58,898 --> 00:35:00,849
So we have a common architecture

910
00:35:01,590 --> 00:35:03,849
to run Nvidia Isaac Lab

911
00:35:04,349 --> 00:35:06,110
for reinforcement learning.

912
00:35:06,780 --> 00:35:08,898
And Nvidia Groot N1

913
00:35:08,898 --> 00:35:10,500
Open Foundation model

914
00:35:10,898 --> 00:35:13,219
on AWS for imitation

915
00:35:13,219 --> 00:35:14,260
learning use cases.

916
00:35:15,530 --> 00:35:17,688
At the center of this architecture, what you

917
00:35:17,688 --> 00:35:19,929
see here is AWS Batch,

918
00:35:20,289 --> 00:35:22,590
which orchestrates the training

919
00:35:22,769 --> 00:35:25,168
as well as the fine tuning across

920
00:35:25,168 --> 00:35:27,369
distributed compute resources.

921
00:35:28,809 --> 00:35:31,090
Batch is a fully managed serverless

922
00:35:31,090 --> 00:35:33,269
batch subservice that offers

923
00:35:33,409 --> 00:35:35,449
a pay as you go operating model

924
00:35:35,449 --> 00:35:37,228
which is really effective

925
00:35:37,688 --> 00:35:39,809
for your cost optimized training

926
00:35:39,809 --> 00:35:40,570
on cloud.

927
00:35:42,110 --> 00:35:42,688
Using

928
00:35:43,030 --> 00:35:45,179
Amazon Elastic container Service,

929
00:35:45,228 --> 00:35:47,469
you can deploy Isaac Lab

930
00:35:47,699 --> 00:35:49,070
or Nvidia Groot

931
00:35:49,579 --> 00:35:50,530
containers

932
00:35:50,918 --> 00:35:53,349
across multiple node and multiple GPU

933
00:35:53,750 --> 00:35:54,550
at scale.

934
00:35:56,010 --> 00:35:57,668
Amazon EFS

935
00:35:58,168 --> 00:36:00,489
serves as a persistent storage

936
00:36:00,489 --> 00:36:02,148
across distributed node

937
00:36:02,648 --> 00:36:05,208
for storing the checkpoints of

938
00:36:05,208 --> 00:36:06,128
training policy.

939
00:36:06,909 --> 00:36:08,489
And finally, Cloudwatch

940
00:36:08,969 --> 00:36:10,969
provides a real-time monitoring

941
00:36:11,228 --> 00:36:11,769
of

942
00:36:12,030 --> 00:36:13,030
training jobs

943
00:36:13,510 --> 00:36:15,550
like loss curves and learning

944
00:36:15,550 --> 00:36:16,110
rates.

945
00:36:17,349 --> 00:36:18,550
For more details,

946
00:36:18,949 --> 00:36:21,119
you can follow the step by step guide

947
00:36:21,119 --> 00:36:23,208
of running Nvidia Isaac Lab

948
00:36:23,349 --> 00:36:25,610
on AWS in the QR code.

949
00:36:27,679 --> 00:36:30,070
Let's talk about simulation pillar,

950
00:36:30,610 --> 00:36:32,510
a critical component

951
00:36:33,010 --> 00:36:35,090
of validating a trained

952
00:36:35,090 --> 00:36:37,128
policy in a virtual environment.

953
00:36:38,628 --> 00:36:40,659
Simulation provides a virtual

954
00:36:40,659 --> 00:36:41,469
environment

955
00:36:41,898 --> 00:36:44,228
to validate a robot behavior

956
00:36:44,228 --> 00:36:45,648
before you deploy them

957
00:36:46,030 --> 00:36:46,949
in a real world.

958
00:36:47,639 --> 00:36:50,269
We can test thousands of scenarios

959
00:36:50,369 --> 00:36:52,878
in parallel with the different lighting

960
00:36:52,878 --> 00:36:54,610
and material conditions,

961
00:36:55,599 --> 00:36:57,228
object object placement,

962
00:36:57,489 --> 00:36:59,489
as well as including some of the

963
00:36:59,489 --> 00:37:01,668
edge cases, which is really

964
00:37:01,969 --> 00:37:04,228
hard and impractical to replicate

965
00:37:04,409 --> 00:37:06,128
in real world scenarios.

966
00:37:07,099 --> 00:37:09,148
This is where Nvidia Isaac SIM

967
00:37:09,148 --> 00:37:11,010
on Amazon EC2

968
00:37:11,349 --> 00:37:12,809
helps our customers

969
00:37:13,148 --> 00:37:15,570
to deploy simulation environment

970
00:37:15,708 --> 00:37:16,989
quickly on AWS.

971
00:37:17,719 --> 00:37:20,188
Isaac Sim is a physically accurate,

972
00:37:20,409 --> 00:37:22,789
photorealistic simulation platform

973
00:37:23,329 --> 00:37:25,309
built on Nvidia Omnivores.

974
00:37:26,228 --> 00:37:29,070
You can simulate physical world dynamics

975
00:37:29,070 --> 00:37:30,340
such as friction,

976
00:37:30,750 --> 00:37:31,750
gravity,

977
00:37:32,070 --> 00:37:33,449
collision, etc.

978
00:37:34,070 --> 00:37:36,300
A great example of a simulation

979
00:37:36,300 --> 00:37:37,329
is training

980
00:37:37,590 --> 00:37:39,610
our own humanoid robot

981
00:37:39,829 --> 00:37:41,750
in a virtual AWS Expo floor

982
00:37:43,250 --> 00:37:45,579
with thousands of virtual attendees,

983
00:37:45,949 --> 00:37:47,949
different carpet and flooring

984
00:37:47,949 --> 00:37:49,949
conditions to ensure it works

985
00:37:49,949 --> 00:37:51,909
before we brought them in to reinvent.

986
00:37:53,760 --> 00:37:56,090
And like that, you can simulate

987
00:37:56,090 --> 00:37:58,530
various various environments

988
00:37:58,530 --> 00:38:00,079
such as warehouse,

989
00:38:00,409 --> 00:38:01,559
factory floors,

990
00:38:02,329 --> 00:38:04,530
healthcare facilities, and many more.

991
00:38:06,389 --> 00:38:07,530
Now traditionally,

992
00:38:07,989 --> 00:38:09,128
running Isaac SIM

993
00:38:10,070 --> 00:38:13,159
requires high-end RTX workstations

994
00:38:13,340 --> 00:38:14,188
on-premise.

995
00:38:14,918 --> 00:38:16,378
It is hard to set up

996
00:38:16,918 --> 00:38:19,280
and it is constrained by the physical

997
00:38:19,280 --> 00:38:20,019
capacity

998
00:38:20,519 --> 00:38:21,280
of your

999
00:38:21,958 --> 00:38:22,840
physical server.

1000
00:38:23,708 --> 00:38:25,958
We made Nvidia Isaac's same deployment

1001
00:38:25,958 --> 00:38:27,260
on AWS

1002
00:38:27,639 --> 00:38:29,719
really simple for our customers.

1003
00:38:30,728 --> 00:38:32,478
AWS Marketplace

1004
00:38:32,769 --> 00:38:35,208
offers a fully configured EMI

1005
00:38:35,860 --> 00:38:38,619
to run Nvidia Isaac SIM workstation

1006
00:38:38,898 --> 00:38:41,619
for both Windows and Linux

1007
00:38:41,619 --> 00:38:42,260
environment.

1008
00:38:43,228 --> 00:38:44,889
You can deploy Isaac SIM

1009
00:38:45,878 --> 00:38:47,969
in a few clicks

1010
00:38:48,228 --> 00:38:50,469
over a 100s of EC2

1011
00:38:50,469 --> 00:38:53,159
environments powered by Nvidia GPUs,

1012
00:38:53,389 --> 00:38:55,010
so you can get started

1013
00:38:55,550 --> 00:38:57,949
to validate within a few hours,

1014
00:38:58,269 --> 00:38:59,309
then a few weeks.

1015
00:39:01,329 --> 00:39:03,590
Let's discuss sim to real.

1016
00:39:04,208 --> 00:39:05,510
Now here is the reality.

1017
00:39:06,389 --> 00:39:08,728
Sometimes the model

1018
00:39:09,110 --> 00:39:11,329
that works in simulation

1019
00:39:11,708 --> 00:39:13,909
does not effectively perform

1020
00:39:14,148 --> 00:39:15,449
in the real world,

1021
00:39:15,989 --> 00:39:18,469
and this is called reality gap.

1022
00:39:19,070 --> 00:39:21,269
Different factors such as physics

1023
00:39:21,269 --> 00:39:23,860
accuracies, sensors, noise,

1024
00:39:24,110 --> 00:39:26,489
as well as the environment variations.

1025
00:39:27,289 --> 00:39:29,570
Can impact the performance of a

1026
00:39:29,570 --> 00:39:31,148
policy in the real world.

1027
00:39:32,239 --> 00:39:34,550
Closing this reality gap

1028
00:39:34,869 --> 00:39:36,889
is an ongoing science

1029
00:39:37,148 --> 00:39:39,188
that requires you to bring

1030
00:39:39,188 --> 00:39:41,389
in a human feedback

1031
00:39:41,628 --> 00:39:42,789
in the training loop.

1032
00:39:44,250 --> 00:39:46,349
With every iteration, you

1033
00:39:46,349 --> 00:39:48,869
need to transfer the model from cloud

1034
00:39:49,168 --> 00:39:50,599
to edge devices,

1035
00:39:50,929 --> 00:39:52,739
manage the deployment fleet,

1036
00:39:53,168 --> 00:39:55,610
and as well as over the air deployment

1037
00:39:55,610 --> 00:39:56,409
and updates.

1038
00:39:57,369 --> 00:39:59,409
This is where AWS IOT

1039
00:39:59,409 --> 00:40:01,590
services can help our customers

1040
00:40:01,648 --> 00:40:03,728
to manage the communications and

1041
00:40:03,728 --> 00:40:06,030
handle the deployment from cloud

1042
00:40:06,030 --> 00:40:07,449
to physical robot.

1043
00:40:08,909 --> 00:40:10,719
So let's explore the architecture.

1044
00:40:11,070 --> 00:40:12,610
AWS IOT

1045
00:40:13,110 --> 00:40:15,688
core Service offers an edge

1046
00:40:15,688 --> 00:40:18,188
to cloud communication hub.

1047
00:40:18,668 --> 00:40:21,590
IoT Core establishes a secure

1048
00:40:21,590 --> 00:40:23,909
bi-directional communication to

1049
00:40:23,909 --> 00:40:26,570
offer a command back to your robot,

1050
00:40:26,648 --> 00:40:28,829
as well as the telemetry

1051
00:40:28,829 --> 00:40:30,648
data back to the cloud.

1052
00:40:31,989 --> 00:40:34,320
Amazon's Sage Maker model registry

1053
00:40:34,500 --> 00:40:35,418
offers you

1054
00:40:36,059 --> 00:40:38,300
a centralized repository

1055
00:40:38,639 --> 00:40:39,539
to store

1056
00:40:39,840 --> 00:40:40,559
version

1057
00:40:40,958 --> 00:40:43,280
and manage the physical AI

1058
00:40:43,280 --> 00:40:43,820
models.

1059
00:40:44,659 --> 00:40:47,199
And AWS IOT Greengrass V2

1060
00:40:47,418 --> 00:40:49,039
facilitate the model

1061
00:40:49,300 --> 00:40:51,539
and component deployment to the Edge

1062
00:40:51,539 --> 00:40:52,260
devices.

1063
00:40:52,780 --> 00:40:54,938
For more details, please do check out

1064
00:40:54,938 --> 00:40:57,458
our blog to deploy robot

1065
00:40:57,739 --> 00:40:59,898
using IoT Greengrass V2.

1066
00:41:02,219 --> 00:41:04,219
Let's discuss the final

1067
00:41:04,219 --> 00:41:06,219
pillar, which is the most exciting

1068
00:41:06,219 --> 00:41:08,478
one, agentic orchestration.

1069
00:41:09,699 --> 00:41:11,728
Which transform a

1070
00:41:11,728 --> 00:41:14,820
robot to handle the intelligent

1071
00:41:14,820 --> 00:41:15,889
generic task.

1072
00:41:17,128 --> 00:41:19,458
Physical AI evolved from a

1073
00:41:19,458 --> 00:41:20,559
specialized

1074
00:41:20,898 --> 00:41:22,938
to a generalist robot.

1075
00:41:23,889 --> 00:41:26,579
A specialized robot follow a rule-based

1076
00:41:26,579 --> 00:41:27,239
logic.

1077
00:41:27,969 --> 00:41:29,179
But the generalist,

1078
00:41:29,559 --> 00:41:30,688
like humanoid,

1079
00:41:31,159 --> 00:41:33,898
requires cognitive reasoning ability

1080
00:41:34,159 --> 00:41:36,199
to respond and operate

1081
00:41:36,199 --> 00:41:37,878
in a dynamic environment.

1082
00:41:39,599 --> 00:41:41,639
This things like goal

1083
00:41:41,639 --> 00:41:43,840
understanding, path planning, task

1084
00:41:43,840 --> 00:41:44,918
decompositions,

1085
00:41:45,438 --> 00:41:46,260
each of them

1086
00:41:46,840 --> 00:41:48,869
are really complex tasks

1087
00:41:48,869 --> 00:41:49,599
to process.

1088
00:41:50,559 --> 00:41:51,309
But using

1089
00:41:51,648 --> 00:41:53,809
Amazon Bedrock Agent Core,

1090
00:41:54,280 --> 00:41:56,510
customers can build these atomic

1091
00:41:56,510 --> 00:41:57,628
AI agent

1092
00:41:58,050 --> 00:42:00,250
at scale for a physical AI

1093
00:42:00,250 --> 00:42:00,780
system.

1094
00:42:01,500 --> 00:42:03,610
Let's dive into the architecture diagram

1095
00:42:03,610 --> 00:42:05,269
for agentic orchestration.

1096
00:42:06,519 --> 00:42:08,728
At the core of this architecture, we

1097
00:42:08,728 --> 00:42:11,050
have Amazon Bedrock Agent Core

1098
00:42:11,530 --> 00:42:13,610
to for building, deploying,

1099
00:42:13,849 --> 00:42:15,110
and operating

1100
00:42:15,369 --> 00:42:17,309
the AI agents at scale.

1101
00:42:18,360 --> 00:42:19,809
Bedrock Agent Core

1102
00:42:20,139 --> 00:42:20,978
can help

1103
00:42:21,309 --> 00:42:23,918
to build a modular AI agents

1104
00:42:24,159 --> 00:42:26,360
for functions such as inventory

1105
00:42:26,360 --> 00:42:26,869
planning,

1106
00:42:27,159 --> 00:42:28,918
task decompositions,

1107
00:42:30,199 --> 00:42:32,309
path planning, and SKU

1108
00:42:32,309 --> 00:42:34,239
identification, and many more.

1109
00:42:35,579 --> 00:42:37,840
For the humanoid robot that you will see

1110
00:42:37,840 --> 00:42:38,849
in a few minutes,

1111
00:42:39,429 --> 00:42:41,000
we have used the

1112
00:42:41,329 --> 00:42:42,860
Bedrock agent core

1113
00:42:43,139 --> 00:42:45,949
with a fine-tuned NOAA

1114
00:42:45,949 --> 00:42:47,639
Light Foundation model

1115
00:42:47,978 --> 00:42:49,800
to do a task validation.

1116
00:42:50,188 --> 00:42:52,300
So every time the robot

1117
00:42:52,300 --> 00:42:54,128
picks an item from the table,

1118
00:42:54,418 --> 00:42:56,610
we validate if the

1119
00:42:56,610 --> 00:42:58,739
operation is successful, otherwise

1120
00:42:58,739 --> 00:43:00,958
provide a dynamic signal for the next step.

1121
00:43:02,148 --> 00:43:04,188
The architecture is fully

1122
00:43:04,188 --> 00:43:05,168
scalable

1123
00:43:05,590 --> 00:43:07,590
from a single robot to a

1124
00:43:07,590 --> 00:43:09,550
wide fleet automation.

1125
00:43:10,030 --> 00:43:12,090
Bedrock Agent Core provides

1126
00:43:12,228 --> 00:43:14,148
an intelligent layer

1127
00:43:14,550 --> 00:43:16,909
that transforms a physical

1128
00:43:16,909 --> 00:43:17,809
automation

1129
00:43:18,070 --> 00:43:19,228
to a physical AI.

1130
00:43:21,478 --> 00:43:23,809
Let me walk you through the top 5

1131
00:43:23,809 --> 00:43:26,300
reasons why you should be choosing

1132
00:43:27,849 --> 00:43:29,849
AWS and Nvidia for

1133
00:43:29,849 --> 00:43:31,510
your physical AI journey.

1134
00:43:32,469 --> 00:43:32,978
First,

1135
00:43:33,340 --> 00:43:35,050
scalable infrastructure.

1136
00:43:35,590 --> 00:43:37,449
Physical AI development

1137
00:43:37,869 --> 00:43:39,909
requires a scalable infrastructure

1138
00:43:39,909 --> 00:43:41,050
for both training

1139
00:43:41,309 --> 00:43:42,699
as well as simulation.

1140
00:43:43,079 --> 00:43:45,119
The training that used to take

1141
00:43:45,119 --> 00:43:45,820
weeks

1142
00:43:46,188 --> 00:43:47,639
now takes hours

1143
00:43:47,909 --> 00:43:50,148
using AWS infrastructure.

1144
00:43:51,239 --> 00:43:51,820
Second,

1145
00:43:53,179 --> 00:43:54,840
a complete physical AI ecosystem.

1146
00:43:55,398 --> 00:43:58,039
Nvidia and AWS have fundamentally

1147
00:43:58,039 --> 00:43:58,820
changed

1148
00:43:59,760 --> 00:44:01,958
and reshaped the stages of

1149
00:44:01,958 --> 00:44:03,750
physical AI development

1150
00:44:04,199 --> 00:44:05,719
from the data curation

1151
00:44:06,079 --> 00:44:08,099
to the model training

1152
00:44:08,398 --> 00:44:10,760
to a photorealistic simulation and

1153
00:44:10,760 --> 00:44:12,800
agentic orchestration. Across

1154
00:44:12,800 --> 00:44:15,280
each of this pillar, we are offering

1155
00:44:15,280 --> 00:44:17,360
a unified solution across

1156
00:44:17,360 --> 00:44:18,599
all industries.

1157
00:44:19,760 --> 00:44:20,418
Third,

1158
00:44:20,719 --> 00:44:22,429
optimized economics.

1159
00:44:23,139 --> 00:44:25,489
Frugality is the core principle

1160
00:44:25,489 --> 00:44:28,059
of a well architected solutions.

1161
00:44:28,429 --> 00:44:31,019
Traditional physical AI developments

1162
00:44:31,030 --> 00:44:33,329
require a large investment of

1163
00:44:33,329 --> 00:44:35,208
GPU cluster on-premise.

1164
00:44:35,750 --> 00:44:37,889
With AWS pay as you go model

1165
00:44:38,030 --> 00:44:40,269
and various savings plans, customers

1166
00:44:40,269 --> 00:44:41,728
can easily save

1167
00:44:42,030 --> 00:44:44,188
cost of training simulation

1168
00:44:44,188 --> 00:44:46,030
at scale on cloud.

1169
00:44:47,168 --> 00:44:47,829
Fourth,

1170
00:44:48,199 --> 00:44:50,539
robust security and compliance.

1171
00:44:51,610 --> 00:44:53,929
When you are deploying a physical AI

1172
00:44:53,929 --> 00:44:55,969
system for a healthcare facility,

1173
00:44:56,369 --> 00:44:58,688
manufacturing plant, or a public

1174
00:44:58,688 --> 00:44:59,389
space,

1175
00:45:00,090 --> 00:45:01,809
security is foundational.

1176
00:45:02,610 --> 00:45:04,820
AWS offers an enterprise

1177
00:45:04,820 --> 00:45:06,800
grade security for you

1178
00:45:07,059 --> 00:45:09,780
to build an end to end encryption,

1179
00:45:10,188 --> 00:45:12,579
role-based security model, granular

1180
00:45:12,579 --> 00:45:14,610
access control to offer a

1181
00:45:14,610 --> 00:45:16,398
secure and compliant

1182
00:45:16,780 --> 00:45:18,059
physical AI systems.

1183
00:45:19,019 --> 00:45:20,128
And finally,

1184
00:45:20,579 --> 00:45:22,780
unified edge to cloud

1185
00:45:22,780 --> 00:45:23,898
architecture.

1186
00:45:24,418 --> 00:45:26,539
We can train a sophisticated model

1187
00:45:26,539 --> 00:45:28,820
on AWS and seamlessly

1188
00:45:28,820 --> 00:45:31,090
deploy on Nvidia jets and devices.

1189
00:45:31,378 --> 00:45:33,539
AWS IT services

1190
00:45:33,889 --> 00:45:36,039
is really closing the gap

1191
00:45:36,458 --> 00:45:39,099
between cloud and edge infrastructure.

1192
00:45:40,949 --> 00:45:42,989
I hope you are excited to see a

1193
00:45:42,989 --> 00:45:45,250
short demo of our humanoid robot

1194
00:45:45,590 --> 00:45:46,728
fully developed

1195
00:45:47,030 --> 00:45:49,208
using Nvidia Physical AI stack

1196
00:45:49,208 --> 00:45:50,429
on AWS.

1197
00:45:50,949 --> 00:45:51,969
Welcome to

1198
00:45:52,228 --> 00:45:54,590
our AWS Gen AI lab

1199
00:45:54,878 --> 00:45:57,300
at HQ2 in Arlington,

1200
00:45:57,378 --> 00:45:57,989
Virginia.

1201
00:45:58,780 --> 00:45:59,679
We are buying

1202
00:45:59,938 --> 00:46:02,188
an item from a fake website.

1203
00:46:02,739 --> 00:46:04,909
As soon as the order is placed, the

1204
00:46:04,909 --> 00:46:06,619
control plane agent

1205
00:46:07,099 --> 00:46:08,438
receives the signal

1206
00:46:08,699 --> 00:46:09,519
and triggers

1207
00:46:09,938 --> 00:46:11,760
multiple AI agents

1208
00:46:12,099 --> 00:46:14,219
that perform different tasks such

1209
00:46:14,219 --> 00:46:15,478
as path planning

1210
00:46:15,820 --> 00:46:17,139
and inventory search.

1211
00:46:20,000 --> 00:46:21,898
Ladies and gentlemen, welcome

1212
00:46:22,280 --> 00:46:24,438
our humanoid robot

1213
00:46:24,438 --> 00:46:25,000
Storm.

1214
00:46:27,269 --> 00:46:28,679
The fine-tuned Groot

1215
00:46:29,070 --> 00:46:30,728
N 1.5 model

1216
00:46:30,989 --> 00:46:33,668
is running on Nvidia G6

1217
00:46:33,989 --> 00:46:35,340
EC2 offering,

1218
00:46:35,789 --> 00:46:37,909
providing a vision language action

1219
00:46:37,909 --> 00:46:40,030
model to perform pick and place

1220
00:46:40,030 --> 00:46:40,728
tasks.

1221
00:46:44,429 --> 00:46:46,679
This is the place where we are running the task

1222
00:46:46,679 --> 00:46:47,398
validation.

1223
00:46:48,570 --> 00:46:50,610
Once the task validation is

1224
00:46:50,610 --> 00:46:52,610
successful, we identify the

1225
00:46:52,610 --> 00:46:54,610
nearest delivery workstations,

1226
00:46:55,090 --> 00:46:56,909
the empty container,

1227
00:46:57,409 --> 00:46:58,668
and drop

1228
00:46:59,050 --> 00:46:59,750
the item

1229
00:47:00,050 --> 00:47:00,750
in the box.

1230
00:47:08,769 --> 00:47:10,860
I hope this demo inspired you to

1231
00:47:10,860 --> 00:47:12,938
build your next physical AI

1232
00:47:12,938 --> 00:47:13,478
system.

1233
00:47:13,938 --> 00:47:16,139
Please do check out our AWS

1234
00:47:16,139 --> 00:47:18,378
guidelines, including the complete

1235
00:47:18,378 --> 00:47:19,840
reference architecture

1236
00:47:20,179 --> 00:47:22,739
for building a physical AI with Nvidia

1237
00:47:22,739 --> 00:47:23,958
and AWS.

1238
00:47:24,539 --> 00:47:26,539
Join us this afternoon if you, if you'd like to

1239
00:47:26,539 --> 00:47:28,739
go hands on on our session IND

1240
00:47:28,739 --> 00:47:29,898
403

1241
00:47:30,329 --> 00:47:32,458
for physical AI for building a physical AI

1242
00:47:32,458 --> 00:47:34,219
with Nvidia and AWS.

1243
00:47:34,978 --> 00:47:36,500
Hope you enjoyed our session.

1244
00:47:36,938 --> 00:47:38,938
We have a humanoid demonstration in this room,

1245
00:47:39,019 --> 00:47:40,619
so feel free to engage and ask.

