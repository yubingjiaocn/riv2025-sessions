1
00:00:00,000 --> 00:00:04,470
- So, welcome everybody
to, I think it's DVT 3.6,

2
00:00:04,470 --> 00:00:05,303
where we're gonna be talking about

3
00:00:05,303 --> 00:00:07,770
customizing Llama models for code.

4
00:00:07,770 --> 00:00:10,170
If that's a session you expect to be in,

5
00:00:10,170 --> 00:00:11,160
that's the session you're at.

6
00:00:11,160 --> 00:00:13,830
If it's not, then hopefully
you're able to find

7
00:00:13,830 --> 00:00:16,590
the other one you wanna be
at. Otherwise, stick around.

8
00:00:16,590 --> 00:00:17,730
My name's Eissa Jamil.

9
00:00:17,730 --> 00:00:20,103
I'm a AI partner engineer at Meta,

10
00:00:21,000 --> 00:00:23,970
and hopefully this is informative
and helpful for everybody.

11
00:00:23,970 --> 00:00:25,590
Thanks also for just showing up

12
00:00:25,590 --> 00:00:28,050
and being ready to learn, hopefully.

13
00:00:28,050 --> 00:00:31,950
So, for today's agenda, we'll
be going over basically ways

14
00:00:31,950 --> 00:00:36,630
to empower developers to
basically build with Llama

15
00:00:36,630 --> 00:00:38,310
using basically tools that we have

16
00:00:38,310 --> 00:00:40,950
that'll help you enhance
coding capabilities

17
00:00:40,950 --> 00:00:42,940
for productivity use cases,

18
00:00:42,940 --> 00:00:44,970
and all of those types of things.

19
00:00:44,970 --> 00:00:47,040
Hopefully some takeaways
you'll get from this session

20
00:00:47,040 --> 00:00:49,860
include learning basically the challenges

21
00:00:49,860 --> 00:00:52,680
you're gonna face when adapting
LLAMs for coding tasks,

22
00:00:52,680 --> 00:00:57,140
understanding data prep,
model training, and evals,

23
00:00:57,140 --> 00:01:00,180
and then also discovering
Llama deployment strategies

24
00:01:00,180 --> 00:01:04,950
on AWS, whether it be
Bedrock, SageMaker, EKS,

25
00:01:04,950 --> 00:01:08,490
even EC2, whatever you end
up doing for your use cases,

26
00:01:08,490 --> 00:01:10,380
and hopefully we'll explore a little bit

27
00:01:10,380 --> 00:01:13,923
into real-world use cases
as we progress here.

28
00:01:15,090 --> 00:01:17,190
So, before we kick into things,

29
00:01:17,190 --> 00:01:20,070
we do have a pre-presentation
anonymous survey.

30
00:01:20,070 --> 00:01:22,620
If you guys have a
minute to scan the code,

31
00:01:22,620 --> 00:01:24,030
I'll keep the slide up.

32
00:01:24,030 --> 00:01:25,680
It takes five seconds to complete.

33
00:01:25,680 --> 00:01:27,720
We'd super appreciate it.

34
00:01:27,720 --> 00:01:30,170
So, I'll go ahead and just
be quiet for a minute.

35
00:01:32,010 --> 00:01:33,780
All right.

36
00:01:33,780 --> 00:01:35,160
So, let's go ahead and jump in.

37
00:01:35,160 --> 00:01:36,510
We're gonna start with a little bit

38
00:01:36,510 --> 00:01:38,100
of an introduction to Llama.

39
00:01:38,100 --> 00:01:39,360
Most of you probably have already

40
00:01:39,360 --> 00:01:42,420
messed around with Llama,
played, deployed it.

41
00:01:42,420 --> 00:01:44,220
Nevertheless, we'll do a
little bit of an introduction,

42
00:01:44,220 --> 00:01:46,020
tell you a little bit
of a background story

43
00:01:46,020 --> 00:01:47,580
on the progression of Llama,

44
00:01:47,580 --> 00:01:48,990
and then we'll dive in a little bit more

45
00:01:48,990 --> 00:01:51,990
into some of the
technicalities and challenges

46
00:01:51,990 --> 00:01:54,420
you'll face when fine-tuning
for coding tasks,

47
00:01:54,420 --> 00:01:57,870
and then at the end, we'll go
through a couple of example,

48
00:01:57,870 --> 00:02:00,870
basically notebooks, which are
from our Llama Recipes repo

49
00:02:00,870 --> 00:02:03,090
where we publish end-to-end use cases,

50
00:02:03,090 --> 00:02:05,283
examples, stuff like that.

51
00:02:06,480 --> 00:02:08,460
But yeah, we'll go ahead
and just jump on in here.

52
00:02:08,460 --> 00:02:10,803
So, looking at this slide,

53
00:02:12,200 --> 00:02:15,690
Llama's really, the idea
behind it is it's open source.

54
00:02:15,690 --> 00:02:19,470
It's a great foundation to really unlock

55
00:02:19,470 --> 00:02:22,770
any sort of use cases
you're trying to build for.

56
00:02:22,770 --> 00:02:24,720
We see a lot of organizations building,

57
00:02:24,720 --> 00:02:28,530
like we have the examples
here for language translation,

58
00:02:28,530 --> 00:02:31,290
use cases, personal assistants, chatbots,

59
00:02:31,290 --> 00:02:34,620
agents, content creation.

60
00:02:34,620 --> 00:02:37,080
It's a wide gamut of topics and use cases,

61
00:02:37,080 --> 00:02:40,050
and that's something which
we are super excited to see

62
00:02:40,050 --> 00:02:43,520
the last few years with
Llama 2 release, Llama 3,

63
00:02:43,520 --> 00:02:45,990
and even Llama 4 as it's released.

64
00:02:45,990 --> 00:02:47,580
We love hearing about
what people are doing,

65
00:02:47,580 --> 00:02:50,060
and that's just kind
of a high level there.

66
00:02:50,060 --> 00:02:53,373
So, talking about a little
bit of the history of Llama,

67
00:02:54,380 --> 00:02:56,940
it's crazy to think Llama 1 came out

68
00:02:56,940 --> 00:02:58,860
just a couple of years ago.

69
00:02:58,860 --> 00:03:03,030
Early February, Llama 1 came
out, research purpose mainly.

70
00:03:03,030 --> 00:03:06,060
We then came out later
that year with Llama 2

71
00:03:06,060 --> 00:03:09,123
with the ability to use it
for commercial use cases.

72
00:03:10,480 --> 00:03:13,470
And later on that year, we had Code Llama.

73
00:03:13,470 --> 00:03:15,940
We released Purple Llama,
kind of built around safety

74
00:03:15,940 --> 00:03:19,620
and content moderation,
input/output filtering.

75
00:03:19,620 --> 00:03:22,590
And you can see the family of
models just kept on evolving.

76
00:03:22,590 --> 00:03:24,480
We had Llama 3 release.

77
00:03:24,480 --> 00:03:27,720
That came with a number of awesome models.

78
00:03:27,720 --> 00:03:31,650
In particular, everybody
loved Llama 3.1, 8B.

79
00:03:31,650 --> 00:03:35,580
And then we had 3.2, which introduced

80
00:03:35,580 --> 00:03:38,220
some multimodal models,
so you had the ability

81
00:03:38,220 --> 00:03:42,780
to do input with text or image.

82
00:03:42,780 --> 00:03:44,250
And then you also had our small models.

83
00:03:44,250 --> 00:03:46,110
If you were using anything
that really needed

84
00:03:46,110 --> 00:03:49,320
fast inference or you wanted really small,

85
00:03:49,320 --> 00:03:51,270
basically ability to deploy a small model,

86
00:03:51,270 --> 00:03:54,780
we released a 1B and a
3B model at that time.

87
00:03:54,780 --> 00:03:58,560
We also released Llama
Stack, basically a framework

88
00:03:58,560 --> 00:04:01,260
for building with Llama, and
then we released Llama 3.3.

89
00:04:01,260 --> 00:04:05,010
That's where we released
an updated version of 70B,

90
00:04:05,010 --> 00:04:07,560
which was a great model all around.

91
00:04:07,560 --> 00:04:10,650
And then in 2025, we released Llama 4,

92
00:04:10,650 --> 00:04:13,170
which came with Scout and Maverick.

93
00:04:13,170 --> 00:04:17,280
So these stats aren't
completely up to date,

94
00:04:17,280 --> 00:04:19,170
but just from earlier this year,

95
00:04:19,170 --> 00:04:22,290
I think this aligns with when
we had our LlamaCon event,

96
00:04:22,290 --> 00:04:24,720
we had over a billion
Hugging Face downloads,

97
00:04:24,720 --> 00:04:26,760
which is amazing for us
to see how many people

98
00:04:26,760 --> 00:04:29,060
are interacting and
actually trying to build

99
00:04:29,060 --> 00:04:34,050
with our models, and that's
what our intention is.

100
00:04:34,050 --> 00:04:36,390
And over 200,000 derivative models existed

101
00:04:36,390 --> 00:04:38,130
at that time that we could see.

102
00:04:38,130 --> 00:04:41,280
And that was mainly from
checking Hugging Face

103
00:04:41,280 --> 00:04:44,730
and the access we could see ourselves.

104
00:04:44,730 --> 00:04:46,200
We don't know what
everybody else is building.

105
00:04:46,200 --> 00:04:47,820
We don't know what's
being deployed in private

106
00:04:47,820 --> 00:04:50,130
or being developed within VPCs.

107
00:04:50,130 --> 00:04:51,990
So this is just data we had at the time.

108
00:04:51,990 --> 00:04:53,790
So it's amazing to see.

109
00:04:53,790 --> 00:04:57,060
And just in general, I
mean, open source is huge.

110
00:04:57,060 --> 00:05:01,620
It's a great area we're very
much always gung-ho about.

111
00:05:01,620 --> 00:05:03,210
It allows for deployment flexibility.

112
00:05:03,210 --> 00:05:06,180
If you wanna go on-prem, you
wanna go within your own VPC,

113
00:05:06,180 --> 00:05:08,910
you wanna go however you
want, it's an option.

114
00:05:08,910 --> 00:05:10,110
You can fine-tune the models,

115
00:05:10,110 --> 00:05:12,210
take 'em, do whatever you want, customize,

116
00:05:12,210 --> 00:05:13,140
all of that good stuff.

117
00:05:13,140 --> 00:05:15,480
And then there's the model
distillation capabilities

118
00:05:15,480 --> 00:05:19,290
where whether you're using
something like Llama 3.370B

119
00:05:19,290 --> 00:05:22,140
or you go over to something like 405B,

120
00:05:22,140 --> 00:05:23,970
all of those give you great options to go

121
00:05:23,970 --> 00:05:26,610
and look at options for distillation.

122
00:05:26,610 --> 00:05:29,820
So just diving in a little bit more

123
00:05:29,820 --> 00:05:33,380
into exploring that family of models.

124
00:05:33,380 --> 00:05:36,567
You can see here, I talked
a little bit about 3.18B,

125
00:05:36,567 --> 00:05:38,900
and I just mentioned 405B.

126
00:05:38,900 --> 00:05:41,430
You could see some recommendations here

127
00:05:41,430 --> 00:05:44,430
for when you would use
them versus other times.

128
00:05:44,430 --> 00:05:48,570
8B has become a super powerful
model for customization.

129
00:05:48,570 --> 00:05:52,020
It's fast to fine-tune, it's
relatively cost-effective,

130
00:05:52,020 --> 00:05:54,540
and it runs fast.

131
00:05:54,540 --> 00:05:58,140
A lot of you have customers
or your company in general

132
00:05:58,140 --> 00:05:59,910
is engaging in a solution that needs

133
00:05:59,910 --> 00:06:02,070
to be able to do quick inference,

134
00:06:02,070 --> 00:06:05,130
and you don't really need
to be running a 70B model.

135
00:06:05,130 --> 00:06:09,330
You might just fine-tune
8B and run with that.

136
00:06:09,330 --> 00:06:11,580
We also talked about 70B.

137
00:06:11,580 --> 00:06:14,640
Performance, super good on that one.

138
00:06:14,640 --> 00:06:16,740
You're able to go customize
that model as well,

139
00:06:16,740 --> 00:06:20,220
and you're seeing a lot of
conversational-type stuff,

140
00:06:20,220 --> 00:06:24,000
a lot of deep context-type
use cases being built there.

141
00:06:24,000 --> 00:06:26,520
And then we had Llama 4 earlier this year.

142
00:06:26,520 --> 00:06:27,570
This was when we released

143
00:06:27,570 --> 00:06:30,990
our first two big multimodal models.

144
00:06:30,990 --> 00:06:34,533
Again, image and text input, text output.

145
00:06:35,670 --> 00:06:37,470
And they allowed for the first time,

146
00:06:37,470 --> 00:06:41,130
our multimodal models allowing multi-image

147
00:06:41,130 --> 00:06:42,720
to be really capable.

148
00:06:42,720 --> 00:06:44,880
And Scout and Maverick were just

149
00:06:44,880 --> 00:06:47,100
two different size parameter models

150
00:06:47,100 --> 00:06:49,650
with slightly different
capabilities and context lengths.

151
00:06:49,650 --> 00:06:52,410
If you wanted 10 million context length,

152
00:06:52,410 --> 00:06:53,850
you could have gone with Scout.

153
00:06:53,850 --> 00:06:55,650
If you wanted something like Maverick,

154
00:06:55,650 --> 00:06:58,200
it would have a slightly
less context length on it.

155
00:06:59,380 --> 00:07:02,730
So let's talk about how can
you actually use these models?

156
00:07:02,730 --> 00:07:06,990
How can you go and deploy
and build for your solutions?

157
00:07:06,990 --> 00:07:09,840
So one of the ways that
you can do it is via AWS.

158
00:07:09,840 --> 00:07:11,280
We're all here for reInvent.

159
00:07:11,280 --> 00:07:13,140
Everybody, I assume, knows some of this.

160
00:07:13,140 --> 00:07:14,670
But just in case, I'll go ahead

161
00:07:14,670 --> 00:07:17,073
and just kinda talk a
little bit about this.

162
00:07:19,950 --> 00:07:23,310
So we have Amazon Bedrock here at the top.

163
00:07:23,310 --> 00:07:25,830
And with Bedrock, you've
got the ability to go

164
00:07:25,830 --> 00:07:29,670
build solutions quickly using
just a model-as-a-service,

165
00:07:29,670 --> 00:07:31,530
hosted API endpoint, right?

166
00:07:31,530 --> 00:07:33,030
You're not gonna run any infrastructure.

167
00:07:33,030 --> 00:07:34,980
You're not gonna handle
any of the deployment.

168
00:07:34,980 --> 00:07:37,890
You have some options to go
and customize a little bit.

169
00:07:37,890 --> 00:07:39,240
But really, you're just
getting an endpoint

170
00:07:39,240 --> 00:07:41,160
and you're sending in your
prompts and getting output.

171
00:07:41,160 --> 00:07:44,430
There's a bunch of features
built in, relatively hands-off.

172
00:07:44,430 --> 00:07:45,990
You don't have to go build that yourself.

173
00:07:45,990 --> 00:07:48,450
So you don't have to
orchestrate as much yourself.

174
00:07:48,450 --> 00:07:49,620
That's the idea there.

175
00:07:49,620 --> 00:07:54,540
All of the Llama models
have been on Bedrock

176
00:07:54,540 --> 00:07:57,207
for the most part, except for 1B and 3B.

177
00:07:57,207 --> 00:07:59,850
And I think the main models
that I'm seeing people

178
00:07:59,850 --> 00:08:02,640
use right now, 8B, 70B, those models

179
00:08:02,640 --> 00:08:04,350
are definitely available on Bedrock.

180
00:08:04,350 --> 00:08:06,060
So that's your fast get-up-and-go.

181
00:08:06,060 --> 00:08:07,470
You wanna prototype, go build.

182
00:08:07,470 --> 00:08:08,760
Don't worry about it.

183
00:08:08,760 --> 00:08:09,930
A lot of times you see people,

184
00:08:09,930 --> 00:08:11,370
when they wanna go to production,

185
00:08:11,370 --> 00:08:14,640
you're looking for more
throughput, lower latency.

186
00:08:14,640 --> 00:08:15,690
Maybe that's when you progress

187
00:08:15,690 --> 00:08:17,910
to using something like SageMaker, right?

188
00:08:17,910 --> 00:08:21,840
And you can go to using the
actual optimized chipsets,

189
00:08:21,840 --> 00:08:23,890
whether it be in French here or Trainium.

190
00:08:25,170 --> 00:08:29,790
Or if you're gonna go and
you wanna deploy on GPU,

191
00:08:29,790 --> 00:08:32,003
then you can go SageMaker, you can go EKS.

192
00:08:32,003 --> 00:08:35,340
You can even do EC2 if you
know how to manage everything.

193
00:08:35,340 --> 00:08:38,100
But those are also really
powerful in general.

194
00:08:38,100 --> 00:08:39,330
When you're thinking about SageMaker,

195
00:08:39,330 --> 00:08:40,500
if you're gonna go into fine-tuning,

196
00:08:40,500 --> 00:08:43,560
if you're gonna do clusters,
there's options there,

197
00:08:43,560 --> 00:08:46,620
like HyperPod to go ahead and
do a lot of that management

198
00:08:46,620 --> 00:08:48,450
without you having to think about it.

199
00:08:48,450 --> 00:08:53,450
All right, so progressing on a
little bit more now on Llama.

200
00:08:55,230 --> 00:08:59,210
So with Llama, something
that we did this year

201
00:08:59,210 --> 00:09:02,220
is we began releasing
tools to help you build.

202
00:09:02,220 --> 00:09:03,540
And the reason I'm talking about this

203
00:09:03,540 --> 00:09:05,820
when we're in a session about fine-tuning

204
00:09:05,820 --> 00:09:08,550
for coding use cases is
because a lot of people

205
00:09:08,550 --> 00:09:10,840
are looking at moving from other models

206
00:09:10,840 --> 00:09:14,760
or from older Llama
models to newer models.

207
00:09:14,760 --> 00:09:16,530
And one of the tools that we developed

208
00:09:16,530 --> 00:09:19,200
was basically Llama PromptOps.

209
00:09:19,200 --> 00:09:22,580
It's a way for you to go
and automatically optimize

210
00:09:22,580 --> 00:09:25,540
your prompts for the Llama models.

211
00:09:25,540 --> 00:09:28,500
It says you're optimizing
prompts for Llama models,

212
00:09:28,500 --> 00:09:30,840
and it doesn't say you
have to be on a Llama model

213
00:09:30,840 --> 00:09:32,820
to optimize for Llama.

214
00:09:32,820 --> 00:09:34,560
If you're coming from another model,

215
00:09:34,560 --> 00:09:38,420
that is an option you can go
and utilize in your workstream.

216
00:09:38,420 --> 00:09:40,410
It's just a simple Python package,

217
00:09:40,410 --> 00:09:43,860
pretty lightweight to run,
but you do have to download,

218
00:09:43,860 --> 00:09:47,340
deploy, and manage that
yourself at this time.

219
00:09:47,340 --> 00:09:50,433
So some of the benefits
there are listed above.

220
00:09:51,660 --> 00:09:54,810
I mean, you can imagine
less trial and error.

221
00:09:54,810 --> 00:09:57,540
You're not gonna be manually
telling your data science team

222
00:09:57,540 --> 00:10:00,690
or some prompt engineer folks to go

223
00:10:00,690 --> 00:10:04,470
and manually tweak your
prompts to make it work.

224
00:10:04,470 --> 00:10:07,860
Instead, you can go take
your entire data set,

225
00:10:07,860 --> 00:10:10,650
all your prompts, throw 'em through this,

226
00:10:10,650 --> 00:10:13,260
and have that conversion
happen for you, right?

227
00:10:13,260 --> 00:10:15,720
Helps accelerate optimizations.

228
00:10:15,720 --> 00:10:19,710
You get data-driven
improvement-type learning from this.

229
00:10:19,710 --> 00:10:21,963
So a lot of benefits overall.

230
00:10:23,070 --> 00:10:24,750
And you can kinda see a little bit

231
00:10:24,750 --> 00:10:27,510
about how the actual workflow is set up.

232
00:10:27,510 --> 00:10:29,670
Pretty simple, straightforward.

233
00:10:29,670 --> 00:10:32,820
But this is one of the things
that we kicked off this year

234
00:10:32,820 --> 00:10:34,980
as a tool in addition to a few others,

235
00:10:34,980 --> 00:10:36,783
which we'll mention a little later.

236
00:10:38,460 --> 00:10:42,210
So now let's dive into customizing
the actual Llama models.

237
00:10:42,210 --> 00:10:47,210
And the idea here being, let's
think about customization,

238
00:10:47,220 --> 00:10:49,920
customization for coding tasks

239
00:10:49,920 --> 00:10:53,820
when we're trying to go
and train for that, right?

240
00:10:53,820 --> 00:10:55,880
So the Llama models as they are

241
00:10:55,880 --> 00:10:59,320
can do some coding
capabilities within them

242
00:10:59,320 --> 00:11:01,830
if you're just using the general models.

243
00:11:01,830 --> 00:11:03,840
It's not necessarily the best.

244
00:11:03,840 --> 00:11:06,060
It's not what you're gonna be wanting

245
00:11:06,060 --> 00:11:07,620
for very specific use cases.

246
00:11:07,620 --> 00:11:09,000
And with specific languages,

247
00:11:09,000 --> 00:11:11,970
you're gonna probably need
to do a lot more fine-tuning

248
00:11:11,970 --> 00:11:13,650
and customization work to ensure

249
00:11:13,650 --> 00:11:15,200
it's working for what you need.

250
00:11:16,080 --> 00:11:21,080
So let's talk about, actually
before I move forward, yeah.

251
00:11:21,120 --> 00:11:23,820
Let's go ahead and talk
about some of the reasons

252
00:11:23,820 --> 00:11:27,180
why adapting the LLAMs in general,

253
00:11:27,180 --> 00:11:29,790
not just Llama, is challenging.

254
00:11:29,790 --> 00:11:32,430
So we all probably have worked

255
00:11:32,430 --> 00:11:34,440
with programming languages before.

256
00:11:34,440 --> 00:11:35,820
They all have different syntaxes

257
00:11:35,820 --> 00:11:39,570
depending on which versions
you're using, things change.

258
00:11:39,570 --> 00:11:41,610
That creates a lot of complication.

259
00:11:41,610 --> 00:11:44,220
Models can get it.

260
00:11:44,220 --> 00:11:46,410
If you use something like CodeLlama,

261
00:11:46,410 --> 00:11:48,570
then it did a pretty good
job at figuring this out

262
00:11:48,570 --> 00:11:51,140
with the main languages it was trained on.

263
00:11:51,140 --> 00:11:53,070
But there's a lot of complication

264
00:11:53,070 --> 00:11:55,800
behind recognizing and generating code

265
00:11:55,800 --> 00:11:59,760
that really adheres to
a specific language.

266
00:11:59,760 --> 00:12:01,743
Next, data quality.

267
00:12:02,820 --> 00:12:05,253
We all know if you're training models,

268
00:12:06,180 --> 00:12:07,380
datasets are super important.

269
00:12:07,380 --> 00:12:11,250
And we're in a completely
different world right now

270
00:12:11,250 --> 00:12:13,320
than we were even a year ago

271
00:12:13,320 --> 00:12:16,140
with regards to datasets
that are available,

272
00:12:16,140 --> 00:12:17,430
datasets you can purchase,

273
00:12:17,430 --> 00:12:19,470
datasets that you are curating yourself

274
00:12:19,470 --> 00:12:21,633
from your internal actual data.

275
00:12:22,800 --> 00:12:25,140
Completely different capability of tools

276
00:12:25,140 --> 00:12:26,730
are available to you.

277
00:12:26,730 --> 00:12:30,900
And that is still a very
challenging thing for coding.

278
00:12:30,900 --> 00:12:32,610
And the reason for that is

279
00:12:32,610 --> 00:12:35,310
coding datasets are generally smaller.

280
00:12:35,310 --> 00:12:36,840
They're a little harder to come by.

281
00:12:36,840 --> 00:12:38,970
They are becoming a lot more available.

282
00:12:38,970 --> 00:12:42,240
You're finding a lot of
stuff available open source,

283
00:12:42,240 --> 00:12:45,060
which is great, but then
that doesn't really tailor

284
00:12:45,060 --> 00:12:46,740
specifically to what you need

285
00:12:46,740 --> 00:12:50,400
or your specific company's
standards, right?

286
00:12:50,400 --> 00:12:51,840
And everybody has those.

287
00:12:51,840 --> 00:12:55,110
So that's an extremely
difficult and challenging task

288
00:12:55,110 --> 00:12:57,600
when you're thinking about coding as well.

289
00:12:57,600 --> 00:12:59,460
And then evaluations.

290
00:12:59,460 --> 00:13:02,280
Evals, I think, have
been a pretty hot topic

291
00:13:02,280 --> 00:13:04,230
'cause everybody went
the last couple of years

292
00:13:04,230 --> 00:13:08,640
from experimenting and
prototyping to figuring out,

293
00:13:08,640 --> 00:13:12,660
okay, these are cool applications
we developed with LLMs,

294
00:13:12,660 --> 00:13:14,130
but now we wanna go to production.

295
00:13:14,130 --> 00:13:15,870
How do we ensure this is actually working

296
00:13:15,870 --> 00:13:19,040
and how do we ensure we're
able to properly validate

297
00:13:19,040 --> 00:13:21,240
and eval these things?

298
00:13:21,240 --> 00:13:23,703
Coding, assessing it is complicated.

299
00:13:25,220 --> 00:13:27,900
It's just not straightforward
to your code base,

300
00:13:27,900 --> 00:13:31,650
kind of like datasets aren't
straightforward always with it,

301
00:13:31,650 --> 00:13:32,850
and there's not a lot out there.

302
00:13:32,850 --> 00:13:36,063
So just some challenges
to talk about there.

303
00:13:36,990 --> 00:13:40,590
So let's think about how to
get started with LLM models

304
00:13:40,590 --> 00:13:42,180
if you're thinking about fine-tuning here.

305
00:13:42,180 --> 00:13:43,920
Really high-level steps,

306
00:13:43,920 --> 00:13:46,920
but this is just kind of a
little flow you would imagine

307
00:13:46,920 --> 00:13:49,230
as you're thinking about
any sort of fine-tuning

308
00:13:49,230 --> 00:13:52,500
you're gonna do, not only coding, right?

309
00:13:52,500 --> 00:13:55,410
So in any sort of fine-tuning
you're gonna be doing,

310
00:13:55,410 --> 00:13:58,110
you're going to be doing
that data ingestion.

311
00:13:58,110 --> 00:14:00,900
You've gotta go ahead
and get your data set up

312
00:14:00,900 --> 00:14:04,260
so you can actually do
that fine-tuning pass

313
00:14:04,260 --> 00:14:05,970
or whatever training you're doing.

314
00:14:05,970 --> 00:14:06,930
So you're gonna be looking at

315
00:14:06,930 --> 00:14:08,730
code and programming resources,

316
00:14:08,730 --> 00:14:12,330
your internal datasets,
your internal code repos.

317
00:14:12,330 --> 00:14:14,340
That's all gonna be raw data to an extent

318
00:14:14,340 --> 00:14:16,440
unless you can find stuff that's curated

319
00:14:16,440 --> 00:14:18,300
and already pre-processed.

320
00:14:18,300 --> 00:14:19,700
So the next thing you're gonna go into

321
00:14:19,700 --> 00:14:24,390
is that step of preparing
that code by pre-processing,

322
00:14:24,390 --> 00:14:27,333
cleaning it up, ensuring you don't have,

323
00:14:29,070 --> 00:14:31,920
let's imagine comments and a whole bunch

324
00:14:31,920 --> 00:14:34,050
of really custom flares sitting in code,

325
00:14:34,050 --> 00:14:37,230
which can exist depending
on your engineers,

326
00:14:37,230 --> 00:14:39,840
depending on the teams,
depending on the resources

327
00:14:39,840 --> 00:14:41,820
that you're actually
downloading this from.

328
00:14:41,820 --> 00:14:44,010
You go into optimizing that code,

329
00:14:44,010 --> 00:14:47,460
optimizing that for code
generation tasks and training,

330
00:14:47,460 --> 00:14:51,270
right, and you're looking
at how do I set this up

331
00:14:51,270 --> 00:14:53,220
in a way which you've got a dataset now

332
00:14:53,220 --> 00:14:55,140
from all this code and the
resources you've developed,

333
00:14:55,140 --> 00:14:57,030
you've pre-processed, and
you're ensuring it's set up

334
00:14:57,030 --> 00:14:58,620
in a way that can begin helping

335
00:14:58,620 --> 00:15:00,333
with your actual training passes.

336
00:15:01,380 --> 00:15:03,750
And then you're gonna go
through your eval steps.

337
00:15:03,750 --> 00:15:06,210
You're gonna ensure that the
quality, all of that stuff,

338
00:15:06,210 --> 00:15:08,670
from a high-level standpoint.

339
00:15:08,670 --> 00:15:10,230
Then you go into your deployment.

340
00:15:10,230 --> 00:15:13,590
So super high-level steps,
this kind of a workflow

341
00:15:13,590 --> 00:15:15,540
that you're gonna run through.

342
00:15:15,540 --> 00:15:17,690
So let's talk a little
bit about data prep.

343
00:15:18,930 --> 00:15:20,880
And some of these slides
here will have a little bit

344
00:15:20,880 --> 00:15:24,180
of duplication between them
because during these initial

345
00:15:24,180 --> 00:15:26,100
stages where you're
setting up for fine-tuning

346
00:15:26,100 --> 00:15:28,020
and doing your training passes,

347
00:15:28,020 --> 00:15:30,300
a lot of times you're gonna be
doing iteration on your data

348
00:15:30,300 --> 00:15:32,500
and you're gonna be figuring
out how to structure things

349
00:15:32,500 --> 00:15:35,100
in better ways and then how to optimize

350
00:15:35,100 --> 00:15:37,590
your configuration for
training, all of that.

351
00:15:37,590 --> 00:15:39,240
So when we think about data prep,

352
00:15:39,240 --> 00:15:40,800
really what you're doing
is you're organizing

353
00:15:40,800 --> 00:15:42,840
and putting together your code datasets

354
00:15:42,840 --> 00:15:45,480
so you can begin processing,
the first step always.

355
00:15:45,480 --> 00:15:47,370
You're gonna be processing
all your datasets

356
00:15:47,370 --> 00:15:50,760
or basically acquiring it and then going

357
00:15:50,760 --> 00:15:52,500
and setting up the pre-processing

358
00:15:52,500 --> 00:15:54,450
and then going into the training.

359
00:15:54,450 --> 00:15:58,920
So then we talk about,
you've now acquired the data,

360
00:15:58,920 --> 00:16:01,680
now we've gotta actually process this

361
00:16:01,680 --> 00:16:04,080
and do some cleaning, right?

362
00:16:04,080 --> 00:16:06,750
So what do you end up doing in this?

363
00:16:06,750 --> 00:16:08,540
You're gonna go in and you need to refine

364
00:16:08,540 --> 00:16:12,180
and secure your code for
whatever your use case is.

365
00:16:12,180 --> 00:16:14,220
Whether it be as a coding assistant

366
00:16:14,220 --> 00:16:15,140
that you're gonna have running

367
00:16:15,140 --> 00:16:18,660
or you have something
specific to mainframes

368
00:16:18,660 --> 00:16:21,600
or let's say low-resource languages

369
00:16:21,600 --> 00:16:24,840
or low-resource coding tasks,

370
00:16:24,840 --> 00:16:26,520
those are all gonna be opportunities there

371
00:16:26,520 --> 00:16:28,500
for you to possibly be setting

372
00:16:28,500 --> 00:16:31,500
your actual training material up for.

373
00:16:31,500 --> 00:16:35,400
And that might include kind
of a handful of these tasks.

374
00:16:35,400 --> 00:16:38,160
Again, this is a pretty
high-level run-through

375
00:16:38,160 --> 00:16:39,990
of the things you're
gonna need to consider,

376
00:16:39,990 --> 00:16:41,460
but you might have to go and segment

377
00:16:41,460 --> 00:16:43,410
and filter the code by language.

378
00:16:43,410 --> 00:16:45,060
You're gonna probably
have multiple languages

379
00:16:45,060 --> 00:16:47,670
in your repos, all of that good stuff.

380
00:16:47,670 --> 00:16:50,130
So you're gonna go and set that up,

381
00:16:50,130 --> 00:16:54,540
make sure that things are
properly pre-processed

382
00:16:54,540 --> 00:16:59,070
and filtered out by languages
and different versionings.

383
00:16:59,070 --> 00:17:01,380
You're gonna sanitize things.

384
00:17:01,380 --> 00:17:04,980
All of our repos probably
have some sort of

385
00:17:04,980 --> 00:17:08,490
sensitive data or information
that may need to get stripped.

386
00:17:08,490 --> 00:17:10,577
You wanna go and make sure
that's been taken care of.

387
00:17:10,577 --> 00:17:13,050
You're gonna begin
looking at tokenization.

388
00:17:13,050 --> 00:17:14,060
Do you have proper tokenization?

389
00:17:14,060 --> 00:17:16,360
Do you have to have custom tokenizer?

390
00:17:16,360 --> 00:17:20,010
Is there something
there that is gonna work

391
00:17:20,010 --> 00:17:20,843
for your data set?

392
00:17:20,843 --> 00:17:23,730
You're gonna have to
iterate on that a few times.

393
00:17:23,730 --> 00:17:27,930
Then we're moving into basically ensuring

394
00:17:27,930 --> 00:17:30,030
if you have anything that
is privacy-sensitive,

395
00:17:30,030 --> 00:17:32,550
you've gotta go and kind
of ensure the transforms

396
00:17:32,550 --> 00:17:34,860
and the transformations
that are sitting there

397
00:17:34,860 --> 00:17:37,440
are properly occurring.

398
00:17:37,440 --> 00:17:40,230
And that's gonna require
a little bit of review.

399
00:17:40,230 --> 00:17:41,790
So you do your pre-processing and cleaning

400
00:17:41,790 --> 00:17:45,360
of your data sets, kind
of iterate from there.

401
00:17:45,360 --> 00:17:49,410
So super simple, a little example here.

402
00:17:49,410 --> 00:17:50,610
I won't even read through this,

403
00:17:50,610 --> 00:17:54,663
but just a way to go ahead
and do some code cleanup.

404
00:17:55,980 --> 00:17:58,320
Very simple.

405
00:17:58,320 --> 00:18:02,430
Then let's go into a little
bit of tokenization again.

406
00:18:02,430 --> 00:18:05,820
Just some really simple
snippets here, just as examples.

407
00:18:05,820 --> 00:18:07,500
A lot of this is a lot more descriptive

408
00:18:07,500 --> 00:18:09,300
if you go into Llama Recipes

409
00:18:09,300 --> 00:18:10,800
and you look at a lot of our fine-tuning

410
00:18:10,800 --> 00:18:12,963
or end-to-end use case examples.

411
00:18:14,250 --> 00:18:18,300
But I figured I'm not gonna
walk through it line by line,

412
00:18:18,300 --> 00:18:19,983
so let's keep it simple here.

413
00:18:23,100 --> 00:18:25,770
So let's talk about the
actual fine-tuning situation.

414
00:18:25,770 --> 00:18:28,650
So you've gone, you've got your,
you've aggregated the data,

415
00:18:28,650 --> 00:18:30,960
you've gone and cleaned
it up and processed.

416
00:18:30,960 --> 00:18:32,460
Now you're actually
getting to fine-tuning.

417
00:18:32,460 --> 00:18:37,260
Fine-tuning is not a
one-pass-and-done type of thing.

418
00:18:37,260 --> 00:18:40,470
A lot of times you're doing
several loops of fine-tuning.

419
00:18:40,470 --> 00:18:41,580
You're gonna go, you're gonna iterate,

420
00:18:41,580 --> 00:18:43,950
you're gonna look at what
your loss values are,

421
00:18:43,950 --> 00:18:45,510
you're gonna look at what
the quality looks like,

422
00:18:45,510 --> 00:18:47,040
you're gonna eval things,

423
00:18:47,040 --> 00:18:49,950
and you're gonna determine, do
you need to adjust datasets?

424
00:18:49,950 --> 00:18:52,260
Do you need to adjust parameters?

425
00:18:52,260 --> 00:18:55,170
It's a involved process.

426
00:18:55,170 --> 00:18:59,160
And when you think about
optimizing code generation

427
00:18:59,160 --> 00:19:02,430
as a particular kind of example here,

428
00:19:02,430 --> 00:19:04,740
we've got some tips here for
how you would think about it.

429
00:19:04,740 --> 00:19:06,660
So if you're looking to do it

430
00:19:06,660 --> 00:19:09,000
and you're looking to
fine-tune a specific model,

431
00:19:09,000 --> 00:19:12,240
8B is still a model that people
are trying to use, right?

432
00:19:12,240 --> 00:19:14,853
8B's very, it's relatively lean.

433
00:19:16,110 --> 00:19:18,300
It works pretty well, it's fast,

434
00:19:18,300 --> 00:19:20,880
and it can solve a lot
of specific problems

435
00:19:20,880 --> 00:19:22,650
if you go and customize the model.

436
00:19:22,650 --> 00:19:24,390
70B has a lot more capability.

437
00:19:24,390 --> 00:19:26,580
A lot of people will start there.

438
00:19:26,580 --> 00:19:28,770
When you're thinking about
which model to choose,

439
00:19:28,770 --> 00:19:30,750
you'll start at 70B or
something like that,

440
00:19:30,750 --> 00:19:32,310
and then if it works well,

441
00:19:32,310 --> 00:19:34,470
they'll look at how do we
maybe compress that down

442
00:19:34,470 --> 00:19:37,860
to an 8B model just to save, right?

443
00:19:37,860 --> 00:19:41,460
So then you'll, as you're
running your fine-tuning passes,

444
00:19:41,460 --> 00:19:43,650
you start thinking about
getting this running,

445
00:19:43,650 --> 00:19:44,483
you're gonna think about

446
00:19:44,483 --> 00:19:47,010
how do you actually
implement this fine-tuning

447
00:19:47,010 --> 00:19:50,160
and how do you run this
in a efficient way?

448
00:19:50,160 --> 00:19:53,160
So you'll look at
different SFT techniques,

449
00:19:53,160 --> 00:19:55,140
so supervised fine-tuning.

450
00:19:55,140 --> 00:20:00,140
You'll look at am I going to
run basically full parameter?

451
00:20:02,400 --> 00:20:05,250
Am I gonna run something compressed?

452
00:20:05,250 --> 00:20:06,780
It's totally up to you,

453
00:20:06,780 --> 00:20:09,750
and a lot of people will
try to go on like Peft

454
00:20:09,750 --> 00:20:11,610
or LoRa or something like that,

455
00:20:11,610 --> 00:20:13,110
and that's what they're gonna do in first,

456
00:20:13,110 --> 00:20:15,180
and then they're gonna
kind of evolve from there.

457
00:20:15,180 --> 00:20:17,250
You're gonna look at nowadays RL,

458
00:20:17,250 --> 00:20:18,660
which is a pretty hot topic,

459
00:20:18,660 --> 00:20:23,660
but in general, RL's a
pretty well-known thing.

460
00:20:24,360 --> 00:20:27,810
People have been doing
RLHF for many, many years.

461
00:20:27,810 --> 00:20:31,860
That's definitely a way to
improve your actual fine-tuning

462
00:20:31,860 --> 00:20:34,680
and optimize your actual process here,

463
00:20:34,680 --> 00:20:36,510
and then you're gonna look at can,

464
00:20:36,510 --> 00:20:38,670
do I wanna actually run
and manage all this stuff

465
00:20:38,670 --> 00:20:41,180
or do I wanna use tools that are available

466
00:20:41,180 --> 00:20:43,590
that can automate and build
out a lot of this process

467
00:20:43,590 --> 00:20:46,080
for me, and that's where I
have the bullet point there,

468
00:20:46,080 --> 00:20:48,690
the check mark around SageMaker.

469
00:20:48,690 --> 00:20:50,040
That's just an option for you to go

470
00:20:50,040 --> 00:20:52,800
and use a robust training infrastructure

471
00:20:52,800 --> 00:20:56,370
with the tools and all the
actual features built in,

472
00:20:56,370 --> 00:20:57,840
so you don't have to go and orchestrate

473
00:20:57,840 --> 00:21:01,470
this fine-tuning pipeline and
all the iterations manually.

474
00:21:01,470 --> 00:21:02,880
You can just use that.

475
00:21:02,880 --> 00:21:05,010
You also have the ability
to go to like Hyperpod,

476
00:21:05,010 --> 00:21:08,430
and if you need to do
distributed-type training,

477
00:21:08,430 --> 00:21:10,503
EKS, and all of that.

478
00:21:12,540 --> 00:21:15,560
So pushing on, just a super easy example

479
00:21:15,560 --> 00:21:17,700
of what you're gonna look
at if you're gonna set up

480
00:21:17,700 --> 00:21:21,240
an actual run of fine-tuning here.

481
00:21:21,240 --> 00:21:22,740
You're gonna go ahead and set up the code,

482
00:21:22,740 --> 00:21:25,080
set up the configuration parameters,

483
00:21:25,080 --> 00:21:26,190
and then let it run, right?

484
00:21:26,190 --> 00:21:28,140
This is an example running through Epochs.

485
00:21:28,140 --> 00:21:31,380
That doesn't mean much,
depending on your use case.

486
00:21:31,380 --> 00:21:33,030
You're gonna go ahead and
have to iterate on that

487
00:21:33,030 --> 00:21:35,880
or figure out what's
worked for you guys before,

488
00:21:35,880 --> 00:21:38,010
look at what your learning
rates are, loss rates,

489
00:21:38,010 --> 00:21:41,940
all of that, understand, am I
getting the progress we want?

490
00:21:41,940 --> 00:21:43,770
Is this eval-ing?

491
00:21:43,770 --> 00:21:46,590
You're gonna pick,
obviously, instance types.

492
00:21:46,590 --> 00:21:47,970
So that's a GPU.

493
00:21:47,970 --> 00:21:52,260
Many of us were probably using,
I think that's the A100s.

494
00:21:52,260 --> 00:21:55,320
So a lot of times you're gonna either pick

495
00:21:55,320 --> 00:21:57,060
with the larger Llama models,

496
00:21:57,060 --> 00:21:59,820
you're gonna be running like a P4D, a P5,

497
00:21:59,820 --> 00:22:01,770
something like that.

498
00:22:01,770 --> 00:22:03,390
You could also use Trainium for this.

499
00:22:03,390 --> 00:22:06,723
It is supported, and those
are all options there.

500
00:22:08,900 --> 00:22:12,360
So eval, next big topic.

501
00:22:12,360 --> 00:22:14,790
Evaluation is super critical.

502
00:22:14,790 --> 00:22:16,520
As you're going to do fine-tuning,

503
00:22:16,520 --> 00:22:18,810
as you're gonna be looking
at, did this actually work?

504
00:22:18,810 --> 00:22:22,560
Is this something that's
sufficient for my customer's needs?

505
00:22:22,560 --> 00:22:25,230
You're probably gonna
have an eval data set

506
00:22:25,230 --> 00:22:26,730
or you're gonna have to generate one

507
00:22:26,730 --> 00:22:28,830
for evaluating and validating

508
00:22:28,830 --> 00:22:32,223
that your fine-tuned model is
performing like you wanted.

509
00:22:33,090 --> 00:22:37,200
So you're gonna have to set
up basically a reliable way

510
00:22:37,200 --> 00:22:40,920
for you to determine, is my
code generation now improved

511
00:22:40,920 --> 00:22:42,060
after I've done my fine-tuning?

512
00:22:42,060 --> 00:22:46,800
That's gonna help inform you,
did my training data set help?

513
00:22:46,800 --> 00:22:48,390
Did my configuration parameters help?

514
00:22:48,390 --> 00:22:50,040
Do I need to run more epochs?

515
00:22:50,040 --> 00:22:52,140
Do I need to change and pivot things?

516
00:22:52,140 --> 00:22:56,070
I'd say at least when I
run through training loops,

517
00:22:56,070 --> 00:22:59,910
it's more so gonna be at
least five or more passes

518
00:22:59,910 --> 00:23:04,740
on my fine-tuning, and that's
gonna lead me towards the,

519
00:23:04,740 --> 00:23:07,500
let's say the eval metrics that I want,

520
00:23:07,500 --> 00:23:09,690
but that's 100% dependent on you

521
00:23:09,690 --> 00:23:12,330
and what sort of data sets that you have

522
00:23:12,330 --> 00:23:13,530
when you go into this.

523
00:23:13,530 --> 00:23:16,680
So that's one of the
reasons we really emphasize

524
00:23:16,680 --> 00:23:19,980
data sets are super critical
when you're going into this,

525
00:23:19,980 --> 00:23:22,050
having a really well-established one,

526
00:23:22,050 --> 00:23:23,550
one that's really well-formed,

527
00:23:23,550 --> 00:23:27,750
is going to help ideally
reduce a lot of the complexity

528
00:23:27,750 --> 00:23:29,400
and passes you'll have to run,

529
00:23:29,400 --> 00:23:32,220
but again, that's not
necessarily guaranteed.

530
00:23:32,220 --> 00:23:33,630
Oops.

531
00:23:33,630 --> 00:23:36,480
So let's just continue through this slide.

532
00:23:36,480 --> 00:23:37,830
So I mentioned things,

533
00:23:37,830 --> 00:23:39,210
but let's look at the bullet points here.

534
00:23:39,210 --> 00:23:42,270
So you're gonna want, with
evals, understand key metrics,

535
00:23:42,270 --> 00:23:44,430
so your customers are gonna have key needs

536
00:23:44,430 --> 00:23:45,930
they want to perform at a certain level

537
00:23:45,930 --> 00:23:48,000
of accuracy and confidence.

538
00:23:48,000 --> 00:23:49,290
That's gonna be one thing.

539
00:23:49,290 --> 00:23:51,720
When their coders are
using a coding assistant

540
00:23:51,720 --> 00:23:53,160
or a code generation,

541
00:23:53,160 --> 00:23:54,480
you're gonna wanna make sure that works.

542
00:23:54,480 --> 00:23:56,820
You're gonna wanna make sure
it's creating compilable code.

543
00:23:56,820 --> 00:23:57,653
You're gonna wanna make sure

544
00:23:57,653 --> 00:24:00,000
it's doing stuff contextually aware.

545
00:24:00,000 --> 00:24:01,020
So that's gonna be something

546
00:24:01,020 --> 00:24:03,090
you need to build into your evals.

547
00:24:03,090 --> 00:24:05,790
That also includes things like human eval

548
00:24:05,790 --> 00:24:07,800
and custom internal datasets.

549
00:24:07,800 --> 00:24:09,420
You're gonna have, ideally,

550
00:24:09,420 --> 00:24:13,410
ways to monitor the
evaluations continuously

551
00:24:13,410 --> 00:24:16,260
and run these things post-deployment.

552
00:24:16,260 --> 00:24:18,270
And then there's gonna
be generalized metrics

553
00:24:18,270 --> 00:24:19,830
that you can also kind of depend on,

554
00:24:19,830 --> 00:24:23,460
like blue or any other
kind of standardized ones

555
00:24:23,460 --> 00:24:24,410
that are out there.

556
00:24:25,290 --> 00:24:27,360
So let's think about deployment now.

557
00:24:27,360 --> 00:24:29,910
So you've gone, you've decided
to customize the model,

558
00:24:29,910 --> 00:24:31,410
you've gone through your pre-processing,

559
00:24:31,410 --> 00:24:33,090
you've fine-tuned, you decide,

560
00:24:33,090 --> 00:24:36,510
this version, this
customized model weights

561
00:24:36,510 --> 00:24:37,740
are ready to go and deploy.

562
00:24:37,740 --> 00:24:39,630
Maybe they're not perfect, maybe they are.

563
00:24:39,630 --> 00:24:40,890
You're gonna go deploy it.

564
00:24:40,890 --> 00:24:42,210
A lot of options to deploy.

565
00:24:42,210 --> 00:24:44,010
We've mentioned them already.

566
00:24:44,010 --> 00:24:46,290
Bedrock allows for managed inference,

567
00:24:46,290 --> 00:24:48,120
but also allows for custom model import,

568
00:24:48,120 --> 00:24:50,040
which you can take advantage of,

569
00:24:50,040 --> 00:24:51,180
especially when you're prototyping

570
00:24:51,180 --> 00:24:52,440
or doing a lot of these loops

571
00:24:52,440 --> 00:24:54,570
and you don't wanna go set
up all the infrastructure

572
00:24:54,570 --> 00:24:56,220
and manage that too.

573
00:24:56,220 --> 00:24:57,450
That's an option.

574
00:24:57,450 --> 00:24:59,130
We have things like EKS.

575
00:24:59,130 --> 00:25:01,830
A lot of people, when they
decide to go into production

576
00:25:01,830 --> 00:25:04,350
or if you have a mature organization

577
00:25:04,350 --> 00:25:06,450
that knows how to manage infrastructure,

578
00:25:06,450 --> 00:25:08,910
you're looking at options like EKS

579
00:25:08,910 --> 00:25:11,610
for custom kind of deployment
of model weights, right?

580
00:25:13,080 --> 00:25:16,770
And then you have other
considerations with code in general.

581
00:25:16,770 --> 00:25:18,180
So now you're looking at,

582
00:25:18,180 --> 00:25:19,890
you're not just deploying to an endpoint

583
00:25:19,890 --> 00:25:21,330
and allowing people to prompt it.

584
00:25:21,330 --> 00:25:23,490
You're looking at integrations with IDEs.

585
00:25:23,490 --> 00:25:25,590
You're looking at, does
this go into VS Code?

586
00:25:25,590 --> 00:25:27,480
Does it go into my other IDEs

587
00:25:27,480 --> 00:25:29,190
that are available to my engineers?

588
00:25:29,190 --> 00:25:30,480
Does it go into a web client?

589
00:25:30,480 --> 00:25:32,640
A lot of options there.

590
00:25:32,640 --> 00:25:34,380
Then a lot of open questions on

591
00:25:34,380 --> 00:25:37,980
how does that actually go
through our repo updates

592
00:25:37,980 --> 00:25:41,520
or our source control
and versioning updates?

593
00:25:41,520 --> 00:25:43,410
Do we have to keep custom
parameters in there

594
00:25:43,410 --> 00:25:46,260
that state where code is being generated

595
00:25:46,260 --> 00:25:48,270
versus being manually edited?

596
00:25:48,270 --> 00:25:50,310
A lot of considerations there.

597
00:25:50,310 --> 00:25:52,350
You're also looking at deployment.

598
00:25:52,350 --> 00:25:55,470
Okay, are you gonna be setting
up custom REST APIs for this?

599
00:25:55,470 --> 00:25:58,260
And then you're gonna be
looking at code review bots.

600
00:25:58,260 --> 00:26:00,360
How do those integrate with things?

601
00:26:00,360 --> 00:26:02,880
A lot of us see that
within our organizations

602
00:26:02,880 --> 00:26:05,310
where you have code review bots

603
00:26:05,310 --> 00:26:09,870
and basically automated systems
to help catch these things,

604
00:26:09,870 --> 00:26:12,480
catch errors, catch issues before they hit

605
00:26:12,480 --> 00:26:15,900
even like a alpha train
or something like that

606
00:26:15,900 --> 00:26:19,260
or before it goes into
any deployment passes.

607
00:26:19,260 --> 00:26:21,080
But all of these are things
you're gonna be considering

608
00:26:21,080 --> 00:26:22,880
as you're thinking about deployment.

609
00:26:24,600 --> 00:26:28,680
So deployment and just another
example here using FastAPI.

610
00:26:28,680 --> 00:26:30,420
I won't necessarily go through this,

611
00:26:30,420 --> 00:26:33,390
but just another option
as you're looking at

612
00:26:33,390 --> 00:26:34,950
basic quick deployment options

613
00:26:34,950 --> 00:26:37,710
and quick deployment capabilities.

614
00:26:37,710 --> 00:26:39,720
There's a lot of options like this too,

615
00:26:39,720 --> 00:26:41,770
but I thought I'd just put up an example.

616
00:26:47,340 --> 00:26:52,340
So I spoke a lot in that
past 25, 30 minutes.

617
00:26:52,950 --> 00:26:55,260
Best practices for deploying Llama models.

618
00:26:55,260 --> 00:26:57,780
Let's ensure that data is available

619
00:26:57,780 --> 00:27:01,320
is basically deduped, cleaned up,

620
00:27:01,320 --> 00:27:03,213
pre-processed before training.

621
00:27:04,510 --> 00:27:06,840
In general, they're open source models.

622
00:27:06,840 --> 00:27:09,990
We like to really create
an open community.

623
00:27:09,990 --> 00:27:12,630
We love the idea of contributing
back as you've learned,

624
00:27:12,630 --> 00:27:14,430
maybe not obviously datasets.

625
00:27:14,430 --> 00:27:15,660
A lot of that has information

626
00:27:15,660 --> 00:27:17,010
that's confidential or private.

627
00:27:17,010 --> 00:27:17,850
We understand that.

628
00:27:17,850 --> 00:27:19,530
That's the power of open source models.

629
00:27:19,530 --> 00:27:22,680
You can train for your own
purposes and use cases,

630
00:27:22,680 --> 00:27:25,500
but then contributing back is a big thing.

631
00:27:25,500 --> 00:27:27,240
Basically optimizing inference costs

632
00:27:27,240 --> 00:27:29,640
by picking the right models
is always a critical thing.

633
00:27:29,640 --> 00:27:31,350
You look to train a model.

634
00:27:31,350 --> 00:27:33,270
You maybe don't wanna be running 70B

635
00:27:33,270 --> 00:27:34,680
unless it's a complex task.

636
00:27:34,680 --> 00:27:36,870
So you look at how do you minimize that?

637
00:27:36,870 --> 00:27:39,000
You look at an option with 8B.

638
00:27:39,000 --> 00:27:41,430
That's why we have so many
models that we've made available.

639
00:27:41,430 --> 00:27:45,990
It gives you that
customization capability,

640
00:27:45,990 --> 00:27:47,610
the ability to choose what you want,

641
00:27:47,610 --> 00:27:48,590
how much you wanna spend

642
00:27:48,590 --> 00:27:51,540
and what sort of
performance you're getting.

643
00:27:51,540 --> 00:27:53,550
And then just in general,

644
00:27:53,550 --> 00:27:55,890
when you're looking at
working with Llama models,

645
00:27:55,890 --> 00:27:59,130
engage with the open source
community, engage with us.

646
00:27:59,130 --> 00:28:02,310
We do obviously look to
developers for feedback

647
00:28:02,310 --> 00:28:05,580
and to continue growing
the Llama community

648
00:28:05,580 --> 00:28:09,690
and also effectively create
better models in the future.

649
00:28:09,690 --> 00:28:12,900
Now, let's think about real world examples

650
00:28:12,900 --> 00:28:13,803
for just a minute.

651
00:28:14,790 --> 00:28:17,550
So real world examples on the code side,

652
00:28:17,550 --> 00:28:19,440
you're looking at internal tools.

653
00:28:19,440 --> 00:28:22,980
You're looking to basically
accelerate development,

654
00:28:22,980 --> 00:28:25,110
boost productivity.

655
00:28:25,110 --> 00:28:27,470
That's a big reasoning we're using LLMs

656
00:28:27,470 --> 00:28:30,570
and that's why we're getting
these assessments developed.

657
00:28:30,570 --> 00:28:33,030
Automated code reviews, huge use case.

658
00:28:33,030 --> 00:28:34,680
A lot of this stuff is happening.

659
00:28:34,680 --> 00:28:38,280
Anytime somebody submits
a diff and that gets PR'd,

660
00:28:38,280 --> 00:28:40,050
those types of things are
going through code reviews.

661
00:28:40,050 --> 00:28:43,470
Even when a diff gets submit,
code reviews kick off.

662
00:28:43,470 --> 00:28:46,290
All of those things take
time, they take money,

663
00:28:46,290 --> 00:28:49,490
they take compute in order
to run and accelerate.

664
00:28:49,490 --> 00:28:51,840
Not all of that's running, obviously,

665
00:28:51,840 --> 00:28:54,450
via humans being tied in.

666
00:28:54,450 --> 00:28:55,350
It's all automated.

667
00:28:55,350 --> 00:28:58,500
There's a lot of old
ML ways of doing that.

668
00:28:58,500 --> 00:29:01,500
Now, LLMs are coming in and
they're helping to use NLP

669
00:29:01,500 --> 00:29:03,630
to be able to do that type of thing too.

670
00:29:03,630 --> 00:29:04,680
And it works very well,

671
00:29:04,680 --> 00:29:06,150
especially when you customize your model

672
00:29:06,150 --> 00:29:09,390
and you orchestrate multi-models
for those use cases.

673
00:29:09,390 --> 00:29:11,250
And then just streamlining DevOps.

674
00:29:11,250 --> 00:29:14,280
So you're talking about customizing models

675
00:29:14,280 --> 00:29:17,010
and basically being able to use agents

676
00:29:17,010 --> 00:29:18,330
for the same purposes,

677
00:29:18,330 --> 00:29:21,810
but then allowing orchestration
of multiple tasks.

678
00:29:21,810 --> 00:29:23,790
If you're looking at orchestrating agents

679
00:29:23,790 --> 00:29:26,160
to go and do your fine
tuning from end to end

680
00:29:26,160 --> 00:29:27,900
or thinking about data processing,

681
00:29:27,900 --> 00:29:29,430
those are all options
that people are looking

682
00:29:29,430 --> 00:29:31,290
when they're customizing
something like Llama

683
00:29:31,290 --> 00:29:32,583
for coding tasks.

684
00:29:33,900 --> 00:29:38,340
So I'm gonna transition to not necessarily

685
00:29:38,340 --> 00:29:40,980
a walkthrough or demo, but
a couple of examples here.

686
00:29:40,980 --> 00:29:45,780
So what I wanted to show everybody
were a couple of examples

687
00:29:45,780 --> 00:29:50,700
where we have basically
built coding assistants

688
00:29:50,700 --> 00:29:52,110
and we've published these

689
00:29:52,110 --> 00:29:54,990
on our open source Llama recipes repo

690
00:29:54,990 --> 00:29:57,873
or Llama cookbooks I believe
is what we've named it now.

691
00:29:58,950 --> 00:30:00,720
And the idea behind them is

692
00:30:00,720 --> 00:30:03,900
these are either getting
started type examples,

693
00:30:03,900 --> 00:30:06,710
they're full end to end POCs

694
00:30:06,710 --> 00:30:10,350
or just use case solutions
for anybody to take

695
00:30:10,350 --> 00:30:12,660
and then use as a frame of reference

696
00:30:12,660 --> 00:30:15,360
for building your own
kind of clone of that.

697
00:30:15,360 --> 00:30:19,110
Or we actually have some
end to end applications

698
00:30:19,110 --> 00:30:20,250
and solutions we've built here

699
00:30:20,250 --> 00:30:22,090
which we also publish in open source.

700
00:30:22,090 --> 00:30:24,150
So I wanted to go through in particular

701
00:30:24,150 --> 00:30:26,970
just a quick run of this example we have

702
00:30:26,970 --> 00:30:30,650
where basically we've built
with a coding assistant

703
00:30:30,650 --> 00:30:32,340
and what we've done is we've gone through

704
00:30:32,340 --> 00:30:35,100
a few different tasks
where we wanted to migrate

705
00:30:35,100 --> 00:30:38,530
basically an open AI API to Llama API.

706
00:30:38,530 --> 00:30:41,570
We went through fine tuning Llama models.

707
00:30:41,570 --> 00:30:43,950
We built a rag chat bot with Llama

708
00:30:43,950 --> 00:30:45,150
and then we did some upgrades

709
00:30:45,150 --> 00:30:46,920
'cause for this particular example

710
00:30:46,920 --> 00:30:48,930
we had originally built it with Llama 3

711
00:30:48,930 --> 00:30:51,030
and then we wanted to migrate to Llama 4.

712
00:30:51,030 --> 00:30:53,040
So it's a mixture of steps here

713
00:30:53,040 --> 00:30:56,430
not necessarily solving
an exact coding use case

714
00:30:56,430 --> 00:30:57,810
but in general this is the process

715
00:30:57,810 --> 00:30:59,670
you're gonna look at when you're running

716
00:30:59,670 --> 00:31:02,823
a similar use case like a
coding assistant, right?

717
00:31:03,660 --> 00:31:07,020
So in the first stage here
we go through some examples

718
00:31:07,020 --> 00:31:10,260
basically migrating an
open AI API to Llama.

719
00:31:10,260 --> 00:31:12,120
We've kind of showed you guys
how we thought about this.

720
00:31:12,120 --> 00:31:14,880
This is something which easily
gets kind of transferred

721
00:31:14,880 --> 00:31:18,390
if you go into using prompt
ops as an opportunity here.

722
00:31:18,390 --> 00:31:21,843
You can go and use that
to really streamline this.

723
00:31:22,920 --> 00:31:26,310
Next what we do is we go
into fine tuning Llama models

724
00:31:26,310 --> 00:31:30,660
and we talk here about how
we go through that process.

725
00:31:30,660 --> 00:31:32,550
All the code is available on this as well

726
00:31:32,550 --> 00:31:36,333
if you just go up a level and
that's gonna be an example.

727
00:31:38,550 --> 00:31:41,490
We talk about actually
building that rag chat bot

728
00:31:41,490 --> 00:31:45,480
with the fine-tuned model
and then we talk about

729
00:31:45,480 --> 00:31:48,450
how you think about going
from Llama 3 to Llama 4.

730
00:31:48,450 --> 00:31:52,353
Again, a perfect example
use case of prompt ops.

731
00:31:54,330 --> 00:31:56,740
So another one I wanted
to kind of share here

732
00:31:58,310 --> 00:32:01,830
was generating code-based
docs because it's related to

733
00:32:01,830 --> 00:32:03,540
if we're looking at coding assistants

734
00:32:03,540 --> 00:32:06,190
or we're looking at
customizing models for code

735
00:32:06,190 --> 00:32:08,340
this is a very common use case

736
00:32:08,340 --> 00:32:10,140
where we've built an example here

737
00:32:10,140 --> 00:32:12,930
of being able to use
Llama to go and build out

738
00:32:12,930 --> 00:32:15,090
useful generation of docs.

739
00:32:15,090 --> 00:32:16,990
And this is something you
can go ahead and build out

740
00:32:16,990 --> 00:32:20,670
kind of run on a small server
or run on a small endpoint

741
00:32:20,670 --> 00:32:22,830
and then go ahead and go
through your entire code base

742
00:32:22,830 --> 00:32:24,543
to begin documenting everything.

743
00:32:25,410 --> 00:32:27,390
I won't run through every step of this,

744
00:32:27,390 --> 00:32:30,060
but again, these are just
examples I wanted to share

745
00:32:30,060 --> 00:32:33,420
to show exactly end-to-end
like a lot of the tools

746
00:32:33,420 --> 00:32:35,880
and the capabilities that
we've already gone and built

747
00:32:35,880 --> 00:32:37,800
and we're trying to make accessible.

748
00:32:37,800 --> 00:32:40,690
So one thing I'll do is I'll
transition over to this.

749
00:32:40,690 --> 00:32:43,170
So this is just the GitHub repo.

750
00:32:43,170 --> 00:32:46,530
This is one file, which
is end-to-end use cases

751
00:32:46,530 --> 00:32:48,310
with a number of examples built in

752
00:32:48,310 --> 00:32:52,140
going across examples
requiring fine tuning,

753
00:32:52,140 --> 00:32:55,083
coding, SQL to text, text to SQL,

754
00:32:56,280 --> 00:32:58,890
multimodal type rag use cases,

755
00:32:58,890 --> 00:33:02,220
a whole bunch of variety of example cases

756
00:33:02,220 --> 00:33:05,100
that we developed and we've
put out there as options.

757
00:33:05,100 --> 00:33:06,970
We also have within Llama Cookbooks

758
00:33:06,970 --> 00:33:09,450
a number of the tools that I mentioned.

759
00:33:09,450 --> 00:33:11,710
So I mentioned PropTops.

760
00:33:11,710 --> 00:33:13,440
If you go through Llama Cookbooks,

761
00:33:13,440 --> 00:33:15,810
if you look through
our actual readme here,

762
00:33:15,810 --> 00:33:19,320
a lot of the links to our
tooling and the capabilities

763
00:33:19,320 --> 00:33:21,840
are already built into this.

764
00:33:21,840 --> 00:33:25,050
But then again, this repo has a wealth

765
00:33:25,050 --> 00:33:27,150
of just getting started information.

766
00:33:27,150 --> 00:33:28,770
Now, one thing I should
have linked in here

767
00:33:28,770 --> 00:33:33,770
is AWS also hosts a repo,
which is Llama on AWS.

768
00:33:34,950 --> 00:33:37,140
That one has a bunch
of end-to-end use cases

769
00:33:37,140 --> 00:33:39,690
we've worked with AWS on,

770
00:33:39,690 --> 00:33:42,420
and that also go ahead and
build this type of stuff out.

771
00:33:42,420 --> 00:33:43,620
We actually just worked with them

772
00:33:43,620 --> 00:33:47,280
on a agentic fine-tuning use case

773
00:33:47,280 --> 00:33:50,770
where we just did a roadshow across the US

774
00:33:50,770 --> 00:33:52,140
and we published those.

775
00:33:52,140 --> 00:33:53,220
So if you're looking at building out

776
00:33:53,220 --> 00:33:56,670
coding assistance via agents,
then that's an option as well

777
00:33:56,670 --> 00:33:57,870
that's published on their repo

778
00:33:57,870 --> 00:33:59,883
and was recently published out there.

779
00:34:00,870 --> 00:34:03,573
So I'm gonna go back to our deck here.

780
00:34:06,370 --> 00:34:08,670
And I'm gonna kind of wrap this up here.

781
00:34:08,670 --> 00:34:10,740
So we went over a high level

782
00:34:10,740 --> 00:34:12,870
of considerations you should make.

783
00:34:12,870 --> 00:34:14,730
We didn't go into super deep detail

784
00:34:14,730 --> 00:34:17,250
on actually doing a lot
of that fine-tuning.

785
00:34:17,250 --> 00:34:18,720
One thing we'd love to do is we'd love

786
00:34:18,720 --> 00:34:20,940
to run one post-presentation survey,

787
00:34:20,940 --> 00:34:22,260
like we ran in the beginning.

788
00:34:22,260 --> 00:34:23,670
I'm gonna keep this slide
up here for a minute.

789
00:34:23,670 --> 00:34:25,680
If you can spend a few seconds to do it,

790
00:34:25,680 --> 00:34:26,823
that would be amazing.

791
00:34:28,020 --> 00:34:29,790
All right, and then we have
some next steps for everybody.

792
00:34:29,790 --> 00:34:32,310
So next steps here are,

793
00:34:32,310 --> 00:34:35,340
this presentation was really
covering tools, thoughts,

794
00:34:35,340 --> 00:34:36,690
the way you think about fine-tuning

795
00:34:36,690 --> 00:34:37,990
and setting up for coding.

796
00:34:39,090 --> 00:34:40,860
I also went into exploring a little bit

797
00:34:40,860 --> 00:34:44,190
of the open source
content we have on GitHub,

798
00:34:44,190 --> 00:34:47,790
but that's something I
think is extremely useful

799
00:34:47,790 --> 00:34:48,630
for you to jump into,

800
00:34:48,630 --> 00:34:50,280
especially if you're trying
to solve a problem now

801
00:34:50,280 --> 00:34:53,250
rather than spinning the wheels
and doing it from scratch.

802
00:34:53,250 --> 00:34:55,350
A lot of this has already
been kind of worked on

803
00:34:55,350 --> 00:34:58,593
and pushed into these repos
already for you guys to access.

804
00:34:59,940 --> 00:35:03,570
Obviously, recommendations,
further reading materials,

805
00:35:03,570 --> 00:35:04,830
papers, all of that.

806
00:35:04,830 --> 00:35:07,260
We're always interested in
reading new content as well,

807
00:35:07,260 --> 00:35:10,450
but a lot of that we post
onto the repos as well.

808
00:35:10,450 --> 00:35:12,270
AWS services can help automate

809
00:35:12,270 --> 00:35:14,190
and build out a lot of stuff quickly.

810
00:35:14,190 --> 00:35:18,630
And then again, you can utilize
EKS and all those things

811
00:35:18,630 --> 00:35:20,370
as you look to deploy and scale.

812
00:35:20,370 --> 00:35:22,770
I'm gonna talk about this
slide just really quickly,

813
00:35:22,770 --> 00:35:25,110
'cause I mentioned our tools earlier,

814
00:35:25,110 --> 00:35:27,230
but I didn't get to talk
about them a lot in detail.

815
00:35:27,230 --> 00:35:29,247
I talked about PromptOps.

816
00:35:30,250 --> 00:35:32,450
I wanna talk about DataKit.

817
00:35:32,450 --> 00:35:37,230
DataKit is useful for
your fine-tuning passes

818
00:35:37,230 --> 00:35:40,050
and any of that you wanna do as well.

819
00:35:40,050 --> 00:35:45,050
So basically, it's a tool for
synthetic data generation.

820
00:35:45,540 --> 00:35:48,270
So we built this out
earlier this year as well.

821
00:35:48,270 --> 00:35:49,890
It goes ahead and builds out

822
00:35:49,890 --> 00:35:52,170
and helps you automate that process

823
00:35:52,170 --> 00:35:54,270
when you're looking to develop
a bunch of synthetic data

824
00:35:54,270 --> 00:35:57,030
for, say you're doing eval data sets,

825
00:35:57,030 --> 00:35:58,950
say you're doing training data sets.

826
00:35:58,950 --> 00:36:01,200
The idea is really open.

827
00:36:01,200 --> 00:36:03,030
And there's a lot of examples on our repo

828
00:36:03,030 --> 00:36:06,030
and the Llama on AWS repo,

829
00:36:06,030 --> 00:36:09,540
which have utilized this
to go and train the models.

830
00:36:09,540 --> 00:36:10,440
So I wanted to make sure

831
00:36:10,440 --> 00:36:12,600
I actually talked about that for a second.

832
00:36:12,600 --> 00:36:14,820
So if you haven't seen these tools,

833
00:36:14,820 --> 00:36:18,390
check out the general repos,
go to the Llama webpage.

834
00:36:18,390 --> 00:36:21,300
We have links to everything there as well.

835
00:36:21,300 --> 00:36:22,530
I'll go ahead and end there,

836
00:36:22,530 --> 00:36:25,980
but I'm gonna mention, I'll
hang around for a few minutes.

837
00:36:25,980 --> 00:36:27,720
If you have more specific questions,

838
00:36:27,720 --> 00:36:29,880
I'm happy to chat about them,

839
00:36:29,880 --> 00:36:31,650
but I'll hang out off stage after this.

840
00:36:31,650 --> 00:36:33,300
So thank you guys for attending.

841
00:36:33,300 --> 00:36:34,470
I appreciate it.

842
00:36:34,470 --> 00:36:36,590
(audience applauding)

