{
  "title": "AWS re:Invent 2025 - vLLM on AWS: testing to production and everything in between (OPN414)",
  "title_cn": "AWS re:Invent 2025 - AWS上的vLLM：从测试到生产及其间的一切 (OPN414)",
  "abstract": "This session explores practical architectural patterns for deploying and scaling large language models (LLMs) in production environments. We'll walk through a comprehensive journey from initial testing to production deployment, covering essential steps including model evaluation with vLLM, performance benchmarking, and optimization techniques. Attendees will learn how to implement efficient autoscaling solutions using Ray and vLLM, compare different inference servers like Triton and vLLM, and understand their trade-offs. The session concludes with a deep dive into productionalization using AIBrix, providing attendees with actionable insights for building robust, scalable LLM infrastructures. Suitable for ML engineers and architects working on LLM deployments.",
  "abstract_cn": "本次会议探讨在生产环境中部署和扩展大语言模型（LLM）的实用架构模式。我们将全面介绍从初始测试到生产部署的完整流程，涵盖使用 vLLM 进行模型评估、性能基准测试和优化技术等关键步骤。参会者将学习如何使用 Ray 和 vLLM 实现高效的自动扩展解决方案，比较 Triton 和 vLLM 等不同推理服务器及其权衡取舍。会议最后将深入探讨使用 AIBrix 进行生产化部署，为参会者提供构建稳健、可扩展 LLM 基础设施的可行性见解。适合从事 LLM 部署工作的机器学习工程师和架构师参加。",
  "presenter": [
    {
      "name": "Phi Nguyen",
      "title": "Sr Generative AI Specialist",
      "company": "AWS"
    },
    {
      "name": "Omri Shiv",
      "title": "Senior Open Source ML Engineer",
      "company": "AWS"
    }
  ],
  "attributes": {
    "topic": "Open Source",
    "area_of_interest": [
      "Generative AI",
      "Machine Learning"
    ],
    "services": [],
    "type": "Breakout session"
  },
  "video_url": "https://www.youtube.com/watch?v=sx-DLGXv49g",
  "session_code": "OPN414"
}