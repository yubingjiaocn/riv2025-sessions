1
00:00:05,079 --> 00:00:08,399
Please welcome to the stage vice president of Technology,

2
00:00:08,439 --> 00:00:10,779
data and analytics at AWS

3
00:00:11,039 --> 00:00:12,760
Mai-Lan Tomsen Bukovec.

4
00:00:18,329 --> 00:00:19,190
Welcome.

5
00:00:21,629 --> 00:00:24,909
Welcome to the analytics leadership talk.

6
00:00:25,030 --> 00:00:26,629
My name is Mylan Thompson Bukovvic,

7
00:00:26,989 --> 00:00:30,049
and I run data and analytics services for AWS.

8
00:00:31,590 --> 00:00:32,090
Today,

9
00:00:32,150 --> 00:00:35,990
we're gonna walk through 3 emerging trends in the world of analytics

10
00:00:36,189 --> 00:00:40,369
and how our customers are steering into them with their data strategies

11
00:00:40,590 --> 00:00:42,409
and their AWS services.

12
00:00:44,169 --> 00:00:47,939
Let's start first with how Agentic AI is delivering

13
00:00:47,939 --> 00:00:50,450
assistance in every step of your data journey,

14
00:00:50,700 --> 00:00:52,180
whether it is writing code,

15
00:00:52,500 --> 00:00:53,900
processing data in pipelines,

16
00:00:53,939 --> 00:00:55,400
or using data products,

17
00:00:55,619 --> 00:00:56,939
just like a human would,

18
00:00:57,139 --> 00:00:58,099
except much,

19
00:00:58,419 --> 00:00:59,380
much faster.

20
00:01:01,799 --> 00:01:05,300
Now,
it's incredible how far agents have come in the last year.

21
00:01:05,760 --> 00:01:08,500
So many of our customers have put AI agents

22
00:01:08,760 --> 00:01:09,260
to work

23
00:01:09,430 --> 00:01:11,160
on real production workloads,

24
00:01:11,199 --> 00:01:12,589
and we have been doing the same.

25
00:01:12,879 --> 00:01:17,339
We have been working hard to integrate AI into our analytic services

26
00:01:17,599 --> 00:01:18,339
for you

27
00:01:18,480 --> 00:01:19,139
to use

28
00:01:19,319 --> 00:01:20,260
for the jobs

29
00:01:20,389 --> 00:01:21,000
to be done.

30
00:01:23,010 --> 00:01:24,589
As data lakes have grown,

31
00:01:24,690 --> 00:01:26,769
if you saw in the keynote this morning,

32
00:01:26,849 --> 00:01:30,269
you saw that data lakes were represented as a data ocean

33
00:01:30,489 --> 00:01:32,050
in the Sony presentation.

34
00:01:32,379 --> 00:01:34,690
Analytics have also been integrated

35
00:01:34,830 --> 00:01:36,639
in every aspect of business,

36
00:01:36,709 --> 00:01:37,879
and agentic AI,

37
00:01:38,150 --> 00:01:42,959
it's starting to become a natural part of how humans and applications work.

38
00:01:43,339 --> 00:01:47,669
Now we know that data and AI are increasingly intertwined,

39
00:01:47,989 --> 00:01:50,080
so we're building AI smarts

40
00:01:50,269 --> 00:01:51,449
into how you

41
00:01:51,589 --> 00:01:52,690
work with data

42
00:01:53,150 --> 00:01:54,209
on AWS.

43
00:01:56,660 --> 00:01:58,400
Let me show you what I mean.

44
00:01:58,779 --> 00:02:00,580
Now,
we just launched this week,

45
00:02:00,589 --> 00:02:02,120
a highly optimized

46
00:02:02,339 --> 00:02:04,739
Spark 3.5.6 engine

47
00:02:04,940 --> 00:02:07,739
as part of the latest versions of EMR,

48
00:02:07,940 --> 00:02:08,440
Glue,

49
00:02:08,500 --> 00:02:09,490
Athena Spark,

50
00:02:09,779 --> 00:02:10,500
and in our,

51
00:02:10,979 --> 00:02:12,729
our new notebook for Sagemaker.

52
00:02:13,020 --> 00:02:18,080
So now you have one AWS optimized Spark engine that's powering

53
00:02:18,220 --> 00:02:21,520
these different AWS services and experiences.

54
00:02:24,789 --> 00:02:26,369
It's a super exciting launch,

55
00:02:26,750 --> 00:02:30,369
especially for the many customers out there using Iceberg as their open

56
00:02:30,589 --> 00:02:31,589
data format.

57
00:02:31,910 --> 00:02:32,949
So as you can see,

58
00:02:33,059 --> 00:02:34,389
our optimized Spark engine,

59
00:02:34,460 --> 00:02:38,070
it speeds up both read and write performance on Iceberg data as

60
00:02:38,070 --> 00:02:41,320
well as any of the interactive queries that you're doing in Athena.

61
00:02:41,940 --> 00:02:42,490
But

62
00:02:43,029 --> 00:02:48,110
Spark major version upgrades often require at least some refactoring

63
00:02:48,380 --> 00:02:50,809
of the applications that are sitting on top.

64
00:02:53,100 --> 00:02:55,770
So we've incorporated almost a decade

65
00:02:55,979 --> 00:03:00,880
of helping customers migrate to new versions of Spark into AI assistance,

66
00:03:01,139 --> 00:03:03,820
so you can now use AWS AI

67
00:03:04,020 --> 00:03:06,479
to more easily migrate your applications

68
00:03:06,740 --> 00:03:09,580
to our new optimized Spark engine and EMR

69
00:03:09,979 --> 00:03:10,660
and Glue.

70
00:03:10,979 --> 00:03:11,449
So

71
00:03:11,449 --> 00:03:13,619
in order to take advantage of this AI assistance,

72
00:03:13,660 --> 00:03:14,919
you have different ways of doing it.

73
00:03:15,059 --> 00:03:19,104
You can do it in local VS code or through Tools and SageMaker Unified Studio,

74
00:03:19,485 --> 00:03:20,975
or you can use our remote

75
00:03:21,125 --> 00:03:26,205
MCP servers server from Kiro or any of your favorite coding assistance

76
00:03:26,604 --> 00:03:27,324
environments.

77
00:03:27,585 --> 00:03:29,764
The Spark upgrade agent is going to go all

78
00:03:29,764 --> 00:03:32,544
the way back to helping you with upgrades from versions

79
00:03:32,764 --> 00:03:34,884
as early as Glue 2.0,

80
00:03:35,205 --> 00:03:36,654
EMR and EC2

81
00:03:36,845 --> 00:03:37,675
5.2,

82
00:03:37,684 --> 00:03:41,365
and any of your previous EMR serverless versions.

83
00:03:44,009 --> 00:03:45,360
We're super excited about this.

84
00:03:45,500 --> 00:03:47,699
Now,
in AWS we have a saying,

85
00:03:47,860 --> 00:03:51,550
we say that there's no compression algorithm for experience,

86
00:03:51,860 --> 00:03:53,750
and our new spark upgrade

87
00:03:53,940 --> 00:03:55,639
runs off of a knowledge base

88
00:03:55,839 --> 00:03:58,259
that's based off thousands

89
00:03:58,419 --> 00:03:59,820
of spark upgrades,

90
00:04:00,139 --> 00:04:01,589
some that have gone well,

91
00:04:01,740 --> 00:04:03,080
and some that have not.

92
00:04:03,449 --> 00:04:04,740
It plans the upgrade.

93
00:04:04,809 --> 00:04:06,699
It works through the error messages.

94
00:04:06,770 --> 00:04:08,800
It remediates failures

95
00:04:09,179 --> 00:04:09,679
automatically.

96
00:04:10,020 --> 00:04:10,750
And more.

97
00:04:11,190 --> 00:04:14,190
So now you can finish the work of upgrading your

98
00:04:14,190 --> 00:04:17,019
app to the latest Spark in EMR and Glue,

99
00:04:17,070 --> 00:04:18,809
and you can do it in weeks

100
00:04:19,269 --> 00:04:20,250
instead of months.

101
00:04:20,750 --> 00:04:26,390
This combination of a highly optimized Spark 3.5.6 engine

102
00:04:26,589 --> 00:04:29,059
and AI assistance for upgrades,

103
00:04:29,369 --> 00:04:32,890
it's a real game changer for our Spark Spark customers.

104
00:04:33,149 --> 00:04:35,709
It lets you modernize quickly

105
00:04:36,029 --> 00:04:38,350
with the help of AI from AWS.

106
00:04:41,260 --> 00:04:41,730
FINRA,

107
00:04:41,730 --> 00:04:44,640
which safeguards the integrity of the US capital markets,

108
00:04:45,070 --> 00:04:48,350
tried using our upgrade AI agent earlier this year,

109
00:04:48,649 --> 00:04:53,239
and now they plan to anchor their Spark version management on it entirely

110
00:04:53,500 --> 00:04:54,899
in 2026.

111
00:04:56,929 --> 00:04:58,549
Now,
agents are a great fit

112
00:04:58,690 --> 00:04:59,880
for spark upgrades

113
00:05:00,250 --> 00:05:00,750
because it,

114
00:05:00,769 --> 00:05:01,239
it

115
00:05:01,239 --> 00:05:03,850
automates work repetitive workflows,

116
00:05:03,970 --> 00:05:07,230
but AI is also really good at helping you write and fix code.

117
00:05:07,690 --> 00:05:10,309
So we've built AI into our new notebook

118
00:05:10,489 --> 00:05:11,329
in SageMaker,

119
00:05:11,369 --> 00:05:12,959
and we're also using AI

120
00:05:13,279 --> 00:05:15,730
under the hood inside our AWS services,

121
00:05:15,809 --> 00:05:18,899
like how Redshift query Engine uses MLAI

122
00:05:19,130 --> 00:05:20,429
to optimize queries

123
00:05:20,609 --> 00:05:22,390
and manage serverless clusters.

124
00:05:24,959 --> 00:05:27,660
Now,
we launched the new Sagemaker notebook

125
00:05:27,799 --> 00:05:28,779
a few weeks ago.

126
00:05:29,119 --> 00:05:30,989
And when we launched this capability,

127
00:05:31,049 --> 00:05:35,940
we also integrated AI capabilities directly into the new notebook experience.

128
00:05:36,200 --> 00:05:37,940
So you have one place

129
00:05:38,200 --> 00:05:40,119
to process and analyze

130
00:05:40,359 --> 00:05:41,230
using SQL,

131
00:05:41,350 --> 00:05:41,850
Python,

132
00:05:41,859 --> 00:05:43,700
or natural language

133
00:05:43,920 --> 00:05:45,640
with AI assistance built in.

134
00:05:48,489 --> 00:05:51,589
We built this new notebook to make working with data easier,

135
00:05:51,809 --> 00:05:55,369
and the goal here is to take away the operational burden of

136
00:05:55,369 --> 00:05:59,510
managing the analytic services that the notebook uses behind the scenes.

137
00:05:59,929 --> 00:06:03,549
And so if you're explorer exploring data analysis,

138
00:06:03,649 --> 00:06:05,570
or you're building an ETL pipeline,

139
00:06:05,850 --> 00:06:07,929
or you're training a machine learning note,

140
00:06:07,980 --> 00:06:08,290
uh,

141
00:06:08,290 --> 00:06:08,790
model,

142
00:06:09,105 --> 00:06:12,026
The notebook provides you the streamlined programming

143
00:06:12,026 --> 00:06:14,205
environment for your end to end workflows,

144
00:06:14,545 --> 00:06:17,006
and it doesn't require you to manage

145
00:06:17,216 --> 00:06:18,446
any infrastructure.

146
00:06:18,746 --> 00:06:21,385
It connects with your data wherever it resides,

147
00:06:21,466 --> 00:06:23,545
such as iceberg tables on a 3

148
00:06:23,705 --> 00:06:25,925
or your Amazon Redshift data warehouse.

149
00:06:26,305 --> 00:06:28,346
Now,
as part of this notebook experience,

150
00:06:28,425 --> 00:06:29,156
we've built in

151
00:06:29,361 --> 00:06:30,622
AI assistance.

152
00:06:30,921 --> 00:06:32,502
And it's there to support you

153
00:06:32,661 --> 00:06:34,652
every step of the way.

154
00:06:34,881 --> 00:06:36,242
It can answer your questions,

155
00:06:36,312 --> 00:06:38,242
it can write scripts or SQL queries.

156
00:06:38,282 --> 00:06:39,772
It can generate pipelines

157
00:06:40,082 --> 00:06:41,601
and create visualizations.

158
00:06:41,842 --> 00:06:44,962
And so the experience of working with data with a notebook

159
00:06:45,122 --> 00:06:46,152
is more intuitive.

160
00:06:46,201 --> 00:06:47,351
It's more efficient,

161
00:06:47,601 --> 00:06:48,462
and frankly,

162
00:06:48,682 --> 00:06:49,661
it's more fun

163
00:06:50,041 --> 00:06:51,802
to do data exploration

164
00:06:51,962 --> 00:06:52,992
and insight generation.

165
00:06:54,899 --> 00:06:58,019
Now,
unlike traditional notebooks like Jupiter Lab IDE,

166
00:06:58,059 --> 00:07:00,170
which Sagemaker also supports,

167
00:07:00,500 --> 00:07:02,980
this new notebook is entirely serverless,

168
00:07:03,260 --> 00:07:06,579
so you don't need to tune or manage or pre-provision

169
00:07:06,809 --> 00:07:08,700
query processing infrastructure.

170
00:07:10,109 --> 00:07:10,609
Now,

171
00:07:10,690 --> 00:07:14,109
what's really interesting about this Sagemaker notebook

172
00:07:14,429 --> 00:07:16,220
is that it's polyglot,

173
00:07:16,470 --> 00:07:20,190
and that means you can express your code in Python

174
00:07:20,459 --> 00:07:24,250
or SQL cells that then interoperate with each other

175
00:07:24,470 --> 00:07:25,529
in the same

176
00:07:25,829 --> 00:07:26,450
notebook.

177
00:07:28,170 --> 00:07:29,390
So that means you can write

178
00:07:29,929 --> 00:07:31,959
PySpark using Spark Connect

179
00:07:32,130 --> 00:07:33,500
in Python cells,

180
00:07:33,850 --> 00:07:34,799
and in Sequel cells,

181
00:07:34,850 --> 00:07:39,149
you can write SQL queries against Redshift or Atheneutrino or a third-party

182
00:07:39,529 --> 00:07:41,390
analytics engine like Snowflake,

183
00:07:41,649 --> 00:07:44,070
and you can reuse your results

184
00:07:44,290 --> 00:07:45,209
in Python.

185
00:07:45,929 --> 00:07:49,369
This notebook is an all in one data notebook

186
00:07:49,369 --> 00:07:53,029
that you can use with any open analytics architecture

187
00:07:53,209 --> 00:07:54,570
now and in the future.

188
00:07:57,940 --> 00:08:00,579
The AI assistance that we've built into the data notebook,

189
00:08:00,660 --> 00:08:02,339
it helps you with the data querying,

190
00:08:02,380 --> 00:08:04,239
the exploratory data analysis,

191
00:08:04,690 --> 00:08:06,220
and the model development.

192
00:08:06,230 --> 00:08:08,500
And the most interesting thing I think,

193
00:08:08,750 --> 00:08:12,480
is you can describe your objectives in natural language

194
00:08:12,859 --> 00:08:14,679
right in notebooks cells.

195
00:08:16,739 --> 00:08:19,660
So the agent that's under the hood for our AI assistance,

196
00:08:19,700 --> 00:08:21,049
it's using context,

197
00:08:21,339 --> 00:08:24,570
and it's using context from the data catalogs and metadata,

198
00:08:24,820 --> 00:08:29,480
and the rest of the work that you do in your notebook to suggest these execution plans

199
00:08:29,890 --> 00:08:32,280
to generate code and just to help you build.

200
00:08:32,700 --> 00:08:34,479
So you can use the notebook

201
00:08:34,780 --> 00:08:35,700
in SageMaker,

202
00:08:35,780 --> 00:08:38,640
you can click into it from the AWS management console,

203
00:08:38,919 --> 00:08:41,058
or if you want to use the notebook capabilities,

204
00:08:41,140 --> 00:08:43,239
you can use a remote MCP server

205
00:08:43,460 --> 00:08:45,080
from your favorite coding environment.

206
00:08:47,919 --> 00:08:48,419
Now,

207
00:08:48,559 --> 00:08:51,950
here to show you how we've integrated our new SageMaker

208
00:08:51,950 --> 00:08:55,929
notebooks and our AI agent together in an integrated experience

209
00:08:56,190 --> 00:08:56,840
is Diane,

210
00:08:57,000 --> 00:09:00,080
the principal engineer from SageMaker Unified Studio.

211
00:09:00,280 --> 00:09:00,780
Diane?

212
00:09:02,919 --> 00:09:03,619
Hi,
Dad.

213
00:09:04,659 --> 00:09:05,159
I got.

214
00:09:10,210 --> 00:09:10,710
Thank you Milan.

215
00:09:11,539 --> 00:09:12,450
Good afternoon everyone.

216
00:09:12,619 --> 00:09:13,659
My name is Diane Alener.

217
00:09:13,729 --> 00:09:16,280
I'm a principal engineer on Sagemaker Unified Studio,

218
00:09:16,820 --> 00:09:19,890
and I'm really excited to be here because I wanna show you guys our new notebooks,

219
00:09:19,940 --> 00:09:20,700
uh,
in action.

220
00:09:22,760 --> 00:09:23,739
So let's take a look.

221
00:09:24,280 --> 00:09:25,739
So this is our notebook interface.

222
00:09:26,450 --> 00:09:27,559
It's a familiar interface,

223
00:09:27,619 --> 00:09:28,010
but it's,

224
00:09:28,010 --> 00:09:28,849
it's more modernized.

225
00:09:28,890 --> 00:09:31,429
It's got a collection of cells where you can express your code

226
00:09:31,809 --> 00:09:32,409
in Python,

227
00:09:32,570 --> 00:09:33,070
SQL,

228
00:09:33,090 --> 00:09:33,570
Markdown,

229
00:09:33,570 --> 00:09:34,270
or natural language.

230
00:09:35,270 --> 00:09:36,479
You see the data explorer here,

231
00:09:36,549 --> 00:09:39,619
I've got the New York City taxi data and an iceberg table on S3,

232
00:09:39,950 --> 00:09:42,200
and I have my chat agent uh open on the right.

233
00:09:43,330 --> 00:09:44,989
So let's get into some of the basics.

234
00:09:45,289 --> 00:09:46,609
I can create a Python cell,

235
00:09:46,849 --> 00:09:48,929
read some sample data into a panda's data frame,

236
00:09:49,250 --> 00:09:51,669
and see the output of the data in rich data tables.

237
00:09:52,700 --> 00:09:53,890
As I explore my data,

238
00:09:54,030 --> 00:09:56,210
I might notice a unit price column here,

239
00:09:56,469 --> 00:09:58,270
uh,
and there might be another column of interest,

240
00:09:58,390 --> 00:09:59,809
well,
say the category column,

241
00:10:00,109 --> 00:10:03,419
and then I can use these pieces of information to write a SQL query

242
00:10:03,590 --> 00:10:05,349
on my Python data frame.

243
00:10:06,000 --> 00:10:08,520
I see the output in the same rich data table,

244
00:10:08,719 --> 00:10:11,159
and if I really wanted to step up my data exploration,

245
00:10:11,320 --> 00:10:12,479
I could use the notebooks,

246
00:10:12,559 --> 00:10:14,539
the native visualization capabilities,

247
00:10:14,780 --> 00:10:16,760
and render the data in interactive charts.

248
00:10:18,070 --> 00:10:18,570
Great.

249
00:10:18,789 --> 00:10:20,049
So the basics out of the way,

250
00:10:20,140 --> 00:10:23,809
let's use that New York City taxi data set to get some real-world jobs to be done.

251
00:10:25,549 --> 00:10:27,809
So what I can do is I can ask the agent

252
00:10:28,570 --> 00:10:31,289
to read my New York City taxi data set into a Spark data frame

253
00:10:31,669 --> 00:10:33,030
and give me the shape of the data.

254
00:10:33,849 --> 00:10:34,599
So immediately,

255
00:10:34,609 --> 00:10:38,489
the agent generates a plan for how it's gonna find and read the data into Spark,

256
00:10:39,010 --> 00:10:41,090
and what kind of exploratory analysis it will do.

257
00:10:42,450 --> 00:10:42,950
Um,

258
00:10:43,090 --> 00:10:46,330
what we can see is that Spark is just available in the notebook.

259
00:10:46,570 --> 00:10:48,039
There's no infrastructure set up,

260
00:10:48,349 --> 00:10:49,010
no clusters,

261
00:10:49,130 --> 00:10:50,609
no complicated configuration.

262
00:10:50,969 --> 00:10:51,729
It's just there,

263
00:10:51,890 --> 00:10:53,289
ready to go to work on your data.

264
00:10:54,330 --> 00:10:57,599
Now what's powerful here is that the system understands where the data lives,

265
00:10:57,969 --> 00:11:00,409
automatically chooses the best way to query the data,

266
00:11:00,849 --> 00:11:02,599
and it shows me the exploration steps,

267
00:11:02,809 --> 00:11:03,830
and upon completion,

268
00:11:03,929 --> 00:11:06,510
it summarizes what the steps have been able to accomplish.

269
00:11:07,760 --> 00:11:10,080
So this is giving me a sense of the scheme of the data set.

270
00:11:10,119 --> 00:11:10,919
It produces

271
00:11:11,140 --> 00:11:12,609
some statistical summaries

272
00:11:12,799 --> 00:11:13,159
on,

273
00:11:13,159 --> 00:11:14,960
on factors such as trip distance,

274
00:11:15,200 --> 00:11:15,559
uh,

275
00:11:15,559 --> 00:11:15,880
fare,

276
00:11:15,880 --> 00:11:17,059
and tip and things like that.

277
00:11:17,479 --> 00:11:20,289
So every analysis comes with generated visualizations,

278
00:11:20,520 --> 00:11:23,380
and it really takes exploratory data analysis to the next level.

279
00:11:23,679 --> 00:11:24,359
I,
as a user,

280
00:11:24,359 --> 00:11:25,559
I can just sit back and watch.

281
00:11:25,640 --> 00:11:26,979
The notebook is doing all the work.

282
00:11:28,849 --> 00:11:29,349
So

283
00:11:29,729 --> 00:11:32,580
as we kind of go through this uh data exploration journey,

284
00:11:32,929 --> 00:11:33,289
um,

285
00:11:33,289 --> 00:11:33,789
there's a,

286
00:11:33,969 --> 00:11:34,450
the,
uh,

287
00:11:34,450 --> 00:11:36,190
the data set might be large or small,

288
00:11:36,729 --> 00:11:39,229
but sometimes what happens is we might run into some issues.

289
00:11:39,989 --> 00:11:41,690
Because not everything goes according to plan.

290
00:11:42,510 --> 00:11:42,960
Um,

291
00:11:42,960 --> 00:11:43,880
when that happens,

292
00:11:44,119 --> 00:11:44,979
it's very difficult

293
00:11:45,320 --> 00:11:45,900
to kind of,

294
00:11:45,960 --> 00:11:46,280
uh,

295
00:11:46,280 --> 00:11:49,419
sift through all the large exceptions or try to stack traces,

296
00:11:49,669 --> 00:11:52,299
and for developers such as myself and many of you out here

297
00:11:52,440 --> 00:11:54,919
it's very difficult to figure out what the actual problem is

298
00:11:55,200 --> 00:11:56,849
when visualizations don't render

299
00:11:57,080 --> 00:11:58,099
it's really not nice.

300
00:11:59,010 --> 00:11:59,510
Um,

301
00:11:59,570 --> 00:12:01,419
so what I can do on the notebook is I can,

302
00:12:01,530 --> 00:12:03,890
I have fixed with AI as a capability,

303
00:12:03,919 --> 00:12:05,390
and all I need to do is click the button.

304
00:12:06,559 --> 00:12:07,909
When I click fix with AI,

305
00:12:08,450 --> 00:12:10,450
the agent will now analyze the exception,

306
00:12:11,090 --> 00:12:12,719
recommend the code that will fix the issue,

307
00:12:13,010 --> 00:12:15,909
and all I have to do is take a look at the code that are generated

308
00:12:16,010 --> 00:12:17,049
and hit accept and run.

309
00:12:18,280 --> 00:12:18,799
Just like that,

310
00:12:18,840 --> 00:12:20,020
my problem is solved.

311
00:12:20,479 --> 00:12:21,820
My visualizations are rendered.

312
00:12:23,260 --> 00:12:23,760
So

313
00:12:23,979 --> 00:12:27,440
what this kind of gives me a sense of is that my data might not be of the best quality.

314
00:12:28,179 --> 00:12:28,659
So,

315
00:12:28,659 --> 00:12:29,409
so given that,

316
00:12:29,419 --> 00:12:30,719
I can go ahead and ask the agent

317
00:12:31,020 --> 00:12:32,239
to clean my data set,

318
00:12:32,580 --> 00:12:33,979
and I could give it some factors it

319
00:12:33,979 --> 00:12:36,359
should consider as it performs the cleaning steps.

320
00:12:37,330 --> 00:12:39,650
So what's happening here is that the agent understands

321
00:12:39,650 --> 00:12:42,070
both the intent and the context of the data.

322
00:12:42,609 --> 00:12:45,530
It's doing in seconds what would usually take multiple scripts,

323
00:12:45,609 --> 00:12:46,039
queries,

324
00:12:46,039 --> 00:12:47,150
and validation steps.

325
00:12:48,059 --> 00:12:50,340
The notebook automatically generates spark code,

326
00:12:50,539 --> 00:12:52,219
and every step of the way it shows

327
00:12:52,219 --> 00:12:54,789
the transformations that it applies to the cleaning process

328
00:12:54,979 --> 00:12:56,219
and a summary of the outcome.

329
00:12:57,109 --> 00:12:59,549
It's really it's really powerful to also see

330
00:12:59,549 --> 00:13:01,190
the outcome of the data and the preparation

331
00:13:01,190 --> 00:13:04,130
steps because I can really get a sense of how the quality of the data is improving

332
00:13:04,510 --> 00:13:06,510
based on the visualizations that I see on screen.

333
00:13:07,619 --> 00:13:08,119
Great.

334
00:13:08,580 --> 00:13:10,349
So now that I have some high quality data,

335
00:13:10,500 --> 00:13:11,770
I can ask the agent to,

336
00:13:11,809 --> 00:13:14,080
to train a model that predicts trip prices.

337
00:13:15,099 --> 00:13:18,690
So the agent does here is that it recommends a regression approach.

338
00:13:19,229 --> 00:13:19,549
Um,

339
00:13:19,549 --> 00:13:20,750
it writes the code to,

340
00:13:20,859 --> 00:13:23,359
to train the model and it returns a summary of the results

341
00:13:23,669 --> 00:13:24,190
and include,

342
00:13:24,469 --> 00:13:25,179
includes things like,

343
00:13:25,190 --> 00:13:26,190
uh,
feature importance,

344
00:13:26,309 --> 00:13:27,890
uh,
accuracy metrics and things like that.

345
00:13:28,070 --> 00:13:29,789
It even generated code so that I can

346
00:13:29,789 --> 00:13:31,890
visualize the prediction that the model would make.

347
00:13:32,909 --> 00:13:33,390
Um,

348
00:13:33,390 --> 00:13:34,020
so once again,

349
00:13:34,190 --> 00:13:37,469
like I can just sit back and relax and watch the notebook as it trains the model,

350
00:13:37,830 --> 00:13:39,010
shows the model performance,

351
00:13:39,390 --> 00:13:41,270
uh,
gives me a sense of the future importance,

352
00:13:41,429 --> 00:13:43,830
and shows the visualizations of the predictions and,

353
00:13:44,130 --> 00:13:44,549
um,

354
00:13:44,549 --> 00:13:45,429
for my success and,

355
00:13:45,510 --> 00:13:46,489
and,
and errors.

356
00:13:46,989 --> 00:13:49,590
Now what's really powerful here is we're not switching tools.

357
00:13:49,789 --> 00:13:52,640
We've gone from raw data to model insights all in

358
00:13:52,640 --> 00:13:54,830
one notebook and AI is doing all the heavy lifting.

359
00:13:56,359 --> 00:13:57,960
So I've done a lot of work and there's a

360
00:13:57,960 --> 00:14:00,070
lot of rich context that's built into the notebook.

361
00:14:00,280 --> 00:14:02,130
So finally I can ask the agent,

362
00:14:02,359 --> 00:14:03,940
uh,
some general questions about the data.

363
00:14:04,010 --> 00:14:04,510
For example,

364
00:14:04,559 --> 00:14:05,429
here I ask it,

365
00:14:05,679 --> 00:14:06,080
you know,

366
00:14:06,080 --> 00:14:07,190
explain the key drivers of,

367
00:14:07,200 --> 00:14:07,880
uh of price,

368
00:14:07,960 --> 00:14:09,400
which is sort of like a general question.

369
00:14:10,710 --> 00:14:11,330
So here

370
00:14:11,469 --> 00:14:14,150
now the agent has all the access to all the rich context.

371
00:14:14,469 --> 00:14:17,859
So what it gives me is some key findings not only in a text summary,

372
00:14:18,150 --> 00:14:19,210
but it's also given me,

373
00:14:19,390 --> 00:14:20,789
uh,
a little bit of code.

374
00:14:21,229 --> 00:14:21,590
Um,

375
00:14:21,590 --> 00:14:23,969
what you'll see here is when I accept and run the code,

376
00:14:24,510 --> 00:14:26,549
uh,
what it will what the notebook will do

377
00:14:26,549 --> 00:14:29,390
is translate those insights into meaningful visualizations,

378
00:14:29,549 --> 00:14:31,650
and I can use this on a dashboard or in a report.

379
00:14:33,520 --> 00:14:34,000
Great,

380
00:14:34,000 --> 00:14:36,669
so now I've cleaned data from which I can generate insights.

381
00:14:36,799 --> 00:14:40,119
It's often useful to write this data back into my data lake so that I can,

382
00:14:40,200 --> 00:14:40,719
I can do,

383
00:14:40,739 --> 00:14:41,239
uh,

384
00:14:41,289 --> 00:14:43,059
business intelligence or things like that.

385
00:14:43,650 --> 00:14:46,340
So I asked the agent to write the data back into my,

386
00:14:46,400 --> 00:14:47,340
uh,
data lake.

387
00:14:47,679 --> 00:14:48,940
It gives me spark code,

388
00:14:49,090 --> 00:14:49,590
um,

389
00:14:49,760 --> 00:14:50,400
which,
uh,

390
00:14:50,400 --> 00:14:52,059
which will write the clean data set

391
00:14:52,320 --> 00:14:54,979
as an iceberg table back into my data lake.

392
00:14:55,440 --> 00:14:55,940
Um,

393
00:14:55,960 --> 00:14:56,760
it runs the code.

394
00:14:56,840 --> 00:14:58,059
I can see the schema

395
00:14:58,320 --> 00:15:01,099
and gives me a little bit of uh statistics here as it does the right.

396
00:15:01,580 --> 00:15:03,919
And now if I dive back into my data explorer

397
00:15:04,119 --> 00:15:05,559
and expand my default database,

398
00:15:05,780 --> 00:15:07,039
I see the New York City

399
00:15:07,140 --> 00:15:07,700
taxi,

400
00:15:07,739 --> 00:15:08,179
uh,

401
00:15:08,179 --> 00:15:08,500
uh,

402
00:15:08,500 --> 00:15:09,359
database here

403
00:15:09,780 --> 00:15:10,880
cleaned and ready to go,

404
00:15:11,099 --> 00:15:11,419
uh,

405
00:15:11,419 --> 00:15:13,280
for further analytics.

406
00:15:15,239 --> 00:15:15,799
So

407
00:15:16,239 --> 00:15:17,659
what we've just seen here though

408
00:15:17,760 --> 00:15:18,880
in the last few minutes

409
00:15:19,190 --> 00:15:19,979
like we've taken

410
00:15:20,200 --> 00:15:22,140
uh some data that was on Iceberg

411
00:15:22,640 --> 00:15:24,840
um on S3 we've cleaned it

412
00:15:24,960 --> 00:15:26,020
we've transformed it

413
00:15:26,280 --> 00:15:27,539
we trained the predictive model

414
00:15:27,840 --> 00:15:29,900
and we produced business ready insights

415
00:15:30,159 --> 00:15:31,219
all within a single

416
00:15:31,719 --> 00:15:33,580
serverless agentic notebooks.

417
00:15:34,280 --> 00:15:37,280
Now this is really what building data products with AI really looks like.

418
00:15:38,090 --> 00:15:41,650
It's less about managing infrastructure and more about accelerating the

419
00:15:41,650 --> 00:15:44,549
accelerating the path from raw data to intelligent outcomes.

420
00:15:45,570 --> 00:15:46,859
Your data is the product,

421
00:15:47,059 --> 00:15:48,119
your notebook is the vehicle,

422
00:15:48,539 --> 00:15:49,599
and AI is the driver.

423
00:15:50,570 --> 00:15:53,440
I'm really excited to see what you all built with the new notebooks.

424
00:15:53,690 --> 00:15:54,609
Thank you for having me.

425
00:15:58,489 --> 00:15:59,250
It's awesome,

426
00:15:59,369 --> 00:15:59,770
Diane.

427
00:15:59,770 --> 00:16:00,479
Thank you.

428
00:16:00,880 --> 00:16:01,450
Diane.

429
00:16:07,030 --> 00:16:07,469
Thanks,

430
00:16:07,469 --> 00:16:08,030
Diane.

431
00:16:08,700 --> 00:16:09,270
Great demo.

432
00:16:09,429 --> 00:16:10,570
It's a lot of fun to use.

433
00:16:11,070 --> 00:16:11,830
You can use it now.

434
00:16:11,909 --> 00:16:13,250
You can get started with this

435
00:16:13,750 --> 00:16:15,669
right after this presentation and,

436
00:16:15,729 --> 00:16:16,150
um,

437
00:16:16,150 --> 00:16:16,650
and try,

438
00:16:16,789 --> 00:16:17,590
try it yourself.

439
00:16:18,150 --> 00:16:19,070
As Diane shared,

440
00:16:19,109 --> 00:16:21,070
when we built SageMaker Unified Studio,

441
00:16:21,109 --> 00:16:22,890
our goal here was to make it easy

442
00:16:23,349 --> 00:16:24,440
to work with your data,

443
00:16:24,789 --> 00:16:26,469
but we also have MCP servers.

444
00:16:26,549 --> 00:16:29,809
We have MCP servers that are both local and remote.

445
00:16:30,250 --> 00:16:32,549
So you can use any of these new capabilities for

446
00:16:32,549 --> 00:16:36,270
whatever your favorite AI or traditional coding tool is.

447
00:16:37,630 --> 00:16:39,669
No matter what your point of access,

448
00:16:39,750 --> 00:16:42,609
our whole goal is to build AI into analytics,

449
00:16:42,679 --> 00:16:44,809
so it becomes just a natural way

450
00:16:45,190 --> 00:16:46,289
for how you work.

451
00:16:47,179 --> 00:16:47,679
Now,

452
00:16:47,739 --> 00:16:48,280
the second

453
00:16:48,539 --> 00:16:51,179
emerging trend in analytics that I want to talk about

454
00:16:51,580 --> 00:16:52,880
is open architectures.

455
00:16:53,059 --> 00:16:54,940
In today's modern analytics world,

456
00:16:55,140 --> 00:16:55,739
customers,

457
00:16:56,010 --> 00:16:58,859
they know that not one size fits all

458
00:16:59,059 --> 00:17:00,409
in an analytic solution.

459
00:17:00,619 --> 00:17:03,239
You can't just jam every capability

460
00:17:03,580 --> 00:17:05,040
behind a single API.

461
00:17:05,380 --> 00:17:08,060
You want the flexibility to pick a solution

462
00:17:08,260 --> 00:17:09,500
that meets a specific

463
00:17:09,618 --> 00:17:10,118
Need.

464
00:17:10,270 --> 00:17:11,390
And at today's scale,

465
00:17:11,430 --> 00:17:15,229
you also know that you don't want to pay for what you don't need.

466
00:17:15,589 --> 00:17:18,569
You want the choice to be there now and in the future,

467
00:17:18,868 --> 00:17:23,030
whether it's a line of business making a decision on what analytic solution to use,

468
00:17:23,310 --> 00:17:26,069
or it's for you at whatever stage you are

469
00:17:26,270 --> 00:17:27,589
in your data journey.

470
00:17:30,750 --> 00:17:35,170
Now,
the thing that makes these open analytics architectures possible

471
00:17:35,390 --> 00:17:35,890
is

472
00:17:36,030 --> 00:17:37,089
open data formats.

473
00:17:37,189 --> 00:17:39,349
It's like it's parquet and it's iceberg.

474
00:17:39,670 --> 00:17:42,510
Open data formats are the key because it means that

475
00:17:42,510 --> 00:17:45,510
you can have an open data format in your data.

476
00:17:45,770 --> 00:17:49,189
And then you can just swap out these different analytic solutions

477
00:17:49,410 --> 00:17:51,449
that you use with your shared data set.

478
00:17:51,729 --> 00:17:54,050
And if you use these open data formats,

479
00:17:54,290 --> 00:17:58,000
you can do those changes for analytics and you don't have to get roped

480
00:17:58,250 --> 00:18:00,810
into an expensive data migration.

481
00:18:02,089 --> 00:18:02,589
Now,

482
00:18:02,609 --> 00:18:03,280
in S3,

483
00:18:03,329 --> 00:18:06,469
we started to see the shift to these open data formats

484
00:18:06,910 --> 00:18:08,089
about 5 years ago.

485
00:18:08,290 --> 00:18:09,050
And today,

486
00:18:09,130 --> 00:18:12,979
S3 stores exabytes of Apache Parquet data,

487
00:18:13,170 --> 00:18:17,079
and we average 25 million requests per second,

488
00:18:17,439 --> 00:18:17,939
just

489
00:18:18,280 --> 00:18:19,530
to parquet.

490
00:18:22,349 --> 00:18:24,449
Now,
these open data formats have really taken

491
00:18:24,449 --> 00:18:27,729
off because customers don't want to deploy monolithic

492
00:18:27,949 --> 00:18:29,589
analytics architectures.

493
00:18:29,910 --> 00:18:32,750
They want to let different organizations pick

494
00:18:32,989 --> 00:18:35,260
the right analytics solution for them.

495
00:18:35,510 --> 00:18:36,630
Whether that's Redshift,

496
00:18:36,709 --> 00:18:39,770
that's bringing the data warehouse smarts to a data lake,

497
00:18:40,099 --> 00:18:42,709
ad hoc querying using the new Athena for Spark,

498
00:18:43,030 --> 00:18:47,150
using our managed Kafka service MSK or managed Flink MSF

499
00:18:47,469 --> 00:18:49,510
or any other analytic solution.

500
00:18:50,209 --> 00:18:51,689
And open architecture,

501
00:18:51,729 --> 00:18:56,180
and open analytics architecture helps you evolve your data strategy

502
00:18:56,489 --> 00:18:59,400
without having to do that data migration when you make

503
00:18:59,400 --> 00:19:02,689
a decision to go with a different analytics provider.

504
00:19:02,930 --> 00:19:04,030
And nobody,

505
00:19:04,369 --> 00:19:05,109
nobody

506
00:19:05,650 --> 00:19:08,310
does composable building blocks better

507
00:19:08,489 --> 00:19:09,599
than AWS.

508
00:19:10,959 --> 00:19:11,459
Now,

509
00:19:11,800 --> 00:19:15,760
the composability also means that it's cost effective.

510
00:19:15,810 --> 00:19:18,099
It's economical because you're not paying

511
00:19:18,319 --> 00:19:20,750
for a bundle of things that you're just not going to need.

512
00:19:21,040 --> 00:19:23,510
And because we have SageMaker Unified Studio,

513
00:19:23,560 --> 00:19:24,939
which you just saw in action,

514
00:19:25,400 --> 00:19:28,239
you don't sacrifice on the simplicity of experience.

515
00:19:28,520 --> 00:19:30,439
You now have an easy to use

516
00:19:30,439 --> 00:19:34,000
analytics development environment with SageMaker Unified Studio,

517
00:19:34,199 --> 00:19:36,780
and I'm sure many of you are going to want to live

518
00:19:37,160 --> 00:19:38,170
in that notebook.

519
00:19:39,859 --> 00:19:41,119
Now,
with AWS

520
00:19:41,579 --> 00:19:42,459
we have this choice.

521
00:19:42,540 --> 00:19:44,109
We have the choice of services,

522
00:19:44,619 --> 00:19:48,439
or you can use the integrated IDE of SageMaker Unified Studio,

523
00:19:48,819 --> 00:19:50,439
but we're also creating something

524
00:19:50,660 --> 00:19:51,160
new.

525
00:19:51,500 --> 00:19:54,949
We're creating something that we call analytics building blocks.

526
00:19:55,060 --> 00:19:55,560
OK.

527
00:19:55,819 --> 00:19:58,619
An analytics building block is a capability that can

528
00:19:58,619 --> 00:20:01,084
be used across all of our analytics services,

529
00:20:01,094 --> 00:20:02,224
and it's only

530
00:20:02,685 --> 00:20:03,305
available

531
00:20:03,444 --> 00:20:05,165
in AWS analytics.

532
00:20:05,484 --> 00:20:07,844
If you think about an analytics building block,

533
00:20:08,084 --> 00:20:10,885
think about it as disaggregating a core primitive,

534
00:20:10,964 --> 00:20:12,824
a core analytics primitive from

535
00:20:13,005 --> 00:20:14,444
a specific service,

536
00:20:14,724 --> 00:20:18,545
and then being able to use them across all of our different

537
00:20:18,805 --> 00:20:19,505
services.

538
00:20:20,479 --> 00:20:23,229
Notebook is a great example with its polygot cells.

539
00:20:23,479 --> 00:20:25,819
Quick dashboards is another example of this.

540
00:20:26,079 --> 00:20:27,270
And this week,

541
00:20:27,599 --> 00:20:29,380
we introduced a new

542
00:20:29,599 --> 00:20:31,260
cross-service building block,

543
00:20:31,520 --> 00:20:35,599
a fully managed materialized view for Iceberg.

544
00:20:37,670 --> 00:20:40,650
It's an incredibly powerful concept for builders

545
00:20:40,829 --> 00:20:42,550
of any type of application.

546
00:20:42,900 --> 00:20:46,670
You can create these materialized views using the Apache Spark

547
00:20:47,030 --> 00:20:50,530
version 3.5.6 engine that we launched this week,

548
00:20:50,880 --> 00:20:53,540
and these views are then automatically stored

549
00:20:53,670 --> 00:20:55,010
in an S3 table

550
00:20:55,229 --> 00:20:56,489
with full iceberg support,

551
00:20:56,780 --> 00:21:00,869
and it automatically appears as part of your AWS Glue data catalog.

552
00:21:01,285 --> 00:21:03,714
It shows up as just another table.

553
00:21:04,175 --> 00:21:07,415
Now,
the materialized view automatically detects changes and

554
00:21:07,415 --> 00:21:09,114
updates your views as the new data

555
00:21:09,574 --> 00:21:09,935
arrives.

556
00:21:09,935 --> 00:21:11,285
It's fully managed.

557
00:21:11,494 --> 00:21:12,935
We have a whole infrastructure,

558
00:21:13,015 --> 00:21:14,734
a whole microservice behind it.

559
00:21:15,094 --> 00:21:17,895
So there's no manual orchestration needed on your behalf.

560
00:21:18,094 --> 00:21:20,535
It runs on our fully managed infrastructure,

561
00:21:20,854 --> 00:21:22,114
so you don't have to worry

562
00:21:22,375 --> 00:21:24,175
about provisioning compute.

563
00:21:25,160 --> 00:21:28,410
Now,
these materialized views are a building block in itself.

564
00:21:28,719 --> 00:21:31,599
And so when you have them in your sage in your,

565
00:21:31,680 --> 00:21:32,160
um,

566
00:21:32,160 --> 00:21:32,500
uh,

567
00:21:32,500 --> 00:21:33,459
glue catalog

568
00:21:33,719 --> 00:21:34,349
as a table,

569
00:21:34,400 --> 00:21:35,780
it can then be queried

570
00:21:35,959 --> 00:21:39,739
by Athena or SageMaker unified tools like the notebook you just saw,

571
00:21:40,160 --> 00:21:41,359
editor for VS Code,

572
00:21:41,439 --> 00:21:42,959
Jupiter Lab IDE

573
00:21:43,359 --> 00:21:45,540
or Redshift can query the iceberg table.

574
00:21:45,635 --> 00:21:46,214
Directly

575
00:21:46,435 --> 00:21:48,954
or any other third-party analytics solution

576
00:21:49,114 --> 00:21:50,915
as long as it's ICEberg compliant.

577
00:21:51,314 --> 00:21:52,074
In addition,

578
00:21:52,354 --> 00:21:57,435
our Spark engines will automatically rewrite queries to use these views,

579
00:21:57,765 --> 00:22:02,094
giving you performance improvements and in our testing up to 8 times

580
00:22:02,275 --> 00:22:03,755
in performance improvement

581
00:22:04,114 --> 00:22:06,074
without any code changes on your behalf.

582
00:22:07,699 --> 00:22:11,760
Another example of a cross-surface building block is an S3 table.

583
00:22:12,140 --> 00:22:13,660
Like our materialized view,

584
00:22:13,739 --> 00:22:15,699
S3 tables can be queried

585
00:22:15,900 --> 00:22:18,579
by any iceberg compliant analytic solution,

586
00:22:18,859 --> 00:22:21,500
and that could be an AWS solution like Athena,

587
00:22:21,640 --> 00:22:22,959
EMR,
Redshift,

588
00:22:23,260 --> 00:22:25,760
or it can be a third-party solution like Snowflake.

589
00:22:28,660 --> 00:22:29,069
Now,

590
00:22:29,069 --> 00:22:30,400
we iterate pretty fast

591
00:22:30,989 --> 00:22:31,989
across these cross,

592
00:22:32,050 --> 00:22:34,119
uh,
on these cross-surface building blocks.

593
00:22:34,530 --> 00:22:35,660
So as an example,

594
00:22:35,819 --> 00:22:38,660
since the launch of S3 tables at reinvent a year ago,

595
00:22:38,979 --> 00:22:41,369
we've added over 15 new features,

596
00:22:41,739 --> 00:22:42,689
including this week,

597
00:22:42,750 --> 00:22:45,420
an intelligent sharing storage support for tables

598
00:22:45,739 --> 00:22:48,180
and cross-region cross-count replication.

599
00:22:50,719 --> 00:22:53,380
Customers all over the world in every industry

600
00:22:53,760 --> 00:22:57,079
are moving to these open analytics architectures because

601
00:22:57,079 --> 00:22:59,359
they want to work with data at scale,

602
00:22:59,650 --> 00:23:03,209
and many of our customers who are living on the frontier of data

603
00:23:03,439 --> 00:23:04,500
are leading the way.

604
00:23:04,959 --> 00:23:09,739
To tell you more about how they've evolved their architectures for humans and AI,

605
00:23:10,079 --> 00:23:11,920
I'd like to invite Tristan Baker,

606
00:23:12,079 --> 00:23:13,380
distinguished engineer

607
00:23:13,680 --> 00:23:14,449
for Intuit,

608
00:23:14,560 --> 00:23:15,160
to the stage.

609
00:23:15,239 --> 00:23:15,839
Tristan?

610
00:23:17,430 --> 00:23:17,930
This is Dent.

611
00:23:19,489 --> 00:23:19,989
Thank you,

612
00:23:20,650 --> 00:23:21,410
thank you,
thank you.

613
00:23:24,189 --> 00:23:24,790
Hello,

614
00:23:25,000 --> 00:23:26,300
I'm so excited to be here,

615
00:23:26,560 --> 00:23:26,880
uh,

616
00:23:26,880 --> 00:23:28,339
to talk a little bit about

617
00:23:28,599 --> 00:23:29,660
how Intuit uses

618
00:23:29,800 --> 00:23:30,760
data and AI,

619
00:23:31,089 --> 00:23:33,000
uh,
to power prosperity for its customers.

620
00:23:33,239 --> 00:23:34,060
If you're not familiar,

621
00:23:34,400 --> 00:23:34,719
uh,

622
00:23:34,719 --> 00:23:37,540
Intuit is built on what we call an AI-driven expert platform.

623
00:23:37,949 --> 00:23:38,270
Um,

624
00:23:38,270 --> 00:23:41,810
and that platform drives our four core products which are TurboTax,

625
00:23:42,030 --> 00:23:42,630
Credit Karma,

626
00:23:42,739 --> 00:23:43,310
QuickBooks,

627
00:23:43,550 --> 00:23:44,250
and MailChimp.

628
00:23:44,550 --> 00:23:46,650
Collectively those products serve

629
00:23:46,869 --> 00:23:49,189
our consumers and small and mid-market businesses,

630
00:23:49,469 --> 00:23:51,540
and I'm gonna talk a little bit about how we pull that off,

631
00:23:51,750 --> 00:23:53,219
uh,
the scale that it operates at,

632
00:23:53,430 --> 00:23:55,290
and the data strategy that sits behind all of it.

633
00:23:55,790 --> 00:23:56,290
Um,

634
00:23:56,390 --> 00:23:59,469
so what exists in the Intuit platform I will get to in a minute,

635
00:23:59,709 --> 00:24:01,199
but I want to give you a sense of the scale.

636
00:24:01,300 --> 00:24:03,530
So the scale is 86 million consumers,

637
00:24:03,829 --> 00:24:04,329
uh,

638
00:24:04,670 --> 00:24:08,979
collectively receiving $105 billion in refunds from our tax product every year,

639
00:24:09,359 --> 00:24:13,250
uh,
managing $11.4 trillion worth of debt with products like Credit Karma.

640
00:24:13,500 --> 00:24:15,959
Uh,
we also have 10 million small admin market businesses

641
00:24:16,189 --> 00:24:18,369
that manage $2 trillion worth of invoices,

642
00:24:18,589 --> 00:24:18,939
um,

643
00:24:18,939 --> 00:24:21,459
and manage the payroll for 18 million US workers.

644
00:24:21,829 --> 00:24:24,209
So it's a lot of stuff and a lot of stuff means a lot of data,

645
00:24:24,469 --> 00:24:24,969
um.

646
00:24:25,130 --> 00:24:28,640
And a lot of data means you need really good data and data services.

647
00:24:28,849 --> 00:24:31,270
You need artificial and human intelligence built on top.

648
00:24:31,569 --> 00:24:33,109
And what that delivers for Intuit

649
00:24:33,209 --> 00:24:35,209
is something that we call a system of intelligence.

650
00:24:35,410 --> 00:24:38,569
And you contrast that with maybe what Intuit products were a few years ago,

651
00:24:38,689 --> 00:24:40,420
which were maybe systems of record.

652
00:24:40,689 --> 00:24:42,089
Our customers relied on us to manage.

653
00:24:42,334 --> 00:24:44,185
Data to manage their compliance workflows,

654
00:24:44,255 --> 00:24:45,094
to file their taxes,

655
00:24:45,175 --> 00:24:46,045
to close their books,

656
00:24:46,334 --> 00:24:48,375
they didn't necessarily rely on us for intelligence,

657
00:24:48,694 --> 00:24:50,035
but if we have this trove of data

658
00:24:50,324 --> 00:24:52,415
and we add intelligence on top of that,

659
00:24:52,694 --> 00:24:55,454
then what you end up having is a system of intelligence that can actually

660
00:24:55,454 --> 00:24:58,954
power the prosperity that we envision powering for our customers on our platform.

661
00:24:59,689 --> 00:25:00,410
So what does all those,

662
00:25:00,510 --> 00:25:02,670
uh,
what do all those customers mean in terms of

663
00:25:03,170 --> 00:25:05,050
what's happening under the covers and the scale that's happening,

664
00:25:05,079 --> 00:25:06,209
uh,
in the data layers?

665
00:25:06,530 --> 00:25:07,709
Well,
we have 70,000,

666
00:25:07,729 --> 00:25:10,250
uh,
tax and financial attributes per customer.

667
00:25:10,609 --> 00:25:12,489
Uh,
we're managing 188 million,

668
00:25:12,569 --> 00:25:14,890
uh,
natural language conversations through some of

669
00:25:14,890 --> 00:25:16,719
our more recent agentic experiences,

670
00:25:17,010 --> 00:25:18,920
and that makes up just a fraction of the 180

671
00:25:18,920 --> 00:25:20,910
petabytes of data that we manage in our data lake.

672
00:25:21,449 --> 00:25:21,949
Uh,

673
00:25:21,969 --> 00:25:22,469
so

674
00:25:22,569 --> 00:25:23,040
the,

675
00:25:23,040 --> 00:25:23,410
uh,

676
00:25:23,410 --> 00:25:24,069
services,

677
00:25:24,170 --> 00:25:27,310
uh,
the AWS services that power all this are highlighted here at the bottom.

678
00:25:27,689 --> 00:25:28,040
Um,

679
00:25:28,040 --> 00:25:29,369
I'm gonna go into more detail and,

680
00:25:29,449 --> 00:25:32,619
and more precision about how exactly those things are arranged

681
00:25:32,770 --> 00:25:33,270
in a moment,

682
00:25:33,329 --> 00:25:34,910
so stick with me for a few more slides.

683
00:25:35,170 --> 00:25:35,670
Um,

684
00:25:35,810 --> 00:25:36,329
the,

685
00:25:36,650 --> 00:25:37,829
uh,
first I want to motivate,

686
00:25:38,050 --> 00:25:38,410
uh,

687
00:25:38,410 --> 00:25:42,250
our strategy with the problem that we were facing maybe 2.5 years ago.

688
00:25:42,670 --> 00:25:43,170
Um,

689
00:25:43,400 --> 00:25:44,119
how do you manage,

690
00:25:44,239 --> 00:25:47,579
how do you better manage a data lake of 320,000 tables?

691
00:25:48,560 --> 00:25:50,699
We established a key metric that we like to call

692
00:25:51,150 --> 00:25:52,660
time to discover and access data,

693
00:25:52,920 --> 00:25:55,270
and when we measured that metric 2.5 years ago,

694
00:25:55,479 --> 00:25:56,599
it was something like 20 days,

695
00:25:56,680 --> 00:25:58,420
which is like eons in

696
00:25:58,760 --> 00:26:01,160
the speed and scale of the businesses that we are running.

697
00:26:01,589 --> 00:26:01,969
Uh,

698
00:26:01,969 --> 00:26:02,780
so how do you

699
00:26:03,109 --> 00:26:04,050
bring that metric down?

700
00:26:04,119 --> 00:26:04,469
Well,
first,

701
00:26:04,469 --> 00:26:05,079
let's understand it.

702
00:26:05,160 --> 00:26:05,839
Why,
why,

703
00:26:05,839 --> 00:26:07,030
why 20 days is what it is.

704
00:26:07,079 --> 00:26:07,439
If you,

705
00:26:07,439 --> 00:26:08,949
if you look at this screen here,

706
00:26:09,160 --> 00:26:11,099
this is a snapshot of a typical

707
00:26:11,439 --> 00:26:14,339
data discovery screen that is part of our internal data discovery

708
00:26:15,079 --> 00:26:15,579
experience,

709
00:26:15,760 --> 00:26:18,199
and what you see is that a lot of important information is missing.

710
00:26:18,500 --> 00:26:20,390
We don't really know what the table contains.

711
00:26:20,640 --> 00:26:21,829
We don't know who owns it.

712
00:26:22,040 --> 00:26:23,989
We don't know where it came from or where it's going,

713
00:26:24,199 --> 00:26:25,479
and it's not that this information is not.

714
00:26:25,584 --> 00:26:26,135
Knowable,

715
00:26:26,305 --> 00:26:28,125
it's just buried in people's heads or in,

716
00:26:28,185 --> 00:26:30,885
uh,
non-standard wiki pages and tribal knowledge

717
00:26:31,025 --> 00:26:32,704
that takes tons of time to unearth,

718
00:26:32,854 --> 00:26:34,744
and that time is measured in days because it takes

719
00:26:34,744 --> 00:26:36,625
about that long for the average team to finally get

720
00:26:36,984 --> 00:26:38,954
access to the data that they need and to be productive.

721
00:26:39,344 --> 00:26:41,064
So this is what motivates our data strategy.

722
00:26:41,145 --> 00:26:42,724
It's a bellwether metric for

723
00:26:42,824 --> 00:26:44,765
whether or not things are generally going right or wrong.

724
00:26:45,064 --> 00:26:46,305
And so we've tracked this over time,

725
00:26:46,344 --> 00:26:47,645
and I'm gonna tell a story about

726
00:26:47,824 --> 00:26:48,844
how that has improved.

727
00:26:49,104 --> 00:26:49,604
Um,

728
00:26:50,025 --> 00:26:51,385
so we have a 4-part data strategy,

729
00:26:51,464 --> 00:26:51,944
as I mentioned,

730
00:26:51,944 --> 00:26:53,824
I'm gonna go through each part here one at a time.

731
00:26:54,140 --> 00:26:55,709
The first part of that 4-part strategy

732
00:26:55,709 --> 00:26:57,699
is what we call standardizing data semantics.

733
00:26:58,030 --> 00:26:58,430
So yes,

734
00:26:58,430 --> 00:27:00,270
we have 4 or 5 different product lines,

735
00:27:00,469 --> 00:27:01,989
uh,
but more and more of these product lines need

736
00:27:01,989 --> 00:27:03,430
to be able to communicate with each other.

737
00:27:03,670 --> 00:27:05,619
And whenever something needs to communicate with something else,

738
00:27:05,670 --> 00:27:07,349
you have to have a common language that everything,

739
00:27:07,449 --> 00:27:07,829
uh,

740
00:27:07,829 --> 00:27:08,550
everyone understands.

741
00:27:08,869 --> 00:27:10,089
Um,
and it wasn't until we put

742
00:27:10,310 --> 00:27:11,609
a concerted effort into it

743
00:27:11,719 --> 00:27:14,540
that we could develop something like a common semantic layer.

744
00:27:14,790 --> 00:27:17,880
Um,
and I'll give you a preview of kind of what that looks like at a very high level.

745
00:27:18,030 --> 00:27:18,530
So,

746
00:27:18,589 --> 00:27:21,780
uh,
the semantic layers divided into what we call domains and subdomains.

747
00:27:22,069 --> 00:27:23,689
One very popular and important domain,

748
00:27:24,020 --> 00:27:24,790
uh,
is

749
00:27:24,790 --> 00:27:25,290
customer.

750
00:27:25,560 --> 00:27:28,069
So all of our customers have identity profiles and

751
00:27:28,069 --> 00:27:30,290
account profiles that help us identify who they are

752
00:27:30,469 --> 00:27:31,430
and where they're from.

753
00:27:31,670 --> 00:27:32,170
Um,

754
00:27:32,510 --> 00:27:34,709
some of our customers are consumers,

755
00:27:34,949 --> 00:27:36,310
so consumers then have credit,

756
00:27:36,630 --> 00:27:37,829
income,
and tax profiles,

757
00:27:38,099 --> 00:27:38,609
and these

758
00:27:38,709 --> 00:27:40,540
profiles are the lingua franca of,

759
00:27:40,550 --> 00:27:43,589
uh,
how our TurboTax and Credit Karma products communicate with each other.

760
00:27:44,000 --> 00:27:44,479
Um,

761
00:27:44,479 --> 00:27:47,000
we also have another domain of information that we call business,

762
00:27:47,359 --> 00:27:50,229
um,
and our businesses manage profiles that describe commerce,

763
00:27:50,439 --> 00:27:51,239
their accounting,

764
00:27:51,489 --> 00:27:52,040
their money,

765
00:27:52,239 --> 00:27:54,310
the customer relationship management behaviors,

766
00:27:54,540 --> 00:27:55,540
as well as the lending.

767
00:27:55,880 --> 00:27:56,380
Uh,

768
00:27:56,420 --> 00:27:57,020
Collectively,

769
00:27:57,079 --> 00:27:57,579
this

770
00:27:57,800 --> 00:27:59,439
produces what we call the semantic layer,

771
00:27:59,589 --> 00:28:00,390
and it's what allows,

772
00:28:00,400 --> 00:28:03,479
it creates the data glue that allows our products to communicate with each other.

773
00:28:03,839 --> 00:28:06,520
And it's not just good enough to have this information available,

774
00:28:06,750 --> 00:28:08,829
you actually need it available in the right places.

775
00:28:09,079 --> 00:28:09,829
And when I say places,

776
00:28:09,880 --> 00:28:11,060
I mean data stores

777
00:28:11,239 --> 00:28:13,540
optimized for particular query patterns because

778
00:28:13,680 --> 00:28:15,189
the transactional query pattern is very,

779
00:28:15,270 --> 00:28:17,000
very different than an analytical query pattern.

780
00:28:17,239 --> 00:28:19,339
The query might be about the same semantic information,

781
00:28:19,439 --> 00:28:21,229
but the questions being asked are vastly different.

782
00:28:21,640 --> 00:28:24,310
So this brings us to the second part of the 4-part strategy,

783
00:28:24,599 --> 00:28:27,510
which is to standardize common data infrastructure and pathways,

784
00:28:27,800 --> 00:28:29,510
and it starts with an observation that,

785
00:28:29,800 --> 00:28:30,300
um.

786
00:28:30,619 --> 00:28:32,969
Uh,
we want to make the typical thing

787
00:28:33,270 --> 00:28:34,170
easy to build

788
00:28:34,589 --> 00:28:35,130
and

789
00:28:35,469 --> 00:28:36,949
ask people to build typical things.

790
00:28:37,030 --> 00:28:38,109
And if we can pull that off,

791
00:28:38,390 --> 00:28:41,890
then what we can do is create one-click tooling and one-click architectures

792
00:28:42,030 --> 00:28:44,609
that allow our development teams to very quickly and easily get

793
00:28:44,760 --> 00:28:45,449
not just

794
00:28:45,670 --> 00:28:46,170
an

795
00:28:46,859 --> 00:28:48,569
application layer that includes your typical app,

796
00:28:48,750 --> 00:28:49,069
service,

797
00:28:49,069 --> 00:28:49,979
and database layer,

798
00:28:50,310 --> 00:28:50,810
but to

799
00:28:50,949 --> 00:28:53,290
get it in a way that allows it to produce data

800
00:28:53,390 --> 00:28:55,650
that is available to the next typical thing in the picture.

801
00:28:56,040 --> 00:28:56,359
Uh,

802
00:28:56,359 --> 00:28:58,060
so we have the typical architecture

803
00:28:58,239 --> 00:28:58,640
of,

804
00:28:58,640 --> 00:28:59,760
uh,
an Intuit product.

805
00:29:00,040 --> 00:29:00,800
We also have,

806
00:29:00,920 --> 00:29:01,270
uh,

807
00:29:01,270 --> 00:29:04,079
acquisition pipelines because a lot of our customer data is managed

808
00:29:04,079 --> 00:29:07,040
by financial institutions that live outside of the Intuit rails,

809
00:29:07,079 --> 00:29:07,900
but we still need

810
00:29:08,079 --> 00:29:10,599
that information in our environment in order to do useful things.

811
00:29:10,920 --> 00:29:13,900
So we make it extremely easy for any development team to both source

812
00:29:14,119 --> 00:29:15,119
data from an external,

813
00:29:15,180 --> 00:29:15,680
uh,

814
00:29:16,150 --> 00:29:17,380
uh,
from an external company

815
00:29:17,599 --> 00:29:18,329
or to,

816
00:29:18,400 --> 00:29:18,800
uh,

817
00:29:18,800 --> 00:29:20,760
produce their own application or service,

818
00:29:20,959 --> 00:29:23,979
and collectively these two things produce what we call either product events

819
00:29:24,160 --> 00:29:25,160
or third party data.

820
00:29:25,640 --> 00:29:26,680
So where do those things go?

821
00:29:26,770 --> 00:29:28,319
They go to the next typical thing,

822
00:29:28,439 --> 00:29:30,890
which is the typical architecture of a data processing system.

823
00:29:31,209 --> 00:29:32,810
Those generally come in two different flavors,

824
00:29:32,930 --> 00:29:36,040
be it either streaming for real-time or near real-time processing,

825
00:29:36,290 --> 00:29:38,630
or batch pipelines for longer latency,

826
00:29:38,689 --> 00:29:39,959
higher workload pipelines.

827
00:29:40,729 --> 00:29:44,430
That kind of processing is ultimately what takes the raw data in

828
00:29:44,650 --> 00:29:46,369
to produce the insights that we need out.

829
00:29:46,670 --> 00:29:50,109
To begin to deliver on our promise of being a system of intelligence.

830
00:29:50,469 --> 00:29:50,910
So,

831
00:29:50,910 --> 00:29:51,410
uh,

832
00:29:51,790 --> 00:29:53,530
one direction of the arrow out of this box

833
00:29:53,910 --> 00:29:56,660
is what we call data and insights about customers,

834
00:29:56,910 --> 00:29:59,729
and this arrow feeds another typical architecture of what we call,

835
00:30:00,030 --> 00:30:01,729
uh,
our business intelligence systems.

836
00:30:02,130 --> 00:30:04,939
So this data flows into another section of our data lake

837
00:30:05,359 --> 00:30:05,859
where

838
00:30:06,319 --> 00:30:08,400
reports and dashboards are easily built on top,

839
00:30:08,560 --> 00:30:10,959
where development and data exploration tools,

840
00:30:11,520 --> 00:30:13,989
the screenshot that I showed earlier is an example of such a tool,

841
00:30:14,199 --> 00:30:15,660
is also available right on top.

842
00:30:15,880 --> 00:30:17,020
And this is what feeds

843
00:30:17,560 --> 00:30:18,550
our into business leaders,

844
00:30:18,599 --> 00:30:19,040
for instance,

845
00:30:19,040 --> 00:30:21,260
with the information they need to make strategic

846
00:30:21,719 --> 00:30:22,780
decisions about changing

847
00:30:23,079 --> 00:30:23,869
product strategy,

848
00:30:24,079 --> 00:30:25,119
changing product behavior,

849
00:30:25,239 --> 00:30:26,739
going after new portions of a market.

850
00:30:27,239 --> 00:30:27,609
Uh,

851
00:30:27,609 --> 00:30:29,869
the other thing that comes out of this typical architecture,

852
00:30:30,109 --> 00:30:31,670
uh,
of a data processing system

853
00:30:31,859 --> 00:30:34,160
is what we call data and insights for customers.

854
00:30:34,530 --> 00:30:36,489
So if it's a data and insight for a customer,

855
00:30:36,729 --> 00:30:38,469
then it's got to have a path that gets back,

856
00:30:38,599 --> 00:30:40,089
uh,
in front of the customer's eyeballs,

857
00:30:40,109 --> 00:30:40,609
and that goes

858
00:30:40,800 --> 00:30:41,790
up and back to the left.

859
00:30:42,349 --> 00:30:42,709
Uh,

860
00:30:42,709 --> 00:30:46,560
and so the data insights for customers go up to what we call our customer data cloud,

861
00:30:46,599 --> 00:30:49,869
which is where some of those 70,000 attributes I mentioned earlier are managed.

862
00:30:50,229 --> 00:30:50,760
On top of that,

863
00:30:50,800 --> 00:30:51,329
we sit,

864
00:30:51,339 --> 00:30:52,599
uh,
into its Gen OS,

865
00:30:52,959 --> 00:30:55,119
uh,
which is a little bit beyond the scope of the conversation today,

866
00:30:55,199 --> 00:30:57,000
but suffice to say it's the core engine that

867
00:30:57,000 --> 00:30:59,500
drives a lot of our genic and intelligent experiences.

868
00:30:59,839 --> 00:31:00,319
Um,

869
00:31:00,319 --> 00:31:00,819
and

870
00:31:00,829 --> 00:31:01,199
when,

871
00:31:01,199 --> 00:31:04,400
uh,
that data flows up through customer data cloud into Gen OS,

872
00:31:04,680 --> 00:31:06,619
it essentially becomes the data and advice,

873
00:31:06,910 --> 00:31:10,020
uh,
that we promise to deliver to our customers that again helps them power their,

874
00:31:10,109 --> 00:31:10,439
uh,

875
00:31:10,439 --> 00:31:11,479
financial prosperity.

876
00:31:12,089 --> 00:31:14,130
Uh,
so that's the second part of the strategy.

877
00:31:14,439 --> 00:31:17,310
Um,
I'll highlight a couple of the important bubbles here,

878
00:31:17,650 --> 00:31:18,569
which are the databases,

879
00:31:18,650 --> 00:31:19,609
the customer data cloud,

880
00:31:19,689 --> 00:31:20,250
the event bus,

881
00:31:20,290 --> 00:31:21,050
and the data lake

882
00:31:21,219 --> 00:31:22,790
that essentially make up the

883
00:31:22,969 --> 00:31:25,099
3 or 4 different typical data stores

884
00:31:25,329 --> 00:31:27,550
where the data and information needs to be available

885
00:31:27,550 --> 00:31:29,699
in different ways for different kinds of query patterns.

886
00:31:30,089 --> 00:31:32,839
And Essentially this makes intuit zero ETL architecture,

887
00:31:32,849 --> 00:31:35,050
and it's not zero ETL in that data is not moving.

888
00:31:35,329 --> 00:31:36,410
Data is definitely moving,

889
00:31:36,609 --> 00:31:38,630
but from the perspective of the teams that use

890
00:31:38,849 --> 00:31:39,290
our,
uh,

891
00:31:39,290 --> 00:31:41,959
the development teams that use our typical architectures,

892
00:31:42,209 --> 00:31:43,550
it appears to work like magic.

893
00:31:43,810 --> 00:31:45,800
If you're writing an application or service,

894
00:31:45,849 --> 00:31:48,390
the data is immediately available to your downstream analytics team.

895
00:31:48,810 --> 00:31:49,119
Um,

896
00:31:49,119 --> 00:31:51,170
if your analytics team is producing new insights that

897
00:31:51,170 --> 00:31:52,530
we need to feed back into the product,

898
00:31:52,729 --> 00:31:55,849
then that's just magically and immediately available back into those products.

899
00:31:56,989 --> 00:31:57,390
Um,

900
00:31:57,390 --> 00:31:58,949
I also said that I would highlight more

901
00:31:58,949 --> 00:32:01,099
precisely where Intuit services are being leveraged,

902
00:32:01,310 --> 00:32:03,290
and this is more or less how they're laid out.

903
00:32:03,510 --> 00:32:03,869
Uh,

904
00:32:03,869 --> 00:32:04,739
many of Intuit's,

905
00:32:04,750 --> 00:32:07,410
uh,
managed database systems like Aurora and Dynamo,

906
00:32:07,630 --> 00:32:08,030
uh,

907
00:32:08,030 --> 00:32:09,099
are driving our,

908
00:32:09,270 --> 00:32:11,500
uh,
transactional workloads for our products.

909
00:32:11,949 --> 00:32:12,469
The

910
00:32:12,579 --> 00:32:14,369
streaming and batch processing is,

911
00:32:14,430 --> 00:32:16,069
uh,
heavily reliant on EMR,

912
00:32:16,390 --> 00:32:17,229
uh,
MSK,

913
00:32:17,469 --> 00:32:18,640
as well as,

914
00:32:18,910 --> 00:32:19,270
uh,

915
00:32:19,270 --> 00:32:21,189
Lake Formation Glue data catalog in S3,

916
00:32:21,510 --> 00:32:24,729
uh,
using many of those OpenTable formats that Mylan was mentioning previously.

917
00:32:25,030 --> 00:32:28,219
And then our BI tools and development tools were built on the backs of Sagemaker,

918
00:32:28,229 --> 00:32:28,729
Athena,

919
00:32:28,910 --> 00:32:29,469
and Quicksight.

920
00:32:30,910 --> 00:32:32,170
Uh,
moving on to the third

921
00:32:32,520 --> 00:32:33,579
leg of our strategy

922
00:32:34,079 --> 00:32:36,380
is the metadata around the data itself.

923
00:32:36,800 --> 00:32:40,300
Often that metadata is what is needed in order to describe this rich context.

924
00:32:40,560 --> 00:32:41,420
And if all you had,

925
00:32:42,040 --> 00:32:44,550
all you knew about a data asset were the 3 boxes drawn on the screen,

926
00:32:44,780 --> 00:32:46,800
you would be left scratching your head with a lot of questions.

927
00:32:47,000 --> 00:32:47,520
What does it mean?

928
00:32:47,599 --> 00:32:48,199
How did it get there?

929
00:32:48,280 --> 00:32:48,869
Who owns it?

930
00:32:49,160 --> 00:32:52,579
Those kinds of things are only answerable if you decorate this kind of information

931
00:32:52,839 --> 00:32:54,040
with more metadata.

932
00:32:54,339 --> 00:32:56,979
And one of the most important and critical pieces of metadata that

933
00:32:56,979 --> 00:32:59,180
we had to collect very early on because we didn't have it,

934
00:32:59,219 --> 00:33:00,650
or at least not in the way that we needed,

935
00:33:00,859 --> 00:33:02,140
was ownership information.

936
00:33:02,420 --> 00:33:03,300
Who are the teams?

937
00:33:03,579 --> 00:33:05,979
What are the business purposes under which they are operating,

938
00:33:06,020 --> 00:33:06,770
what projects?

939
00:33:07,010 --> 00:33:08,339
What are the roles of the different team,

940
00:33:08,349 --> 00:33:08,849
team members,

941
00:33:08,939 --> 00:33:10,839
be they data stewards or data developers?

942
00:33:11,339 --> 00:33:13,599
And it wasn't until we had that information that we could then

943
00:33:13,819 --> 00:33:14,589
say,
Guess what,

944
00:33:14,619 --> 00:33:16,109
thank you for telling us you're an owner of something.

945
00:33:16,260 --> 00:33:18,500
Now I have 15 more things that I need you to tell me.

946
00:33:18,849 --> 00:33:19,880
Some of those things become,

947
00:33:19,979 --> 00:33:22,540
how are you organizing your data assets into data products?

948
00:33:22,939 --> 00:33:24,890
What is the semantic meaning of these data products,

949
00:33:24,900 --> 00:33:26,500
and how is it related to other data products

950
00:33:26,500 --> 00:33:28,319
that your other team members might be creating.

951
00:33:28,660 --> 00:33:30,719
So you start stitching all of this information together

952
00:33:30,859 --> 00:33:32,099
and you begin to get the picture of the

953
00:33:32,099 --> 00:33:34,400
metadata and the context that Intuit has available.

954
00:33:34,979 --> 00:33:35,479
Um,

955
00:33:35,540 --> 00:33:36,010
so

956
00:33:36,010 --> 00:33:36,770
now that we have,

957
00:33:37,099 --> 00:33:38,420
especially the ownership information,

958
00:33:38,500 --> 00:33:40,260
we can get to the last part of our strategy.

959
00:33:40,680 --> 00:33:42,729
Which is to start coming up with rules and requirements and a

960
00:33:42,729 --> 00:33:44,719
way of judging people and the work that they are doing,

961
00:33:45,069 --> 00:33:45,640
which,
uh,

962
00:33:45,640 --> 00:33:47,040
maybe the people being judged don't like to hear,

963
00:33:47,119 --> 00:33:48,300
but I certainly like to do it,

964
00:33:48,530 --> 00:33:48,920
uh,

965
00:33:48,920 --> 00:33:50,359
because there's nothing that makes me happier

966
00:33:50,359 --> 00:33:52,439
than clear rules and time-bound goals,

967
00:33:52,670 --> 00:33:53,040
uh,

968
00:33:53,040 --> 00:33:53,979
that our organization,

969
00:33:54,479 --> 00:33:55,439
uh,
can chase after.

970
00:33:55,890 --> 00:33:58,760
So we've developed a clear set of requirements

971
00:33:58,979 --> 00:34:00,040
broken up across

972
00:34:00,219 --> 00:34:01,689
5 important dimensions,

973
00:34:01,819 --> 00:34:02,890
one being stewardship,

974
00:34:03,219 --> 00:34:03,979
documentation,

975
00:34:04,140 --> 00:34:04,699
data model,

976
00:34:04,819 --> 00:34:05,810
data observability,

977
00:34:06,060 --> 00:34:07,300
and operational stability,

978
00:34:07,579 --> 00:34:09,600
and we have clear requirements that allow us to

979
00:34:09,978 --> 00:34:13,530
measure the degree to which everybody's data products are meeting these criteria.

980
00:34:13,820 --> 00:34:14,978
If you're doing a very good job,

981
00:34:15,080 --> 00:34:17,080
then you end up somewhere on the right-hand side of the spectrum.

982
00:34:17,379 --> 00:34:18,810
And if you're doing a less than good job,

983
00:34:19,020 --> 00:34:20,688
then you're somewhere on the left-hand side of the spectrum.

984
00:34:20,699 --> 00:34:21,719
And then I come knocking

985
00:34:21,820 --> 00:34:22,320
and asking,

986
00:34:22,340 --> 00:34:24,820
what are you going to do for me by the end of this fiscal year.

987
00:34:25,260 --> 00:34:25,770
Um,

988
00:34:25,820 --> 00:34:26,719
so now that we have

989
00:34:26,820 --> 00:34:27,958
all of this set up,

990
00:34:28,418 --> 00:34:28,918
uh,

991
00:34:28,978 --> 00:34:29,820
what does that get us?

992
00:34:29,978 --> 00:34:30,350
Well,

993
00:34:30,350 --> 00:34:31,000
that gets us,

994
00:34:31,100 --> 00:34:31,449
uh,

995
00:34:31,449 --> 00:34:33,639
a screenshot that looks a little bit more like this one than it did,

996
00:34:33,699 --> 00:34:34,600
uh,
in the previous,

997
00:34:34,820 --> 00:34:35,458
uh,
slide.

998
00:34:35,949 --> 00:34:38,100
So instead of seeing no context and no descriptions,

999
00:34:38,149 --> 00:34:40,120
we see rich context and rich descriptions.

1000
00:34:40,399 --> 00:34:40,899
Um,

1001
00:34:40,909 --> 00:34:41,820
even though it's blurred out here,

1002
00:34:41,909 --> 00:34:43,649
there are names of people that own these things,

1003
00:34:43,870 --> 00:34:46,168
uh,
and we can follow up with them if we have more questions.

1004
00:34:46,469 --> 00:34:49,250
Uh,
we can see data quality measures measured over time,

1005
00:34:49,668 --> 00:34:50,530
um,
and advertise,

1006
00:34:50,668 --> 00:34:52,449
giving a sense of the reliability of the data.

1007
00:34:52,929 --> 00:34:54,850
Um,
and you also see self-serve access at the top,

1008
00:34:54,870 --> 00:34:55,810
so we can automate

1009
00:34:55,949 --> 00:34:58,010
the permissions and the management that allows us

1010
00:34:58,270 --> 00:35:00,780
to grant teams the ability to touch this data,

1011
00:35:00,790 --> 00:35:01,290
given

1012
00:35:01,310 --> 00:35:02,149
that they have the right,

1013
00:35:02,340 --> 00:35:03,750
uh,
justification to do so.

1014
00:35:04,100 --> 00:35:04,979
And that means that,

1015
00:35:04,989 --> 00:35:05,340
um,

1016
00:35:05,340 --> 00:35:06,340
2.5 years ago,

1017
00:35:06,379 --> 00:35:07,320
our data was,

1018
00:35:07,959 --> 00:35:09,429
well,
less than 1% clean,

1019
00:35:09,699 --> 00:35:12,699
we can now claim that it is at least 80% clean with

1020
00:35:12,699 --> 00:35:14,860
goals of getting even higher by the end of this fiscal year.

1021
00:35:15,219 --> 00:35:18,280
And that means that we took that 20 hour metric over the course of

1022
00:35:18,409 --> 00:35:19,090
2 years ago,

1023
00:35:19,169 --> 00:35:20,850
uh,
we took that from 20 years to

1024
00:35:21,120 --> 00:35:22,820
20 days to 14.2 hours,

1025
00:35:22,889 --> 00:35:23,810
and then in the most recent year,

1026
00:35:23,850 --> 00:35:26,469
we dropped that from 14.2 hours to 9.4,

1027
00:35:26,860 --> 00:35:29,100
um,
and my team has goals to get that even lower this year.

1028
00:35:29,800 --> 00:35:30,300
Uh,

1029
00:35:30,560 --> 00:35:33,080
so that's kind of what this all looks like in slides and screenshots,

1030
00:35:33,120 --> 00:35:34,419
but I wanted to give you a sense of,

1031
00:35:34,560 --> 00:35:36,219
um,
how this comes to life in a demo.

1032
00:35:36,600 --> 00:35:38,879
Uh,
so how does Intuit use this to build a better,

1033
00:35:39,060 --> 00:35:40,260
better together product?

1034
00:35:40,760 --> 00:35:41,300
Uh,

1035
00:35:41,520 --> 00:35:44,000
there's recently been an effort to bring together more of

1036
00:35:44,000 --> 00:35:46,709
the customer bases that use both TurboTax and Credit Karma,

1037
00:35:46,919 --> 00:35:49,520
so that there's more of a seamless experience between these two offerings.

1038
00:35:49,979 --> 00:35:50,479
The,

1039
00:35:50,780 --> 00:35:51,280
uh,

1040
00:35:51,379 --> 00:35:52,500
I'll pull back up some of the,

1041
00:35:52,550 --> 00:35:53,959
the diagrams we saw earlier,

1042
00:35:54,300 --> 00:35:57,889
and what this looks like in 3 reasonably simple steps is that first,

1043
00:35:57,979 --> 00:35:58,929
as most people do,

1044
00:35:59,139 --> 00:35:59,959
you analyze

1045
00:36:00,250 --> 00:36:01,250
a business question.

1046
00:36:01,610 --> 00:36:03,250
What table and query will show the number of

1047
00:36:03,250 --> 00:36:05,739
Credit Karma members that are also TurboTax users?

1048
00:36:06,060 --> 00:36:09,100
Assuming you see a total addressable market and a market opportunity,

1049
00:36:09,419 --> 00:36:11,540
you then ask your your team to go build something.

1050
00:36:11,919 --> 00:36:12,409
Uh,

1051
00:36:12,409 --> 00:36:14,229
what API that that team

1052
00:36:14,689 --> 00:36:15,949
will probably have a question like

1053
00:36:16,090 --> 00:36:18,820
what API will return a Credit Karma member's unified profile,

1054
00:36:19,209 --> 00:36:20,510
including their tax filing history

1055
00:36:20,889 --> 00:36:22,050
and refund status.

1056
00:36:22,330 --> 00:36:22,770
Um,

1057
00:36:22,770 --> 00:36:25,570
and if you can answer those two questions reasonably simply and quickly,

1058
00:36:25,729 --> 00:36:27,939
then you can reasonably quickly justify the,

1059
00:36:28,060 --> 00:36:30,719
uh,
effort to build something and then reasonably quickly build it,

1060
00:36:30,969 --> 00:36:32,070
and that allows you to just

1061
00:36:32,409 --> 00:36:32,929
pretty much ship,

1062
00:36:33,000 --> 00:36:33,689
ship something.

1063
00:36:34,159 --> 00:36:34,659
So,

1064
00:36:34,760 --> 00:36:35,159
uh,

1065
00:36:35,159 --> 00:36:36,979
in this demo what you'll see is some of the same,

1066
00:36:37,159 --> 00:36:37,760
um,

1067
00:36:38,429 --> 00:36:39,780
application screens that we saw earlier,

1068
00:36:39,989 --> 00:36:41,320
uh,
but now they're kind of in action.

1069
00:36:42,300 --> 00:36:43,679
So somebody like a business analyst might ask how

1070
00:36:43,679 --> 00:36:45,899
many Credit Karma members are also TurboTax users.

1071
00:36:46,199 --> 00:36:48,879
All that rich metadata allows us to run a semantic search that.

1072
00:36:49,199 --> 00:36:50,040
Uh,

1073
00:36:50,159 --> 00:36:52,419
levels that table right to the top of the search result.

1074
00:36:52,560 --> 00:36:54,320
You'll see the star ratings as I described them

1075
00:36:54,320 --> 00:36:56,459
earlier for this data set laid out here.

1076
00:36:56,879 --> 00:36:59,979
You get a sense of where the data is coming from and where it's going because lineage

1077
00:37:00,120 --> 00:37:01,719
metadata is rich and available.

1078
00:37:02,010 --> 00:37:05,620
The data quality gives you a sense of how reliable this data has been over time,

1079
00:37:06,320 --> 00:37:06,820
and

1080
00:37:07,030 --> 00:37:10,280
all of this rich metadata is powering what we call intuit assist,

1081
00:37:10,360 --> 00:37:12,060
which is intuit's generative AI

1082
00:37:12,709 --> 00:37:13,879
analytics assistant.

1083
00:37:14,260 --> 00:37:15,580
Um,
and the same question can be asked,

1084
00:37:15,689 --> 00:37:16,600
asked of this assistant,

1085
00:37:16,860 --> 00:37:18,739
and we'll simply just spit out the answer for you,

1086
00:37:19,020 --> 00:37:21,560
which you can then take into your notebook experience.

1087
00:37:21,929 --> 00:37:22,429
Um,

1088
00:37:22,649 --> 00:37:24,840
and I'm gonna cut away from the screen before you see the answer,

1089
00:37:25,379 --> 00:37:26,939
because that's proprietary intuit information.

1090
00:37:27,060 --> 00:37:27,550
But,

1091
00:37:27,550 --> 00:37:28,050
uh,

1092
00:37:28,060 --> 00:37:30,020
suffice it to say there was an opportunity there.

1093
00:37:30,379 --> 00:37:31,899
So now you can tell your development team,

1094
00:37:32,060 --> 00:37:33,439
all right,
let's go build something.

1095
00:37:33,820 --> 00:37:34,320
Well,

1096
00:37:34,419 --> 00:37:36,020
we take them to the development portal.

1097
00:37:36,260 --> 00:37:38,040
Uh,
the development portal has the same,

1098
00:37:38,139 --> 00:37:38,580
uh,

1099
00:37:38,580 --> 00:37:40,699
has a different API surface area to the data,

1100
00:37:40,780 --> 00:37:42,040
but the same semantic meaning.

1101
00:37:42,360 --> 00:37:43,760
So if you ask a slightly different but

1102
00:37:43,760 --> 00:37:46,229
semantically similar question of our development portal,

1103
00:37:46,449 --> 00:37:46,820
like

1104
00:37:46,820 --> 00:37:48,689
what is a Credit Karma member's unified profile,

1105
00:37:48,889 --> 00:37:51,129
including their tax filing history and refund status,

1106
00:37:51,530 --> 00:37:53,189
you get a different answer,

1107
00:37:53,620 --> 00:37:56,050
an answer delivered in the form of an API rather than a SQL query.

1108
00:37:56,370 --> 00:37:57,629
But the developer is able to see

1109
00:37:57,919 --> 00:37:59,199
of the 70,000 attributes,

1110
00:37:59,209 --> 00:38:00,770
which are the 5 or 6,

1111
00:38:01,010 --> 00:38:01,840
or maybe there's a dozen here,

1112
00:38:01,889 --> 00:38:04,239
which are the dozen here that are the most relevant to my question,

1113
00:38:04,489 --> 00:38:07,649
and what is the graphQL query that I can then build my application around.

1114
00:38:08,310 --> 00:38:10,669
And that developer is then able to build that application

1115
00:38:11,010 --> 00:38:13,590
again using some of those application rails,

1116
00:38:13,610 --> 00:38:15,750
uh,
typical architectures that we discussed previously,

1117
00:38:15,949 --> 00:38:17,939
and now you see what an experience might look like.

1118
00:38:18,340 --> 00:38:21,040
A customer of TurboTax finishes their tax return.

1119
00:38:22,000 --> 00:38:22,620
They're then,

1120
00:38:22,719 --> 00:38:25,020
uh,
taken immediately into Credit Karma.

1121
00:38:25,229 --> 00:38:28,439
Uh,
we've been able to negotiate many of the compliance and security rules that come

1122
00:38:28,439 --> 00:38:30,040
into play when you're trying to transition

1123
00:38:30,040 --> 00:38:31,610
information between two different products like this,

1124
00:38:31,879 --> 00:38:32,659
and you're able to see

1125
00:38:32,949 --> 00:38:33,580
things like

1126
00:38:33,760 --> 00:38:35,070
federal refund and tax refund,

1127
00:38:35,139 --> 00:38:35,510
uh,

1128
00:38:35,510 --> 00:38:36,860
California state refund status

1129
00:38:37,080 --> 00:38:37,580
in the,

1130
00:38:37,639 --> 00:38:39,030
in,
in a slightly different application,

1131
00:38:39,080 --> 00:38:41,080
but have confidence that it means semantically the same thing

1132
00:38:41,080 --> 00:38:42,580
that you thought it did when you developed the idea.

1133
00:38:43,419 --> 00:38:43,820
Um,

1134
00:38:43,820 --> 00:38:46,620
so I'm really proud and excited of the work that we've done,

1135
00:38:46,699 --> 00:38:48,379
and I see some of my team here in the audience,

1136
00:38:48,659 --> 00:38:49,070
so I'm,

1137
00:38:49,070 --> 00:38:49,689
yeah,
there you go,

1138
00:38:49,689 --> 00:38:50,379
put your hands up.

1139
00:38:50,780 --> 00:38:51,179
Uh,

1140
00:38:51,179 --> 00:38:51,939
so I'm representing,

1141
00:38:52,050 --> 00:38:54,820
I'm representing the work of hundreds of people over many years,

1142
00:38:54,959 --> 00:38:56,459
um,
so I'm really proud of what we've done.

1143
00:38:56,659 --> 00:38:57,649
If you've learned anything today,

1144
00:38:57,699 --> 00:39:00,000
I hope you can take some of these ideas back to your own teams

1145
00:39:00,139 --> 00:39:01,610
and develop them for yourselves because,

1146
00:39:01,620 --> 00:39:02,120
uh.

1147
00:39:02,229 --> 00:39:03,600
Uh,
if you've learned nothing else today,

1148
00:39:03,719 --> 00:39:05,060
metadata is super important

1149
00:39:05,360 --> 00:39:06,060
when building both,

1150
00:39:06,199 --> 00:39:06,699
uh,

1151
00:39:06,879 --> 00:39:08,989
uh,
agentic experiences for your developers and

1152
00:39:08,989 --> 00:39:10,800
just building more productive development teams.

1153
00:39:11,120 --> 00:39:11,540
So with that,

1154
00:39:11,540 --> 00:39:13,399
I will finish up and hand it back to Milan.

1155
00:39:17,560 --> 00:39:18,270
Thank you very much.

1156
00:39:20,560 --> 00:39:21,330
It's pretty amazing,

1157
00:39:21,399 --> 00:39:21,899
huh?

1158
00:39:22,169 --> 00:39:23,770
What Tristan and the team have done.

1159
00:39:24,110 --> 00:39:25,320
Let's hear it F3 and Twit.

1160
00:39:25,449 --> 00:39:25,949
Woohoo.

1161
00:39:29,030 --> 00:39:31,750
It's just exciting to see what Intuit and so

1162
00:39:31,750 --> 00:39:34,709
many customers have been doing with their data strategy,

1163
00:39:34,750 --> 00:39:37,520
and this level of semantic understanding

1164
00:39:37,750 --> 00:39:39,889
is going to be increasingly important

1165
00:39:40,070 --> 00:39:42,949
as people's data lakes get bigger and bigger,

1166
00:39:43,030 --> 00:39:45,209
and AWS is with you every step

1167
00:39:45,669 --> 00:39:46,389
of the way.

1168
00:39:46,850 --> 00:39:49,489
Now,
as part of our commitment to these open architectures,

1169
00:39:49,570 --> 00:39:51,350
we have been investing heavily

1170
00:39:51,530 --> 00:39:56,010
in Iceberg support across all of our analytic services from the data layer

1171
00:39:56,330 --> 00:39:58,030
in Amazon S3 tables

1172
00:39:58,250 --> 00:40:00,320
to our analytic services like Athena,

1173
00:40:00,409 --> 00:40:02,290
EMR and Amazon Redshift.

1174
00:40:02,530 --> 00:40:06,350
And we focus a lot on performance across our services.

1175
00:40:08,669 --> 00:40:12,100
I talked a little bit about the performance that we did with the,

1176
00:40:12,110 --> 00:40:12,550
um,

1177
00:40:12,550 --> 00:40:13,179
with the spark,

1178
00:40:13,229 --> 00:40:17,040
the optimized Spark 3.5.6 engine that's now powering Glue

1179
00:40:17,310 --> 00:40:18,169
and Athena

1180
00:40:18,469 --> 00:40:18,899
and,

1181
00:40:18,899 --> 00:40:19,260
uh,

1182
00:40:19,260 --> 00:40:19,870
and EMR,

1183
00:40:19,949 --> 00:40:22,500
but we've been investing very heavily in Redshift as well.

1184
00:40:22,570 --> 00:40:23,020
In fact,

1185
00:40:23,020 --> 00:40:24,169
in this year alone,

1186
00:40:24,500 --> 00:40:27,050
Redshift has launched 37 new.

1187
00:40:27,254 --> 00:40:29,074
Features and capabilities,

1188
00:40:29,334 --> 00:40:32,854
and a couple of them are related back to Iceberg performance.

1189
00:40:33,215 --> 00:40:37,655
So we recently improved the redshift read performance for queries on Iceberg

1190
00:40:37,875 --> 00:40:39,284
by more than 2 times,

1191
00:40:39,574 --> 00:40:42,604
and we did that by supporting distributed bloom filters,

1192
00:40:42,935 --> 00:40:44,014
metadata caching,

1193
00:40:44,094 --> 00:40:45,875
and optimized query planning.

1194
00:40:46,284 --> 00:40:46,935
And

1195
00:40:47,175 --> 00:40:48,455
right here at Reinvent,

1196
00:40:48,735 --> 00:40:49,735
we are announcing.

1197
00:40:50,580 --> 00:40:55,800
The capability of Iceberg table append write support in Redshift.

1198
00:40:56,060 --> 00:41:00,219
It's a big shift for Redshift to be able to write as well as read

1199
00:41:00,500 --> 00:41:01,810
into these Iceberg tables,

1200
00:41:01,860 --> 00:41:05,560
and I know many of our Redshift customers are very excited about that.

1201
00:41:05,939 --> 00:41:08,550
Now,
this move to open formats

1202
00:41:08,820 --> 00:41:11,830
is something that one of our AWS data partners,

1203
00:41:12,020 --> 00:41:12,820
Superbase,

1204
00:41:13,179 --> 00:41:14,639
knows a lot about.

1205
00:41:15,020 --> 00:41:17,620
Let's hear from our joint customer Snapchat.

1206
00:41:20,159 --> 00:41:20,550
Hi,

1207
00:41:20,550 --> 00:41:21,439
uh,
my name is Derek.

1208
00:41:21,479 --> 00:41:23,939
I'm a product manager working on spectacles.

1209
00:41:24,120 --> 00:41:27,270
Lens developers were constrained to client-site development only,

1210
00:41:27,399 --> 00:41:28,399
and SnapCloud,

1211
00:41:28,520 --> 00:41:29,469
powered by Super Base,

1212
00:41:29,520 --> 00:41:30,300
now allows them

1213
00:41:30,719 --> 00:41:33,139
to build more sophisticated and complex lenses.

1214
00:41:33,320 --> 00:41:34,600
Over the past 5 years,

1215
00:41:34,639 --> 00:41:36,260
lenses have gone through

1216
00:41:36,479 --> 00:41:38,300
an incredible transformation from

1217
00:41:38,439 --> 00:41:39,280
their origin,

1218
00:41:39,399 --> 00:41:41,679
which was lightweight face filters,

1219
00:41:42,000 --> 00:41:43,949
to what they are on spectacles today,

1220
00:41:44,239 --> 00:41:45,520
something a lot closer to a.

1221
00:41:46,387 --> 00:41:50,117
application we were quite attracted to the developer experience of Supabase.

1222
00:41:50,197 --> 00:41:53,337
It was just amazing to see how fast you could get started.

1223
00:41:53,637 --> 00:41:55,157
We have come to appreciate what I would

1224
00:41:55,157 --> 00:41:57,907
call the unique attributes of the Postress community.

1225
00:41:58,048 --> 00:42:00,998
So when you think about the geo extensions that are possible within Postress,

1226
00:42:01,068 --> 00:42:04,387
how do you store data that is tied to physical locations and stuff like that?

1227
00:42:04,558 --> 00:42:05,827
I have to say the more we learned,

1228
00:42:05,877 --> 00:42:06,377
the,

1229
00:42:06,558 --> 00:42:08,877
um,
the more excited we got about this direction.

1230
00:42:09,318 --> 00:42:10,538
The hardest part about

1231
00:42:10,687 --> 00:42:11,187
building.

1232
00:42:11,236 --> 00:42:15,716
A platform that provides backend as a service is the the the isolation,

1233
00:42:15,875 --> 00:42:17,666
being able to support multi-tenants,

1234
00:42:17,706 --> 00:42:18,206
for example,

1235
00:42:18,395 --> 00:42:21,156
the scaling and backing up of data and all these things that you

1236
00:42:21,156 --> 00:42:24,976
need to build a resilient infrastructure that can grow with your product,

1237
00:42:25,156 --> 00:42:26,055
all that is

1238
00:42:26,275 --> 00:42:29,115
basically handled by the super-based platform.

1239
00:42:29,315 --> 00:42:30,996
One of the features that we released I found to be

1240
00:42:30,996 --> 00:42:34,186
one of the most exciting ones was uh support for WebSockets,

1241
00:42:34,236 --> 00:42:36,575
so that actually allowed developers to connect.

1242
00:42:36,874 --> 00:42:40,043
The glasses to other remote destinations in real time,

1243
00:42:40,194 --> 00:42:43,124
which unlocks a lot of really cool use cases.

1244
00:42:43,313 --> 00:42:45,313
So Base built a poster extension that makes it

1245
00:42:45,313 --> 00:42:48,394
possible to easily query data from Iceberg files.

1246
00:42:48,593 --> 00:42:50,714
Since Iceberg is an open data format,

1247
00:42:50,914 --> 00:42:54,543
we can easily take those iceberg files and use them with our existing tooling,

1248
00:42:54,704 --> 00:42:56,164
and all of that runs on AWS.

1249
00:42:56,503 --> 00:42:58,063
With a medium like the glasses,

1250
00:42:58,273 --> 00:43:00,513
you think about the type of product it is.

1251
00:43:00,624 --> 00:43:02,293
It is this is an incredibly.

1252
00:43:02,882 --> 00:43:05,912
Product that has access to camera and microphones

1253
00:43:05,912 --> 00:43:07,552
able to sense the world around you.

1254
00:43:07,751 --> 00:43:08,652
The camera is

1255
00:43:08,912 --> 00:43:11,352
the most sort of like fundamental building block of

1256
00:43:11,352 --> 00:43:12,941
an AI experience if you think about it.

1257
00:43:13,072 --> 00:43:13,572
Having

1258
00:43:13,791 --> 00:43:14,291
that

1259
00:43:14,592 --> 00:43:16,572
infrastructure isolated in a

1260
00:43:16,791 --> 00:43:20,981
in an AWS instance that provides you all the flexibility was very,

1261
00:43:21,162 --> 00:43:21,951
very critical.

1262
00:43:22,112 --> 00:43:24,592
We are building the future of connected AR together

1263
00:43:24,592 --> 00:43:28,072
using AWS scale to bay speed and creativity.

1264
00:43:29,489 --> 00:43:30,090
It's pretty awesome,

1265
00:43:30,159 --> 00:43:30,659
huh?

1266
00:43:31,840 --> 00:43:32,860
Now to tell you more

1267
00:43:33,280 --> 00:43:34,439
about Superbase

1268
00:43:34,639 --> 00:43:36,659
and what Supabase is doing

1269
00:43:37,239 --> 00:43:39,379
with AWS analytics

1270
00:43:39,520 --> 00:43:41,439
and Iceberg and open formats,

1271
00:43:41,679 --> 00:43:43,600
let's welcome Paul Copplestone,

1272
00:43:43,800 --> 00:43:46,300
the CEO of Superbase and the co-founder

1273
00:43:46,520 --> 00:43:47,449
to the stage.

1274
00:43:47,879 --> 00:43:48,399
Coopel.

1275
00:43:50,209 --> 00:43:50,729
Awesome.

1276
00:43:52,260 --> 00:43:52,760
Thank you.

1277
00:43:57,889 --> 00:43:58,639
Thanks,
Malan.

1278
00:44:02,179 --> 00:44:03,899
One of the most critical decisions that

1279
00:44:03,899 --> 00:44:06,280
any business can make when they're getting started

1280
00:44:06,620 --> 00:44:08,360
is choosing the right database.

1281
00:44:09,209 --> 00:44:11,060
For companies that choose the wrong database,

1282
00:44:11,209 --> 00:44:12,209
one that can't scale,

1283
00:44:12,330 --> 00:44:14,659
they face a pretty daunting exercise.

1284
00:44:15,500 --> 00:44:17,340
Migrating to a new database,

1285
00:44:17,939 --> 00:44:20,850
and this always happens at the worst possible time

1286
00:44:21,100 --> 00:44:23,000
when their business is scaling fast.

1287
00:44:24,659 --> 00:44:27,280
So choosing a database isn't about what you need today.

1288
00:44:27,540 --> 00:44:29,219
It's a very forward-thinking decision.

1289
00:44:29,260 --> 00:44:30,840
You have to think about what you might need

1290
00:44:31,020 --> 00:44:31,949
23,

1291
00:44:32,179 --> 00:44:33,780
or even 5 years down the line.

1292
00:44:36,760 --> 00:44:40,469
A lot of companies recognize how high stakes this decision is,

1293
00:44:40,479 --> 00:44:42,560
and they turn to Superbase to help them get it right.

1294
00:44:43,929 --> 00:44:44,570
I'm Paul,

1295
00:44:44,699 --> 00:44:46,010
the CEO of Superbase.

1296
00:44:46,209 --> 00:44:47,300
We are

1297
00:44:47,409 --> 00:44:49,550
the world's fastest growing post-growth company.

1298
00:44:50,540 --> 00:44:54,840
And we've launched over 10 million databases for all different types of companies,

1299
00:44:55,500 --> 00:44:57,320
including AI platforms like

1300
00:44:57,600 --> 00:44:58,179
Lovable,

1301
00:44:58,340 --> 00:44:59,699
v0 by Versal,

1302
00:44:59,939 --> 00:45:01,020
and Sigma Make.

1303
00:45:02,209 --> 00:45:04,610
They choose us because we're the most developer

1304
00:45:04,610 --> 00:45:07,209
friendly way to get a production grade database,

1305
00:45:07,610 --> 00:45:09,750
and we've built everything on AWS.

1306
00:45:11,790 --> 00:45:14,949
After working with over 5 million developers,

1307
00:45:15,270 --> 00:45:18,510
I've come to see that there are several common data challenges.

1308
00:45:19,219 --> 00:45:21,719
The most frequent is choosing which

1309
00:45:22,379 --> 00:45:24,860
technology to pair with their Postgress database.

1310
00:45:26,570 --> 00:45:29,909
Almost every developer today chooses Postgress when they're getting started.

1311
00:45:31,090 --> 00:45:35,370
It's the world's most popular database because it's both versatile and robust,

1312
00:45:35,610 --> 00:45:38,250
but the versatility also has a downside.

1313
00:45:39,340 --> 00:45:43,830
Developers often fall into something that I call the convenience trap.

1314
00:45:44,870 --> 00:45:47,750
They start storing analytical data inside their posts growth

1315
00:45:47,750 --> 00:45:50,370
database because it's the one that they already have.

1316
00:45:51,149 --> 00:45:53,030
And this actually works pretty well for a while.

1317
00:45:53,350 --> 00:45:54,729
But as we all know,

1318
00:45:55,570 --> 00:46:00,709
Analytical data grows very fast and Postgress isn't designed for analytical data.

1319
00:46:01,330 --> 00:46:03,050
I'm sure many of you have faced this situation,

1320
00:46:03,129 --> 00:46:04,370
so you know how to solve it.

1321
00:46:05,080 --> 00:46:06,030
We introduced

1322
00:46:06,159 --> 00:46:06,659
an

1323
00:46:06,750 --> 00:46:08,010
analytics engine.

1324
00:46:09,030 --> 00:46:11,830
And this is absolutely the right thing to do,

1325
00:46:12,149 --> 00:46:15,120
but it also introduces another set of challenges.

1326
00:46:15,429 --> 00:46:17,239
We now have two sets of everything,

1327
00:46:17,790 --> 00:46:18,830
2 schemas,

1328
00:46:19,610 --> 00:46:19,989
Uh,

1329
00:46:19,989 --> 00:46:21,290
2 sets of data,

1330
00:46:21,409 --> 00:46:22,129
2 libraries,

1331
00:46:22,250 --> 00:46:25,860
and often 2 sets of expertise to manage the different databases.

1332
00:46:27,100 --> 00:46:27,989
Even harder,

1333
00:46:28,419 --> 00:46:31,459
the data is now fragmented between two different databases,

1334
00:46:31,699 --> 00:46:35,889
so getting critical business insights can become a bit of a nuisance.

1335
00:46:36,300 --> 00:46:38,500
So how do we all solve this in the industry?

1336
00:46:39,310 --> 00:46:40,310
A third database,

1337
00:46:40,429 --> 00:46:40,929
of course,

1338
00:46:40,989 --> 00:46:44,090
and this time in the form of a data warehouse.

1339
00:46:44,830 --> 00:46:45,330
Then

1340
00:46:45,629 --> 00:46:47,350
as the data starts to pile up,

1341
00:46:47,550 --> 00:46:49,979
we have our final challenge which is costs.

1342
00:46:50,350 --> 00:46:51,500
So to solve costs,

1343
00:46:51,709 --> 00:46:53,000
we use S3,

1344
00:46:53,159 --> 00:46:53,889
which is

1345
00:46:54,790 --> 00:46:55,860
Good for long term

1346
00:46:56,360 --> 00:46:58,939
and uh affordable data storage.

1347
00:47:00,959 --> 00:47:03,899
The craziest thing about this architecture is that

1348
00:47:04,320 --> 00:47:08,320
every one of these services has their own proprietary data format.

1349
00:47:08,800 --> 00:47:12,159
So the data ecosystem has redefined redundancy.

1350
00:47:15,360 --> 00:47:16,199
At Superbase,

1351
00:47:16,280 --> 00:47:17,879
we don't like complexity,

1352
00:47:18,040 --> 00:47:21,860
so we set out to reimagine the best data architecture

1353
00:47:22,080 --> 00:47:23,699
using estuary tables.

1354
00:47:24,360 --> 00:47:29,020
Estuary Tables stores data in a format called Apache Iceberg.

1355
00:47:29,699 --> 00:47:32,790
Which is an industry-standard OpenTable format

1356
00:47:33,070 --> 00:47:36,050
specifically for analytics and data warehouses.

1357
00:47:37,520 --> 00:47:40,860
S3 tables will become our unified storage layer

1358
00:47:41,159 --> 00:47:41,719
for data.

1359
00:47:43,449 --> 00:47:44,239
To build

1360
00:47:44,350 --> 00:47:48,239
applications,
we still need a database that can provide low latency queries

1361
00:47:48,399 --> 00:47:49,189
and no surprises,

1362
00:47:49,280 --> 00:47:50,379
that's Postgras,

1363
00:47:51,159 --> 00:47:53,860
which delivers millisecond-level response times

1364
00:47:54,120 --> 00:47:55,080
and is scalable,

1365
00:47:55,090 --> 00:47:57,580
powering some of the world's largest applications.

1366
00:48:01,679 --> 00:48:04,310
Superbass keeps these perfectly in sync.

1367
00:48:05,149 --> 00:48:07,189
When you create a table in Postgress,

1368
00:48:07,469 --> 00:48:09,550
it's automatically created in an iceberg.

1369
00:48:09,870 --> 00:48:11,590
When you insert data into Postgress,

1370
00:48:11,709 --> 00:48:12,850
it appears an iceberg.

1371
00:48:13,110 --> 00:48:14,939
And when you no longer need the data,

1372
00:48:15,110 --> 00:48:17,010
you simply delete it out of Postgress,

1373
00:48:17,229 --> 00:48:19,070
and it's already stored in S3,

1374
00:48:19,149 --> 00:48:19,669
which is

1375
00:48:19,919 --> 00:48:21,989
incredibly low cost and resilient.

1376
00:48:23,870 --> 00:48:25,479
Estuary is not only low cost,

1377
00:48:25,919 --> 00:48:27,659
it's extremely accessible,

1378
00:48:27,959 --> 00:48:29,540
and Apache iceberg

1379
00:48:29,870 --> 00:48:31,639
increases this accessibility.

1380
00:48:33,570 --> 00:48:37,449
Every major data warehouse already works with Iceberg

1381
00:48:38,110 --> 00:48:41,429
and all major analytical engines also work with Iceberg.

1382
00:48:42,159 --> 00:48:44,540
It's become the universal language

1383
00:48:44,719 --> 00:48:46,959
for analytics and data warehouses.

1384
00:48:49,810 --> 00:48:51,360
For any CIOs in the audience,

1385
00:48:51,479 --> 00:48:53,419
this is important for two key reasons.

1386
00:48:54,120 --> 00:48:54,560
One,

1387
00:48:54,560 --> 00:48:58,320
it gives freedom of choice to your developers and data engineers.

1388
00:48:59,060 --> 00:49:00,600
They can choose their favorite data

1389
00:49:01,139 --> 00:49:03,949
analytics engine to work with the with their data.

1390
00:49:05,570 --> 00:49:06,250
And 2,

1391
00:49:06,330 --> 00:49:09,110
it reduces costs because there's no more waste.

1392
00:49:10,090 --> 00:49:13,870
Analytical data is stored once inside S3 tables,

1393
00:49:14,530 --> 00:49:16,689
and instead of moving data to every warehouse,

1394
00:49:16,729 --> 00:49:20,489
you can simply connect the engine and query directly from S3.

1395
00:49:24,100 --> 00:49:25,560
This is an emerging pattern

1396
00:49:25,899 --> 00:49:27,620
that we're seeing across the industry and

1397
00:49:27,620 --> 00:49:30,510
we call it the open warehouse architecture.

1398
00:49:30,860 --> 00:49:32,040
We believe it's the most

1399
00:49:32,169 --> 00:49:35,020
efficient and cost-effective way to manage your data.

1400
00:49:37,780 --> 00:49:40,479
We're launching this today on the Super Base platform,

1401
00:49:41,100 --> 00:49:42,280
and we're not stopping there.

1402
00:49:42,689 --> 00:49:46,639
We think data warehousing needs to go beyond traditional analytics.

1403
00:49:50,229 --> 00:49:51,110
Today at Reinvent,

1404
00:49:51,270 --> 00:49:53,860
AWS are announcing S3 vectors,

1405
00:49:53,870 --> 00:49:56,179
which adds support for embeddings.

1406
00:49:56,590 --> 00:50:00,870
We can use embeddings to build AI native features for our applications,

1407
00:50:01,110 --> 00:50:04,250
but they also allow us to extract deeper insights

1408
00:50:04,580 --> 00:50:06,030
from unstructured data.

1409
00:50:07,899 --> 00:50:08,719
From today,

1410
00:50:09,100 --> 00:50:13,219
you can use vector uh vector buckets on the superbase platform,

1411
00:50:13,459 --> 00:50:17,439
and we plan to make them an integral piece of the open warehouse architecture.

1412
00:50:20,989 --> 00:50:21,699
The old

1413
00:50:22,040 --> 00:50:24,060
architecture is expensive.

1414
00:50:24,560 --> 00:50:25,760
It actually goes beyond that,

1415
00:50:25,840 --> 00:50:27,320
it's extremely siloed,

1416
00:50:27,439 --> 00:50:29,399
every engine is isolated.

1417
00:50:30,100 --> 00:50:32,570
Apache iceberg is moving the data world from.

1418
00:50:34,209 --> 00:50:34,909
Siloed

1419
00:50:35,209 --> 00:50:35,850
to stinct.

1420
00:50:37,520 --> 00:50:39,709
With the open warehouse architecture,

1421
00:50:40,159 --> 00:50:42,379
everything is connected and there's no more waste.

1422
00:50:45,689 --> 00:50:48,129
I believe that the future of data is open.

1423
00:50:49,090 --> 00:50:50,530
Postgress is open source

1424
00:50:50,729 --> 00:50:52,870
and it's the world's most popular database.

1425
00:50:53,560 --> 00:50:55,179
Apache Iceberg

1426
00:50:55,399 --> 00:50:59,959
is an open format and has become the universal language for analytics.

1427
00:51:00,520 --> 00:51:04,310
S3 has become the fundamental substrate for data and AI.

1428
00:51:07,370 --> 00:51:08,489
Within 6 years,

1429
00:51:08,860 --> 00:51:12,479
Superbase has become the most popular platform for postgrads,

1430
00:51:12,659 --> 00:51:15,800
and today we're starting our next journey with Iceberg,

1431
00:51:16,340 --> 00:51:21,840
again on AWS bringing the same developer friendliness to this new open format.

1432
00:51:22,179 --> 00:51:22,679
Thank you.

1433
00:51:28,239 --> 00:51:28,739
Hey,

1434
00:51:29,959 --> 00:51:30,780
thank you.

1435
00:51:31,719 --> 00:51:32,219
Uh,

1436
00:51:32,560 --> 00:51:34,030
Superbase is pretty amazing.

1437
00:51:34,409 --> 00:51:34,909
Um,

1438
00:51:35,010 --> 00:51:38,270
you know,
I've enjoyed the many years that I've been working with them,

1439
00:51:38,320 --> 00:51:40,550
and they have always been on the frontier of data,

1440
00:51:40,770 --> 00:51:43,689
and it's exciting to see what they're doing with both iceberg

1441
00:51:43,929 --> 00:51:44,850
and vectors,

1442
00:51:44,929 --> 00:51:47,449
and that is in fact where the world is going.

1443
00:51:48,540 --> 00:51:49,479
Vectors

1444
00:51:50,469 --> 00:51:52,439
Gives semantic meaning to your data.

1445
00:51:52,949 --> 00:51:59,030
And it means that you can easily search and use your data no matter how much you have.

1446
00:51:59,429 --> 00:52:02,489
Now,
what I am excited about for vectors

1447
00:52:02,790 --> 00:52:04,530
is that vectors are

1448
00:52:04,870 --> 00:52:05,810
the tool

1449
00:52:06,149 --> 00:52:13,010
that let you understand your data without having to understand what is in your data.

1450
00:52:13,590 --> 00:52:18,850
And it is a tool that we can use now because of the power of these AI embedding models.

1451
00:52:21,100 --> 00:52:21,790
And it is,

1452
00:52:21,830 --> 00:52:22,469
in fact,

1453
00:52:22,590 --> 00:52:26,689
why vector is emerging as a building block for both data,

1454
00:52:26,949 --> 00:52:28,770
data semantic understanding

1455
00:52:28,989 --> 00:52:30,209
and AI

1456
00:52:30,750 --> 00:52:31,489
in the form

1457
00:52:31,790 --> 00:52:34,070
of extended agent memory.

1458
00:52:34,389 --> 00:52:38,129
Vectors are emerging across both of those domains,

1459
00:52:38,139 --> 00:52:40,590
and it's equally useful in each.

1460
00:52:41,939 --> 00:52:44,020
Now,
a year or so ago in S3,

1461
00:52:44,100 --> 00:52:48,129
we saw that vectors were becoming one of our fastest growing storage types,

1462
00:52:48,459 --> 00:52:52,300
and so we launched our S3 vector preview this past summer.

1463
00:52:53,239 --> 00:52:55,239
And in only a few

1464
00:52:55,719 --> 00:52:57,510
months of preview availability,

1465
00:52:57,800 --> 00:52:59,850
customers have run over 1

1466
00:53:00,159 --> 00:53:01,659
billion queries.

1467
00:53:01,959 --> 00:53:04,280
Now,
I have worked on S3 for a minute,

1468
00:53:04,600 --> 00:53:07,600
and I have to say this type of super rapid

1469
00:53:07,760 --> 00:53:08,419
adoption

1470
00:53:08,959 --> 00:53:10,889
of a new feature in preview is

1471
00:53:11,280 --> 00:53:12,639
incredibly fast,

1472
00:53:12,879 --> 00:53:14,040
even for F3.

1473
00:53:14,669 --> 00:53:16,969
And so here's what we're doing with vectors.

1474
00:53:17,520 --> 00:53:19,969
Just like we introduced S3.

1475
00:53:21,120 --> 00:53:26,560
At a cost in economics that made it possible to add any type of data into a data lake,

1476
00:53:26,889 --> 00:53:28,810
we're doing the same thing with vectors.

1477
00:53:29,090 --> 00:53:31,909
We're making vectors cost effective.

1478
00:53:32,169 --> 00:53:33,320
And when we do that,

1479
00:53:33,409 --> 00:53:34,870
we are going to find

1480
00:53:35,250 --> 00:53:37,610
that customers of data and AI,

1481
00:53:37,860 --> 00:53:39,399
the application builders,

1482
00:53:39,850 --> 00:53:43,870
they're going to use vectors in every aspect of their application.

1483
00:53:44,040 --> 00:53:47,189
We're going to do with vectors what we did with data and data lakes.

1484
00:53:47,810 --> 00:53:51,449
And customers can now treat this new data set of AI and,

1485
00:53:51,469 --> 00:53:52,729
and data understanding,

1486
00:53:52,770 --> 00:53:54,080
semantic understanding,

1487
00:53:54,489 --> 00:53:57,439
like any other data in their S3 data lake.

1488
00:53:57,520 --> 00:53:58,850
They don't have to worry

1489
00:53:59,489 --> 00:54:00,510
about security

1490
00:54:00,729 --> 00:54:01,790
and durability

1491
00:54:01,929 --> 00:54:03,070
and availability,

1492
00:54:03,250 --> 00:54:06,550
and you can let your vector data set grow

1493
00:54:07,010 --> 00:54:10,399
at S3 scale because of the cost and economics

1494
00:54:10,770 --> 00:54:12,050
of vectors in S3.

1495
00:54:12,520 --> 00:54:13,020
Now,

1496
00:54:13,189 --> 00:54:16,520
vector APIs are just part of S3 now,

1497
00:54:16,719 --> 00:54:18,389
and what that means is that

1498
00:54:18,600 --> 00:54:19,399
it's simple.

1499
00:54:20,310 --> 00:54:21,500
To use vectors.

1500
00:54:21,909 --> 00:54:24,070
And like objects and tables in S3,

1501
00:54:24,149 --> 00:54:28,770
S3 vectors are usable at even the smallest scale indexes

1502
00:54:29,030 --> 00:54:30,010
all the way up

1503
00:54:30,229 --> 00:54:31,350
to billions of vectors,

1504
00:54:31,469 --> 00:54:34,250
meaning that you can focus on building your applications

1505
00:54:34,459 --> 00:54:37,510
and not thinking about how to size and configure

1506
00:54:37,709 --> 00:54:38,899
a vector database.

1507
00:54:40,310 --> 00:54:41,050
And this morning,

1508
00:54:41,300 --> 00:54:44,169
Matt Garman announced the GA of S3 vectors,

1509
00:54:44,179 --> 00:54:46,649
and with it some pretty impressive changes

1510
00:54:46,830 --> 00:54:48,139
since our July preview.

1511
00:54:49,199 --> 00:54:51,840
We've increased the max capacity.

1512
00:54:52,560 --> 00:54:54,739
A vector stored per index

1513
00:54:54,959 --> 00:54:56,060
to 2 billion.

1514
00:54:56,399 --> 00:54:59,020
That is 40 times the preview capability.

1515
00:54:59,439 --> 00:55:00,159
That's a lot.

1516
00:55:00,840 --> 00:55:03,350
You get 10,000 indexes per bucket,

1517
00:55:03,399 --> 00:55:08,699
which means that 11 vector bucket can store up to 20 trillion vectors.

1518
00:55:09,040 --> 00:55:10,000
Now,
as you all know,

1519
00:55:10,040 --> 00:55:11,199
as S3 users,

1520
00:55:11,439 --> 00:55:13,020
S3 is very good

1521
00:55:13,360 --> 00:55:14,699
at massive throughput,

1522
00:55:14,879 --> 00:55:16,820
and that's what you get with vectors too.

1523
00:55:17,399 --> 00:55:21,550
1000 vectors per second when you stream a single vector,

1524
00:55:21,800 --> 00:55:23,280
and all of this

1525
00:55:23,679 --> 00:55:26,419
at 100 milliseconds or less

1526
00:55:26,550 --> 00:55:28,639
latency for warm queries.

1527
00:55:29,370 --> 00:55:30,969
It is a game changer

1528
00:55:31,090 --> 00:55:34,629
and it is going to fundamentally change how people think

1529
00:55:35,129 --> 00:55:37,669
about semantic understanding of their data

1530
00:55:38,010 --> 00:55:40,649
and really other things like agent memory.

1531
00:55:43,139 --> 00:55:45,350
We're going to see a ton of variety

1532
00:55:45,780 --> 00:55:48,610
of how customers take advantage of our new vector storage,

1533
00:55:48,659 --> 00:55:51,520
and we're already seeing that diversity in use cases.

1534
00:55:52,790 --> 00:55:55,280
If you think about the use cases that we've seen,

1535
00:55:55,600 --> 00:55:58,000
it's been a combination of two things.

1536
00:55:58,080 --> 00:55:58,620
It's been,

1537
00:55:58,750 --> 00:55:59,250
um,

1538
00:55:59,280 --> 00:56:01,459
being able to get a semantic understanding

1539
00:56:01,679 --> 00:56:02,659
of metadata

1540
00:56:03,320 --> 00:56:05,300
for a semantic understanding of media.

1541
00:56:05,830 --> 00:56:07,620
But it's also being able

1542
00:56:08,000 --> 00:56:10,389
to extend agent memory.

1543
00:56:10,639 --> 00:56:11,000
OK?

1544
00:56:11,000 --> 00:56:13,030
So if you think about extending agent memory,

1545
00:56:13,280 --> 00:56:17,070
the way that you extend agent memory is that you add more context and vectors,

1546
00:56:17,280 --> 00:56:17,780
right?

1547
00:56:18,080 --> 00:56:19,739
And if you add more context and vectors,

1548
00:56:19,870 --> 00:56:20,239
boy,

1549
00:56:20,239 --> 00:56:20,639
the,
the,

1550
00:56:20,639 --> 00:56:22,780
the smarter those chatbots get

1551
00:56:22,959 --> 00:56:24,469
and the more human they get,

1552
00:56:24,840 --> 00:56:27,540
because you're able to vectorize context

1553
00:56:27,840 --> 00:56:28,340
about

1554
00:56:28,629 --> 00:56:31,070
A human that is interacting with a chatbot.

1555
00:56:31,110 --> 00:56:33,250
You're able to vectorize context

1556
00:56:33,510 --> 00:56:34,820
about the agent inform,

1557
00:56:34,909 --> 00:56:35,320
uh,
sorry,

1558
00:56:35,320 --> 00:56:36,689
the account information

1559
00:56:36,870 --> 00:56:39,290
of a human that the chatbot is interacting with,

1560
00:56:39,469 --> 00:56:42,530
and all of that is going into vector storage

1561
00:56:42,790 --> 00:56:45,449
and being used to add and create

1562
00:56:45,590 --> 00:56:47,590
and personalize the responses to it.

1563
00:56:47,669 --> 00:56:49,889
You can see where that can really take off

1564
00:56:50,070 --> 00:56:52,909
when you can have virtually unlimited vector storage.

1565
00:56:53,770 --> 00:56:55,860
One of the fastest growing new types

1566
00:56:56,050 --> 00:56:58,070
is also hybrid search,

1567
00:56:58,409 --> 00:57:01,570
and vector storage in S3 has integration

1568
00:57:01,570 --> 00:57:04,570
with both Bedrock knowledge bases and OpenSearch.

1569
00:57:05,010 --> 00:57:07,669
Hybrid search has really taken off.

1570
00:57:08,419 --> 00:57:09,360
With OpenSearch.

1571
00:57:11,229 --> 00:57:15,129
Now this year we focused on open search on a lot of AI capabilities,

1572
00:57:15,350 --> 00:57:17,540
such as agentic search that

1573
00:57:18,270 --> 00:57:19,729
that automatically

1574
00:57:20,350 --> 00:57:23,050
converts natural language to precise queries,

1575
00:57:23,270 --> 00:57:24,610
and we've also added

1576
00:57:24,949 --> 00:57:28,100
automatic semantic enrichment and as of this year,

1577
00:57:28,459 --> 00:57:33,149
GPU acceleration and auto optimization for uh vector indexing.

1578
00:57:38,929 --> 00:57:40,370
With GPU acceleration,

1579
00:57:40,449 --> 00:57:42,270
you can now build these billion

1580
00:57:42,449 --> 00:57:44,129
scale vector databases,

1581
00:57:44,199 --> 00:57:45,530
but you can do it much faster.

1582
00:57:45,610 --> 00:57:46,870
You can do it under 1 hour,

1583
00:57:47,370 --> 00:57:50,310
and index vectors up to 10 times faster

1584
00:57:50,530 --> 00:57:52,370
at 25% of the cost.

1585
00:57:53,649 --> 00:57:55,649
It's going to be super helpful for customers

1586
00:57:55,649 --> 00:57:58,929
that are building multi-billion scale vector indices.

1587
00:57:59,010 --> 00:58:00,050
And if you think about it,

1588
00:58:00,330 --> 00:58:01,850
GPU acceleration

1589
00:58:02,060 --> 00:58:03,689
is just going to be a game changer

1590
00:58:03,689 --> 00:58:06,750
for dynamic AI applications that have heavy rights.

1591
00:58:09,570 --> 00:58:11,969
And because open search serverless gives you

1592
00:58:11,969 --> 00:58:15,169
a GP GPU acceleration that's completely serverless,

1593
00:58:15,209 --> 00:58:16,429
you don't have to manage

1594
00:58:16,810 --> 00:58:19,469
any GPU instances and you only pay

1595
00:58:19,889 --> 00:58:21,020
for what you use.

1596
00:58:22,000 --> 00:58:24,040
And so when the right volume drops,

1597
00:58:24,159 --> 00:58:26,780
the GPUs will automatically scale down

1598
00:58:26,959 --> 00:58:28,469
and they'll return to the pool.

1599
00:58:29,070 --> 00:58:31,429
That means you're only paying for acceleration time,

1600
00:58:31,840 --> 00:58:33,600
which is monitored through cloud watch.

1601
00:58:33,879 --> 00:58:34,919
It's fully managed,

1602
00:58:34,989 --> 00:58:36,239
it's fully automated,

1603
00:58:36,260 --> 00:58:38,020
and it's paid as you use.

1604
00:58:39,590 --> 00:58:40,030
Now,

1605
00:58:40,030 --> 00:58:43,949
we also worked on how we could help simplify how you find

1606
00:58:44,149 --> 00:58:47,250
the optimal configuration for your specific use case

1607
00:58:47,469 --> 00:58:50,489
so you can balance between cost and performance.

1608
00:58:51,649 --> 00:58:54,479
The difference between an optimal configuration

1609
00:58:54,919 --> 00:58:57,770
for a vector database and one that just works

1610
00:58:58,090 --> 00:58:59,550
can be 10%

1611
00:58:59,689 --> 00:59:00,570
in search quality,

1612
00:59:00,649 --> 00:59:02,510
it can be hundreds of milliseconds

1613
00:59:02,689 --> 00:59:03,330
in latency,

1614
00:59:03,370 --> 00:59:05,510
or it can actually be 3 times the cost.

1615
00:59:05,889 --> 00:59:07,760
So getting the right configuration,

1616
00:59:07,889 --> 00:59:09,090
it just matters a lot.

1617
00:59:10,459 --> 00:59:12,500
And so we build auto optimization.

1618
00:59:12,620 --> 00:59:13,239
You simply

1619
00:59:14,100 --> 00:59:16,139
specify what you think your acceptable

1620
00:59:16,139 --> 00:59:18,330
search latency and quality requirements are,

1621
00:59:18,659 --> 00:59:22,300
and there's no expertise in tuning algorithms or quantization

1622
00:59:22,540 --> 00:59:23,500
that's required.

1623
00:59:24,830 --> 00:59:28,020
It's also integrated with vector ingestion pipelines,

1624
00:59:28,310 --> 00:59:32,899
so you can build optimized indexes directly from Amazon S3 data sources.

1625
00:59:33,139 --> 00:59:33,629
And again,

1626
00:59:33,629 --> 00:59:34,969
you're paying a flat

1627
00:59:35,189 --> 00:59:36,330
rate per job,

1628
00:59:36,350 --> 00:59:37,310
and it's simple,

1629
00:59:37,409 --> 00:59:38,229
it's fast,

1630
00:59:38,260 --> 00:59:39,590
and it's fully automated.

1631
00:59:40,820 --> 00:59:41,510
As you can see,

1632
00:59:41,590 --> 00:59:44,850
this world of vector is evolving rapidly as a building block,

1633
00:59:45,030 --> 00:59:46,850
and with AWS vectors

1634
00:59:46,989 --> 00:59:48,469
and open search combined,

1635
00:59:48,709 --> 00:59:50,550
we're here to help you steer into

1636
00:59:50,550 --> 00:59:53,790
this rapidly emerging trend for semantic understanding.

1637
00:59:53,909 --> 00:59:55,000
I believe

1638
00:59:55,310 --> 00:59:57,790
that in the upcoming 12 to 18 months,

1639
00:59:58,389 --> 01:00:00,389
anybody who is going to work with data

1640
01:00:00,590 --> 01:00:03,830
is going to want to build a semantic layer on top.

1641
01:00:03,919 --> 01:00:06,459
Of their data set so they can understand

1642
01:00:06,760 --> 01:00:07,560
what's in their data.

1643
01:00:07,639 --> 01:00:09,939
And I think this combination of vectors

1644
01:00:10,280 --> 01:00:11,949
and AI embedding models,

1645
01:00:12,159 --> 01:00:16,020
and the vector databases like OpenSearch to make it very,

1646
01:00:16,030 --> 01:00:17,320
very fast for your high,

1647
01:00:17,639 --> 01:00:18,510
uh,
your high scale,

1648
01:00:18,560 --> 01:00:19,159
high fast,

1649
01:00:19,169 --> 01:00:20,080
uh,
workloads

1650
01:00:20,399 --> 01:00:21,739
are going to be critical

1651
01:00:22,120 --> 01:00:25,260
for you to steer into this rapidly emerging trend

1652
01:00:25,479 --> 01:00:27,060
of semantic understanding.

1653
01:00:28,949 --> 01:00:30,790
It's a super interesting time,

1654
01:00:30,949 --> 01:00:35,149
I feel to be here working at this intersection of data and AI

1655
01:00:35,830 --> 01:00:38,699
and here at AWS we are deeply committed to

1656
01:00:38,699 --> 01:00:41,350
helping you build not just what you need today,

1657
01:00:41,790 --> 01:00:47,290
but a data foundation that you can evolve easily into all of these emerging trends.

1658
01:00:48,830 --> 01:00:52,600
Speaking on behalf of all of our teams in data and analytics,

1659
01:00:52,679 --> 01:00:54,310
I want to say a big thank you

1660
01:00:54,629 --> 01:00:56,129
to everyone in this room

1661
01:00:56,439 --> 01:00:58,459
who may who uses our services

1662
01:00:58,590 --> 01:01:01,000
to make data the backbone of your business.

1663
01:01:01,310 --> 01:01:01,889
We

1664
01:01:02,120 --> 01:01:03,060
are inspired

1665
01:01:03,239 --> 01:01:06,189
by what you do and your commitment to your customers,

1666
01:01:06,399 --> 01:01:08,280
and we're here to build what you need

1667
01:01:08,600 --> 01:01:10,100
now and in the future.

1668
01:01:10,479 --> 01:01:10,979
Thank you,

1669
01:01:11,080 --> 01:01:11,399
everybody,

1670
01:01:11,399 --> 01:01:12,639
and have a great reinvent.

