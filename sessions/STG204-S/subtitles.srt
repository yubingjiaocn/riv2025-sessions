1
00:00:00,140 --> 00:00:01,169
OK, cool.

2
00:00:04,000 --> 00:00:05,379
Um, let's get started.

3
00:00:06,669 --> 00:00:07,908
So H2AI,

4
00:00:08,429 --> 00:00:10,710
um, HOAI basically is a global

5
00:00:10,710 --> 00:00:13,050
leader in the enterprise AI realm.

6
00:00:13,550 --> 00:00:14,890
H2AI does both

7
00:00:15,220 --> 00:00:17,339
generative and and predictive

8
00:00:17,339 --> 00:00:19,620
AI. And the

9
00:00:20,649 --> 00:00:22,690
Have an entire platform which

10
00:00:22,690 --> 00:00:25,039
we are able to install in on-prem environments,

11
00:00:25,048 --> 00:00:27,079
in clouds, and in air gapped

12
00:00:27,079 --> 00:00:27,609
environments.

13
00:00:28,010 --> 00:00:30,068
H2AI is a leader

14
00:00:30,068 --> 00:00:30,629
in

15
00:00:30,969 --> 00:00:31,568
um

16
00:00:31,870 --> 00:00:33,000
the Gaia benchmark,

17
00:00:33,289 --> 00:00:34,020
and we deliver

18
00:00:34,329 --> 00:00:35,469
an entire platform.

19
00:00:37,069 --> 00:00:39,689
Um, So

20
00:00:39,689 --> 00:00:40,880
our technology stack,

21
00:00:41,219 --> 00:00:43,450
basically our entire platform runs entirely

22
00:00:43,450 --> 00:00:44,630
on Kubernetes.

23
00:00:45,209 --> 00:00:47,569
Everything runs in EKS in the cloud,

24
00:00:48,009 --> 00:00:50,289
and we rely very heavily on

25
00:00:50,289 --> 00:00:51,590
EBS storage.

26
00:00:51,889 --> 00:00:53,929
The reason we rely on EBS storage is when

27
00:00:53,929 --> 00:00:54,789
we train data

28
00:00:55,289 --> 00:00:57,569
and we we train our models, we need very

29
00:00:57,569 --> 00:00:58,490
fast and

30
00:00:58,810 --> 00:01:00,848
um good storage for our AI

31
00:01:00,848 --> 00:01:04,709
engines. The

32
00:01:04,709 --> 00:01:06,838
problem that we had with H2AI with

33
00:01:06,838 --> 00:01:07,939
EBS storage

34
00:01:08,359 --> 00:01:09,980
is that we had a lot of

35
00:01:10,239 --> 00:01:11,579
unutilized storage.

36
00:01:12,500 --> 00:01:14,888
Basically we were storing over 2 petabytes

37
00:01:14,888 --> 00:01:16,900
of storage and it was growing very

38
00:01:16,900 --> 00:01:17,439
quickly

39
00:01:18,109 --> 00:01:20,260
and the issue is that we couldn't scale very good.

40
00:01:20,540 --> 00:01:21,638
We couldn't scale down,

41
00:01:21,900 --> 00:01:24,480
we couldn't be more efficient with our cloud storage,

42
00:01:25,019 --> 00:01:25,558
and

43
00:01:26,099 --> 00:01:28,120
we ended up wasting a lot

44
00:01:28,120 --> 00:01:30,079
of storage in the cloud.

45
00:01:31,000 --> 00:01:32,659
We looked at this

46
00:01:33,000 --> 00:01:34,219
issue and we tried to find

47
00:01:34,719 --> 00:01:35,980
several solutions.

48
00:01:36,989 --> 00:01:38,888
Most of the solutions that we found out there

49
00:01:39,329 --> 00:01:41,989
were solutions that required us to migrate

50
00:01:41,989 --> 00:01:42,629
data

51
00:01:43,180 --> 00:01:45,269
from old EBS storages to

52
00:01:45,269 --> 00:01:46,569
new EBS volumes,

53
00:01:47,150 --> 00:01:49,349
so basically it was a very hard and

54
00:01:49,349 --> 00:01:50,088
painful

55
00:01:50,469 --> 00:01:51,730
um process

56
00:01:52,430 --> 00:01:54,489
and um that was the

57
00:01:54,989 --> 00:01:57,230
issue that we had previously before we started

58
00:01:57,230 --> 00:01:58,219
working with Dattafi.

59
00:02:00,709 --> 00:02:01,459
Thanks Ophira.

60
00:02:02,150 --> 00:02:04,269
So when we met H2O initially, we were

61
00:02:04,269 --> 00:02:06,349
very pleasantly surprised because the problems

62
00:02:06,349 --> 00:02:08,569
that Ophira has described are exactly

63
00:02:08,868 --> 00:02:10,439
what we set out to solve with Datafi

64
00:02:11,588 --> 00:02:13,088
large EBS capacity,

65
00:02:13,379 --> 00:02:15,528
overprovisioned and underutilized.

66
00:02:16,229 --> 00:02:18,429
Datafi is an autonomous storage

67
00:02:18,429 --> 00:02:19,050
solution

68
00:02:19,629 --> 00:02:22,270
which manages cloud storage

69
00:02:22,270 --> 00:02:24,629
automatically, autonomously

70
00:02:24,629 --> 00:02:26,699
for AWS customers.

71
00:02:26,949 --> 00:02:29,229
You can deploy it on the fly and

72
00:02:29,229 --> 00:02:31,270
it will adjust your volume capacity in

73
00:02:31,270 --> 00:02:32,909
EBS automatically,

74
00:02:33,229 --> 00:02:35,258
auto scaling it based on your needs.

75
00:02:35,588 --> 00:02:37,899
If you fill it up, it will grow automatically,

76
00:02:37,909 --> 00:02:40,139
and if you delete files, it will shrink

77
00:02:40,139 --> 00:02:41,330
it automatically.

78
00:02:41,710 --> 00:02:43,899
It has no impact to performance. You can

79
00:02:43,899 --> 00:02:45,149
deploy it in real time.

80
00:02:49,319 --> 00:02:49,849
With Datai

81
00:02:51,118 --> 00:02:53,169
you get dynamic auto scaling,

82
00:02:53,599 --> 00:02:55,879
so the solution is completely autonomous

83
00:02:55,879 --> 00:02:58,000
and it overcomes those EBS limitations

84
00:02:58,000 --> 00:03:00,099
that Air mentioned that prevented him

85
00:03:00,099 --> 00:03:02,399
from being efficient in his consumption

86
00:03:02,399 --> 00:03:03,278
of EBS.

87
00:03:03,679 --> 00:03:05,960
It allows to basically endlessly

88
00:03:05,960 --> 00:03:08,240
scale your EBS capacity both

89
00:03:08,240 --> 00:03:10,319
up and down as customers

90
00:03:10,319 --> 00:03:12,379
fill up data and delete and enter

91
00:03:12,719 --> 00:03:14,679
these cycles of growth and deletion.

92
00:03:15,460 --> 00:03:17,740
Whenever you use data file, there's no downtime

93
00:03:17,740 --> 00:03:20,199
at all. We've always thought that customers

94
00:03:20,338 --> 00:03:22,580
will not tolerate introducing downtime

95
00:03:22,580 --> 00:03:24,659
to their applications when they want

96
00:03:24,659 --> 00:03:26,439
to improve their storage utilization.

97
00:03:27,250 --> 00:03:29,538
You can install it or uninstall it without

98
00:03:29,538 --> 00:03:31,528
any impact to applications,

99
00:03:31,939 --> 00:03:34,219
to your file system, or to any other

100
00:03:34,219 --> 00:03:35,868
thing running in your stack.

101
00:03:36,610 --> 00:03:38,649
Furthermore, there's no changes needed

102
00:03:38,649 --> 00:03:39,490
to your stack.

103
00:03:39,808 --> 00:03:42,159
It integrates seamlessly with Kubertis,

104
00:03:42,199 --> 00:03:43,550
with cloud formation,

105
00:03:43,889 --> 00:03:45,129
with Terraform,

106
00:03:45,409 --> 00:03:47,528
and it supports any Linux operating

107
00:03:47,528 --> 00:03:48,210
system

108
00:03:48,528 --> 00:03:50,669
and any tech stack you're running on top of it.

109
00:03:50,889 --> 00:03:52,979
So you don't need to inform your customers,

110
00:03:53,050 --> 00:03:55,050
oh, we're going to take down time because we want to do

111
00:03:55,050 --> 00:03:56,000
something with the storage.

112
00:03:56,288 --> 00:03:58,330
The whole thing is done automatically and

113
00:03:58,330 --> 00:03:59,069
seamlessly

114
00:03:59,368 --> 00:04:01,659
without any intervention required by you

115
00:04:01,819 --> 00:04:03,429
or your application owners.

116
00:04:06,429 --> 00:04:08,139
So how does Data fight work?

117
00:04:08,429 --> 00:04:10,710
It is based on a low level agent

118
00:04:10,710 --> 00:04:12,028
that you install

119
00:04:12,308 --> 00:04:14,550
on your EC2 servers or you recupernate

120
00:04:14,550 --> 00:04:15,429
these clusters.

121
00:04:15,710 --> 00:04:17,829
This agent changes the

122
00:04:17,829 --> 00:04:19,629
destination volumes of EBS

123
00:04:19,949 --> 00:04:22,108
automatically and dynamically without

124
00:04:22,108 --> 00:04:23,569
impacting your applications.

125
00:04:24,548 --> 00:04:26,829
In addition to that, we have a SAS control

126
00:04:26,829 --> 00:04:29,569
plane or. A back end that runs in RVPC,

127
00:04:29,829 --> 00:04:32,028
it monitors the agents and it

128
00:04:32,028 --> 00:04:34,428
gives them commands like growing volumes

129
00:04:34,428 --> 00:04:35,678
or shrinking volumes.

130
00:04:36,028 --> 00:04:36,809
It also

131
00:04:37,069 --> 00:04:39,230
provides analytics and shows

132
00:04:39,230 --> 00:04:41,470
you what exactly is going on with your EBS

133
00:04:41,470 --> 00:04:42,338
deployments,

134
00:04:42,709 --> 00:04:45,149
how much storage you're consuming, how efficient

135
00:04:45,149 --> 00:04:47,119
are you, how much efficiency Datafi brought to the table.

136
00:04:49,389 --> 00:04:51,439
Finally, we've spent a lot of effort

137
00:04:51,439 --> 00:04:53,629
into integrating with infrastructurist code

138
00:04:53,629 --> 00:04:55,588
environments, both Kubernetes

139
00:04:55,988 --> 00:04:58,250
and other infrastructurist code environments.

140
00:04:58,540 --> 00:05:00,649
This allows you to use

141
00:05:00,790 --> 00:05:03,629
the product without making any modifications

142
00:05:03,829 --> 00:05:05,970
and it completely uh

143
00:05:05,980 --> 00:05:07,988
uh integrates into the life

144
00:05:07,988 --> 00:05:09,230
cycle of CICD.

145
00:05:12,699 --> 00:05:14,750
So along the way when we started

146
00:05:14,750 --> 00:05:16,819
working with Dattaify we had a couple of issues

147
00:05:16,819 --> 00:05:18,858
and we had a couple of challenges that we needed to

148
00:05:18,858 --> 00:05:20,358
solve in order to

149
00:05:21,040 --> 00:05:23,100
streamline the entire process and make sure

150
00:05:23,100 --> 00:05:25,079
everything works with our existing platform.

151
00:05:25,619 --> 00:05:27,670
The first thing that we had a challenge with was bottle

152
00:05:27,670 --> 00:05:28,199
Rocket.

153
00:05:28,540 --> 00:05:30,660
So together with Dattaify we worked together and

154
00:05:30,660 --> 00:05:33,660
we made sure that the Dataify agent

155
00:05:33,939 --> 00:05:35,949
basically runs on our existing bottle rocket

156
00:05:35,949 --> 00:05:37,959
infrastructure that we have in EKS.

157
00:05:39,129 --> 00:05:40,338
The second thing that we

158
00:05:40,660 --> 00:05:42,699
had, the second challenge that we had was

159
00:05:42,699 --> 00:05:45,040
security. Basically as an AI platform

160
00:05:45,379 --> 00:05:47,069
we host our customers' data

161
00:05:47,379 --> 00:05:49,019
and we wanted to make sure that

162
00:05:49,338 --> 00:05:51,410
working with Dataify everything is just

163
00:05:51,410 --> 00:05:53,160
as secure as it was previously.

164
00:05:53,738 --> 00:05:55,829
And the main part here was with Dataify

165
00:05:55,829 --> 00:05:57,858
that no data leaves

166
00:05:57,858 --> 00:05:59,069
the actual cluster,

167
00:05:59,420 --> 00:06:00,889
no data leaves the EKS,

168
00:06:01,220 --> 00:06:01,738
and

169
00:06:02,100 --> 00:06:04,338
the task thativan talked about is

170
00:06:04,338 --> 00:06:05,139
only management,

171
00:06:05,699 --> 00:06:06,358
so we keep.

172
00:06:06,819 --> 00:06:08,920
The same level of security

173
00:06:09,178 --> 00:06:11,619
and the same level of reliability for our customers'

174
00:06:11,619 --> 00:06:13,899
data. The last challenge

175
00:06:13,899 --> 00:06:16,019
that we had with um with working

176
00:06:16,019 --> 00:06:17,399
with Datafi is basically

177
00:06:17,738 --> 00:06:19,959
keeping our existing

178
00:06:20,500 --> 00:06:22,519
backup implementation

179
00:06:23,129 --> 00:06:25,259
so we have a solution that

180
00:06:25,259 --> 00:06:26,298
we work with with Vallevo

181
00:06:26,850 --> 00:06:29,119
to make backups send to backup PVs

182
00:06:29,619 --> 00:06:31,949
and together with Datafi we made sure that

183
00:06:32,178 --> 00:06:34,540
we're able to to continue using

184
00:06:34,540 --> 00:06:35,959
the existing solution

185
00:06:36,379 --> 00:06:36,939
with Vallejo.

186
00:06:37,899 --> 00:06:39,358
And seamlessly

187
00:06:39,778 --> 00:06:42,220
continue to back up and restore dated

188
00:06:42,220 --> 00:06:44,778
volumes without any, any issues.

189
00:06:49,100 --> 00:06:51,160
So we thought it would be nice to show you an example

190
00:06:51,298 --> 00:06:53,500
of the kind of optimization that Data I

191
00:06:53,500 --> 00:06:54,699
brought to H2O.

192
00:06:55,338 --> 00:06:57,579
On the left side chart, you see

193
00:06:57,579 --> 00:06:59,488
the capacity utilization.

194
00:06:59,858 --> 00:07:01,939
Customers typically have low

195
00:07:01,939 --> 00:07:04,439
capacity utilization, especially on EBS.

196
00:07:04,809 --> 00:07:06,858
They tend to overprovision because they're

197
00:07:06,858 --> 00:07:08,939
afraid they're going to run out of space, and

198
00:07:08,939 --> 00:07:11,259
they're afraid they're not going to be able to grow in time.

199
00:07:11,738 --> 00:07:13,199
So on the left side.

200
00:07:14,019 --> 00:07:16,298
The beginning of the chart, you see that the capacity

201
00:07:16,298 --> 00:07:18,588
utilization is just 25%,

202
00:07:18,899 --> 00:07:20,928
which means that they're overpaying

203
00:07:20,928 --> 00:07:21,559
4x

204
00:07:22,139 --> 00:07:24,358
compared to if they could have paid

205
00:07:24,358 --> 00:07:25,399
based on utilization

206
00:07:26,220 --> 00:07:28,540
rather than capacity reservation

207
00:07:28,540 --> 00:07:30,199
as EBS charges them today.

208
00:07:30,838 --> 00:07:32,970
As they deploy data on more

209
00:07:32,970 --> 00:07:34,319
and more environments,

210
00:07:34,608 --> 00:07:36,899
the chart for utilization keeps

211
00:07:36,899 --> 00:07:38,358
growing and improving

212
00:07:38,689 --> 00:07:40,730
to the point where it stabilizes

213
00:07:40,730 --> 00:07:42,000
around 80%,

214
00:07:42,290 --> 00:07:44,149
which is our natural

215
00:07:44,528 --> 00:07:46,809
place where we want to hold the buffer. We don't want to

216
00:07:46,809 --> 00:07:47,358
make it

217
00:07:47,649 --> 00:07:49,750
hit 100% because that means we're

218
00:07:50,108 --> 00:07:51,230
running out of space.

219
00:07:51,528 --> 00:07:54,329
So it's quite typical to stabilize

220
00:07:54,329 --> 00:07:56,689
around 80%, which is what we consider

221
00:07:56,689 --> 00:07:57,619
success criteria.

222
00:07:58,238 --> 00:08:00,809
On the right chart you can see what's happening

223
00:08:00,809 --> 00:08:02,970
in terms of the capacity

224
00:08:02,970 --> 00:08:05,170
itself. When we started out, their

225
00:08:05,170 --> 00:08:07,410
total capacity footprint was about

226
00:08:07,410 --> 00:08:08,639
2 petabytes,

227
00:08:08,970 --> 00:08:10,980
but in terms of how much data they've

228
00:08:10,980 --> 00:08:12,470
actually written to it,

229
00:08:12,928 --> 00:08:14,970
the green line shows you only 0.5

230
00:08:14,970 --> 00:08:16,199
petabyte written.

231
00:08:16,528 --> 00:08:19,709
That's why we say the capacity utilization is 25%.

232
00:08:20,259 --> 00:08:22,699
As time progressed with the utilization

233
00:08:22,699 --> 00:08:23,309
of Dafi,

234
00:08:24,269 --> 00:08:26,309
what you're seeing is really interesting. Even though

235
00:08:26,309 --> 00:08:28,389
the green line grows a bit because

236
00:08:28,389 --> 00:08:29,970
they wrote more data,

237
00:08:30,470 --> 00:08:32,869
the blue line, which is how much capacity

238
00:08:32,869 --> 00:08:34,450
they're paying for an EBS,

239
00:08:34,788 --> 00:08:35,859
keeps dropping

240
00:08:36,229 --> 00:08:38,928
because of the data file or scale capacity.

241
00:08:40,070 --> 00:08:41,899
A long time, the blue line

242
00:08:42,259 --> 00:08:44,710
almost reaches the green line, which is the 80%

243
00:08:44,710 --> 00:08:46,710
utilization that we were targeting.

244
00:08:47,038 --> 00:08:48,840
The savings for H2O

245
00:08:49,119 --> 00:08:51,629
are not paying for 2 petabytes,

246
00:08:51,639 --> 00:08:53,259
but actually paying for

247
00:08:53,710 --> 00:08:55,719
less than 1 petabyte,

248
00:08:55,918 --> 00:08:58,000
and that's significant savings, and this

249
00:08:58,000 --> 00:08:59,019
is applicable

250
00:08:59,320 --> 00:09:01,519
for just about any EBS customer

251
00:09:01,519 --> 00:09:02,139
out there.

252
00:09:04,048 --> 00:09:06,048
So basically the

253
00:09:06,048 --> 00:09:07,879
result of our work with Dattaify

254
00:09:08,690 --> 00:09:11,210
was that we were able to deploy the

255
00:09:11,210 --> 00:09:13,109
Dataify solution across all our customers.

256
00:09:14,250 --> 00:09:16,369
The Dataify solution is deployed with our existing tools

257
00:09:16,369 --> 00:09:17,239
and solutions,

258
00:09:17,609 --> 00:09:19,859
meaning that we still use Terrafone to deploy,

259
00:09:19,928 --> 00:09:20,548
um, DataF.

260
00:09:21,489 --> 00:09:23,960
It's, it's integrated into our GitOps

261
00:09:23,960 --> 00:09:26,080
process, and we didn't have to make

262
00:09:26,080 --> 00:09:28,229
any substantial changes to our

263
00:09:28,229 --> 00:09:30,548
infrastructure to get Datafi to work.

264
00:09:31,308 --> 00:09:33,399
Of course we have a very big um cost

265
00:09:33,399 --> 00:09:35,058
saving which Devan just showed

266
00:09:35,440 --> 00:09:35,979
um

267
00:09:36,288 --> 00:09:37,460
we were able to.

268
00:09:38,840 --> 00:09:40,859
Take up our utilization and reduce our

269
00:09:40,859 --> 00:09:41,418
cost.

270
00:09:43,469 --> 00:09:45,599
While we're we're doing that, we were also

271
00:09:45,599 --> 00:09:48,200
able to give better performance to our customers.

272
00:09:48,599 --> 00:09:50,678
So basically the, the data 5 solution

273
00:09:50,678 --> 00:09:52,969
was able to reduce

274
00:09:53,279 --> 00:09:55,279
our cost for EBS and

275
00:09:55,279 --> 00:09:57,359
also increase our performance for our

276
00:09:57,359 --> 00:09:58,440
EBS volumes.

277
00:09:59,538 --> 00:10:01,779
And I think that the most important part of

278
00:10:01,779 --> 00:10:03,558
this solution, the data file solution,

279
00:10:03,969 --> 00:10:05,759
is that there was zero downtime.

280
00:10:06,418 --> 00:10:08,619
So once the data file agent was

281
00:10:08,619 --> 00:10:11,038
deployed and was in all the clusters

282
00:10:11,168 --> 00:10:12,320
in a read-only mode,

283
00:10:12,619 --> 00:10:14,239
we just had to flip a switch

284
00:10:14,658 --> 00:10:17,099
and basically from that point on it

285
00:10:17,099 --> 00:10:18,340
started doing its magic.

286
00:10:19,190 --> 00:10:21,820
Basically reducing our storage

287
00:10:21,820 --> 00:10:24,038
without any manual intervention

288
00:10:24,038 --> 00:10:25,048
that we had to do.

289
00:10:25,519 --> 00:10:26,450
So just,

290
00:10:26,960 --> 00:10:28,840
again, on the flip of a switch,

291
00:10:29,359 --> 00:10:31,879
we started seeing reduced um storage

292
00:10:31,879 --> 00:10:32,408
costs.

293
00:10:36,109 --> 00:10:37,798
Thank you, thank you very much.

