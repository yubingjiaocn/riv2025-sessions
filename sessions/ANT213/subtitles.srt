1
00:00:00,250 --> 00:00:01,268
Well hello everybody.

2
00:00:01,758 --> 00:00:04,299
It's day 4 reinvent and it's lunchtime,

3
00:00:04,448 --> 00:00:07,049
so I'm so honored to have such a dedicated

4
00:00:07,049 --> 00:00:08,269
audience. There's a lot

5
00:00:08,569 --> 00:00:09,550
to learn today.

6
00:00:10,028 --> 00:00:12,409
Um, we're also joined by an audience

7
00:00:12,409 --> 00:00:13,310
out in Wyn

8
00:00:13,689 --> 00:00:15,689
as well as the Venetian. So hello

9
00:00:15,689 --> 00:00:17,199
from Mandalay Bay.

10
00:00:17,489 --> 00:00:19,859
I'm Dylan Tong. I'm from OpenSearch

11
00:00:19,859 --> 00:00:21,548
Product. I'm responsible for

12
00:00:21,908 --> 00:00:24,170
AI and vector workloads. I'm joined

13
00:00:24,170 --> 00:00:25,109
by my colleague.

14
00:00:25,609 --> 00:00:27,769
Hello everyone, this is Wamshivijay Nirta

15
00:00:27,769 --> 00:00:29,388
working as a senior software manager.

16
00:00:29,818 --> 00:00:32,098
Uh, working on search at Amazon Open Search Service.

17
00:00:32,500 --> 00:00:34,259
Hope you guys are having an amazing reinvent.

18
00:00:35,319 --> 00:00:37,598
Yeah, so this session is about building billion

19
00:00:37,598 --> 00:00:39,639
scale vector databases. We're

20
00:00:39,639 --> 00:00:41,340
gonna be introducing two

21
00:00:41,918 --> 00:00:44,158
reinvent launches for open

22
00:00:44,158 --> 00:00:47,158
search GPU vector GPU acceleration

23
00:00:47,399 --> 00:00:48,819
and auto optimize.

24
00:00:49,848 --> 00:00:50,929
So with that said,

25
00:00:51,408 --> 00:00:53,529
can I have a quick show of hands who here is currently

26
00:00:53,529 --> 00:00:54,709
using OpenSearch?

27
00:00:55,450 --> 00:00:57,200
All right, so a few of you, um,

28
00:00:57,490 --> 00:00:59,689
any of you have experience with vector

29
00:00:59,689 --> 00:01:00,270
search?

30
00:01:00,969 --> 00:01:02,149
A couple, OK,

31
00:01:02,490 --> 00:01:03,548
so we have,

32
00:01:03,969 --> 00:01:04,609
um,

33
00:01:05,168 --> 00:01:07,370
so we're gonna spend some time just a brief

34
00:01:07,370 --> 00:01:09,448
moment to get everybody up to speed

35
00:01:09,448 --> 00:01:11,569
for some of you maybe a refresher, but we're

36
00:01:11,569 --> 00:01:12,918
gonna keep it succinct,

37
00:01:13,250 --> 00:01:13,769
OK?

38
00:01:15,069 --> 00:01:17,109
So quickly open search, it's a

39
00:01:17,109 --> 00:01:19,299
search and analytics engine. It's

40
00:01:19,299 --> 00:01:21,388
Apache open source, it's part of the

41
00:01:21,388 --> 00:01:22,549
Linux Foundation.

42
00:01:24,338 --> 00:01:26,168
And on AWS

43
00:01:26,448 --> 00:01:28,930
there's a couple of flavors. One, managed clusters

44
00:01:28,930 --> 00:01:31,159
where you have full control of selecting your

45
00:01:31,159 --> 00:01:33,409
instances. We provide automation

46
00:01:33,769 --> 00:01:35,888
to provision those clusters as well

47
00:01:35,888 --> 00:01:37,829
as operational functionality.

48
00:01:38,088 --> 00:01:39,989
And then there's a serus version,

49
00:01:40,448 --> 00:01:41,870
right, with auto scaling.

50
00:01:42,489 --> 00:01:43,250
And such

51
00:01:43,609 --> 00:01:45,329
there's peripheral services,

52
00:01:45,650 --> 00:01:47,650
open search ingestion for you

53
00:01:47,650 --> 00:01:50,010
to, uh, process and

54
00:01:50,010 --> 00:01:52,010
load your data into open search

55
00:01:52,010 --> 00:01:53,668
and open search dashboards

56
00:01:53,969 --> 00:01:55,489
for visualizations.

57
00:01:56,049 --> 00:01:58,250
People use OpenSearch for a lot of things, but

58
00:01:58,250 --> 00:02:00,528
there's really kind of two main use cases. There's

59
00:02:00,528 --> 00:02:02,680
one half that uses it for log analytics

60
00:02:02,680 --> 00:02:03,668
and observability,

61
00:02:04,088 --> 00:02:06,129
and then the other half is search, which is the focus

62
00:02:06,129 --> 00:02:07,609
of this presentation today.

63
00:02:08,487 --> 00:02:10,568
It could be keyword search, it's

64
00:02:10,568 --> 00:02:12,649
vector search, as well as a hybrid,

65
00:02:12,729 --> 00:02:13,889
a blend of the two.

66
00:02:16,110 --> 00:02:18,149
Don't worry if you're not familiar with vector

67
00:02:18,149 --> 00:02:20,270
search, we'll get you up to speed, but for

68
00:02:20,270 --> 00:02:22,300
now, just know, hey, why do people use

69
00:02:22,300 --> 00:02:23,250
vector search.

70
00:02:23,819 --> 00:02:25,979
Foremost, it's simply to improve search quality,

71
00:02:26,069 --> 00:02:28,110
vector hybrid variance, it's the state

72
00:02:28,110 --> 00:02:28,770
of the art.

73
00:02:29,659 --> 00:02:32,189
It's also very uh versatile,

74
00:02:32,349 --> 00:02:34,379
right? It supports text-based

75
00:02:34,379 --> 00:02:35,399
search, but you can search

76
00:02:35,699 --> 00:02:37,349
audio images

77
00:02:37,699 --> 00:02:39,099
across between the two.

78
00:02:39,508 --> 00:02:41,849
There are also diverse applications,

79
00:02:42,058 --> 00:02:44,210
the obvious ones being semantic search,

80
00:02:44,538 --> 00:02:46,618
but also you can use vector

81
00:02:46,618 --> 00:02:48,849
search for recommendations, personalization,

82
00:02:49,058 --> 00:02:50,879
anomaly detection as well.

83
00:02:51,460 --> 00:02:54,139
Talk more about this, but it's also a key ingredient

84
00:02:54,379 --> 00:02:56,000
in agentic applications.

85
00:02:58,830 --> 00:03:00,699
A lot of people are surprised

86
00:03:01,199 --> 00:03:03,399
about our history, our long history in vector

87
00:03:03,399 --> 00:03:04,460
search. Did you know

88
00:03:05,199 --> 00:03:07,599
we began our journey back in February 2020.

89
00:03:07,679 --> 00:03:09,679
It actually predates the birth of open

90
00:03:09,679 --> 00:03:11,919
search. We contributed something

91
00:03:11,919 --> 00:03:12,838
called the KNN

92
00:03:13,189 --> 00:03:14,758
K nearest neighbor plug-in

93
00:03:15,080 --> 00:03:16,599
to the elastic open distro.

94
00:03:17,939 --> 00:03:20,179
And it wasn't until 2021 where

95
00:03:20,179 --> 00:03:22,338
OpenSearch 1.0 was released, this

96
00:03:22,338 --> 00:03:24,338
was a fork of Elastic Search

97
00:03:24,338 --> 00:03:25,770
7.10.

98
00:03:26,528 --> 00:03:28,528
And with that also, you

99
00:03:28,528 --> 00:03:29,889
know, built an Apache Luce,

100
00:03:30,330 --> 00:03:31,349
so we inherited

101
00:03:31,770 --> 00:03:34,050
a wide breadth of advanced search

102
00:03:34,050 --> 00:03:35,929
capabilities from that foundation.

103
00:03:36,649 --> 00:03:37,929
We already had

104
00:03:38,528 --> 00:03:40,689
a growing number of vector search

105
00:03:40,689 --> 00:03:41,368
users,

106
00:03:41,770 --> 00:03:43,229
some very large scale,

107
00:03:43,849 --> 00:03:46,008
you know, for example, folks in Amazon pushing

108
00:03:46,008 --> 00:03:47,229
the envelope, retail,

109
00:03:47,689 --> 00:03:50,210
Amazon Music, Amazon recognition

110
00:03:50,490 --> 00:03:52,490
use cases back then were around like image

111
00:03:52,490 --> 00:03:54,069
search and a lot of personalization,

112
00:03:54,919 --> 00:03:56,750
um, but we wanted to uplevel

113
00:03:57,210 --> 00:03:59,338
our scalability with 1.0,

114
00:03:59,409 --> 00:04:01,028
so we integrated Face,

115
00:04:01,368 --> 00:04:03,490
which is Meta's vector search library

116
00:04:03,490 --> 00:04:05,210
well known for performance and scale.

117
00:04:06,088 --> 00:04:08,129
I think we all remember 2023, right? That

118
00:04:08,129 --> 00:04:10,300
was the year of Gen AI,

119
00:04:10,689 --> 00:04:12,729
and with that there was the, the buzz around

120
00:04:12,729 --> 00:04:14,778
vector database, right? It's vector

121
00:04:14,778 --> 00:04:16,410
database instead of like KNN.

122
00:04:17,209 --> 00:04:17,988
So we

123
00:04:18,649 --> 00:04:20,790
brought our vector search capabilities

124
00:04:21,129 --> 00:04:22,399
to open search Servius.

125
00:04:22,689 --> 00:04:25,048
We became the default for Amazon

126
00:04:25,048 --> 00:04:27,250
Bedrock knowledge bases where people

127
00:04:27,250 --> 00:04:29,769
can, you know, build on bedrock foundation

128
00:04:29,769 --> 00:04:31,149
models and agents and

129
00:04:31,488 --> 00:04:33,470
reg retrieval augmented generation.

130
00:04:34,149 --> 00:04:36,588
We created something called hybrid

131
00:04:36,588 --> 00:04:38,670
search, so out of the box you can

132
00:04:38,670 --> 00:04:41,230
blend the traditional lexical

133
00:04:41,230 --> 00:04:43,350
keyword search with vector search which turns

134
00:04:43,350 --> 00:04:45,009
out improves search quality.

135
00:04:45,389 --> 00:04:46,290
We started thinking

136
00:04:46,750 --> 00:04:48,829
beyond vectors as well, you know, how

137
00:04:48,829 --> 00:04:49,528
can we create

138
00:04:49,869 --> 00:04:52,028
an AI native search engine. So

139
00:04:52,028 --> 00:04:54,428
we built these connectors, integrations

140
00:04:54,428 --> 00:04:56,850
to third party services, open AI.

141
00:04:57,500 --> 00:04:59,619
Uh, cohere as well as Amazon

142
00:04:59,619 --> 00:05:01,699
StageMaker and Bedrock so that we

143
00:05:01,699 --> 00:05:04,259
can automate the generation of vectors.

144
00:05:05,149 --> 00:05:07,319
Throughout the year, the deployment started getting

145
00:05:07,319 --> 00:05:09,399
bigger, so it became a priority to help our

146
00:05:09,399 --> 00:05:11,000
customers reduce costs.

147
00:05:11,278 --> 00:05:13,608
Cost optimizations was top of mind,

148
00:05:13,798 --> 00:05:15,910
building disk optimizations,

149
00:05:16,199 --> 00:05:17,519
tiered storage,

150
00:05:17,798 --> 00:05:19,579
um, automatic quantization.

151
00:05:20,314 --> 00:05:22,855
word for basically compression capabilities

152
00:05:23,113 --> 00:05:25,194
and we continue our journey around ease of use

153
00:05:25,194 --> 00:05:26,673
and AI native functionality.

154
00:05:26,954 --> 00:05:29,553
So not just vectors but general AI enrichments

155
00:05:29,553 --> 00:05:31,494
like language detection,

156
00:05:32,134 --> 00:05:34,423
translation, and other automated metadata

157
00:05:34,423 --> 00:05:35,213
extraction.

158
00:05:35,595 --> 00:05:38,035
So today in 2025, naturally

159
00:05:38,035 --> 00:05:39,053
cost optimization

160
00:05:39,814 --> 00:05:41,355
continues to be a priority.

161
00:05:41,665 --> 00:05:44,153
AI Native, we built agentic and AI

162
00:05:44,153 --> 00:05:46,153
search flows into our engine.

163
00:05:46,314 --> 00:05:48,355
We have MCP integration, but

164
00:05:48,355 --> 00:05:49,574
there was a new theme.

165
00:05:50,259 --> 00:05:52,540
Deployments are getting larger and larger,

166
00:05:52,660 --> 00:05:54,720
so a new theme we doubled down on

167
00:05:54,720 --> 00:05:56,798
empowering scale, and that's to focus on

168
00:05:56,798 --> 00:05:57,798
a conversation.

169
00:05:58,160 --> 00:06:00,160
Two new features focus on

170
00:06:00,160 --> 00:06:01,439
customers coming to us.

171
00:06:01,838 --> 00:06:04,009
We're growing, we're, we're building bigger vector

172
00:06:04,009 --> 00:06:06,358
databases. We like open search, but

173
00:06:06,358 --> 00:06:07,059
help,

174
00:06:07,579 --> 00:06:10,139
please make it faster, easier, and cheaper

175
00:06:10,358 --> 00:06:12,579
to scale your vector database.

176
00:06:13,088 --> 00:06:15,459
So before we talk about these two features, let's

177
00:06:15,459 --> 00:06:17,920
take a look at cus a couple of customer examples

178
00:06:18,100 --> 00:06:19,959
and that trend towards billion scale.

179
00:06:20,738 --> 00:06:22,079
As mentioned before, we

180
00:06:22,459 --> 00:06:25,160
already had large scale customers early on 2020.

181
00:06:25,298 --> 00:06:27,160
Example of that, right, is

182
00:06:27,500 --> 00:06:28,819
Amazon brand protection.

183
00:06:29,649 --> 00:06:30,750
So this is the unit

184
00:06:31,048 --> 00:06:33,569
within Amazon that protects our customers

185
00:06:33,569 --> 00:06:35,928
and partners, our retail customers and, and

186
00:06:35,928 --> 00:06:37,709
partners from abuse,

187
00:06:38,369 --> 00:06:40,410
and so they have a large number

188
00:06:40,410 --> 00:06:42,569
of automated systems in the

189
00:06:42,569 --> 00:06:45,309
background to detect things like IP infringement

190
00:06:45,639 --> 00:06:48,278
anomalies throughout the entire product catalog.

191
00:06:48,528 --> 00:06:50,428
So they take the entire catalog,

192
00:06:50,730 --> 00:06:52,809
they basically vector encoded it

193
00:06:52,809 --> 00:06:55,153
into 68. Billion vectors

194
00:06:55,494 --> 00:06:57,535
and they're running an automation system

195
00:06:57,535 --> 00:06:59,375
on open search

196
00:06:59,694 --> 00:07:02,004
searching for anomalies potential IP

197
00:07:02,004 --> 00:07:03,595
infringement and it's

198
00:07:03,975 --> 00:07:06,053
this among you know other automation

199
00:07:06,053 --> 00:07:08,053
capabilities enables us to

200
00:07:08,053 --> 00:07:09,634
detect abuse

201
00:07:10,494 --> 00:07:12,564
before 99% of them are

202
00:07:12,564 --> 00:07:13,494
even reported.

203
00:07:14,678 --> 00:07:15,819
So early days,

204
00:07:16,199 --> 00:07:18,519
a lot of the scale was driven

205
00:07:18,519 --> 00:07:20,819
by, you know, the likes of like Amazon,

206
00:07:21,160 --> 00:07:22,619
but today it's prevalent.

207
00:07:23,189 --> 00:07:25,319
Um, and part of that trend

208
00:07:25,319 --> 00:07:27,519
you probably heard it if you've been to the AI

209
00:07:27,519 --> 00:07:29,569
talks, right, is agentic

210
00:07:29,569 --> 00:07:32,278
applications, more and more customers

211
00:07:32,278 --> 00:07:34,600
building agentic features into

212
00:07:34,600 --> 00:07:36,759
their apps, and a good example of that is a

213
00:07:36,759 --> 00:07:38,759
startup called Devrev AI. We've been

214
00:07:38,759 --> 00:07:39,639
working with them

215
00:07:39,920 --> 00:07:41,358
for about a year now.

216
00:07:41,759 --> 00:07:42,540
So Devrev,

217
00:07:42,879 --> 00:07:45,000
they have this product they call a computer

218
00:07:45,000 --> 00:07:47,238
and just think about it as your AI

219
00:07:47,238 --> 00:07:47,838
teammates.

220
00:07:49,139 --> 00:07:51,178
The challenge or the problems they're helping

221
00:07:51,178 --> 00:07:53,379
their their customers to solve,

222
00:07:53,459 --> 00:07:55,540
it's an age-old one nothing special. It's

223
00:07:55,540 --> 00:07:57,540
about everybody has siloed

224
00:07:57,540 --> 00:07:59,899
enterprise data sources, right, and

225
00:07:59,899 --> 00:08:02,358
tools, and they're looking to bring it together.

226
00:08:03,199 --> 00:08:05,329
But they're rethinking it, they're reimagining it,

227
00:08:05,608 --> 00:08:07,639
how we can better solve the problem with

228
00:08:07,639 --> 00:08:09,928
AI agents to connect,

229
00:08:10,088 --> 00:08:11,838
unify the data sources,

230
00:08:12,170 --> 00:08:14,170
drive search and automation.

231
00:08:15,059 --> 00:08:15,798
OK,

232
00:08:16,420 --> 00:08:18,459
so, and, and they're, and basically

233
00:08:18,459 --> 00:08:20,939
they're driving it specifically across

234
00:08:20,939 --> 00:08:22,079
product support,

235
00:08:22,699 --> 00:08:23,559
sales,

236
00:08:24,059 --> 00:08:24,600
and

237
00:08:25,059 --> 00:08:26,019
product management.

238
00:08:27,528 --> 00:08:29,509
They built it on open search. Today

239
00:08:29,889 --> 00:08:32,070
hundreds of millions of vectors.

240
00:08:32,859 --> 00:08:34,837
And a million vectors

241
00:08:35,418 --> 00:08:37,619
changing on a daily basis, so growing

242
00:08:37,619 --> 00:08:38,717
really rapidly,

243
00:08:39,058 --> 00:08:41,178
um, already great successes with their

244
00:08:41,178 --> 00:08:43,479
customers in support, uh, they've

245
00:08:44,119 --> 00:08:46,837
achieved, um, 88, I think%

246
00:08:47,138 --> 00:08:49,138
of their 85% of their

247
00:08:49,138 --> 00:08:51,778
tickets are resolved without human interaction,

248
00:08:52,258 --> 00:08:54,898
you know, 30%

249
00:08:54,898 --> 00:08:57,058
cost reduction in customer support, saving

250
00:08:57,058 --> 00:08:58,658
their customers a lot of time.

251
00:09:00,940 --> 00:09:03,019
Let's dive a little bit deeper into

252
00:09:03,019 --> 00:09:04,960
the components of agentic systems.

253
00:09:05,418 --> 00:09:07,259
So imagine we have a travel system,

254
00:09:07,538 --> 00:09:09,590
right? Agentic features allow us,

255
00:09:10,178 --> 00:09:12,379
give us a conversational interface that you can

256
00:09:12,379 --> 00:09:14,759
perhaps request. Help me plan my trip

257
00:09:15,099 --> 00:09:15,619
to

258
00:09:15,940 --> 00:09:17,788
reinvent 2025.

259
00:09:18,700 --> 00:09:21,029
That request gets processed by an AI agent,

260
00:09:21,190 --> 00:09:22,759
typically large language model.

261
00:09:23,340 --> 00:09:25,739
It creates an execution

262
00:09:25,739 --> 00:09:27,599
plan usually you'll then reach out

263
00:09:27,940 --> 00:09:29,639
to knowledge bases,

264
00:09:30,019 --> 00:09:32,500
uh, application history for context,

265
00:09:32,739 --> 00:09:33,840
and then compile

266
00:09:34,099 --> 00:09:35,879
some automation, perhaps

267
00:09:36,418 --> 00:09:38,619
putting together like a travel itinerary in this

268
00:09:38,619 --> 00:09:40,080
case, well.

269
00:09:41,399 --> 00:09:43,519
You know what the saying goes, garbage in

270
00:09:43,519 --> 00:09:45,519
garbage out. So when it reaches out to

271
00:09:45,519 --> 00:09:47,558
the knowledge bases, let's say in this case

272
00:09:47,558 --> 00:09:49,119
hotel venues,

273
00:09:49,440 --> 00:09:50,038
flights,

274
00:09:50,320 --> 00:09:52,320
if it's not able to retrieve

275
00:09:52,558 --> 00:09:54,678
the, you know, high quality search

276
00:09:54,678 --> 00:09:56,719
results, you're gonna exact you're gonna

277
00:09:56,719 --> 00:09:57,580
expect poor.

278
00:09:58,014 --> 00:10:00,075
Responses and automation

279
00:10:00,264 --> 00:10:02,404
vector search is the state of the art.

280
00:10:02,455 --> 00:10:04,614
Let's take a look at the example to understand why

281
00:10:04,614 --> 00:10:05,504
that's the case.

282
00:10:05,933 --> 00:10:07,474
So here I'm running a search

283
00:10:07,854 --> 00:10:09,413
on a wiki data set.

284
00:10:09,734 --> 00:10:11,455
I'm searching for wild west,

285
00:10:11,774 --> 00:10:14,154
and you see on the left hand side

286
00:10:14,815 --> 00:10:17,094
keyword search, it's returning

287
00:10:17,094 --> 00:10:17,994
results like

288
00:10:18,254 --> 00:10:20,375
the West Virginia basketball team because

289
00:10:20,375 --> 00:10:22,394
it's keying in on the term West.

290
00:10:22,695 --> 00:10:24,774
On the right hand side it's semantic

291
00:10:24,774 --> 00:10:27,134
search using a vector based implementation.

292
00:10:27,519 --> 00:10:30,029
And I'm getting like cowboys and rodeos because

293
00:10:30,200 --> 00:10:33,090
vector search is matching on semantic

294
00:10:33,090 --> 00:10:34,070
similarity

295
00:10:34,408 --> 00:10:36,830
and I'm getting much better results.

296
00:10:37,450 --> 00:10:38,469
So if we're gonna build

297
00:10:39,009 --> 00:10:41,440
high quality agentic features,

298
00:10:41,769 --> 00:10:42,629
we need to

299
00:10:42,969 --> 00:10:45,090
build these vector databases or run

300
00:10:45,090 --> 00:10:47,489
vector search across our vast

301
00:10:47,489 --> 00:10:49,808
enterprise data sources that

302
00:10:49,808 --> 00:10:50,908
requires scale.

303
00:10:52,250 --> 00:10:54,369
But easier said than done, there

304
00:10:54,369 --> 00:10:56,509
are a lot of challenges, some of you may know,

305
00:10:56,690 --> 00:10:58,668
to scaling your vector database.

306
00:10:59,250 --> 00:11:01,590
Let's get everyone up to speed on the basics.

307
00:11:02,538 --> 00:11:04,639
Highest level you start with your

308
00:11:04,639 --> 00:11:06,940
content you have an embedding model

309
00:11:06,940 --> 00:11:09,038
that's specially designed to encode

310
00:11:09,038 --> 00:11:09,820
that content

311
00:11:10,279 --> 00:11:11,418
into vectors

312
00:11:11,719 --> 00:11:14,219
and then open search provides APIs

313
00:11:14,440 --> 00:11:16,719
for you to ingest and

314
00:11:16,719 --> 00:11:19,158
build an index on those vectors

315
00:11:19,158 --> 00:11:20,779
and then that enables you

316
00:11:21,119 --> 00:11:23,418
to then run search queries

317
00:11:23,678 --> 00:11:26,158
like similarity search queries on

318
00:11:26,158 --> 00:11:27,418
that content.

319
00:11:28,479 --> 00:11:30,548
But what are those vectors exactly?

320
00:11:30,869 --> 00:11:32,950
The vectors are the same vectors that

321
00:11:32,950 --> 00:11:35,109
you learned back in physics and linear

322
00:11:35,109 --> 00:11:36,009
algebra,

323
00:11:36,308 --> 00:11:38,349
right? Main difference is back then is, you know,

324
00:11:38,389 --> 00:11:40,700
X, Y, Z, typically 3 dimensions.

325
00:11:41,029 --> 00:11:43,058
These vectors are typically over 1000

326
00:11:43,058 --> 00:11:45,070
dimensions these days, but they're just a

327
00:11:45,070 --> 00:11:47,500
long list of numerical values.

328
00:11:47,788 --> 00:11:50,269
But what's interesting is that these vectors

329
00:11:50,269 --> 00:11:52,678
that are encoded by these embedding models,

330
00:11:53,229 --> 00:11:55,548
when you measure the distance between the two, that

331
00:11:55,548 --> 00:11:57,269
represents a degree of similarity.

332
00:11:57,639 --> 00:11:59,668
So imagine I had a music

333
00:11:59,668 --> 00:12:01,859
corpus. I got two music tracks

334
00:12:02,119 --> 00:12:04,239
coded into vectors. If I measure, let's

335
00:12:04,239 --> 00:12:06,590
say the cosine or the Euclidean distance,

336
00:12:06,840 --> 00:12:09,029
that's the degree of similarity between those

337
00:12:09,029 --> 00:12:09,859
two songs.

338
00:12:10,200 --> 00:12:10,960
If we take,

339
00:12:11,239 --> 00:12:13,279
if we, we do take those vectors and we just

340
00:12:13,279 --> 00:12:15,989
superimpose it into two dimensions

341
00:12:16,038 --> 00:12:18,200
so that we can visualize it, it's gonna look something

342
00:12:18,200 --> 00:12:20,408
like, um, this

343
00:12:20,418 --> 00:12:21,440
scatter graph.

344
00:12:22,190 --> 00:12:24,460
And this arrow, pretend that's

345
00:12:24,460 --> 00:12:26,469
your favorite song it's been encoded to

346
00:12:26,469 --> 00:12:27,190
a vector

347
00:12:27,460 --> 00:12:29,668
basically the area around

348
00:12:29,668 --> 00:12:30,519
that vector,

349
00:12:30,788 --> 00:12:33,109
those are similar songs, the most similar

350
00:12:33,109 --> 00:12:33,969
songs and perhaps

351
00:12:34,668 --> 00:12:36,830
the songs that you may be interested

352
00:12:36,830 --> 00:12:38,918
in. That's

353
00:12:38,918 --> 00:12:40,288
how vector search works.

354
00:12:40,678 --> 00:12:42,719
So you have things turned into vectors, you

355
00:12:42,719 --> 00:12:44,719
can measure similarity, how do you run

356
00:12:44,719 --> 00:12:45,769
search queries?

357
00:12:46,158 --> 00:12:47,639
Couple of ways.

358
00:12:48,080 --> 00:12:50,200
So the first one is brute force exact.

359
00:12:50,340 --> 00:12:52,558
So imagine you have a query vector, that's your favorite

360
00:12:52,558 --> 00:12:53,700
song, it's a vector.

361
00:12:54,070 --> 00:12:56,190
And then you have the rest of the music

362
00:12:56,190 --> 00:12:57,080
corpus,

363
00:12:57,509 --> 00:12:59,590
um, all the songs, they're all vectors as well,

364
00:12:59,629 --> 00:13:00,210
and literally

365
00:13:00,509 --> 00:13:02,769
if you find a top K, you know, maybe

366
00:13:02,769 --> 00:13:04,979
top 100, top 10 top K

367
00:13:04,979 --> 00:13:05,690
songs known as

368
00:13:05,989 --> 00:13:07,129
K nearest neighbors,

369
00:13:07,389 --> 00:13:09,450
I literally do a comparison between

370
00:13:09,450 --> 00:13:11,658
everything. That query vector, compare

371
00:13:11,658 --> 00:13:12,479
it to one song,

372
00:13:12,820 --> 00:13:13,798
measure the distance,

373
00:13:14,139 --> 00:13:16,739
and then figure out which ones are the most closest

374
00:13:16,739 --> 00:13:18,899
together. Obviously that takes a very long

375
00:13:18,899 --> 00:13:21,058
time when you scale to a billion, it's

376
00:13:21,058 --> 00:13:21,739
gonna take, you know,

377
00:13:22,408 --> 00:13:24,099
minutes at least to run a query.

378
00:13:24,940 --> 00:13:26,609
And that's why we have indexes.

379
00:13:26,979 --> 00:13:29,048
So we have these algorithms,

380
00:13:29,259 --> 00:13:30,200
popular one like

381
00:13:30,570 --> 00:13:32,629
hierarchical navigable navigable small

382
00:13:32,629 --> 00:13:35,219
worlds, HNSW. There's other algorithms

383
00:13:35,928 --> 00:13:38,058
where you preprocess these vectors

384
00:13:38,058 --> 00:13:40,219
and you build these graph-like structures that you see

385
00:13:40,219 --> 00:13:42,899
here. And once you have this index,

386
00:13:43,259 --> 00:13:45,500
you can then do real-time queries. I

387
00:13:45,500 --> 00:13:47,219
can now run that top que query

388
00:13:47,619 --> 00:13:49,719
in milliseconds. It traverses this graph.

389
00:13:49,779 --> 00:13:51,058
It gets the results for you.

390
00:13:51,349 --> 00:13:53,379
Um, the trade-off mainly is it's a

391
00:13:53,379 --> 00:13:55,658
lot of heavy duty processing to build that index

392
00:13:55,658 --> 00:13:57,779
up front and it's also approximation,

393
00:13:57,849 --> 00:14:00,399
but the approximation of exact canon is generally

394
00:14:00,820 --> 00:14:02,908
very close, right? You can get 0.99

395
00:14:02,908 --> 00:14:03,519
or higher.

396
00:14:05,009 --> 00:14:07,119
OK, so what does open search do exactly

397
00:14:07,119 --> 00:14:09,119
with the indexes? So it does build these indexes,

398
00:14:09,200 --> 00:14:11,219
but it does more to scale,

399
00:14:11,558 --> 00:14:12,940
um, so open search,

400
00:14:13,200 --> 00:14:15,500
the way it scales is it basically

401
00:14:15,500 --> 00:14:17,509
creates a search cluster. It runs across

402
00:14:17,509 --> 00:14:18,788
a whole bunch of servers,

403
00:14:19,139 --> 00:14:20,379
a whole bunch of instances.

404
00:14:20,798 --> 00:14:22,279
So think about billion,

405
00:14:22,609 --> 00:14:24,239
a billion vector corpus.

406
00:14:24,639 --> 00:14:26,349
It's distributing it,

407
00:14:26,639 --> 00:14:28,678
partitioning it across a whole bunch of

408
00:14:28,678 --> 00:14:29,460
servers,

409
00:14:29,918 --> 00:14:32,038
and we build these indexes and

410
00:14:32,038 --> 00:14:32,639
host them,

411
00:14:32,960 --> 00:14:34,399
provide security and management.

412
00:14:34,940 --> 00:14:37,219
Right across all these data partitions

413
00:14:37,219 --> 00:14:39,288
and these indexes when you run a search

414
00:14:39,288 --> 00:14:41,399
query, what's happening is in parallel

415
00:14:41,658 --> 00:14:44,168
it's traversing all these indexes

416
00:14:44,168 --> 00:14:46,570
across all these servers and that's how it's possible

417
00:14:46,570 --> 00:14:48,599
to deliver millisecond search queries

418
00:14:48,769 --> 00:14:50,619
on a billion scale vector data set.

419
00:14:51,989 --> 00:14:54,109
So that's managed clusters on the left hand

420
00:14:54,109 --> 00:14:56,149
side. Servius on the

421
00:14:56,149 --> 00:14:58,428
right. The architecture you can see is more fancy,

422
00:14:58,500 --> 00:15:00,590
but the con the concept in terms of how we're

423
00:15:00,590 --> 00:15:03,269
scaling vector search is the same right across

424
00:15:03,269 --> 00:15:03,859
servers.

425
00:15:04,149 --> 00:15:06,349
The only difference here is what you're seeing is the search

426
00:15:06,349 --> 00:15:08,808
and the indexing workloads are separated

427
00:15:09,349 --> 00:15:11,529
and storage and compute separated,

428
00:15:11,590 --> 00:15:13,710
and this is how it enables more auto

429
00:15:13,710 --> 00:15:14,450
scaling,

430
00:15:14,820 --> 00:15:16,450
um, capabilities, but

431
00:15:16,830 --> 00:15:17,908
same type of scaling.

432
00:15:20,259 --> 00:15:20,940
So

433
00:15:21,340 --> 00:15:23,340
great, we could scale, we've always been able

434
00:15:23,340 --> 00:15:25,940
to scale, but like I said, these indexes

435
00:15:25,940 --> 00:15:28,009
they require a lot of memory, a

436
00:15:28,009 --> 00:15:30,460
lot of compute, more so than traditional database

437
00:15:30,460 --> 00:15:32,879
indexes or traditional search indexes.

438
00:15:34,298 --> 00:15:36,489
I'm curious for the folks

439
00:15:36,489 --> 00:15:38,570
especially who don't have experience. I mean, what, what would

440
00:15:38,570 --> 00:15:40,519
you expect? I mean, do you,

441
00:15:41,178 --> 00:15:43,798
do you think that if I were to build

442
00:15:43,940 --> 00:15:46,379
a 1 billion scale index,

443
00:15:46,590 --> 00:15:48,729
who thinks it should take, shouldn't take

444
00:15:48,729 --> 00:15:49,700
more than half a day.

445
00:15:50,639 --> 00:15:52,769
Who thinks it shouldn't take more than half a day?

446
00:15:53,239 --> 00:15:55,320
OK, I probably gave it away. I

447
00:15:55,320 --> 00:15:57,440
probably gave it away, but yes, it takes

448
00:15:57,440 --> 00:15:58,719
more than half a day.

449
00:15:59,200 --> 00:16:01,529
Generally in the wild it takes

450
00:16:01,529 --> 00:16:02,038
days.

451
00:16:02,320 --> 00:16:04,440
So that's tough, right,

452
00:16:04,519 --> 00:16:06,599
in terms of productivity and your innovation

453
00:16:06,599 --> 00:16:08,440
velocity. Typically in the wild,

454
00:16:08,759 --> 00:16:10,879
lots, there's variables, but it typically

455
00:16:10,879 --> 00:16:11,899
takes days.

456
00:16:12,349 --> 00:16:14,418
And the catch is, is not just a one

457
00:16:14,418 --> 00:16:16,700
time thing. It's not like build the index, you know, OK, it's

458
00:16:16,700 --> 00:16:18,779
gonna take a couple of days, it's just a one time

459
00:16:18,779 --> 00:16:20,969
thing. There's a life cycle to it. These

460
00:16:20,969 --> 00:16:22,239
indexes are special.

461
00:16:22,580 --> 00:16:24,619
Um, you have to rebuild them more often than your

462
00:16:24,619 --> 00:16:26,139
traditional database index.

463
00:16:26,500 --> 00:16:28,658
So first of all, HNSW,

464
00:16:28,940 --> 00:16:32,090
it does degrade when the data changes. So you have an agentic

465
00:16:32,090 --> 00:16:34,099
app. It's very dynamic, right? The

466
00:16:34,099 --> 00:16:36,219
vectors, the content is changing all the

467
00:16:36,219 --> 00:16:38,349
time. Well, you're gonna have

468
00:16:38,349 --> 00:16:39,529
you, you gotta mitigate

469
00:16:40,190 --> 00:16:42,190
the search quality degradation and

470
00:16:42,190 --> 00:16:43,609
rebuild that index

471
00:16:44,269 --> 00:16:46,308
if you change that embedding model

472
00:16:46,308 --> 00:16:48,619
as well, it means you're gonna have to regenerate

473
00:16:48,619 --> 00:16:50,769
the vectors and rebuild that index.

474
00:16:51,389 --> 00:16:53,428
So you know when can that happen? You know,

475
00:16:53,509 --> 00:16:55,629
maybe you're switching between OpenAI

476
00:16:55,629 --> 00:16:57,109
and Tropic cohere.

477
00:16:57,548 --> 00:16:59,849
OK, you're gonna have to rebuild that index,

478
00:17:00,389 --> 00:17:00,989
um.

479
00:17:01,719 --> 00:17:03,879
These vendors, they

480
00:17:03,879 --> 00:17:05,959
produce multiple versions of

481
00:17:05,959 --> 00:17:08,039
their model every year that happens, you know, 1

482
00:17:08,039 --> 00:17:10,078
to 4 times I think typically per year.

483
00:17:10,279 --> 00:17:12,630
You wanna benefit from the latest and greatest,

484
00:17:12,959 --> 00:17:13,818
you're gonna have to

485
00:17:14,199 --> 00:17:15,729
update, rebuild your index,

486
00:17:16,118 --> 00:17:18,299
but it could be even more frequent than

487
00:17:18,299 --> 00:17:18,939
that,

488
00:17:19,640 --> 00:17:21,680
especially for special use cases, maybe like

489
00:17:21,680 --> 00:17:22,759
personalized search,

490
00:17:23,118 --> 00:17:25,509
taking an example from Amazon Music.

491
00:17:25,920 --> 00:17:28,279
So Amazon Music is a streaming

492
00:17:28,279 --> 00:17:29,180
service

493
00:17:29,608 --> 00:17:30,739
and when you see.

494
00:17:31,328 --> 00:17:32,588
If you've used it before,

495
00:17:32,930 --> 00:17:34,348
you may see, hey, um,

496
00:17:34,969 --> 00:17:37,209
because you listen to this song we

497
00:17:37,209 --> 00:17:39,328
think he may be interested in these other songs

498
00:17:39,328 --> 00:17:41,108
that is powered by vector search

499
00:17:41,608 --> 00:17:43,729
so they have this recommender type model

500
00:17:43,729 --> 00:17:45,868
it's built for vectors generates vectors

501
00:17:46,209 --> 00:17:48,250
from um their song

502
00:17:48,250 --> 00:17:50,630
selection, all the older music tracks.

503
00:17:51,449 --> 00:17:53,539
Over a billion vectors running

504
00:17:53,539 --> 00:17:55,539
an open search and they use

505
00:17:55,539 --> 00:17:58,479
that to power that rec those recommendations.

506
00:17:59,410 --> 00:18:00,519
If you're familiar with

507
00:18:00,959 --> 00:18:03,489
recommendations personalizations,

508
00:18:03,500 --> 00:18:04,799
it's important

509
00:18:05,140 --> 00:18:06,719
for you to continually

510
00:18:07,170 --> 00:18:09,618
retrain or fine tune those models

511
00:18:09,979 --> 00:18:12,059
on your users' latest behaviors

512
00:18:12,059 --> 00:18:14,140
and profiles if you wanna have the

513
00:18:14,140 --> 00:18:14,699
best recommendation.

514
00:18:15,140 --> 00:18:17,500
Foundations their case need to do it on a daily

515
00:18:17,500 --> 00:18:19,959
basis so there's these use cases

516
00:18:20,259 --> 00:18:22,259
you have to rebuild the indexes on

517
00:18:22,259 --> 00:18:24,459
a daily, weekly, or monthly basis

518
00:18:24,459 --> 00:18:26,618
depending on your use case. What happens if that

519
00:18:26,618 --> 00:18:29,098
takes days? It's gonna be tough making your SLA

520
00:18:29,098 --> 00:18:30,180
or it's gonna be a burden.

521
00:18:32,348 --> 00:18:33,068
Also

522
00:18:34,809 --> 00:18:37,130
For those you know on the managed cluster, your

523
00:18:37,130 --> 00:18:39,328
search and your indexing workloads are also

524
00:18:39,328 --> 00:18:41,338
running on that same infrastructure.

525
00:18:41,729 --> 00:18:44,209
Well, you have these graphs

526
00:18:44,529 --> 00:18:47,009
that take a lot of

527
00:18:47,009 --> 00:18:49,088
RAM, a lot of compute, and

528
00:18:49,088 --> 00:18:51,269
at the same time you're running search,

529
00:18:51,689 --> 00:18:53,949
there's a tug of war going on right

530
00:18:54,170 --> 00:18:55,670
for these resources.

531
00:18:56,539 --> 00:18:58,930
And there's gonna then be also the challenge

532
00:18:58,930 --> 00:19:01,160
right operationally to maintain good

533
00:19:01,160 --> 00:19:02,098
search times.

534
00:19:02,660 --> 00:19:04,160
So there's also that challenge,

535
00:19:04,420 --> 00:19:06,880
right, in terms of separation of workloads.

536
00:19:07,390 --> 00:19:09,439
So these are the problems that our customers are

537
00:19:09,439 --> 00:19:10,779
facing at large scale,

538
00:19:11,328 --> 00:19:13,880
uh, vector search systems.

539
00:19:14,400 --> 00:19:16,519
Building, maintaining them take days.

540
00:19:16,880 --> 00:19:18,259
It's tough to maintain,

541
00:19:18,640 --> 00:19:20,920
right? A vector ingestion can impact

542
00:19:20,920 --> 00:19:21,618
search speed.

543
00:19:22,170 --> 00:19:24,209
So we need to help our customers

544
00:19:24,209 --> 00:19:26,598
maintain their innovation velocity,

545
00:19:26,670 --> 00:19:27,989
their productivity,

546
00:19:28,489 --> 00:19:30,759
and build responsive

547
00:19:30,759 --> 00:19:31,289
applications.

548
00:19:33,108 --> 00:19:35,469
So we knew that one

549
00:19:35,469 --> 00:19:37,670
potential solution could be GPU,

550
00:19:37,789 --> 00:19:39,469
right? So we all know GPU

551
00:19:40,509 --> 00:19:42,750
from things like computer graphics,

552
00:19:42,828 --> 00:19:44,039
right, in gaming

553
00:19:44,390 --> 00:19:45,549
and AI

554
00:19:45,910 --> 00:19:47,939
and what's going on in those cases, it's

555
00:19:47,939 --> 00:19:49,689
a lot of vector math

556
00:19:49,949 --> 00:19:51,239
it's really good at that,

557
00:19:51,509 --> 00:19:53,979
so we knew that there's a lot of potential, so we worked,

558
00:19:54,229 --> 00:19:56,549
which is the same as what's going on with vector

559
00:19:56,549 --> 00:19:57,170
search

560
00:19:57,709 --> 00:19:59,088
and building these indexes

561
00:19:59,509 --> 00:20:01,229
so we worked with Nvidia.

562
00:20:01,828 --> 00:20:03,838
And Nvidia has this library

563
00:20:03,838 --> 00:20:06,390
called QVS bunch of vector

564
00:20:06,390 --> 00:20:08,088
algorithms optimized

565
00:20:08,630 --> 00:20:09,709
for GPU.

566
00:20:10,699 --> 00:20:11,328
And

567
00:20:11,650 --> 00:20:13,689
we worked with them to explore things like they

568
00:20:13,689 --> 00:20:15,459
had this algorithm called CAR.

569
00:20:16,358 --> 00:20:18,189
And so we looked at that.

570
00:20:18,858 --> 00:20:21,009
And we worked with them on enhancements as well

571
00:20:21,009 --> 00:20:23,170
because we wanted to be able to

572
00:20:23,170 --> 00:20:24,078
build on Kagara

573
00:20:24,848 --> 00:20:27,209
but we wanna run those indexes in

574
00:20:27,209 --> 00:20:29,469
RAM, right, and run search in RAM still

575
00:20:30,209 --> 00:20:31,828
so we can run other workloads.

576
00:20:32,739 --> 00:20:34,910
So we worked with them on enhancements in

577
00:20:34,910 --> 00:20:35,650
that area

578
00:20:35,949 --> 00:20:38,029
and we want it to run in face so build

579
00:20:38,029 --> 00:20:38,789
on CR,

580
00:20:39,189 --> 00:20:41,588
transfer to RAM, run within Meta's

581
00:20:41,588 --> 00:20:42,529
face libraries.

582
00:20:44,479 --> 00:20:46,598
With those enhancements, we also ran a whole bunch

583
00:20:46,598 --> 00:20:47,939
of tests that you see here,

584
00:20:48,400 --> 00:20:50,479
uh, data set sizes running from 1

585
00:20:50,479 --> 00:20:51,880
million, 10 million,

586
00:20:52,140 --> 00:20:54,318
100 to 1 billion different

587
00:20:54,318 --> 00:20:54,900
vector

588
00:20:55,318 --> 00:20:56,739
data set sizes.

589
00:20:57,660 --> 00:20:59,759
So we start off by creating first

590
00:20:59,900 --> 00:21:02,410
a baseline, right? Let's just run it on CPU.

591
00:21:02,969 --> 00:21:04,469
We build the index,

592
00:21:04,858 --> 00:21:06,900
uh, we run a something called a merge

593
00:21:06,900 --> 00:21:09,299
command if you're familiar with it, but that's important basically

594
00:21:09,299 --> 00:21:11,449
you build the index and with, uh,

595
00:21:11,459 --> 00:21:12,630
Lucine, right,

596
00:21:12,900 --> 00:21:15,059
you need to do a merge to ensure that the

597
00:21:15,059 --> 00:21:17,400
afterwards the index has high search

598
00:21:17,739 --> 00:21:18,400
performance.

599
00:21:19,239 --> 00:21:21,259
So we include that in the test and you can see

600
00:21:21,259 --> 00:21:23,348
here. It takes a long time. Once

601
00:21:23,348 --> 00:21:25,390
you get to the large scale, the 100

602
00:21:25,390 --> 00:21:27,509
to the 1 billion cases, it now takes over

603
00:21:27,509 --> 00:21:29,509
a day to perform that on

604
00:21:29,509 --> 00:21:30,789
a right size cluster.

605
00:21:31,568 --> 00:21:34,000
What happens when we add a GPU

606
00:21:34,000 --> 00:21:35,269
instances into the mix.

607
00:21:36,680 --> 00:21:37,939
Huge difference.

608
00:21:38,759 --> 00:21:41,640
See a speed up ranging from 6

609
00:21:41,640 --> 00:21:43,180
to 14 times.

610
00:21:44,459 --> 00:21:45,170
And

611
00:21:45,449 --> 00:21:46,588
there's also.

612
00:21:47,420 --> 00:21:49,539
The cost of building that index, the

613
00:21:49,539 --> 00:21:51,539
cost of that tax, uh, building

614
00:21:51,539 --> 00:21:52,868
that index of task

615
00:21:53,219 --> 00:21:55,299
is also much lower cost because it's

616
00:21:55,299 --> 00:21:57,039
doing it so much faster

617
00:21:57,299 --> 00:21:59,140
and so much more efficient

618
00:21:59,529 --> 00:22:01,699
even though we're, we have more infrastructure. Not

619
00:22:01,699 --> 00:22:03,900
only do we have a CPU infrastructure, but we

620
00:22:03,900 --> 00:22:04,519
added

621
00:22:04,818 --> 00:22:06,979
additional GPU infrastructure, so it's

622
00:22:06,979 --> 00:22:09,009
more, but because it does the task so

623
00:22:09,009 --> 00:22:11,088
much faster and so much more efficient,

624
00:22:11,420 --> 00:22:13,640
you can see cost savings from 6

625
00:22:13,640 --> 00:22:15,250
to 12x

626
00:22:16,180 --> 00:22:17,118
substantial.

627
00:22:17,479 --> 00:22:19,519
The cost savings are a little bit more um

628
00:22:19,519 --> 00:22:21,838
easier to understand from a servalist standpoint. Remember

629
00:22:21,838 --> 00:22:22,650
I said that

630
00:22:22,959 --> 00:22:25,318
the search and the indexing workloads

631
00:22:25,318 --> 00:22:26,920
are separated on serval list.

632
00:22:27,959 --> 00:22:30,078
So what that means is that when you get your bill from

633
00:22:30,078 --> 00:22:32,130
Servius, you actually see a line item that says

634
00:22:32,130 --> 00:22:34,039
these are your index this is your index cost.

635
00:22:34,338 --> 00:22:35,739
This is your search cost

636
00:22:36,118 --> 00:22:38,160
and in along with that cost

637
00:22:38,160 --> 00:22:40,750
it's gonna say, OK, you use this many OCU

638
00:22:40,750 --> 00:22:43,189
hours that's open search compute

639
00:22:43,189 --> 00:22:43,838
units.

640
00:22:44,199 --> 00:22:46,459
So with the same test, basically you're seeing

641
00:22:46,680 --> 00:22:48,910
let's look at the 113 million case

642
00:22:48,910 --> 00:22:50,750
1,024 dimensions.

643
00:22:51,118 --> 00:22:54,640
It's saying that you use 2,721

644
00:22:54,640 --> 00:22:56,769
OCU hours. That's a lot.

645
00:22:56,880 --> 00:22:58,959
It's basically a lot just to build that

646
00:22:58,959 --> 00:22:59,900
big index.

647
00:23:02,170 --> 00:23:04,410
Well, when we add GPU and

648
00:23:04,410 --> 00:23:06,489
I converted the GPU

649
00:23:06,489 --> 00:23:08,848
cost to OCU so it's kind of easy to

650
00:23:08,848 --> 00:23:09,390
understand

651
00:23:10,170 --> 00:23:12,549
again we're adding GPU infrastructure,

652
00:23:12,848 --> 00:23:15,130
but it's reducing the

653
00:23:15,130 --> 00:23:17,209
CPU utilization

654
00:23:17,209 --> 00:23:18,709
and again it's tremendous

655
00:23:19,009 --> 00:23:20,088
so if we look at the.

656
00:23:21,009 --> 00:23:23,170
113 million

657
00:23:23,170 --> 00:23:25,269
test case we brought that down

658
00:23:25,269 --> 00:23:27,769
to 104.5 OCUs

659
00:23:27,769 --> 00:23:29,269
to accomplish that task.

660
00:23:29,608 --> 00:23:31,189
That's 8.9 times

661
00:23:31,789 --> 00:23:33,848
um cost reduction to put that

662
00:23:33,848 --> 00:23:34,989
in hard numbers

663
00:23:35,689 --> 00:23:37,729
in US uh

664
00:23:37,729 --> 00:23:40,170
North Virginia, the cost for one OCU

665
00:23:40,170 --> 00:23:42,289
hour is 24 cents. If we do the

666
00:23:42,289 --> 00:23:44,150
math, basically we're going from.

667
00:23:44,719 --> 00:23:48,039
Um, was it $853

668
00:23:48,039 --> 00:23:50,358
to $73. That's huge. Now it's,

669
00:23:50,719 --> 00:23:52,838
now it's manageable, right? If you're at

670
00:23:52,838 --> 00:23:55,000
that scale you have to rebuild the indexes. Now

671
00:23:55,000 --> 00:23:55,709
the cost is,

672
00:23:56,000 --> 00:23:56,939
is manageable.

673
00:23:59,088 --> 00:24:01,279
The other thing I want to look at is.

674
00:24:02,680 --> 00:24:05,039
We want to simulate a dynamic

675
00:24:05,039 --> 00:24:07,400
application. Now I think we have agentic

676
00:24:07,400 --> 00:24:09,489
app. We have a whole bunch of concurrent

677
00:24:09,489 --> 00:24:11,318
users are searching for things,

678
00:24:11,650 --> 00:24:13,578
but at the same time they're also updating,

679
00:24:14,170 --> 00:24:16,170
inserting content, so we're, we're

680
00:24:16,170 --> 00:24:17,868
indexing at the same time.

681
00:24:18,729 --> 00:24:20,729
To set the search speed can be impacted. So

682
00:24:20,729 --> 00:24:22,809
this test is demonstrating that, right? We're doing

683
00:24:22,809 --> 00:24:23,789
a mixed workload,

684
00:24:24,410 --> 00:24:26,689
mixed workloads on the Y axis. This is

685
00:24:26,689 --> 00:24:29,000
indexing clients. Just think of that we're ramping

686
00:24:29,000 --> 00:24:29,509
up

687
00:24:29,769 --> 00:24:32,088
the number of writers, like the amount of

688
00:24:32,088 --> 00:24:34,209
updates and indexing operations on the

689
00:24:34,209 --> 00:24:34,828
system,

690
00:24:35,130 --> 00:24:37,209
and in the line what we see is expected

691
00:24:37,209 --> 00:24:39,250
the CPU utilization on a cluster

692
00:24:39,250 --> 00:24:40,170
is increasing.

693
00:24:40,989 --> 00:24:41,500
And

694
00:24:41,799 --> 00:24:43,959
the bar graph is showing that the

695
00:24:43,959 --> 00:24:46,000
search latency as a result is

696
00:24:46,000 --> 00:24:48,000
increasing because there's a pressure

697
00:24:48,358 --> 00:24:50,469
competition between CPU

698
00:24:50,469 --> 00:24:52,420
and RAM between the reason rights.

699
00:24:52,959 --> 00:24:55,358
When we add GPU and we offload

700
00:24:55,358 --> 00:24:56,279
the CPU,

701
00:24:56,920 --> 00:24:59,598
right, we bring down the CPU utilization

702
00:24:59,598 --> 00:25:01,578
and the search latency now becomes

703
00:25:02,118 --> 00:25:04,279
much more manageable, you know, now

704
00:25:04,279 --> 00:25:06,219
it's a much better user experience,

705
00:25:06,640 --> 00:25:07,660
you know, we're back to

706
00:25:07,969 --> 00:25:08,739
good latency.

707
00:25:09,989 --> 00:25:10,578
So

708
00:25:11,289 --> 00:25:12,689
We saw we proved it out,

709
00:25:13,049 --> 00:25:15,049
hey, a lot of potential for

710
00:25:15,049 --> 00:25:16,959
GPU, but there's one more challenge,

711
00:25:17,410 --> 00:25:19,219
exactly how are we gonna integrate

712
00:25:20,009 --> 00:25:22,209
GPU into open search?

713
00:25:23,108 --> 00:25:25,289
The obvious thing is, OK, we just

714
00:25:25,289 --> 00:25:27,559
support GPU instances, right? That's the obvious,

715
00:25:27,640 --> 00:25:29,299
that'll be really easy, but

716
00:25:29,920 --> 00:25:30,880
if we did that,

717
00:25:31,160 --> 00:25:32,618
that would cause

718
00:25:33,549 --> 00:25:35,759
economic problems. Let's take a look at an example.

719
00:25:35,920 --> 00:25:37,019
Let's say we did that.

720
00:25:37,309 --> 00:25:38,430
Let's take a look at a

721
00:25:38,709 --> 00:25:39,299
deployment

722
00:25:39,838 --> 00:25:42,959
for 1 billion vectors, 1,0024

723
00:25:42,959 --> 00:25:43,789
dimensions.

724
00:25:44,078 --> 00:25:46,199
We're gonna apply aggressive

725
00:25:46,199 --> 00:25:47,828
cost optimizations.

726
00:25:48,118 --> 00:25:50,118
We use binary quantization, basically

727
00:25:50,118 --> 00:25:51,759
32X compression.

728
00:25:52,259 --> 00:25:54,650
To make the cluster as small as possible

729
00:25:55,068 --> 00:25:56,250
on CPU.

730
00:25:57,439 --> 00:25:59,559
What that means is that we have, it's

731
00:25:59,559 --> 00:26:02,068
gonna be memory bound generally we need 3X,

732
00:26:02,118 --> 00:26:03,959
you know, more than a terabyte of RAM,

733
00:26:04,358 --> 00:26:05,420
right, for good performance.

734
00:26:06,529 --> 00:26:07,140
So

735
00:26:07,420 --> 00:26:09,858
we use 3 of these um

736
00:26:10,259 --> 00:26:12,439
R R R 8 instances.

737
00:26:13,519 --> 00:26:15,289
OK, so that's gonna cost some amount.

738
00:26:15,608 --> 00:26:17,930
So let's say if I wanted to bring GPU, I'm

739
00:26:17,930 --> 00:26:19,930
gonna have to pick a GPU instances in, you

740
00:26:19,930 --> 00:26:21,318
know, use GPU instances.

741
00:26:21,689 --> 00:26:23,910
We use the G6s because those are very

742
00:26:23,910 --> 00:26:26,209
cost effective, but they're not

743
00:26:26,489 --> 00:26:28,769
optimized for RAM capacity.

744
00:26:29,838 --> 00:26:31,920
So I need 6 of those now

745
00:26:31,920 --> 00:26:34,578
to meet the RAM requirements. That's 2.4

746
00:26:34,578 --> 00:26:35,719
times the cost.

747
00:26:37,150 --> 00:26:39,380
I got GPU now that's great, but

748
00:26:39,588 --> 00:26:41,578
it's, it's a huge premium

749
00:26:41,838 --> 00:26:43,068
for the speed gains

750
00:26:43,400 --> 00:26:45,479
and I'm not always indexing so that's the

751
00:26:45,479 --> 00:26:46,019
problem.

752
00:26:46,420 --> 00:26:48,519
So we knew that we for this to be

753
00:26:48,519 --> 00:26:50,519
practical for most of our customers we have

754
00:26:50,519 --> 00:26:51,920
to do things differently.

755
00:26:52,670 --> 00:26:55,250
So working backwards from the ideal scenario

756
00:26:55,250 --> 00:26:57,549
is wanna let you continue using

757
00:26:57,549 --> 00:26:59,549
your existing collections and domains

758
00:26:59,549 --> 00:27:01,549
with, you know, running on your CPU

759
00:27:01,549 --> 00:27:02,170
instances

760
00:27:02,789 --> 00:27:04,979
you wanna use the same APIs we don't wanna change

761
00:27:04,979 --> 00:27:06,130
any of that, but let's,

762
00:27:06,469 --> 00:27:08,180
can we bring the GPU

763
00:27:08,828 --> 00:27:11,088
to your CPU based clusters.

764
00:27:12,019 --> 00:27:13,160
Only when you need it,

765
00:27:13,578 --> 00:27:15,618
and you only pay for value, only when

766
00:27:15,618 --> 00:27:16,279
you benefit

767
00:27:16,939 --> 00:27:19,180
from the indexing, the GPU acceleration.

768
00:27:21,009 --> 00:27:23,170
And we did that, we did just that. Exactly

769
00:27:23,170 --> 00:27:25,250
how did we do it behind the scenes

770
00:27:25,449 --> 00:27:27,430
we built the system that you see here,

771
00:27:27,930 --> 00:27:28,868
so number one.

772
00:27:30,078 --> 00:27:32,318
That's you as a user. There's no change

773
00:27:32,318 --> 00:27:33,578
to how you

774
00:27:34,039 --> 00:27:35,160
use open search

775
00:27:35,469 --> 00:27:37,559
your indexing vectors, same thing, use the

776
00:27:37,559 --> 00:27:39,818
index API to reindex bulk.

777
00:27:41,660 --> 00:27:44,328
But what happens is when you have it enabled.

778
00:27:45,088 --> 00:27:47,088
And we detect that now you

779
00:27:47,088 --> 00:27:49,680
have high write throughput, let's say you're writing 10,000

780
00:27:49,680 --> 00:27:50,828
vectors per second,

781
00:27:51,250 --> 00:27:53,750
it's gonna trigger a configurable threshold.

782
00:27:54,489 --> 00:27:56,309
And we are then going to allocate

783
00:27:56,969 --> 00:27:58,430
GPU instances

784
00:27:58,848 --> 00:28:01,469
to your CPU cluster offload

785
00:28:01,729 --> 00:28:03,088
those graph builds.

786
00:28:05,009 --> 00:28:07,289
We behind the scenes are gonna

787
00:28:07,289 --> 00:28:09,529
manage a warm pool of GPU

788
00:28:09,529 --> 00:28:11,729
instances for you. You don't need to worry about

789
00:28:11,729 --> 00:28:14,439
that. Multi-tenant

790
00:28:14,439 --> 00:28:16,660
because you then benefit from the economies

791
00:28:16,660 --> 00:28:17,549
of scale,

792
00:28:17,959 --> 00:28:20,439
but we do a single tenant secure

793
00:28:20,439 --> 00:28:21,098
assignment,

794
00:28:21,799 --> 00:28:24,199
so that you know it's only your workloads

795
00:28:24,199 --> 00:28:25,699
running on those instances.

796
00:28:28,828 --> 00:28:30,989
We scale, basically scale automate

797
00:28:30,989 --> 00:28:33,029
the scaling for you when you're

798
00:28:33,029 --> 00:28:35,348
done with the GPU acceleration we

799
00:28:35,348 --> 00:28:36,588
return it to that pool

800
00:28:36,910 --> 00:28:38,989
so we do all that behind the scenes

801
00:28:38,989 --> 00:28:40,130
so that you only

802
00:28:40,910 --> 00:28:41,729
need to pay

803
00:28:42,509 --> 00:28:44,029
for what you benefit from.

804
00:28:46,939 --> 00:28:48,078
From your standpoint

805
00:28:48,380 --> 00:28:50,680
it's as simple as a light switch.

806
00:28:51,219 --> 00:28:53,890
You just enable it through the APIs,

807
00:28:53,900 --> 00:28:55,900
CLI through the console like you

808
00:28:55,900 --> 00:28:56,598
see here,

809
00:28:57,180 --> 00:28:58,759
and you pay

810
00:28:59,299 --> 00:29:00,009
it's serless.

811
00:29:02,019 --> 00:29:04,338
Doesn't matter if you're using managed clusters

812
00:29:04,618 --> 00:29:07,699
or open source erus this acceleration

813
00:29:07,699 --> 00:29:09,029
it's serless GPU

814
00:29:09,900 --> 00:29:12,219
and it's at 24

815
00:29:12,219 --> 00:29:14,699
cents basically per per OCU

816
00:29:14,699 --> 00:29:16,939
hour. In North Virginia,

817
00:29:17,068 --> 00:29:19,189
so, um, pretty much same

818
00:29:19,189 --> 00:29:21,289
as the standard indexing OCUs.

819
00:29:23,939 --> 00:29:26,439
So that's it, GPU accelerate vector indexing,

820
00:29:26,539 --> 00:29:28,539
building these indexes much faster, but there's

821
00:29:28,539 --> 00:29:29,358
actually more

822
00:29:30,019 --> 00:29:30,699
to it

823
00:29:31,180 --> 00:29:32,239
to create these,

824
00:29:32,699 --> 00:29:33,400
um,

825
00:29:34,019 --> 00:29:35,479
production systems.

826
00:29:36,750 --> 00:29:37,549
So

827
00:29:37,949 --> 00:29:38,789
with that.

828
00:29:40,380 --> 00:29:42,709
My colleague is gonna talk about,

829
00:29:43,459 --> 00:29:44,959
you know, what else we need to do.

830
00:29:47,150 --> 00:29:47,680
Thanks, Julian.

831
00:29:48,799 --> 00:29:50,838
So my colleague has covered about the basics

832
00:29:50,838 --> 00:29:53,180
of vector database, generating the vectors,

833
00:29:53,598 --> 00:29:56,439
ingesting the vectors, and building these indexes,

834
00:29:56,880 --> 00:29:58,939
so we can enable the real-time vector search

835
00:29:59,039 --> 00:30:00,180
for the end customers.

836
00:30:01,930 --> 00:30:02,469
But

837
00:30:04,170 --> 00:30:05,019
There is more to it.

838
00:30:05,739 --> 00:30:08,059
There's one more important aspect in this flow. Can

839
00:30:08,059 --> 00:30:08,660
anyone guess?

840
00:30:10,650 --> 00:30:12,108
It's the index configuration

841
00:30:12,368 --> 00:30:13,809
and optimizing the configuration.

842
00:30:14,789 --> 00:30:16,920
So to build any vector index,

843
00:30:17,309 --> 00:30:18,390
you need index configuration.

844
00:30:19,509 --> 00:30:22,209
This index configuration determines

845
00:30:22,549 --> 00:30:23,630
the search quality,

846
00:30:23,989 --> 00:30:25,608
cost and speed

847
00:30:25,949 --> 00:30:26,930
of your indexes,

848
00:30:27,338 --> 00:30:28,189
of your applications.

849
00:30:29,039 --> 00:30:30,779
And optimizing these indexes

850
00:30:31,250 --> 00:30:33,750
can cut down the cost by 1/3 of the total cost,

851
00:30:34,289 --> 00:30:36,289
and this is more evident as the scale of

852
00:30:36,289 --> 00:30:37,769
the vector workload increases.

853
00:30:38,519 --> 00:30:39,578
So let's

854
00:30:40,189 --> 00:30:41,759
take a look at the scale.

855
00:30:42,279 --> 00:30:44,358
So, on the X-axis, I have the number

856
00:30:44,358 --> 00:30:45,180
of vectors.

857
00:30:45,868 --> 00:30:47,890
On the y axis, I have the memory footprint.

858
00:30:49,390 --> 00:30:50,289
As you see,

859
00:30:50,910 --> 00:30:52,660
from 10 million onwards,

860
00:30:53,189 --> 00:30:55,709
the memory footprint reduction is almost 75%.

861
00:30:56,150 --> 00:30:58,949
So at 10 million, we almost see around 300

862
00:30:58,949 --> 00:31:01,289
gigabytes of memory footprint savings.

863
00:31:02,000 --> 00:31:03,130
At a billion scale,

864
00:31:03,400 --> 00:31:05,479
we are able to see around 3 terabytes of

865
00:31:05,479 --> 00:31:06,598
memory footprint savings.

866
00:31:07,279 --> 00:31:09,719
So it's very important to optimize this index configuration.

867
00:31:10,939 --> 00:31:12,989
Well, What are these index

868
00:31:12,989 --> 00:31:13,818
configurations?

869
00:31:16,140 --> 00:31:18,380
So Amazon Open Search Service

870
00:31:18,689 --> 00:31:20,890
has a rich feature set of index

871
00:31:20,890 --> 00:31:21,789
configurations

872
00:31:22,140 --> 00:31:23,578
and its tunable parameters.

873
00:31:24,410 --> 00:31:26,578
That lets our customers make a very smart

874
00:31:26,578 --> 00:31:27,358
trade-off between

875
00:31:27,739 --> 00:31:29,420
what should be my quality, cost,

876
00:31:29,699 --> 00:31:30,299
and speed.

877
00:31:31,630 --> 00:31:34,009
So on x-axis, we have the search quality,

878
00:31:34,348 --> 00:31:36,709
which we call recall. On the y-axis we have the latency.

879
00:31:37,949 --> 00:31:39,939
As we trade off between this quality

880
00:31:40,309 --> 00:31:41,239
and the latency,

881
00:31:41,500 --> 00:31:43,519
you could see those purple bubbles, all of those

882
00:31:43,519 --> 00:31:44,900
are index configurations,

883
00:31:45,318 --> 00:31:46,640
and each index configuration

884
00:31:47,009 --> 00:31:48,420
have attunable parameters.

885
00:31:50,019 --> 00:31:50,608
On a high level

886
00:31:50,989 --> 00:31:53,068
you can think of these tunable parameters into like 3

887
00:31:53,068 --> 00:31:55,229
categories. One category helps to determine

888
00:31:55,229 --> 00:31:57,529
the type of algorithm. Is it like HNSW,

889
00:31:57,709 --> 00:31:58,789
as Dylan talked about it?

890
00:31:59,068 --> 00:32:00,430
That's graph-based algorithm.

891
00:32:00,750 --> 00:32:03,108
And another one is IVF, which is a bucket-based

892
00:32:03,108 --> 00:32:03,809
algorithm.

893
00:32:04,348 --> 00:32:06,549
Similarly, the second kind of category is

894
00:32:06,549 --> 00:32:07,809
the compression techniques

895
00:32:08,150 --> 00:32:10,348
like do you need a binary quantization, scalar

896
00:32:10,348 --> 00:32:11,068
quantization,

897
00:32:11,348 --> 00:32:12,189
product quantization.

898
00:32:13,299 --> 00:32:15,400
Like you can achieve the compression from 2X all

899
00:32:15,400 --> 00:32:17,009
the way to 664X,

900
00:32:17,299 --> 00:32:18,279
which helps you trade off

901
00:32:18,858 --> 00:32:20,539
the quality for the memory footprint.

902
00:32:21,529 --> 00:32:23,799
Similarly, the third category talks about the mode

903
00:32:23,799 --> 00:32:26,049
of operating these graph data structures.

904
00:32:26,640 --> 00:32:27,348
Do you want them

905
00:32:27,729 --> 00:32:28,549
in memory

906
00:32:28,930 --> 00:32:30,250
or on disk

907
00:32:30,608 --> 00:32:33,229
or somewhere remote like S3 S3 vectors.

908
00:32:33,608 --> 00:32:35,578
It helps you trade off the latencies.

909
00:32:36,949 --> 00:32:38,848
Our customers love these trade-offs.

910
00:32:39,269 --> 00:32:39,930
It helps them

911
00:32:40,309 --> 00:32:42,500
get the best of their hardware resources,

912
00:32:42,880 --> 00:32:45,279
but. It's not straightforward,

913
00:32:45,529 --> 00:32:47,328
right? It needs some expertise

914
00:32:48,088 --> 00:32:50,630
in KNN vector search algorithms,

915
00:32:51,088 --> 00:32:51,989
and also

916
00:32:52,568 --> 00:32:53,318
it takes time.

917
00:32:55,108 --> 00:32:55,689
So,

918
00:32:56,150 --> 00:32:57,009
the experts

919
00:32:57,269 --> 00:32:58,568
would generally start with

920
00:32:59,068 --> 00:33:00,229
picking up the configuration.

921
00:33:00,890 --> 00:33:03,108
Algorithm configuration, the quantization technique,

922
00:33:03,289 --> 00:33:05,150
and the mode of operating these algorithms.

923
00:33:06,049 --> 00:33:07,118
And then you would run,

924
00:33:07,489 --> 00:33:09,509
you would build, build these indexes,

925
00:33:10,088 --> 00:33:11,949
have your ground truth, so you would evaluate

926
00:33:12,529 --> 00:33:13,529
what is my quality,

927
00:33:13,848 --> 00:33:14,608
latency.

928
00:33:15,430 --> 00:33:16,549
And then the cost

929
00:33:17,459 --> 00:33:19,549
And if it doesn't meet your requirements,

930
00:33:20,239 --> 00:33:21,959
We adjust and repeat.

931
00:33:22,729 --> 00:33:23,650
So this can take time.

932
00:33:24,420 --> 00:33:25,559
Let's take an example.

933
00:33:25,939 --> 00:33:27,979
So let's say I'm building an application which

934
00:33:27,979 --> 00:33:29,900
needs 95% accuracy.

935
00:33:31,660 --> 00:33:33,739
So I pick up a 32 X compression, the

936
00:33:33,739 --> 00:33:36,059
binary quantization is very popular.

937
00:33:36,739 --> 00:33:38,979
And I build my indexes, I run my quality

938
00:33:38,979 --> 00:33:41,078
check, and then I realize, oh, it's 90%,

939
00:33:41,219 --> 00:33:42,380
it's not 95%.

940
00:33:43,250 --> 00:33:43,890
So

941
00:33:44,250 --> 00:33:46,328
maybe I should reduce my compression level

942
00:33:46,328 --> 00:33:48,750
so I can preserve more vector precision

943
00:33:49,088 --> 00:33:50,289
and I can get better quality.

944
00:33:51,640 --> 00:33:53,618
Uh, but the trade-off is in the memory.

945
00:33:54,009 --> 00:33:56,029
I might need to pay more memory, so

946
00:33:56,170 --> 00:33:56,759
I'll go back,

947
00:33:57,130 --> 00:33:59,420
reduce my compression techniques to 16X,

948
00:33:59,848 --> 00:34:01,269
and then rerun the builds

949
00:34:01,729 --> 00:34:03,959
and then validate what is my recall quality.

950
00:34:04,250 --> 00:34:06,368
Maybe this time it's 93% but still not

951
00:34:06,368 --> 00:34:07,348
meeting my requirement.

952
00:34:07,650 --> 00:34:09,760
I go back and then make it 8X, so on and so

953
00:34:09,760 --> 00:34:11,769
forth. I repeat this process, build the

954
00:34:11,769 --> 00:34:13,728
index, and validate my configuration

955
00:34:14,050 --> 00:34:16,188
to check whether it meets my requirements or not.

956
00:34:17,637 --> 00:34:19,059
And one more important aspect.

957
00:34:20,217 --> 00:34:22,197
There is no one optimal configuration

958
00:34:22,739 --> 00:34:24,518
that works for all the use cases.

959
00:34:25,550 --> 00:34:26,829
The index configuration.

960
00:34:27,519 --> 00:34:29,800
Is determined is is more subjected

961
00:34:29,800 --> 00:34:32,418
to the data set and the business requirements.

962
00:34:32,878 --> 00:34:34,418
So let's, let's take an example.

963
00:34:34,840 --> 00:34:37,019
So let's say I'm building

964
00:34:37,360 --> 00:34:39,559
a recommendation systems for an e-commerce

965
00:34:39,559 --> 00:34:41,659
application. I need a high

966
00:34:41,659 --> 00:34:42,929
performance vector database.

967
00:34:43,329 --> 00:34:44,438
The quality has to be better.

968
00:34:44,898 --> 00:34:46,898
The latency has to be near single digit or

969
00:34:46,898 --> 00:34:48,179
double digit milliseconds.

970
00:34:49,309 --> 00:34:52,028
So I would need something like the hyperparameterss.

971
00:34:52,068 --> 00:34:53,298
It could be like in memory,

972
00:34:53,588 --> 00:34:55,668
and then I have a NSW algorithm and

973
00:34:55,668 --> 00:34:56,688
different parameters.

974
00:34:57,108 --> 00:34:59,447
And similarly, I have another application where

975
00:34:59,789 --> 00:35:01,809
I'm building a search

976
00:35:01,809 --> 00:35:03,568
application for my company internal

977
00:35:03,829 --> 00:35:04,927
for internal usage.

978
00:35:06,030 --> 00:35:08,148
I need quality, but I don't need a single digit

979
00:35:08,148 --> 00:35:08,668
millisecond.

980
00:35:09,418 --> 00:35:11,539
Even 100 millisecond or 200 milliseconds is

981
00:35:11,539 --> 00:35:13,760
fine. So then

982
00:35:13,760 --> 00:35:16,199
accordingly, I'll choose, choose my hyperparameterss

983
00:35:16,199 --> 00:35:17,099
configurations,

984
00:35:17,398 --> 00:35:19,519
or maybe you're trying to have something

985
00:35:19,519 --> 00:35:20,260
in between,

986
00:35:20,840 --> 00:35:21,978
right? So,

987
00:35:22,320 --> 00:35:24,628
arriving at these optimal configurations

988
00:35:24,958 --> 00:35:25,918
takes time.

989
00:35:28,000 --> 00:35:28,599
So

990
00:35:28,958 --> 00:35:30,340
how did we simplify this?

991
00:35:30,760 --> 00:35:32,699
So Amazon Open Search Service

992
00:35:33,280 --> 00:35:35,849
wants to simplify the onboarding experience.

993
00:35:36,360 --> 00:35:37,378
Whether you may be building.

994
00:35:38,530 --> 00:35:40,168
Apiboy or

995
00:35:41,010 --> 00:35:42,250
A large vector workload

996
00:35:42,530 --> 00:35:44,719
we need to we wanted to reduce the

997
00:35:44,719 --> 00:35:47,228
expertise needed to build and manage these indexes.

998
00:35:47,688 --> 00:35:49,769
So what we did is we built an auto optimization

999
00:35:49,769 --> 00:35:50,469
framework

1000
00:35:50,889 --> 00:35:53,599
and we let the framework learn all of these techniques,

1001
00:35:53,610 --> 00:35:55,688
all of these algorithms, parameters,

1002
00:35:56,050 --> 00:35:58,128
the tuning configurations, so we could do

1003
00:35:58,128 --> 00:35:59,090
the heavy lifting for you.

1004
00:35:59,898 --> 00:36:01,478
So now it gets simplified.

1005
00:36:02,139 --> 00:36:04,199
All a customer needs to provide is

1006
00:36:04,860 --> 00:36:06,800
what is my acceptable search quality

1007
00:36:07,059 --> 00:36:09,099
and what is my acceptable search

1008
00:36:09,099 --> 00:36:09,840
latencies.

1009
00:36:11,878 --> 00:36:13,139
And behind the scenes,

1010
00:36:13,719 --> 00:36:15,199
Amazon Open Search Service

1011
00:36:15,478 --> 00:36:17,478
will take your data, will take your business

1012
00:36:17,478 --> 00:36:19,478
requirements, analyze your vectors,

1013
00:36:20,000 --> 00:36:22,199
run a bunch of hyperparameter optimization

1014
00:36:22,199 --> 00:36:22,938
jobs,

1015
00:36:23,398 --> 00:36:24,860
and get you the recommendations.

1016
00:36:27,250 --> 00:36:28,349
And the good thing is

1017
00:36:28,610 --> 00:36:29,789
we also provide you

1018
00:36:30,289 --> 00:36:32,329
the, the, we are very transparent. We provide you

1019
00:36:32,329 --> 00:36:34,530
all the details, we provide you a detailed

1020
00:36:34,530 --> 00:36:36,128
report for each of the recommendation.

1021
00:36:36,789 --> 00:36:38,829
You can look at your performance metrics like what is

1022
00:36:38,829 --> 00:36:39,590
my recall,

1023
00:36:39,869 --> 00:36:41,010
what is my memory footprint,

1024
00:36:41,510 --> 00:36:43,789
you can look at your algorithms, algorithm

1025
00:36:43,789 --> 00:36:45,789
parameters, hyperparametters, compression

1026
00:36:45,789 --> 00:36:47,829
levels. So all the focus can now

1027
00:36:47,829 --> 00:36:49,050
be on the recommendations

1028
00:36:49,349 --> 00:36:51,228
and then you can pick up your best recommendation.

1029
00:36:53,090 --> 00:36:53,760
And

1030
00:36:54,688 --> 00:36:55,688
The best part

1031
00:36:56,510 --> 00:36:59,000
Our customers don't have to manage the infrastructure.

1032
00:36:59,809 --> 00:37:02,590
Amazon Open Search Service has a serverless

1033
00:37:02,969 --> 00:37:05,389
auto optimization framework. It's built on a serverless fleet

1034
00:37:05,728 --> 00:37:07,030
where we paralyze

1035
00:37:07,289 --> 00:37:09,289
all of our hyper optimization jobs.

1036
00:37:09,978 --> 00:37:11,340
And we build it under an hour.

1037
00:37:12,418 --> 00:37:14,628
Under 1 hour we are able to build the optimizations.

1038
00:37:14,869 --> 00:37:17,139
We are able to provide you the recommendations, optimizations

1039
00:37:17,139 --> 00:37:18,110
as recommendations.

1040
00:37:19,228 --> 00:37:20,628
So with this

1041
00:37:21,188 --> 00:37:23,199
one. We cut

1042
00:37:23,199 --> 00:37:24,668
off the expertise required.

1043
00:37:25,000 --> 00:37:27,079
All we need to do is to focus on the business

1044
00:37:27,079 --> 00:37:27,878
requirements.

1045
00:37:28,389 --> 00:37:30,429
2. We cut down

1046
00:37:30,429 --> 00:37:30,938
the time.

1047
00:37:31,228 --> 00:37:33,539
We are able to give your recommendations under

1048
00:37:33,539 --> 00:37:35,619
an hour, so all the iterations that you

1049
00:37:35,619 --> 00:37:37,469
need to do for a POC.

1050
00:37:38,019 --> 00:37:40,070
So we are doing it behind the scenes

1051
00:37:40,070 --> 00:37:42,918
for you. So under an hour you get your recommendations,

1052
00:37:43,269 --> 00:37:44,128
and 3,

1053
00:37:44,590 --> 00:37:45,809
they're cutting down the cost.

1054
00:37:46,349 --> 00:37:48,769
You don't have to manage the infrastructure

1055
00:37:49,340 --> 00:37:51,429
needed to run these experiments, the

1056
00:37:51,429 --> 00:37:53,349
POC, to get to the production.

1057
00:37:56,840 --> 00:37:57,728
Let's see it in action.

1058
00:38:07,869 --> 00:38:09,719
OK. So

1059
00:38:11,208 --> 00:38:13,590
Once you land on your open search service,

1060
00:38:17,500 --> 00:38:19,418
On the left hand side under the injection,

1061
00:38:19,750 --> 00:38:21,708
we added a new entry, vector injection.

1062
00:38:25,510 --> 00:38:26,679
For the purpose of the demo.

1063
00:38:28,039 --> 00:38:30,039
I'm going to play a role of a search engineer

1064
00:38:30,239 --> 00:38:31,418
who is trying to upgrade

1065
00:38:32,360 --> 00:38:33,820
application from a lexical search

1066
00:38:34,398 --> 00:38:36,159
to a semantic search like vector search.

1067
00:38:37,099 --> 00:38:39,489
So, I have the work, we have the workflow

1068
00:38:39,489 --> 00:38:40,378
explaining the process here.

1069
00:38:41,360 --> 00:38:42,860
Step one is preparing data set.

1070
00:38:43,760 --> 00:38:46,119
So I have all my documents. It could be text,

1071
00:38:46,228 --> 00:38:47,099
image, audio,

1072
00:38:47,438 --> 00:38:48,079
or video.

1073
00:38:49,530 --> 00:38:51,199
So we take all of the documents.

1074
00:38:51,938 --> 00:38:53,398
Pass it to the embedding models,

1075
00:38:53,659 --> 00:38:54,780
generate the vectors.

1076
00:38:55,389 --> 00:38:56,929
And have them in the S3 bucket.

1077
00:38:58,099 --> 00:38:59,110
Then step 2

1078
00:39:00,199 --> 00:39:01,539
I provide my business

1079
00:39:02,119 --> 00:39:02,820
requirements.

1080
00:39:03,239 --> 00:39:04,039
So I need

1081
00:39:04,309 --> 00:39:06,820
X search quality and Y latency,

1082
00:39:07,039 --> 00:39:08,260
so I provide the recommendations.

1083
00:39:08,760 --> 00:39:09,519
And step 3.

1084
00:39:10,929 --> 00:39:12,929
We would have the recommendation. I provide the requirements,

1085
00:39:12,938 --> 00:39:15,019
and step 3, we would have the recommendations

1086
00:39:15,019 --> 00:39:15,559
available.

1087
00:39:16,019 --> 00:39:16,918
And step 4.

1088
00:39:17,769 --> 00:39:19,309
I can choose one of the recommendations

1089
00:39:19,688 --> 00:39:21,128
and say build index.

1090
00:39:21,409 --> 00:39:23,409
So we ingest and accelerate the

1091
00:39:23,409 --> 00:39:24,349
building process.

1092
00:39:25,079 --> 00:39:26,179
With GPU acceleration.

1093
00:39:27,610 --> 00:39:28,550
So let's,

1094
00:39:29,010 --> 00:39:31,309
uh, let's do it, let's create one injection job.

1095
00:39:38,139 --> 00:39:40,208
So let's say when at step one,

1096
00:39:40,219 --> 00:39:42,418
we have to configure the data source and destination.

1097
00:39:42,820 --> 00:39:44,829
So my data source is an S3

1098
00:39:44,829 --> 00:39:45,599
bucket

1099
00:39:46,059 --> 00:39:47,159
and it's in

1100
00:39:47,728 --> 00:39:49,780
US East one. So within this, I

1101
00:39:49,780 --> 00:39:51,119
have the data folder.

1102
00:39:51,708 --> 00:39:52,800
So let me select my

1103
00:39:53,300 --> 00:39:54,478
uh folder.

1104
00:39:56,378 --> 00:39:58,530
And then I need to provide the destination details.

1105
00:39:58,659 --> 00:40:00,820
So this could be your Amazon open search domain

1106
00:40:00,820 --> 00:40:02,039
or Amazon open search

1107
00:40:02,438 --> 00:40:03,219
uh collection.

1108
00:40:04,159 --> 00:40:04,820
So,

1109
00:40:05,079 --> 00:40:07,340
let's, I'm going to pick up my domain.

1110
00:40:07,918 --> 00:40:09,780
So I'm going to pick up.

1111
00:40:11,530 --> 00:40:13,590
Of the domain and then you configure the permissions.

1112
00:40:15,500 --> 00:40:16,699
And then go to the 2nd step

1113
00:40:16,978 --> 00:40:18,059
as part of the 2nd step.

1114
00:40:19,030 --> 00:40:20,050
I configured my index.

1115
00:40:21,090 --> 00:40:21,760
So here,

1116
00:40:22,139 --> 00:40:24,059
I have my vector field named train.

1117
00:40:25,909 --> 00:40:28,070
And my space type is inner product,

1118
00:40:28,090 --> 00:40:29,708
and the dimension is 768.

1119
00:40:30,969 --> 00:40:33,070
So this particular thing has to match

1120
00:40:33,070 --> 00:40:34,989
with what we have in the S3 bucket.

1121
00:40:35,289 --> 00:40:37,289
So when we generate those embeddings from the

1122
00:40:37,289 --> 00:40:39,369
model. So I'm trying,

1123
00:40:39,449 --> 00:40:41,829
I'm, I'm putting the name of those vectors as train

1124
00:40:42,010 --> 00:40:44,128
and then having the embeddings there. So the mapping would be something

1125
00:40:44,128 --> 00:40:45,909
like train and then the vector

1126
00:40:46,289 --> 00:40:48,050
and the dimension of the vector is 768.

1127
00:40:48,780 --> 00:40:50,978
And the space type you could determine from the model

1128
00:40:50,978 --> 00:40:52,978
when you run it, so it tells you what is

1129
00:40:52,978 --> 00:40:53,619
the space type.

1130
00:40:55,449 --> 00:40:57,539
And then this is the best part of this. So here I

1131
00:40:57,539 --> 00:40:58,619
focus on the requirements.

1132
00:40:58,978 --> 00:41:00,119
So like I said, if I'm building

1133
00:41:00,780 --> 00:41:02,938
an application which needs a, which needs high

1134
00:41:02,938 --> 00:41:05,530
performance, I would choose something like 0.95

1135
00:41:05,530 --> 00:41:06,199
recall

1136
00:41:06,619 --> 00:41:07,219
and,

1137
00:41:07,579 --> 00:41:08,918
and how fast,

1138
00:41:09,340 --> 00:41:11,639
fast means within a single digit or under 50

1139
00:41:11,639 --> 00:41:12,878
milliseconds latencies.

1140
00:41:13,179 --> 00:41:14,699
So it's like a high performance application.

1141
00:41:15,179 --> 00:41:17,418
Similarly, if I want to build something like a chart

1142
00:41:17,418 --> 00:41:19,500
application, maybe this quality 90% quality

1143
00:41:19,500 --> 00:41:20,840
is good and it could be modest,

1144
00:41:21,099 --> 00:41:23,039
modest, so it's more like a balanced,

1145
00:41:23,300 --> 00:41:24,239
uh, profile.

1146
00:41:24,679 --> 00:41:26,679
So, accordingly, based on your

1147
00:41:26,679 --> 00:41:28,760
application, you could provide the requirements on

1148
00:41:28,760 --> 00:41:29,559
this page.

1149
00:41:32,489 --> 00:41:34,570
And, and then here you could review

1150
00:41:34,570 --> 00:41:36,329
your you can review the configuration.

1151
00:41:37,059 --> 00:41:38,139
And then click create.

1152
00:41:39,869 --> 00:41:40,530
So,

1153
00:41:41,070 --> 00:41:42,969
once you click on this button, so we are

1154
00:41:43,309 --> 00:41:45,309
sending a signal to our back end system.

1155
00:41:46,228 --> 00:41:48,679
To run the optimization, hyper optimization

1156
00:41:48,679 --> 00:41:50,860
jobs, hyperparameter optimization jobs,

1157
00:41:50,869 --> 00:41:52,179
and provide the recommendations.

1158
00:41:53,179 --> 00:41:54,280
So this would take

1159
00:41:55,099 --> 00:41:56,780
between 30 minutes to 1 hour.

1160
00:41:57,378 --> 00:41:58,780
So for the purpose of the demo,

1161
00:41:59,059 --> 00:42:01,179
I already had a job that is run, so I'll,

1162
00:42:01,228 --> 00:42:02,639
let me, uh, show the

1163
00:42:02,969 --> 00:42:04,239
recommendations from that job.

1164
00:42:05,079 --> 00:42:07,539
So once the job is complete, so you would have

1165
00:42:08,039 --> 00:42:10,409
a banner saying it's completed. It says that

1166
00:42:10,409 --> 00:42:12,378
you are in which step of the workflow.

1167
00:42:12,719 --> 00:42:14,809
So right now, I am at the recommendation step.

1168
00:42:16,050 --> 00:42:16,628
So

1169
00:42:16,889 --> 00:42:18,929
this is how my recommendations would look like. So

1170
00:42:18,929 --> 00:42:19,750
I have 3

1171
00:42:20,409 --> 00:42:21,849
recommended configurations.

1172
00:42:22,329 --> 00:42:24,228
It took 42 minutes to run this job,

1173
00:42:24,739 --> 00:42:25,840
and the,

1174
00:42:26,128 --> 00:42:28,418
the configurations, the business configuration

1175
00:42:28,418 --> 00:42:30,628
business requirements were 90% recall

1176
00:42:31,168 --> 00:42:31,889
and fast.

1177
00:42:32,898 --> 00:42:33,510
So

1178
00:42:34,570 --> 00:42:37,188
When we project our recommendations, we always

1179
00:42:37,329 --> 00:42:39,449
optimize them based on the cost, so we

1180
00:42:39,449 --> 00:42:40,829
sort them based on the cost.

1181
00:42:41,128 --> 00:42:42,168
The first recommendation.

1182
00:42:42,800 --> 00:42:44,179
Is a baseline for us.

1183
00:42:44,760 --> 00:42:46,958
So we're able to achieve 92

1184
00:42:46,958 --> 00:42:49,039
uh around uh yeah, 0.9

1185
00:42:49,039 --> 00:42:49,699
to recall.

1186
00:42:50,438 --> 00:42:52,449
And uh then between

1187
00:42:52,449 --> 00:42:54,360
option 2 and option 3, I'm able to get

1188
00:42:54,898 --> 00:42:57,019
more recall, but there is an increase in the memory

1189
00:42:57,019 --> 00:42:59,099
footprint. So this is the trade-off I

1190
00:42:59,099 --> 00:42:59,760
need to make

1191
00:43:00,099 --> 00:43:02,119
uh to get a 0.5%

1192
00:43:02,659 --> 00:43:03,878
uh better quality.

1193
00:43:04,719 --> 00:43:06,958
Will I, will I be OK to have 50%

1194
00:43:06,958 --> 00:43:07,739
memory footprint?

1195
00:43:08,280 --> 00:43:10,360
Or maybe I need 3% more better

1196
00:43:10,360 --> 00:43:12,719
recall, and I'm OK for having a 100%

1197
00:43:12,719 --> 00:43:13,378
memory footprint.

1198
00:43:14,148 --> 00:43:15,530
A 2X memory footprint.

1199
00:43:16,059 --> 00:43:16,610
So,

1200
00:43:17,070 --> 00:43:19,409
you can look at the performance metrics on this page.

1201
00:43:20,208 --> 00:43:22,329
And let's say you want to understand what

1202
00:43:22,329 --> 00:43:23,559
was actually the process,

1203
00:43:23,849 --> 00:43:25,889
what were the algorithms options that were picked

1204
00:43:25,889 --> 00:43:27,849
up behind each of these recommendations.

1205
00:43:28,750 --> 00:43:30,648
You can also look at the detailed report.

1206
00:43:31,699 --> 00:43:32,688
So you can click on the report.

1207
00:43:33,449 --> 00:43:35,289
So you can look at the engine parameters.

1208
00:43:36,039 --> 00:43:36,780
The mode

1209
00:43:37,079 --> 00:43:39,159
and the engine. So here we are picking up in memory

1210
00:43:39,159 --> 00:43:39,699
because

1211
00:43:40,079 --> 00:43:41,898
the business requirement was fast.

1212
00:43:42,438 --> 00:43:44,469
So to make it a single digit millisecond or

1213
00:43:44,469 --> 00:43:46,829
a double digit, we have to host the vector

1214
00:43:46,829 --> 00:43:48,519
data structures in the memory.

1215
00:43:49,610 --> 00:43:51,389
And then we are using the fires algorithm

1216
00:43:52,010 --> 00:43:54,469
and And files library

1217
00:43:54,469 --> 00:43:56,628
and the algorithm is HNSW. So here we, we

1218
00:43:56,628 --> 00:43:58,929
talk about all the hyperparametters that we picked up

1219
00:43:59,378 --> 00:44:01,289
and the value of the parameters

1220
00:44:01,550 --> 00:44:03,369
and what each parameter means.

1221
00:44:03,840 --> 00:44:05,949
So on a high level, HNSW has different

1222
00:44:05,949 --> 00:44:08,148
parameters like M EF construction,

1223
00:44:08,239 --> 00:44:09,030
EF search.

1224
00:44:09,840 --> 00:44:10,829
Which talks about

1225
00:44:11,128 --> 00:44:13,599
how many edges I need to establish between the nodes

1226
00:44:13,929 --> 00:44:16,168
or how many neighbors I need to consider

1227
00:44:16,168 --> 00:44:17,699
while I'm walking through the graph.

1228
00:44:18,128 --> 00:44:19,829
So the more the neighbors, the better the quality,

1229
00:44:20,250 --> 00:44:21,878
the more the edges, the better the quality,

1230
00:44:22,168 --> 00:44:24,030
but at the cost of memory footprint.

1231
00:44:24,610 --> 00:44:26,789
So these are all the evaluations we do behind the scenes

1232
00:44:26,889 --> 00:44:28,500
and come up with these parameters.

1233
00:44:28,889 --> 00:44:31,239
And then here we are using the binary quantization technique,

1234
00:44:31,409 --> 00:44:32,489
the 32X compression.

1235
00:44:33,329 --> 00:44:35,610
So essentially we are taking your dimension, which is 32

1236
00:44:35,610 --> 00:44:36,769
bit floating vector,

1237
00:44:37,050 --> 00:44:38,409
and making it to a single bit.

1238
00:44:39,989 --> 00:44:42,000
So likewise, I can go to different

1239
00:44:42,000 --> 00:44:42,898
recommendations

1240
00:44:43,559 --> 00:44:45,579
and then look at the configurations,

1241
00:44:46,110 --> 00:44:48,360
the engine parameters, algorithm parameters,

1242
00:44:48,519 --> 00:44:50,878
the compression parameters. So here it's using 16X

1243
00:44:50,878 --> 00:44:52,750
compression. So

1244
00:44:53,570 --> 00:44:55,760
So that's why, uh, the memory footprint has

1245
00:44:55,760 --> 00:44:58,050
been increased because previously the, the baseline

1246
00:44:58,050 --> 00:44:59,228
is using 32X,

1247
00:44:59,929 --> 00:45:00,750
but the second option,

1248
00:45:01,610 --> 00:45:03,610
we're able to recommend you the 16X compression

1249
00:45:03,610 --> 00:45:05,760
because it gives you around 0.3% better

1250
00:45:05,760 --> 00:45:08,500
recall. So

1251
00:45:08,820 --> 00:45:11,010
we look at all these options, the recommendations, and

1252
00:45:11,010 --> 00:45:13,090
for me and for my application,

1253
00:45:13,300 --> 00:45:15,619
uh, I'm, I'm willing to have option one

1254
00:45:15,978 --> 00:45:16,918
because my

1255
00:45:17,340 --> 00:45:19,539
accuracy requirement was 90%. This

1256
00:45:19,539 --> 00:45:21,539
is well above my requirement and uh

1257
00:45:21,539 --> 00:45:22,329
it is fast,

1258
00:45:22,619 --> 00:45:23,699
so I'm going with option one.

1259
00:45:24,329 --> 00:45:26,269
And then you click on the build index.

1260
00:45:27,869 --> 00:45:29,510
So as part of the index,

1261
00:45:30,110 --> 00:45:31,329
provide the name of the index.

1262
00:45:32,800 --> 00:45:34,840
So this is the index which we are going to

1263
00:45:34,840 --> 00:45:36,239
create on your destination.

1264
00:45:37,208 --> 00:45:39,289
So on the uh like in this case I have chosen

1265
00:45:39,289 --> 00:45:41,289
the Amazon open search domain. So

1266
00:45:41,289 --> 00:45:42,429
on that particular domain

1267
00:45:42,809 --> 00:45:44,628
I'm going to create this index

1268
00:45:45,090 --> 00:45:47,128
with the following configuration, the option one.

1269
00:45:47,489 --> 00:45:48,829
So finally, all the,

1270
00:45:49,369 --> 00:45:51,409
the configurations that we see in the report,

1271
00:45:51,849 --> 00:45:53,039
it boils down to the JSON,

1272
00:45:53,800 --> 00:45:55,929
uh, configuration. So this is how it looks

1273
00:45:55,929 --> 00:45:57,340
like it talks about the files.

1274
00:45:57,800 --> 00:45:59,829
It talks about this construction parameters

1275
00:45:59,929 --> 00:46:02,289
in our product. It talks about this compression

1276
00:46:02,289 --> 00:46:03,320
level in memory,

1277
00:46:03,648 --> 00:46:05,619
yeah. So we are able to finally get to the.

1278
00:46:06,378 --> 00:46:08,309
Jason, which OpenSearch can understand.

1279
00:46:09,668 --> 00:46:11,728
And let's say along with vector fields,

1280
00:46:12,039 --> 00:46:14,228
you also have other fields. Maybe you have

1281
00:46:14,228 --> 00:46:14,929
some metadata

1282
00:46:15,269 --> 00:46:17,429
like text, date, you can add

1283
00:46:17,429 --> 00:46:18,128
your fields here.

1284
00:46:18,829 --> 00:46:20,728
So let's say I have a field text.

1285
00:46:23,070 --> 00:46:24,148
So I'm going to

1286
00:46:24,510 --> 00:46:26,168
add a text field. Similarly,

1287
00:46:26,590 --> 00:46:27,628
I have a date field.

1288
00:46:31,208 --> 00:46:33,489
So on. So you can add your fields,

1289
00:46:33,500 --> 00:46:35,989
and this is, so when you're ingesting,

1290
00:46:36,340 --> 00:46:37,760
you look for these fields

1291
00:46:38,059 --> 00:46:39,780
on your documents in a 3 bucket.

1292
00:46:41,438 --> 00:46:42,079
And then

1293
00:46:43,059 --> 00:46:44,659
I can click build index.

1294
00:46:45,438 --> 00:46:47,840
The most important thing here is the GPU acceleration.

1295
00:46:49,699 --> 00:46:51,820
So, as my colleague talked about, so this

1296
00:46:51,820 --> 00:46:54,099
building these graphs are computer intensive operations.

1297
00:46:54,969 --> 00:46:57,570
So one part of it is ingesting into the destination,

1298
00:46:57,809 --> 00:46:59,849
but the second part is once it arrives at

1299
00:46:59,849 --> 00:47:01,590
the destination, the GPU acceleration

1300
00:47:02,289 --> 00:47:04,389
speeds up the building process,

1301
00:47:04,849 --> 00:47:06,889
so we can enable the search applications faster.

1302
00:47:07,628 --> 00:47:09,369
So you could enable from here.

1303
00:47:11,619 --> 00:47:13,469
So it takes you to the edit cluster page,

1304
00:47:13,860 --> 00:47:15,898
and then under the vector database features, we

1305
00:47:15,898 --> 00:47:18,059
have the option to enable or disable the acceleration.

1306
00:47:20,019 --> 00:47:22,219
So for this particular destination, I already had it

1307
00:47:22,219 --> 00:47:22,800
enabled.

1308
00:47:23,599 --> 00:47:24,139
So,

1309
00:47:24,559 --> 00:47:26,840
I can enable this, and then

1310
00:47:26,840 --> 00:47:27,648
click on Dryden.

1311
00:47:29,688 --> 00:47:31,780
So I need to choose a new generation instance

1312
00:47:31,780 --> 00:47:33,519
type. And that's it.

1313
00:47:34,510 --> 00:47:35,378
So the dry run,

1314
00:47:35,750 --> 00:47:37,938
it, it evaluates all the configurations,

1315
00:47:37,989 --> 00:47:40,099
just make sure that everything is correct, and then

1316
00:47:40,099 --> 00:47:42,148
it clicks the update process where

1317
00:47:42,148 --> 00:47:43,188
the flag gets enabled.

1318
00:47:44,378 --> 00:47:46,059
And then you can click on the build index.

1319
00:47:48,099 --> 00:47:48,688
So,

1320
00:47:48,978 --> 00:47:50,800
once we click on the build index,

1321
00:47:51,260 --> 00:47:52,918
what happened behind the scenes is

1322
00:47:53,800 --> 00:47:55,168
We kick off the ingestion workflow.

1323
00:47:55,878 --> 00:47:57,409
So we have an injection pipeline.

1324
00:47:58,128 --> 00:48:00,128
That takes the source, in

1325
00:48:00,128 --> 00:48:01,320
this case your S3 bucket,

1326
00:48:01,648 --> 00:48:02,320
the sink,

1327
00:48:02,728 --> 00:48:04,840
which is our destination, which is our domain or

1328
00:48:04,840 --> 00:48:07,010
a collection. And

1329
00:48:07,010 --> 00:48:07,969
then stream the data.

1330
00:48:08,699 --> 00:48:10,898
And the injection pipeline is also serverless,

1331
00:48:11,010 --> 00:48:11,699
so it can,

1332
00:48:11,978 --> 00:48:14,059
uh, based on your volume of your data

1333
00:48:14,059 --> 00:48:16,280
and based on how much your destination can,

1334
00:48:16,289 --> 00:48:18,599
can take, it can increase or decrease

1335
00:48:18,599 --> 00:48:19,958
the computing units.

1336
00:48:21,500 --> 00:48:23,300
And then once the injection is done,

1337
00:48:23,570 --> 00:48:25,780
so you can look at all the job configurations

1338
00:48:25,780 --> 00:48:27,639
here, you can go to the vector injection page.

1339
00:48:28,559 --> 00:48:30,378
And then look at all the completed jobs here.

1340
00:48:30,958 --> 00:48:33,289
So, For that optimization,

1341
00:48:33,369 --> 00:48:35,570
there are multiple states like completed and

1342
00:48:35,570 --> 00:48:37,030
then active

1343
00:48:37,369 --> 00:48:38,188
or failed

1344
00:48:38,449 --> 00:48:39,519
for the injection status.

1345
00:48:39,929 --> 00:48:42,199
Right now, I just, I, I just triggered the injection,

1346
00:48:42,250 --> 00:48:43,728
so it's in the creating state.

1347
00:48:44,389 --> 00:48:46,188
And then it becomes active.

1348
00:48:47,019 --> 00:48:49,300
And once the injection is completed,

1349
00:48:49,750 --> 00:48:50,829
we stop the pipeline.

1350
00:48:53,139 --> 00:48:53,659
So

1351
00:48:54,610 --> 00:48:55,309
With this,

1352
00:48:56,539 --> 00:48:57,309
We are able to,

1353
00:48:57,619 --> 00:48:58,688
within a few clicks,

1354
00:48:59,059 --> 00:48:59,958
we are able to

1355
00:49:00,300 --> 00:49:01,320
take your data,

1356
00:49:01,898 --> 00:49:03,378
take your business requirements.

1357
00:49:04,378 --> 00:49:06,619
Run the hyperparameter optimization jobs

1358
00:49:06,978 --> 00:49:07,639
under an hour.

1359
00:49:08,099 --> 00:49:09,398
Provide you the recommendations

1360
00:49:09,659 --> 00:49:11,679
so you can look at your recommendations

1361
00:49:12,099 --> 00:49:13,478
and choose the best recommendation.

1362
00:49:13,898 --> 00:49:15,239
And then within a single click

1363
00:49:15,619 --> 00:49:17,719
we are able to take the index configuration,

1364
00:49:18,139 --> 00:49:19,179
create an index,

1365
00:49:19,610 --> 00:49:21,519
and stream your data to the destination

1366
00:49:21,869 --> 00:49:24,458
all behind the scenes without having to, uh,

1367
00:49:24,659 --> 00:49:26,300
without any manual intervention.

1368
00:49:27,168 --> 00:49:29,409
So this is how without optimization

1369
00:49:29,688 --> 00:49:31,398
and the GPU acceleration features,

1370
00:49:31,849 --> 00:49:34,030
we had tried to simplify the onboarding experience

1371
00:49:34,369 --> 00:49:35,849
for Amazon Open Search Service.

1372
00:49:37,659 --> 00:49:39,378
So this concludes my demo.

1373
00:49:40,978 --> 00:49:43,340
I think we have some time to maybe take a few

1374
00:49:43,340 --> 00:49:45,409
questions a little bit early. What do you think?

1375
00:49:45,619 --> 00:49:46,489
Yeah, and

1376
00:49:46,889 --> 00:49:46,898
yeah.

