1
00:00:00,540 --> 00:00:02,070
- Good afternoon, everyone,

2
00:00:02,070 --> 00:00:06,390
and welcome to day one of re:Invent.

3
00:00:06,390 --> 00:00:08,730
We're very pleased to have you all join us

4
00:00:08,730 --> 00:00:13,020
to discuss training
high-performance AI models

5
00:00:13,020 --> 00:00:16,230
on AWS using SageMaker.

6
00:00:16,230 --> 00:00:21,230
I am Michael Oguike, I'm a
senior product manager on Amazon,

7
00:00:23,040 --> 00:00:25,830
and I work on AWS SageMaker

8
00:00:25,830 --> 00:00:29,580
where we help customers train
high-performance models.

9
00:00:29,580 --> 00:00:33,660
With me today is Tomonori Shimomura,

10
00:00:33,660 --> 00:00:36,510
who is a principal solutions architect

11
00:00:36,510 --> 00:00:38,100
who also works on SageMaker

12
00:00:38,100 --> 00:00:41,160
and helps customers train AI models.

13
00:00:41,160 --> 00:00:45,240
We're very pleased to be
joined by Denis Goupil,

14
00:00:45,240 --> 00:00:48,420
who's also a principal
machine learning scientist

15
00:00:48,420 --> 00:00:49,620
from Roblox,

16
00:00:49,620 --> 00:00:53,130
and they're gonna be sharing
an exciting use case with us.

17
00:00:53,130 --> 00:00:57,150
Today, we're gonna go through the need

18
00:00:57,150 --> 00:01:00,240
to train large AI models.

19
00:01:00,240 --> 00:01:02,070
We're gonna discuss the challenges

20
00:01:02,070 --> 00:01:05,340
that we see in training
these large models,

21
00:01:05,340 --> 00:01:08,160
and then we'll discuss how SageMaker helps

22
00:01:08,160 --> 00:01:10,380
to resolve these challenges.

23
00:01:10,380 --> 00:01:13,350
Then Tomonori would come back up,

24
00:01:13,350 --> 00:01:17,783
and show us an actual
demo followed by Roblox

25
00:01:18,630 --> 00:01:22,170
with Denis coming back up to
show us a real life example

26
00:01:22,170 --> 00:01:26,130
of how they've built a 4D
foundational model using SageMaker

27
00:01:26,130 --> 00:01:27,123
and HyperPod.

28
00:01:28,020 --> 00:01:30,660
Before we go on, by a quick show of hands,

29
00:01:30,660 --> 00:01:32,940
how many of us in the
audience have trained

30
00:01:32,940 --> 00:01:35,313
or customized a large model?

31
00:01:37,350 --> 00:01:41,223
Okay. And how many of you have
done that using SageMaker?

32
00:01:43,050 --> 00:01:43,883
Great.

33
00:01:43,883 --> 00:01:46,380
So there's gonna be a lot of capabilities

34
00:01:46,380 --> 00:01:49,080
that we'll share and a
lot to also learn here.

35
00:01:49,080 --> 00:01:52,260
But before we even go deeper,

36
00:01:52,260 --> 00:01:54,540
I think a couple of you may have noticed

37
00:01:54,540 --> 00:01:58,710
the growing popularity
of this sparkle icon.

38
00:01:58,710 --> 00:02:02,730
You may have seen it on your re:Invent app

39
00:02:02,730 --> 00:02:06,300
where it's helping you to try
to get more out of your week,

40
00:02:06,300 --> 00:02:08,520
or you may have noticed
it on your playlist

41
00:02:08,520 --> 00:02:11,730
where AI is trying to help
you create a better playlist,

42
00:02:11,730 --> 00:02:15,390
or even on autonomous
driverless cars on the road.

43
00:02:15,390 --> 00:02:18,030
For me, even while
building this presentation,

44
00:02:18,030 --> 00:02:20,310
as I was trying to crop photos,

45
00:02:20,310 --> 00:02:24,570
AI was there trying to help
me crop my photos better,

46
00:02:24,570 --> 00:02:29,570
but behind each of these
icons is a trained model,

47
00:02:30,060 --> 00:02:33,720
and as your customers and your
users are getting more used

48
00:02:33,720 --> 00:02:37,350
to using AI to improve their experience,

49
00:02:37,350 --> 00:02:41,640
you are also gonna be needing
to add more capable models

50
00:02:41,640 --> 00:02:43,620
to the experiences that
you already deliver

51
00:02:43,620 --> 00:02:44,673
for your customers.

52
00:02:45,720 --> 00:02:48,270
And in fact, we see this in studies.

53
00:02:48,270 --> 00:02:53,070
A study by McKinsey showed
that 47% of companies

54
00:02:53,070 --> 00:02:57,660
who said they were using
generative AI were also training

55
00:02:57,660 --> 00:03:00,030
or customizing these AI models

56
00:03:00,030 --> 00:03:03,060
to better serve their use cases.

57
00:03:03,060 --> 00:03:07,320
Now, we built SageMaker for AI

58
00:03:07,320 --> 00:03:11,190
as we've seen the trend moving
on forward for you to help

59
00:03:11,190 --> 00:03:13,743
to build more capable
models for your customers,

60
00:03:14,730 --> 00:03:17,490
it's important that we have
the right capabilities for you,

61
00:03:17,490 --> 00:03:22,260
our customers, to build and
train and deploy these models.

62
00:03:22,260 --> 00:03:24,960
We started this journey in 2017

63
00:03:24,960 --> 00:03:28,080
when we launched SageMaker training jobs.

64
00:03:28,080 --> 00:03:32,640
SageMaker training jobs
offers you a fully managed API

65
00:03:32,640 --> 00:03:36,030
where all you need to do is
bring in your training data,

66
00:03:36,030 --> 00:03:37,920
you bring in your training script,

67
00:03:37,920 --> 00:03:40,620
you tell us which instances you wanna run,

68
00:03:40,620 --> 00:03:42,690
and we take all that input,

69
00:03:42,690 --> 00:03:45,717
we spin up a cluster, we train the model,

70
00:03:45,717 --> 00:03:50,220
and we deliver the model
artifact to an S3 bucket.

71
00:03:50,220 --> 00:03:54,453
It's an ephemeral cluster and
you only pay for what you use.

72
00:03:55,500 --> 00:03:58,107
And our customers had a
great experience with that,

73
00:03:58,107 --> 00:04:00,360
and they continue to use training jobs.

74
00:04:00,360 --> 00:04:03,690
But then we had customers like you ask

75
00:04:03,690 --> 00:04:06,660
for more capabilities such as being able

76
00:04:06,660 --> 00:04:10,560
to manage the cluster
using Slurm or Kubernetes.

77
00:04:10,560 --> 00:04:14,820
We had customers asking
for more granular control

78
00:04:14,820 --> 00:04:18,090
and observability to these clusters.

79
00:04:18,090 --> 00:04:19,230
And there were also customers

80
00:04:19,230 --> 00:04:21,750
who wanted more persistent clusters

81
00:04:21,750 --> 00:04:24,690
so they could both run training jobs

82
00:04:24,690 --> 00:04:27,780
and inference jobs on
the same infrastructure.

83
00:04:27,780 --> 00:04:32,100
So we launched SageMaker
HyperPod to meet these needs.

84
00:04:32,100 --> 00:04:34,980
So in the conversation today,

85
00:04:34,980 --> 00:04:38,940
and the two key capabilities
that we have are one,

86
00:04:38,940 --> 00:04:42,240
SageMaker training jobs, which
is the ephemeral compute,

87
00:04:42,240 --> 00:04:45,660
or SageMaker HyperPod,
which is more persistent.

88
00:04:45,660 --> 00:04:48,810
These are the two key
services that SageMaker offers

89
00:04:48,810 --> 00:04:51,003
for you to train your models.

90
00:04:53,820 --> 00:04:55,650
As we've continued to work with you

91
00:04:55,650 --> 00:04:59,220
and other customers to learn
about what matters most to you

92
00:04:59,220 --> 00:05:04,110
for training large AI models,
we've seen six key dimensions.

93
00:05:04,110 --> 00:05:09,110
One, compute availability,
two, performance,

94
00:05:09,420 --> 00:05:13,410
then there's resiliency,
there's observability.

95
00:05:13,410 --> 00:05:18,410
Number five, ease of use,
and most importantly, cost.

96
00:05:18,960 --> 00:05:20,580
Every level that we move on each

97
00:05:20,580 --> 00:05:25,050
of these five dimensions has
a direct impact on your cost.

98
00:05:25,050 --> 00:05:27,060
And in the next couple of slides,

99
00:05:27,060 --> 00:05:29,820
I'm gonna dive deeper
into the first three,

100
00:05:29,820 --> 00:05:33,507
computer availability,
performance, and resiliency.

101
00:05:33,507 --> 00:05:35,430
And when Tomonori comes up,

102
00:05:35,430 --> 00:05:37,667
he's gonna go deeper into observability,

103
00:05:37,667 --> 00:05:41,373
and ease of use when we walk
through the demo together.

104
00:05:43,470 --> 00:05:46,140
With compute availability,

105
00:05:46,140 --> 00:05:48,780
let's first of all look at the trend

106
00:05:48,780 --> 00:05:52,050
that we've been observing
over the last couple of years.

107
00:05:52,050 --> 00:05:54,390
We've seen that over the
last couple of years,

108
00:05:54,390 --> 00:05:57,360
we've been needing to use more compute

109
00:05:57,360 --> 00:05:59,880
to train more capable models.

110
00:05:59,880 --> 00:06:01,920
This is primarily driven by the fact

111
00:06:01,920 --> 00:06:04,740
that we need more training data,

112
00:06:04,740 --> 00:06:08,670
and we're also training these
models with more parameters

113
00:06:08,670 --> 00:06:11,880
so they can be more capable,
they're more accurate,

114
00:06:11,880 --> 00:06:15,150
like using them in cases
like in the legal field

115
00:06:15,150 --> 00:06:18,330
or in the medical sciences
or in autonomous driving.

116
00:06:18,330 --> 00:06:21,300
We need bigger models
so they're more capable.

117
00:06:21,300 --> 00:06:23,940
Now, over the last two to three years,

118
00:06:23,940 --> 00:06:25,800
we've actually gotten to the point

119
00:06:25,800 --> 00:06:28,620
where we're now using 10 to the power

120
00:06:28,620 --> 00:06:31,770
of 24 FLOPs of compute power

121
00:06:31,770 --> 00:06:35,070
to train these very
highly performance models.

122
00:06:35,070 --> 00:06:39,840
And the scientific notation
for 10 to the power of 24 is,

123
00:06:39,840 --> 00:06:40,673
wait for it.

124
00:06:40,673 --> 00:06:44,400
It's a (indistinct) FLOP, but
let's make that more concrete.

125
00:06:44,400 --> 00:06:45,750
What's a (indistinct) FLOP?

126
00:06:47,580 --> 00:06:50,130
Five (indistinct) FLOPs is the equivalent

127
00:06:50,130 --> 00:06:55,130
of running 1000 p5s GPUs
consistently for one month.

128
00:06:58,320 --> 00:07:01,470
That's a lot of compute
power, that's very expensive,

129
00:07:01,470 --> 00:07:02,880
and that's why it's essential

130
00:07:02,880 --> 00:07:06,300
that we optimize the
utilization of these GPUs,

131
00:07:06,300 --> 00:07:09,450
but we also want access to
the most performance compute

132
00:07:09,450 --> 00:07:12,033
so we can get our training
jobs done quickly.

133
00:07:13,710 --> 00:07:18,710
To do this, SageMaker offers
multiple capabilities for you

134
00:07:19,740 --> 00:07:23,760
to get the best compute
whenever you want it.

135
00:07:23,760 --> 00:07:27,030
So we have a wide selection of your GPUs

136
00:07:27,030 --> 00:07:31,050
and accelerators from the H100 single GPUs

137
00:07:31,050 --> 00:07:33,960
to GP 200 ultra servers.

138
00:07:33,960 --> 00:07:35,670
You can get single GPU jobs

139
00:07:35,670 --> 00:07:39,150
or even a 72 GPU ultra server within Iraq

140
00:07:39,150 --> 00:07:40,980
depending on your needs.

141
00:07:40,980 --> 00:07:44,040
But that's one thing, selecting the GPU.

142
00:07:44,040 --> 00:07:46,950
We also understand that
your business needs

143
00:07:46,950 --> 00:07:48,720
and your timelines are different.

144
00:07:48,720 --> 00:07:51,840
So we have various options
to secure capacity,

145
00:07:51,840 --> 00:07:55,320
whether that's using on-demand capacity

146
00:07:55,320 --> 00:07:59,970
or you can get spot capacity
for up to 90% cheaper

147
00:07:59,970 --> 00:08:01,980
than on-demand capacity.

148
00:08:01,980 --> 00:08:05,790
And in fact, just last
week on SageMaker HyperPod,

149
00:08:05,790 --> 00:08:08,460
we launched support for spot instances,

150
00:08:08,460 --> 00:08:11,040
which is great for
fault tolerant workloads

151
00:08:11,040 --> 00:08:12,393
and experimentation.

152
00:08:13,260 --> 00:08:16,020
Or if you want a longer term reservation

153
00:08:16,020 --> 00:08:17,940
where you have guaranteed capacity,

154
00:08:17,940 --> 00:08:19,890
you can get a one year reservation

155
00:08:19,890 --> 00:08:22,800
or three-year reservation
with reserved capacity.

156
00:08:22,800 --> 00:08:24,960
And if you're looking
for something in between,

157
00:08:24,960 --> 00:08:27,960
where there's a calendar
period, say, next month,

158
00:08:27,960 --> 00:08:31,260
you want to get capacity for
a particular period of time,

159
00:08:31,260 --> 00:08:34,500
you can use the flexible training plans.

160
00:08:34,500 --> 00:08:37,470
Well, now that we've got GPUs,

161
00:08:37,470 --> 00:08:39,020
what are we gonna do with them?

162
00:08:39,870 --> 00:08:44,280
As important as it is to
get the compute capacity,

163
00:08:44,280 --> 00:08:48,660
it is even more important that
we utilize them effectively

164
00:08:48,660 --> 00:08:50,340
to avoid wastage.

165
00:08:50,340 --> 00:08:51,930
With SageMaker,

166
00:08:51,930 --> 00:08:56,250
you have the option to use
the training jobs managed API,

167
00:08:56,250 --> 00:08:59,280
where you just submit the job
and we handle the scheduling

168
00:08:59,280 --> 00:09:02,490
and the distribution of
the jobs across the GPUs.

169
00:09:02,490 --> 00:09:04,740
Or you can also use Slurm and Kubernetes

170
00:09:04,740 --> 00:09:08,430
depending on what your scientists
are more familiar with.

171
00:09:08,430 --> 00:09:11,580
Or if you have a team where
you have multiple team members

172
00:09:11,580 --> 00:09:13,590
and you have different type of jobs

173
00:09:13,590 --> 00:09:16,200
and you wanna prioritize
the different jobs,

174
00:09:16,200 --> 00:09:18,270
you wanna allocate capacity

175
00:09:18,270 --> 00:09:21,390
to different teams
within your organization,

176
00:09:21,390 --> 00:09:26,160
you can use a capability
we call task governance

177
00:09:26,160 --> 00:09:28,950
on HyperPod where you
can allocate resources

178
00:09:28,950 --> 00:09:32,400
to different teams or you
can give a higher preference

179
00:09:32,400 --> 00:09:35,700
to say, an inference
job over a training job.

180
00:09:35,700 --> 00:09:38,220
And whenever an inference job comes,

181
00:09:38,220 --> 00:09:40,230
it preempts the training job.

182
00:09:40,230 --> 00:09:43,260
We also have an integration with AWS batch

183
00:09:43,260 --> 00:09:44,820
that also allows you to do this

184
00:09:44,820 --> 00:09:47,133
with SageMaker training jobs.

185
00:09:48,690 --> 00:09:51,150
All these capabilities
come together to ensure

186
00:09:51,150 --> 00:09:53,580
that you have the right compute capability

187
00:09:53,580 --> 00:09:57,120
and you're able to optimize
utilization of these GPUs

188
00:09:57,120 --> 00:10:01,083
or accelerators, so you
reduce your cost for training.

189
00:10:02,700 --> 00:10:05,100
We then talk about performance.

190
00:10:05,100 --> 00:10:06,840
And to discuss performance,

191
00:10:06,840 --> 00:10:10,590
let's quickly demonstrate the relationship

192
00:10:10,590 --> 00:10:15,590
between the amount of memory
required by the parameters

193
00:10:15,633 --> 00:10:20,460
in a model compared to the
available memory per GPU

194
00:10:20,460 --> 00:10:24,480
for a lot of the latest
GPUs and accelerators.

195
00:10:24,480 --> 00:10:27,450
On this chart, we have two trends.

196
00:10:27,450 --> 00:10:30,480
There's one trend that's
showing the higher incline,

197
00:10:30,480 --> 00:10:32,820
which shows the number
of parameters in a lot

198
00:10:32,820 --> 00:10:36,690
of the popular models
over the last 10 years.

199
00:10:36,690 --> 00:10:39,240
And we can see, like we
talked about earlier,

200
00:10:39,240 --> 00:10:42,180
this growth in the number
of parameters required

201
00:10:42,180 --> 00:10:43,860
for these large models.

202
00:10:43,860 --> 00:10:46,320
On the other hand, on the gentler slope,

203
00:10:46,320 --> 00:10:51,320
we can see the slope on how
the memory per GPU is changing

204
00:10:51,930 --> 00:10:54,210
over time, and we can see that the number

205
00:10:54,210 --> 00:10:58,860
of parameters is growing
faster than the memory per GPU.

206
00:10:58,860 --> 00:11:00,690
Now, the implication of this really,

207
00:11:00,690 --> 00:11:03,030
we can demonstrate this with one example,

208
00:11:03,030 --> 00:11:06,273
say the Llama 3-70B model,

209
00:11:07,290 --> 00:11:10,710
the amount of GPU memory that we need

210
00:11:10,710 --> 00:11:13,530
to train a 70 billion parameter model,

211
00:11:13,530 --> 00:11:16,050
we can calculate that
by getting the amount

212
00:11:16,050 --> 00:11:18,600
of memory each parameter requires.

213
00:11:18,600 --> 00:11:22,680
And on average the rule of
thumb is like 18 to 20 bytes

214
00:11:22,680 --> 00:11:25,830
when you account for the weights

215
00:11:25,830 --> 00:11:28,800
and you account for the gradients
and the optimizer states,

216
00:11:28,800 --> 00:11:33,720
so we say 20 bytes times
70 billion parameters,

217
00:11:33,720 --> 00:11:37,980
that requires almost 1.4 terabytes

218
00:11:37,980 --> 00:11:41,640
of data of memory on the GPU.

219
00:11:41,640 --> 00:11:46,380
Now, even the NVIDIA H100
is 80 gigabytes of memory.

220
00:11:46,380 --> 00:11:48,990
So you do not have enough memory per GPU

221
00:11:48,990 --> 00:11:51,390
to load and train the model.

222
00:11:51,390 --> 00:11:54,270
So to overcome this scaling challenge,

223
00:11:54,270 --> 00:11:57,453
we then have to do what we
call distributed training.

224
00:11:58,350 --> 00:12:00,870
And some of you are
probably familiar with this,

225
00:12:00,870 --> 00:12:02,790
so I'll go over this quickly,

226
00:12:02,790 --> 00:12:07,140
but basically, for example,
in the Llama 70B model case

227
00:12:07,140 --> 00:12:12,140
where we have more parameters
and more memory from the model

228
00:12:13,920 --> 00:12:17,190
than we can put on one
GPU, what we then do is

229
00:12:17,190 --> 00:12:20,140
to shard the model
across the different GPUs

230
00:12:21,060 --> 00:12:23,967
and distribute the training
across the different GPUs.

231
00:12:23,967 --> 00:12:27,660
And as training happens
in step synchronously,

232
00:12:27,660 --> 00:12:31,140
the different GPUs can then
communicate across each other

233
00:12:31,140 --> 00:12:32,970
to update their state,

234
00:12:32,970 --> 00:12:36,300
but then the data is small enough for us

235
00:12:36,300 --> 00:12:39,150
to replicate the data
across the different GPUs.

236
00:12:39,150 --> 00:12:43,830
So each GPU is actually doing
a micro batch of the training

237
00:12:43,830 --> 00:12:46,800
and then updating states as they go along.

238
00:12:46,800 --> 00:12:49,800
And that's model parallelism
where we split the model,

239
00:12:49,800 --> 00:12:51,213
but we replicate the data.

240
00:12:52,140 --> 00:12:55,620
Conversely, if we have a
large training data set,

241
00:12:55,620 --> 00:12:57,210
but we have a small enough model,

242
00:12:57,210 --> 00:13:00,900
say, a Llama 3.21 billion parameter model

243
00:13:00,900 --> 00:13:03,600
that we can fit onto one H100,

244
00:13:03,600 --> 00:13:06,810
because again, it's 20 gigabytes of model

245
00:13:06,810 --> 00:13:09,810
versus 80 gigabytes of GPU memory.

246
00:13:09,810 --> 00:13:11,040
In that case,

247
00:13:11,040 --> 00:13:15,450
we can split the training data
across the different GPUs,

248
00:13:15,450 --> 00:13:18,240
replicate the model
across the different GPUs,

249
00:13:18,240 --> 00:13:20,010
do the training in batches,

250
00:13:20,010 --> 00:13:22,500
and have all the GPUs synchronized states

251
00:13:22,500 --> 00:13:24,963
as they train oversteps and epochs.

252
00:13:25,830 --> 00:13:29,250
One thing we might observe
here is at this point,

253
00:13:29,250 --> 00:13:32,370
the networking becomes very important

254
00:13:32,370 --> 00:13:34,770
because the GPUs have to update states

255
00:13:34,770 --> 00:13:37,653
and communicate with
one another over time.

256
00:13:38,880 --> 00:13:42,630
If we even step more complex

257
00:13:42,630 --> 00:13:46,830
where we're both distributing
the training data

258
00:13:46,830 --> 00:13:48,840
as well as sharding the model.

259
00:13:48,840 --> 00:13:50,970
We've sharded the model
across various layers

260
00:13:50,970 --> 00:13:51,960
in the neural network.

261
00:13:51,960 --> 00:13:54,780
We've also sharded the model
across different tensiles.

262
00:13:54,780 --> 00:13:57,660
So in this case, we're doing a hybrid

263
00:13:57,660 --> 00:14:02,580
of both data parallelism
and model parallelism.

264
00:14:02,580 --> 00:14:04,080
But what's most important here

265
00:14:04,080 --> 00:14:07,950
is that your networking can
quickly become a bottleneck.

266
00:14:07,950 --> 00:14:09,420
You need high bandwidth,

267
00:14:09,420 --> 00:14:12,630
low latency communication
between all the nodes to ensure

268
00:14:12,630 --> 00:14:14,790
that you're not spending
more time than you need

269
00:14:14,790 --> 00:14:15,790
on the training job.

270
00:14:16,890 --> 00:14:19,860
SageMaker offers multiple
capabilities to ensure

271
00:14:19,860 --> 00:14:22,380
that you're getting the
most performant high speed

272
00:14:22,380 --> 00:14:25,830
networking when you are
running these training jobs.

273
00:14:25,830 --> 00:14:28,590
First, we have an automated easy setup

274
00:14:28,590 --> 00:14:32,700
where you can use the
CLI, SDK, CloudFormation,

275
00:14:32,700 --> 00:14:36,600
or even the Console to set up
your distributed training job.

276
00:14:36,600 --> 00:14:39,750
We have access to multiple
popular frameworks

277
00:14:39,750 --> 00:14:43,140
like PyTorch, TensorFlow, or even Ray.

278
00:14:43,140 --> 00:14:45,450
And on the communication libraries,

279
00:14:45,450 --> 00:14:48,120
you can use NVIDIA NCCL,

280
00:14:48,120 --> 00:14:50,130
or if you're using Trainium instances,

281
00:14:50,130 --> 00:14:55,050
you can use the Neuron Collective
Communication libraries.

282
00:14:55,050 --> 00:14:56,400
And for high-speed networking,

283
00:14:56,400 --> 00:15:00,060
you have access to the NVLink
technology from NVIDIA,

284
00:15:00,060 --> 00:15:02,040
or for internet connectivity,

285
00:15:02,040 --> 00:15:06,870
you have access to Elastic
Fabric Adapter from Amazon

286
00:15:06,870 --> 00:15:11,870
where you can get up to
3,200 GPS of bandwidth.

287
00:15:12,120 --> 00:15:14,730
All these together help to ensure

288
00:15:14,730 --> 00:15:18,450
that you have an efficient
distributed training set up,

289
00:15:18,450 --> 00:15:21,720
where your GPUs and your
training jobs are connecting

290
00:15:21,720 --> 00:15:24,840
and moving at a very high
speed, which is essential

291
00:15:24,840 --> 00:15:27,813
for reducing your time to
market and being cost-efficient.

292
00:15:29,610 --> 00:15:32,103
Finally, we talk about resiliency.

293
00:15:33,120 --> 00:15:34,920
To discuss resiliency,

294
00:15:34,920 --> 00:15:37,653
we quickly bring back
our favorite single GPU.

295
00:15:38,550 --> 00:15:42,900
Each of these GPUs has
a mean time to failure.

296
00:15:42,900 --> 00:15:47,310
And when we scale these
GPUs to a large number,

297
00:15:47,310 --> 00:15:49,860
say, hundreds or thousands of GPUs,

298
00:15:49,860 --> 00:15:52,980
we then move from a GPU-level resiliency

299
00:15:52,980 --> 00:15:56,190
to a job-level resiliency

300
00:15:56,190 --> 00:15:59,970
because every failure from
every GPU in the cluster

301
00:15:59,970 --> 00:16:03,870
can potentially interrupt the
progress of your entire job.

302
00:16:03,870 --> 00:16:05,730
And we've actually seen this in research.

303
00:16:05,730 --> 00:16:08,730
So research from Meta that was published

304
00:16:08,730 --> 00:16:12,870
in IEEE early this year showed

305
00:16:12,870 --> 00:16:16,770
where they run an experiment
on two large state

306
00:16:16,770 --> 00:16:19,710
of the art clusters for about 11 months.

307
00:16:19,710 --> 00:16:21,960
And if you follow the lower line,

308
00:16:21,960 --> 00:16:26,960
they saw that for jobs
running with 1024 nodes,

309
00:16:28,200 --> 00:16:32,523
the average time to failure
was about eight hours.

310
00:16:33,420 --> 00:16:37,680
So that's three failures in
one day, one for breakfast,

311
00:16:37,680 --> 00:16:40,740
one for lunch, and one to
wake you up at two o'clock

312
00:16:40,740 --> 00:16:42,090
in the morning with a page.

313
00:16:43,050 --> 00:16:47,280
But how does SageMaker help
us resolve these issues

314
00:16:47,280 --> 00:16:49,113
and prevent us from getting paged?

315
00:16:50,430 --> 00:16:55,430
We have a four pronged strategy,
mitigation, prevention,

316
00:16:56,160 --> 00:16:58,890
detection, and recovery.

317
00:16:58,890 --> 00:17:02,580
And the way we achieve this
is basically with mitigation,

318
00:17:02,580 --> 00:17:04,860
we encourage and we make it easy for you

319
00:17:04,860 --> 00:17:06,540
to checkpoint your training.

320
00:17:06,540 --> 00:17:08,940
So you're taking multiple safe steps.

321
00:17:08,940 --> 00:17:12,000
So even if something goes wrong two days,

322
00:17:12,000 --> 00:17:13,590
one week into the future,

323
00:17:13,590 --> 00:17:15,900
you're not losing all
your training progress

324
00:17:15,900 --> 00:17:19,230
and we're able to restart
from a checkpoint.

325
00:17:19,230 --> 00:17:20,100
Sometime this year,

326
00:17:20,100 --> 00:17:24,420
we also launched
managed-tiered checkpointing,

327
00:17:24,420 --> 00:17:27,810
which basically reduces
the penalty of having

328
00:17:27,810 --> 00:17:29,190
to checkpoint frequently

329
00:17:29,190 --> 00:17:31,680
because there could be network penalties

330
00:17:31,680 --> 00:17:33,450
or even storage penalties.

331
00:17:33,450 --> 00:17:35,400
But with managed-tiered checkpointing,

332
00:17:35,400 --> 00:17:38,130
you have a more efficient,
faster checkpoint process,

333
00:17:38,130 --> 00:17:42,180
and you see that when
Tomonori also shows his demo.

334
00:17:42,180 --> 00:17:45,480
For prevention, we have
multiple types of health checks

335
00:17:45,480 --> 00:17:47,970
with standard health checks
or deep health checks

336
00:17:47,970 --> 00:17:51,720
where essentially before
we add a node to your job,

337
00:17:51,720 --> 00:17:53,070
we do a burning test

338
00:17:53,070 --> 00:17:55,830
and we really pressure
test these instances.

339
00:17:55,830 --> 00:17:59,250
So if there's actually a problem
with any of these instances

340
00:17:59,250 --> 00:18:02,730
or GPUs, we can isolate that
instance from the cluster

341
00:18:02,730 --> 00:18:05,400
so it never even makes
it into your cluster.

342
00:18:05,400 --> 00:18:08,700
For detection, we have automatic
health monitoring agents

343
00:18:08,700 --> 00:18:11,100
that are continually
running through the cluster

344
00:18:11,100 --> 00:18:12,540
and running through your job

345
00:18:12,540 --> 00:18:15,420
to identify any issues and resolve them.

346
00:18:15,420 --> 00:18:17,880
And then finally, for recovery,

347
00:18:17,880 --> 00:18:19,980
in the event that something does happen,

348
00:18:19,980 --> 00:18:23,250
we identify like there's
a slow GPU or a slow node,

349
00:18:23,250 --> 00:18:26,160
we can automatically reboot the node.

350
00:18:26,160 --> 00:18:28,020
Or if that doesn't resolve the issue,

351
00:18:28,020 --> 00:18:30,750
we will automatically replace the node

352
00:18:30,750 --> 00:18:33,690
and then restart the job
from your checkpoint.

353
00:18:33,690 --> 00:18:36,270
So once again, all these
capabilities come together

354
00:18:36,270 --> 00:18:39,570
and we've seen cases
where it saves up to 40%

355
00:18:39,570 --> 00:18:41,490
of the time spent on training,

356
00:18:41,490 --> 00:18:45,120
which essentially helps you
save on your overall cost

357
00:18:45,120 --> 00:18:48,840
because you're able to
respond to failures faster.

358
00:18:48,840 --> 00:18:53,100
So I would now invite
Tomonori on to show us a demo

359
00:18:53,100 --> 00:18:54,543
of these in action.

360
00:18:57,270 --> 00:18:59,890
- Thank you. Yeah, thank you, Michael.

361
00:19:03,570 --> 00:19:06,690
Am I audible? Yeah, thank you.

362
00:19:06,690 --> 00:19:08,730
So I'm Tomonori Shimomura,

363
00:19:08,730 --> 00:19:12,540
a solutions architect on
SageMaker AI organization

364
00:19:12,540 --> 00:19:16,140
that's helping SageMaker
AI customers every day.

365
00:19:16,140 --> 00:19:20,174
And now, Michael covered
the fundamental concept

366
00:19:20,174 --> 00:19:23,040
of SageMaker AI training capability.

367
00:19:23,040 --> 00:19:28,040
So let's dive deep into the
actual developer experience

368
00:19:28,230 --> 00:19:29,943
including demonstration.

369
00:19:31,680 --> 00:19:33,180
So as Michael explained,

370
00:19:33,180 --> 00:19:36,000
there are two training capabilities,

371
00:19:36,000 --> 00:19:38,463
SageMaker training jobs and HyperPod.

372
00:19:39,360 --> 00:19:44,360
And SageMaker training job
provides free managed APIs.

373
00:19:44,730 --> 00:19:49,470
So before executing the job,

374
00:19:49,470 --> 00:19:53,610
SageMaker training job
automatically creates a cluster

375
00:19:53,610 --> 00:19:56,520
and after the execution of the job,

376
00:19:56,520 --> 00:19:58,260
it automatically (indistinct) the cluster.

377
00:19:58,260 --> 00:20:01,980
So essentially the computer
resource is allocated only

378
00:20:01,980 --> 00:20:05,163
during the job execution, so
you can inspect a lower cost.

379
00:20:06,480 --> 00:20:10,380
And you can use high
(indistinct) Python SDK,

380
00:20:10,380 --> 00:20:14,610
and it's a kind of wrapper
Python library on top

381
00:20:14,610 --> 00:20:17,610
of SageMaker service API.

382
00:20:17,610 --> 00:20:22,170
And you can easily use
SageMaker training capability

383
00:20:22,170 --> 00:20:26,010
by writing Python code
or running notebooks.

384
00:20:26,010 --> 00:20:30,600
And because this is based on
the SageMaker service API,

385
00:20:30,600 --> 00:20:33,960
you can easily integrate the training job

386
00:20:33,960 --> 00:20:35,823
with your MLops pipeline.

387
00:20:37,350 --> 00:20:41,160
This is an example called
fragment of training job.

388
00:20:41,160 --> 00:20:43,500
And as you can see,

389
00:20:43,500 --> 00:20:47,070
this Python code is
importing a Python module

390
00:20:47,070 --> 00:20:52,070
called SageMaker core,
and instantiating, sorry,

391
00:20:52,680 --> 00:20:55,020
creating a training job object,

392
00:20:55,020 --> 00:20:57,510
bypassing some parameters
like a hyper parameter

393
00:20:57,510 --> 00:21:00,360
configuration instance
type, instance count,

394
00:21:00,360 --> 00:21:02,940
and container image, right?

395
00:21:02,940 --> 00:21:06,917
By providing these say,
parameters, you can define,

396
00:21:08,520 --> 00:21:12,540
oh say, configure the training
job, start the training job

397
00:21:12,540 --> 00:21:16,233
and automatically create
cluster internally.

398
00:21:19,950 --> 00:21:24,950
And another training capability
from SageMaker is HyperPod.

399
00:21:25,500 --> 00:21:28,290
And unlike SageMaker training job,

400
00:21:28,290 --> 00:21:31,590
HyperPod is for persistent clusters.

401
00:21:31,590 --> 00:21:34,740
So it means cluster
creation is not automatic,

402
00:21:34,740 --> 00:21:36,810
unlike SageMaker training job.

403
00:21:36,810 --> 00:21:40,110
And that you need to say,

404
00:21:40,110 --> 00:21:42,750
explicitly create and delete clusters,

405
00:21:42,750 --> 00:21:44,790
but the HyperPod helps you create

406
00:21:44,790 --> 00:21:48,750
such a high performance cluster easily.

407
00:21:48,750 --> 00:21:53,750
So you can customize the
cluster based on your needs

408
00:21:53,790 --> 00:21:58,110
and share the cluster with
your teammate, multiple users,

409
00:21:58,110 --> 00:22:01,950
and execute multiple
jobs on the same cluster.

410
00:22:01,950 --> 00:22:04,953
That's the difference from
the SageMaker training job.

411
00:22:06,930 --> 00:22:08,940
And as Michael said,

412
00:22:08,940 --> 00:22:11,910
SageMaker comes with advanced
resiliency capability.

413
00:22:11,910 --> 00:22:14,640
I'd like to explain a
little bit more later,

414
00:22:14,640 --> 00:22:19,640
and I also included some
demonstration in my say, a video.

415
00:22:22,950 --> 00:22:24,420
Yeah.

416
00:22:24,420 --> 00:22:26,340
And because SageMaker HyperPod

417
00:22:26,340 --> 00:22:29,520
is for AI development purpose, right?

418
00:22:29,520 --> 00:22:31,123
Not a normal cluster.

419
00:22:31,123 --> 00:22:33,810
So it's an AI development
purpose to cluster.

420
00:22:33,810 --> 00:22:36,780
So it comes with AI purpose,

421
00:22:36,780 --> 00:22:39,750
comprehensive observability dashboard,

422
00:22:39,750 --> 00:22:42,360
and also some useful libraries

423
00:22:42,360 --> 00:22:44,613
and tools to optimize your training.

424
00:22:47,070 --> 00:22:49,650
HyperPod supports two
orchestrator options.

425
00:22:49,650 --> 00:22:53,607
One is from, and another
is EKS or Kubernetes.

426
00:22:53,607 --> 00:22:57,210
EKS is Amazon's Elastic Kubernetes Service

427
00:22:57,210 --> 00:22:59,340
and it's actually Kubernetes, right?

428
00:22:59,340 --> 00:23:02,700
And both support,

429
00:23:02,700 --> 00:23:06,300
I think Slurm and the EKS
are both powerful framework

430
00:23:06,300 --> 00:23:08,820
to run distributed training,

431
00:23:08,820 --> 00:23:12,060
but developed by experience are different.

432
00:23:12,060 --> 00:23:16,380
So I'd like to explain how
they are different briefly.

433
00:23:16,380 --> 00:23:20,040
So Slurm is our open source
job orchestration framework,

434
00:23:20,040 --> 00:23:24,030
and you can basically run a same batch job

435
00:23:24,030 --> 00:23:25,890
on multiple nodes.

436
00:23:25,890 --> 00:23:27,150
Simple word, right?

437
00:23:27,150 --> 00:23:30,240
And in case of HyperPod,

438
00:23:30,240 --> 00:23:33,390
you will create three types of node,

439
00:23:33,390 --> 00:23:36,690
control (indistinct) node,
computer nodes, and a login node.

440
00:23:36,690 --> 00:23:39,930
Typically you log into log nodes

441
00:23:39,930 --> 00:23:43,350
and execute Slurm commands such as srun,

442
00:23:44,359 --> 00:23:48,030
srun (indistinct) for
sbatch, and SQ, et cetera,

443
00:23:48,030 --> 00:23:50,553
to learn from features.

444
00:23:52,320 --> 00:23:56,190
And another orchestrator
is EKS or Kubernetes.

445
00:23:56,190 --> 00:23:58,800
I think many people have had Kubernetes

446
00:23:58,800 --> 00:24:00,300
because it's popular, right?

447
00:24:00,300 --> 00:24:03,480
And Kubernetes is, let's
say framework to run,

448
00:24:03,480 --> 00:24:05,640
or containerized the applications,

449
00:24:05,640 --> 00:24:08,130
various types of
containerized applications

450
00:24:08,130 --> 00:24:11,940
including distributed
training applications.

451
00:24:11,940 --> 00:24:16,140
And unlike Slurm orchestration,

452
00:24:16,140 --> 00:24:19,680
in case of choosing Kubernetes,

453
00:24:19,680 --> 00:24:23,100
the HyperPod cluster only
contains computer nodes

454
00:24:23,100 --> 00:24:25,410
because controlled brain, say,

455
00:24:25,410 --> 00:24:28,050
EKS plays that load of control brain,

456
00:24:28,050 --> 00:24:32,340
HyperPod cluster itself doesn't
contain controlled brain

457
00:24:32,340 --> 00:24:33,870
or login nodes.

458
00:24:33,870 --> 00:24:35,940
And you can learn,

459
00:24:35,940 --> 00:24:38,790
say, Kubernetes clients
like a cube controller

460
00:24:38,790 --> 00:24:43,790
or your preferred Kubernetes client

461
00:24:43,950 --> 00:24:45,810
on your development machine,

462
00:24:45,810 --> 00:24:48,363
even on your local laptop, right?

463
00:24:49,290 --> 00:24:50,490
Yeah.

464
00:24:50,490 --> 00:24:53,550
So which is better choice for you?

465
00:24:53,550 --> 00:24:56,760
This is a tough question
because this is case by case.

466
00:24:56,760 --> 00:25:00,750
So if you already have a
preferred choice of orchestration,

467
00:25:00,750 --> 00:25:01,980
please choose it.

468
00:25:01,980 --> 00:25:06,660
And if you want to
directly access hardware,

469
00:25:06,660 --> 00:25:10,740
if you want to run application
on hostable directory,

470
00:25:10,740 --> 00:25:12,570
so that may be, right, the choice,

471
00:25:12,570 --> 00:25:15,690
if you love containerized,

472
00:25:15,690 --> 00:25:20,690
maybe Kubernetes or EKS
maybe the better choice.

473
00:25:22,800 --> 00:25:26,220
Yeah, I think Michael
already covered some portion

474
00:25:26,220 --> 00:25:28,770
of resiliency, but let me recap quickly

475
00:25:28,770 --> 00:25:31,710
before going to the actual demonstration.

476
00:25:31,710 --> 00:25:33,660
So HyperPod resiliency,

477
00:25:33,660 --> 00:25:37,560
so resiliency consists
of three steps, right?

478
00:25:37,560 --> 00:25:41,340
Health monitoring, instance replacement,

479
00:25:41,340 --> 00:25:42,660
the job auto-resuming.

480
00:25:42,660 --> 00:25:46,410
So these three are the,
let's say, core, let's say,

481
00:25:46,410 --> 00:25:48,450
flow of HyperPod resiliency.

482
00:25:48,450 --> 00:25:50,850
So HyperPod constantly health checks.

483
00:25:50,850 --> 00:25:54,840
And if we find some GPU 40 instance,

484
00:25:54,840 --> 00:25:57,030
we triggered instance replacement,

485
00:25:57,030 --> 00:25:59,010
and after the instance replacement,

486
00:25:59,010 --> 00:26:01,290
they suspended the job
automatically (indistinct).

487
00:26:01,290 --> 00:26:02,690
We provide such a framework.

488
00:26:05,460 --> 00:26:08,940
Yeah, so we directed to two demos.

489
00:26:08,940 --> 00:26:10,830
I captured pre-recorded videos,

490
00:26:10,830 --> 00:26:13,050
and partially I accelerated video

491
00:26:13,050 --> 00:26:16,590
because in order to fit
this limited time session.

492
00:26:16,590 --> 00:26:19,980
So starting from SageMaker training jobs,

493
00:26:19,980 --> 00:26:24,360
and I'm going to open
a code data on browser.

494
00:26:24,360 --> 00:26:27,600
SageMaker has a code, say,
code data browser by the way.

495
00:26:27,600 --> 00:26:31,500
And I execute a notebook
in order to trigger

496
00:26:31,500 --> 00:26:32,790
that training job.

497
00:26:32,790 --> 00:26:35,850
And eventually we will
see the execution result

498
00:26:35,850 --> 00:26:38,373
on training metrics by MLflow.

499
00:26:40,260 --> 00:26:45,260
Yeah, so firstly I'm opening
Code Editor on SageMaker

500
00:26:45,780 --> 00:26:47,013
of clicking open bottom,

501
00:26:48,120 --> 00:26:50,070
and it looks like say, visual studio.

502
00:26:50,070 --> 00:26:52,050
I said this is based on
the open source version

503
00:26:52,050 --> 00:26:54,570
of visual studio, code editor.

504
00:26:54,570 --> 00:26:57,660
And checking, browsing the training code.

505
00:26:57,660 --> 00:27:01,863
This is the code actually
executed on computer resource.

506
00:27:02,880 --> 00:27:04,860
And I'm executing notebook.

507
00:27:04,860 --> 00:27:07,170
And the first half of this notebook

508
00:27:07,170 --> 00:27:12,170
is for data pre-processing
and uploading to S3 bucket.

509
00:27:12,210 --> 00:27:14,163
This is a preparation step.

510
00:27:17,070 --> 00:27:21,570
Yeah, visualizing the training data set

511
00:27:21,570 --> 00:27:25,983
and configuring some
parameters for training job,

512
00:27:26,880 --> 00:27:28,023
uploading S3.

513
00:27:31,213 --> 00:27:32,046
Okay.

514
00:27:34,350 --> 00:27:38,613
And then creating or
configuring the training job.

515
00:27:39,706 --> 00:27:42,873
(audience chattering)

516
00:27:45,480 --> 00:27:46,313
Yeah.

517
00:27:50,610 --> 00:27:53,790
Yeah, so you can see
a model trainer class,

518
00:27:53,790 --> 00:27:55,890
I mean instant setting model trainer class

519
00:27:55,890 --> 00:27:57,363
with some parameters.

520
00:27:58,410 --> 00:27:59,887
This is the Python API.

521
00:28:02,280 --> 00:28:06,240
And then actually say,
configuring input channels,

522
00:28:06,240 --> 00:28:09,813
the input data channels
and starting the training.

523
00:28:10,710 --> 00:28:13,340
And you can also see the list

524
00:28:13,340 --> 00:28:16,860
of SageMaker training job on
SageMaker management console

525
00:28:16,860 --> 00:28:17,693
like this.

526
00:28:17,693 --> 00:28:20,700
So now it's showing training status,

527
00:28:20,700 --> 00:28:25,700
training is in progress, and
the progress can be also seen

528
00:28:26,310 --> 00:28:30,330
by visiting crowd of watch
logs management console.

529
00:28:30,330 --> 00:28:32,763
You can see output from
(indistinct), right?

530
00:28:36,900 --> 00:28:39,120
Yeah, so there are some options,

531
00:28:39,120 --> 00:28:40,380
how to monitor the progress.

532
00:28:40,380 --> 00:28:45,380
You can see CloudWatch logs,
you can use also a notebook

533
00:28:45,540 --> 00:28:49,140
by interacting, by calling
some method of Python object,

534
00:28:49,140 --> 00:28:51,390
you can get the log output as well.

535
00:28:51,390 --> 00:28:53,700
And also after this, I will show,

536
00:28:53,700 --> 00:28:56,400
but you can also check MLflow out of,

537
00:28:56,400 --> 00:28:59,493
or your preferred
observability solution as well.

538
00:29:00,533 --> 00:29:03,210
So SageMaker has MLflow,
manage the MLflow.

539
00:29:03,210 --> 00:29:06,707
So I click the MLflow button
and opening the MLflow,

540
00:29:07,920 --> 00:29:12,447
and I'm seeing a list of
executed training jobs

541
00:29:12,447 --> 00:29:16,053
and you can see training
metrics like this.

542
00:29:21,840 --> 00:29:26,670
Yep, so next demo is SageMaker HyperPod.

543
00:29:26,670 --> 00:29:31,670
And in this demo, I'm going
to create a cluster easily

544
00:29:32,220 --> 00:29:37,080
and configuring, so
customizing the cluster

545
00:29:37,080 --> 00:29:42,080
by installing some add-ons
and learning some training job

546
00:29:42,510 --> 00:29:45,780
and I'm going to, so in this demo,

547
00:29:45,780 --> 00:29:50,780
I did say simulating some
hardware failure artificially

548
00:29:50,790 --> 00:29:53,040
by injecting (indistinct) simulated error

549
00:29:53,040 --> 00:29:56,040
and verify that HyperPod resiliency works,

550
00:29:56,040 --> 00:29:59,373
and eventually see the
observability dashboard.

551
00:30:02,370 --> 00:30:06,273
Yeah, so you can easily create
a SageMaker HyperPod cluster.

552
00:30:07,290 --> 00:30:09,210
And in case of quick setup,

553
00:30:09,210 --> 00:30:13,500
what you have to do is just
choose right instance type

554
00:30:13,500 --> 00:30:16,443
and instance count,
basically that's it, right?

555
00:30:17,550 --> 00:30:21,030
So cluster creation can be
complex if you do manually,

556
00:30:21,030 --> 00:30:25,443
but we provide such an
easy setup experience.

557
00:30:26,520 --> 00:30:30,300
And actual cluster creation
progress can be monitored

558
00:30:30,300 --> 00:30:34,110
on cloud formation management console

559
00:30:34,110 --> 00:30:37,500
because it requires not
only cluster itself,

560
00:30:37,500 --> 00:30:40,950
but also require various politic resources

561
00:30:40,950 --> 00:30:45,950
such as VPC or subnet
security group, S3 bucket,

562
00:30:46,470 --> 00:30:50,280
and (indistinct), et cetera, right?

563
00:30:50,280 --> 00:30:54,540
And when HyperPod cluster
is getting created,

564
00:30:54,540 --> 00:30:57,690
we can visit SageMaker management console

565
00:30:57,690 --> 00:31:00,840
and see the instance
initialization progress

566
00:31:00,840 --> 00:31:02,040
at the instance level.

567
00:31:02,040 --> 00:31:06,240
Now, so yeah, I think you
just saw a deep health check

568
00:31:06,240 --> 00:31:08,073
was being executed.

569
00:31:08,940 --> 00:31:11,730
Now, let's install some add-ons starting

570
00:31:11,730 --> 00:31:14,940
with HyperPod observability add-on.

571
00:31:14,940 --> 00:31:16,650
It's just literally one click.

572
00:31:16,650 --> 00:31:20,310
But although I'm enabling
some additional say,

573
00:31:20,310 --> 00:31:23,040
advanced metrics, so it's not one click,

574
00:31:23,040 --> 00:31:25,410
but I am literally, well, say,

575
00:31:25,410 --> 00:31:28,200
one click in case of basic installation.

576
00:31:28,200 --> 00:31:32,070
And then enabling
HyperPod task governance.

577
00:31:32,070 --> 00:31:36,990
This is for, I say, task prioritization,

578
00:31:36,990 --> 00:31:40,020
and a dynamic computer quota allocation

579
00:31:40,020 --> 00:31:41,760
between multiple teams.

580
00:31:41,760 --> 00:31:45,210
This is useful in case of, how to say,

581
00:31:45,210 --> 00:31:49,770
sharing across between multiple
teams and multiple purposes.

582
00:31:49,770 --> 00:31:53,400
So I'm configuring the, I say, priorities,

583
00:31:53,400 --> 00:31:55,890
job priorities depending on the task types

584
00:31:55,890 --> 00:31:59,670
like influencing,
training, or experiments.

585
00:31:59,670 --> 00:32:04,260
And I'm also configuring two
teams, team A and team B,

586
00:32:04,260 --> 00:32:08,970
and defining eight instances
as a default quota,

587
00:32:08,970 --> 00:32:13,950
but allowing landing and
borrowing compute resource

588
00:32:13,950 --> 00:32:15,510
between teams.

589
00:32:15,510 --> 00:32:19,950
And then I'm configuring
training job, right?

590
00:32:19,950 --> 00:32:23,880
So as you can see in the second
line, HyperPod training job,

591
00:32:23,880 --> 00:32:28,410
it's custom resource definition installed

592
00:32:28,410 --> 00:32:30,720
by a HyperPod training operator

593
00:32:30,720 --> 00:32:32,973
and I'm using it for advanced training.

594
00:32:34,800 --> 00:32:36,910
Installing additional software

595
00:32:37,860 --> 00:32:41,790
and modifying checkpointing code in order

596
00:32:41,790 --> 00:32:44,970
to enable manage the
(indistinct) checkpointing.

597
00:32:44,970 --> 00:32:46,353
I will explain more later.

598
00:32:49,560 --> 00:32:53,860
Yeah, so using some SageMaker
specific Python classes

599
00:32:55,200 --> 00:32:58,170
for checkpointing code.

600
00:32:58,170 --> 00:33:01,710
Now, on time now, I'm
monitoring three information,

601
00:33:01,710 --> 00:33:04,710
top left, I'm monitoring the node status,

602
00:33:04,710 --> 00:33:06,840
top right, portal status,

603
00:33:06,840 --> 00:33:11,670
and center is log out to
put from training job,

604
00:33:11,670 --> 00:33:14,790
I executed training job
using (indistinct) control.

605
00:33:14,790 --> 00:33:19,500
There is standard CRI interface, alright?

606
00:33:19,500 --> 00:33:21,273
Yeah, training is progressing.

607
00:33:23,490 --> 00:33:27,090
Next, I'm going to, let's say,

608
00:33:27,090 --> 00:33:32,090
inject artificial GPU failure
in order to simulate the,

609
00:33:32,550 --> 00:33:36,180
and to verify the resiliency capability.

610
00:33:36,180 --> 00:33:40,620
So a good feature of HyperPod
is you have direct access

611
00:33:40,620 --> 00:33:42,660
to the infrastructure.

612
00:33:42,660 --> 00:33:44,850
So not at the container-level,

613
00:33:44,850 --> 00:33:46,980
not only just monitoring logs,

614
00:33:46,980 --> 00:33:51,980
but you can log into computer
nodes like this by SSM session

615
00:33:53,460 --> 00:33:56,400
and in this case, I
injected some error message

616
00:33:56,400 --> 00:33:59,610
to the counter log to simulate hardware.

617
00:33:59,610 --> 00:34:04,413
And you can see, yeah, so one
node became not ready status,

618
00:34:05,430 --> 00:34:08,520
and one pod, I think it
became pending status.

619
00:34:08,520 --> 00:34:11,400
So it means your job got suspended

620
00:34:11,400 --> 00:34:15,273
because of the injected
artificially injected error.

621
00:34:16,290 --> 00:34:20,670
And yeah, so new node came in,

622
00:34:20,670 --> 00:34:22,860
thanks to that instance replacement,

623
00:34:22,860 --> 00:34:27,090
and a new container is, let's say,

624
00:34:27,090 --> 00:34:31,383
creating status now and soon
it should start executing.

625
00:34:40,590 --> 00:34:43,410
Yeah, so container, say,

626
00:34:43,410 --> 00:34:48,410
newly created container started
and job resumed like this.

627
00:34:51,420 --> 00:34:55,803
And then yeah, job resumed.

628
00:34:57,210 --> 00:34:59,850
The next step I'd like to show you

629
00:34:59,850 --> 00:35:03,060
how the observability
dashboard looks like.

630
00:35:03,060 --> 00:35:04,800
This is based on (indistinct)

631
00:35:04,800 --> 00:35:07,560
and Amazon managed the (indistinct)

632
00:35:07,560 --> 00:35:09,273
and Amazon managed the Grafana,

633
00:35:10,140 --> 00:35:15,140
so I'm using task, say,
cluster dashboard first.

634
00:35:15,180 --> 00:35:18,990
And the cluster dashboard
shows you hardware resource

635
00:35:18,990 --> 00:35:21,330
related to information
at the cluster-level

636
00:35:21,330 --> 00:35:23,973
or instance-level like this.

637
00:35:26,190 --> 00:35:29,760
And next dashboard I'm
showing is task dashboard.

638
00:35:29,760 --> 00:35:31,290
It's pretty powerful.

639
00:35:31,290 --> 00:35:34,770
You can see key metrics like a CPU, GPU,

640
00:35:34,770 --> 00:35:37,740
oh say, memory utilization
at a task level,

641
00:35:37,740 --> 00:35:39,150
not instance level.

642
00:35:39,150 --> 00:35:42,600
So because HyperPod is for distributed,

643
00:35:42,600 --> 00:35:44,940
so useful for distributed training, right?

644
00:35:44,940 --> 00:35:47,610
So I think you are
interested in which team

645
00:35:47,610 --> 00:35:51,510
is using GPU efficiently,
which teams need improvement,

646
00:35:51,510 --> 00:35:56,510
and which job is, let's
say, not performing well,

647
00:35:58,200 --> 00:36:00,780
because of what, bottleneck, right?

648
00:36:00,780 --> 00:36:03,390
So those informations are really important

649
00:36:03,390 --> 00:36:06,390
to use cluster.

650
00:36:06,390 --> 00:36:09,813
So this observability dashboard
is very useful for you.

651
00:36:10,950 --> 00:36:13,200
Okay, so in the demo,

652
00:36:13,200 --> 00:36:18,090
I included some newly
introduced features this year.

653
00:36:18,090 --> 00:36:21,363
So let me highlight some new features.

654
00:36:22,710 --> 00:36:24,660
The first feature I'd like to highlight

655
00:36:24,660 --> 00:36:28,080
is enhanced HyperPod, say,

656
00:36:28,080 --> 00:36:32,607
cluster creation experience
on management console.

657
00:36:32,607 --> 00:36:37,470
And you can easily set
up cluster by, let's say,

658
00:36:37,470 --> 00:36:38,850
entering some parameters.

659
00:36:38,850 --> 00:36:41,850
And configuring cluster
can be very complex

660
00:36:41,850 --> 00:36:46,850
because say, you need
to not only configure

661
00:36:46,860 --> 00:36:50,580
the cluster itself, but
also prerequisite resources.

662
00:36:50,580 --> 00:36:53,640
But you can use this
browser based experience

663
00:36:53,640 --> 00:36:56,310
to easily configure a cluster.

664
00:36:56,310 --> 00:36:59,040
Next feature is one click observability.

665
00:36:59,040 --> 00:37:04,040
And so observability is
important for you as I said,

666
00:37:05,100 --> 00:37:07,020
but setting up is complex,

667
00:37:07,020 --> 00:37:09,660
because it requires
metric emitting component

668
00:37:09,660 --> 00:37:13,440
on the cluster itself
and time series database

669
00:37:13,440 --> 00:37:14,880
and visualization layer.

670
00:37:14,880 --> 00:37:17,853
But yeah, you can literally
install this by one click.

671
00:37:19,140 --> 00:37:21,210
And the HyperPod training operator,

672
00:37:21,210 --> 00:37:24,660
this is for efficient
distributed training,

673
00:37:24,660 --> 00:37:29,660
and it has capability to
recover intelligently.

674
00:37:31,920 --> 00:37:34,053
It has a job hanging detection.

675
00:37:36,840 --> 00:37:39,150
Managed tiered checkpointing.

676
00:37:39,150 --> 00:37:42,363
This is to reduce the
overhead of checkpointing.

677
00:37:45,120 --> 00:37:48,990
And this is a feature I
didn't include in my demo,

678
00:37:48,990 --> 00:37:53,990
but AWS, sorry, training
job has a new feature,

679
00:37:54,990 --> 00:37:55,920
AWS Batch support,

680
00:37:55,920 --> 00:38:00,780
and training job itself doesn't
have a batch scheduling,

681
00:38:00,780 --> 00:38:04,350
sorry, job scheduling capability,
it just runs training.

682
00:38:04,350 --> 00:38:07,470
But by combining a SageMaker training job

683
00:38:07,470 --> 00:38:12,470
and AWS batch, you can include
training job in the queue

684
00:38:13,050 --> 00:38:15,603
and intelligently schedule by priority.

685
00:38:18,090 --> 00:38:21,450
And this is also a new feature
I didn't include in the demo,

686
00:38:21,450 --> 00:38:26,450
but say, now, you can
run IDEs and notebook

687
00:38:26,670 --> 00:38:29,310
on a HyperPod cluster.

688
00:38:29,310 --> 00:38:32,190
So you can use notebook, sorry,

689
00:38:32,190 --> 00:38:35,580
HyperPod cluster not only
for learning training jobs,

690
00:38:35,580 --> 00:38:39,243
but also for interactive
development and experiment.

691
00:38:40,620 --> 00:38:43,103
And lastly, we released MCP Server.

692
00:38:43,103 --> 00:38:48,103
So for SageMaker AI, so you
can use this MCP Server to,

693
00:38:48,887 --> 00:38:52,260
I say, create and
maintain HyperPod cluster

694
00:38:52,260 --> 00:38:54,720
by natural language.

695
00:38:54,720 --> 00:38:55,770
For example, you can say,

696
00:38:55,770 --> 00:38:59,913
create a cluster with a four
RG five (indistinct) instances

697
00:39:00,990 --> 00:39:05,760
or you can also say, scale up
my cluster from four instances

698
00:39:05,760 --> 00:39:08,313
to 80 instances by natural language.

699
00:39:10,230 --> 00:39:14,340
Okay, so yeah.

700
00:39:14,340 --> 00:39:18,150
So now, we covered fundamental concept

701
00:39:18,150 --> 00:39:20,850
of SageMaker training capability.

702
00:39:20,850 --> 00:39:24,900
And let's say, I provided
some demonstration.

703
00:39:24,900 --> 00:39:27,447
Now, I think you are
interested in what HyperPod

704
00:39:27,447 --> 00:39:30,750
or SageMaker AI capability
can do in real world, right?

705
00:39:30,750 --> 00:39:35,750
So I'm interested, say, introduce
Denis Goupil from Roblox.

706
00:39:38,280 --> 00:39:40,590
His team is using SageMaker HyperPod

707
00:39:40,590 --> 00:39:45,333
for one of core components
of their AI platform.

708
00:39:48,960 --> 00:39:50,760
- Thank you. Hello, can you hear me?

709
00:39:51,750 --> 00:39:53,190
Cool. Yes.

710
00:39:53,190 --> 00:39:54,033
So, I'm Denis.

711
00:39:56,670 --> 00:39:57,870
I work at Roblox

712
00:39:57,870 --> 00:40:01,650
where I lead the AI infrastructure
and AI platform team.

713
00:40:01,650 --> 00:40:04,740
And so today I'm gonna
talk about AI at Roblox

714
00:40:04,740 --> 00:40:06,510
to give you some context.

715
00:40:06,510 --> 00:40:09,270
And then we dig into our
training infrastructure,

716
00:40:09,270 --> 00:40:11,340
especially how it has been impacted

717
00:40:11,340 --> 00:40:14,910
by training for 4D Foundation model,

718
00:40:14,910 --> 00:40:17,643
finish by what we want to build next.

719
00:40:19,440 --> 00:40:21,900
So if you are not familiar with Roblox,

720
00:40:21,900 --> 00:40:25,410
Roblox is a platform where
million of people come together

721
00:40:25,410 --> 00:40:29,640
to create, play, and
interact with each other

722
00:40:29,640 --> 00:40:32,880
with one of the million of
experience that we have.

723
00:40:32,880 --> 00:40:35,970
And those experiences have
been created by our community

724
00:40:35,970 --> 00:40:36,803
of (indistinct).

725
00:40:38,477 --> 00:40:39,840
And so as of last quarter,

726
00:40:39,840 --> 00:40:43,620
we have 150 million daily active users,

727
00:40:43,620 --> 00:40:46,470
and we eat 45 million peak concurrency.

728
00:40:46,470 --> 00:40:51,450
That means that 45 million
people come together

729
00:40:51,450 --> 00:40:53,763
to play at the same time in Roblox.

730
00:40:54,780 --> 00:40:56,310
That's pretty huge.

731
00:40:56,310 --> 00:40:59,550
And in terms of AI, our
AI infrastructure supports

732
00:40:59,550 --> 00:41:03,570
about 1 million plus query per second

733
00:41:03,570 --> 00:41:06,360
across our 350 plus,

734
00:41:06,360 --> 00:41:10,233
50 model that we have
running in production.

735
00:41:11,160 --> 00:41:12,210
So as you can see,

736
00:41:12,210 --> 00:41:15,873
AI is pretty much everywhere
on what we do at Roblox.

737
00:41:17,700 --> 00:41:19,740
And especially when it comes to safety,

738
00:41:19,740 --> 00:41:22,293
which is core to our principle.

739
00:41:23,220 --> 00:41:27,690
We train and we serve a model
for real time moderation,

740
00:41:27,690 --> 00:41:30,420
for voice and for text just to make sure

741
00:41:30,420 --> 00:41:31,620
that there is no bad content

742
00:41:31,620 --> 00:41:34,170
that is happening within the platform.

743
00:41:34,170 --> 00:41:36,120
And actually those models are open source

744
00:41:36,120 --> 00:41:39,480
so you can use it for your own use cases.

745
00:41:39,480 --> 00:41:43,470
For safety, we also do
bad avatar moderation

746
00:41:43,470 --> 00:41:45,840
as well as bot detection, abuse report,

747
00:41:45,840 --> 00:41:47,160
and clickbait detection.

748
00:41:47,160 --> 00:41:52,160
So a lot of use cases of AI use
cases being used for safety.

749
00:41:53,850 --> 00:41:57,720
For our players, AI
drives our recommendation

750
00:41:57,720 --> 00:42:01,290
and search system just to make sure

751
00:42:01,290 --> 00:42:04,500
and help our users to
get the right content.

752
00:42:04,500 --> 00:42:06,930
So that means we have game recommendation

753
00:42:06,930 --> 00:42:08,520
as you can see on the screen as well

754
00:42:08,520 --> 00:42:11,133
as friend recommendation
for social interaction.

755
00:42:12,330 --> 00:42:15,444
And for assets that you
can use for your game

756
00:42:15,444 --> 00:42:16,710
or for your avatar,

757
00:42:16,710 --> 00:42:21,423
we also have the marketplace
search driven by AI.

758
00:42:24,270 --> 00:42:29,040
Finally for our creators we
introduced a set of gen AI tools

759
00:42:29,040 --> 00:42:34,040
to help them improve
their creative processes.

760
00:42:34,140 --> 00:42:37,080
And so in that example,

761
00:42:37,080 --> 00:42:41,160
you have a creator
assistant built into Roblox

762
00:42:41,160 --> 00:42:46,160
that helps you create a
three by three or upgrade

763
00:42:47,280 --> 00:42:49,697
that's like one set of the assistance.

764
00:42:49,697 --> 00:42:53,407
The other set is a script
that actually helps you turn

765
00:42:55,650 --> 00:42:58,440
this blue (indistinct) into red

766
00:42:58,440 --> 00:43:01,050
and disappear when a player touches it.

767
00:43:01,050 --> 00:43:04,230
So similar that what you can
do as a developer using cursor,

768
00:43:04,230 --> 00:43:06,903
we have the same set of
tools for our creators.

769
00:43:08,520 --> 00:43:11,760
And more recently, we
introduced our cube model.

770
00:43:11,760 --> 00:43:14,513
So it's a 4D object generation, and by 4D,

771
00:43:14,513 --> 00:43:19,513
I mean it's a 3D object
that you can interact

772
00:43:19,710 --> 00:43:21,300
and it's functional.

773
00:43:21,300 --> 00:43:23,160
So for example, as you get a car,

774
00:43:23,160 --> 00:43:25,740
you can actually open the
door and get into the car

775
00:43:25,740 --> 00:43:28,680
and drive it and the wheel will turn.

776
00:43:28,680 --> 00:43:31,590
And so there's two examples on the screen.

777
00:43:31,590 --> 00:43:33,483
On the left side you have a gun,

778
00:43:34,413 --> 00:43:36,480
a watermelon gun in the shape of a banana

779
00:43:36,480 --> 00:43:38,670
that actually shoots watermelon.

780
00:43:38,670 --> 00:43:41,580
And on the right side, you
have a magic carpet built

781
00:43:41,580 --> 00:43:45,270
by our 4D model that you can fly.

782
00:43:45,270 --> 00:43:46,680
That's pretty cool.

783
00:43:46,680 --> 00:43:50,040
So you can see that in the future,

784
00:43:50,040 --> 00:43:54,000
our players and creators will
be pretty much being able

785
00:43:54,000 --> 00:43:56,643
to create anything that they can imagine.

786
00:43:58,410 --> 00:44:03,410
Alright, so now let's
see how we actually train

787
00:44:03,660 --> 00:44:07,020
this foundation model and how
our training infrastructure

788
00:44:07,020 --> 00:44:08,703
has been impacted by that.

789
00:44:10,470 --> 00:44:14,610
But first, let's step back
and look at the AI platform

790
00:44:14,610 --> 00:44:16,890
as a whole to give you some context.

791
00:44:16,890 --> 00:44:18,720
So on the left side in purple,

792
00:44:18,720 --> 00:44:21,690
you have all our data components.

793
00:44:21,690 --> 00:44:23,160
So starting with our data lake

794
00:44:23,160 --> 00:44:25,590
with structure and unstructured data,

795
00:44:25,590 --> 00:44:30,590
our feature store and embedding
where you can with online

796
00:44:30,600 --> 00:44:32,433
and offline retrieval.

797
00:44:33,540 --> 00:44:36,870
In the middle top, you
have the blue component

798
00:44:36,870 --> 00:44:39,180
which is used for training,
so that's data processing.

799
00:44:39,180 --> 00:44:42,450
So you go from raw data to
features for your training,

800
00:44:42,450 --> 00:44:44,670
and actually the training
component itself.

801
00:44:44,670 --> 00:44:49,530
So when you go for features
to like, build those model.

802
00:44:49,530 --> 00:44:52,050
And then the yellow one is
what we use for serving.

803
00:44:52,050 --> 00:44:54,330
So when you get your training
and you have your model,

804
00:44:54,330 --> 00:44:56,400
you can push it to a model registry

805
00:44:56,400 --> 00:44:58,170
and then you have our serving component

806
00:44:58,170 --> 00:45:00,570
that reads the model from the registry

807
00:45:00,570 --> 00:45:05,280
and serve it for real time
use cases as use case in green

808
00:45:05,280 --> 00:45:07,770
and that's already described before.

809
00:45:07,770 --> 00:45:10,740
And finally on the right side,

810
00:45:10,740 --> 00:45:15,030
you have the rest of what
we use for observability,

811
00:45:15,030 --> 00:45:18,840
cost tracking, and our
experimentation platform.

812
00:45:18,840 --> 00:45:20,490
So that's pretty much
AI platform as a whole.

813
00:45:20,490 --> 00:45:23,490
And so today we are going
to focus on this red box,

814
00:45:23,490 --> 00:45:25,470
which is our distributed
training components.

815
00:45:25,470 --> 00:45:28,710
So how we do multi GPU multi-node

816
00:45:28,710 --> 00:45:30,410
in a distributed training fashion.

817
00:45:32,670 --> 00:45:34,020
And spoiler alert,

818
00:45:34,020 --> 00:45:36,483
is gonna use a HyperPod cluster obviously.

819
00:45:38,670 --> 00:45:41,010
So let's go back to the 4D model

820
00:45:41,010 --> 00:45:44,760
and to the scaling factor that
Michael was talking about.

821
00:45:44,760 --> 00:45:48,060
And so how it's gonna be
applied to our use case.

822
00:45:48,060 --> 00:45:49,980
So first was the data site.

823
00:45:49,980 --> 00:45:53,430
In our case, we have about
100 million 3D assets

824
00:45:53,430 --> 00:45:58,430
that are available that's like
roughly 6.5 petabyte of data.

825
00:45:58,740 --> 00:46:00,840
We don't necessarily use
everything for training,

826
00:46:00,840 --> 00:46:03,210
but that gives you an idea of the scale.

827
00:46:03,210 --> 00:46:07,560
Then the model size, the
requirement was between 1 billion

828
00:46:07,560 --> 00:46:09,270
and 70 billion parameters.

829
00:46:09,270 --> 00:46:11,700
So it's a very large requirement.

830
00:46:11,700 --> 00:46:15,033
We don't know exactly
what's gonna be built.

831
00:46:15,930 --> 00:46:19,380
In terms of available hardware,

832
00:46:19,380 --> 00:46:22,350
all we know is we will
need a lot of GPU memory

833
00:46:22,350 --> 00:46:25,200
to train those models.

834
00:46:25,200 --> 00:46:29,823
And so the New Year's better
and H200 is what we went for.

835
00:46:31,170 --> 00:46:33,060
In terms of training technique,

836
00:46:33,060 --> 00:46:36,000
the research team doesn't
have any specific requirement,

837
00:46:36,000 --> 00:46:39,300
except that it starts to
be performant, stable,

838
00:46:39,300 --> 00:46:41,430
and easy to use.

839
00:46:41,430 --> 00:46:43,440
And finally, probably
the most important point

840
00:46:43,440 --> 00:46:46,623
for them is the number of
jobs that they can run.

841
00:46:48,955 --> 00:46:50,970
And why it's more important

842
00:46:50,970 --> 00:46:54,240
is as we never train a foundation model,

843
00:46:54,240 --> 00:46:56,253
and especially a 4D foundation model,

844
00:46:57,540 --> 00:47:00,840
being able to run as many
jobs as possible in parallel,

845
00:47:00,840 --> 00:47:03,390
makes them more efficient and actually

846
00:47:03,390 --> 00:47:05,370
like refine the hypotheses that they need

847
00:47:05,370 --> 00:47:08,223
to create that model and
especially the model size.

848
00:47:10,770 --> 00:47:15,770
Alright, so how does that
affect our AI platform?

849
00:47:17,760 --> 00:47:20,340
Well, first of all, you go
talk to your finance team

850
00:47:20,340 --> 00:47:22,710
and making sure that you
have enough budget for it,

851
00:47:22,710 --> 00:47:23,850
that's one.

852
00:47:23,850 --> 00:47:25,410
But once you get the budget,

853
00:47:25,410 --> 00:47:29,430
we actually sit down and look
at what is going to break

854
00:47:29,430 --> 00:47:32,910
or what is missing from
our current solution.

855
00:47:32,910 --> 00:47:35,133
And so we came up with three challenges.

856
00:47:36,120 --> 00:47:39,900
The first one was how we
actually get capacity.

857
00:47:39,900 --> 00:47:42,540
We already have thousand
of GPU across training

858
00:47:42,540 --> 00:47:46,410
and inference, but those
are usually like tiny GPU

859
00:47:46,410 --> 00:47:49,260
for inference or they're
used by other team.

860
00:47:49,260 --> 00:47:52,110
And so in that case, we just
needed like more capacity

861
00:47:52,110 --> 00:47:54,303
and more high performance GPU.

862
00:47:55,230 --> 00:47:58,350
And so how we get that was one challenge.

863
00:47:58,350 --> 00:48:02,670
The second challenge is, was
how do we ensure resiliency?

864
00:48:02,670 --> 00:48:05,730
As Michael was describing,
as you get more data,

865
00:48:05,730 --> 00:48:09,630
as the model size go
up, you are most likely

866
00:48:09,630 --> 00:48:12,273
to fail at some point
during your training.

867
00:48:14,160 --> 00:48:18,390
And so we have to make sure
that whatever this new project

868
00:48:18,390 --> 00:48:21,600
is coming to the platform,
we have hundred of,

869
00:48:21,600 --> 00:48:23,880
many engineers are
working on other project

870
00:48:23,880 --> 00:48:26,480
that should not be stopped
because of a new project.

871
00:48:27,840 --> 00:48:31,020
And the last one is, okay,
you get GPU, get capacity,

872
00:48:31,020 --> 00:48:32,520
you get resiliency.

873
00:48:32,520 --> 00:48:35,130
The first one is how we
make it like cost efficient

874
00:48:35,130 --> 00:48:40,130
and making sure that we
utilize GPU resources

875
00:48:40,440 --> 00:48:41,943
in the best way possible.

876
00:48:44,760 --> 00:48:46,950
And that's where HyperPod
come into the picture

877
00:48:46,950 --> 00:48:49,350
and helps us get there.

878
00:48:49,350 --> 00:48:53,763
So the first point, was it
actually fairly easy to set up?

879
00:48:54,630 --> 00:48:56,010
It's not, don't get me wrong,

880
00:48:56,010 --> 00:48:58,860
not as easy as setting
up an easy two instances

881
00:48:58,860 --> 00:49:02,640
or not good for your case
cluster, but it's reasonable.

882
00:49:02,640 --> 00:49:06,183
It took us one month from
prototyping into production.

883
00:49:07,290 --> 00:49:09,270
And so that includes
making sure that everything

884
00:49:09,270 --> 00:49:11,310
that we do on the platform
works the same way

885
00:49:11,310 --> 00:49:15,180
in a HyperPod cluster with InfoSec review

886
00:49:15,180 --> 00:49:16,740
and actually setting up the network.

887
00:49:16,740 --> 00:49:20,073
So our EKS control plan
can talk to HyperPod node.

888
00:49:20,970 --> 00:49:24,720
And we recently created a
new EKS HyperPod cluster

889
00:49:24,720 --> 00:49:27,200
and it took us less
than one hour to do it.

890
00:49:27,200 --> 00:49:29,103
So I would say it's reasonable.

891
00:49:31,230 --> 00:49:34,260
The next point was GPU
capacity, as I was saying,

892
00:49:34,260 --> 00:49:36,470
we need this high-performance GPU,

893
00:49:36,470 --> 00:49:39,390
and so HyperPod was one way to get there.

894
00:49:39,390 --> 00:49:41,250
And especially that with HyperPod,

895
00:49:41,250 --> 00:49:42,570
you have like multiple option

896
00:49:42,570 --> 00:49:46,140
to get more capacity either
with reserved capacity,

897
00:49:46,140 --> 00:49:48,780
spot instances or flexible training plan.

898
00:49:48,780 --> 00:49:51,240
We don't choose flexible
training plan today,

899
00:49:51,240 --> 00:49:55,200
but as we get more timely
or ephemeral project,

900
00:49:55,200 --> 00:49:57,453
I do see that happening in the future.

901
00:49:59,880 --> 00:50:03,480
Continuing on our HyperPod
helps us with those changes.

902
00:50:03,480 --> 00:50:06,030
The next one was stability.

903
00:50:06,030 --> 00:50:11,030
Again, as we scale up, a failure
is most likely to happen.

904
00:50:14,148 --> 00:50:17,070
And so building those features
ourself will take a lot

905
00:50:17,070 --> 00:50:21,600
of time and most likely
being delayed that project.

906
00:50:21,600 --> 00:50:25,410
And so HyperPod helps us get there faster.

907
00:50:25,410 --> 00:50:28,440
And actually as I was
preparing the slide for today,

908
00:50:28,440 --> 00:50:30,150
I couldn't find a single node failure

909
00:50:30,150 --> 00:50:31,320
in the last three months,

910
00:50:31,320 --> 00:50:33,120
even so I can tell you
that there were a lot

911
00:50:33,120 --> 00:50:35,760
of GPU utilization during that time.

912
00:50:35,760 --> 00:50:36,993
So kudos to the team.

913
00:50:37,860 --> 00:50:39,840
But as I mentioned that something

914
00:50:39,840 --> 00:50:43,200
that is actually missing
though is some visibility

915
00:50:43,200 --> 00:50:46,560
into the recurring node detection,

916
00:50:46,560 --> 00:50:48,000
and like what failure (indistinct),

917
00:50:48,000 --> 00:50:51,660
I would've loved to show you
actually a thousand of tests

918
00:50:51,660 --> 00:50:53,970
that AWS is running behind the scene

919
00:50:53,970 --> 00:50:55,950
where everything is green
and everything is fine.

920
00:50:55,950 --> 00:50:57,540
I couldn't find this information,

921
00:50:57,540 --> 00:50:59,140
so we should probably take that.

922
00:51:01,110 --> 00:51:03,390
The last one is flexibility.

923
00:51:03,390 --> 00:51:07,620
And honestly to us, that was
the most important point,

924
00:51:07,620 --> 00:51:09,000
where you have an AI platform,

925
00:51:09,000 --> 00:51:11,040
we already invest a lot of time with that,

926
00:51:11,040 --> 00:51:12,090
where you have like tooling

927
00:51:12,090 --> 00:51:14,910
that we want to reuse as much as possible.

928
00:51:14,910 --> 00:51:17,640
And the fact that HyperPod
is able to integrate

929
00:51:17,640 --> 00:51:21,390
with EKS is phenomenal to us.

930
00:51:21,390 --> 00:51:22,560
And what I mean by that,

931
00:51:22,560 --> 00:51:25,620
is when you create a HyperPod cluster,

932
00:51:25,620 --> 00:51:28,950
your HyperPod node is going
to join your EKS cluster,

933
00:51:28,950 --> 00:51:29,820
and at that point,

934
00:51:29,820 --> 00:51:34,680
it looks like just any
other node in your cluster.

935
00:51:34,680 --> 00:51:36,630
That means that you can turn those nodes,

936
00:51:36,630 --> 00:51:40,260
you cannot do selection, you
can run any demand set on it,

937
00:51:40,260 --> 00:51:44,340
and any tooling that you
have, can work the same way.

938
00:51:44,340 --> 00:51:47,670
And so that means that
HyperPod is actually

939
00:51:47,670 --> 00:51:51,297
like framework agnostics for
both scheduling and training.

940
00:51:51,297 --> 00:51:55,770
And so in our case, that means
we were able to use unicorn

941
00:51:55,770 --> 00:51:59,010
just like any user AI workload
that we have on the platform

942
00:51:59,010 --> 00:52:02,550
and we are able to use Ray for
distributed training, again,

943
00:52:02,550 --> 00:52:03,900
just like any other workload.

944
00:52:03,900 --> 00:52:07,623
So that was definitely a
good selling point for us.

945
00:52:09,630 --> 00:52:12,783
I'll finish with what
we want to build next,

946
00:52:14,040 --> 00:52:17,253
and that's what we call
a decentralized compute.

947
00:52:18,600 --> 00:52:20,700
The problem still remains the same.

948
00:52:20,700 --> 00:52:25,700
Having access to GPU
is always a bottleneck.

949
00:52:26,100 --> 00:52:29,880
And so HyperPod was able to
solve that in a single region,

950
00:52:29,880 --> 00:52:32,010
single cluster environment.

951
00:52:32,010 --> 00:52:35,220
But as we want to get
access to more capacity,

952
00:52:35,220 --> 00:52:36,930
we need to extend that.

953
00:52:36,930 --> 00:52:39,840
And so the solution that
we come up with is again,

954
00:52:39,840 --> 00:52:41,850
what we call a decentralized compute.

955
00:52:41,850 --> 00:52:44,910
And so we need a way to
schedule workloads dynamically

956
00:52:44,910 --> 00:52:49,020
across all resources
that are available to us.

957
00:52:49,020 --> 00:52:52,080
And it doesn't matter which
region, which cluster,

958
00:52:52,080 --> 00:52:55,023
where the capacity is,
is all that matters.

959
00:52:56,220 --> 00:53:00,246
And we need to make that in a
transparent way for our user.

960
00:53:00,246 --> 00:53:03,330
And so how we are going to
abstract the scheduling process

961
00:53:03,330 --> 00:53:07,260
so the user gets a unified experience,

962
00:53:07,260 --> 00:53:10,650
and just schedule a job and submit it

963
00:53:10,650 --> 00:53:14,100
and it doesn't actually care
of where it's actually running.

964
00:53:14,100 --> 00:53:15,660
That's where we want to be.

965
00:53:15,660 --> 00:53:17,820
And so finally that means that we want

966
00:53:17,820 --> 00:53:19,740
to extend the HyperPod support

967
00:53:19,740 --> 00:53:24,030
to get like multi-region
and multi cluster for that.

968
00:53:24,030 --> 00:53:27,120
And so we hope to work
with the SageMaker team

969
00:53:27,120 --> 00:53:27,953
to make that happen,

970
00:53:27,953 --> 00:53:30,483
and hopefully, you can
benefit from it as well.

971
00:53:31,470 --> 00:53:35,913
And that's all for me. I'll
let Michael do the recap.

972
00:53:38,823 --> 00:53:43,823
- Thank you. Thank you very
much, Denis and Tomonori.

973
00:53:44,790 --> 00:53:47,190
And thank you so much for sticking around.

974
00:53:47,190 --> 00:53:50,940
We have just three more slides
to close out this session.

975
00:53:50,940 --> 00:53:55,860
And as a quick recap, we talked
about the various dimensions

976
00:53:55,860 --> 00:53:58,770
of the critical things that
are important to customers,

977
00:53:58,770 --> 00:54:02,160
compute availability,
performance, resiliency,

978
00:54:02,160 --> 00:54:04,497
observability, and ease of use,

979
00:54:04,497 --> 00:54:07,143
and how those all apply to cost.

980
00:54:09,060 --> 00:54:13,050
I recall Denis talked about
flexibility of choice,

981
00:54:13,050 --> 00:54:15,750
and on this slide, we're
just gonna summarize that.

982
00:54:15,750 --> 00:54:17,460
Right from the bottom of the stack,

983
00:54:17,460 --> 00:54:20,010
for hardware we have multiple options,

984
00:54:20,010 --> 00:54:23,910
different compute options,
storage, and networking.

985
00:54:23,910 --> 00:54:26,190
On the software and driver layer,

986
00:54:26,190 --> 00:54:28,740
we have multiple pre-built images,

987
00:54:28,740 --> 00:54:31,320
different device drivers and toolkits.

988
00:54:31,320 --> 00:54:34,230
Moving further up for
multiple training libraries

989
00:54:34,230 --> 00:54:38,130
to ensure highly performant
distributed training

990
00:54:38,130 --> 00:54:41,490
with PyTorch, TensorFlow,

991
00:54:41,490 --> 00:54:45,240
multiple capabilities for
distributed training strategies

992
00:54:45,240 --> 00:54:48,690
and even various MLOps
third party solutions,

993
00:54:48,690 --> 00:54:52,890
we integrate with MLflow
or weights and biases.

994
00:54:52,890 --> 00:54:57,840
For tools, you can submit
jobs with Ray or kubectl.

995
00:54:57,840 --> 00:55:00,870
For observability like Tomonori showed,

996
00:55:00,870 --> 00:55:04,860
we have support for Prometheus,
Grafana, or CloudWatch.

997
00:55:04,860 --> 00:55:09,300
You can even use SageMaker
Studio or notebooks

998
00:55:09,300 --> 00:55:11,340
or even use it directly from your machine

999
00:55:11,340 --> 00:55:13,380
to actually submit jobs.

1000
00:55:13,380 --> 00:55:17,220
And at the top layer,
whether you're using HyperPod

1001
00:55:17,220 --> 00:55:22,220
to orchestrate your jobs with
Kubernetes and EKS or Slurm,

1002
00:55:22,560 --> 00:55:24,540
those options are available to you.

1003
00:55:24,540 --> 00:55:27,570
Or if you just want to
use a fully managed API,

1004
00:55:27,570 --> 00:55:31,764
you can actually use the
SageMaker training jobs

1005
00:55:31,764 --> 00:55:32,880
managed API.

1006
00:55:32,880 --> 00:55:36,090
So what this shows is we
understand that for you,

1007
00:55:36,090 --> 00:55:38,130
your use cases might be different,

1008
00:55:38,130 --> 00:55:40,620
your technology team might
have different interest

1009
00:55:40,620 --> 00:55:42,390
in using various frameworks.

1010
00:55:42,390 --> 00:55:44,850
We offer that capability for you to pick

1011
00:55:44,850 --> 00:55:48,900
and choose what works
best for your use case.

1012
00:55:48,900 --> 00:55:53,760
So we are sharing this link,
it's the link to a blog

1013
00:55:53,760 --> 00:55:55,560
where we discuss both the capabilities

1014
00:55:55,560 --> 00:55:57,886
for training jobs and HyperPod.

1015
00:55:57,886 --> 00:56:00,090
It's a good resource
that I usually recommend.

1016
00:56:00,090 --> 00:56:03,630
It also has example notebooks on GitHub

1017
00:56:03,630 --> 00:56:07,830
where you can literally
just copy the code and run.

1018
00:56:07,830 --> 00:56:09,690
So these examples are set up for you.

1019
00:56:09,690 --> 00:56:13,230
We have hundreds of them
available for you to get started.

1020
00:56:13,230 --> 00:56:16,830
So thank you so much for
joining this session,

1021
00:56:16,830 --> 00:56:20,700
and please leave feedback on
the app at the end of the day

1022
00:56:20,700 --> 00:56:23,910
or later on what we've really
enjoyed talking to you today.

1023
00:56:23,910 --> 00:56:26,160
We're gonna also be outside
if you have more questions

1024
00:56:26,160 --> 00:56:27,780
or things you want us to talk about.

1025
00:56:27,780 --> 00:56:30,580
But thank you very much and
enjoy the rest of re:Invent.

