1
00:00:04,809 --> 00:00:05,328
Good afternoon,

2
00:00:05,610 --> 00:00:08,189
everybody. Can you folks hear me on your headsets?

3
00:00:08,728 --> 00:00:10,269
OK, good. Awesome.

4
00:00:10,608 --> 00:00:12,618
Now, quick show of hands,

5
00:00:13,050 --> 00:00:15,130
how many of you have experimented

6
00:00:15,130 --> 00:00:17,489
with foundation models in the past 6 months?

7
00:00:18,888 --> 00:00:20,940
OK, now keep your hands up

8
00:00:20,940 --> 00:00:22,978
if you have actually gotten those experiments

9
00:00:22,978 --> 00:00:23,978
into production.

10
00:00:26,000 --> 00:00:27,888
There that gap

11
00:00:28,318 --> 00:00:30,519
between what works in the demo and

12
00:00:30,519 --> 00:00:31,908
what works in production,

13
00:00:32,319 --> 00:00:34,539
that is what we're here to talk about

14
00:00:35,079 --> 00:00:37,219
now teams everywhere are building

15
00:00:37,219 --> 00:00:39,439
impressive prototypes, demos

16
00:00:39,439 --> 00:00:41,000
with foundation models.

17
00:00:41,279 --> 00:00:42,618
The demos look great.

18
00:00:43,399 --> 00:00:45,039
stakeholders are excited.

19
00:00:45,478 --> 00:00:46,658
Budgets are approved.

20
00:00:47,368 --> 00:00:48,380
And then

21
00:00:49,020 --> 00:00:50,500
production requirements show up.

22
00:00:51,560 --> 00:00:52,500
Now production

23
00:00:52,840 --> 00:00:54,789
doesn't care about your demos.

24
00:00:55,319 --> 00:00:58,240
Production has two non-negotiable

25
00:00:58,240 --> 00:01:00,389
demands. The first

26
00:01:00,569 --> 00:01:02,868
is accuracy that you can defend.

27
00:01:03,450 --> 00:01:05,689
Production means mistakes have

28
00:01:05,689 --> 00:01:07,230
real consequences

29
00:01:07,528 --> 00:01:09,049
compliance violations,

30
00:01:09,480 --> 00:01:11,698
broken customer trust, and bad

31
00:01:11,698 --> 00:01:12,849
business decisions.

32
00:01:13,709 --> 00:01:15,909
The second non-negotiable requirement

33
00:01:15,909 --> 00:01:17,948
is economics that works.

34
00:01:19,058 --> 00:01:20,088
What works,

35
00:01:20,500 --> 00:01:23,049
you know, in the demos and costs just dollars

36
00:01:23,219 --> 00:01:25,269
can cost thousands of dollars in

37
00:01:25,269 --> 00:01:27,370
production. And when those

38
00:01:27,370 --> 00:01:29,969
inference costs hit your P&L,

39
00:01:31,500 --> 00:01:34,109
Your CFO is going to ask you pointed

40
00:01:34,109 --> 00:01:35,230
questions about ROI.

41
00:01:36,659 --> 00:01:38,370
And so teams get stuck.

42
00:01:39,088 --> 00:01:41,370
They try hitting accuracy targets

43
00:01:41,370 --> 00:01:43,719
with larger and more expensive

44
00:01:43,719 --> 00:01:44,888
foundation models.

45
00:01:45,838 --> 00:01:47,909
Or they cut costs with

46
00:01:47,909 --> 00:01:50,040
smaller models and the accuracy

47
00:01:50,040 --> 00:01:52,239
drops below what the business can actually

48
00:01:52,239 --> 00:01:55,000
accept. So teams

49
00:01:55,000 --> 00:01:56,379
accept the false choice.

50
00:01:58,198 --> 00:02:00,870
But this is not a law of physics.

51
00:02:01,558 --> 00:02:03,620
It is a limitation of approach.

52
00:02:04,558 --> 00:02:06,558
There is a way to deliver

53
00:02:06,558 --> 00:02:08,639
both higher accuracy as

54
00:02:08,639 --> 00:02:11,778
well as dramatically lower costs simultaneously,

55
00:02:12,258 --> 00:02:14,960
and that is through model customization.

56
00:02:16,868 --> 00:02:19,050
I am Summedhaswamy, and I lead

57
00:02:19,050 --> 00:02:21,069
product management for developer

58
00:02:21,069 --> 00:02:23,569
experiences in the SageMaker AI

59
00:02:23,569 --> 00:02:25,710
organization. And today I'll discuss

60
00:02:25,710 --> 00:02:28,028
how to customize and scale foundation

61
00:02:28,028 --> 00:02:29,500
models using Sage Maker AI.

62
00:02:30,409 --> 00:02:31,949
Following my talk,

63
00:02:32,330 --> 00:02:34,308
my colleague Giuseppe, who's standing there,

64
00:02:34,770 --> 00:02:36,770
will demonstrate these capabilities

65
00:02:36,770 --> 00:02:38,969
live through product demos. You're actually

66
00:02:38,969 --> 00:02:41,349
going to see advanced model customization

67
00:02:41,349 --> 00:02:42,520
techniques in action,

68
00:02:42,969 --> 00:02:45,210
so it's going to be an exciting

69
00:02:45,210 --> 00:02:46,058
60 minutes.

70
00:02:48,159 --> 00:02:50,338
So let's be clear, why customized

71
00:02:50,338 --> 00:02:50,879
models?

72
00:02:51,618 --> 00:02:53,659
Every business now has

73
00:02:53,659 --> 00:02:55,979
access to the same foundation models

74
00:02:56,338 --> 00:02:56,899
GPT,

75
00:02:57,338 --> 00:02:57,899
Lama,

76
00:02:58,169 --> 00:03:00,179
Queen. They're all available

77
00:03:00,179 --> 00:03:01,199
through APIs.

78
00:03:01,699 --> 00:03:03,939
Your competitors have

79
00:03:03,939 --> 00:03:05,699
access to the same models.

80
00:03:06,139 --> 00:03:08,080
They're solving the same problems

81
00:03:08,490 --> 00:03:10,699
and they're targeting the same customers.

82
00:03:11,569 --> 00:03:14,028
So where is your competitive advantage?

83
00:03:15,080 --> 00:03:17,479
It is not in the models that you can

84
00:03:17,479 --> 00:03:19,479
access, it is in what you can

85
00:03:19,479 --> 00:03:20,719
actually teach them.

86
00:03:22,008 --> 00:03:24,169
Now foundation models are

87
00:03:24,179 --> 00:03:26,490
powerful generalists. They have a

88
00:03:26,490 --> 00:03:27,550
broad knowledge

89
00:03:28,050 --> 00:03:30,368
but no depth in your specific

90
00:03:30,368 --> 00:03:32,969
domain. They don't know your

91
00:03:32,969 --> 00:03:34,149
industry terminology,

92
00:03:35,008 --> 00:03:36,349
your company's data,

93
00:03:36,808 --> 00:03:38,149
your business rules,

94
00:03:38,490 --> 00:03:39,750
or your brand voice.

95
00:03:41,360 --> 00:03:43,360
So in production you're forced to

96
00:03:43,360 --> 00:03:45,610
choose the largest and most expensive

97
00:03:45,610 --> 00:03:47,849
models just to approach

98
00:03:47,849 --> 00:03:49,008
acceptable accuracy.

99
00:03:49,710 --> 00:03:51,710
And they still don't fully

100
00:03:51,710 --> 00:03:53,050
understand your context.

101
00:03:54,338 --> 00:03:56,379
And in addition to that, you're paying

102
00:03:56,379 --> 00:03:58,618
premium inference costs to access those

103
00:03:58,618 --> 00:04:01,058
largest and expensive foundation models.

104
00:04:03,169 --> 00:04:05,550
Now when I tell you this, some of you may be thinking,

105
00:04:05,889 --> 00:04:08,069
well, I can use prompt engineering.

106
00:04:08,368 --> 00:04:10,868
I can use rag to add context

107
00:04:11,288 --> 00:04:13,080
so that you can get higher accuracy,

108
00:04:13,338 --> 00:04:14,229
and you're right,

109
00:04:14,490 --> 00:04:16,569
for simple use cases that works

110
00:04:16,569 --> 00:04:18,689
great. But at

111
00:04:18,689 --> 00:04:19,759
production scale,

112
00:04:20,048 --> 00:04:22,250
which has strict accuracy

113
00:04:22,250 --> 00:04:24,369
as well as cost requirements,

114
00:04:24,809 --> 00:04:26,809
those approaches, when they're used

115
00:04:26,809 --> 00:04:28,889
alone, they hit a ceiling.

116
00:04:30,338 --> 00:04:32,858
For example, take transaction

117
00:04:32,858 --> 00:04:35,059
categorization where you are categorizing

118
00:04:35,059 --> 00:04:37,199
millions of transactions daily

119
00:04:37,858 --> 00:04:40,319
for reporting, for fraud detection.

120
00:04:41,689 --> 00:04:43,970
You may think that getting a 92%

121
00:04:43,970 --> 00:04:46,170
accuracy on those models sounds

122
00:04:46,170 --> 00:04:48,250
great, but that also means

123
00:04:48,250 --> 00:04:50,649
that for 8% of the cases

124
00:04:51,009 --> 00:04:53,048
you're miscategorizing these

125
00:04:53,048 --> 00:04:54,069
transactions,

126
00:04:54,528 --> 00:04:56,600
that is compliance violations,

127
00:04:56,949 --> 00:04:58,988
incorrect fraud flags,

128
00:04:59,488 --> 00:05:01,809
and essentially you're losing your customers'

129
00:05:01,809 --> 00:05:04,069
trust. That is

130
00:05:04,069 --> 00:05:06,230
because your model is reasoning

131
00:05:06,230 --> 00:05:08,428
like a generalist, not as

132
00:05:08,428 --> 00:05:10,149
a specialist in your domain.

133
00:05:11,579 --> 00:05:13,660
With model customization and fine

134
00:05:13,660 --> 00:05:14,278
tuning,

135
00:05:14,569 --> 00:05:16,639
this is fundamentally different.

136
00:05:17,139 --> 00:05:19,178
What you're doing there is you're

137
00:05:19,178 --> 00:05:21,500
encoding your business's domain

138
00:05:21,500 --> 00:05:22,230
expertise

139
00:05:22,819 --> 00:05:24,819
directly into the model's weights.

140
00:05:25,509 --> 00:05:27,949
So the model learns to think like

141
00:05:27,949 --> 00:05:29,548
experts in your domain.

142
00:05:30,480 --> 00:05:31,428
So here is an

143
00:05:31,858 --> 00:05:33,970
you know an example of

144
00:05:33,970 --> 00:05:36,009
what changes when you customize a

145
00:05:36,009 --> 00:05:38,209
model. You take a

146
00:05:38,209 --> 00:05:38,988
smaller model,

147
00:05:39,329 --> 00:05:41,730
let's say an 8 billion parameter

148
00:05:41,730 --> 00:05:44,170
model rather than a 400 billion parameter

149
00:05:44,170 --> 00:05:44,678
model,

150
00:05:45,079 --> 00:05:47,410
and you can fine tune it on your proprietary

151
00:05:47,410 --> 00:05:47,949
data,

152
00:05:48,410 --> 00:05:51,238
data such as your transaction patterns,

153
00:05:51,649 --> 00:05:53,170
customer interactions,

154
00:05:53,528 --> 00:05:55,528
or your domain-specific rules.

155
00:05:56,449 --> 00:05:58,480
That fine tuned small

156
00:05:58,480 --> 00:05:59,059
model

157
00:05:59,519 --> 00:06:01,720
outperforms larger models

158
00:06:01,720 --> 00:06:03,759
on your specific task. Maybe not

159
00:06:03,759 --> 00:06:05,759
on all general tasks, but on the

160
00:06:05,759 --> 00:06:07,798
tasks that you care about, it can actually

161
00:06:07,798 --> 00:06:09,809
outperform the larger models

162
00:06:10,278 --> 00:06:12,480
because it has actually learned your domain

163
00:06:12,480 --> 00:06:13,160
patterns.

164
00:06:14,009 --> 00:06:15,869
And because it is smaller,

165
00:06:16,420 --> 00:06:18,579
inference of those models is

166
00:06:18,579 --> 00:06:19,720
actually going to cost you

167
00:06:20,170 --> 00:06:22,290
one or two orders of magnitude,

168
00:06:22,338 --> 00:06:23,970
that is 10x to 100x

169
00:06:24,500 --> 00:06:25,040
cheaper

170
00:06:25,709 --> 00:06:27,500
than doing inference with a larger model.

171
00:06:29,350 --> 00:06:31,480
And most importantly, you're not trading

172
00:06:31,480 --> 00:06:33,619
off performance for cost.

173
00:06:33,959 --> 00:06:35,798
Customization delivers both.

174
00:06:38,470 --> 00:06:40,069
In your organizations,

175
00:06:40,470 --> 00:06:42,588
your proprietary data is

176
00:06:42,588 --> 00:06:43,970
the one asset

177
00:06:44,230 --> 00:06:46,588
that your competitors cannot replicate.

178
00:06:47,678 --> 00:06:49,798
Model customization is how you

179
00:06:49,798 --> 00:06:50,858
take that data

180
00:06:51,238 --> 00:06:53,720
and turn that data into your competitive

181
00:06:53,720 --> 00:06:54,500
advantage.

182
00:06:56,189 --> 00:06:58,410
This all sounds great. So if customization

183
00:06:58,410 --> 00:07:00,470
is so great it solves both these

184
00:07:00,470 --> 00:07:02,629
problems, why isn't everyone doing

185
00:07:02,629 --> 00:07:03,269
it already?

186
00:07:05,399 --> 00:07:07,928
Because there was a tax.

187
00:07:08,920 --> 00:07:11,079
Here is what customization actually

188
00:07:11,079 --> 00:07:11,819
looks like.

189
00:07:12,238 --> 00:07:14,139
Let's say you want to fine tune a model,

190
00:07:14,439 --> 00:07:16,100
it sounds very straightforward,

191
00:07:16,559 --> 00:07:18,759
but before you write even a single

192
00:07:18,759 --> 00:07:19,759
line of code.

193
00:07:20,379 --> 00:07:22,899
You're tweaking YAML configuration

194
00:07:22,899 --> 00:07:23,639
files.

195
00:07:23,980 --> 00:07:25,980
You're configuring your GPUs

196
00:07:25,980 --> 00:07:27,480
for distributed training.

197
00:07:27,858 --> 00:07:29,858
What instances should I use? How

198
00:07:29,858 --> 00:07:30,730
many of them?

199
00:07:31,059 --> 00:07:33,019
What is the fault tolerance strategy?

200
00:07:34,259 --> 00:07:36,199
Then you're stitching together tools,

201
00:07:36,579 --> 00:07:38,379
one tool for data preparation,

202
00:07:38,858 --> 00:07:40,259
another tool for training,

203
00:07:40,528 --> 00:07:42,879
a third tool for evaluations.

204
00:07:43,759 --> 00:07:45,778
Nothing really talks to each other

205
00:07:45,778 --> 00:07:48,040
and so you're writing glue code to tie

206
00:07:48,040 --> 00:07:49,238
all of these tools together.

207
00:07:50,428 --> 00:07:52,449
Finally, let's say you're running your experiment.

208
00:07:52,790 --> 00:07:55,040
Let's say you're getting promising results,

209
00:07:55,588 --> 00:07:57,528
but try to reproduce them.

210
00:07:57,829 --> 00:08:00,608
Let's say you got 94% accuracy.

211
00:08:01,149 --> 00:08:03,189
Which data set gave you that 94%

212
00:08:03,189 --> 00:08:05,750
accuracy? What were the hyperparametters?

213
00:08:06,928 --> 00:08:09,230
Without centralized tracking,

214
00:08:09,569 --> 00:08:11,649
you're piecing together this information

215
00:08:11,649 --> 00:08:14,170
through scattered notebooks, slack

216
00:08:14,170 --> 00:08:14,970
messages.

217
00:08:16,238 --> 00:08:18,608
So let's say you have battled through all of that

218
00:08:18,809 --> 00:08:19,759
and then

219
00:08:20,088 --> 00:08:22,088
your model is actually working and you

220
00:08:22,088 --> 00:08:23,629
want to deploy that model.

221
00:08:24,329 --> 00:08:26,528
Now that's a completely different set of

222
00:08:26,528 --> 00:08:27,189
challenges.

223
00:08:27,459 --> 00:08:29,778
You need to build infrastructure from scratch,

224
00:08:30,119 --> 00:08:31,389
containerization,

225
00:08:31,809 --> 00:08:32,599
versioning,

226
00:08:32,928 --> 00:08:35,548
monitoring, rollback strategies.

227
00:08:36,619 --> 00:08:37,859
And the audit trail.

228
00:08:38,847 --> 00:08:41,067
Which data trained this model?

229
00:08:41,729 --> 00:08:42,989
What is the lineage?

230
00:08:43,408 --> 00:08:45,758
You're reconstructing all of this

231
00:08:45,758 --> 00:08:47,308
information after the fact.

232
00:08:48,080 --> 00:08:50,080
And so it's no surprise that teams

233
00:08:50,080 --> 00:08:52,330
are spending like 80% of their time

234
00:08:52,330 --> 00:08:54,450
on infrastructure and tooling and

235
00:08:54,450 --> 00:08:56,969
only 20% of their time on the actual

236
00:08:56,969 --> 00:08:58,869
AI problem that they want to solve.

237
00:08:59,739 --> 00:09:01,840
So teams make the rational decision.

238
00:09:02,219 --> 00:09:04,119
They stay with generic models

239
00:09:04,500 --> 00:09:05,719
and they skip customization.

240
00:09:07,798 --> 00:09:09,599
That's the customization tax.

241
00:09:10,428 --> 00:09:11,428
But that friction,

242
00:09:11,719 --> 00:09:12,658
it is solved.

243
00:09:14,739 --> 00:09:17,379
So here is how we solve the customization

244
00:09:17,379 --> 00:09:17,960
tax.

245
00:09:19,178 --> 00:09:21,820
We rebuilt the entire customization

246
00:09:21,820 --> 00:09:24,418
experience around 3 specific principles.

247
00:09:25,340 --> 00:09:28,019
The first is serverless infrastructure.

248
00:09:30,408 --> 00:09:32,879
With this, you don't configure clusters,

249
00:09:33,129 --> 00:09:34,908
you don't plan capacity,

250
00:09:35,250 --> 00:09:37,009
you don't select instances.

251
00:09:37,609 --> 00:09:39,469
You simply submit a job

252
00:09:39,729 --> 00:09:41,590
and we provision the resources,

253
00:09:41,989 --> 00:09:44,168
we do the job of doing distributed

254
00:09:44,168 --> 00:09:46,570
training, and we scale automatically.

255
00:09:48,090 --> 00:09:50,450
You only pay for what you use,

256
00:09:50,658 --> 00:09:52,658
not what you actually reserve like you

257
00:09:52,658 --> 00:09:53,700
do with instances.

258
00:09:55,259 --> 00:09:57,820
The second principle here is integrated

259
00:09:57,820 --> 00:09:58,558
workflow.

260
00:09:59,178 --> 00:10:00,340
One environment,

261
00:10:00,619 --> 00:10:02,820
Sagemker Studio, takes you from

262
00:10:02,820 --> 00:10:04,340
experiment to production.

263
00:10:05,229 --> 00:10:07,918
We provide built-in data preparation

264
00:10:07,918 --> 00:10:08,460
tooling.

265
00:10:08,879 --> 00:10:11,000
We provide built-in evaluation

266
00:10:11,000 --> 00:10:13,798
tooling. We provide built-in observability

267
00:10:13,798 --> 00:10:15,969
tooling. You don't have to switch

268
00:10:15,969 --> 00:10:16,519
tools.

269
00:10:16,808 --> 00:10:18,889
You don't have to write glue code to connect

270
00:10:18,889 --> 00:10:19,750
these tools.

271
00:10:20,129 --> 00:10:22,210
You don't have to hunt through scattered notebooks

272
00:10:22,210 --> 00:10:23,428
and Slack messages

273
00:10:23,849 --> 00:10:25,149
to find what you did.

274
00:10:26,479 --> 00:10:27,940
Everything is tracked,

275
00:10:28,440 --> 00:10:29,099
versioned,

276
00:10:29,359 --> 00:10:31,739
reproducible in one place.

277
00:10:33,288 --> 00:10:35,580
And the 3rd principle here is production

278
00:10:35,580 --> 00:10:36,538
ready techniques.

279
00:10:37,710 --> 00:10:39,029
Supervised fine tuning,

280
00:10:39,389 --> 00:10:40,690
direct preference optimization,

281
00:10:40,950 --> 00:10:42,229
reinforcement learning,

282
00:10:42,979 --> 00:10:45,619
these advanced model customization techniques,

283
00:10:45,798 --> 00:10:48,190
they are available to you across a

284
00:10:48,190 --> 00:10:50,710
broad set of models. Amazon

285
00:10:50,710 --> 00:10:51,889
Nova models,

286
00:10:52,389 --> 00:10:52,989
Lama,

287
00:10:53,428 --> 00:10:55,428
Gwen, and many, many

288
00:10:55,428 --> 00:10:55,950
more.

289
00:10:56,389 --> 00:10:58,509
Both open weights as well as

290
00:10:58,509 --> 00:10:59,950
proprietary models are available.

291
00:11:01,658 --> 00:11:03,769
These are the same methods

292
00:11:03,779 --> 00:11:05,869
that the top frontier labs

293
00:11:05,869 --> 00:11:06,558
are using.

294
00:11:07,058 --> 00:11:09,379
What we have done is we have packaged

295
00:11:09,379 --> 00:11:10,558
these methods

296
00:11:11,058 --> 00:11:13,178
for your production use cases

297
00:11:13,379 --> 00:11:15,538
so that with a simple configuration

298
00:11:15,538 --> 00:11:17,619
change you can make them work for

299
00:11:17,619 --> 00:11:19,658
you rather than writing these

300
00:11:19,658 --> 00:11:20,519
from scratch.

301
00:11:21,200 --> 00:11:23,279
And you're going to see a demo of all of this very,

302
00:11:23,340 --> 00:11:27,038
very soon. Focus

303
00:11:27,038 --> 00:11:28,538
on your domain problem.

304
00:11:28,840 --> 00:11:30,558
We handle infrastructure.

305
00:11:30,889 --> 00:11:33,019
That is the fundamental shift.

306
00:11:33,399 --> 00:11:35,739
The 80/20 split I spoke about

307
00:11:35,830 --> 00:11:36,450
earlier,

308
00:11:37,119 --> 00:11:38,080
we inverted it.

309
00:11:38,369 --> 00:11:40,519
Now you can spend 80% of your time

310
00:11:40,519 --> 00:11:42,678
on your AI problem and 20%

311
00:11:42,678 --> 00:11:44,759
on the infrastructure and tooling.

312
00:11:45,629 --> 00:11:47,710
Because that tax is

313
00:11:47,710 --> 00:11:50,029
gone. So now

314
00:11:50,029 --> 00:11:52,450
let me dive a little bit deeper into the customization

315
00:11:52,450 --> 00:11:54,570
techniques that you can actually use.

316
00:11:57,440 --> 00:11:59,298
If it moves forward. There you go.

317
00:11:59,719 --> 00:12:01,719
Now, SageMaker gives you 3

318
00:12:01,719 --> 00:12:03,739
production-ready techniques to choose from.

319
00:12:04,279 --> 00:12:05,139
The first

320
00:12:05,639 --> 00:12:07,678
is supervised fine tuning or

321
00:12:07,678 --> 00:12:10,029
SFT. SFT

322
00:12:10,029 --> 00:12:12,408
teaches your model domain knowledge.

323
00:12:12,750 --> 00:12:15,168
You provide it with labeled examples, saying

324
00:12:15,168 --> 00:12:16,609
that, for example,

325
00:12:17,149 --> 00:12:18,379
here is the transaction,

326
00:12:18,710 --> 00:12:20,369
here is the category, or

327
00:12:20,629 --> 00:12:22,750
here is the customer question, here

328
00:12:22,750 --> 00:12:23,509
is the answer.

329
00:12:24,759 --> 00:12:26,798
The model learns through pattern

330
00:12:26,798 --> 00:12:29,320
matching, you know, it absorbs your terminology,

331
00:12:29,440 --> 00:12:30,668
your procedures,

332
00:12:30,960 --> 00:12:32,779
your institutional knowledge.

333
00:12:33,599 --> 00:12:36,058
This is how you move from a generic

334
00:12:36,058 --> 00:12:36,678
AI

335
00:12:36,960 --> 00:12:39,038
to AI that understands your

336
00:12:39,038 --> 00:12:41,359
domain. The

337
00:12:41,359 --> 00:12:43,099
model absorbs your patterns,

338
00:12:43,678 --> 00:12:45,879
learns to recognize similar situations

339
00:12:45,879 --> 00:12:47,830
when there is new data that is coming in.

340
00:12:49,000 --> 00:12:51,000
But it only learns explicitly

341
00:12:51,000 --> 00:12:53,500
what you teach it, right? It can't,

342
00:12:53,678 --> 00:12:56,210
you know, start developing judgment

343
00:12:56,210 --> 00:12:58,308
or reasoning beyond the data that you have

344
00:12:58,308 --> 00:12:59,139
actually given it.

345
00:13:00,349 --> 00:13:02,869
The second customization technique is

346
00:13:02,869 --> 00:13:04,229
direct preference optimization.

347
00:13:05,119 --> 00:13:07,190
DPO DPO teaches a

348
00:13:07,190 --> 00:13:09,178
model judgment and style.

349
00:13:09,969 --> 00:13:12,330
Now instead of labeled examples

350
00:13:12,330 --> 00:13:14,489
like you saw with supervised fine tuning with

351
00:13:14,489 --> 00:13:16,428
DPO you're showing the model

352
00:13:17,489 --> 00:13:19,139
pairs of

353
00:13:19,649 --> 00:13:20,288
responses,

354
00:13:20,649 --> 00:13:22,908
things like, hey, this response

355
00:13:22,908 --> 00:13:23,668
represents

356
00:13:24,210 --> 00:13:25,629
our brand voice,

357
00:13:26,048 --> 00:13:28,489
our risk tolerance. This response

358
00:13:28,489 --> 00:13:30,649
does not. So you give it these kind of

359
00:13:30,649 --> 00:13:32,729
response pairs and you're going to see an example in the demo.

360
00:13:33,558 --> 00:13:34,190
The model

361
00:13:34,450 --> 00:13:36,489
using those response pairs learns

362
00:13:36,489 --> 00:13:39,048
your quality standards, your

363
00:13:39,048 --> 00:13:40,109
decision making criteria,

364
00:13:40,529 --> 00:13:42,609
and the subjective preferences that

365
00:13:42,609 --> 00:13:45,330
define how your company operates.

366
00:13:46,649 --> 00:13:48,859
This is particularly powerful

367
00:13:48,859 --> 00:13:51,129
when the outcomes are hard to define

368
00:13:51,129 --> 00:13:51,928
programmatically.

369
00:13:52,719 --> 00:13:54,279
But once you

370
00:13:55,529 --> 00:13:57,599
see the outputs, it's easy to recognize

371
00:13:57,599 --> 00:13:59,599
which is a better response. This is when

372
00:13:59,599 --> 00:14:01,099
you would use something like DPO,

373
00:14:01,450 --> 00:14:02,379
for example,

374
00:14:02,879 --> 00:14:04,960
asking a question such as, hey, does this

375
00:14:04,960 --> 00:14:06,229
response sound professional?

376
00:14:06,519 --> 00:14:08,379
Does it align with values?

377
00:14:09,158 --> 00:14:11,320
DPO captures these nuanced

378
00:14:11,320 --> 00:14:13,399
preferences that cannot easily

379
00:14:13,399 --> 00:14:15,119
be reduced to simple rules.

380
00:14:16,070 --> 00:14:18,389
This is how you move from AI

381
00:14:18,389 --> 00:14:20,469
that knows your facts, which is what

382
00:14:20,469 --> 00:14:21,750
you did with SFT,

383
00:14:22,149 --> 00:14:24,269
to AI that actually sounds like you.

384
00:14:26,219 --> 00:14:28,298
Now the 3rd technique

385
00:14:28,298 --> 00:14:30,178
is reinforcement fine tuning.

386
00:14:31,178 --> 00:14:33,418
This is to teach your model complex

387
00:14:33,418 --> 00:14:35,558
reasoning. Reinforcement

388
00:14:35,558 --> 00:14:37,678
fine tuning trains a model to

389
00:14:37,678 --> 00:14:39,759
achieve outcomes that are very difficult to

390
00:14:39,759 --> 00:14:41,340
define programmatically,

391
00:14:41,918 --> 00:14:42,460
but

392
00:14:42,840 --> 00:14:45,000
once you actually see the output, it

393
00:14:45,000 --> 00:14:47,158
is easy for an AI to

394
00:14:47,158 --> 00:14:48,259
evaluate

395
00:14:48,548 --> 00:14:49,580
and give feedback,

396
00:14:49,950 --> 00:14:52,000
AI or any other tool to be able

397
00:14:52,000 --> 00:14:53,460
to evaluate and give feedback.

398
00:14:53,918 --> 00:14:55,019
Now, for example,

399
00:14:55,509 --> 00:14:57,558
uh, training a model to sound

400
00:14:57,558 --> 00:14:59,320
helpful and empathetic,

401
00:14:59,840 --> 00:15:02,000
this is a very interesting challenge. It's

402
00:15:02,000 --> 00:15:04,080
very hard to capture this in code.

403
00:15:04,928 --> 00:15:07,529
But you can't write explicit

404
00:15:07,529 --> 00:15:10,080
code to define what is warmth

405
00:15:10,080 --> 00:15:12,129
in speech, what is understanding in

406
00:15:12,129 --> 00:15:14,168
speech, but you can give that

407
00:15:14,168 --> 00:15:15,750
response to an AI model

408
00:15:16,048 --> 00:15:17,469
and ask it feedback

409
00:15:17,928 --> 00:15:19,649
on whether the response feels right.

410
00:15:21,109 --> 00:15:23,298
So that's how reinforcement learning with

411
00:15:23,298 --> 00:15:25,428
AI feedback works. The model generates

412
00:15:25,428 --> 00:15:27,210
output. You take that output.

413
00:15:27,899 --> 00:15:30,580
AI gives you feedback and a reward,

414
00:15:30,989 --> 00:15:33,469
and then the model learns to actually optimize

415
00:15:33,469 --> 00:15:34,450
for that reward.

416
00:15:34,788 --> 00:15:36,960
That is RLAIF, reinforcement

417
00:15:36,960 --> 00:15:38,070
learning with AI feedback.

418
00:15:38,469 --> 00:15:40,048
This is great when

419
00:15:40,308 --> 00:15:42,489
it's very difficult to say there's a clear right

420
00:15:42,489 --> 00:15:44,769
and wrong answer, but it's somewhat nuanced.

421
00:15:45,070 --> 00:15:47,229
But in some cases there's a very clear right

422
00:15:47,229 --> 00:15:49,428
and wrong answer like coding or

423
00:15:49,428 --> 00:15:51,500
math. And in these cases you

424
00:15:51,500 --> 00:15:53,580
use something like reinforcement learning with

425
00:15:53,580 --> 00:15:55,619
verifiable rewards wherein you

426
00:15:55,619 --> 00:15:57,658
take a function that actually

427
00:15:57,658 --> 00:15:59,658
verifies whether the output is correct or

428
00:15:59,658 --> 00:16:01,359
not, gives that output

429
00:16:02,139 --> 00:16:04,519
feedback to your model, and the model

430
00:16:04,519 --> 00:16:06,899
will get the reward and try to optimize

431
00:16:06,899 --> 00:16:07,719
for that reward.

432
00:16:08,178 --> 00:16:10,840
So all of these techniques are

433
00:16:10,849 --> 00:16:13,298
interesting in moving the complex reasoning

434
00:16:13,298 --> 00:16:14,658
part of the model forward.

435
00:16:16,298 --> 00:16:16,830
Now,

436
00:16:17,158 --> 00:16:19,379
you can start simple and evolve as

437
00:16:19,379 --> 00:16:21,379
needed based on your use case because

438
00:16:21,379 --> 00:16:23,719
Sagemaker supports all these three.

439
00:16:24,178 --> 00:16:26,219
So you can begin where it makes sense for your

440
00:16:26,219 --> 00:16:26,960
use case

441
00:16:27,298 --> 00:16:29,599
and scale up as your use case demands

442
00:16:30,019 --> 00:16:32,658
more. So

443
00:16:32,918 --> 00:16:35,200
where do you get started with building

444
00:16:35,200 --> 00:16:36,349
with these techniques?

445
00:16:38,070 --> 00:16:40,349
So what we realized was different

446
00:16:40,349 --> 00:16:41,830
teams work differently.

447
00:16:42,639 --> 00:16:44,840
So we built 3 experiences

448
00:16:44,840 --> 00:16:45,940
for model customization.

449
00:16:46,820 --> 00:16:48,349
The first is

450
00:16:49,219 --> 00:16:51,479
In Sagemaker Studio UI

451
00:16:51,750 --> 00:16:54,099
for visual workflows, you select

452
00:16:54,099 --> 00:16:54,879
a model,

453
00:16:55,178 --> 00:16:56,038
upload data,

454
00:16:56,500 --> 00:16:57,580
configure training,

455
00:16:57,979 --> 00:16:59,879
no code required. This is

456
00:17:00,379 --> 00:17:02,418
perfect for rapid experimentation.

457
00:17:03,450 --> 00:17:06,049
The second is AI agent guided

458
00:17:06,049 --> 00:17:06,630
experience.

459
00:17:07,368 --> 00:17:09,368
This one is in preview as of

460
00:17:09,368 --> 00:17:11,759
today. And here

461
00:17:11,898 --> 00:17:14,128
you describe your goal. You tell the

462
00:17:14,128 --> 00:17:16,138
agent something like, Hey, I need to

463
00:17:16,138 --> 00:17:18,577
fine tune a model for contract analysis.

464
00:17:19,198 --> 00:17:21,278
The agent then asks you a bunch of

465
00:17:21,278 --> 00:17:23,398
questions to learn more about your use case, and

466
00:17:23,398 --> 00:17:25,118
it actually builds a spec,

467
00:17:25,438 --> 00:17:27,638
a specification of what needs

468
00:17:27,638 --> 00:17:29,718
to be done in order to customize a model.

469
00:17:30,380 --> 00:17:32,549
You can read that spec. You can edit

470
00:17:32,549 --> 00:17:33,209
that spec.

471
00:17:33,799 --> 00:17:34,578
And then

472
00:17:35,039 --> 00:17:37,279
the AI agent actually helps

473
00:17:37,279 --> 00:17:39,358
take that forward. For example, if it

474
00:17:39,358 --> 00:17:41,459
has to generate synthetic data in order

475
00:17:41,459 --> 00:17:43,640
to train your model, it goes ahead and

476
00:17:43,640 --> 00:17:44,979
generates the synthetic data.

477
00:17:45,239 --> 00:17:46,500
And then with your permission,

478
00:17:46,868 --> 00:17:48,959
it can actually execute the end to end

479
00:17:48,959 --> 00:17:51,078
pipeline as defined in that spec.

480
00:17:51,479 --> 00:17:53,719
So you can go from the spec to

481
00:17:53,719 --> 00:17:56,000
training to evaluation to a production

482
00:17:56,000 --> 00:17:58,318
model just by talking to that agent

483
00:17:58,318 --> 00:18:00,650
in natural language. It's like an AI

484
00:18:00,650 --> 00:18:02,670
expert. That is helping

485
00:18:02,670 --> 00:18:04,469
you with your model customization journey.

486
00:18:05,858 --> 00:18:08,019
Finally, the code-based SDK. This is

487
00:18:08,019 --> 00:18:10,660
super important for full programmatic

488
00:18:10,660 --> 00:18:13,539
control so you can integrate model customization

489
00:18:13,539 --> 00:18:14,759
directly into your

490
00:18:15,140 --> 00:18:16,400
MLOps pipelines.

491
00:18:16,939 --> 00:18:19,098
Now, here is the important thing. Here

492
00:18:19,098 --> 00:18:20,160
is what matters.

493
00:18:20,500 --> 00:18:23,019
These are not three separate

494
00:18:23,019 --> 00:18:23,630
products.

495
00:18:24,140 --> 00:18:26,608
These are three interfaces to the

496
00:18:26,608 --> 00:18:28,640
same underlying service, which is Sagemaker

497
00:18:28,640 --> 00:18:30,640
AI. It's the same

498
00:18:30,640 --> 00:18:31,689
model registry.

499
00:18:31,969 --> 00:18:34,209
It's the same version control, the

500
00:18:34,209 --> 00:18:36,410
same audit trail, so your

501
00:18:36,410 --> 00:18:37,588
security teams

502
00:18:37,930 --> 00:18:40,000
will just see one compliance trail.

503
00:18:41,630 --> 00:18:43,338
So you have 3 different paths,

504
00:18:43,818 --> 00:18:45,338
1 production model,

505
00:18:45,699 --> 00:18:46,750
1 audit trail.

506
00:18:49,098 --> 00:18:51,459
All of these, the 3 paths,

507
00:18:51,699 --> 00:18:53,640
the customization techniques,

508
00:18:54,219 --> 00:18:54,959
it all comes together

509
00:18:55,699 --> 00:18:56,969
in Sagemaker Studio.

510
00:18:57,930 --> 00:19:00,170
Studio is the web based IDE

511
00:19:00,170 --> 00:19:02,049
for your AI workflows.

512
00:19:02,818 --> 00:19:03,479
You get

513
00:19:03,818 --> 00:19:06,439
visual model customization forms

514
00:19:06,739 --> 00:19:08,759
and you get Jupiter notebook

515
00:19:09,019 --> 00:19:10,279
code OSS

516
00:19:11,098 --> 00:19:13,559
visual VS code IDE

517
00:19:13,818 --> 00:19:16,059
for your code-based development, whatever

518
00:19:16,059 --> 00:19:18,279
matches how you work, we provide

519
00:19:18,279 --> 00:19:20,390
all of those development tools in Sage Maker

520
00:19:20,390 --> 00:19:22,660
Studio. And you can seamlessly

521
00:19:22,660 --> 00:19:24,779
switch between them as well, and you're going to see a

522
00:19:24,779 --> 00:19:26,939
demo of StageMaker Studio very, very soon.

523
00:19:27,719 --> 00:19:29,430
But here is what actually matters.

524
00:19:29,689 --> 00:19:32,019
The entire workflow here is

525
00:19:32,019 --> 00:19:32,858
connected

526
00:19:33,170 --> 00:19:34,789
from experimentation

527
00:19:35,049 --> 00:19:37,209
to training to evaluation

528
00:19:37,209 --> 00:19:39,328
to deployment to monitoring. All

529
00:19:39,328 --> 00:19:41,568
of this is connected. It all happens in

530
00:19:41,568 --> 00:19:42,779
one environment.

531
00:19:43,328 --> 00:19:45,039
You can experiment in a notebook.

532
00:19:45,568 --> 00:19:47,650
You can train and evaluate with just

533
00:19:47,650 --> 00:19:49,469
a few click clicks with the form.

534
00:19:50,229 --> 00:19:52,430
When the results meet your bar, you

535
00:19:52,430 --> 00:19:54,729
can click deploy, and the model

536
00:19:55,068 --> 00:19:57,009
goes and gets deployed live

537
00:19:57,588 --> 00:19:59,939
for your inference either on Sagemaker

538
00:19:59,939 --> 00:20:00,828
or on Bedrock.

539
00:20:02,868 --> 00:20:04,920
So I'm excited to share that we are adding

540
00:20:04,920 --> 00:20:07,160
2 more new capabilities into

541
00:20:07,160 --> 00:20:09,380
Sagemaker Studio to make your workflow even

542
00:20:09,380 --> 00:20:10,160
more powerful.

543
00:20:10,828 --> 00:20:13,088
The first is serverless model

544
00:20:13,088 --> 00:20:13,910
evaluation.

545
00:20:15,338 --> 00:20:17,338
It is really super important to be able

546
00:20:17,338 --> 00:20:19,539
to evaluate your models to

547
00:20:19,539 --> 00:20:20,799
understand how they are performing

548
00:20:21,259 --> 00:20:23,660
with this feature. You can evaluate

549
00:20:23,660 --> 00:20:25,660
what actually matters, whether

550
00:20:25,660 --> 00:20:27,939
it is customized model versus

551
00:20:27,939 --> 00:20:28,729
base model,

552
00:20:29,098 --> 00:20:31,739
or when you're doing model customization, different

553
00:20:31,739 --> 00:20:33,640
checkpoints of your customization,

554
00:20:34,140 --> 00:20:36,299
or you may want to experiment with multiple

555
00:20:36,299 --> 00:20:38,420
fine tuning techniques, SFT and

556
00:20:38,420 --> 00:20:40,059
DPO and evaluate them.

557
00:20:41,118 --> 00:20:43,799
We've built several evaluation methods

558
00:20:43,799 --> 00:20:44,920
right into studio.

559
00:20:45,279 --> 00:20:47,279
We provide standard benchmarks

560
00:20:47,279 --> 00:20:48,439
like, for example, um,

561
00:20:49,920 --> 00:20:52,118
multitask language understanding,

562
00:20:52,160 --> 00:20:54,439
the MMLU that you see for

563
00:20:54,439 --> 00:20:56,640
many foundation model benchmarks

564
00:20:56,640 --> 00:20:58,858
that's available in Sagemaker Studio.

565
00:20:59,239 --> 00:21:01,368
Math problem solving benchmarks, they're

566
00:21:01,368 --> 00:21:03,459
available already. There are a bunch of other

567
00:21:03,719 --> 00:21:05,140
standard benchmarks available.

568
00:21:05,818 --> 00:21:08,289
But sometimes you may want to use something like an LLM

569
00:21:08,289 --> 00:21:10,529
as a judge, wherein you enter a custom

570
00:21:10,529 --> 00:21:12,608
prompt to define your evaluation

571
00:21:12,608 --> 00:21:13,348
criteria.

572
00:21:13,689 --> 00:21:14,920
That's available as well.

573
00:21:15,209 --> 00:21:18,130
Or if you want to write your own custom evaluators

574
00:21:18,130 --> 00:21:18,789
to determine

575
00:21:19,259 --> 00:21:21,529
what constitutes good for your model

576
00:21:21,529 --> 00:21:22,568
for your use case,

577
00:21:22,890 --> 00:21:25,049
you can go ahead and bring your custom evaluators as

578
00:21:25,049 --> 00:21:27,130
well, and you're going to see a demo of that very, very soon.

579
00:21:27,880 --> 00:21:29,328
So the idea here is,

580
00:21:29,680 --> 00:21:31,739
you know if your customization

581
00:21:31,739 --> 00:21:33,880
is actually working

582
00:21:33,880 --> 00:21:35,880
before you deploy your model to production.

583
00:21:36,680 --> 00:21:38,848
And most interestingly, all

584
00:21:38,848 --> 00:21:40,969
of this is serverless, so you don't

585
00:21:40,969 --> 00:21:43,180
worry about managing infrastructure,

586
00:21:43,449 --> 00:21:45,568
evaluation infrastructure. You just

587
00:21:45,568 --> 00:21:47,969
focus on actually what you're trying to evaluate

588
00:21:47,969 --> 00:21:48,489
in the model.

589
00:21:50,279 --> 00:21:52,289
The second thing, the second feature I want to

590
00:21:52,289 --> 00:21:54,400
talk about is serverless

591
00:21:54,400 --> 00:21:55,150
ML flow.

592
00:21:55,949 --> 00:21:57,959
Now every model customization

593
00:21:57,959 --> 00:21:58,578
training

594
00:21:58,920 --> 00:21:59,979
on SageMaker

595
00:22:00,719 --> 00:22:01,338
Studio

596
00:22:02,000 --> 00:22:03,699
automatically logs your

597
00:22:03,959 --> 00:22:06,160
metrics, your hyperparametters,

598
00:22:06,358 --> 00:22:08,900
your data sets, your model artifacts.

599
00:22:09,358 --> 00:22:11,545
We've built. Experiment tracking right

600
00:22:11,545 --> 00:22:13,545
into studio. So key metrics

601
00:22:13,545 --> 00:22:15,743
like loss training loss

602
00:22:15,743 --> 00:22:17,983
for your model, they are directly available

603
00:22:17,983 --> 00:22:19,314
in your studio interface.

604
00:22:19,664 --> 00:22:21,743
If you want to dig into more details into

605
00:22:21,743 --> 00:22:22,324
metrics

606
00:22:22,584 --> 00:22:25,114
with one click you can launch MLFlow UI

607
00:22:25,424 --> 00:22:26,243
and you can

608
00:22:26,505 --> 00:22:28,545
start looking into more details into your

609
00:22:28,545 --> 00:22:29,664
model training metrics.

610
00:22:30,680 --> 00:22:32,920
You can view the complete lineage for any

611
00:22:32,920 --> 00:22:34,509
model, you know, things like,

612
00:22:34,838 --> 00:22:36,989
hey, which data trained that model, which

613
00:22:36,989 --> 00:22:39,150
configuration did I use? Who ran that

614
00:22:39,150 --> 00:22:40,019
experiment,

615
00:22:41,039 --> 00:22:43,239
so you know exactly which experiments

616
00:22:43,239 --> 00:22:45,318
are running and you know how to reproduce

617
00:22:45,318 --> 00:22:47,559
them maybe a year later, 6 months

618
00:22:47,559 --> 00:22:49,910
later. Once

619
00:22:49,910 --> 00:22:52,229
again, all of this is serverless. You

620
00:22:52,229 --> 00:22:54,269
don't need to worry about infrastructure. We take care

621
00:22:54,269 --> 00:22:55,618
of it. And here,

622
00:22:55,989 --> 00:22:56,910
the best of all,

623
00:22:57,229 --> 00:22:59,390
there is no additional charge for you to use this

624
00:22:59,390 --> 00:22:59,949
capability.

625
00:23:01,769 --> 00:23:03,809
Now let's talk a little bit about compute

626
00:23:03,809 --> 00:23:05,809
options because you have two options over

627
00:23:05,809 --> 00:23:07,848
here. The first is,

628
00:23:08,049 --> 00:23:10,130
uh, serviceless model customization, and we've

629
00:23:10,130 --> 00:23:12,250
been talking about this, um, a little bit

630
00:23:12,250 --> 00:23:13,108
in the past.

631
00:23:13,568 --> 00:23:15,650
Um, in this case, you

632
00:23:15,650 --> 00:23:16,469
submit a job,

633
00:23:17,199 --> 00:23:19,650
uh, we do the job of provisioning

634
00:23:19,650 --> 00:23:21,068
this GPU cluster,

635
00:23:21,449 --> 00:23:23,479
we take care of distributed training,

636
00:23:23,608 --> 00:23:25,689
you know, model parallel, data parallel,

637
00:23:25,769 --> 00:23:27,949
we take care of all of those details for you,

638
00:23:28,390 --> 00:23:30,469
um. We train the model

639
00:23:30,769 --> 00:23:32,868
and then once the model has been trained,

640
00:23:32,890 --> 00:23:34,559
we tear down the infrastructure.

641
00:23:35,358 --> 00:23:37,650
You only pay for the resources that you actually

642
00:23:37,650 --> 00:23:40,009
use for training. You're not paying for idle

643
00:23:40,009 --> 00:23:41,588
compute resources over here.

644
00:23:42,588 --> 00:23:45,160
This is perfect for rapid

645
00:23:45,160 --> 00:23:47,759
iteration and experimentation,

646
00:23:47,920 --> 00:23:49,920
and this is new. It is launched

647
00:23:49,920 --> 00:23:50,719
as of yesterday.

648
00:23:52,368 --> 00:23:54,410
The second compute option is

649
00:23:54,410 --> 00:23:56,769
for customers who want maximum

650
00:23:56,769 --> 00:23:58,848
control, and this is Sagemaker

651
00:23:58,848 --> 00:23:59,578
Hyperpod.

652
00:24:00,009 --> 00:24:01,910
This is a persistent cluster.

653
00:24:02,250 --> 00:24:03,750
You can share it across

654
00:24:04,009 --> 00:24:05,910
training, inference,

655
00:24:06,650 --> 00:24:08,900
your interactive machine learning

656
00:24:08,900 --> 00:24:09,608
workloads,

657
00:24:10,049 --> 00:24:12,608
and it automatically prioritizes

658
00:24:12,608 --> 00:24:13,750
your workloads

659
00:24:14,009 --> 00:24:16,328
so you can maximize the usage of these

660
00:24:16,328 --> 00:24:18,049
expensive GPU resources.

661
00:24:19,348 --> 00:24:22,068
The killer feature here is resiliency.

662
00:24:23,318 --> 00:24:25,559
Node failures are very common

663
00:24:25,559 --> 00:24:27,680
when you're running a large distributed training

664
00:24:27,680 --> 00:24:31,140
system. Hyperpod automatically

665
00:24:31,838 --> 00:24:33,338
creates checkpoints

666
00:24:33,920 --> 00:24:35,989
and then recovers from checkpoints

667
00:24:35,989 --> 00:24:38,160
and does process level restarts. So

668
00:24:38,160 --> 00:24:40,068
that means that when a fault occurs,

669
00:24:40,358 --> 00:24:42,380
you resume from the last checkpoint,

670
00:24:42,759 --> 00:24:44,828
and this is really cool because it saves

671
00:24:44,828 --> 00:24:46,838
up to 40% of time savings

672
00:24:46,838 --> 00:24:47,838
on a long running job.

673
00:24:48,848 --> 00:24:51,150
Now we try to enhance

674
00:24:51,150 --> 00:24:53,239
this even more. So just yesterday we

675
00:24:53,239 --> 00:24:55,680
launched that hyperpod now supports

676
00:24:55,680 --> 00:24:57,259
checkpoint less training,

677
00:24:57,640 --> 00:24:59,680
so that means that you're going

678
00:24:59,680 --> 00:25:01,719
to be able to recover from your faults

679
00:25:01,719 --> 00:25:03,799
even faster in minutes rather

680
00:25:03,799 --> 00:25:04,588
than hours.

681
00:25:04,920 --> 00:25:07,078
So if you're running your distributed training

682
00:25:07,078 --> 00:25:09,019
job for days or months,

683
00:25:09,920 --> 00:25:12,078
this quickly adds up to huge amounts

684
00:25:12,078 --> 00:25:12,799
of savings.

685
00:25:13,838 --> 00:25:16,049
So I presented these two compute options, which

686
00:25:16,049 --> 00:25:17,640
should you choose for your jobs?

687
00:25:18,130 --> 00:25:20,160
Now, if you're iterating quickly,

688
00:25:20,410 --> 00:25:21,949
training smaller models,

689
00:25:22,250 --> 00:25:24,250
uh, you should simply useser

690
00:25:24,250 --> 00:25:26,259
training. But if you're

691
00:25:26,259 --> 00:25:28,420
running long training jobs

692
00:25:28,420 --> 00:25:30,779
for days or months on frontier

693
00:25:30,779 --> 00:25:33,578
scale models or you want granular

694
00:25:33,578 --> 00:25:35,979
control, you

695
00:25:35,979 --> 00:25:37,979
should be using hyperpod in those cases.

696
00:25:38,338 --> 00:25:40,539
Hyperpod is also super flexible. I've

697
00:25:40,539 --> 00:25:42,743
shown. Here Amazon EKS as the

698
00:25:42,743 --> 00:25:45,434
control plane, but Hyperpod supports

699
00:25:45,463 --> 00:25:47,594
orchestration with both SLUM as

700
00:25:47,594 --> 00:25:50,193
well as EKS. And if you want this kind of granular

701
00:25:50,193 --> 00:25:52,314
control, this is the best way to be

702
00:25:52,314 --> 00:25:53,015
able to train

703
00:25:53,604 --> 00:25:56,005
frontier scale foundation

704
00:25:56,005 --> 00:25:58,799
models. So

705
00:25:59,009 --> 00:26:01,118
finally, let me bring this all together

706
00:26:01,118 --> 00:26:02,709
with the 3 pillars

707
00:26:02,969 --> 00:26:04,969
that comprise

708
00:26:04,969 --> 00:26:07,170
of Sagemaker AI model customization.

709
00:26:08,009 --> 00:26:08,598
The

710
00:26:08,858 --> 00:26:10,640
first is choice.

711
00:26:11,140 --> 00:26:13,489
You're not locked into one model

712
00:26:13,489 --> 00:26:15,239
family or one technique.

713
00:26:15,779 --> 00:26:18,118
You can either use open weight models

714
00:26:18,420 --> 00:26:20,699
or you can use proprietary models. You

715
00:26:20,699 --> 00:26:22,328
can use supervised fine tuning,

716
00:26:22,739 --> 00:26:24,118
direct preference optimization,

717
00:26:24,539 --> 00:26:26,199
reinforcement fine tuning.

718
00:26:26,618 --> 00:26:28,779
You can work through the UI. You can

719
00:26:28,779 --> 00:26:30,969
work through an AI agent. You can

720
00:26:30,969 --> 00:26:31,939
work through an SDK.

721
00:26:32,880 --> 00:26:35,118
You can choose what fits your use

722
00:26:35,118 --> 00:26:36,380
case and your team,

723
00:26:36,759 --> 00:26:38,059
so we give you choice.

724
00:26:38,799 --> 00:26:40,838
The second pillar here is

725
00:26:40,838 --> 00:26:41,670
efficiency.

726
00:26:42,420 --> 00:26:44,709
Serverless training eliminates

727
00:26:44,709 --> 00:26:46,068
infrastructure management.

728
00:26:46,939 --> 00:26:49,039
But if you want control, hyperport

729
00:26:49,039 --> 00:26:50,868
gives you the control when you need it.

730
00:26:51,719 --> 00:26:53,880
The customization technique

731
00:26:54,118 --> 00:26:55,059
that used to take

732
00:26:55,559 --> 00:26:56,539
months before

733
00:26:56,838 --> 00:26:58,219
now takes days,

734
00:26:58,519 --> 00:27:00,479
and you will see Giuseppe demo it

735
00:27:02,259 --> 00:27:04,439
and you can see that it is actually that

736
00:27:04,439 --> 00:27:06,578
simple. So your

737
00:27:06,578 --> 00:27:07,539
teams move faster.

738
00:27:08,380 --> 00:27:09,578
The third is safety.

739
00:27:10,759 --> 00:27:13,910
With enterprises, it is non-negotiable

740
00:27:13,910 --> 00:27:16,368
that safety is like a number one priority.

741
00:27:16,769 --> 00:27:18,608
So you get built-in governance,

742
00:27:18,930 --> 00:27:21,170
built-in lineage, you know what

743
00:27:21,170 --> 00:27:22,199
data was used,

744
00:27:22,529 --> 00:27:24,529
which model was used, which

745
00:27:24,529 --> 00:27:25,969
version of that model was used,

746
00:27:26,250 --> 00:27:27,229
who trained it.

747
00:27:27,650 --> 00:27:29,890
You get IAM-based access

748
00:27:29,890 --> 00:27:31,989
control. Everything operates

749
00:27:31,989 --> 00:27:33,890
securely in your VPC,

750
00:27:34,189 --> 00:27:36,588
and your data never leaves your

751
00:27:36,588 --> 00:27:38,979
control. Choice,

752
00:27:39,279 --> 00:27:40,299
efficiency,

753
00:27:40,630 --> 00:27:42,719
safety. That's how

754
00:27:42,828 --> 00:27:45,568
the customization tax is eliminated,

755
00:27:45,949 --> 00:27:48,029
and you turn foundation models to

756
00:27:48,029 --> 00:27:49,650
your competitive advantage.

757
00:27:51,368 --> 00:27:53,608
Now Giuseppe is going to show you a demo

758
00:27:53,608 --> 00:27:55,848
um that can, that's going to bring all of this

759
00:27:55,848 --> 00:27:57,890
together. The problem that we're going

760
00:27:57,890 --> 00:27:59,930
to be solving in this demo is we want

761
00:27:59,930 --> 00:28:02,029
to customize a LA

762
00:28:02,029 --> 00:28:04,328
33.21 billion

763
00:28:04,328 --> 00:28:06,410
instruct model to sound more

764
00:28:06,410 --> 00:28:08,608
humanlike in conversations.

765
00:28:09,279 --> 00:28:11,519
For that we're going to use a data

766
00:28:11,519 --> 00:28:14,390
set that comprises of

767
00:28:14,390 --> 00:28:16,578
conversational questions and two types

768
00:28:16,578 --> 00:28:18,640
of responses. The first response is

769
00:28:18,640 --> 00:28:20,828
human-like responses, and the second

770
00:28:20,828 --> 00:28:22,838
response is formal responses.

771
00:28:23,640 --> 00:28:25,640
Now we're Giuseppe is going to

772
00:28:25,640 --> 00:28:27,759
show you two demos. In the first demo, he's

773
00:28:27,759 --> 00:28:29,818
going to show you um using

774
00:28:30,160 --> 00:28:32,759
direct preference optimization using a UI-based

775
00:28:32,759 --> 00:28:33,459
workflow.

776
00:28:34,459 --> 00:28:36,509
Remember again, DPO is

777
00:28:36,509 --> 00:28:38,670
where you teach a model judgment and

778
00:28:38,670 --> 00:28:41,029
style by showing it a pair of responses.

779
00:28:41,239 --> 00:28:43,559
In this case we're showing it human-like

780
00:28:43,559 --> 00:28:45,680
responses and formal responses, and

781
00:28:45,680 --> 00:28:47,799
we'll train the model to prefer the

782
00:28:47,799 --> 00:28:49,068
human-like responses.

783
00:28:49,358 --> 00:28:50,959
That's what we're going to be doing with DPO.

784
00:28:51,449 --> 00:28:53,739
And you will see how easy it is to

785
00:28:53,739 --> 00:28:55,779
configure this training without writing

786
00:28:55,779 --> 00:28:57,900
any code. In the

787
00:28:57,900 --> 00:29:00,239
second demo, Giuseppe is going to demonstrate

788
00:29:01,049 --> 00:29:03,189
reinforcement learning with AI feedback, and

789
00:29:03,189 --> 00:29:05,358
this is going to be a code-based workflow.

790
00:29:05,979 --> 00:29:08,140
Again, remember, RLAIF is

791
00:29:08,140 --> 00:29:10,289
a technique wherein a model generates a

792
00:29:10,289 --> 00:29:10,799
response

793
00:29:11,059 --> 00:29:13,259
and gets feedback from an AI

794
00:29:13,259 --> 00:29:13,838
model

795
00:29:14,559 --> 00:29:16,689
as to how that model

796
00:29:16,689 --> 00:29:19,338
is evaluated. It gets a reward,

797
00:29:19,618 --> 00:29:21,618
and then your model that you're training

798
00:29:21,618 --> 00:29:23,680
actually works to maximize

799
00:29:23,680 --> 00:29:26,098
that reward. That's what you're doing with RLAIF.

800
00:29:26,920 --> 00:29:28,279
So it's the same model,

801
00:29:28,719 --> 00:29:29,719
same data set,

802
00:29:30,189 --> 00:29:31,670
two different techniques,

803
00:29:32,000 --> 00:29:34,019
all of it training, running

804
00:29:34,019 --> 00:29:35,160
on Sagemaker AI.

805
00:29:36,719 --> 00:29:38,130
Josepe, take it away.

806
00:29:38,759 --> 00:29:40,078
Thanks, thanks, Sumaida.

807
00:29:41,588 --> 00:29:43,578
Let's move on to the demo part

808
00:29:44,068 --> 00:29:44,959
of this session.

809
00:29:46,509 --> 00:29:48,289
Uh, is the video readable?

810
00:29:48,750 --> 00:29:49,789
That's OK, great.

811
00:29:50,828 --> 00:29:53,108
So we are here on SageMaker AI Studio

812
00:29:53,108 --> 00:29:55,229
and we are going to start the model customization

813
00:29:55,229 --> 00:29:55,969
workflow.

814
00:29:56,789 --> 00:29:59,088
Before we get started, let's take a look at the

815
00:29:59,348 --> 00:30:01,709
dataset that we are going to use today. It's

816
00:30:01,709 --> 00:30:03,828
the human-like dataset

817
00:30:03,828 --> 00:30:05,189
which comes with a prompt,

818
00:30:05,509 --> 00:30:07,809
a chosen response, and a rejected,

819
00:30:07,969 --> 00:30:10,380
rejected response since we want to

820
00:30:10,380 --> 00:30:12,549
use direct preference optimization

821
00:30:12,549 --> 00:30:14,088
here. Great.

822
00:30:15,150 --> 00:30:17,189
In order to use this data

823
00:30:17,189 --> 00:30:19,390
set for model customization in S StageMaker

824
00:30:19,390 --> 00:30:21,549
AI, we can take a look at the supported

825
00:30:21,549 --> 00:30:23,549
formats for the data sets. That's this

826
00:30:23,549 --> 00:30:25,549
new section assets in Sage

827
00:30:25,549 --> 00:30:26,650
Maker AI Studio,

828
00:30:27,189 --> 00:30:29,209
which gives you the opportunity to

829
00:30:29,920 --> 00:30:32,108
collect all your datasets here and version

830
00:30:32,108 --> 00:30:33,410
all your datasets here.

831
00:30:33,739 --> 00:30:36,118
So I can click on upload dataset,

832
00:30:36,250 --> 00:30:38,250
and in the required data input

833
00:30:38,250 --> 00:30:40,608
formats I can find the various input

834
00:30:40,608 --> 00:30:42,029
formats that are required

835
00:30:42,289 --> 00:30:44,568
to customize our model depending on the

836
00:30:44,568 --> 00:30:46,549
technique that we are utilizing. So

837
00:30:46,828 --> 00:30:48,809
for direct preference optimization.

838
00:30:50,009 --> 00:30:52,078
We have a prompt, a chosen,

839
00:30:52,199 --> 00:30:54,209
and a rejected response to be

840
00:30:54,209 --> 00:30:56,489
provided. Indeed, it actually matches

841
00:30:56,489 --> 00:30:58,789
the format of the dataset that we have found

842
00:30:59,009 --> 00:31:00,729
on hugging face.

843
00:31:01,049 --> 00:31:03,130
However, since we also want to

844
00:31:03,130 --> 00:31:05,289
evaluate this model, we are going to do some

845
00:31:05,289 --> 00:31:07,289
data preparation on the dataset to prepare

846
00:31:07,289 --> 00:31:08,900
also the evaluation data set.

847
00:31:09,209 --> 00:31:10,670
And in order to do that,

848
00:31:10,969 --> 00:31:13,410
I've written some simple code in a managed

849
00:31:13,410 --> 00:31:15,969
notebook environment provided by Amazon Sagemaker

850
00:31:15,969 --> 00:31:18,009
Studio to transform the data set and

851
00:31:18,009 --> 00:31:20,390
generate the training and evaluation.

852
00:31:21,559 --> 00:31:22,180
Split.

853
00:31:22,818 --> 00:31:25,338
Great. It's a pretty simple

854
00:31:25,338 --> 00:31:27,479
code. We are just loading the data set

855
00:31:27,900 --> 00:31:28,838
and then you

856
00:31:29,380 --> 00:31:31,699
just observing, observing the structure

857
00:31:31,699 --> 00:31:33,818
of the data set that we have already seen on

858
00:31:33,818 --> 00:31:34,759
the hugging face

859
00:31:35,338 --> 00:31:36,939
website and then you can easily see.

860
00:31:37,568 --> 00:31:39,890
How for a given prompt the chosen

861
00:31:39,890 --> 00:31:42,559
response sounds more humanlike, more

862
00:31:42,559 --> 00:31:43,098
conversational,

863
00:31:43,368 --> 00:31:45,959
and the less preferred, the rejected

864
00:31:45,959 --> 00:31:48,529
response is more like an LLM generated

865
00:31:48,529 --> 00:31:50,848
response which is more artificial, more robotic.

866
00:31:51,519 --> 00:31:53,608
Great. This status, we count the

867
00:31:53,608 --> 00:31:55,729
elements in the data set. It has

868
00:31:55,729 --> 00:31:57,650
around 10,000 elements for training.

869
00:31:58,568 --> 00:32:00,930
We are doing a split of training and evaluation

870
00:32:00,930 --> 00:32:02,930
here with the 10% of the

871
00:32:02,930 --> 00:32:05,118
data set used for evaluation. And

872
00:32:05,118 --> 00:32:07,160
then I'm defining a couple of functions that we're

873
00:32:07,160 --> 00:32:09,239
going to use to transform the data set.

874
00:32:09,479 --> 00:32:11,640
No specific transformation for the

875
00:32:11,640 --> 00:32:12,838
training set,

876
00:32:13,118 --> 00:32:15,680
but just a simple transformation to

877
00:32:15,680 --> 00:32:17,959
obtain a query and response for the

878
00:32:17,959 --> 00:32:18,959
evaluation set.

879
00:32:20,348 --> 00:32:22,439
Then we are executing these

880
00:32:22,439 --> 00:32:24,699
transformations here in this part of the code,

881
00:32:25,160 --> 00:32:27,368
and finally displaying just an example,

882
00:32:27,559 --> 00:32:29,680
a random example from the validation data

883
00:32:29,680 --> 00:32:32,118
set to check that we have achieved the

884
00:32:32,118 --> 00:32:33,039
expected response,

885
00:32:33,390 --> 00:32:35,140
the expected structure.

886
00:32:35,719 --> 00:32:37,920
Finally we can save these data sets

887
00:32:37,920 --> 00:32:40,118
to files or we can decide to upload

888
00:32:40,118 --> 00:32:42,180
the data sets to S3 depending on

889
00:32:42,180 --> 00:32:44,439
how we want to handle the customization

890
00:32:44,439 --> 00:32:45,000
workflow.

891
00:32:45,358 --> 00:32:46,019
OK, great.

892
00:32:46,390 --> 00:32:47,239
Our data is ready.

893
00:32:47,680 --> 00:32:49,818
We can. Jump out of the notebook

894
00:32:49,818 --> 00:32:51,529
environment and we can just

895
00:32:51,900 --> 00:32:53,939
use the functionality in the UI to

896
00:32:53,939 --> 00:32:56,049
upload the data set. I have my training

897
00:32:56,049 --> 00:32:56,880
data set here,

898
00:32:57,459 --> 00:32:59,459
then we can save the data set and

899
00:32:59,459 --> 00:33:01,779
this will just upload the data set to Amazon

900
00:33:01,779 --> 00:33:03,759
S3 object storage and

901
00:33:04,779 --> 00:33:07,000
then store the data set metadata

902
00:33:07,170 --> 00:33:08,539
in the registry. And

903
00:33:08,828 --> 00:33:11,078
if we create the data set here, we should,

904
00:33:11,799 --> 00:33:13,959
um sorry, I forgot giving it a name.

905
00:33:14,380 --> 00:33:16,279
We can put just random name for now.

906
00:33:17,088 --> 00:33:19,279
Then the dataset appears in

907
00:33:19,279 --> 00:33:21,368
the catalog, and we can also take a

908
00:33:21,368 --> 00:33:23,578
look at some samples in this data set.

909
00:33:25,259 --> 00:33:27,299
For example, we can see the first sample just

910
00:33:27,299 --> 00:33:29,420
to check the structure, that's what's

911
00:33:29,420 --> 00:33:31,358
your favorite type of cuisine and then

912
00:33:31,779 --> 00:33:33,880
the chosen response, I'm a big fan of

913
00:33:33,880 --> 00:33:34,759
Italian cuisine.

914
00:33:35,140 --> 00:33:37,338
I promise that I didn't put any bias

915
00:33:37,338 --> 00:33:38,459
in this data set.

916
00:33:39,269 --> 00:33:42,229
Great. Let's

917
00:33:42,229 --> 00:33:44,709
move on to the model customization

918
00:33:44,709 --> 00:33:46,969
itself. We're going to customize

919
00:33:46,969 --> 00:33:49,348
a LAMA 3.21 billion

920
00:33:49,348 --> 00:33:50,568
model so

921
00:33:51,189 --> 00:33:53,309
we can search in this new model

922
00:33:53,309 --> 00:33:53,818
hub.

923
00:33:54,189 --> 00:33:56,189
We can search for our model, which is the

924
00:33:56,189 --> 00:33:57,900
3.21 billion instruct.

925
00:33:58,229 --> 00:34:00,618
This is just a model detailed page with all the

926
00:34:00,618 --> 00:34:02,750
information about the model, and then we can move

927
00:34:02,750 --> 00:34:04,750
to the customized section, this

928
00:34:04,750 --> 00:34:06,939
new customized with UI or customize

929
00:34:06,939 --> 00:34:08,148
with code section.

930
00:34:08,748 --> 00:34:10,759
Let's go with the customization through the user

931
00:34:10,759 --> 00:34:11,648
interface,

932
00:34:11,998 --> 00:34:14,458
and what we need to do here is

933
00:34:14,717 --> 00:34:16,878
giving it a name to our model,

934
00:34:16,958 --> 00:34:18,338
give the custom model a name,

935
00:34:18,677 --> 00:34:21,079
then we can choose the technique that we want to use

936
00:34:21,079 --> 00:34:23,188
direct preference optimization

937
00:34:23,188 --> 00:34:25,278
for the data set. We could have also uploaded the

938
00:34:25,278 --> 00:34:27,318
data set here, but since I already have the

939
00:34:27,318 --> 00:34:28,938
dataset in the registry, then

940
00:34:29,197 --> 00:34:31,438
I can select my data set.

941
00:34:31,978 --> 00:34:34,030
And then we find a number

942
00:34:34,030 --> 00:34:36,030
of defaults already configured

943
00:34:36,030 --> 00:34:38,208
in the user interface like the output

944
00:34:38,208 --> 00:34:40,510
location where we want our artifacts to be stored.

945
00:34:40,840 --> 00:34:43,199
As well as the hyperparameter configuration,

946
00:34:43,320 --> 00:34:44,300
all of that comes

947
00:34:44,800 --> 00:34:47,079
out of the box preconfigured recipes

948
00:34:47,079 --> 00:34:49,099
for you to get started. You don't have to

949
00:34:49,800 --> 00:34:51,800
necessarily modify these parameters, but if you want,

950
00:34:51,869 --> 00:34:53,898
you can change the parameters according

951
00:34:53,898 --> 00:34:56,059
to the results of your experiments. And then

952
00:34:56,239 --> 00:34:58,280
you also have an advanced configuration

953
00:34:58,280 --> 00:35:00,599
section where other hyperparameterss

954
00:35:00,599 --> 00:35:02,059
can be tuned, as well as

955
00:35:02,679 --> 00:35:05,340
the configuration of the MLFlow

956
00:35:05,340 --> 00:35:07,679
application that we want to use for experiment tracking.

957
00:35:07,898 --> 00:35:08,530
Um, you know,

958
00:35:08,809 --> 00:35:09,429
you, you,

959
00:35:09,728 --> 00:35:11,769
you can, you know, set that in this, in this

960
00:35:11,769 --> 00:35:12,340
section.

961
00:35:12,889 --> 00:35:13,898
Great, um, yeah,

962
00:35:14,409 --> 00:35:16,840
what we then need to do is just submit

963
00:35:16,840 --> 00:35:17,590
our job

964
00:35:18,010 --> 00:35:19,329
and accept, and

965
00:35:19,849 --> 00:35:22,128
this will trigger a training job,

966
00:35:22,489 --> 00:35:24,570
which is the actual customization job.

967
00:35:24,648 --> 00:35:26,800
This will take some time. We are training for 3

968
00:35:26,800 --> 00:35:28,809
epochs. It's around 1

969
00:35:28,809 --> 00:35:31,239
hour training based on my experiments,

970
00:35:31,610 --> 00:35:33,628
so. What I'm going to do

971
00:35:33,628 --> 00:35:36,168
is show you an example

972
00:35:36,168 --> 00:35:38,188
that I have already pre-run

973
00:35:38,668 --> 00:35:39,458
for DPO.

974
00:35:39,789 --> 00:35:42,070
We can take a look at the custom model

975
00:35:42,070 --> 00:35:44,668
screen, which is where you will land after the customization

976
00:35:44,668 --> 00:35:46,570
job gets executed, and

977
00:35:46,989 --> 00:35:48,679
here you can see

978
00:35:49,188 --> 00:35:51,309
first of all you can see the logs from the model

979
00:35:51,309 --> 00:35:53,369
training. In this

980
00:35:53,369 --> 00:35:56,389
screen we can also look at the performance

981
00:35:56,389 --> 00:35:58,599
that we have achieved some performance metrics

982
00:35:58,599 --> 00:35:59,510
for our job.

983
00:35:59,969 --> 00:36:02,409
One interesting

984
00:36:02,409 --> 00:36:04,559
metric is the margin of the rewards,

985
00:36:04,648 --> 00:36:06,648
which represents essentially the

986
00:36:06,648 --> 00:36:09,369
difference between the rewards

987
00:36:09,369 --> 00:36:11,659
given from our model to the chosen versus

988
00:36:11,659 --> 00:36:13,769
the rejected response. So it means that in some

989
00:36:13,769 --> 00:36:15,949
way it can give us an indication

990
00:36:15,949 --> 00:36:17,489
on how the model is learning.

991
00:36:18,179 --> 00:36:20,208
And if we want to browse

992
00:36:20,208 --> 00:36:22,389
the full metrics, we can open

993
00:36:22,389 --> 00:36:24,429
MLflow directly from the user interface,

994
00:36:24,478 --> 00:36:26,849
and this will open the serverless MLflow

995
00:36:26,849 --> 00:36:27,648
interface

996
00:36:28,000 --> 00:36:30,458
to dive deep into the various model metrics

997
00:36:30,458 --> 00:36:31,659
and how our

998
00:36:32,148 --> 00:36:33,760
training has been behaving great.

999
00:36:35,659 --> 00:36:38,179
We have run

1000
00:36:38,179 --> 00:36:40,219
the fine tuning. The model is ready.

1001
00:36:40,378 --> 00:36:42,378
The next thing that we might want to do

1002
00:36:42,378 --> 00:36:44,418
is test this model very quickly, run

1003
00:36:44,418 --> 00:36:47,090
some inferences, and see if we have

1004
00:36:47,090 --> 00:36:49,500
really fine tuned and adapted our model

1005
00:36:49,500 --> 00:36:51,579
with respect to the task that we wanted

1006
00:36:51,579 --> 00:36:53,739
to achieve. In order to do that, we can

1007
00:36:53,739 --> 00:36:55,898
use the deploy functionality which is

1008
00:36:55,898 --> 00:36:58,059
still available from the user interface

1009
00:36:58,059 --> 00:37:00,179
and You can decide to deploy to Sage

1010
00:37:00,179 --> 00:37:02,579
maker or you can deploy to serverless

1011
00:37:02,579 --> 00:37:04,688
inference in Bedrock. In the case of Sage

1012
00:37:04,688 --> 00:37:06,898
maker, you can choose whether using an existing

1013
00:37:06,898 --> 00:37:09,099
endpoint that is already running, and

1014
00:37:09,099 --> 00:37:09,918
this means that

1015
00:37:10,309 --> 00:37:12,590
Sage maker gives you the ability to

1016
00:37:12,590 --> 00:37:14,898
bring back models on an existing endpoint

1017
00:37:14,898 --> 00:37:17,168
infrastructure so that you can optimize

1018
00:37:17,168 --> 00:37:17,840
on the cost,

1019
00:37:18,099 --> 00:37:20,320
or you can create a brand new

1020
00:37:20,320 --> 00:37:22,989
endpoint. Let's create a new endpoint,

1021
00:37:23,449 --> 00:37:25,849
for example, and just in a few clicks

1022
00:37:25,849 --> 00:37:27,289
we give a name to the endpoint.

1023
00:37:27,648 --> 00:37:28,989
The compute

1024
00:37:29,878 --> 00:37:31,978
I type is already pre-selected

1025
00:37:31,978 --> 00:37:32,619
based on the model

1026
00:37:32,929 --> 00:37:35,168
that we have fine tuned, and we can just

1027
00:37:35,168 --> 00:37:35,849
hit deploy.

1028
00:37:36,119 --> 00:37:38,289
Deployment. I'm not going to run deployment for

1029
00:37:38,289 --> 00:37:40,688
now, but we can go back to the model

1030
00:37:40,688 --> 00:37:42,949
screen where I have already run a deployment

1031
00:37:44,289 --> 00:37:46,728
and look at the details of the deployment.

1032
00:37:46,809 --> 00:37:48,949
We can see here that we have our

1033
00:37:49,409 --> 00:37:51,469
Model deployed

1034
00:37:51,469 --> 00:37:53,628
to the endpoint and since

1035
00:37:54,289 --> 00:37:56,329
for the model training we use

1036
00:37:56,329 --> 00:37:58,668
the lower techniques to optimize

1037
00:37:58,849 --> 00:38:00,929
the fine tuning

1038
00:38:00,929 --> 00:38:02,929
process we have we have deployed

1039
00:38:02,929 --> 00:38:05,050
on the endpoint the

1040
00:38:05,050 --> 00:38:07,050
service automatically deploys on the endpoint

1041
00:38:07,050 --> 00:38:09,050
both the base model as

1042
00:38:09,050 --> 00:38:11,159
well as the adapters as separate

1043
00:38:11,159 --> 00:38:13,369
components and the reason is that in this

1044
00:38:13,369 --> 00:38:15,590
way you have the. Ability to load

1045
00:38:15,590 --> 00:38:18,010
multiple adapters on the same endpoint

1046
00:38:18,010 --> 00:38:20,590
that would be merged at one time

1047
00:38:20,590 --> 00:38:22,668
with the base model to execute the inference. This

1048
00:38:22,668 --> 00:38:25,208
is an additional optimization functionality

1049
00:38:25,208 --> 00:38:26,659
that Sagemaker provides.

1050
00:38:27,119 --> 00:38:29,469
Great. Let's see indeed if our model

1051
00:38:29,469 --> 00:38:31,789
gave us some good results, and

1052
00:38:31,789 --> 00:38:33,688
we can use this playground to do that.

1053
00:38:34,269 --> 00:38:36,449
Here on the right hand side you can choose which

1054
00:38:37,030 --> 00:38:39,070
model you want to invoke, whether the base model or

1055
00:38:39,070 --> 00:38:39,978
the fine tuned one.

1056
00:38:40,289 --> 00:38:42,090
Let's try to invoke

1057
00:38:42,550 --> 00:38:43,409
the base model.

1058
00:38:44,010 --> 00:38:45,889
With a simple question.

1059
00:38:47,590 --> 00:38:48,628
How are you today?

1060
00:38:49,739 --> 00:38:51,739
And the base model is replying,

1061
00:38:51,780 --> 00:38:52,389
I'm

1062
00:38:52,659 --> 00:38:54,820
just a language model. I don't have

1063
00:38:54,820 --> 00:38:57,699
emotions or feelings and so on. So the

1064
00:38:57,699 --> 00:38:58,840
response that we would expect

1065
00:38:59,168 --> 00:39:00,539
from the genetic model.

1066
00:39:01,010 --> 00:39:03,300
Now let's see if I just

1067
00:39:03,300 --> 00:39:05,570
try to invoke the adapter and clear the

1068
00:39:05,570 --> 00:39:08,398
conversation. Today

1069
00:39:12,128 --> 00:39:14,398
We can see that indeed we are having a

1070
00:39:14,398 --> 00:39:16,458
way more informal response

1071
00:39:16,458 --> 00:39:19,059
from our model, including emoticons, exclamation

1072
00:39:19,059 --> 00:39:20,059
marks, and so on.

1073
00:39:20,340 --> 00:39:22,699
Great, but that's not enough. We have deployed

1074
00:39:22,699 --> 00:39:25,719
the model. We want to actually assess its

1075
00:39:25,719 --> 00:39:27,478
performance using an evaluation data set

1076
00:39:27,739 --> 00:39:29,208
and how can I do that?

1077
00:39:29,619 --> 00:39:31,820
So we have a built-in capability

1078
00:39:31,820 --> 00:39:33,860
from the user interface to evaluate

1079
00:39:33,860 --> 00:39:35,659
our model, to set up an evaluation.

1080
00:39:36,019 --> 00:39:38,389
And the way you can do that, so you can use multiple

1081
00:39:38,389 --> 00:39:40,389
techniques as Sumaida mentioned,

1082
00:39:40,550 --> 00:39:41,510
you can use

1083
00:39:42,228 --> 00:39:44,309
standard well known benchmarks for doing

1084
00:39:44,309 --> 00:39:46,349
that. You can implement a customs

1085
00:39:46,349 --> 00:39:48,349
coder which is essentially using

1086
00:39:48,349 --> 00:39:50,750
code, your own function to evaluate

1087
00:39:50,750 --> 00:39:51,369
the model,

1088
00:39:51,628 --> 00:39:53,750
or you can use the LLM as a judge

1089
00:39:53,750 --> 00:39:55,429
technique where we are asking.

1090
00:39:55,719 --> 00:39:57,829
And another bigger model

1091
00:39:58,228 --> 00:40:00,349
to judge the response that

1092
00:40:00,349 --> 00:40:02,750
is coming from our model

1093
00:40:02,929 --> 00:40:05,010
with respect also to the ground truth that

1094
00:40:05,010 --> 00:40:07,010
we are providing to the model from

1095
00:40:07,010 --> 00:40:09,199
the evaluation set that we

1096
00:40:09,199 --> 00:40:10,550
that we select as an input.

1097
00:40:10,889 --> 00:40:12,989
Great. That's indeed what I have used

1098
00:40:12,989 --> 00:40:15,389
as a technique to evaluate this model

1099
00:40:15,648 --> 00:40:16,219
and

1100
00:40:16,958 --> 00:40:19,030
There are multiple models that we can choose for

1101
00:40:19,030 --> 00:40:21,030
the evaluation. I've chosen Cloud

1102
00:40:21,030 --> 00:40:23,469
3.5 for this evaluation,

1103
00:40:23,500 --> 00:40:25,829
and on the metrics side you have some built-in

1104
00:40:25,829 --> 00:40:27,849
metrics, but as you can imagine

1105
00:40:28,128 --> 00:40:30,429
for evaluating how a model,

1106
00:40:30,750 --> 00:40:32,750
the tone and style of a model,

1107
00:40:32,829 --> 00:40:34,869
we might want to use a custom metric

1108
00:40:34,869 --> 00:40:37,050
and instruct our judge model

1109
00:40:37,050 --> 00:40:39,128
on how to interpret whether

1110
00:40:39,789 --> 00:40:41,820
the responses from the fine-tuned model

1111
00:40:41,820 --> 00:40:43,929
are actually humanlike

1112
00:40:43,929 --> 00:40:46,059
responses. And that's what indeed

1113
00:40:46,128 --> 00:40:48,250
I have done, and you can indeed use

1114
00:40:48,250 --> 00:40:50,208
the custom metric screen to do that.

1115
00:40:50,489 --> 00:40:53,280
Custom metric screen is

1116
00:40:53,628 --> 00:40:55,760
Supports both importing a metric

1117
00:40:55,760 --> 00:40:57,860
that you have saved in a JSON file

1118
00:40:57,860 --> 00:41:00,079
for your consumption at any

1119
00:41:00,079 --> 00:41:02,179
time, or you can use the custom

1120
00:41:02,840 --> 00:41:05,519
screen where you can input the instructions

1121
00:41:05,519 --> 00:41:07,519
that you want to provide to the judge model. You

1122
00:41:07,519 --> 00:41:09,800
can also select a scale type

1123
00:41:09,800 --> 00:41:11,898
for rating the response

1124
00:41:11,898 --> 00:41:14,199
that has been provided

1125
00:41:14,199 --> 00:41:14,978
by your model,

1126
00:41:15,478 --> 00:41:16,639
and this rate,

1127
00:41:17,159 --> 00:41:19,599
scale, the scale type could be numerical

1128
00:41:19,599 --> 00:41:20,918
or categorical.

1129
00:41:21,539 --> 00:41:23,889
Let's take a look at the metric that

1130
00:41:24,030 --> 00:41:25,829
I want to use for this evaluation.

1131
00:41:26,280 --> 00:41:28,349
That I have defined is called humanlike

1132
00:41:28,349 --> 00:41:31,110
metric, and as you can see we have a few instructions.

1133
00:41:31,199 --> 00:41:32,458
I hope this is readable,

1134
00:41:32,719 --> 00:41:34,300
but we have a few instructions

1135
00:41:35,239 --> 00:41:37,438
which instruct indeed the

1136
00:41:37,438 --> 00:41:39,820
large model on how to evaluate

1137
00:41:40,199 --> 00:41:42,449
the outputs from the fine-tuned model

1138
00:41:42,449 --> 00:41:44,519
with respect also to the ground truth that we

1139
00:41:44,519 --> 00:41:46,679
are providing from the evaluation data set.

1140
00:41:47,030 --> 00:41:49,860
Indeed you can see here that we have some placeholders

1141
00:41:49,860 --> 00:41:51,869
for the prompt, the ground truth,

1142
00:41:51,958 --> 00:41:52,929
and the prediction.

1143
00:41:53,478 --> 00:41:54,840
Great, let's see.

1144
00:41:55,360 --> 00:41:56,938
The results that they have achieved,

1145
00:41:57,360 --> 00:41:59,579
well, the remaining steps here are just about

1146
00:41:59,800 --> 00:42:01,978
specifying the data set that we want to use

1147
00:42:02,159 --> 00:42:04,320
and submitting the evaluation job. Let's

1148
00:42:04,320 --> 00:42:06,059
see the results that I have

1149
00:42:07,360 --> 00:42:09,398
achieved on this model. I've run a

1150
00:42:09,398 --> 00:42:10,619
few evaluations, but

1151
00:42:10,918 --> 00:42:12,918
we can take a look at the evaluation results from

1152
00:42:12,918 --> 00:42:13,739
this screen

1153
00:42:14,559 --> 00:42:15,909
using LLM as a judge.

1154
00:42:16,309 --> 00:42:18,369
And as you can see, we have here all the

1155
00:42:18,369 --> 00:42:20,059
records that have been evaluated

1156
00:42:20,639 --> 00:42:22,719
also with respect to the base model.

1157
00:42:22,800 --> 00:42:25,059
So I have decided to compare side by side

1158
00:42:25,059 --> 00:42:27,349
the base model results

1159
00:42:27,349 --> 00:42:29,739
and the fine tuned model results, and

1160
00:42:30,119 --> 00:42:32,159
we can see here that for example for

1161
00:42:32,159 --> 00:42:34,280
this prompt the base

1162
00:42:34,280 --> 00:42:36,659
model score was somewhat

1163
00:42:36,659 --> 00:42:38,929
kind of humanlike for the humanlike metric

1164
00:42:39,199 --> 00:42:41,280
for the custom model it's completely

1165
00:42:41,280 --> 00:42:43,239
and so forth for the various prompts,

1166
00:42:43,599 --> 00:42:45,648
but the results that you get from the evolut.

1167
00:42:46,023 --> 00:42:48,273
are still accessible at any time.

1168
00:42:48,554 --> 00:42:50,715
So while you see these results from the user

1169
00:42:50,715 --> 00:42:52,905
interface, they are still accessible at any

1170
00:42:52,905 --> 00:42:55,215
time from S3, which

1171
00:42:55,215 --> 00:42:57,273
you see the links here for the base

1172
00:42:57,273 --> 00:42:59,385
model evaluation results and the custom

1173
00:42:59,385 --> 00:43:01,494
model evaluation results. Indeed, what I have done

1174
00:43:02,155 --> 00:43:04,635
to to have an overall summary

1175
00:43:04,635 --> 00:43:06,375
of the evaluation results, I have.

1176
00:43:07,139 --> 00:43:08,820
Downloaded these results,

1177
00:43:09,829 --> 00:43:12,168
those who are stored in the S3 bucket

1178
00:43:12,418 --> 00:43:14,530
to my notebook, and then I've mapped

1179
00:43:16,070 --> 00:43:17,469
the various categories of the scores,

1180
00:43:17,829 --> 00:43:19,610
not at all, somewhat and completely

1181
00:43:19,869 --> 00:43:21,949
to numbers so that I can compare

1182
00:43:21,949 --> 00:43:23,619
the Responses on the overall

1183
00:43:24,148 --> 00:43:26,280
results and then what I get with the

1184
00:43:26,280 --> 00:43:28,639
simple function is that our fine-tuned

1185
00:43:28,639 --> 00:43:31,418
model has improved in 49%

1186
00:43:31,418 --> 00:43:33,699
of the cases with respect to the base model

1187
00:43:34,000 --> 00:43:36,438
as equal performance for 41%

1188
00:43:36,438 --> 00:43:39,030
of the cases and in about 10%

1189
00:43:39,030 --> 00:43:41,039
of the cases as a degraded

1190
00:43:41,039 --> 00:43:43,039
degraded performance according to the

1191
00:43:43,039 --> 00:43:45,119
judge model that we have used for this

1192
00:43:45,119 --> 00:43:46,489
evaluation. Great.

1193
00:43:48,039 --> 00:43:50,079
Finally, before we move to the next

1194
00:43:50,079 --> 00:43:52,619
example, I wanted to show you that

1195
00:43:52,958 --> 00:43:55,179
all the actions that we execute

1196
00:43:55,179 --> 00:43:57,398
in in the user interface are

1197
00:43:57,398 --> 00:43:59,840
tracked, and we get the full lineage graph

1198
00:43:59,840 --> 00:44:01,878
of the operations that we have

1199
00:44:01,878 --> 00:44:04,030
executed. We can check this from the

1200
00:44:04,030 --> 00:44:06,188
UI. And as you can see

1201
00:44:06,188 --> 00:44:08,898
for the model that we are customizing,

1202
00:44:09,148 --> 00:44:11,530
we get all the links to the

1203
00:44:11,590 --> 00:44:13,809
training job that was used for customization

1204
00:44:13,809 --> 00:44:16,378
as well as the deployments that we have executed

1205
00:44:16,378 --> 00:44:18,510
and the various evaluation runs and

1206
00:44:18,510 --> 00:44:19,110
so forth.

1207
00:44:19,550 --> 00:44:21,449
Great. Finally, last thing

1208
00:44:22,309 --> 00:44:24,628
on this example, you can always

1209
00:44:24,628 --> 00:44:26,208
continue the customization

1210
00:44:26,869 --> 00:44:29,070
starting from the fine tuned

1211
00:44:29,070 --> 00:44:31,909
model that we have obtained by

1212
00:44:31,909 --> 00:44:34,349
either changing hyperparametters on the customization

1213
00:44:34,349 --> 00:44:36,590
using the same technique or changing

1214
00:44:36,590 --> 00:44:37,708
completely the technique.

1215
00:44:38,228 --> 00:44:40,260
Great. This concludes

1216
00:44:40,260 --> 00:44:42,429
the DPO demo. Let

1217
00:44:42,429 --> 00:44:44,389
me move now to the

1218
00:44:44,918 --> 00:44:46,289
Code-based workflow.

1219
00:44:46,570 --> 00:44:48,969
So whatever we have done in the user interface

1220
00:44:48,969 --> 00:44:51,090
can be achieved using this SageMaker

1221
00:44:51,090 --> 00:44:53,449
Python SDK as a code-based

1222
00:44:53,449 --> 00:44:54,050
workflow.

1223
00:44:54,478 --> 00:44:56,668
So similar to the previous example,

1224
00:44:57,369 --> 00:44:58,269
I have

1225
00:44:59,208 --> 00:45:01,329
prepared the data set. I will skip these

1226
00:45:01,329 --> 00:45:01,909
steps

1227
00:45:02,809 --> 00:45:04,889
because they are almost the same steps. The

1228
00:45:04,889 --> 00:45:07,188
only thing to note here is that for

1229
00:45:07,369 --> 00:45:09,780
reinforcement learning with AI feedback.

1230
00:45:10,300 --> 00:45:12,500
The structure of the input format

1231
00:45:12,500 --> 00:45:14,519
is slightly different because we want

1232
00:45:14,519 --> 00:45:16,769
to provide the model that

1233
00:45:16,769 --> 00:45:18,978
is going to give us the feedback on

1234
00:45:18,978 --> 00:45:21,378
the generated responses from the fine-tuned

1235
00:45:21,378 --> 00:45:23,659
model. We need to give this model, we

1236
00:45:23,659 --> 00:45:25,619
can give this model,

1237
00:45:26,139 --> 00:45:27,639
the ground truth information.

1238
00:45:29,128 --> 00:45:31,139
That's the expected structure that you

1239
00:45:31,139 --> 00:45:33,300
can find also from the same

1240
00:45:33,300 --> 00:45:35,360
user interface where the dataset

1241
00:45:35,360 --> 00:45:36,519
formats are defined.

1242
00:45:38,070 --> 00:45:40,449
For evaluation, we are using the same the

1243
00:45:40,449 --> 00:45:41,010
same

1244
00:45:42,188 --> 00:45:42,789
format.

1245
00:45:43,360 --> 00:45:45,530
Great, I will not spend too much

1246
00:45:45,530 --> 00:45:47,969
time on these data preparation stages, but

1247
00:45:47,969 --> 00:45:50,119
what I want to show you then

1248
00:45:50,119 --> 00:45:52,289
is that we are uploading the

1249
00:45:52,289 --> 00:45:54,369
two datasets to S3 in this case, so

1250
00:45:54,369 --> 00:45:56,489
we are not using the user interface for

1251
00:45:56,489 --> 00:45:58,688
the upload. We just upload via code to S3.

1252
00:45:58,929 --> 00:46:01,489
Then what we are doing is we are importing

1253
00:46:01,489 --> 00:46:02,148
from this

1254
00:46:02,550 --> 00:46:04,809
module SageMaker AI registry the

1255
00:46:04,809 --> 00:46:06,228
dataset class,

1256
00:46:06,530 --> 00:46:08,909
and this helps us creating

1257
00:46:08,909 --> 00:46:11,000
the datasets, so the same operation that we have

1258
00:46:11,000 --> 00:46:13,079
seen. The user interface can be done from

1259
00:46:13,079 --> 00:46:16,000
the user interface we create from the

1260
00:46:16,000 --> 00:46:16,978
programmatic interface,

1261
00:46:17,239 --> 00:46:19,360
so we create the two datasets that get

1262
00:46:19,360 --> 00:46:20,958
added to the registry.

1263
00:46:21,280 --> 00:46:23,478
Then the next important thing

1264
00:46:23,478 --> 00:46:25,760
is defining the reward prompt.

1265
00:46:25,918 --> 00:46:28,619
So what the model,

1266
00:46:28,918 --> 00:46:31,320
the feedback model, needs to

1267
00:46:31,320 --> 00:46:33,360
look at to understand how to

1268
00:46:33,360 --> 00:46:35,360
score the responses from

1269
00:46:35,360 --> 00:46:37,949
the model that we are fine tuning at each iteration

1270
00:46:37,949 --> 00:46:41,199
of the reinforcement learning. and

1271
00:46:41,199 --> 00:46:43,389
this is the prompt that I have used. You can

1272
00:46:43,389 --> 00:46:44,289
see that it's a pretty

1273
00:46:44,789 --> 00:46:46,728
sophisticated prompt to check for

1274
00:46:47,489 --> 00:46:49,860
if some response is humanlike or not,

1275
00:46:50,148 --> 00:46:52,228
but what is important is that we need

1276
00:46:52,228 --> 00:46:54,349
to return from this prompt

1277
00:46:54,349 --> 00:46:56,809
adjacent that contains the score

1278
00:46:57,478 --> 00:46:59,539
in a range that I have defined from

1279
00:46:59,539 --> 00:47:00,389
0 to 1,

1280
00:47:00,668 --> 00:47:03,269
the score that has been given by the

1281
00:47:03,269 --> 00:47:04,289
feedback model

1282
00:47:05,550 --> 00:47:07,708
to the prompt to the.

1283
00:47:08,610 --> 00:47:10,628
Responses generated by the

1284
00:47:10,628 --> 00:47:13,179
model that we are fine tuning through reinforcement,

1285
00:47:13,269 --> 00:47:14,429
reinforcement learning.

1286
00:47:14,878 --> 00:47:17,530
Great. Once we define the reward prompt,

1287
00:47:17,668 --> 00:47:20,349
just to show you where this ends, it

1288
00:47:20,349 --> 00:47:22,989
ends up being available also in the evaluator's

1289
00:47:22,989 --> 00:47:25,030
screen inmer AI Studio, so you

1290
00:47:25,030 --> 00:47:27,628
can review the prompt at any time and

1291
00:47:27,909 --> 00:47:30,070
take a look at what the structure

1292
00:47:30,070 --> 00:47:32,179
of the prompt is and the text also

1293
00:47:32,179 --> 00:47:33,570
from the user interface.

1294
00:47:34,389 --> 00:47:36,849
Great. Next

1295
00:47:36,849 --> 00:47:39,050
thing that we have to do is define the

1296
00:47:39,050 --> 00:47:40,250
name of the model,

1297
00:47:40,610 --> 00:47:42,668
the same thing that we have seen in the UI. We will

1298
00:47:42,668 --> 00:47:44,389
do, we will create

1299
00:47:45,329 --> 00:47:47,570
the name for the custom model that we are

1300
00:47:48,119 --> 00:47:48,909
fine tuning

1301
00:47:49,289 --> 00:47:51,469
and The

1302
00:47:51,469 --> 00:47:53,449
key step here is using the

1303
00:47:54,539 --> 00:47:56,579
trainer class of the decision maker

1304
00:47:56,579 --> 00:47:58,699
Python SDK which configures the

1305
00:47:58,699 --> 00:48:00,918
reinforcement learning with the AI feedback

1306
00:48:01,500 --> 00:48:03,820
process, and this class takes as input

1307
00:48:03,820 --> 00:48:04,878
the base model

1308
00:48:05,179 --> 00:48:07,208
that is indeed set to LAA

1309
00:48:07,208 --> 00:48:09,340
3.21 billion instruct.

1310
00:48:09,579 --> 00:48:12,059
It needs the information about the reward

1311
00:48:12,059 --> 00:48:14,458
model and in this case as a reward

1312
00:48:14,458 --> 00:48:16,530
model for the reinforcement learning, we

1313
00:48:16,530 --> 00:48:19,378
are using 120

1314
00:48:19,378 --> 00:48:21,418
billion. And then a

1315
00:48:21,418 --> 00:48:23,648
few other parameters for the S3

1316
00:48:23,648 --> 00:48:26,300
location where we want the outputs to be stored, the

1317
00:48:26,300 --> 00:48:28,659
input data set that

1318
00:48:28,659 --> 00:48:30,820
we are providing for the fine tuning and

1319
00:48:30,820 --> 00:48:32,519
so forth. Great.

1320
00:48:33,438 --> 00:48:35,519
After we create the class, we have

1321
00:48:35,519 --> 00:48:37,958
this method which is called we

1322
00:48:37,958 --> 00:48:41,110
have property to access the hyperparametters

1323
00:48:41,320 --> 00:48:43,438
and look at the default configuration of the hyper

1324
00:48:43,438 --> 00:48:45,579
parameters, but we can also set

1325
00:48:45,719 --> 00:48:47,809
the various hyperparameter values as needed.

1326
00:48:47,918 --> 00:48:48,978
For example, in this case,

1327
00:48:49,320 --> 00:48:51,760
I wanted to just train for one epoch, and

1328
00:48:51,760 --> 00:48:53,760
I have modified the configuration of

1329
00:48:53,760 --> 00:48:54,760
the hyperparametters.

1330
00:48:55,409 --> 00:48:57,500
Finally, the last step is

1331
00:48:57,760 --> 00:48:59,878
running the train, so calling

1332
00:48:59,878 --> 00:49:03,119
the train method of the

1333
00:49:03,119 --> 00:49:05,199
trainer to execute the training to

1334
00:49:05,199 --> 00:49:06,679
trigger the training job.

1335
00:49:07,030 --> 00:49:07,820
Great. What

1336
00:49:09,228 --> 00:49:11,039
the result of this operation

1337
00:49:11,478 --> 00:49:13,809
is available already in my UI,

1338
00:49:14,119 --> 00:49:16,159
and I can show you the model that I

1339
00:49:16,159 --> 00:49:16,840
have trained.

1340
00:49:17,469 --> 00:49:18,478
And it's indeed

1341
00:49:19,260 --> 00:49:22,039
Here For

1342
00:49:22,039 --> 00:49:24,398
this model we are getting quite

1343
00:49:24,398 --> 00:49:26,398
interesting results as well as you can

1344
00:49:26,398 --> 00:49:27,280
see we have

1345
00:49:27,559 --> 00:49:30,070
the rewards which are increasing,

1346
00:49:30,188 --> 00:49:32,478
which we expect, and also the policy

1347
00:49:32,478 --> 00:49:34,639
entropy is decreasing, which means

1348
00:49:34,639 --> 00:49:36,898
that the models during the reinforcement learning

1349
00:49:38,469 --> 00:49:40,679
is going towards more exploitation

1350
00:49:40,679 --> 00:49:42,800
rather than exploration. So it means that it's

1351
00:49:42,800 --> 00:49:45,159
kind of converging in the right direction.

1352
00:49:45,590 --> 00:49:47,938
Great. Same thing, we can monitor the metrics

1353
00:49:47,938 --> 00:49:49,010
from ML flow.

1354
00:49:49,340 --> 00:49:49,969
But again,

1355
00:49:50,369 --> 00:49:52,550
now let's take a look at if

1356
00:49:52,550 --> 00:49:54,750
we had good results with fine tuning

1357
00:49:54,750 --> 00:49:56,750
this model. I have also deployed

1358
00:49:56,750 --> 00:49:58,809
this model to a age make a real-time

1359
00:49:58,809 --> 00:49:59,889
end point. We can go

1360
00:50:00,228 --> 00:50:02,128
and open the playground and see

1361
00:50:02,909 --> 00:50:04,989
what we get from this model, and

1362
00:50:04,989 --> 00:50:06,918
we can ask the same question, Hey,

1363
00:50:07,590 --> 00:50:09,590
how are you today?

1364
00:50:12,610 --> 00:50:14,800
And well, that's the expected answer

1365
00:50:14,800 --> 00:50:16,989
also in this case from the base model. Now

1366
00:50:16,989 --> 00:50:19,039
let's take a look at the adapter model

1367
00:50:19,039 --> 00:50:21,530
that we have fine tuned. Let's clear the

1368
00:50:21,530 --> 00:50:22,760
conversation. Hey,

1369
00:50:23,228 --> 00:50:24,840
how are you

1370
00:50:25,119 --> 00:50:28,860
today? We

1371
00:50:28,860 --> 00:50:31,500
see that again we have a more informal,

1372
00:50:31,619 --> 00:50:33,619
more humanlike response from

1373
00:50:33,619 --> 00:50:35,820
the model, although as you can see in this

1374
00:50:35,820 --> 00:50:37,978
case, we don't see a lot of

1375
00:50:37,978 --> 00:50:40,739
emoticons or exclamation

1376
00:50:40,739 --> 00:50:42,800
marks, and the main reason here

1377
00:50:42,800 --> 00:50:43,349
is that

1378
00:50:43,769 --> 00:50:46,010
We are getting the feedback from

1379
00:50:46,010 --> 00:50:48,168
another large language model and we didn't

1380
00:50:48,168 --> 00:50:49,708
instruct this model

1381
00:50:50,010 --> 00:50:53,030
to give more weight to this kind of

1382
00:50:53,039 --> 00:50:54,840
style, so we didn't put this,

1383
00:50:55,458 --> 00:50:56,958
we didn't inform the model towards

1384
00:50:59,090 --> 00:51:00,728
to achieve this objective.

1385
00:51:01,188 --> 00:51:03,639
Great. Similar to the previous

1386
00:51:03,639 --> 00:51:05,780
example, we can see how the evaluation

1387
00:51:05,780 --> 00:51:06,478
is triggered.

1388
00:51:06,760 --> 00:51:08,800
The same evaluation with as judge

1389
00:51:08,800 --> 00:51:10,878
can be triggered using SageMaker

1390
00:51:10,878 --> 00:51:11,800
Python as the case,

1391
00:51:12,119 --> 00:51:14,438
same thing that we have seen in the UI, and

1392
00:51:14,438 --> 00:51:16,438
we are doing exactly the

1393
00:51:16,438 --> 00:51:18,559
same type of evaluation that I have

1394
00:51:18,559 --> 00:51:20,659
run for the

1395
00:51:21,039 --> 00:51:21,820
use case.

1396
00:51:22,398 --> 00:51:23,750
And in order to do that,

1397
00:51:24,159 --> 00:51:26,659
the custom metrics are defined

1398
00:51:26,800 --> 00:51:27,639
as adjacent.

1399
00:51:28,119 --> 00:51:30,179
In the

1400
00:51:30,179 --> 00:51:32,469
in this variable custom metric list for

1401
00:51:32,469 --> 00:51:34,050
LLM as a judge, and then

1402
00:51:34,309 --> 00:51:35,050
I'm running,

1403
00:51:35,309 --> 00:51:38,090
I'm using this class LLM as a judge evaluator

1404
00:51:38,309 --> 00:51:40,030
to execute the

1405
00:51:40,739 --> 00:51:42,369
LLM as a judge task

1406
00:51:42,668 --> 00:51:44,869
and the model that we are using

1407
00:51:44,869 --> 00:51:47,128
as an evaluator in this case is anthropic cloud

1408
00:51:47,128 --> 00:51:49,219
3.5, so the same, the same model

1409
00:51:49,219 --> 00:51:50,539
that we used in the previous case.

1410
00:51:50,958 --> 00:51:53,159
For sure it's important to make sure that we are using

1411
00:51:53,159 --> 00:51:55,289
a different model from the reward model that

1412
00:51:55,289 --> 00:51:55,829
we use

1413
00:51:56,610 --> 00:51:58,969
during the during the AI feedback.

1414
00:51:59,489 --> 00:52:00,610
Great. That said,

1415
00:52:01,050 --> 00:52:03,119
we can then run the evaluator evaluate

1416
00:52:03,119 --> 00:52:04,659
method and this

1417
00:52:05,168 --> 00:52:07,250
produces the evaluation that you

1418
00:52:07,250 --> 00:52:09,260
see. In this

1419
00:52:09,260 --> 00:52:11,489
screen. Which has

1420
00:52:11,489 --> 00:52:13,309
similar results to

1421
00:52:13,570 --> 00:52:15,590
the previous evaluation. We have a prompt

1422
00:52:15,969 --> 00:52:18,050
the model response, the base score, custom

1423
00:52:18,050 --> 00:52:18,829
score

1424
00:52:19,769 --> 00:52:21,929
that have been provided by the judge

1425
00:52:21,929 --> 00:52:24,010
model, and now we can again

1426
00:52:24,010 --> 00:52:24,860
compare

1427
00:52:26,489 --> 00:52:28,510
the results, the overall results

1428
00:52:28,889 --> 00:52:31,168
using just a couple of

1429
00:52:31,168 --> 00:52:32,188
lines of code

1430
00:52:32,769 --> 00:52:33,429
and

1431
00:52:33,918 --> 00:52:37,079
We can see that in 74%

1432
00:52:37,079 --> 00:52:38,699
of the cases here

1433
00:52:39,000 --> 00:52:41,059
our model has improved

1434
00:52:41,360 --> 00:52:43,378
and has equal performance

1435
00:52:43,378 --> 00:52:46,478
in 24% of the cases degraded

1436
00:52:46,478 --> 00:52:47,059
way less

1437
00:52:47,639 --> 00:52:48,599
than the other models.

1438
00:52:49,059 --> 00:52:51,139
Well, that's kind of expected because

1439
00:52:51,139 --> 00:52:53,139
we have used a large language,

1440
00:52:53,500 --> 00:52:55,739
large language model as a reward model

1441
00:52:55,739 --> 00:52:58,219
during the reinforcement learning, and then we have used

1442
00:52:58,219 --> 00:53:00,780
another large language model to evaluate

1443
00:53:00,780 --> 00:53:01,500
the performance,

1444
00:53:02,010 --> 00:53:04,378
but we didn't consider specific aspects

1445
00:53:04,378 --> 00:53:06,438
as for example, including emoticons

1446
00:53:06,438 --> 00:53:08,489
and so on. So the

1447
00:53:08,489 --> 00:53:10,579
expectation would be that the evaluation

1448
00:53:10,579 --> 00:53:12,780
run through entropic cloud in this case would

1449
00:53:12,780 --> 00:53:14,449
have been, would have been higher.

1450
00:53:14,898 --> 00:53:17,320
Great. That said,

1451
00:53:18,929 --> 00:53:20,489
This concludes the

1452
00:53:20,958 --> 00:53:21,579
the demo

1453
00:53:21,898 --> 00:53:22,559
for today.

1454
00:53:23,039 --> 00:53:25,039
Thanks, and we can now move

1455
00:53:25,039 --> 00:53:27,458
back to the presentation.

1456
00:53:28,398 --> 00:53:30,398
So the key that I would like you to note here

1457
00:53:30,398 --> 00:53:32,559
is Giuseppe very, very

1458
00:53:32,559 --> 00:53:34,958
seamlessly moved between the UI

1459
00:53:34,958 --> 00:53:35,889
and the SDK.

1460
00:53:36,360 --> 00:53:38,360
What you were actually able to do in the SDK, you can come

1461
00:53:38,360 --> 00:53:40,478
into the UI. This is really what I meant by

1462
00:53:40,478 --> 00:53:41,260
saying that

1463
00:53:41,679 --> 00:53:43,119
these are not different products, right?

1464
00:53:43,438 --> 00:53:45,378
That is the same service.

1465
00:53:45,820 --> 00:53:48,019
Depending upon who you are as a user, you

1466
00:53:48,019 --> 00:53:50,179
can choose which interface you want

1467
00:53:50,179 --> 00:53:52,438
or maybe you know, mix and match different interfaces.

1468
00:53:52,780 --> 00:53:54,809
And I don't know how many of you, how

1469
00:53:54,809 --> 00:53:57,300
many of you have actually tried running reinforcement

1470
00:53:57,300 --> 00:53:58,320
learning by yourself.

1471
00:53:59,280 --> 00:54:01,458
These are all extremely complex

1472
00:54:02,449 --> 00:54:05,079
model customization techniques, and

1473
00:54:05,079 --> 00:54:07,438
you're able to actually with a few clicks

1474
00:54:07,438 --> 00:54:09,519
or a few lines of code you're

1475
00:54:09,519 --> 00:54:11,679
able to actually run that and actually add

1476
00:54:11,679 --> 00:54:12,898
real business value,

1477
00:54:13,438 --> 00:54:15,800
and these are all production recipes

1478
00:54:15,800 --> 00:54:18,119
like we have a large team of

1479
00:54:18,119 --> 00:54:20,159
scientists who are actually building these

1480
00:54:20,159 --> 00:54:22,418
recipes so that customers such as you

1481
00:54:22,559 --> 00:54:24,639
can actually use these and run these in production.

1482
00:54:25,179 --> 00:54:27,260
So our goal here is to make all of

1483
00:54:27,260 --> 00:54:29,619
the customization techniques as accessible

1484
00:54:29,619 --> 00:54:30,639
to you as possible,

1485
00:54:30,978 --> 00:54:33,260
and I hope you saw a glimpse of

1486
00:54:33,260 --> 00:54:34,769
what this experience looked like,

1487
00:54:35,059 --> 00:54:37,099
looks like. Please feel free to explore more of

1488
00:54:37,099 --> 00:54:38,478
this on Sagemaker Studio,

1489
00:54:39,099 --> 00:54:39,878
uh, and we have uh

1490
00:54:40,639 --> 00:54:42,699
lots more to uh to

1491
00:54:42,699 --> 00:54:44,918
tell you folks. So feel free to reach out to us and

1492
00:54:45,019 --> 00:54:46,280
we're happy to talk to you about it.

1493
00:54:46,679 --> 00:54:47,429
Thank you so much.

