# AWS re:Invent 2025 SPS-402 会议总结：多智能体编排的LLM微调

## 会议概述

本次会议（SPS-402）是一场400级别的技术深度分享，主题聚焦于如何通过微调大语言模型（LLM）来优化多智能体系统的编排。会议由AWS生成式AI创新中心的模型定制团队成员Hannah Marlo和Charlina Kashaba主讲，并邀请了Cosine AI的Alistair Pulland分享实际案例。

会议首先介绍了智能体（Agent）和多智能体系统的基本概念，强调智能体不仅仅是LLM本身，而是构建在LLM之上、具有自主决策能力的系统。随着模型推理能力的提升、模型选择的多样化、数据集成能力的增强以及智能体开发工具的成熟，智能体应用正在从概念走向生产环境。多智能体系统采用模块化架构，通常包含一个编排器（Orchestrator）和多个专门的子智能体（Sub-agents），这种架构类似于微服务，能够实现独立扩展和灵活调整。

然而，多智能体系统也面临诸多挑战：高延迟、高成本、复杂的任务分解、上下文管理困难以及错误传播问题（类似"传话游戏"效应）。为了应对这些挑战，会议重点介绍了四种模型定制技术：模型蒸馏（Model Distillation）、监督微调（Supervised Fine-tuning）、偏好优化（Preference Optimization）和强化微调（Reinforcement Fine-tuning）。这些技术能够在准确性、延迟和成本之间找到最佳平衡点，通过专业化的智能体提升整体系统性能。

## 详细时间线与关键要点

00:00 - 02:30 | 会议开场与听众调研
- 会议编号SPS-402，400级别技术深度会议
- 通过举手互动了解听众背景：大部分参会者日常使用生成式AI，部分已在工作中应用，少数正在构建多智能体系统

02:30 - 05:00 | 团队介绍与会议议程
- 介绍AWS生成式AI创新中心（Generative AI Innovation Center）
- 该项目于2023年启动，投资2亿美元用于加速客户采用生成式AI
- 团队包括战略顾问、应用科学家和机器学习工程师，已完成超过1000个客户项目
- 模型定制团队专注于三个领域：模型定制、智能体优化、硬件优化

05:00 - 08:00 | 优化目标三角形
- 模型优化需要在三个维度间平衡：准确性（Accuracy）、延迟（Latency）、成本（Cost）
- 不同用例对这三个维度的要求不同，需要根据具体场景调整优先级

08:00 - 12:00 | 什么是智能体？
- 智能体定义：利用AI进行推理、规划并代表人类或系统完成任务的自主软件系统
- LLM本身不是智能体，智能体是构建在LLM之上的系统
- 智能体具有更高级别的自动化能力，可以利用记忆和工具，具有特定目标
- 相比基于规则的系统，智能体更灵活，能够适应意外情况和模糊场景

12:00 - 15:00 | 智能体技术进步的关键因素
- 模型推理能力持续提升，模型选择更加多样化（从大型通用模型到轻量级专用模型）
- 数据和知识库集成能力改善
- 基础设施和智能体协议（如MCP和Agent-to-Agent）更加成熟
- 智能体开发和编排工具不断完善

15:00 - 18:00 | 智能体成熟度阶梯
- 低智能体能力：基于规则的引擎，需要大量人工监督
- 生成式AI助手：单一智能体执行明确定义的任务（如聊天机器人）
- 目标驱动的AI智能体：多智能体系统，能够分解高层目标并协调完成
- 完全自主的智能体系统：最少的人工干预，主要进行治理和审计监督

18:00 - 21:00 | 多智能体系统的驱动因素
- 模型选择的多样性使得可以为不同任务选择合适大小的模型
- 能够开发专门化的智能体并针对特定任务进行微调
- 模块化架构类似于微服务，支持独立扩展和灵活调整
- 降低对单一模型或解决方案的依赖

21:00 - 24:00 | 典型的多智能体架构模式
- 编排器-工作者（Orchestrator-Worker）模式
- 编排器：接收用户全局任务，分解为子任务，通常使用更强大的通用模型
- 子智能体：执行特定子任务，只需了解自己的任务上下文和可用工具
- 编排器负责启动和关闭智能体，协调整体流程

24:00 - 28:00 | 多智能体系统的挑战
- 高延迟：多次模型调用累积导致端到端响应时间过长
- 高成本：使用大型模型执行所有任务会显著增加计算成本
- 任务分解复杂性：如何正确划分问题和分配工作
- 上下文管理困难：在多步骤流程中维护相关信息
- 错误传播：类似"传话游戏"，一个智能体的错误会影响下游所有智能体

28:00 - 32:00 | 智能体定制技术概览
- 四种主要定制技术：模型蒸馏、监督微调、偏好优化、强化微调
- 建议从简单技术开始：先尝试提示工程和RAG，再考虑修改模型权重
- 这些技术不是互斥的，通常以互补方式组合使用
- 定制需要前期投资，但能带来长期的总拥有成本优势

32:00 - 36:00 | 定制的成本与收益
- 前期成本：数据整理和增强、科学与工程投入、基础设施成本、定期模型更新
- 长期收益：优化的token消耗、降低推理费用、更少的错误、更高的效率
- 定制的小型模型可以达到大型模型的性能水平，同时降低延迟和成本

36:00 - 40:00 | 模型蒸馏（Model Distillation）
- 适用场景：智能体重复执行相同任务（每天数千或数百万次）
- 方法：使用大型"教师模型"生成训练数据，训练小型"学生模型"模仿教师行为
- 优势：降低成本和延迟，小型模型需要更少的计算资源和内存
- 特别适合编排器等高频调用的组件

40:00 - 45:00 | 监督微调（Supervised Fine-tuning）
- 适用场景：智能体需要处理领域特定场景，如医疗术语、监管格式、结构化输出
- 方法：使用输入-输出对训练模型，展示期望的行为模式
- 优势：减少错误级联、改善任务分解、更好的上下文管理
- 编排器通过微调学习如何有效分解复杂请求为子任务

45:00 - 48:00 | 监督微调的数据格式示例
- 展示了数学问题求解的输入-输出对示例
- 用户请求帮助解决数学问题，助手提供逐步解决方案

48:00 - 52:00 | 全量微调 vs 参数高效微调（PEFT）
- 全量微调：更新所有模型参数，定制能力最强但计算成本高
- 参数高效方法（如LoRA）：只更新少量参数，更快、更便宜
- PEFT允许训练多个任务特定适配器共享同一基础模型
- 全量微调面临"灾难性遗忘"问题，需要更多数据和计算资源
- 建议优先尝试参数高效方法

52:00 - 57:00 | 偏好优化（Preference Optimization）
- 适用场景：智能体需要以特定风格、语气或格式呈现信息
- 方法：使用RLHF或DPO技术，展示成对的响应（首选和非首选）
- 优势：跨智能体的一致响应、标准化格式、更易解析的输出
- 特别适用于面向客户的智能体或智能体间通信场景

57:00 - 60:00 | 偏好优化的数据格式示例
- 展示了数学问题的两种响应：首选响应提供清晰的逐步说明，被拒绝的响应跳过解释
- 通过数千个这样的示例，模型学习生成有用、清晰、格式正确的响应

60:00 - 65:00 | 强化微调（Reinforcement Fine-tuning）
- 适用场景：需要做出一系列决策的任务，如代码生成、工具选择、多步推理
- 方法：智能体通过试错学习，使用可验证的奖励信号（如单元测试）
- 与监督微调的区别：允许智能体探索不同路径，学习哪些策略有效
- GRPO（Group Relative Policy Optimization）是最常见的技术

65:00+ | GRPO工作原理
- 为每个问题生成多个轨迹（多次尝试）
- 每个轨迹从奖励函数获得评分
- 计算优势项（Advantage Term）区分成功和失败的轨迹
- 模型学习偏好导致成功结果的行动

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


注：本总结基于会议字幕转录，涵盖了会议的主要技术内容。Cosine AI案例研究部分在字幕中未完整呈现。