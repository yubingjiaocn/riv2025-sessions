1
00:00:00,030 --> 00:00:01,380
- Hello, everyone.

2
00:00:01,380 --> 00:00:03,783
Thank you for joining our session today.

3
00:00:05,843 --> 00:00:08,343
We are going to talk about
very interesting topic.

4
00:00:09,390 --> 00:00:13,110
My name is Yaron and I'm
senior engineering manager.

5
00:00:13,110 --> 00:00:15,780
And here with me Kevin
McGehee, a principal engineer,

6
00:00:15,780 --> 00:00:17,843
and we are both from the ElastiCache team.

7
00:00:20,070 --> 00:00:23,040
So today, I'm going to
start with introduction

8
00:00:23,040 --> 00:00:24,340
to ElastiCache and Valkey.

9
00:00:25,380 --> 00:00:27,180
And later, we are going
to dive deep together

10
00:00:27,180 --> 00:00:28,770
on some advanced use cases

11
00:00:28,770 --> 00:00:31,170
and data modeling that
I hope that you're going

12
00:00:31,170 --> 00:00:33,963
to benefit from building your application.

13
00:00:35,550 --> 00:00:38,790
So for those of you of not
familiar with Amazon ElastiCache,

14
00:00:38,790 --> 00:00:43,200
ElastiCache is a fully managed
service that makes it easy

15
00:00:43,200 --> 00:00:45,990
to deploy, operate, and scale

16
00:00:45,990 --> 00:00:49,050
the in-memory data store on the cloud.

17
00:00:49,050 --> 00:00:52,230
Now ElastiCache improves
significantly the performance

18
00:00:52,230 --> 00:00:54,690
of your application by providing

19
00:00:54,690 --> 00:00:56,493
a microseconds response time.

20
00:00:58,080 --> 00:01:01,410
We support three open source engines.

21
00:01:01,410 --> 00:01:06,410
We started with Redis open
source, Memcached, and Valkey.

22
00:01:08,400 --> 00:01:10,740
Valkey is open source,

23
00:01:10,740 --> 00:01:13,260
high performance key value data store.

24
00:01:13,260 --> 00:01:16,890
It was fogged from Redis
open source last year

25
00:01:16,890 --> 00:01:19,290
when Redis changed the license.

26
00:01:19,290 --> 00:01:23,130
And last year, we also
announced that ElastiCache

27
00:01:23,130 --> 00:01:26,013
and MemoryDB support the Valkey engine.

28
00:01:28,680 --> 00:01:30,990
So today, all the example will be based

29
00:01:30,990 --> 00:01:32,550
on the Valkey engine.

30
00:01:32,550 --> 00:01:35,790
And to make it even more fun,
we decided to build together

31
00:01:35,790 --> 00:01:40,140
with you, an application so
we can understand together

32
00:01:40,140 --> 00:01:43,560
how all this data modeling can help us

33
00:01:43,560 --> 00:01:47,790
to build a highly scalable application.

34
00:01:47,790 --> 00:01:50,230
So Kevin and I choose to
build together with you

35
00:01:51,534 --> 00:01:54,300
a massive multiplayer online game.

36
00:01:54,300 --> 00:01:57,180
We definitely need to have
highly scalable technology

37
00:01:57,180 --> 00:01:58,890
to support that.

38
00:01:58,890 --> 00:01:59,940
We need to make sure

39
00:01:59,940 --> 00:02:03,240
that we are providing a very low latency

40
00:02:03,240 --> 00:02:05,340
and high throughput to our customers

41
00:02:05,340 --> 00:02:07,083
so they can enjoy the game.

42
00:02:10,110 --> 00:02:11,520
So let's start from caching

43
00:02:11,520 --> 00:02:15,510
because caching is the fundamental
to make your application

44
00:02:15,510 --> 00:02:16,833
to run much faster.

45
00:02:18,300 --> 00:02:22,200
But I wanna start with
some high level concept.

46
00:02:22,200 --> 00:02:23,310
So let's assume that we start

47
00:02:23,310 --> 00:02:25,800
to build our application on EC2

48
00:02:25,800 --> 00:02:29,220
and we have the persistent data stored

49
00:02:29,220 --> 00:02:32,190
on some relational database.

50
00:02:32,190 --> 00:02:34,350
I'm using here RDS.

51
00:02:34,350 --> 00:02:39,350
So as long as our data is
going on a steady stage,

52
00:02:39,780 --> 00:02:42,300
this architecture can works well.

53
00:02:42,300 --> 00:02:45,990
But when we start to see
more workloads kicks in,

54
00:02:45,990 --> 00:02:49,200
with read and write throughput,
we have multiple option.

55
00:02:49,200 --> 00:02:52,920
One of them is to scale up our RDS

56
00:02:52,920 --> 00:02:56,160
or to add more replica so we can scale out

57
00:02:56,160 --> 00:02:59,520
and benefit from reading throughput

58
00:02:59,520 --> 00:03:03,030
using the replication itself.

59
00:03:03,030 --> 00:03:06,540
Now, RDS itself has the caching layer,

60
00:03:06,540 --> 00:03:08,580
but it keeps the caching on paging

61
00:03:08,580 --> 00:03:12,210
and blocks that does not
contain the result itself.

62
00:03:12,210 --> 00:03:15,900
So from time to time, you
need to fetch the data

63
00:03:15,900 --> 00:03:17,400
from the disk itself,

64
00:03:17,400 --> 00:03:20,793
and there is some additional
latency that involved in that.

65
00:03:22,260 --> 00:03:24,720
For that, we can choose
the Amazon Elastic Cache

66
00:03:24,720 --> 00:03:28,590
that explicitly store the data in memory.

67
00:03:28,590 --> 00:03:33,000
And once we start to populate
the result into the cache,

68
00:03:33,000 --> 00:03:35,310
immediately, we can start with the data

69
00:03:35,310 --> 00:03:40,230
from the cache itself and to
improve the query response time

70
00:03:40,230 --> 00:03:44,703
and actually to receive a
microseconds response read time.

71
00:03:46,110 --> 00:03:48,390
We can also relieve some
pressure from the backend

72
00:03:48,390 --> 00:03:50,370
and from the database itself,

73
00:03:50,370 --> 00:03:52,770
and we can optimize our cost

74
00:03:52,770 --> 00:03:55,233
by allowing the backend to scale down.

75
00:03:58,770 --> 00:04:02,640
Now, let's move forward and
see more advanced use cases.

76
00:04:02,640 --> 00:04:05,790
I want to start to talk
about lazy loading,

77
00:04:05,790 --> 00:04:09,750
which is one of the strategies
to handle the cache.

78
00:04:09,750 --> 00:04:12,690
And I want to show you how
I'm managing that together

79
00:04:12,690 --> 00:04:15,060
with in validation to the cache itself

80
00:04:15,060 --> 00:04:18,120
so we can make sure the cache stay fresh

81
00:04:18,120 --> 00:04:19,803
and without stale data.

82
00:04:20,850 --> 00:04:22,920
So I'm using Amazon ElastiCache

83
00:04:22,920 --> 00:04:26,130
and I'm starting to read the
data directly from the cache.

84
00:04:26,130 --> 00:04:28,290
If we receive the data,
if the data is there,

85
00:04:28,290 --> 00:04:30,990
we have cache it, all is good, we get that

86
00:04:30,990 --> 00:04:33,000
in a microseconds response time.

87
00:04:33,000 --> 00:04:36,180
If the data is not there, we
have a cache miss, then we need

88
00:04:36,180 --> 00:04:40,410
to go to the persistent
layer, the storage itself.

89
00:04:40,410 --> 00:04:42,780
Here, I'm using Amazon DynamoDB,

90
00:04:42,780 --> 00:04:46,470
and right after that, I'm
updating the data into the cache.

91
00:04:46,470 --> 00:04:49,590
So this is the way I'm
lazily populate the data,

92
00:04:49,590 --> 00:04:53,130
warming the data upon every read.

93
00:04:53,130 --> 00:04:56,700
Now, let's assume that
what of the item now

94
00:04:56,700 --> 00:05:00,900
has been up-to-date and
what we are doing here,

95
00:05:00,900 --> 00:05:04,950
I want to make sure the cache stay valid

96
00:05:04,950 --> 00:05:06,900
and have the not stale data.

97
00:05:06,900 --> 00:05:10,740
And for that, I'm going to
use the trigger function

98
00:05:10,740 --> 00:05:13,470
that they have on the Amazon DynamoDB

99
00:05:13,470 --> 00:05:17,040
and to trigger the lambda responsible

100
00:05:17,040 --> 00:05:21,510
to invalidate the
specific item on the cache

101
00:05:21,510 --> 00:05:24,390
so we can make sure that
next time on the lazy loading

102
00:05:24,390 --> 00:05:26,913
on the same path, we're
going to update that data.

103
00:05:29,370 --> 00:05:33,480
Now sometimes, we have a
very hot data in our cache.

104
00:05:33,480 --> 00:05:35,310
And we also use time to leave

105
00:05:35,310 --> 00:05:37,200
because this is the best way

106
00:05:37,200 --> 00:05:40,920
to manage our cache memory.

107
00:05:40,920 --> 00:05:41,970
And we want want to make sure

108
00:05:41,970 --> 00:05:45,000
that the data is also
evicted from time to time.

109
00:05:45,000 --> 00:05:47,220
So we are going to use expiry,

110
00:05:47,220 --> 00:05:51,240
but if we have a very hot item
on the cache, we still want

111
00:05:51,240 --> 00:05:55,860
to make sure that we have the way to read,

112
00:05:55,860 --> 00:05:59,220
to update the time to leave
before it's got expired.

113
00:05:59,220 --> 00:06:01,380
So we can benefit from keep reading it

114
00:06:01,380 --> 00:06:02,760
from the cache itself.

115
00:06:02,760 --> 00:06:06,090
And for that, we can allow
some background task to read

116
00:06:06,090 --> 00:06:08,760
after the data, before
it's going to expire.

117
00:06:08,760 --> 00:06:13,140
In my example here, I'm
using Valkey MULTI command,

118
00:06:13,140 --> 00:06:15,839
and the MULTI command actually queued

119
00:06:15,839 --> 00:06:17,931
multiple different commands and execute it

120
00:06:17,931 --> 00:06:20,640
anatomically on the server side.

121
00:06:20,640 --> 00:06:23,550
What you see here, I'm using the GET first

122
00:06:23,550 --> 00:06:25,140
to receive the item itself.

123
00:06:25,140 --> 00:06:28,050
And then I'm checking
what is the expiry time

124
00:06:28,050 --> 00:06:29,283
that left for this item.

125
00:06:31,500 --> 00:06:34,413
Now, in different example, as you can see,

126
00:06:36,120 --> 00:06:38,850
we have multiple client
that GET slowly listening

127
00:06:38,850 --> 00:06:41,850
the expiry until I'm going
to get the last client

128
00:06:41,850 --> 00:06:44,310
that crossed the five-second threshold

129
00:06:44,310 --> 00:06:49,170
and is the one that going to
update the data on the cache

130
00:06:49,170 --> 00:06:53,400
to make sure that we
have more expiry period

131
00:06:53,400 --> 00:06:55,233
for this hot item.

132
00:06:57,270 --> 00:06:59,160
Now, as a connected to that,

133
00:06:59,160 --> 00:07:02,490
there is the turn head
problem that we want to avoid.

134
00:07:02,490 --> 00:07:05,430
This problem occur when multiple processes

135
00:07:05,430 --> 00:07:09,150
or threads attempt to
proceed simultaneously,

136
00:07:09,150 --> 00:07:10,800
often resulting in high content

137
00:07:10,800 --> 00:07:13,620
and resource strain or a bottlenecks.

138
00:07:13,620 --> 00:07:15,820
And this is something
that we want to avoid.

139
00:07:16,860 --> 00:07:20,400
So let's assume that
I have a very hot item

140
00:07:20,400 --> 00:07:22,890
on my cache that's about to expire.

141
00:07:22,890 --> 00:07:25,410
Now, if all my microservice will try

142
00:07:25,410 --> 00:07:27,575
to fetch the item from the cache now,

143
00:07:27,575 --> 00:07:30,363
now they all will receive a cache miss.

144
00:07:31,290 --> 00:07:32,460
What eventually will happen,

145
00:07:32,460 --> 00:07:34,710
they all automatically
altogether we will go

146
00:07:34,710 --> 00:07:39,710
to their database and we'll put
much more pressure on my RDS

147
00:07:40,410 --> 00:07:42,750
and eventually, we're going
to impact our latency.

148
00:07:42,750 --> 00:07:44,843
And this is something
that we want to avoid.

149
00:07:46,290 --> 00:07:48,840
And for that, we can build kind of barrier

150
00:07:48,840 --> 00:07:52,590
and synchronization
mechanism that allow only one

151
00:07:52,590 --> 00:07:57,270
of the clients to request the
data from the database itself.

152
00:07:57,270 --> 00:07:59,070
And only him going to update

153
00:07:59,070 --> 00:08:01,200
and to populate it into the cache.

154
00:08:01,200 --> 00:08:02,220
So once he finished

155
00:08:02,220 --> 00:08:04,410
and when all the other is waiting for him,

156
00:08:04,410 --> 00:08:07,653
we can read the data
and receive a cache hit.

157
00:08:08,880 --> 00:08:11,130
So how we do that, how we build that?

158
00:08:11,130 --> 00:08:13,770
For that example, I took
only two microservices

159
00:08:13,770 --> 00:08:15,660
and I will go step by step

160
00:08:15,660 --> 00:08:17,860
and follow you how we
can build it together.

161
00:08:18,810 --> 00:08:21,420
So the first microservice,

162
00:08:21,420 --> 00:08:23,910
try to fetch the item from the cache,

163
00:08:23,910 --> 00:08:24,743
but the item is not there.

164
00:08:24,743 --> 00:08:27,600
So we are going to receive a nil.

165
00:08:27,600 --> 00:08:30,570
The second microservice
now is a bit slower,

166
00:08:30,570 --> 00:08:31,950
try to fetch the same item.

167
00:08:31,950 --> 00:08:33,000
You will receive a nil as well

168
00:08:33,000 --> 00:08:35,220
because the item is not there.

169
00:08:35,220 --> 00:08:38,640
Now the first microservice
will try to acquire a lock.

170
00:08:38,640 --> 00:08:40,650
I'm using here the NX

171
00:08:40,650 --> 00:08:43,830
and the EX arguments, which means that NX,

172
00:08:43,830 --> 00:08:47,310
if the lock already exists, I should fail.

173
00:08:47,310 --> 00:08:50,070
And EX means that I'm
going to expire the lock

174
00:08:50,070 --> 00:08:51,303
after five seconds.

175
00:08:53,730 --> 00:08:55,260
Because the lock is now is not there,

176
00:08:55,260 --> 00:08:58,260
I was successful able to acquire them.

177
00:08:58,260 --> 00:09:01,833
And then I'm starting to query
the data from the database.

178
00:09:03,510 --> 00:09:06,900
Now my second microservice
try to acquire the lock.

179
00:09:06,900 --> 00:09:11,130
And remember, because I'm
using the NX, I'm going to fail

180
00:09:11,130 --> 00:09:13,470
because the lock is already there.

181
00:09:13,470 --> 00:09:16,440
Now what I'm going to
do, I'm going to wait

182
00:09:16,440 --> 00:09:18,180
until the lock will be freed.

183
00:09:18,180 --> 00:09:21,540
So I'm waiting and periodically
until the lock is freed

184
00:09:21,540 --> 00:09:22,983
by the first microservice.

185
00:09:23,850 --> 00:09:26,580
In the meanwhile, the
first microservice finished

186
00:09:26,580 --> 00:09:29,790
to fetch the item from
the database is going

187
00:09:29,790 --> 00:09:33,660
to populate it into the
cache and free the lock.

188
00:09:33,660 --> 00:09:37,200
After that, my second microservice
will be able to continue

189
00:09:37,200 --> 00:09:40,353
and to fetch the item
from the cache itself.

190
00:09:43,080 --> 00:09:45,963
Another advanced approach
is the client-side caching.

191
00:09:46,800 --> 00:09:51,540
That means that if I'm even
more sensitive to latency,

192
00:09:51,540 --> 00:09:54,060
I can use my local cache

193
00:09:54,060 --> 00:09:57,630
to serve my on demand workloads,

194
00:09:57,630 --> 00:10:02,013
which means I can benefit from
very low latency capability.

195
00:10:03,090 --> 00:10:07,230
So for that, we have multiple
ways to achieve that.

196
00:10:07,230 --> 00:10:09,000
First, the idea is very simple.

197
00:10:09,000 --> 00:10:11,823
We're fetching the item
from the remote cache.

198
00:10:12,840 --> 00:10:15,480
Once we receive it, we
store it on the local cache.

199
00:10:15,480 --> 00:10:17,760
And from that point, I don't need to talk

200
00:10:17,760 --> 00:10:20,370
with the remote cache because
I can serve the items,

201
00:10:20,370 --> 00:10:22,830
the request for my local cache.

202
00:10:22,830 --> 00:10:25,030
But there is a problem
I need to manage now

203
00:10:26,705 --> 00:10:30,000
the data, the freshness of
the data on my local cache.

204
00:10:30,000 --> 00:10:32,790
One approach is to use the
time to leave, the expiry

205
00:10:32,790 --> 00:10:35,250
that I can put on my local cache.

206
00:10:35,250 --> 00:10:39,210
The second one is to
subscribe to the remote cache

207
00:10:39,210 --> 00:10:42,720
and to receive notification
upon every changes.

208
00:10:42,720 --> 00:10:44,223
So let's see how it works.

209
00:10:45,600 --> 00:10:50,130
The first and the default
approach of Valkey is to remember

210
00:10:50,130 --> 00:10:53,940
to store all the data, the
mapping between the clients

211
00:10:53,940 --> 00:10:55,950
and the keys that they accessed.

212
00:10:55,950 --> 00:10:59,790
And every time we change one of the keys,

213
00:10:59,790 --> 00:11:01,257
I have the data store in my Valkey

214
00:11:01,257 --> 00:11:04,230
and I can update the specific client.

215
00:11:04,230 --> 00:11:05,820
This is very efficient,

216
00:11:05,820 --> 00:11:09,780
but from the other hand, we
need to have additional memory

217
00:11:09,780 --> 00:11:11,793
to store on my server side.

218
00:11:13,260 --> 00:11:17,010
The second way is to
subscribe to a prefix.

219
00:11:17,010 --> 00:11:21,300
In my example here, I can
subscribe to user as a prefix.

220
00:11:21,300 --> 00:11:25,650
So every time any microservice
that's going to update a key

221
00:11:25,650 --> 00:11:26,970
with the same prefix,

222
00:11:26,970 --> 00:11:29,463
I'm going to receive a
notification on that.

223
00:11:31,650 --> 00:11:35,550
So how I'm going to implement
that on my client side?

224
00:11:35,550 --> 00:11:39,540
First, we recommend as a key principle

225
00:11:39,540 --> 00:11:43,140
to use a connection pool,
a long live connection.

226
00:11:43,140 --> 00:11:46,800
The reason for that is because
creating a TCP connection

227
00:11:46,800 --> 00:11:48,728
is an expensive operation,

228
00:11:48,728 --> 00:11:53,728
specifically if you compare
to Valkey get and set,

229
00:11:53,760 --> 00:11:56,070
which is in order of magnitude faster

230
00:11:56,070 --> 00:11:57,453
than creating a connection.

231
00:11:58,980 --> 00:12:01,470
Additionally, using a
client connection pool

232
00:12:01,470 --> 00:12:02,700
with finite side,

233
00:12:02,700 --> 00:12:05,280
reduce the overhead of
connection management

234
00:12:05,280 --> 00:12:08,280
and it's also bound the
concurrent incoming connection

235
00:12:08,280 --> 00:12:10,173
from then client application.

236
00:12:11,070 --> 00:12:14,220
So what I'm doing here, I'm
splitting my connection pool

237
00:12:14,220 --> 00:12:15,900
to two types.

238
00:12:15,900 --> 00:12:17,640
I'm going to have connection zero

239
00:12:17,640 --> 00:12:20,910
as the invalidation connection,
the control connection,

240
00:12:20,910 --> 00:12:23,910
and all the rest from one
to nine going to serve

241
00:12:23,910 --> 00:12:25,503
as the data connections.

242
00:12:27,900 --> 00:12:32,900
Connection zero will subscribe
to the invalidation channel

243
00:12:33,240 --> 00:12:37,020
and all the other connection
will perform the tracking

244
00:12:37,020 --> 00:12:39,840
on redirect, on connection
zero, which mean

245
00:12:39,840 --> 00:12:42,930
that connection zero will encapsulate

246
00:12:42,930 --> 00:12:46,620
all the invalidation to all
the rest of the connection.

247
00:12:46,620 --> 00:12:49,143
So let's assume that I'm
updating one of the keys.

248
00:12:50,160 --> 00:12:51,960
Immediately, connection zero will receive

249
00:12:51,960 --> 00:12:55,500
that update on the invalidation connection

250
00:12:55,500 --> 00:12:57,780
with specific the specific item.

251
00:12:57,780 --> 00:13:01,953
And we'll make sure to update
the data on my local cache.

252
00:13:04,860 --> 00:13:08,310
Now sometimes, we are
thinking how to store the data

253
00:13:08,310 --> 00:13:11,760
on our Valkey engine.

254
00:13:11,760 --> 00:13:15,120
So it's very rich there,
there we have a lot of way

255
00:13:15,120 --> 00:13:16,590
to store the data there.

256
00:13:16,590 --> 00:13:19,230
I want to talk about two
types because in my example,

257
00:13:19,230 --> 00:13:22,500
I want to store some session information.

258
00:13:22,500 --> 00:13:24,540
I will start with hash data structure,

259
00:13:24,540 --> 00:13:28,170
which is a very common data
structure to solve session data.

260
00:13:28,170 --> 00:13:31,140
It's very simple because
it's mapped between key

261
00:13:31,140 --> 00:13:33,340
and value pairs and we have the key

262
00:13:35,100 --> 00:13:37,263
that directly access to the cache itself.

263
00:13:38,400 --> 00:13:40,680
So in order to create the hash itself,

264
00:13:40,680 --> 00:13:42,720
we're going to use the HSET command

265
00:13:42,720 --> 00:13:45,840
and we are going to
provide them the hash name

266
00:13:45,840 --> 00:13:48,963
and a list of peer pairs of key values.

267
00:13:50,670 --> 00:13:53,910
We can also benefit from
a very fast random access

268
00:13:53,910 --> 00:13:56,160
in a constant complexity

269
00:13:56,160 --> 00:13:59,403
to receive a specific item
from the cache itself.

270
00:14:00,990 --> 00:14:02,670
Let's go through a simple code to see

271
00:14:02,670 --> 00:14:04,830
how easy it is to use that.

272
00:14:04,830 --> 00:14:06,900
So here, I'm using a
Valkey GLIDE to connect

273
00:14:06,900 --> 00:14:10,833
to the Valkey server.

274
00:14:12,390 --> 00:14:14,310
And I'm generating a session

275
00:14:14,310 --> 00:14:18,480
and I'm using the native hash map

276
00:14:18,480 --> 00:14:22,110
that I have on my Python
to use the hash map.

277
00:14:22,110 --> 00:14:25,020
Then I'm generating a
UUID that will be served

278
00:14:25,020 --> 00:14:26,673
as the session ID.

279
00:14:28,260 --> 00:14:32,130
Later, I'm going to use the
Hset command, the Valkey command

280
00:14:32,130 --> 00:14:35,250
to serialize the hash data,

281
00:14:35,250 --> 00:14:37,210
and using the session ID itself

282
00:14:38,640 --> 00:14:41,010
to receive the data
from Valkey, I just need

283
00:14:41,010 --> 00:14:43,680
to use the session ID that I generated

284
00:14:43,680 --> 00:14:47,280
and I'm going to use
also the hgetall command.

285
00:14:47,280 --> 00:14:50,940
And later, I can serialize
it to my local objects.

286
00:14:50,940 --> 00:14:51,783
Very simple.

287
00:14:54,750 --> 00:14:57,510
A different data structure
that we also recommend

288
00:14:57,510 --> 00:14:59,040
to use is a JSON.

289
00:14:59,040 --> 00:15:00,540
Very popular.

290
00:15:00,540 --> 00:15:03,570
So JSON is a data structure that consists

291
00:15:03,570 --> 00:15:08,570
of two main object, array
and key value pairs.

292
00:15:10,710 --> 00:15:12,960
In this example, I'm
creating the JSON document

293
00:15:12,960 --> 00:15:14,853
using JSON.SET.

294
00:15:16,020 --> 00:15:18,990
and I can simply use the JSON path

295
00:15:18,990 --> 00:15:22,890
to retrieve specific items from the JSON.

296
00:15:22,890 --> 00:15:25,800
I can use to retrieve
it from the array itself

297
00:15:25,800 --> 00:15:28,683
or from the upper level of the JSON.

298
00:15:30,990 --> 00:15:33,720
I also can use the array append

299
00:15:33,720 --> 00:15:36,783
to add additional item
into the existing JSON.

300
00:15:38,100 --> 00:15:38,933
Very simply.

301
00:15:40,320 --> 00:15:43,260
Let's go through a simple
code to see how it works.

302
00:15:43,260 --> 00:15:47,010
So I'm connecting here to my Valkey server

303
00:15:47,010 --> 00:15:50,313
and I'm creating the JSON
document at the beginning.

304
00:15:51,330 --> 00:15:55,800
Then later, I'm receiving the email

305
00:15:55,800 --> 00:15:58,620
of one of my users, very simply.

306
00:15:58,620 --> 00:16:01,800
And same, I'm doing that
for to retrieve the age.

307
00:16:01,800 --> 00:16:06,570
And I can also use the increment number

308
00:16:06,570 --> 00:16:10,593
to increment one of the
indexes in my JSON itself.

309
00:16:13,170 --> 00:16:16,110
So what is the main
difference between them?

310
00:16:16,110 --> 00:16:17,490
JSON usually used

311
00:16:17,490 --> 00:16:22,490
for a nested document while
hash is a key value pair

312
00:16:22,680 --> 00:16:25,890
for more simple objects
like a session store,

313
00:16:25,890 --> 00:16:29,730
while with JSON, we can do
use it for more complex one.

314
00:16:29,730 --> 00:16:34,110
Now Jason supported Jason pass
to filter on the server side.

315
00:16:34,110 --> 00:16:39,110
While with the hash, and
manipulate it on the client side,

316
00:16:39,120 --> 00:16:43,593
but both of them we can modify
the items on the server side.

317
00:16:47,070 --> 00:16:51,270
So we announced a few
weeks ago, the support

318
00:16:51,270 --> 00:16:55,930
of semantic cache in on
ElastiCache, which can also retrieve

319
00:16:57,960 --> 00:17:02,220
one of the best performance
for vector search in high ratio

320
00:17:02,220 --> 00:17:04,740
of recall, of 95 recall.

321
00:17:04,740 --> 00:17:07,800
So to understand the semantic caching,

322
00:17:07,800 --> 00:17:10,150
we first need to understand
the vector banding.

323
00:17:11,700 --> 00:17:14,550
You can think about
translating unstructured data

324
00:17:14,550 --> 00:17:17,800
like documents, videos, audio, images

325
00:17:20,127 --> 00:17:22,860
to representation of your data.

326
00:17:22,860 --> 00:17:26,010
Now the vector embedding capture
the semantic relationship

327
00:17:26,010 --> 00:17:28,770
between the different semantic elements.

328
00:17:28,770 --> 00:17:33,720
For example, we text the
semantic awareness of a word

329
00:17:33,720 --> 00:17:38,010
like a book can be sometimes
when we read in a book

330
00:17:38,010 --> 00:17:40,983
and sometimes when we
are reserving something.

331
00:17:42,480 --> 00:17:44,370
So to process the create the vector,

332
00:17:44,370 --> 00:17:47,700
we involve ingesting data
from the source, splitting it

333
00:17:47,700 --> 00:17:50,943
to chunks, and then it to vectors.

334
00:17:52,350 --> 00:17:54,510
So let's assume that we have a chat bot

335
00:17:54,510 --> 00:17:57,030
or some other application in our game

336
00:17:57,030 --> 00:18:00,360
and we want to get a query
prompt from the user.

337
00:18:00,360 --> 00:18:03,090
So we will use the same example
like I mentioned before,

338
00:18:03,090 --> 00:18:05,280
like a book, but I'm
going to add the furniture

339
00:18:05,280 --> 00:18:08,820
and finance here and we
will again visualize it

340
00:18:08,820 --> 00:18:11,733
with only three dimensions,
only for the simplicity.

341
00:18:13,170 --> 00:18:16,080
So let's assume that we
already we have a question

342
00:18:16,080 --> 00:18:17,820
on our vector that say,

343
00:18:17,820 --> 00:18:21,360
how long do I have to
wait for the next quest?

344
00:18:21,360 --> 00:18:23,610
And then I'm receiving a new query.

345
00:18:23,610 --> 00:18:27,600
That said, when does the next quest start?

346
00:18:27,600 --> 00:18:32,080
So the cool thing here is
that the semantic search

347
00:18:33,061 --> 00:18:35,790
can actually understand that
this have the same meaning

348
00:18:35,790 --> 00:18:37,593
in returning us the results.

349
00:18:38,820 --> 00:18:40,680
So let's see how it works.

350
00:18:40,680 --> 00:18:44,610
Let's assume that we have
the generative AI application

351
00:18:44,610 --> 00:18:49,610
and our user is now query what
is the next quest to start.

352
00:18:49,620 --> 00:18:51,960
When it should supposed to start.

353
00:18:51,960 --> 00:18:53,190
So what we are going to do,

354
00:18:53,190 --> 00:18:56,250
we are going to query the foundation model

355
00:18:56,250 --> 00:19:00,600
and to ask them when the
next quest is going to start.

356
00:19:00,600 --> 00:19:05,010
This is a bit cost, costly
and expensive from both cost

357
00:19:05,010 --> 00:19:08,100
and performance perspective,

358
00:19:08,100 --> 00:19:10,020
but once we receive the
data, we can return it

359
00:19:10,020 --> 00:19:11,943
to the client itself.

360
00:19:13,320 --> 00:19:15,600
Now, let's assume that
I'm adding ElastiCache

361
00:19:15,600 --> 00:19:17,400
into the architecture.

362
00:19:17,400 --> 00:19:19,950
I'm using the vector search here,

363
00:19:19,950 --> 00:19:24,450
and the user now try to
generate the same question

364
00:19:24,450 --> 00:19:26,880
for my application, but before that,

365
00:19:26,880 --> 00:19:29,550
I'm going to use the
Amazon Teton to generate

366
00:19:29,550 --> 00:19:31,773
for me the embedding vector.

367
00:19:33,600 --> 00:19:36,600
Once I have the vector
itself, I can fetch the item,

368
00:19:36,600 --> 00:19:40,113
the data from my
ElastiCache vector search.

369
00:19:41,130 --> 00:19:43,500
Because this is the first
time I don't have the data yet

370
00:19:43,500 --> 00:19:47,220
on my cache, I'm going
to receive a cache miss

371
00:19:47,220 --> 00:19:50,670
and now I need to go again
to my foundation model,

372
00:19:50,670 --> 00:19:54,570
ask the question, and
get back the response.

373
00:19:54,570 --> 00:19:58,800
But this time I store the
item on my cache for a woman

374
00:19:58,800 --> 00:20:01,230
for the next time I'm going to read it.

375
00:20:01,230 --> 00:20:04,920
But let's make it even
more cool, more complex.

376
00:20:04,920 --> 00:20:07,410
And now I'm receiving similar query,

377
00:20:07,410 --> 00:20:09,510
not this exact, the same query.

378
00:20:09,510 --> 00:20:11,970
And we can understand

379
00:20:11,970 --> 00:20:15,900
that traditional caching
will not work in that way.

380
00:20:15,900 --> 00:20:20,160
We cannot just store the data on the cache

381
00:20:20,160 --> 00:20:24,840
and expect it to be as a cache
sheet in the next iteration.

382
00:20:24,840 --> 00:20:27,540
And for that, we have the vector search.

383
00:20:27,540 --> 00:20:29,880
So let's go through the flow.

384
00:20:29,880 --> 00:20:31,830
We go to the generative AI,

385
00:20:31,830 --> 00:20:34,263
then we create the embedding vector.

386
00:20:35,220 --> 00:20:36,846
Once we generate the impacting vector now

387
00:20:36,846 --> 00:20:38,700
with a different question,

388
00:20:38,700 --> 00:20:41,790
we are asking the
ElastiCache vector search,

389
00:20:41,790 --> 00:20:44,350
but this time we're going
to receive a cache hit

390
00:20:45,420 --> 00:20:49,290
because contextually, understand

391
00:20:49,290 --> 00:20:51,900
that they have the same context.

392
00:20:51,900 --> 00:20:55,620
And with that way, we
are saving the need to go

393
00:20:55,620 --> 00:20:58,290
to foundation model, which is expensive,

394
00:20:58,290 --> 00:21:01,503
both from cost and performance.

395
00:21:04,680 --> 00:21:07,680
Let's go through the API
itself and see how, it works.

396
00:21:07,680 --> 00:21:12,680
So I'm using here the FT.CREATE
to create the index itself.

397
00:21:13,680 --> 00:21:14,910
I'm going to use hash.

398
00:21:14,910 --> 00:21:16,560
I can use also JSON to store the data

399
00:21:16,560 --> 00:21:18,843
but here, I'm showing an example of hash.

400
00:21:19,920 --> 00:21:24,120
We're using the HSNW as the algorithm

401
00:21:24,120 --> 00:21:27,390
because it works well in
high dimensional space

402
00:21:27,390 --> 00:21:30,333
while very optimized for performance.

403
00:21:32,010 --> 00:21:35,463
I'm going to use here dimensions of 128.

404
00:21:36,570 --> 00:21:38,250
In my previous example,
I show you only three,

405
00:21:38,250 --> 00:21:41,250
but here, I'm going to use more dimensions

406
00:21:41,250 --> 00:21:43,260
and I'm going to use this distance metric,

407
00:21:43,260 --> 00:21:45,123
the cosine algorithm.

408
00:21:45,990 --> 00:21:50,990
Now to search the item, I'm
using closest neighbors retrieve

409
00:21:52,560 --> 00:21:57,273
only 10 in that example
for my vector search.

410
00:22:01,260 --> 00:22:04,020
Now I have another option
that I can filter out

411
00:22:04,020 --> 00:22:06,600
some of the results on the
fly on the server side.

412
00:22:06,600 --> 00:22:08,880
And for that, I'm using the index here.

413
00:22:08,880 --> 00:22:13,233
In that example, I'm using
filtering only for per country.

414
00:22:15,630 --> 00:22:18,120
So in this code that
I'm going to show you,

415
00:22:18,120 --> 00:22:22,320
we populate the vector
store in the text data

416
00:22:22,320 --> 00:22:27,320
that build the foundation
of the semantic search body.

417
00:22:27,360 --> 00:22:30,810
So first, we are going
to load the text data

418
00:22:30,810 --> 00:22:32,763
and to split it into chunks.

419
00:22:33,600 --> 00:22:37,023
And for that, we're going to
use the log chain library.

420
00:22:39,720 --> 00:22:41,580
Next we are going to use the chunk

421
00:22:41,580 --> 00:22:45,000
that converted into embedding
using the embedding model.

422
00:22:45,000 --> 00:22:47,520
And we are using the vector Teton

423
00:22:47,520 --> 00:22:50,253
to use the embedded vectors.

424
00:22:54,000 --> 00:22:59,000
And to query the data, we're
going to use the FT.CREATE

425
00:22:59,130 --> 00:23:01,080
that we use for FT.CREATE.

426
00:23:01,080 --> 00:23:04,563
We are going to store it
first on the vector search.

427
00:23:05,970 --> 00:23:08,562
So to fetch the item, to fetch the data,

428
00:23:08,562 --> 00:23:09,990
we are going to use the FT search.

429
00:23:09,990 --> 00:23:11,807
And we use the same here.

430
00:23:11,807 --> 00:23:15,450
We are going to use the closest neighbors.

431
00:23:15,450 --> 00:23:16,750
Here, we using only three.

432
00:23:17,880 --> 00:23:21,810
And once we have it,
we can search the item

433
00:23:21,810 --> 00:23:23,643
on the vector search itself.

434
00:23:27,930 --> 00:23:30,780
Now I want to call to Kevin to continue

435
00:23:30,780 --> 00:23:33,150
with more advanced use cases.

436
00:23:33,150 --> 00:23:33,983
Thank you.

437
00:23:35,358 --> 00:23:38,441
(audience applauds)

438
00:23:40,170 --> 00:23:42,570
- Cool, let's talk about some
additional data structures

439
00:23:42,570 --> 00:23:46,800
that you can do in
Valkey, starting with one

440
00:23:46,800 --> 00:23:51,360
of the functionalities of
publish and subscribing.

441
00:23:51,360 --> 00:23:52,800
So Valkey, as we talked about before,

442
00:23:52,800 --> 00:23:55,500
is a rich data structure server.

443
00:23:55,500 --> 00:24:00,450
And with our MMORPG, we wanna be able

444
00:24:00,450 --> 00:24:03,750
to answer questions about an in-game chat.

445
00:24:03,750 --> 00:24:05,810
We wanna build a chat functionality

446
00:24:05,810 --> 00:24:09,450
for our game where players can join

447
00:24:09,450 --> 00:24:12,600
and participate in an in-game
chat that's ephemeral.

448
00:24:12,600 --> 00:24:14,760
So let's say here we have
a Shadow Dragons Guild

449
00:24:14,760 --> 00:24:16,800
and a bunch of users
that wanna participate

450
00:24:16,800 --> 00:24:18,630
and communicate with each other.

451
00:24:18,630 --> 00:24:22,263
For that, we can use this
pub/subs feature within Valkey.

452
00:24:23,130 --> 00:24:25,560
So if you're not familiar
with the pub/subpattern,

453
00:24:25,560 --> 00:24:29,160
it's a messaging pattern where
there are producer publishers

454
00:24:29,160 --> 00:24:32,280
of messages that are decoupled
from the subscribers.

455
00:24:32,280 --> 00:24:37,260
So a subscriber comes in and
subscribes to a name topic.

456
00:24:37,260 --> 00:24:40,290
So let's say our topic
here, just Shadow Dragons

457
00:24:40,290 --> 00:24:42,000
and a user can come in
and say, I wanna listen

458
00:24:42,000 --> 00:24:44,130
to messages on this topic.

459
00:24:44,130 --> 00:24:46,980
Another user can come and
publish messages to those topics.

460
00:24:46,980 --> 00:24:49,380
And so I can come say hello

461
00:24:49,380 --> 00:24:50,360
and then anyone that subscribed

462
00:24:50,360 --> 00:24:52,653
to that topic will get those messages.

463
00:24:54,000 --> 00:24:57,150
So how does this work inside Valkey?

464
00:24:57,150 --> 00:24:58,830
So the data here is ephemeral.

465
00:24:58,830 --> 00:25:01,530
Valkey is basically just
used as a messaging bus

466
00:25:01,530 --> 00:25:04,770
so that the data is relayed from producers

467
00:25:04,770 --> 00:25:05,910
to consumers directly.

468
00:25:05,910 --> 00:25:09,690
It's not stored in memory
other than for the transition

469
00:25:09,690 --> 00:25:11,580
between the producer and consumer.

470
00:25:11,580 --> 00:25:13,560
It allows at most, once delivery.

471
00:25:13,560 --> 00:25:15,090
So if you're not listening to the topic

472
00:25:15,090 --> 00:25:16,980
then you'll miss the message.

473
00:25:16,980 --> 00:25:18,690
There is, however another data structure

474
00:25:18,690 --> 00:25:20,910
that we're not gonna go
into today called streams

475
00:25:20,910 --> 00:25:24,690
that allows you to store
messages inside Valkey

476
00:25:24,690 --> 00:25:27,270
and in memory for long-term storage

477
00:25:27,270 --> 00:25:29,790
and has a number of other rich features

478
00:25:29,790 --> 00:25:31,830
such as consumer groups
to be able to resume

479
00:25:31,830 --> 00:25:33,112
where you left off.

480
00:25:33,112 --> 00:25:36,180
But pub/sub is off our choice

481
00:25:36,180 --> 00:25:40,293
for this since the chat
message is just ephemeral here.

482
00:25:42,240 --> 00:25:45,330
So there's two different
types of pub/sub in Valkey.

483
00:25:45,330 --> 00:25:47,280
One is what I call classic pub/sub

484
00:25:47,280 --> 00:25:51,090
and one is sharded pub/sub,
which is a relatively new.

485
00:25:51,090 --> 00:25:52,920
We recommend using sharded pub/sub

486
00:25:52,920 --> 00:25:56,190
because it has better
scalability properties.

487
00:25:56,190 --> 00:25:59,310
So in classic pub/sub, all of the messages

488
00:25:59,310 --> 00:26:00,690
that you send are forwarded to all

489
00:26:00,690 --> 00:26:02,070
of the nodes in the cluster.

490
00:26:02,070 --> 00:26:04,170
So you kind of have this large fan out

491
00:26:04,170 --> 00:26:06,360
and that means any
subscriber can go to any node

492
00:26:06,360 --> 00:26:08,820
and get all of the messages on a topic.

493
00:26:08,820 --> 00:26:10,890
But of course, that doesn't
scale as well as you have

494
00:26:10,890 --> 00:26:13,680
very high volume pub/sub traffic.

495
00:26:13,680 --> 00:26:15,990
Whereas with Sharded pub/sub, just like

496
00:26:15,990 --> 00:26:19,230
with the sharded key space
in cluster mode enabled,

497
00:26:19,230 --> 00:26:22,560
it goes to a particular shard
and all of the producers

498
00:26:22,560 --> 00:26:24,780
and consumers know which shard to go to.

499
00:26:24,780 --> 00:26:28,230
And so the fan out is much smaller.

500
00:26:28,230 --> 00:26:30,210
The main downside of using sharded pub/sub

501
00:26:30,210 --> 00:26:34,830
is it doesn't support these
wild card identifiers.

502
00:26:34,830 --> 00:26:37,920
So you can't subscribe to
a large number of topics.

503
00:26:37,920 --> 00:26:40,410
You have to do a specific name
topic so that you get hashed

504
00:26:40,410 --> 00:26:42,003
to the right shard.

505
00:26:43,920 --> 00:26:45,150
So how does this work in practice?

506
00:26:45,150 --> 00:26:46,440
How do you actually do it?

507
00:26:46,440 --> 00:26:48,404
Let's say we have two users
here that like we had before

508
00:26:48,404 --> 00:26:51,690
of Alice and Bob and
they want to join this

509
00:26:51,690 --> 00:26:54,060
and participate in this chat room.

510
00:26:54,060 --> 00:26:56,850
So for sharded pub/sub we'll
use this SSUBSCRIBE command

511
00:26:56,850 --> 00:26:59,310
for Alice to subscribe

512
00:26:59,310 --> 00:27:02,700
and we'll prefix our
key name here with chat,

513
00:27:02,700 --> 00:27:05,130
and then Shadow Dragons being the name.

514
00:27:05,130 --> 00:27:09,270
It'll return back the
identifier of the topic

515
00:27:09,270 --> 00:27:12,810
as well as the one saying
it's successfully subscribed.

516
00:27:12,810 --> 00:27:15,780
Now, Alice can publish, it
doesn't have to be subscribed,

517
00:27:15,780 --> 00:27:18,630
but she can publish to this message

518
00:27:18,630 --> 00:27:23,250
and we can do any sort of
string to publish to a topic,

519
00:27:23,250 --> 00:27:26,250
but here, we'll format it in
JSON so it has enough metadata

520
00:27:26,250 --> 00:27:30,180
to render the chat in the application.

521
00:27:30,180 --> 00:27:33,600
So we'll do SPUBLISH,
we'll get back a one saying

522
00:27:33,600 --> 00:27:35,940
it's been successfully done and
then immediately after that,

523
00:27:35,940 --> 00:27:37,290
we would get this SMESSAGE

524
00:27:37,290 --> 00:27:40,620
because Alice has also
subscribed to that topic

525
00:27:40,620 --> 00:27:42,630
and the message has in it that the topic

526
00:27:42,630 --> 00:27:45,630
that it was published to as
well as the actual message

527
00:27:45,630 --> 00:27:48,123
that the publisher sent.

528
00:27:49,230 --> 00:27:51,660
Meanwhile, Bob can come along

529
00:27:51,660 --> 00:27:55,680
and subscribe to it,
similar to how Alice did

530
00:27:55,680 --> 00:28:00,420
and then do a publish just
before saying Bob here.

531
00:28:00,420 --> 00:28:03,390
And then that message
shows up on both of them

532
00:28:03,390 --> 00:28:06,112
because they're both
subscribed at the same time.

533
00:28:06,112 --> 00:28:09,510
And so you can build pretty
rich applications this way.

534
00:28:09,510 --> 00:28:11,010
It doesn't require a whole lot of memory

535
00:28:11,010 --> 00:28:14,040
since these messages are just used, stored

536
00:28:14,040 --> 00:28:18,633
transiently and relayed
to the subscribers.

537
00:28:21,150 --> 00:28:22,590
Next, let's look at a couple different

538
00:28:22,590 --> 00:28:24,750
probabilistic data structures

539
00:28:24,750 --> 00:28:26,655
that we'll talk about in Valkey.

540
00:28:26,655 --> 00:28:28,740
Valkey of course, is an
in-memory data structure server.

541
00:28:28,740 --> 00:28:30,510
So storage matters, you
don't want to be able,

542
00:28:30,510 --> 00:28:32,649
you don't wanna have to
store a large amount of data

543
00:28:32,649 --> 00:28:35,520
if you don't have to.

544
00:28:35,520 --> 00:28:38,820
And there are some
applications in our MMORPG

545
00:28:38,820 --> 00:28:41,060
where accuracy isn't paramount.

546
00:28:41,060 --> 00:28:44,381
And so we can get away with answers

547
00:28:44,381 --> 00:28:48,180
that perhaps off by a bit.

548
00:28:48,180 --> 00:28:50,940
So one such example is
a unique user count.

549
00:28:50,940 --> 00:28:54,240
So say we wanna keep track of
how many daily active users,

550
00:28:54,240 --> 00:28:57,120
unique daily active users
are playing our game.

551
00:28:57,120 --> 00:28:58,200
Of course, we can do a counter

552
00:28:58,200 --> 00:29:01,260
and get, you know, non-unique users,

553
00:29:01,260 --> 00:29:02,340
but we want to do something

554
00:29:02,340 --> 00:29:04,740
that is a little bit more advanced.

555
00:29:04,740 --> 00:29:06,180
And so normally, let's say

556
00:29:06,180 --> 00:29:08,010
we wanna do it on a day by day basis.

557
00:29:08,010 --> 00:29:11,100
We have four different users
here that are playing our game.

558
00:29:11,100 --> 00:29:12,570
And traditionally, if you just were to use

559
00:29:12,570 --> 00:29:15,150
a set data structure, it
would take O of N space,

560
00:29:15,150 --> 00:29:17,490
we would store all of
the different username

561
00:29:17,490 --> 00:29:20,400
that are user IDs that are
associated with that counter,

562
00:29:20,400 --> 00:29:22,230
and then we could just take the set

563
00:29:22,230 --> 00:29:24,810
and that way, we'd ensure uniqueness.

564
00:29:24,810 --> 00:29:27,630
But within Valkey, there
is a data structure

565
00:29:27,630 --> 00:29:29,640
called a HyperLogLog.

566
00:29:29,640 --> 00:29:32,010
And this can approximate set cardinality.

567
00:29:32,010 --> 00:29:34,380
So it can answer the question
of how many unique users

568
00:29:34,380 --> 00:29:36,270
you might see.

569
00:29:36,270 --> 00:29:39,720
The advantage here is it's
doesn't take linear space.

570
00:29:39,720 --> 00:29:43,530
It is bounded to 12 kilobytes in Valkey

571
00:29:43,530 --> 00:29:46,710
and it has a less than
1% standard error rate,

572
00:29:46,710 --> 00:29:48,513
which is pretty remarkable.

573
00:29:49,350 --> 00:29:51,180
So how exactly does it

574
00:29:51,180 --> 00:29:52,980
and it allows constant time operations

575
00:29:52,980 --> 00:29:56,460
because it's small, so it
doesn't require, you know,

576
00:29:56,460 --> 00:30:00,107
O of N space or login access.

577
00:30:03,720 --> 00:30:06,330
So what is the intuition
behind how HyperLogLog works?

578
00:30:06,330 --> 00:30:09,300
So say these are some poorly drawn coins

579
00:30:09,300 --> 00:30:11,100
that we're flipping.

580
00:30:11,100 --> 00:30:13,050
If I tell you I'm
flipping a bunch of coins,

581
00:30:13,050 --> 00:30:17,400
but I get five heads in a row, it's likely

582
00:30:17,400 --> 00:30:18,660
that I've flipped a bunch of coins

583
00:30:18,660 --> 00:30:21,450
in order to see a sequence
of five heads in a row.

584
00:30:21,450 --> 00:30:23,220
And this is sort of the intuition

585
00:30:23,220 --> 00:30:25,530
behind how the algorithm works,

586
00:30:25,530 --> 00:30:30,210
is it takes these values, observes them

587
00:30:30,210 --> 00:30:33,210
and then it counts repeating patterns

588
00:30:33,210 --> 00:30:36,180
that are less and less likely over time.

589
00:30:36,180 --> 00:30:38,370
So what does that exactly mean?

590
00:30:38,370 --> 00:30:42,210
The algorithm of
HyperLogLog takes your user

591
00:30:42,210 --> 00:30:45,660
or whatever string that you
want to be able to count.

592
00:30:45,660 --> 00:30:47,970
And it runs it through this murmur hash,

593
00:30:47,970 --> 00:30:52,970
which produces a 64-bit uniformly
distributed binary string.

594
00:30:54,300 --> 00:30:58,560
So it's just a, what
seemingly random value

595
00:30:58,560 --> 00:31:00,900
of zeros and ones.

596
00:31:00,900 --> 00:31:04,950
And in Valkey, it takes the first 14 byte

597
00:31:04,950 --> 00:31:08,700
or bits of that binary value

598
00:31:08,700 --> 00:31:11,490
and computes a hash bucket out of it.

599
00:31:11,490 --> 00:31:15,900
So there's 16,000 buckets
that can be computed

600
00:31:15,900 --> 00:31:17,313
within the HyperLogLog.

601
00:31:18,900 --> 00:31:21,630
And then the remaining digits, it takes

602
00:31:21,630 --> 00:31:23,490
and accounts the number of leading zeros.

603
00:31:23,490 --> 00:31:26,160
And this is the analogy of coin flips.

604
00:31:26,160 --> 00:31:28,110
The larger number of leading zeros

605
00:31:28,110 --> 00:31:29,010
that are happening

606
00:31:29,010 --> 00:31:33,063
in this uniformly
distributed binary string.

607
00:31:34,255 --> 00:31:36,570
The, you know, more values

608
00:31:36,570 --> 00:31:41,133
you may have seen in order
to observe that pattern.

609
00:31:42,510 --> 00:31:44,460
And what it does is it takes the bucket

610
00:31:44,460 --> 00:31:47,220
and then the number of leading zeros

611
00:31:47,220 --> 00:31:50,460
and it stores the maximum number
of leading zeros per bucket

612
00:31:50,460 --> 00:31:52,900
that it has observed

613
00:31:55,003 --> 00:31:58,020
in the sequence of strings
that being added to it.

614
00:31:58,020 --> 00:32:00,450
And then it takes the
harmonic mean of this overall.

615
00:32:00,450 --> 00:32:04,000
So it's a mathematical
equation to basically identify

616
00:32:05,028 --> 00:32:07,263
the total number of observations.

617
00:32:09,270 --> 00:32:11,160
Luckily, you don't have
to actually deal with any

618
00:32:11,160 --> 00:32:14,010
of that math when you're using
the HyperLogLog in Valkey.

619
00:32:14,010 --> 00:32:16,920
It has a pretty simple
API to interact with it,

620
00:32:16,920 --> 00:32:19,710
which is just these PF prefix

621
00:32:19,710 --> 00:32:22,680
and PFADD will add a number

622
00:32:22,680 --> 00:32:26,010
of items into a particular HyperLogLog.

623
00:32:26,010 --> 00:32:28,890
So say, we are want to
name it daily active users

624
00:32:28,890 --> 00:32:31,020
with the date, we can add three to it,

625
00:32:31,020 --> 00:32:33,630
we'll get back a one
saying it's been successful

626
00:32:33,630 --> 00:32:35,820
and if we issue a count command,

627
00:32:35,820 --> 00:32:38,283
then we'll get back three in this case.

628
00:32:39,510 --> 00:32:40,590
And if we were to try

629
00:32:40,590 --> 00:32:44,220
and add the same user, it'll
come up with the same hash,

630
00:32:44,220 --> 00:32:46,020
it shouldn't modify it.

631
00:32:46,020 --> 00:32:49,450
We'll get back a zero saying
it has been unsuccessful

632
00:32:50,490 --> 00:32:53,133
and the count is unchanged in this case.

633
00:32:54,960 --> 00:32:57,150
There's some other interesting
properties here too.

634
00:32:57,150 --> 00:32:58,830
So say we wanna be able to subdivide it,

635
00:32:58,830 --> 00:33:01,350
we now wanna, instead of
just doing daily active users

636
00:33:01,350 --> 00:33:02,910
over the entire MMORPG,

637
00:33:02,910 --> 00:33:06,060
we wanna kind of partition
it into different games.

638
00:33:06,060 --> 00:33:09,270
So we can add three, the
three users to game one

639
00:33:09,270 --> 00:33:12,930
and maybe just Alice and
Doris to game two here.

640
00:33:12,930 --> 00:33:16,290
So they're different HyperLogLogs.

641
00:33:16,290 --> 00:33:17,910
HyperLogLogs are composable.

642
00:33:17,910 --> 00:33:21,210
So we can use this command called PFMERGE

643
00:33:21,210 --> 00:33:23,160
where we can take the two of them.

644
00:33:23,160 --> 00:33:24,000
And it basically goes through

645
00:33:24,000 --> 00:33:25,770
in that data structure we talked about

646
00:33:25,770 --> 00:33:29,670
and just updates the max
number of leading zeros

647
00:33:29,670 --> 00:33:33,900
based off of the number in each bucket.

648
00:33:33,900 --> 00:33:37,620
And it allows you to then get a total sum

649
00:33:37,620 --> 00:33:41,130
of unique users across
multiple HyperLogLogs.

650
00:33:41,130 --> 00:33:43,830
So in this case, we have
only four unique users

651
00:33:43,830 --> 00:33:46,050
even though we've had five
different observations.

652
00:33:46,050 --> 00:33:47,610
So we'll get back a four when we count

653
00:33:47,610 --> 00:33:49,923
that aggregated HyperLogLog.

654
00:33:52,680 --> 00:33:57,540
So I ran a quick little
simulation of the memory usage

655
00:33:57,540 --> 00:34:01,173
and error rate of a set and a HyperLogLog.

656
00:34:02,160 --> 00:34:05,700
So in adding 10,000 numbers to a set,

657
00:34:05,700 --> 00:34:08,220
you'd get back 10,000 exactly

658
00:34:08,220 --> 00:34:10,680
because it's storing every
single number in the string

659
00:34:10,680 --> 00:34:13,590
and then calculating that cardinality.

660
00:34:13,590 --> 00:34:15,660
But the memory usage of
the set as calculated

661
00:34:15,660 --> 00:34:18,003
by Valkey is about 420 kilobytes.

662
00:34:19,380 --> 00:34:22,140
Whereas doing the same
thing with a HyperLogLog,

663
00:34:22,140 --> 00:34:24,390
I got back 9,987.

664
00:34:24,390 --> 00:34:26,073
So a 0.13% error,

665
00:34:27,210 --> 00:34:30,090
but the memory usage here is 12 kilobytes.

666
00:34:30,090 --> 00:34:31,800
So you can see that that 12 kilobytes

667
00:34:31,800 --> 00:34:33,840
will not expand beyond that.

668
00:34:33,840 --> 00:34:36,660
It's the maximum size of the HyperLogLog,

669
00:34:36,660 --> 00:34:39,840
but the set usage would
increase with the number

670
00:34:39,840 --> 00:34:41,673
of unique users that get added.

671
00:34:44,580 --> 00:34:46,860
Next, let's answer a
slightly different question

672
00:34:46,860 --> 00:34:48,720
of whether two players know each other.

673
00:34:48,720 --> 00:34:50,070
So say you wanna keep track of your game,

674
00:34:50,070 --> 00:34:51,420
whether you've observed or interacted

675
00:34:51,420 --> 00:34:52,740
with a player before,

676
00:34:52,740 --> 00:34:55,260
basically, forms this
graph that you'd have

677
00:34:55,260 --> 00:34:58,050
to keep track of the different
users that have engaged

678
00:34:58,050 --> 00:35:00,840
with each other, which
you could store in Valkey

679
00:35:00,840 --> 00:35:02,160
with sets again.

680
00:35:02,160 --> 00:35:03,540
We have the different interactions

681
00:35:03,540 --> 00:35:05,700
for each individual player.

682
00:35:05,700 --> 00:35:06,961
The downside of course

683
00:35:06,961 --> 00:35:10,320
is that this is also O of N storage space.

684
00:35:10,320 --> 00:35:11,460
So can we do better than that?

685
00:35:11,460 --> 00:35:12,960
Can we not have to store

686
00:35:12,960 --> 00:35:16,650
all these sort of common combinations?

687
00:35:16,650 --> 00:35:18,690
And with that we can use a bloom filter,

688
00:35:18,690 --> 00:35:22,950
which is a relatively recent
data structure added to Valkey.

689
00:35:22,950 --> 00:35:25,380
And as answers a different question,

690
00:35:25,380 --> 00:35:27,960
it answers the membership
testing question.

691
00:35:27,960 --> 00:35:32,100
So whether a given user has interacted

692
00:35:32,100 --> 00:35:37,100
with the particular user
that you're querying for.

693
00:35:37,500 --> 00:35:40,860
This can result in over
90% memory usage savings

694
00:35:40,860 --> 00:35:44,490
over a set, depending
on your configuration.

695
00:35:44,490 --> 00:35:45,600
Unlike the HyperLogLog,

696
00:35:45,600 --> 00:35:47,250
this has some configuration options,

697
00:35:47,250 --> 00:35:50,160
it has this tuneable false positive rate

698
00:35:50,160 --> 00:35:52,470
that you're able to tune.

699
00:35:52,470 --> 00:35:54,720
The lower the false positive rate,

700
00:35:54,720 --> 00:35:58,650
the higher the memory usage
of that data structure.

701
00:35:58,650 --> 00:36:02,490
But by default it stores it
is a 1% false positive rate.

702
00:36:02,490 --> 00:36:04,320
So the error that you can get

703
00:36:04,320 --> 00:36:06,960
is that you would've seen interacted

704
00:36:06,960 --> 00:36:10,083
with someone when in reality,
you hadn't done that.

705
00:36:12,360 --> 00:36:14,880
This is also a constant time operation.

706
00:36:14,880 --> 00:36:17,040
It uses a number of
different hash functions,

707
00:36:17,040 --> 00:36:20,040
which we'll see in a
minute in order to test

708
00:36:20,040 --> 00:36:24,450
and to add members to this set.

709
00:36:24,450 --> 00:36:28,983
But it's typically a relatively
a small constant time.

710
00:36:30,360 --> 00:36:31,950
So how does this work under the cover?

711
00:36:31,950 --> 00:36:36,780
So the intuition is that you
have this fixed size bit array

712
00:36:36,780 --> 00:36:38,790
that has a number of
different hash functions

713
00:36:38,790 --> 00:36:39,623
that operate on it.

714
00:36:39,623 --> 00:36:42,730
So say for this example,
we have 10 bits here

715
00:36:43,860 --> 00:36:45,780
and we wanna add Bob to it.

716
00:36:45,780 --> 00:36:48,300
So let's say we just
have two hash functions.

717
00:36:48,300 --> 00:36:51,390
We'll calculate the hash one of bob

718
00:36:51,390 --> 00:36:54,360
and then modular it by the number of bits

719
00:36:54,360 --> 00:36:56,310
and we get bit number five.

720
00:36:56,310 --> 00:36:59,550
We'll go and update that in our bid array

721
00:36:59,550 --> 00:37:00,383
and we'll do the same thing

722
00:37:00,383 --> 00:37:02,490
with the second hash function here,

723
00:37:02,490 --> 00:37:05,100
which is another unique hash function.

724
00:37:05,100 --> 00:37:08,703
We'll get the value of nine
and we'll update those too.

725
00:37:10,320 --> 00:37:13,350
And we can do the same thing
with Alice, with everyone

726
00:37:13,350 --> 00:37:16,170
that we add goes through
the same function,

727
00:37:16,170 --> 00:37:19,110
uses this murmur hash as well.

728
00:37:19,110 --> 00:37:20,340
Well we can update Alice here.

729
00:37:20,340 --> 00:37:22,680
There's conflict, you get back five

730
00:37:22,680 --> 00:37:25,230
and five is already one, we
don't do anything with it.

731
00:37:25,230 --> 00:37:29,400
And let's say hash two of
Alice is the first position.

732
00:37:29,400 --> 00:37:31,173
And so we'll go and update that.

733
00:37:33,960 --> 00:37:35,490
Let's go back one second.

734
00:37:35,490 --> 00:37:39,870
So when you go and test membership,

735
00:37:39,870 --> 00:37:42,600
it goes through the
same exact process too.

736
00:37:42,600 --> 00:37:46,710
So say we want to check to see
if someone else is present,

737
00:37:46,710 --> 00:37:50,580
we'll go through and observe each

738
00:37:50,580 --> 00:37:55,080
whether the bits that are
hashed are one or zero.

739
00:37:55,080 --> 00:37:57,030
And this is why because the hash function

740
00:37:57,030 --> 00:38:00,810
is non-unique, you can
have false positives.

741
00:38:00,810 --> 00:38:02,010
So in this case, we can see

742
00:38:02,010 --> 00:38:03,960
that that Alice was not previously added,

743
00:38:03,960 --> 00:38:05,940
that that first bit is a zero.

744
00:38:05,940 --> 00:38:10,920
But some, if for example the
you get back position bit five

745
00:38:10,920 --> 00:38:13,290
and bit nine for another user,

746
00:38:13,290 --> 00:38:16,170
then you might have a false positive

747
00:38:16,170 --> 00:38:18,210
where it conflicted with Bob.

748
00:38:18,210 --> 00:38:21,600
So if they're all zero or
you know, at least one zero,

749
00:38:21,600 --> 00:38:23,670
then it was definitely
never added to the set.

750
00:38:23,670 --> 00:38:26,670
But if all of the values
that we look at are one,

751
00:38:26,670 --> 00:38:29,580
then it was maybe added
to the set previously.

752
00:38:29,580 --> 00:38:32,370
So we don't have to actually
store the underlying strings,

753
00:38:32,370 --> 00:38:35,283
just basically some representation
of the hashed value.

754
00:38:36,840 --> 00:38:40,620
So the memory usage savings
are pretty significant here.

755
00:38:40,620 --> 00:38:44,220
So if you look at the a
1% false positive rate,

756
00:38:44,220 --> 00:38:49,220
you can get about 112 million
items with 128 megabytes

757
00:38:49,230 --> 00:38:53,340
or scaling linearly, 448
million items with 512.

758
00:38:53,340 --> 00:38:55,770
And if you were to drop
the false positive rate

759
00:38:55,770 --> 00:39:00,770
to 0.1%, then you can get,
using the same memory usage,

760
00:39:01,530 --> 00:39:04,500
you have still a relatively
large capacity, 74 million

761
00:39:04,500 --> 00:39:08,643
with 128 megs or 298 million with 512.

762
00:39:09,780 --> 00:39:13,203
And of course, you can kinda
tune this as your needs fit.

763
00:39:16,050 --> 00:39:19,110
How do you actually use this in Valkey?

764
00:39:19,110 --> 00:39:21,060
So again, it has a relatively simple API

765
00:39:21,060 --> 00:39:23,100
where you can just interact
with it as you would've set.

766
00:39:23,100 --> 00:39:26,340
So there's this BFADD
command which allows you

767
00:39:26,340 --> 00:39:30,210
to add strings into a given bloom filter.

768
00:39:30,210 --> 00:39:33,270
And we get back a one here
saying Bob has been added.

769
00:39:33,270 --> 00:39:36,870
We can similarly add
multiple users simultaneously

770
00:39:36,870 --> 00:39:39,393
atomically to the interactions here.

771
00:39:40,290 --> 00:39:43,290
And then this exists is the
membership testing command

772
00:39:43,290 --> 00:39:46,530
where you can just test
if a given user was there,

773
00:39:46,530 --> 00:39:48,210
we'll get back a zero.

774
00:39:48,210 --> 00:39:51,810
However, and Bob of course
would return back one

775
00:39:51,810 --> 00:39:55,020
saying that it does
exist, it has been added,

776
00:39:55,020 --> 00:39:55,980
but what can happen

777
00:39:55,980 --> 00:39:57,690
is that you get back
a false positive here.

778
00:39:57,690 --> 00:39:59,850
So we're testing whether
Frank has been added

779
00:39:59,850 --> 00:40:01,200
to the interactions with Alice,

780
00:40:01,200 --> 00:40:05,010
but we get it will conflict
in this case with either Bob,

781
00:40:05,010 --> 00:40:07,660
Charlie or Doris's hash
values and we get back a one.

782
00:40:12,030 --> 00:40:15,540
So let's move on to another data structure

783
00:40:15,540 --> 00:40:19,803
within Valkey that can keep
track of geospatial data.

784
00:40:21,210 --> 00:40:23,790
So let's say we are managing our MMORPG

785
00:40:23,790 --> 00:40:25,920
and we have this, you
know, virtual rich world

786
00:40:25,920 --> 00:40:27,870
where users are in different locations

787
00:40:27,870 --> 00:40:29,610
and we wanna see which users

788
00:40:29,610 --> 00:40:32,746
and points of interest are
located near a given user

789
00:40:32,746 --> 00:40:34,500
at a point in time.

790
00:40:34,500 --> 00:40:37,383
And for this, we can use the
ALKIS geospatial commands.

791
00:40:38,580 --> 00:40:39,900
So what is geospatial?

792
00:40:39,900 --> 00:40:41,610
So how does it work?

793
00:40:41,610 --> 00:40:44,910
So in Valkey, you would map first

794
00:40:44,910 --> 00:40:46,890
with a latitude and longitude.

795
00:40:46,890 --> 00:40:48,840
Basically, you would
take your virtual world

796
00:40:48,840 --> 00:40:53,840
and divide it into a normal grid here.

797
00:40:54,510 --> 00:40:57,690
That latitude longitude
that you store as input

798
00:40:57,690 --> 00:41:01,020
is taken into a geohash algorithm

799
00:41:01,020 --> 00:41:05,430
and stored as a traditional
geo hash, which it works

800
00:41:05,430 --> 00:41:06,990
by sort of subdividing the world

801
00:41:06,990 --> 00:41:09,690
into smaller and smaller grids.

802
00:41:09,690 --> 00:41:12,540
And it generates this normal geo hash

803
00:41:12,540 --> 00:41:15,000
is a string of characters.

804
00:41:15,000 --> 00:41:19,080
We would take it into a
sort of this 52 bit integer.

805
00:41:19,080 --> 00:41:20,790
And the properties of the
integer are interesting

806
00:41:20,790 --> 00:41:23,430
in that the closer these numbers are,

807
00:41:23,430 --> 00:41:26,640
the closer they are in
location to each other

808
00:41:26,640 --> 00:41:28,470
because of the way that it interleaves

809
00:41:28,470 --> 00:41:30,720
the latitude and longitude.

810
00:41:30,720 --> 00:41:32,490
And this has the nice property

811
00:41:32,490 --> 00:41:34,110
that you can kinda use it as a score.

812
00:41:34,110 --> 00:41:35,850
So things that are close to each other

813
00:41:35,850 --> 00:41:37,560
and numerically are close

814
00:41:37,560 --> 00:41:39,780
to each other in latitude and longitude.

815
00:41:39,780 --> 00:41:43,620
And so this score is then fed
into another data structure

816
00:41:43,620 --> 00:41:45,780
within Valkey called assorted set.

817
00:41:45,780 --> 00:41:48,420
Sorted sets are often used
for things like leaderboards

818
00:41:48,420 --> 00:41:49,500
where you just have high scores,

819
00:41:49,500 --> 00:41:52,890
you wanna be able to keep
track of like the top end.

820
00:41:52,890 --> 00:41:54,690
In this case, the sorted
set is actually used

821
00:41:54,690 --> 00:41:55,830
to query things

822
00:41:55,830 --> 00:42:00,750
that are nearby a given
latitude and longitude.

823
00:42:00,750 --> 00:42:03,210
However, it has some convenience
functions on top of it.

824
00:42:03,210 --> 00:42:06,000
So this allows you to
just sort of bounding box

825
00:42:06,000 --> 00:42:09,633
and radius querying in O of log N time.

826
00:42:12,600 --> 00:42:15,210
So how would you actually
use it in practice?

827
00:42:15,210 --> 00:42:17,610
So let's see how it works.

828
00:42:17,610 --> 00:42:19,230
You'd have these geo add commands.

829
00:42:19,230 --> 00:42:21,030
So it has these convenience functions

830
00:42:21,030 --> 00:42:22,530
on top of the sorted set.

831
00:42:22,530 --> 00:42:25,140
You can do a geo add with
different latitude longitude.

832
00:42:25,140 --> 00:42:28,020
Here, we're adding Alice as
well as a point of interest

833
00:42:28,020 --> 00:42:31,593
of the beach down here on the bottom.

834
00:42:32,760 --> 00:42:35,610
We'll get back a one saying
it's been successfully added.

835
00:42:37,050 --> 00:42:40,125
And then let's say we
have another player here

836
00:42:40,125 --> 00:42:41,110
that wants to query

837
00:42:42,855 --> 00:42:44,910
for the points of interest around them.

838
00:42:44,910 --> 00:42:48,600
So you use this geo radius
command and you query the map.

839
00:42:48,600 --> 00:42:51,030
It has a slightly different
latitude longitude.

840
00:42:51,030 --> 00:42:52,950
So we can do different radii here.

841
00:42:52,950 --> 00:42:55,620
So let's say we do a
three kilometer radius,

842
00:42:55,620 --> 00:42:59,760
we would get back both
items that we've added here,

843
00:42:59,760 --> 00:43:02,403
Alice as well as the beach at the bottom.

844
00:43:03,510 --> 00:43:05,850
However, if we were to
do a smaller radius here,

845
00:43:05,850 --> 00:43:09,900
just a one kilometer radius
among along that point,

846
00:43:09,900 --> 00:43:11,763
we would just get back Alice.

847
00:43:13,140 --> 00:43:16,260
So you can use it to
query things around it.

848
00:43:16,260 --> 00:43:18,120
You can then use it to
kind of filter further

849
00:43:18,120 --> 00:43:22,170
so that you can query the
distance between any two points

850
00:43:22,170 --> 00:43:23,880
in that as well and get it back

851
00:43:23,880 --> 00:43:25,380
in a number of different units.

852
00:43:25,380 --> 00:43:28,050
So say we wanna see the
distance between Alice

853
00:43:28,050 --> 00:43:30,570
and beach one in meters

854
00:43:30,570 --> 00:43:33,690
and get back that it's about
1,600 meters between them.

855
00:43:33,690 --> 00:43:36,180
So you can use it to
further filter the results

856
00:43:36,180 --> 00:43:39,030
of the radius if you wanna do
things like bounding boxes.

857
00:43:42,450 --> 00:43:45,300
Now let's look at a usage

858
00:43:45,300 --> 00:43:48,300
of rate limiting in our application.

859
00:43:48,300 --> 00:43:51,660
So you might wanna use
rate limiting in an MMORPG

860
00:43:51,660 --> 00:43:53,640
to enforce game mechanics here.

861
00:43:53,640 --> 00:43:55,860
So let's say we wanna answer the question,

862
00:43:55,860 --> 00:43:57,957
does a player have enough
stamina to cast a give

863
00:43:57,957 --> 00:43:59,733
and spell at a point in time?

864
00:44:00,930 --> 00:44:03,450
So as Bob is trying to access
a resource, in this case,

865
00:44:03,450 --> 00:44:05,010
spell casting and can only do

866
00:44:05,010 --> 00:44:06,630
so many spells in a certain time period

867
00:44:06,630 --> 00:44:08,343
before that quota resets.

868
00:44:10,020 --> 00:44:12,990
So we can do this simply in Valkey

869
00:44:12,990 --> 00:44:15,985
using a counter effectively,

870
00:44:15,985 --> 00:44:19,410
and the string data type here can be used

871
00:44:19,410 --> 00:44:21,270
to also support numerical operations.

872
00:44:21,270 --> 00:44:22,920
So you have some increment

873
00:44:22,920 --> 00:44:27,420
and decrement commands in
Valkey to manipulate numbers.

874
00:44:27,420 --> 00:44:31,170
So if I were to increment
just to give counter here,

875
00:44:31,170 --> 00:44:33,490
it'll create it and set it to one

876
00:44:34,560 --> 00:44:36,660
and I can further manipulate that

877
00:44:36,660 --> 00:44:39,480
and you know, if I increment
it again, it'll go to two.

878
00:44:39,480 --> 00:44:41,520
And this is of course a
constant time operation

879
00:44:41,520 --> 00:44:42,780
that you're performing.

880
00:44:42,780 --> 00:44:46,500
And so we can use this
to build a rate limiter

881
00:44:46,500 --> 00:44:50,133
for Valkey for our application.

882
00:44:51,420 --> 00:44:53,730
So let's say we wanna be able
to start our counter here

883
00:44:53,730 --> 00:44:56,220
at zero and allow three requests

884
00:44:56,220 --> 00:45:00,180
until a given time delay is up.

885
00:45:00,180 --> 00:45:01,980
So you just have three requests,

886
00:45:01,980 --> 00:45:04,020
let's say in a like a five second period.

887
00:45:04,020 --> 00:45:06,120
So what we can do is we
can start our counter

888
00:45:06,120 --> 00:45:11,070
and use a Lua script
to do this atomic logic

889
00:45:11,070 --> 00:45:14,520
to be able to count return back of value,

890
00:45:14,520 --> 00:45:16,410
and then make sure it expires at the end

891
00:45:16,410 --> 00:45:17,520
of that time period.

892
00:45:17,520 --> 00:45:19,500
So we'll use a TTL basically

893
00:45:19,500 --> 00:45:24,450
to reset our rate limiter
at the end of the duration.

894
00:45:24,450 --> 00:45:26,400
So just looking at this
numerically, users trying

895
00:45:26,400 --> 00:45:30,270
to offer requests, we'll
count up to three here.

896
00:45:30,270 --> 00:45:33,540
And then once it gets to three,
no more requests are allowed

897
00:45:33,540 --> 00:45:35,580
until the TTL fires.

898
00:45:35,580 --> 00:45:36,810
As soon as the TTL fires

899
00:45:36,810 --> 00:45:40,530
that item is basically
removed from the cache

900
00:45:40,530 --> 00:45:43,470
and then that same
process can restart again

901
00:45:43,470 --> 00:45:46,503
as if the user has sufficient credits.

902
00:45:48,120 --> 00:45:52,590
So let's walk through that Lua
script to see how it works.

903
00:45:52,590 --> 00:45:53,880
So as I said,

904
00:45:53,880 --> 00:45:56,580
Valkey supports some
simple scripting operations

905
00:45:56,580 --> 00:45:59,220
where you can do compound
operations atomically.

906
00:45:59,220 --> 00:46:00,690
In a Lua script.

907
00:46:00,690 --> 00:46:02,490
Lua a language that you
might not be familiar with,

908
00:46:02,490 --> 00:46:04,590
but it's relatively straightforward.

909
00:46:04,590 --> 00:46:06,330
It can interact with the server command

910
00:46:06,330 --> 00:46:08,340
to do simple logic on top of it.

911
00:46:08,340 --> 00:46:10,940
So let's say we have a
couple different globals here

912
00:46:10,940 --> 00:46:12,780
in our script.

913
00:46:12,780 --> 00:46:14,220
We wanna set our limit to three

914
00:46:14,220 --> 00:46:18,060
and our expire time would be
four seconds in this case.

915
00:46:18,060 --> 00:46:20,340
We'll start by fetching the key

916
00:46:20,340 --> 00:46:21,270
that we're operating on.

917
00:46:21,270 --> 00:46:24,900
In this case, the given user's stamina.

918
00:46:24,900 --> 00:46:25,733
And we'll fetch it

919
00:46:25,733 --> 00:46:28,593
from the Valkey server
that we're running on.

920
00:46:29,970 --> 00:46:31,080
If it doesn't exist,

921
00:46:31,080 --> 00:46:34,440
meaning that it hasn't
either has been evicted

922
00:46:34,440 --> 00:46:38,130
or it's starting from scratch,
then we'll set it to zero

923
00:46:38,130 --> 00:46:41,310
and set the expiry time to four seconds.

924
00:46:41,310 --> 00:46:43,260
And if it does exist,

925
00:46:43,260 --> 00:46:45,840
then we have already
previously fetched the value

926
00:46:45,840 --> 00:46:48,360
and then we just check if
we're within our limit.

927
00:46:48,360 --> 00:46:50,850
If we are, then we'll be
able to increment the key

928
00:46:50,850 --> 00:46:52,020
and return one saying

929
00:46:52,020 --> 00:46:53,580
that the user's allowed to cast the spell.

930
00:46:53,580 --> 00:46:55,830
Otherwise, we'll return zero.

931
00:46:55,830 --> 00:46:58,710
So this is sufficient to
build a simple rate limiter

932
00:46:58,710 --> 00:47:01,290
that just allows a specific number.

933
00:47:01,290 --> 00:47:02,640
But let's say we wanna have something

934
00:47:02,640 --> 00:47:04,590
that's a little bit more, oh,

935
00:47:04,590 --> 00:47:05,910
let's see how you actually use it too.

936
00:47:05,910 --> 00:47:09,810
Let's, so we can load the script first

937
00:47:09,810 --> 00:47:12,360
by just inserting it into Valkey.

938
00:47:12,360 --> 00:47:13,950
We can do script load

939
00:47:13,950 --> 00:47:16,890
and put that entire string
that you just saw before

940
00:47:16,890 --> 00:47:21,890
into the server, which then
gives you back this SHA ID.

941
00:47:22,740 --> 00:47:27,000
This is the unique identifier
hash value of the script

942
00:47:27,000 --> 00:47:32,000
that you can then use in a
subsequent to invoke that script.

943
00:47:32,340 --> 00:47:36,300
So then you can do eval SHA, the script ID

944
00:47:36,300 --> 00:47:38,640
and then pass at the arguments here.

945
00:47:38,640 --> 00:47:39,473
Like we saw before,

946
00:47:39,473 --> 00:47:41,820
we wanna give it the key
name, the number of spells

947
00:47:41,820 --> 00:47:46,380
that Bob has placed in
this particular example.

948
00:47:46,380 --> 00:47:49,500
And it will then return back a one

949
00:47:49,500 --> 00:47:52,200
or a zero, whether that
is an allowed spell

950
00:47:52,200 --> 00:47:53,823
or a disallowed spell.

951
00:47:55,530 --> 00:47:56,640
This is relatively simple,

952
00:47:56,640 --> 00:48:00,060
but let's say we wanna have
spells that have different types

953
00:48:00,060 --> 00:48:02,970
of a different cost.

954
00:48:02,970 --> 00:48:05,640
So it can do a simple spell
or a more complex spell.

955
00:48:05,640 --> 00:48:07,800
So our previous example
wouldn't have worked with that

956
00:48:07,800 --> 00:48:10,770
because it just allows a simple counter.

957
00:48:10,770 --> 00:48:14,520
And with this, we can do a token bucket,

958
00:48:14,520 --> 00:48:17,880
which is a little bit more
advanced rate limiting algorithm

959
00:48:17,880 --> 00:48:19,680
where you have a given capacity

960
00:48:19,680 --> 00:48:21,720
and refill rate of that bucket.

961
00:48:21,720 --> 00:48:24,930
And if the bucket has enough
credits for your given spell,

962
00:48:24,930 --> 00:48:26,580
we'll subtract from it,

963
00:48:26,580 --> 00:48:29,370
otherwise, it will be disallowed.

964
00:48:29,370 --> 00:48:31,830
So we keep track of a little
bit more metadata associated

965
00:48:31,830 --> 00:48:35,013
with a given user's spell count here.

966
00:48:36,810 --> 00:48:40,140
We can do this in Valkey by
using the hash data structure

967
00:48:40,140 --> 00:48:42,120
that Yaron talked about.

968
00:48:42,120 --> 00:48:43,860
And for this, we need to keep track

969
00:48:43,860 --> 00:48:45,750
of two different pieces of information.

970
00:48:45,750 --> 00:48:47,970
One is the number of tokens

971
00:48:47,970 --> 00:48:49,980
that work are currently in the bucket,

972
00:48:49,980 --> 00:48:52,560
and two is the last time we
updated or touched that bucket.

973
00:48:52,560 --> 00:48:56,910
And you need the timestamp
because tokens accrue over time.

974
00:48:56,910 --> 00:48:59,583
And so you can calculate
the difference there.

975
00:49:01,080 --> 00:49:03,180
So let's break down what
the script looks like

976
00:49:03,180 --> 00:49:04,013
in this scenario.

977
00:49:04,013 --> 00:49:04,980
Still using Lua script,

978
00:49:04,980 --> 00:49:07,020
but it's a little bit more complex

979
00:49:07,020 --> 00:49:09,570
so we can look at some
advanced Lua scripting here.

980
00:49:09,570 --> 00:49:11,730
So rather than having hardcoded values,

981
00:49:11,730 --> 00:49:14,250
like global values in the script before,

982
00:49:14,250 --> 00:49:17,730
we're gonna take in these
values as arguments here.

983
00:49:17,730 --> 00:49:19,230
So we have the bucket size

984
00:49:19,230 --> 00:49:22,524
and the refill rate as
arguments to the script

985
00:49:22,524 --> 00:49:27,524
as well as the number of
tokens for the given request.

986
00:49:27,690 --> 00:49:29,140
And we still get the key out.

987
00:49:30,030 --> 00:49:32,520
So the first thing we need
to do is get the value

988
00:49:32,520 --> 00:49:34,560
of the hash, which may or may not exist

989
00:49:34,560 --> 00:49:36,510
if we haven't run this before.

990
00:49:36,510 --> 00:49:40,110
So we'll fetch using HMGET

991
00:49:40,110 --> 00:49:41,280
as well as the current time,

992
00:49:41,280 --> 00:49:44,013
which is a value, a command within balkey.

993
00:49:46,350 --> 00:49:48,330
The first step is to refill the bucket.

994
00:49:48,330 --> 00:49:51,690
So we check to see if
there was an update time

995
00:49:51,690 --> 00:49:54,840
and if that exists, then we'll go ahead

996
00:49:54,840 --> 00:49:57,630
and calculate the difference
in terms of credits.

997
00:49:57,630 --> 00:49:59,250
We'll refill the bucket as the first step.

998
00:49:59,250 --> 00:50:00,840
And this means you don't have

999
00:50:00,840 --> 00:50:02,700
to be running this periodically.

1000
00:50:02,700 --> 00:50:06,630
You can just run it any time
that you need to query it

1001
00:50:06,630 --> 00:50:10,443
and it will refill the
bucket up to the max size.

1002
00:50:12,390 --> 00:50:14,790
If the bucket's already at the max size

1003
00:50:14,790 --> 00:50:16,770
or if it's beyond that, then we'll just go

1004
00:50:16,770 --> 00:50:20,583
and reset it to the max capacity.

1005
00:50:22,140 --> 00:50:24,810
And then we need to determine
if the request is allowed.

1006
00:50:24,810 --> 00:50:28,230
So if we have enough tokens,
if our current token count

1007
00:50:28,230 --> 00:50:32,760
is more than the number
of requested tokens, then,

1008
00:50:32,760 --> 00:50:36,450
we will subtract and allow the request.

1009
00:50:36,450 --> 00:50:37,770
Otherwise, it will be disallowed

1010
00:50:37,770 --> 00:50:40,263
and the number of tokens are not updated.

1011
00:50:44,040 --> 00:50:46,660
And then we'll go and
update that data structure

1012
00:50:48,510 --> 00:50:50,727
to represent the current number

1013
00:50:50,727 --> 00:50:53,130
of tokens in the last update time.

1014
00:50:53,130 --> 00:50:54,810
And lastly, we'll do
some optimization here.

1015
00:50:54,810 --> 00:50:57,600
We could keep this hash in
the data structure forever

1016
00:50:57,600 --> 00:50:59,610
and it'll eventually get updated,

1017
00:50:59,610 --> 00:51:01,860
but you know, say the user
goes away, we wanna be able

1018
00:51:01,860 --> 00:51:03,930
to reap these at some point in time.

1019
00:51:03,930 --> 00:51:06,000
And so we can again use the TTL

1020
00:51:06,000 --> 00:51:08,400
to expire this hash data
structure at the point

1021
00:51:08,400 --> 00:51:13,080
where basically it would
have reached the ceiling

1022
00:51:13,080 --> 00:51:15,540
of the bucket capacity anyway.

1023
00:51:15,540 --> 00:51:18,030
And so we'll take the bucket
size by the refill rate

1024
00:51:18,030 --> 00:51:20,013
and just expired at that point in time.

1025
00:51:23,130 --> 00:51:26,220
And with that, I want
to thank you for coming

1026
00:51:26,220 --> 00:51:29,340
and we'll be available off the side here

1027
00:51:29,340 --> 00:51:33,060
to answer any questions that
you may have about Valkey

1028
00:51:33,060 --> 00:51:36,000
or data structure modeling.

1029
00:51:36,000 --> 00:51:36,972
Thank you so much.

1030
00:51:36,972 --> 00:51:40,055
(audience applauds)

