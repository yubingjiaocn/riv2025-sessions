1
00:00:01,080 --> 00:00:01,913
- Excellent.

2
00:00:01,913 --> 00:00:03,390
Well, good afternoon, everybody,

3
00:00:03,390 --> 00:00:04,560
and welcome to re:Invent.

4
00:00:04,560 --> 00:00:06,810
Everyone having a good time so far?

5
00:00:06,810 --> 00:00:09,450
Enjoying the keynotes this
morning, some cool things?

6
00:00:09,450 --> 00:00:10,283
Excellent.

7
00:00:10,283 --> 00:00:13,080
Well, welcome to Best practices
for serverless developers.

8
00:00:13,080 --> 00:00:13,913
But you know what?

9
00:00:13,913 --> 00:00:15,330
Actually forget the serverless part.

10
00:00:15,330 --> 00:00:18,090
This is actually for all
developers building in the cloud.

11
00:00:18,090 --> 00:00:20,010
Serverless is rarely the default way

12
00:00:20,010 --> 00:00:22,770
to get the most out of the cloud
when building applications.

13
00:00:22,770 --> 00:00:23,603
My name's Julian.

14
00:00:23,603 --> 00:00:25,290
I'm a developer advocate
here for serverless,

15
00:00:25,290 --> 00:00:27,660
specializing in serverless at AWS.

16
00:00:27,660 --> 00:00:28,740
We've got lovely people in the room.

17
00:00:28,740 --> 00:00:30,120
We've got some overflow rooms

18
00:00:30,120 --> 00:00:32,310
in other parts of Las
Vegas at other hotels.

19
00:00:32,310 --> 00:00:33,810
So hello to you all virtually.

20
00:00:33,810 --> 00:00:35,850
And if you're watching this
from the YouTube recording,

21
00:00:35,850 --> 00:00:37,170
which will be posted shortly,

22
00:00:37,170 --> 00:00:38,310
I suppose hello into the future.

23
00:00:38,310 --> 00:00:40,980
So thank you, everybody, for joining me.

24
00:00:40,980 --> 00:00:42,450
Today I'm gonna take you through a story

25
00:00:42,450 --> 00:00:44,700
that actually mirrors
the serverless evolution

26
00:00:44,700 --> 00:00:46,530
of, I don't know, thousands of companies.

27
00:00:46,530 --> 00:00:49,170
And in this talk, we're gonna
follow Amelie's transformation

28
00:00:49,170 --> 00:00:51,990
from a single patisserie
to international success.

29
00:00:51,990 --> 00:00:54,030
And I promise you there's
nothing that I like more

30
00:00:54,030 --> 00:00:55,560
than a delicious pastry.

31
00:00:55,560 --> 00:00:56,700
It's absolutely my weakness

32
00:00:56,700 --> 00:00:57,533
and I'm just thinking

33
00:00:57,533 --> 00:00:59,310
I should have actually brought one now.

34
00:00:59,310 --> 00:01:01,830
So the serverless best
practices I'm gonna teach you

35
00:01:01,830 --> 00:01:04,710
is gonna help to enable
this incredible growth

36
00:01:04,710 --> 00:01:06,690
and understand the
real-world scaling challenges

37
00:01:06,690 --> 00:01:08,490
that every developer is going to face.

38
00:01:08,490 --> 00:01:09,720
And you can see the evolution

39
00:01:09,720 --> 00:01:11,220
from just getting something out there

40
00:01:11,220 --> 00:01:13,443
to a mature serverless architecture.

41
00:01:14,430 --> 00:01:16,330
You can see here five years of growth,

42
00:01:17,400 --> 00:01:21,060
both business growth and
architectural growth and evolution.

43
00:01:21,060 --> 00:01:22,770
Each phase brought new challenges

44
00:01:22,770 --> 00:01:24,450
and also there were obviously lessons.

45
00:01:24,450 --> 00:01:26,670
Notice the dramatic scaling from 40 orders

46
00:01:26,670 --> 00:01:29,220
to 320,000 orders per month.

47
00:01:29,220 --> 00:01:30,053
Huge scale.

48
00:01:30,053 --> 00:01:32,340
And the architecture had
to support this growth.

49
00:01:32,340 --> 00:01:33,750
And we're gonna see
how serverless patterns

50
00:01:33,750 --> 00:01:35,100
helped this transformation.

51
00:01:36,030 --> 00:01:38,850
So things started, Amelie
opened during lockdown

52
00:01:38,850 --> 00:01:40,800
following her absolute baking passion.

53
00:01:40,800 --> 00:01:42,600
A single Lambda function
handled everything,

54
00:01:42,600 --> 00:01:44,040
just 200 lines of code,

55
00:01:44,040 --> 00:01:47,010
basic DynamoDB table for object storage,

56
00:01:47,010 --> 00:01:48,990
for database storage for orders,

57
00:01:48,990 --> 00:01:51,360
and then manual payment
processing via phone calls,

58
00:01:51,360 --> 00:01:52,590
you know, 40 orders per month

59
00:01:52,590 --> 00:01:54,540
from local neighborhood customers.

60
00:01:54,540 --> 00:01:56,970
A great sort of minimal
viable architecture

61
00:01:56,970 --> 00:01:58,140
with basic Lambda functions.

62
00:01:58,140 --> 00:01:59,520
Great way to get started.

63
00:01:59,520 --> 00:02:01,290
Obviously the pay per use model is perfect

64
00:02:01,290 --> 00:02:02,490
for uncertain demand

65
00:02:02,490 --> 00:02:04,170
and, you know, zero
infrastructure management

66
00:02:04,170 --> 00:02:07,020
during a crisis was perfect as well.

67
00:02:07,020 --> 00:02:09,180
2021, the simple architecture still holds,

68
00:02:09,180 --> 00:02:10,860
you know, with some
splitting of Lambda functions

69
00:02:10,860 --> 00:02:13,770
for payment and inventory
tracking, a good idea.

70
00:02:13,770 --> 00:02:15,660
And then, you know, the
first scaling challenges

71
00:02:15,660 --> 00:02:16,500
start to emerge.

72
00:02:16,500 --> 00:02:18,660
Lambda timeouts during the morning rush,

73
00:02:18,660 --> 00:02:21,360
so a performance degradation
during peak powers,

74
00:02:21,360 --> 00:02:22,920
you know, something that's
always hard to troubleshoot,

75
00:02:22,920 --> 00:02:25,530
and team coordination issues too.

76
00:02:25,530 --> 00:02:28,200
2022, more locations, many more orders,

77
00:02:28,200 --> 00:02:31,230
now 42 Lambda functions,
complex dependency chains,

78
00:02:31,230 --> 00:02:32,820
and not enough error handling.

79
00:02:32,820 --> 00:02:34,590
The morning rush cascading failures,

80
00:02:34,590 --> 00:02:36,570
spiking customer complaints.

81
00:02:36,570 --> 00:02:37,470
Performance suffering,

82
00:02:37,470 --> 00:02:39,600
six plus seconds order
processing, that's crazy.

83
00:02:39,600 --> 00:02:42,570
3% failure rate during peak hours, oh, no.

84
00:02:42,570 --> 00:02:45,663
A breaking point that forces
architectural transformational.

85
00:02:46,560 --> 00:02:47,607
And so in,

86
00:02:47,607 --> 00:02:50,220
and with customers getting
frustrated with the slow service.

87
00:02:50,220 --> 00:02:53,940
And in 2024 year was the year
to get down and fix things.

88
00:02:53,940 --> 00:02:56,730
A proper serverless, gradual
architecture sort out,

89
00:02:56,730 --> 00:02:58,620
bringing in Step Functions
for orchestration,

90
00:02:58,620 --> 00:02:59,790
EventBridge for events,

91
00:02:59,790 --> 00:03:01,530
SQS for liable processing,

92
00:03:01,530 --> 00:03:04,440
and reducing 42 Lambda
functions down to just 10,

93
00:03:04,440 --> 00:03:07,440
six seconds down to 400
milliseconds processing time,

94
00:03:07,440 --> 00:03:10,713
and a foundation set for some
cool international expansion.

95
00:03:12,000 --> 00:03:13,890
And it all made a difference
for global expansion.

96
00:03:13,890 --> 00:03:16,616
60 locations across
Europe and North America.

97
00:03:16,616 --> 00:03:19,350
3.8 million orders per year

98
00:03:19,350 --> 00:03:21,360
with great performance and reliability.

99
00:03:21,360 --> 00:03:24,600
Handling 40 times the holiday
traffic spike seamlessly

100
00:03:24,600 --> 00:03:28,560
and a 35% cost reduction
despite three times the growth.

101
00:03:28,560 --> 00:03:32,370
Amelie's passion project goes
to international success.

102
00:03:32,370 --> 00:03:34,470
And so we're gonna follow
Amelie's transformation

103
00:03:34,470 --> 00:03:35,940
through a number of different areas,

104
00:03:35,940 --> 00:03:37,410
show the problem, some solutions,

105
00:03:37,410 --> 00:03:38,610
and some of the lessons learned.

106
00:03:38,610 --> 00:03:40,290
And I'm gonna give you actionable insights

107
00:03:40,290 --> 00:03:43,020
that you can apply to your
own serverless applications.

108
00:03:43,020 --> 00:03:45,720
Now, I'm gonna warn you,
there's going to be a lot here.

109
00:03:45,720 --> 00:03:48,030
I'm gonna go both broad and deep.

110
00:03:48,030 --> 00:03:50,160
So there's gonna be a huge amount of info.

111
00:03:50,160 --> 00:03:51,930
And I've included this QR code,

112
00:03:51,930 --> 00:03:53,910
which is a link to a handy resources page

113
00:03:53,910 --> 00:03:55,290
with the slides from the presentation

114
00:03:55,290 --> 00:03:57,030
and plenty of other reference links.

115
00:03:57,030 --> 00:03:58,770
And I am gonna do the QR code at the end.

116
00:03:58,770 --> 00:04:00,630
So if you miss it, don't worry.

117
00:04:00,630 --> 00:04:02,520
And also, if you are watching
this on the recording,

118
00:04:02,520 --> 00:04:04,140
this is probably not one of those talks

119
00:04:04,140 --> 00:04:05,758
that you wanna watch at double speed.

120
00:04:05,758 --> 00:04:07,680
Maybe half speed's gonna
be a little easier,

121
00:04:07,680 --> 00:04:09,690
but I'll leave that up to you.

122
00:04:09,690 --> 00:04:10,740
A lot we're gonna cover.

123
00:04:10,740 --> 00:04:12,120
So let's start with the foundations,

124
00:04:12,120 --> 00:04:14,970
how to size and organize
your serverless applications.

125
00:04:14,970 --> 00:04:17,760
By 2023, what started as
simple order processing

126
00:04:17,760 --> 00:04:19,710
had become a monolithic nightmare.

127
00:04:19,710 --> 00:04:21,510
A single Lambda function handling orders,

128
00:04:21,510 --> 00:04:23,490
validation, notifications, analytics,

129
00:04:23,490 --> 00:04:24,960
and a whole bunch of more stuff.

130
00:04:24,960 --> 00:04:27,054
Deployment time stretching
to five minutes.

131
00:04:27,054 --> 00:04:29,550
Debugging is I suppose
like finding a croissant

132
00:04:29,550 --> 00:04:31,320
in a bakery full of pastries.

133
00:04:31,320 --> 00:04:32,760
And then a holiday rush incident.

134
00:04:32,760 --> 00:04:34,980
A single bug in the notification service

135
00:04:34,980 --> 00:04:36,810
brought down all the order processing.

136
00:04:36,810 --> 00:04:37,680
That is bad.

137
00:04:37,680 --> 00:04:39,120
And this crisis forced them

138
00:04:39,120 --> 00:04:42,480
to learn proper Lambda sizing principles.

139
00:04:42,480 --> 00:04:44,820
And there's some universal
principles that do apply.

140
00:04:44,820 --> 00:04:45,653
Each Lambda function

141
00:04:45,653 --> 00:04:48,240
should generally have
one clear responsibility,

142
00:04:48,240 --> 00:04:50,850
avoiding a complex
do-it-all Lambda functions.

143
00:04:50,850 --> 00:04:52,590
And what you wanna do is actually size it

144
00:04:52,590 --> 00:04:56,070
for what it needs to do, not
just for your convenience.

145
00:04:56,070 --> 00:04:57,390
Partly. We'll get to that.

146
00:04:57,390 --> 00:04:59,580
But Lambda allocates also
CPU memory proportionally

147
00:04:59,580 --> 00:05:00,720
to memory configuration

148
00:05:00,720 --> 00:05:03,060
and so adding more memory
improves performance

149
00:05:03,060 --> 00:05:07,050
and also may reduce costs.

150
00:05:07,050 --> 00:05:09,690
Package size also directly
impacts cold start times.

151
00:05:09,690 --> 00:05:10,523
So you do wanna make sure

152
00:05:10,523 --> 00:05:13,830
your function's small and
nimble for your performance.

153
00:05:13,830 --> 00:05:15,570
But you can also easily swing the pendulum

154
00:05:15,570 --> 00:05:16,650
to the opposite extremes

155
00:05:16,650 --> 00:05:18,420
with too many functions or repos

156
00:05:18,420 --> 00:05:20,310
or stacks or things to manage.

157
00:05:20,310 --> 00:05:21,420
Resist the temptation

158
00:05:21,420 --> 00:05:23,970
to create these sort of
micro functions prematurely.

159
00:05:23,970 --> 00:05:25,500
Start with cohesive functions

160
00:05:25,500 --> 00:05:28,290
and split only when you
experience real pain.

161
00:05:28,290 --> 00:05:31,110
The 42-function chaos was organic growth

162
00:05:31,110 --> 00:05:32,610
really gone probably a bit wrong.

163
00:05:32,610 --> 00:05:34,590
And same for also stacks and repos.

164
00:05:34,590 --> 00:05:36,540
The goal is a pragmatic approach

165
00:05:36,540 --> 00:05:38,940
for your organizational
and for your current needs,

166
00:05:38,940 --> 00:05:41,130
not your sort of future
theoretical perfection.

167
00:05:41,130 --> 00:05:42,960
And this is actually probably
more an architectural

168
00:05:42,960 --> 00:05:44,910
and an organization decision

169
00:05:44,910 --> 00:05:46,170
rather than just lines of code.

170
00:05:46,170 --> 00:05:48,270
And when you do create clear boundaries,

171
00:05:48,270 --> 00:05:50,730
each team can choose a runtime and tooling

172
00:05:50,730 --> 00:05:52,440
based on their expertise and needs.

173
00:05:52,440 --> 00:05:54,360
And this is unique pretty
much to serverless,

174
00:05:54,360 --> 00:05:55,290
that you can pick and choose

175
00:05:55,290 --> 00:05:56,790
the different kind of things you wanna do.

176
00:05:56,790 --> 00:05:59,820
Maybe the orders team is gonna
prefer using SAM or Java,

177
00:05:59,820 --> 00:06:01,320
the inventory team uses Python

178
00:06:01,320 --> 00:06:03,630
for maybe some great data-heavy workloads,

179
00:06:03,630 --> 00:06:06,030
and the customer experience
team wants to use TypeScript

180
00:06:06,030 --> 00:06:06,863
because they want to use it

181
00:06:06,863 --> 00:06:08,820
for both the front end and the backend.

182
00:06:08,820 --> 00:06:10,950
And so your infrastructure
choices can align

183
00:06:10,950 --> 00:06:12,300
with your team's strengths.

184
00:06:13,140 --> 00:06:16,020
And this extends also to
domain-driven organization

185
00:06:16,020 --> 00:06:18,420
where each domain can
also own its own data

186
00:06:18,420 --> 00:06:20,010
and own its own functionality

187
00:06:20,010 --> 00:06:21,750
and be right-sized for the team,

188
00:06:21,750 --> 00:06:22,950
right-sized for what a team

189
00:06:22,950 --> 00:06:24,693
can effectively manage by itself.

190
00:06:25,770 --> 00:06:26,603
And if you're thinking

191
00:06:26,603 --> 00:06:27,436
and you're building serverless apps,

192
00:06:27,436 --> 00:06:28,290
just use a framework.

193
00:06:28,290 --> 00:06:30,180
It doesn't actually
really matter which one.

194
00:06:30,180 --> 00:06:31,380
Pick the one that works for you.

195
00:06:31,380 --> 00:06:32,910
There are many third-party ones,

196
00:06:32,910 --> 00:06:33,743
or you've got SAM,

197
00:06:33,743 --> 00:06:34,923
which is an extension of CloudFormation

198
00:06:34,923 --> 00:06:36,690
with some simplified syntax,

199
00:06:36,690 --> 00:06:39,450
or CDK to create CloudFormation
in programming languages.

200
00:06:39,450 --> 00:06:40,500
Just use a framework.

201
00:06:40,500 --> 00:06:42,713
It's just gonna make
your life so much easier.

202
00:06:43,560 --> 00:06:45,720
Many people also overcomplicate repos too.

203
00:06:45,720 --> 00:06:48,690
You know, please don't have
a repo for each function,

204
00:06:48,690 --> 00:06:49,560
I beg you, please.

205
00:06:49,560 --> 00:06:50,580
That's gonna be a nightmare.

206
00:06:50,580 --> 00:06:53,190
And you can easily have a
single repo for many services

207
00:06:53,190 --> 00:06:55,770
with separation between, you
know, some domain-specific

208
00:06:55,770 --> 00:06:57,450
and also some shared infrastructure.

209
00:06:57,450 --> 00:06:59,520
You know, the decision
on size for actual repos

210
00:06:59,520 --> 00:07:01,563
is also all about manageability.

211
00:07:02,790 --> 00:07:04,980
So Amelie's transformation
was thinking about sizing,

212
00:07:04,980 --> 00:07:06,360
making a function smaller,

213
00:07:06,360 --> 00:07:08,310
reducing the cold
starts, deployment times,

214
00:07:08,310 --> 00:07:09,600
and also test runs.

215
00:07:09,600 --> 00:07:12,030
40% lower compute costs
through right-sizing.

216
00:07:12,030 --> 00:07:13,260
Right-sizing is the key.

217
00:07:13,260 --> 00:07:16,650
And the key principle being
keep it big until it hurts.

218
00:07:16,650 --> 00:07:19,920
Right-sizing for workloads
organized by business domain

219
00:07:19,920 --> 00:07:21,450
and you can enable team autonomy

220
00:07:21,450 --> 00:07:23,103
in your technological choices.

221
00:07:23,940 --> 00:07:26,010
So next up, you know, Amelie learned

222
00:07:26,010 --> 00:07:27,810
that too much sync, not enough async

223
00:07:27,810 --> 00:07:30,090
was killing performance
during the morning rush,

224
00:07:30,090 --> 00:07:33,120
from sort of blocking
chains to event-driven flow.

225
00:07:33,120 --> 00:07:35,670
2023, synchronous architecture starting

226
00:07:35,670 --> 00:07:37,800
to create some nightmares
during peak hours.

227
00:07:37,800 --> 00:07:40,050
The morning rush brought
cascading system failures

228
00:07:40,050 --> 00:07:41,760
and, of course, frustrated customers.

229
00:07:41,760 --> 00:07:42,840
The synchronous chain meant

230
00:07:42,840 --> 00:07:45,210
one slow payment API call, for example,

231
00:07:45,210 --> 00:07:47,190
blocked everything downstream.

232
00:07:47,190 --> 00:07:48,570
Business impact not good.

233
00:07:48,570 --> 00:07:50,640
23% of orders abandoned.

234
00:07:50,640 --> 00:07:52,260
You know, a huge increase in complaints.

235
00:07:52,260 --> 00:07:55,080
A revenue loss during the
highest traffic periods.

236
00:07:55,080 --> 00:07:57,150
And this crisis forced them

237
00:07:57,150 --> 00:07:59,370
to understand the fundamental problems

238
00:07:59,370 --> 00:08:01,203
with synchronous architectures.

239
00:08:02,340 --> 00:08:03,630
The sequential weight train,

240
00:08:03,630 --> 00:08:05,070
if you look through the
mermaid diagram here,

241
00:08:05,070 --> 00:08:08,160
means every step must finish
before you get a response.

242
00:08:08,160 --> 00:08:10,530
Everything had to complete
in Lambda functions

243
00:08:10,530 --> 00:08:12,240
before the customer got a confirmation.

244
00:08:12,240 --> 00:08:15,960
And so one slow service
affected the entire workflow.

245
00:08:15,960 --> 00:08:18,570
And it's all waiting,
waiting, and waiting.

246
00:08:18,570 --> 00:08:21,900
You know, long chains can
also approach API Gateway

247
00:08:21,900 --> 00:08:23,790
and even Lambda's timeout limits.

248
00:08:23,790 --> 00:08:26,643
Customers abandon orders while
waiting for confirmation.

249
00:08:27,630 --> 00:08:29,370
And this can manifest itself in two ways.

250
00:08:29,370 --> 00:08:30,990
You can have, you know,
one big Lambda function

251
00:08:30,990 --> 00:08:32,280
doing absolutely everything

252
00:08:32,280 --> 00:08:34,260
and it becomes brittle and tough to manage

253
00:08:34,260 --> 00:08:36,180
and also you're paying
for the waiting time

254
00:08:36,180 --> 00:08:37,920
within a Lambda function.

255
00:08:37,920 --> 00:08:39,600
Or actually, I think probably even worse

256
00:08:39,600 --> 00:08:42,210
is multiple Lambda functions in series,

257
00:08:42,210 --> 00:08:45,650
each one a fragile cascading chain.

258
00:08:45,650 --> 00:08:48,660
If a third-party payment service
has a failure, for example,

259
00:08:48,660 --> 00:08:50,010
the rest just stops.

260
00:08:50,010 --> 00:08:52,623
A poor user experience and
tough to come back from.

261
00:08:53,580 --> 00:08:55,140
So Amelie and her team did their research

262
00:08:55,140 --> 00:08:57,510
and realized an event-driven
design would help,

263
00:08:57,510 --> 00:09:00,480
using an event bus for
service-to-service calls,

264
00:09:00,480 --> 00:09:02,790
stopping direct service dependencies,

265
00:09:02,790 --> 00:09:04,200
which allowed for loose coupling,

266
00:09:04,200 --> 00:09:05,550
some independent scaling,

267
00:09:05,550 --> 00:09:08,550
and means failures in one
system don't break the others.

268
00:09:08,550 --> 00:09:10,080
And you can still use async callbacks

269
00:09:10,080 --> 00:09:12,980
to update the original caller
when the work does complete.

270
00:09:13,860 --> 00:09:14,730
Which means bringing in

271
00:09:14,730 --> 00:09:16,140
some other of our serverless friends,

272
00:09:16,140 --> 00:09:18,390
like Step Functions, EventBridge, SQS,

273
00:09:18,390 --> 00:09:20,460
and even AppSync Events,

274
00:09:20,460 --> 00:09:22,890
which is great for the front
end notifications actually.

275
00:09:22,890 --> 00:09:24,990
And what's the mantra for serverless?

276
00:09:24,990 --> 00:09:27,450
You wanna use the best
service for the job.

277
00:09:27,450 --> 00:09:30,270
And so the customer gets an
immediate order confirmation

278
00:09:30,270 --> 00:09:32,340
instead of waiting for
absolutely everything.

279
00:09:32,340 --> 00:09:34,710
And then EventBridge
decouples all the services,

280
00:09:34,710 --> 00:09:36,570
each service processes asynchronously

281
00:09:36,570 --> 00:09:39,280
and publishes its own events while done.

282
00:09:39,280 --> 00:09:41,730
And AppSync Events
provides real-time updates

283
00:09:41,730 --> 00:09:43,530
back to the client as work completes

284
00:09:43,530 --> 00:09:45,690
and so the customer's gonna
see the various progresses,

285
00:09:45,690 --> 00:09:47,910
you know, payment confirmed,
kitchen preparing,

286
00:09:47,910 --> 00:09:50,040
and ready for pickups.

287
00:09:50,040 --> 00:09:51,210
And you get the best of both worlds.

288
00:09:51,210 --> 00:09:53,460
You get that immediate
response back to your client,

289
00:09:53,460 --> 00:09:55,290
plus real-time progress updates.

290
00:09:55,290 --> 00:09:56,123
You get resilience.

291
00:09:56,123 --> 00:09:58,920
The payment delays don't affect
inventory or notifications.

292
00:09:58,920 --> 00:10:01,500
Scalability, each service
can scale independently

293
00:10:01,500 --> 00:10:02,460
based on demand.

294
00:10:02,460 --> 00:10:04,080
And a clear event trail shows

295
00:10:04,080 --> 00:10:06,483
exactly where issues are occurring.

296
00:10:07,500 --> 00:10:09,990
And another thing to bring in
for asynchronous processing

297
00:10:09,990 --> 00:10:11,430
is Lambda's event source mapping.

298
00:10:11,430 --> 00:10:13,680
One of the best use cases for Lambda,

299
00:10:13,680 --> 00:10:15,900
aync processing from a
number of event sources

300
00:10:15,900 --> 00:10:18,630
with direct Lambda integrations.

301
00:10:18,630 --> 00:10:19,950
And event source mapping is a resource

302
00:10:19,950 --> 00:10:21,450
that get events from a source

303
00:10:21,450 --> 00:10:24,488
and ultimately sends the event
to Lambda for processing.

304
00:10:24,488 --> 00:10:27,348
The Lambda service polls
event sources automatically.

305
00:10:27,348 --> 00:10:28,920
There's some content-based filtering,

306
00:10:28,920 --> 00:10:30,510
which can reduce invocations for events

307
00:10:30,510 --> 00:10:32,430
you don't need to process.

308
00:10:32,430 --> 00:10:34,620
The ASM can batch or
group records together

309
00:10:34,620 --> 00:10:36,000
for efficient processing

310
00:10:36,000 --> 00:10:37,860
and then ultimately
invoke the Lambda function

311
00:10:37,860 --> 00:10:39,363
with the batched records.

312
00:10:40,230 --> 00:10:42,240
And there's actually an
architectural difference

313
00:10:42,240 --> 00:10:45,960
between two types of event
sources, queues and streams.

314
00:10:45,960 --> 00:10:47,730
And queues is great for task processing

315
00:10:47,730 --> 00:10:49,500
when each message is independent

316
00:10:49,500 --> 00:10:52,020
and messages are deleted
after the processing.

317
00:10:52,020 --> 00:10:53,730
Streams is more event processing

318
00:10:53,730 --> 00:10:55,428
when perhaps you need multiple consumers

319
00:10:55,428 --> 00:10:58,380
who need the same data or order matters

320
00:10:58,380 --> 00:11:00,570
and messages are retained for replay.

321
00:11:00,570 --> 00:11:02,400
And so a key decision you may have is,

322
00:11:02,400 --> 00:11:06,030
you know, do you have one
consumer doing work once?

323
00:11:06,030 --> 00:11:07,140
Maybe you're gonna use a queue.

324
00:11:07,140 --> 00:11:08,370
Or you've got multiple consumers

325
00:11:08,370 --> 00:11:09,630
or need some sort of replay,

326
00:11:09,630 --> 00:11:12,641
then you're gonna think
about doing a stream.

327
00:11:12,641 --> 00:11:14,670
And the Lambda ESM provides
a bunch of functionality

328
00:11:14,670 --> 00:11:16,020
across all of these event sources.

329
00:11:16,020 --> 00:11:17,550
You've got the filtering,
which I mentioned,

330
00:11:17,550 --> 00:11:19,200
batch controls, including for streams,

331
00:11:19,200 --> 00:11:21,330
being able to split a batch
to find a faulty record,

332
00:11:21,330 --> 00:11:23,280
and then choosing where
to start in a stream,

333
00:11:23,280 --> 00:11:25,140
you know, from the beginning,
from the latest stream,

334
00:11:25,140 --> 00:11:26,640
or at a particular timestamp.

335
00:11:26,640 --> 00:11:29,010
There's some retry and failure
handling options built in,

336
00:11:29,010 --> 00:11:30,600
some analytics for Kinesis,

337
00:11:30,600 --> 00:11:32,610
and platform performance
configuration options.

338
00:11:32,610 --> 00:11:33,510
So a whole bunch.

339
00:11:33,510 --> 00:11:34,650
It's just called an ESM,

340
00:11:34,650 --> 00:11:37,200
but all of this functionality
that is built in.

341
00:11:37,200 --> 00:11:40,564
Sorry, there's the performance
configuration options.

342
00:11:40,564 --> 00:11:43,320
And so Amelie needed a message
buffer during traffic spikes

343
00:11:43,320 --> 00:11:45,270
to prevent system overload.

344
00:11:45,270 --> 00:11:47,340
And so SQS provides perfect decoupling

345
00:11:47,340 --> 00:11:49,440
between producers and consumers.

346
00:11:49,440 --> 00:11:51,630
Lambda ESM automatically polls SQS

347
00:11:51,630 --> 00:11:53,430
and then scales based on the queue depth

348
00:11:53,430 --> 00:11:55,650
and messages are then
gonna be consumed once

349
00:11:55,650 --> 00:11:58,077
and then deleted after
a successful processing.

350
00:11:58,077 --> 00:12:01,020
And SQS just automatically
handles the complexity

351
00:12:01,020 --> 00:12:03,510
of any other message
durability and delivery.

352
00:12:03,510 --> 00:12:05,880
SQS is an incredible service.

353
00:12:05,880 --> 00:12:08,070
And there's a new mode
joining Kafka for SQS

354
00:12:08,070 --> 00:12:09,540
called provisioned mode.

355
00:12:09,540 --> 00:12:12,540
And this allows you to provision
event pollers in advance

356
00:12:12,540 --> 00:12:14,290
to handle sudden spikes in traffic,

357
00:12:16,080 --> 00:12:16,920
and you've got controls

358
00:12:16,920 --> 00:12:18,570
to be able to optimize the throughput.

359
00:12:18,570 --> 00:12:20,430
And there is an additional charge

360
00:12:20,430 --> 00:12:23,070
based on what's called
an event processing unit.

361
00:12:23,070 --> 00:12:24,750
Amelie doesn't actually
need provisioned mode,

362
00:12:24,750 --> 00:12:26,010
but this can be super useful

363
00:12:26,010 --> 00:12:28,623
for high throughput, low
latency queue polling.

364
00:12:30,446 --> 00:12:32,400
So let's look at some
of the best practices

365
00:12:32,400 --> 00:12:34,590
for SQS queue configuration.

366
00:12:34,590 --> 00:12:37,140
You want to set the visibility timeout,

367
00:12:37,140 --> 00:12:38,970
I nearly said invisibility timeout,

368
00:12:38,970 --> 00:12:40,470
the visibility timeout

369
00:12:40,470 --> 00:12:42,120
to at least six times the Lambda function

370
00:12:42,120 --> 00:12:45,150
so messages are available during
processing during retries.

371
00:12:45,150 --> 00:12:48,147
I suggest also setting the
redrive policy for the SQS queue

372
00:12:48,147 --> 00:12:50,220
to, you know, three or probably five

373
00:12:50,220 --> 00:12:52,740
to also give more chances
for successful delivery.

374
00:12:52,740 --> 00:12:55,380
And then for message retention,
make sure it's long enough

375
00:12:55,380 --> 00:12:57,780
to handle any possible
break in the system.

376
00:12:57,780 --> 00:12:59,070
This is not just a new ESM,

377
00:12:59,070 --> 00:13:01,320
but maybe in your whole company
or maybe in a whole region,

378
00:13:01,320 --> 00:13:03,963
so messages aren't lost
during some downtime.

379
00:13:05,280 --> 00:13:07,200
And then you must set a DLQ,

380
00:13:07,200 --> 00:13:08,130
which is a dead-letter queue,

381
00:13:08,130 --> 00:13:09,960
so any messages that Lambda can't process

382
00:13:09,960 --> 00:13:12,570
are sent to this other
queue and then are not lost.

383
00:13:12,570 --> 00:13:13,980
And then you want also some process

384
00:13:13,980 --> 00:13:16,050
to be able to recover
these messages from the DLQ

385
00:13:16,050 --> 00:13:19,290
and be able to replay them
into the main SQS queue.

386
00:13:19,290 --> 00:13:20,940
And from the Lambda ESM side,

387
00:13:20,940 --> 00:13:23,100
you want to also ensure that
you can process messages

388
00:13:23,100 --> 00:13:25,860
and also protect downstream
sources from a search.

389
00:13:25,860 --> 00:13:28,230
That is the point of having a queue.

390
00:13:28,230 --> 00:13:29,430
For processing messages,

391
00:13:29,430 --> 00:13:31,230
the Lambda on-demand ESM can scale

392
00:13:31,230 --> 00:13:34,800
up to 1,250 concurrent
invokes per event source.

393
00:13:34,800 --> 00:13:36,780
And then use provisioned mode, as I said,

394
00:13:36,780 --> 00:13:37,950
if you want faster scaling

395
00:13:37,950 --> 00:13:40,563
and also up to 16 times high concurrency.

396
00:13:41,536 --> 00:13:44,100
16 times high concurrency, that's 20,000,

397
00:13:44,100 --> 00:13:45,180
and that's some impressive throughput

398
00:13:45,180 --> 00:13:48,480
which you can use for your
SQS queues with Lambda.

399
00:13:48,480 --> 00:13:49,500
Filtering also allows you

400
00:13:49,500 --> 00:13:51,570
to only process the messages you need

401
00:13:51,570 --> 00:13:52,920
and you can use positive filtering

402
00:13:52,920 --> 00:13:54,990
to pick which messages
you do want to process,

403
00:13:54,990 --> 00:13:58,350
also negative filtering
for a flexible query

404
00:13:58,350 --> 00:14:00,660
to say which messages you
don't want to process.

405
00:14:00,660 --> 00:14:02,190
And this is a very powerful thing,

406
00:14:02,190 --> 00:14:03,960
saves you money, saves you time.

407
00:14:03,960 --> 00:14:07,560
And it also uses the EventBridge
syntax for filtering,

408
00:14:07,560 --> 00:14:09,420
so it's also consistency.

409
00:14:09,420 --> 00:14:10,470
And also mentioned

410
00:14:10,470 --> 00:14:12,090
obviously this can save
you a bunch of money,

411
00:14:12,090 --> 00:14:14,280
and using this, Amelie
was actually just able

412
00:14:14,280 --> 00:14:17,460
to reduce costs for one
of their ESMs for 92%

413
00:14:17,460 --> 00:14:18,660
by just using filtering.

414
00:14:20,400 --> 00:14:22,890
Batch sizes, I recommend
just tend to get started.

415
00:14:22,890 --> 00:14:24,240
You may as well start with that.

416
00:14:24,240 --> 00:14:26,930
It does mean fewer invokes
and still decent throughput

417
00:14:26,930 --> 00:14:30,030
because the larger the batch
size, the fewer invokes,

418
00:14:30,030 --> 00:14:32,340
and so that's one thing
you need to think about.

419
00:14:32,340 --> 00:14:33,173
See how you go,

420
00:14:33,173 --> 00:14:35,422
but, you know, batch
sizes can go up to 10,000,

421
00:14:35,422 --> 00:14:37,260
which is a huge batch size,

422
00:14:37,260 --> 00:14:40,170
and that can be super useful
in some of your scenarios.

423
00:14:40,170 --> 00:14:41,070
The batch window here

424
00:14:41,070 --> 00:14:42,450
is then to improve the latency

425
00:14:42,450 --> 00:14:44,039
when you actually don't have much traffic.

426
00:14:44,039 --> 00:14:45,660
So you get to process messages

427
00:14:45,660 --> 00:14:47,640
before a batch number is formed.

428
00:14:47,640 --> 00:14:50,070
So you can use these
two different settings

429
00:14:50,070 --> 00:14:51,390
for different use cases.

430
00:14:51,390 --> 00:14:53,970
And then also you've
got the partial batch,

431
00:14:53,970 --> 00:14:55,920
the report batch item failures,

432
00:14:55,920 --> 00:14:58,410
so if you do have a faulty
record in your processing,

433
00:14:58,410 --> 00:15:00,503
you can return a list of
failed records back to SQS

434
00:15:00,503 --> 00:15:02,970
and SQS is then gonna
delete the process messages,

435
00:15:02,970 --> 00:15:03,803
which is more efficient

436
00:15:03,803 --> 00:15:05,950
rather than having to
retry an entire batch.

437
00:15:07,530 --> 00:15:09,450
So then to control the flow control.

438
00:15:09,450 --> 00:15:11,970
This is the rate at which
Lambda consumes the messages.

439
00:15:11,970 --> 00:15:14,910
You want to set the max
concurrency on the ESM

440
00:15:14,910 --> 00:15:18,150
and this is gonna control how
many concurrent invocations

441
00:15:18,150 --> 00:15:20,610
the ESM attempts to send to Lambda,

442
00:15:20,610 --> 00:15:22,795
which prevents overwhelming
downstream services.

443
00:15:22,795 --> 00:15:25,070
These could be things
like databases or APIs

444
00:15:25,070 --> 00:15:26,670
or any third-party services,

445
00:15:26,670 --> 00:15:28,920
which can only handle a
certain amount of throughput.

446
00:15:28,920 --> 00:15:29,760
And this is actually

447
00:15:29,760 --> 00:15:33,060
the all-important
buffering controls system.

448
00:15:33,060 --> 00:15:35,310
And then Lambda reserve
concurrency is a separate setting

449
00:15:35,310 --> 00:15:37,050
which you can set at the
Lambda function level

450
00:15:37,050 --> 00:15:39,960
and this actually reserves
capacity for this function,

451
00:15:39,960 --> 00:15:42,000
ensuring it can scale up when needed

452
00:15:42,000 --> 00:15:43,707
within the account concurrency.

453
00:15:43,707 --> 00:15:46,020
And so you wanna rather
use max concurrency

454
00:15:46,020 --> 00:15:47,190
to manage the buffer,

455
00:15:47,190 --> 00:15:48,840
but if you do want to use both together,

456
00:15:48,840 --> 00:15:50,460
make sure that your reserve concurrency

457
00:15:50,460 --> 00:15:53,250
is higher than the max
concurrency to prevent throttling.

458
00:15:53,250 --> 00:15:56,310
So two different settings
for two different things

459
00:15:56,310 --> 00:15:57,810
and you can use them together.

460
00:15:58,680 --> 00:16:00,600
And then the last bit is
for more error handling.

461
00:16:00,600 --> 00:16:03,090
So this is configuring Lambda
on-failure destinations

462
00:16:03,090 --> 00:16:05,280
for function invoke issues.

463
00:16:05,280 --> 00:16:06,113
And then you may be thinking,

464
00:16:06,113 --> 00:16:07,440
"So, well, why are there actually

465
00:16:07,440 --> 00:16:09,150
two different error handling parts?"

466
00:16:09,150 --> 00:16:12,360
Well, the SQS DLQ captures
messages that fail repeatedly

467
00:16:12,360 --> 00:16:13,830
during polling or processing

468
00:16:13,830 --> 00:16:15,510
and that is on the SQS side,

469
00:16:15,510 --> 00:16:17,670
while Lambda on-failure
destinations captures

470
00:16:17,670 --> 00:16:20,550
invocation failures like
network issues, throttling,

471
00:16:20,550 --> 00:16:22,170
or maybe even if you deleted a function

472
00:16:22,170 --> 00:16:23,760
or you've got some kind of IAM issue.

473
00:16:23,760 --> 00:16:25,800
And so both serve different processes

474
00:16:25,800 --> 00:16:26,760
and should be used together

475
00:16:26,760 --> 00:16:28,413
for comprehensive error handling.

476
00:16:29,760 --> 00:16:32,190
So moving to async is
a powerful architecture

477
00:16:32,190 --> 00:16:34,080
that actually should really
be your default choice

478
00:16:34,080 --> 00:16:35,130
in my opinion.

479
00:16:35,130 --> 00:16:37,803
Avoiding synchronous
chains that can go wrong.

480
00:16:38,777 --> 00:16:40,389
You wanna store first,

481
00:16:40,389 --> 00:16:42,630
you wanna reply quickly
for better user experience,

482
00:16:42,630 --> 00:16:43,860
and then process things later.

483
00:16:43,860 --> 00:16:47,250
And this is a very powerful,
resilient, and stable model.

484
00:16:47,250 --> 00:16:50,430
Understanding also when to
use queues versus streams.

485
00:16:50,430 --> 00:16:52,680
And async also means
better failure isolation,

486
00:16:52,680 --> 00:16:54,510
but you then do need
to think about retries

487
00:16:54,510 --> 00:16:56,403
and not losing any messages.

488
00:16:57,660 --> 00:17:00,540
So next, Amelie learned how
to avoid unnecessary work

489
00:17:00,540 --> 00:17:02,430
through using Step
Functions, orchestration,

490
00:17:02,430 --> 00:17:05,280
and some direct service integrations.

491
00:17:05,280 --> 00:17:07,070
2024, Amelie had successfully moved over

492
00:17:07,070 --> 00:17:08,220
to async architecture

493
00:17:08,220 --> 00:17:09,630
but created a new problem,

494
00:17:09,630 --> 00:17:11,850
42 Lambda functions, as we said before,

495
00:17:11,850 --> 00:17:13,080
including many doing

496
00:17:13,080 --> 00:17:15,600
just simple data
transformations and routing.

497
00:17:15,600 --> 00:17:17,700
That means high compute
costs for operations

498
00:17:17,700 --> 00:17:20,700
that didn't require any sort
of custom business logic,

499
00:17:20,700 --> 00:17:22,200
cold starts affecting performance

500
00:17:22,200 --> 00:17:24,780
for basically crud database updates,

501
00:17:24,780 --> 00:17:26,160
and complex failure scenarios

502
00:17:26,160 --> 00:17:28,830
across multiple function invocations.

503
00:17:28,830 --> 00:17:30,210
And this led to the discovery

504
00:17:30,210 --> 00:17:32,220
that configuration as code

505
00:17:32,220 --> 00:17:34,323
could replace many Lambda functions.

506
00:17:35,610 --> 00:17:36,990
You basically want to avoid Lambda

507
00:17:36,990 --> 00:17:39,180
when you need to use native capabilities

508
00:17:39,180 --> 00:17:40,560
in other services.

509
00:17:40,560 --> 00:17:41,880
Less code to manage.

510
00:17:41,880 --> 00:17:43,230
I don't know about your code,

511
00:17:43,230 --> 00:17:45,210
my code, I want less
code of that to manage.

512
00:17:45,210 --> 00:17:47,580
Your code is probably great,
so you may be good with that.

513
00:17:47,580 --> 00:17:50,160
But for things like simple
data transformations,

514
00:17:50,160 --> 00:17:51,960
direct service-to-service calls,

515
00:17:51,960 --> 00:17:54,300
only routing data from
one place to another,

516
00:17:54,300 --> 00:17:55,500
or simple data enrichment,

517
00:17:55,500 --> 00:17:56,950
you don't need to use Lambda.

518
00:17:58,260 --> 00:18:00,300
If you do have Lambda functions
that serve only as a proxy,

519
00:18:00,300 --> 00:18:02,439
for example, I'm gonna go
through some direct integrations

520
00:18:02,439 --> 00:18:04,980
between API Gateway
and downstream service,

521
00:18:04,980 --> 00:18:06,050
you can optimize that.

522
00:18:06,050 --> 00:18:07,050
So you can see on the right,

523
00:18:07,050 --> 00:18:09,150
API Gateway can actually connect directly

524
00:18:09,150 --> 00:18:10,710
to multiple AWS services,

525
00:18:10,710 --> 00:18:13,803
such as Dynamo, SQS, Step
Functions, and many more.

526
00:18:14,880 --> 00:18:17,370
Once again, no need to use
Lambda just as a proxy.

527
00:18:17,370 --> 00:18:19,980
And I understand that VTL
is a bit of a pain to test,

528
00:18:19,980 --> 00:18:22,710
but also once you do have it
configured, it's done forever.

529
00:18:22,710 --> 00:18:24,270
And for a simple example

530
00:18:24,270 --> 00:18:26,790
like this DynamoDB get item request,

531
00:18:26,790 --> 00:18:28,923
it's easy, cheap, and it's fast.

532
00:18:29,970 --> 00:18:32,340
And more great opportunities
to remove code.

533
00:18:32,340 --> 00:18:34,290
One common pattern that
a lot of customers use

534
00:18:34,290 --> 00:18:37,234
is using Lambda to capture
DynamoDB change events

535
00:18:37,234 --> 00:18:41,460
or change data capture events
from a lot of other services

536
00:18:41,460 --> 00:18:42,690
if they support it.

537
00:18:42,690 --> 00:18:44,130
Lambda then passes the events

538
00:18:44,130 --> 00:18:45,950
and then sends them to another service,

539
00:18:45,950 --> 00:18:47,460
an event bus or API,

540
00:18:47,460 --> 00:18:49,833
for some other downstream processing.

541
00:18:51,210 --> 00:18:52,770
EventBridge Pipes, well, this provides

542
00:18:52,770 --> 00:18:55,050
built-in transformation capabilities.

543
00:18:55,050 --> 00:18:57,420
JSONPath provides a
powerful data manipulation

544
00:18:57,420 --> 00:18:59,280
and filtering only needed events.

545
00:18:59,280 --> 00:19:01,470
You can enrich data as path of the pipe,

546
00:19:01,470 --> 00:19:03,840
do data format, conversions,
transformations.

547
00:19:03,840 --> 00:19:05,160
You can also use Step Functions

548
00:19:05,160 --> 00:19:09,180
and you can also use Lambda
as part of the step as well.

549
00:19:09,180 --> 00:19:10,980
And then it's gonna batch it up

550
00:19:10,980 --> 00:19:12,720
for efficient downstream processing.

551
00:19:12,720 --> 00:19:14,820
And this is perfect for
simple data transformations

552
00:19:14,820 --> 00:19:17,940
that don't necessarily require
complex business logic.

553
00:19:17,940 --> 00:19:19,830
This is it just if you're
using the pipe by itself

554
00:19:19,830 --> 00:19:21,360
without Step Functions and EventBridge.

555
00:19:21,360 --> 00:19:23,640
But, yeah, this, you know,
reduces your Lambda invocations

556
00:19:23,640 --> 00:19:25,290
for basic event processing

557
00:19:25,290 --> 00:19:27,750
and it's really simple to set up.

558
00:19:27,750 --> 00:19:30,120
But actually, the big
winner in Amelie's use case

559
00:19:30,120 --> 00:19:32,100
is using Step Functions.

560
00:19:32,100 --> 00:19:34,050
Being able to remove a
bunch of Lambda functions

561
00:19:34,050 --> 00:19:35,790
for direct service calls.

562
00:19:35,790 --> 00:19:38,550
You can see all of the AWS
SDK actions are available.

563
00:19:38,550 --> 00:19:39,383
So in effect,

564
00:19:39,383 --> 00:19:41,460
there are thousands and
thousands of integrations.

565
00:19:41,460 --> 00:19:44,160
And here's the config to
update inventory in Dynamo.

566
00:19:44,160 --> 00:19:45,930
Just a simple configuration call

567
00:19:45,930 --> 00:19:49,290
that you put in a Step Functions state

568
00:19:49,290 --> 00:19:50,340
and it's really easy to call

569
00:19:50,340 --> 00:19:51,930
and there's a whole
bunch of stuff around it

570
00:19:51,930 --> 00:19:53,809
with no cold states, no cold starts,

571
00:19:53,809 --> 00:19:56,973
and safe retries are also built in.

572
00:19:57,810 --> 00:20:00,090
And there's also built in JSONata support

573
00:20:00,090 --> 00:20:01,200
with loads of these functions,

574
00:20:01,200 --> 00:20:03,810
including some Step
Functions specific ones

575
00:20:03,810 --> 00:20:05,340
like generating a UUID.

576
00:20:05,340 --> 00:20:06,360
So this is super useful.

577
00:20:06,360 --> 00:20:07,193
You wanna count things.

578
00:20:07,193 --> 00:20:08,490
You want to, you know, do up some numbers,

579
00:20:08,490 --> 00:20:09,870
the length and all these kind of things.

580
00:20:09,870 --> 00:20:11,130
No need to have a Lambda function.

581
00:20:11,130 --> 00:20:13,770
You can just use pretty much
free JSONato expressions

582
00:20:13,770 --> 00:20:15,150
to be able to do it.

583
00:20:15,150 --> 00:20:16,380
So these are very powerful ways

584
00:20:16,380 --> 00:20:17,910
to build up a lot of functionality

585
00:20:17,910 --> 00:20:20,613
without having to maintain
any of your custom code.

586
00:20:22,080 --> 00:20:23,130
In many scenarios,

587
00:20:23,130 --> 00:20:26,130
Step Functions is then used in
conjunction with EventBridge,

588
00:20:26,130 --> 00:20:27,270
and Step Functions is great

589
00:20:27,270 --> 00:20:29,400
probably within a domain microservice,

590
00:20:29,400 --> 00:20:30,810
and then the pattern is to finish

591
00:20:30,810 --> 00:20:32,370
what the service needs to do

592
00:20:32,370 --> 00:20:34,950
and that's gonna be whether
that is Step Functions,

593
00:20:34,950 --> 00:20:36,480
whether it's even a Lambda function,

594
00:20:36,480 --> 00:20:39,600
and the pattern is to then emit
an event onto the event bus

595
00:20:39,600 --> 00:20:40,740
and this is then gonna may enable

596
00:20:40,740 --> 00:20:43,740
the event-driven communication
between different domains.

597
00:20:43,740 --> 00:20:45,090
Very powerful model,

598
00:20:45,090 --> 00:20:47,430
allows you to add as
many domains as you need

599
00:20:47,430 --> 00:20:49,430
without sort of trampling on each other.

600
00:20:50,700 --> 00:20:51,533
And if you didn't know,

601
00:20:51,533 --> 00:20:53,250
there are actually two different
flavors of Step Functions.

602
00:20:53,250 --> 00:20:54,390
There's standard workflows,

603
00:20:54,390 --> 00:20:57,060
which can run for up to a
year and are asynchronous,

604
00:20:57,060 --> 00:20:59,370
and express workflows,
which are fast and furious,

605
00:20:59,370 --> 00:21:01,410
and these are specifically
built for high throughput

606
00:21:01,410 --> 00:21:03,330
and they can run for a
maximum of five minutes

607
00:21:03,330 --> 00:21:04,620
and they can also be synchronous.

608
00:21:04,620 --> 00:21:06,900
So super useful for doing
that synchronous calling

609
00:21:06,900 --> 00:21:08,280
we were talking about earlier.

610
00:21:08,280 --> 00:21:09,360
The pricing is different.

611
00:21:09,360 --> 00:21:12,030
Standard workflows are
priced per state transition

612
00:21:12,030 --> 00:21:13,590
and express workflows are then priced

613
00:21:13,590 --> 00:21:16,110
by the number of executions
and memory consumption.

614
00:21:16,110 --> 00:21:18,240
And express workflows also run in memory.

615
00:21:18,240 --> 00:21:19,340
So they're super fast.

616
00:21:20,370 --> 00:21:22,497
And the best part is you can,

617
00:21:22,497 --> 00:21:23,790
and actually you should,

618
00:21:23,790 --> 00:21:26,250
combine standard and express workflows.

619
00:21:26,250 --> 00:21:28,860
You can combine the durability
of standard workflows

620
00:21:28,860 --> 00:21:30,867
with the speed of express workflows.

621
00:21:30,867 --> 00:21:32,850
And so this is perfect for scenarios

622
00:21:32,850 --> 00:21:34,650
where most of the process is long running,

623
00:21:34,650 --> 00:21:36,300
but some of the parts
of your state machine

624
00:21:36,300 --> 00:21:38,130
need some real-time responses.

625
00:21:38,130 --> 00:21:40,080
And so here we can see
the standard workflow

626
00:21:40,080 --> 00:21:41,670
is gonna run start execution

627
00:21:41,670 --> 00:21:44,820
and orchestrate the complete
order processing lifecycle

628
00:21:44,820 --> 00:21:48,090
with a full audit trail as
part of the standard workflows.

629
00:21:48,090 --> 00:21:49,800
And then it's gonna
run the start execution

630
00:21:49,800 --> 00:21:51,570
and run the nested express workflow

631
00:21:51,570 --> 00:21:54,180
to handle real-time inventory validation

632
00:21:54,180 --> 00:21:55,860
in some second time.

633
00:21:55,860 --> 00:21:57,290
And what it can do in
the express workflow,

634
00:21:57,290 --> 00:21:59,160
it can do other cool Step Functions things

635
00:21:59,160 --> 00:22:00,330
like running a parallel state

636
00:22:00,330 --> 00:22:03,600
to check the inventory and the
pricing rules simultaneously

637
00:22:03,600 --> 00:22:04,530
and then it's also using

638
00:22:04,530 --> 00:22:06,600
DynamoDB direct service integrations.

639
00:22:06,600 --> 00:22:08,670
So, again, no Lambda cold starts.

640
00:22:08,670 --> 00:22:11,193
All becomes nice and simple.

641
00:22:12,450 --> 00:22:15,510
But actually, as announced
in the keynote today,

642
00:22:15,510 --> 00:22:18,030
there's a new kid on the block
for asynchronous processing

643
00:22:18,030 --> 00:22:20,292
but still within a Lambda function

644
00:22:20,292 --> 00:22:22,650
with durable functions
being announced today.

645
00:22:22,650 --> 00:22:24,540
And this actually does completely change

646
00:22:24,540 --> 00:22:26,040
the way you think about Lambda functions.

647
00:22:26,040 --> 00:22:27,480
You can now build workflows

648
00:22:27,480 --> 00:22:29,490
in your favorite programming language.

649
00:22:29,490 --> 00:22:30,930
You're able to use checkpointing

650
00:22:30,930 --> 00:22:33,690
to suspend and resume
long-running operations

651
00:22:33,690 --> 00:22:35,433
with item potency just built in.

652
00:22:36,420 --> 00:22:37,440
So this is actually useful

653
00:22:37,440 --> 00:22:39,720
when you have a bunch of
Lambda code in your workflows

654
00:22:39,720 --> 00:22:43,290
and you can now use the
enhanced context object

655
00:22:43,290 --> 00:22:44,610
like I'm highlighting over here

656
00:22:44,610 --> 00:22:46,560
and I'm showing you you can run steps,

657
00:22:46,560 --> 00:22:47,970
you can do parallel processing,

658
00:22:47,970 --> 00:22:49,290
and even wait for callback

659
00:22:49,290 --> 00:22:51,243
all within an async Lambda function,

660
00:22:52,770 --> 00:22:53,700
within the Lambda function,

661
00:22:53,700 --> 00:22:55,710
and you don't pay for
that wait time as well

662
00:22:55,710 --> 00:22:57,030
and they can run for up to a year.

663
00:22:57,030 --> 00:22:58,507
So if your mind's going a bit,

664
00:22:58,507 --> 00:22:59,490
"Hang on, this is all a bit

665
00:22:59,490 --> 00:23:00,870
strange and different for Lambda,"

666
00:23:00,870 --> 00:23:02,880
I'm with you, but this
is a super powerful way,

667
00:23:02,880 --> 00:23:05,010
a new capability that we announced today.

668
00:23:05,010 --> 00:23:06,660
Step Functions, of course, is still great,

669
00:23:06,660 --> 00:23:09,960
particularly for those
direct service integrations.

670
00:23:09,960 --> 00:23:13,440
But if your workflow does
require a lot of your own code

671
00:23:13,440 --> 00:23:15,930
or you'd prefer just managing
your workflow in code

672
00:23:15,930 --> 00:23:18,000
with async checkpointing and resume,

673
00:23:18,000 --> 00:23:20,490
this is certainly worth taking a look.

674
00:23:20,490 --> 00:23:22,560
And my good friends Michael
Gasch and Eric Johnson

675
00:23:22,560 --> 00:23:24,480
are doing a dedicated talk tomorrow

676
00:23:24,480 --> 00:23:26,310
and that's going to be
diving deep into it.

677
00:23:26,310 --> 00:23:27,420
I think it may be at Mandalay Bay,

678
00:23:27,420 --> 00:23:28,680
but I'm not entirely sure.

679
00:23:28,680 --> 00:23:29,760
It's CNS380.

680
00:23:29,760 --> 00:23:30,593
So have that look.

681
00:23:30,593 --> 00:23:32,070
That hopefully should be released

682
00:23:32,070 --> 00:23:33,720
in the content catalog today.

683
00:23:33,720 --> 00:23:35,220
Durable functions is gonna be awesome.

684
00:23:35,220 --> 00:23:36,220
Well, it is awesome.

685
00:23:37,140 --> 00:23:39,000
So going all async helped Amelie

686
00:23:39,000 --> 00:23:40,920
being able to drastically
reduce Lambda functions

687
00:23:40,920 --> 00:23:41,753
and cold starts,

688
00:23:41,753 --> 00:23:43,920
improving latency, better timeouts,

689
00:23:43,920 --> 00:23:46,800
better error handling due to
reliable service integrations,

690
00:23:46,800 --> 00:23:48,570
and a monthly saving of $1,500

691
00:23:48,570 --> 00:23:51,270
just from eliminating
unnecessary Lambda functions.

692
00:23:51,270 --> 00:23:52,620
So this demonstrates the power

693
00:23:52,620 --> 00:23:55,713
of configuration as code
over custom compute.

694
00:23:57,150 --> 00:23:59,610
Remember the good old adage
the best Lambda function

695
00:23:59,610 --> 00:24:01,923
is often the one you
don't even have to write.

696
00:24:02,910 --> 00:24:05,190
So next up, Amelie
optimized Lambda performance

697
00:24:05,190 --> 00:24:07,320
for functions that truly
needed the custom logic

698
00:24:07,320 --> 00:24:08,820
'cause Lambda still is a great service

699
00:24:08,820 --> 00:24:10,680
when you need to use it.

700
00:24:10,680 --> 00:24:14,490
2023, 35 locations,
180,000 orders monthly.

701
00:24:14,490 --> 00:24:16,590
The morning rush created a perfect storm

702
00:24:16,590 --> 00:24:18,360
of performance problems.

703
00:24:18,360 --> 00:24:20,160
Customers abandoned orders faster

704
00:24:20,160 --> 00:24:21,690
than Amelie could even bake croissants,

705
00:24:21,690 --> 00:24:23,880
which meant a revenue
loss, and that's not good.

706
00:24:23,880 --> 00:24:26,190
And this was an existential
business threat,

707
00:24:26,190 --> 00:24:28,083
not even just a technical problem.

708
00:24:29,220 --> 00:24:31,620
And so quick sidebar for
performance and cost,

709
00:24:31,620 --> 00:24:33,540
there's also another
Lambda kid on the block

710
00:24:33,540 --> 00:24:36,120
announced this week,
Lambda Managed Instances.

711
00:24:36,120 --> 00:24:37,050
And this actually gives you

712
00:24:37,050 --> 00:24:38,970
the Lambda operational simplicity

713
00:24:38,970 --> 00:24:40,440
and serverless operational model

714
00:24:40,440 --> 00:24:44,280
to go with EC2's full
ranges and specificity.

715
00:24:44,280 --> 00:24:47,160
We manage the hardware
and it runs in your VPC.

716
00:24:47,160 --> 00:24:48,690
You can choose specialized instance

717
00:24:48,690 --> 00:24:51,330
based on memory or CPU or network

718
00:24:51,330 --> 00:24:52,890
and then Lambda's just gonna manage

719
00:24:52,890 --> 00:24:54,120
in handling this instance.

720
00:24:54,120 --> 00:24:55,800
And there's multicurrency
actually built in,

721
00:24:55,800 --> 00:24:57,420
another minor thing and another change

722
00:24:57,420 --> 00:25:01,680
that Lambda's gonna be able
to support for this model.

723
00:25:01,680 --> 00:25:03,420
But you do need to
handle this in your code.

724
00:25:03,420 --> 00:25:04,920
There's gonna be a lot
of information coming out

725
00:25:04,920 --> 00:25:06,540
about how to do that.

726
00:25:06,540 --> 00:25:08,250
The same timeouts do apply,

727
00:25:08,250 --> 00:25:11,370
but this can be a big cost
saving for at-scale workloads

728
00:25:11,370 --> 00:25:14,220
where you can use also
your EC2 pricing plans

729
00:25:14,220 --> 00:25:15,750
for things like reserved instances.

730
00:25:15,750 --> 00:25:16,800
So this isn't for everybody.

731
00:25:16,800 --> 00:25:17,670
If you're at high scale

732
00:25:17,670 --> 00:25:19,470
and you've got really
steady state functions,

733
00:25:19,470 --> 00:25:20,580
it doesn't spin up as quickly

734
00:25:20,580 --> 00:25:22,830
in being able to burst and
all that kind of thing,

735
00:25:22,830 --> 00:25:24,960
but it at least is an option at high scale

736
00:25:24,960 --> 00:25:26,460
if you wanna be able to manage your costs

737
00:25:26,460 --> 00:25:28,560
when you grow with Lambda.

738
00:25:28,560 --> 00:25:30,390
Again, there's another
dedicated talk on Thursday

739
00:25:30,390 --> 00:25:32,760
from Stephen and Archana that's
going to dive deep into it

740
00:25:32,760 --> 00:25:33,990
and it's worth watching

741
00:25:33,990 --> 00:25:36,810
to understand how concurrency
changes for this model

742
00:25:36,810 --> 00:25:37,920
and definitely check for this one,

743
00:25:37,920 --> 00:25:39,693
and this is in the content catalog.

744
00:25:41,250 --> 00:25:42,420
So let's take a look back

745
00:25:42,420 --> 00:25:44,340
at the normal Lambda function lifecycle.

746
00:25:44,340 --> 00:25:46,140
Lambda creates a new
execution environments,

747
00:25:46,140 --> 00:25:47,220
or for managed instances,

748
00:25:47,220 --> 00:25:50,070
actually, it deploys a container
onto the EC2 instances.

749
00:25:50,070 --> 00:25:51,180
It then downloads your code

750
00:25:51,180 --> 00:25:53,220
or your container image starts the runtime

751
00:25:53,220 --> 00:25:54,053
and then we have to run

752
00:25:54,053 --> 00:25:56,310
your function handler
pre-initialization code

753
00:25:56,310 --> 00:25:57,570
and your first invoke.

754
00:25:57,570 --> 00:25:59,340
And then after that point,
your function is warm

755
00:25:59,340 --> 00:26:01,890
and ready to run the event
that's been sent to it.

756
00:26:01,890 --> 00:26:03,210
There's actually a separation here

757
00:26:03,210 --> 00:26:06,780
between what you can
control and what we control.

758
00:26:06,780 --> 00:26:09,450
And, essentially, everything
up to the init of the runtime

759
00:26:09,450 --> 00:26:10,560
is AWS's problem

760
00:26:10,560 --> 00:26:12,750
and the Lambda team spends
an inordinate amount of time

761
00:26:12,750 --> 00:26:13,590
over the years here,

762
00:26:13,590 --> 00:26:15,215
you know, literally shaving
down to nanoseconds,

763
00:26:15,215 --> 00:26:16,800
trying to improve performance

764
00:26:16,800 --> 00:26:17,820
and trying to make everything

765
00:26:17,820 --> 00:26:19,740
that becomes on our side of the line

766
00:26:19,740 --> 00:26:21,363
faster and faster and faster.

767
00:26:22,470 --> 00:26:24,000
So the parts that you can control

768
00:26:24,000 --> 00:26:25,980
are memory allocation for performance,

769
00:26:25,980 --> 00:26:28,890
your initialization code,
your function handler code,

770
00:26:28,890 --> 00:26:30,363
and then the package size.

771
00:26:31,560 --> 00:26:33,570
So Lambda allocates CPU
power proportionately

772
00:26:33,570 --> 00:26:35,460
to memory configurations up to 10 gig

773
00:26:35,460 --> 00:26:36,293
for on-demand functions

774
00:26:36,293 --> 00:26:39,450
and up to 30 gig for
managed instance functions.

775
00:26:39,450 --> 00:26:41,790
Lambda also proportionately
allocates CPU power

776
00:26:41,790 --> 00:26:42,990
based on memory,

777
00:26:42,990 --> 00:26:44,850
and for managed instances,
you can actually configure

778
00:26:44,850 --> 00:26:47,310
the ratio of memory to virtual CPU

779
00:26:47,310 --> 00:26:49,860
that you actually want for your workload.

780
00:26:49,860 --> 00:26:52,200
For on-demand, once you
allocated more memory

781
00:26:52,200 --> 00:26:53,370
past about 1.8 gig,

782
00:26:53,370 --> 00:26:54,870
you can actually use more calls,

783
00:26:54,870 --> 00:26:57,420
some multi-threading becomes possible.

784
00:26:57,420 --> 00:26:58,590
And it's counterintuitive,

785
00:26:58,590 --> 00:27:01,620
but actually higher memory
often reduces total cost.

786
00:27:01,620 --> 00:27:03,270
If your code is CPU bound,

787
00:27:03,270 --> 00:27:05,340
adding memory improves the performance

788
00:27:05,340 --> 00:27:06,510
and may reduce the cost

789
00:27:06,510 --> 00:27:09,180
and the key is to finding the sweet spot

790
00:27:09,180 --> 00:27:11,330
where performance gains
justify memory costs.

791
00:27:11,330 --> 00:27:13,680
So you add more memory,
it adds more CPU oomph,

792
00:27:13,680 --> 00:27:16,710
and so things can be processed quickly

793
00:27:16,710 --> 00:27:18,260
and that can reduce your costs.

794
00:27:19,110 --> 00:27:19,950
Now, for on-demand,

795
00:27:19,950 --> 00:27:21,510
if you wanna take advantage of more power

796
00:27:21,510 --> 00:27:23,130
with more than one vCPU,

797
00:27:23,130 --> 00:27:24,390
you can use parallel processing

798
00:27:24,390 --> 00:27:26,160
within your Lambda function code.

799
00:27:26,160 --> 00:27:28,920
So if we say here our
processing a batch of records,

800
00:27:28,920 --> 00:27:29,880
doing this sequentially

801
00:27:29,880 --> 00:27:32,940
is gonna take 300 milliseconds
for three records.

802
00:27:32,940 --> 00:27:34,170
And, you know, within your code,

803
00:27:34,170 --> 00:27:35,940
when you use the process
record in your code,

804
00:27:35,940 --> 00:27:38,733
this is gonna process
each record at a time.

805
00:27:39,750 --> 00:27:42,000
But you can actually speed
this up using multiple calls

806
00:27:42,000 --> 00:27:43,440
and running this in parallel.

807
00:27:43,440 --> 00:27:45,630
And so it's only gonna
take a hundred milliseconds

808
00:27:45,630 --> 00:27:46,463
for all three

809
00:27:46,463 --> 00:27:49,200
because you can use promises
to run the code in parallel,

810
00:27:49,200 --> 00:27:51,870
and this is a super unlock
for batch processing

811
00:27:51,870 --> 00:27:53,763
to get the most for your CPUs.

812
00:27:55,605 --> 00:27:58,431
Now, working out the optimal
memory configuration,

813
00:27:58,431 --> 00:28:00,240
you know, that can be a manual process.

814
00:28:00,240 --> 00:28:02,970
You've gotta try all the
options to do it all yourself.

815
00:28:02,970 --> 00:28:04,980
And so Lambda Power Tuning
is an open-source tool

816
00:28:04,980 --> 00:28:07,530
that gives you a data-driven
approach to help you visualize

817
00:28:07,530 --> 00:28:10,260
and fine-tune the memory power
of your Lambda functions.

818
00:28:10,260 --> 00:28:14,670
It runs multiple current
executions of your function

819
00:28:14,670 --> 00:28:16,680
at all the different
memory allocations you pick

820
00:28:16,680 --> 00:28:18,000
and then measures how they perform

821
00:28:18,000 --> 00:28:19,200
and it runs in your accounts,

822
00:28:19,200 --> 00:28:20,850
performing your real function calls,

823
00:28:20,850 --> 00:28:22,440
you know, showing the real cost and speed,

824
00:28:22,440 --> 00:28:24,000
and this helps you to
find the right balance

825
00:28:24,000 --> 00:28:25,383
in an automated way.

826
00:28:26,820 --> 00:28:28,890
Okay, so onto cold starts.

827
00:28:28,890 --> 00:28:30,330
Here, Amelie has her figures

828
00:28:30,330 --> 00:28:32,970
and cold starts affect
less than 1% of invocations

829
00:28:32,970 --> 00:28:34,590
in her production workload

830
00:28:34,590 --> 00:28:37,470
and they primarily
occurred during scaling up

831
00:28:37,470 --> 00:28:39,303
or after function updates.

832
00:28:40,440 --> 00:28:43,530
And so you generally wanna
focus cold start optimization

833
00:28:43,530 --> 00:28:47,670
on latency-sensitive
user-facing workloads.

834
00:28:47,670 --> 00:28:51,060
Async workloads can usually
tolerate some cold start latency

835
00:28:51,060 --> 00:28:52,920
unless you may be using
something like provisioned mode

836
00:28:52,920 --> 00:28:55,200
and you need to do a whole
bunch of throughput processing.

837
00:28:55,200 --> 00:28:57,180
But generally, all those async processes,

838
00:28:57,180 --> 00:28:58,500
you know, if it takes a few milliseconds

839
00:28:58,500 --> 00:29:00,480
or even a second for a
payment notification email

840
00:29:00,480 --> 00:29:01,590
to come through,

841
00:29:01,590 --> 00:29:03,120
you know, nobody's really gonna mind.

842
00:29:03,120 --> 00:29:05,550
And so don't over-optimize cold starts

843
00:29:05,550 --> 00:29:07,350
or background processing functions.

844
00:29:07,350 --> 00:29:09,000
Although, you know, I did mention

845
00:29:10,182 --> 00:29:11,430
there's a cost win for doing this

846
00:29:11,430 --> 00:29:12,480
but, you know, isn't the biggest thing

847
00:29:12,480 --> 00:29:14,080
that you can do for performance.

848
00:29:15,000 --> 00:29:16,440
But there is a bunch of
stuff that you can do

849
00:29:16,440 --> 00:29:18,450
to make your init more efficient.

850
00:29:18,450 --> 00:29:20,190
Focus on what you can control,

851
00:29:20,190 --> 00:29:21,900
the package size, the imports,

852
00:29:21,900 --> 00:29:23,610
and the initialization logic.

853
00:29:23,610 --> 00:29:26,640
You only want to import
specific models that you need.

854
00:29:26,640 --> 00:29:30,150
So you want to avoid
importing huge big SDKs

855
00:29:30,150 --> 00:29:32,584
when you're only gonna
use a small part of it.

856
00:29:32,584 --> 00:29:34,350
You know, if you're
using Node or TypeScript

857
00:29:34,350 --> 00:29:35,183
or that kind of thing,

858
00:29:35,183 --> 00:29:38,370
you can minify production code
to reduce the download time,

859
00:29:38,370 --> 00:29:40,290
lazy initialization if
you do have functions

860
00:29:40,290 --> 00:29:41,670
that do some multiple processes,

861
00:29:41,670 --> 00:29:44,040
and so this can defer
some heavy operations

862
00:29:44,040 --> 00:29:45,390
until they're actually needed

863
00:29:45,390 --> 00:29:47,253
for maybe only part of the invokes.

864
00:29:48,300 --> 00:29:50,040
You can also think about
establishing connections

865
00:29:50,040 --> 00:29:50,970
during the init phase

866
00:29:50,970 --> 00:29:53,490
and then reusing them during
subsequent invocations

867
00:29:53,490 --> 00:29:55,530
when we're talking about warm starts.

868
00:29:55,530 --> 00:29:56,880
But, of course, you do need to think

869
00:29:56,880 --> 00:29:59,310
and you need to make sure that
you can handle reconnections

870
00:29:59,310 --> 00:30:00,180
in your handler

871
00:30:00,180 --> 00:30:02,520
to deal with any of
those stale connections.

872
00:30:02,520 --> 00:30:04,740
Keep-alives also maintain
persistent connections

873
00:30:04,740 --> 00:30:06,300
to other AWS services,

874
00:30:06,300 --> 00:30:09,060
and then also you can cache reusable data

875
00:30:09,060 --> 00:30:11,610
but also you need to think
about cleaning up sensitive data

876
00:30:11,610 --> 00:30:13,920
if you've got, you
know, subsequent invokes

877
00:30:13,920 --> 00:30:15,210
and you've got, you know, information

878
00:30:15,210 --> 00:30:16,980
from different customers
or things like that

879
00:30:16,980 --> 00:30:19,260
or secrets that pertain
to different IAM roles

880
00:30:19,260 --> 00:30:20,700
or, you know, all those kind of things,

881
00:30:20,700 --> 00:30:23,700
you do need to think about that
to make your functions safe.

882
00:30:24,630 --> 00:30:26,880
Now, there are also a lot of
code optimization strategies.

883
00:30:26,880 --> 00:30:29,010
I'm not gonna go through all of these

884
00:30:29,010 --> 00:30:31,800
because each runtime has
specific optimization techniques

885
00:30:31,800 --> 00:30:33,780
and really there's nothing
groundbreaking here.

886
00:30:33,780 --> 00:30:34,770
You know, these are just often,

887
00:30:34,770 --> 00:30:37,410
a lot of them are just
normal code best practices.

888
00:30:37,410 --> 00:30:39,681
You know, Java can benefit
from using SnapStart

889
00:30:39,681 --> 00:30:41,370
and proper SDK usage.

890
00:30:41,370 --> 00:30:44,370
JavaScript or TypeScript
can use modular SDKs

891
00:30:44,370 --> 00:30:47,850
and specifically using the
version three of AWS SDK,

892
00:30:47,850 --> 00:30:49,957
and also things like tree shaking.

893
00:30:49,957 --> 00:30:53,430
.NET can leverage AOT
compilation for faster startup.

894
00:30:53,430 --> 00:30:54,840
And then Python optimization

895
00:30:54,840 --> 00:30:56,850
is often all about your import strategy

896
00:30:56,850 --> 00:30:58,290
and the package size.

897
00:30:58,290 --> 00:30:59,580
But remember, you know, for all of these,

898
00:30:59,580 --> 00:31:02,910
connection reuse is critical
across all the runtimes

899
00:31:02,910 --> 00:31:03,783
to save you time.

900
00:31:05,100 --> 00:31:06,630
There are some native compilation options

901
00:31:06,630 --> 00:31:09,030
that can provide significant
performance benefits.

902
00:31:09,030 --> 00:31:11,970
GraalVM compiles Java
to native executables

903
00:31:11,970 --> 00:31:14,557
with subsecond cold start times.

904
00:31:14,557 --> 00:31:18,930
.NET AOT does a similar kind
of thing for .NET applications.

905
00:31:18,930 --> 00:31:21,510
Now, obviously there are
trade offs with this approach.

906
00:31:21,510 --> 00:31:23,220
There may be some longer build times

907
00:31:23,220 --> 00:31:24,690
and some runtime limitations

908
00:31:24,690 --> 00:31:27,390
and a different sort of runtime
that you maybe need to use.

909
00:31:27,390 --> 00:31:29,820
But this is a useful
thing that you can look at

910
00:31:29,820 --> 00:31:31,227
and this is, you know, often best

911
00:31:31,227 --> 00:31:34,140
for CPU intensive workloads

912
00:31:34,140 --> 00:31:36,090
with predictable execution patterns

913
00:31:36,090 --> 00:31:37,140
so you can run these.

914
00:31:38,340 --> 00:31:40,320
It does obviously
require some code changes

915
00:31:40,320 --> 00:31:43,590
and, you know, the framework
compatibility considerations,

916
00:31:43,590 --> 00:31:45,420
but more and more frameworks
are supporting them.

917
00:31:45,420 --> 00:31:46,830
And so this is most
effective for functions

918
00:31:46,830 --> 00:31:48,780
that are also frequently invoked

919
00:31:48,780 --> 00:31:50,913
or also are cold start sensitive.

920
00:31:51,960 --> 00:31:53,700
And we also have some platform features

921
00:31:53,700 --> 00:31:55,530
like provisioned concurrency and SnapStart

922
00:31:55,530 --> 00:31:58,650
to eliminate cold starts
without any cold changes,

923
00:31:58,650 --> 00:31:59,730
without any code changes,

924
00:31:59,730 --> 00:32:00,993
and we'll get into that.

925
00:32:01,890 --> 00:32:04,680
So provisioned concurrency is
available for all languages.

926
00:32:04,680 --> 00:32:06,930
You configure it then for a certain value,

927
00:32:06,930 --> 00:32:08,490
and then Lambda effectively goes ahead

928
00:32:08,490 --> 00:32:10,380
and prewarms those execution environments,

929
00:32:10,380 --> 00:32:12,780
runs that initialization
process in advance,

930
00:32:12,780 --> 00:32:13,680
and then we're gonna keep

931
00:32:13,680 --> 00:32:15,720
those execution
environments around for you.

932
00:32:15,720 --> 00:32:18,240
As they age out, we're gonna
replace them automatically.

933
00:32:18,240 --> 00:32:20,490
And see, here I've turned
on provisioned concurrency

934
00:32:20,490 --> 00:32:22,110
for this function for version 10.

935
00:32:22,110 --> 00:32:23,850
You can see the inits
are running in advance,

936
00:32:23,850 --> 00:32:25,230
and then when the invokes come in,

937
00:32:25,230 --> 00:32:28,380
they land on those already
warmed execution environments.

938
00:32:28,380 --> 00:32:30,000
And so you won't see a cold start

939
00:32:30,000 --> 00:32:31,380
in front of these environments

940
00:32:31,380 --> 00:32:32,370
and so this is helpful

941
00:32:32,370 --> 00:32:34,920
just before that sort of
8:00 p.m. patisserie rush

942
00:32:34,920 --> 00:32:37,560
from the morning rush
that Amelie would need it.

943
00:32:37,560 --> 00:32:38,393
Two other things,

944
00:32:38,393 --> 00:32:40,710
don't actually pay for provisioned
currency you don't use.

945
00:32:40,710 --> 00:32:42,300
There's a CloudWatch
metric to monitor this,

946
00:32:42,300 --> 00:32:44,610
so don't waste any of your money.

947
00:32:44,610 --> 00:32:46,590
And I pushed over too fast.

948
00:32:46,590 --> 00:32:48,600
And then also, you must use

949
00:32:48,600 --> 00:32:50,700
the function version alias, not $LATEST.

950
00:32:50,700 --> 00:32:52,087
That trips people out when they wonder,

951
00:32:52,087 --> 00:32:55,230
"Why is my provisioned
concurrency not doing anything?"

952
00:32:55,230 --> 00:32:56,610
So when to use it.

953
00:32:56,610 --> 00:32:57,630
This is for applications

954
00:32:57,630 --> 00:32:59,340
that need less than a hundred milliseconds

955
00:32:59,340 --> 00:33:00,780
of startup latency.

956
00:33:00,780 --> 00:33:02,430
So you need to configure the value

957
00:33:02,430 --> 00:33:03,450
for provisioned concurrency,

958
00:33:03,450 --> 00:33:05,640
so it's best for
predictable traffic patterns

959
00:33:05,640 --> 00:33:06,690
which justify the cost

960
00:33:06,690 --> 00:33:09,480
of keeping those warm
environments up and running.

961
00:33:09,480 --> 00:33:12,360
So obviously it's gonna be
good for mission-critical APIs

962
00:33:12,360 --> 00:33:14,670
where any latency variable is unacceptable

963
00:33:14,670 --> 00:33:16,380
or some high-traffic applications

964
00:33:16,380 --> 00:33:19,740
needing consistent performance
during heavy periods.

965
00:33:19,740 --> 00:33:20,910
Now, for implementation,

966
00:33:20,910 --> 00:33:22,860
I suggest you sort of
need to do some research

967
00:33:22,860 --> 00:33:24,720
and understand your execution patterns.

968
00:33:24,720 --> 00:33:27,000
What times of the day do
you need to pre-provision

969
00:33:27,000 --> 00:33:28,230
the provisioned concurrency?

970
00:33:28,230 --> 00:33:30,120
And you can just start
with a static figure

971
00:33:30,120 --> 00:33:32,610
and then maybe evolve it
into something dynamic,

972
00:33:32,610 --> 00:33:35,520
configuring it using
application order scaling

973
00:33:35,520 --> 00:33:36,353
for just when you need it.

974
00:33:36,353 --> 00:33:37,440
And that can be very efficient

975
00:33:37,440 --> 00:33:40,560
to be able to ramp up beforehand
and ramp down afterwards.

976
00:33:40,560 --> 00:33:42,270
And with Lambda's new,

977
00:33:42,270 --> 00:33:45,570
or recent, I suppose a year
ago now, scalability things,

978
00:33:45,570 --> 00:33:47,280
it's really quick to get
provisioned concurrency

979
00:33:47,280 --> 00:33:48,580
up and running in advance.

980
00:33:49,800 --> 00:33:52,380
So then Lambda SnapStart
for Java, Python, and .NET

981
00:33:52,380 --> 00:33:53,550
is another platform feature

982
00:33:53,550 --> 00:33:55,170
that runs the cold start process

983
00:33:55,170 --> 00:33:57,180
when you publish a function in advance,

984
00:33:57,180 --> 00:33:59,460
so not actually just before an invoke.

985
00:33:59,460 --> 00:34:01,290
And so when you need
to invoke the function,

986
00:34:01,290 --> 00:34:02,850
it's gonna resume the snapshot

987
00:34:02,850 --> 00:34:05,070
and it's generally gonna
do this in under a second.

988
00:34:05,070 --> 00:34:06,270
And so it makes it perfect

989
00:34:06,270 --> 00:34:09,220
for cost-sensitive applications
with unpredictable traffic.

990
00:34:10,170 --> 00:34:11,070
There's no additional cost,

991
00:34:11,070 --> 00:34:12,450
which makes it a great workload

992
00:34:12,450 --> 00:34:14,910
where provisioned concurrency
would be expensive.

993
00:34:14,910 --> 00:34:16,680
Functions with heavy runtime startup

994
00:34:16,680 --> 00:34:18,240
or loading heavy dependencies,

995
00:34:18,240 --> 00:34:20,760
they're gonna be the ones
that see the biggest benefit.

996
00:34:20,760 --> 00:34:22,530
Java is gonna see great performance gains

997
00:34:22,530 --> 00:34:25,230
to, you know, having to start up the JVM.

998
00:34:25,230 --> 00:34:27,150
Python is gonna be benefit significantly

999
00:34:27,150 --> 00:34:28,980
when loading big ML frameworks.

1000
00:34:28,980 --> 00:34:32,700
And .NET gains from eliminating
the compilation delays.

1001
00:34:32,700 --> 00:34:35,010
And remember, again, to
target a function version

1002
00:34:35,010 --> 00:34:37,170
and you also cannot use both SnapStart

1003
00:34:37,170 --> 00:34:39,420
and provisioned concurrency
at the same time.

1004
00:34:40,560 --> 00:34:43,560
So what can you do to actually
make that snapshot process

1005
00:34:43,560 --> 00:34:45,330
as efficient as possible?

1006
00:34:45,330 --> 00:34:48,210
For Java, what you can use
the beforeCheckpoint hook

1007
00:34:48,210 --> 00:34:49,650
to preload dependencies

1008
00:34:49,650 --> 00:34:52,260
and initialize resources
using the init phase.

1009
00:34:52,260 --> 00:34:54,630
And this actually uses
the CRaC runtime hooks,

1010
00:34:54,630 --> 00:34:56,430
which is from an open-source project.

1011
00:34:56,430 --> 00:34:57,900
And there are two ways to do this.

1012
00:34:57,900 --> 00:34:59,370
You can actually use invoke priming,

1013
00:34:59,370 --> 00:35:01,230
which does have the highest performance,

1014
00:35:01,230 --> 00:35:03,270
but make sure that you then use stub data

1015
00:35:03,270 --> 00:35:04,950
and your code must be item potent

1016
00:35:04,950 --> 00:35:06,930
as actually running real live invokes

1017
00:35:06,930 --> 00:35:08,700
for your Lambda function to set this up.

1018
00:35:08,700 --> 00:35:10,440
Or you can choose to use class priming

1019
00:35:10,440 --> 00:35:11,820
and this is then gonna load classes

1020
00:35:11,820 --> 00:35:14,550
without actually the method of execution.

1021
00:35:14,550 --> 00:35:15,780
So two different ways,

1022
00:35:15,780 --> 00:35:17,923
but you wanna run as much as
you can in this init process

1023
00:35:17,923 --> 00:35:19,953
so that's gonna optimize your invokes.

1024
00:35:21,120 --> 00:35:25,560
And for .NET, it's actually you
want to run multiple invokes

1025
00:35:25,560 --> 00:35:27,240
as part of the init

1026
00:35:27,240 --> 00:35:29,550
using the RegisterBeforeSnapshot,

1027
00:35:29,550 --> 00:35:32,850
which is what makes .NET's
snapshot really effective.

1028
00:35:32,850 --> 00:35:34,650
And literally you run wanna run probably,

1029
00:35:34,650 --> 00:35:36,780
you know, maybe even 10 or 20 times

1030
00:35:36,780 --> 00:35:38,640
and the signals .NET runtime

1031
00:35:38,640 --> 00:35:41,070
to perform tiered
compilation aggressively.

1032
00:35:41,070 --> 00:35:43,627
It's sort of, I suppose, think
of saying to the runtime,

1033
00:35:43,627 --> 00:35:44,910
"This code is hot.

1034
00:35:44,910 --> 00:35:47,670
Optimize it as aggressively as you can."

1035
00:35:47,670 --> 00:35:49,830
And again, you know,
use dummy or stub data

1036
00:35:49,830 --> 00:35:52,320
to avoid side effects during warmup

1037
00:35:52,320 --> 00:35:53,850
because you are running real invokes.

1038
00:35:53,850 --> 00:35:56,600
And this, you know, works
with many frameworks as well.

1039
00:35:57,570 --> 00:35:59,700
So there's some universal
best practices for SnapStart.

1040
00:35:59,700 --> 00:36:02,160
You wanna reestablish network
connections after restore.

1041
00:36:02,160 --> 00:36:03,750
State isn't gonna be guaranteed.

1042
00:36:03,750 --> 00:36:06,690
You wanna generate unique IDs and secrets

1043
00:36:06,690 --> 00:36:08,310
after the initialization,

1044
00:36:08,310 --> 00:36:09,480
and then you obviously wanna refresh

1045
00:36:09,480 --> 00:36:11,850
credentials and timestamps
then in your handler.

1046
00:36:11,850 --> 00:36:15,480
And, of course, only executed
init code paths are captured.

1047
00:36:15,480 --> 00:36:17,760
So you want to use those
runtime hooks we spoke about

1048
00:36:17,760 --> 00:36:19,320
to warm those critical paths.

1049
00:36:19,320 --> 00:36:22,530
And then again, you must use
function versions, not $LATEST,

1050
00:36:22,530 --> 00:36:25,080
and obviously you wanna test
with your realistic workloads.

1051
00:36:25,080 --> 00:36:26,610
And CloudWatch Metrics can be your friend

1052
00:36:26,610 --> 00:36:28,170
to validate the performance improvement

1053
00:36:28,170 --> 00:36:29,700
with your business impact.

1054
00:36:29,700 --> 00:36:31,830
Snapshot is actually a really great,

1055
00:36:31,830 --> 00:36:32,760
you know, quick and easy win

1056
00:36:32,760 --> 00:36:35,010
to be able to improve
cold start performance.

1057
00:36:36,030 --> 00:36:37,399
Now, another optimization tip,

1058
00:36:37,399 --> 00:36:38,970
just upgrade your runtime.

1059
00:36:38,970 --> 00:36:41,610
Seriously, it's often one of
the simplest things to do.

1060
00:36:41,610 --> 00:36:42,570
You know, in this example,

1061
00:36:42,570 --> 00:36:44,940
upgrading Python had a
dramatic increase in latency,

1062
00:36:44,940 --> 00:36:47,790
which improves performance
and reduces cost.

1063
00:36:47,790 --> 00:36:49,620
You know, languages are
continually evolving

1064
00:36:49,620 --> 00:36:51,630
and help keeping up to
date with runtime means

1065
00:36:51,630 --> 00:36:53,460
you also get the best
performance for your buck

1066
00:36:53,460 --> 00:36:55,650
and, of course, there's
a security implication

1067
00:36:55,650 --> 00:36:56,500
for this as well.

1068
00:36:57,570 --> 00:37:00,060
So a systematic approach
delivered dramatic improvements,

1069
00:37:00,060 --> 00:37:01,740
starting with memory right-sizing,

1070
00:37:01,740 --> 00:37:02,610
the main kind of thing,

1071
00:37:02,610 --> 00:37:04,890
the biggest impact, the
easiest implementation.

1072
00:37:04,890 --> 00:37:07,200
Code optimization
provided additional gains

1073
00:37:07,200 --> 00:37:08,880
through better resource utilization.

1074
00:37:08,880 --> 00:37:11,340
Cold start mitigation ensured
consistent performance

1075
00:37:11,340 --> 00:37:12,360
during traffic spikes,

1076
00:37:12,360 --> 00:37:14,010
and native compilation provided

1077
00:37:14,010 --> 00:37:16,110
that last little performance boost

1078
00:37:16,110 --> 00:37:18,603
for CPU intensive workloads.

1079
00:37:19,830 --> 00:37:21,720
So next up, how Amelie
built resilient systems

1080
00:37:21,720 --> 00:37:25,050
through proper failure
handling and error recovery.

1081
00:37:25,050 --> 00:37:26,970
Now, lots of locations, 35 of them,

1082
00:37:26,970 --> 00:37:29,160
processing thousands of orders daily.

1083
00:37:29,160 --> 00:37:31,110
That holiday rush I spoke
about earlier created

1084
00:37:31,110 --> 00:37:33,420
this sort of perfect storm
of cascading failures.

1085
00:37:33,420 --> 00:37:35,700
Payment issues, a crashed order system,

1086
00:37:35,700 --> 00:37:37,800
no notifications, manual recovery,

1087
00:37:37,800 --> 00:37:39,450
obviously this is a big business impact.

1088
00:37:39,450 --> 00:37:41,340
Lost revenue, angry customers,

1089
00:37:41,340 --> 00:37:42,330
a confused kitchen.

1090
00:37:42,330 --> 00:37:43,380
What's going on?

1091
00:37:43,380 --> 00:37:45,752
So, of course, this crisis
forced them to understand

1092
00:37:45,752 --> 00:37:48,813
that resilience is also
a business imperative.

1093
00:37:49,890 --> 00:37:50,723
So with all of this,

1094
00:37:50,723 --> 00:37:54,300
the most important thing to
think about is item potency.

1095
00:37:54,300 --> 00:37:57,131
Item potency ensures
operations have the same effect

1096
00:37:57,131 --> 00:37:59,940
whether executed once or multiple times.

1097
00:37:59,940 --> 00:38:02,070
And this is critical
for distributed systems

1098
00:38:02,070 --> 00:38:03,420
where retries, duplicates,

1099
00:38:03,420 --> 00:38:05,850
and out of order processing occur.

1100
00:38:05,850 --> 00:38:08,100
Basically, I don't want
to be charged twice,

1101
00:38:08,100 --> 00:38:11,910
so please build all your
item potency operations.

1102
00:38:11,910 --> 00:38:14,100
But you do then need to think
about your item potency,

1103
00:38:14,100 --> 00:38:15,720
what that item potency key is,

1104
00:38:15,720 --> 00:38:18,000
that sort of unique
transaction identifier.

1105
00:38:18,000 --> 00:38:19,290
You know, is it gonna be an order ID

1106
00:38:19,290 --> 00:38:21,890
or something that you can
use for your item potency?

1107
00:38:22,980 --> 00:38:25,390
And many AWS services provide
built-in item potency,

1108
00:38:25,390 --> 00:38:27,450
which you may not know about.

1109
00:38:27,450 --> 00:38:29,610
DynamoDB, you can use conditional rights,

1110
00:38:29,610 --> 00:38:32,190
and so this is gonna prevent
duplicate operations.

1111
00:38:32,190 --> 00:38:35,520
SQS FIFO queues can use a
message deduplication ID

1112
00:38:35,520 --> 00:38:37,260
for automatic deduplication.

1113
00:38:37,260 --> 00:38:38,760
And did you actually
know for Step Functions,

1114
00:38:38,760 --> 00:38:41,070
you can just specify an execution name

1115
00:38:41,070 --> 00:38:42,210
to prevent duplicates?

1116
00:38:42,210 --> 00:38:43,650
So you've got a Step Functions workflow,

1117
00:38:43,650 --> 00:38:44,790
just provide the name.

1118
00:38:44,790 --> 00:38:46,050
If the same name has happened before,

1119
00:38:46,050 --> 00:38:47,760
and you can use, you
know, maybe an order ID

1120
00:38:47,760 --> 00:38:48,600
or that kind of thing,

1121
00:38:48,600 --> 00:38:49,710
Step Functions won't run it.

1122
00:38:49,710 --> 00:38:51,840
So simple and a lot of people don't know.

1123
00:38:51,840 --> 00:38:54,480
And, of course, Lambda durable
functions is another way

1124
00:38:54,480 --> 00:38:56,040
because item potency is just part

1125
00:38:56,040 --> 00:38:57,783
of the checkpointing part of it.

1126
00:38:58,800 --> 00:39:01,650
And for your code,
Powertools for AWS Lambda,

1127
00:39:01,650 --> 00:39:05,220
please, this is a tool and library

1128
00:39:05,220 --> 00:39:08,100
for .NET, for Python, for Java,

1129
00:39:08,100 --> 00:39:09,788
and for Node and TypeScript,

1130
00:39:09,788 --> 00:39:11,550
just use this for your functions.

1131
00:39:11,550 --> 00:39:13,710
It has a huge amount of functionality.

1132
00:39:13,710 --> 00:39:15,240
Item potency is included

1133
00:39:15,240 --> 00:39:18,060
and it actually just uses
DynamoDB behind the scenes.

1134
00:39:18,060 --> 00:39:19,230
You just set it up in your code

1135
00:39:19,230 --> 00:39:20,970
and item potency is handled for you.

1136
00:39:20,970 --> 00:39:21,803
Super useful.

1137
00:39:23,070 --> 00:39:24,810
So we already covered SQS's deal queues

1138
00:39:24,810 --> 00:39:25,890
and Lambda destinations.

1139
00:39:25,890 --> 00:39:28,530
This is all part of your
failure handling toolkit.

1140
00:39:28,530 --> 00:39:32,610
And remember, retries are
all built into Step Functions

1141
00:39:32,610 --> 00:39:35,310
when using Step Functions
set as configuration,

1142
00:39:35,310 --> 00:39:38,040
part of your failure and error handling.

1143
00:39:38,040 --> 00:39:39,030
And you have retry,

1144
00:39:39,030 --> 00:39:39,960
you've got catch dates,

1145
00:39:39,960 --> 00:39:41,430
you've got choice and parallel states,

1146
00:39:41,430 --> 00:39:42,630
and this can give you a whole lot

1147
00:39:42,630 --> 00:39:44,040
of error handling goodness.

1148
00:39:44,040 --> 00:39:45,060
If you're building saga patterns

1149
00:39:45,060 --> 00:39:45,983
or other kind of things,

1150
00:39:45,983 --> 00:39:48,870
a whole bunch of functionality.

1151
00:39:48,870 --> 00:39:50,730
And so Amelie's order processing workflow

1152
00:39:50,730 --> 00:39:53,144
has comprehensive error
handling at each step

1153
00:39:53,144 --> 00:39:54,780
to preventing single failures

1154
00:39:54,780 --> 00:39:57,570
from bringing down the
entire order process.

1155
00:39:57,570 --> 00:39:59,310
And if you prefer the full code approach,

1156
00:39:59,310 --> 00:40:00,960
you know, we've got new options now

1157
00:40:00,960 --> 00:40:02,463
with Lambda durable functions.

1158
00:40:04,080 --> 00:40:07,050
So next, Amelie gained complete visibility

1159
00:40:07,050 --> 00:40:09,240
into their serverless systems

1160
00:40:09,240 --> 00:40:11,727
through advanced observability.

1161
00:40:11,727 --> 00:40:13,470
You know, 35 locations,

1162
00:40:13,470 --> 00:40:15,990
system visibility gaps across services,

1163
00:40:15,990 --> 00:40:17,640
difficult to trace orders,

1164
00:40:17,640 --> 00:40:20,190
gonna identify bottlenecks
causing the slow processing,

1165
00:40:20,190 --> 00:40:21,240
where they're coming from,

1166
00:40:21,240 --> 00:40:23,460
or trying to connect actually
some of the technical metrics

1167
00:40:23,460 --> 00:40:25,410
also to the business impact.

1168
00:40:25,410 --> 00:40:27,690
And a breaking point was
an undetected payment issue

1169
00:40:27,690 --> 00:40:28,920
which happened for two hours.

1170
00:40:28,920 --> 00:40:31,200
Resulted in, you know,
hundreds of lost orders

1171
00:40:31,200 --> 00:40:34,410
and $12,000 in revenue loss
during the dinner rush.

1172
00:40:34,410 --> 00:40:36,223
Now, that's not a good thing

1173
00:40:36,223 --> 00:40:37,770
that any fledgling business wants to run.

1174
00:40:37,770 --> 00:40:39,243
No one wants to fly blind.

1175
00:40:40,380 --> 00:40:42,780
Now, observability is a massive topic

1176
00:40:42,780 --> 00:40:44,250
and it's in a fast-moving place,

1177
00:40:44,250 --> 00:40:46,260
and there are many
breakouts here at re:Invent,

1178
00:40:46,260 --> 00:40:47,550
which is all about observability.

1179
00:40:47,550 --> 00:40:49,470
So I'm just gonna give
you some of the new things

1180
00:40:49,470 --> 00:40:52,590
that you can explore to
find out to help you out.

1181
00:40:52,590 --> 00:40:53,970
I love Charity Majors' work.

1182
00:40:53,970 --> 00:40:54,810
She works for Honeycomb

1183
00:40:54,810 --> 00:40:56,250
and she talks a lot more

1184
00:40:56,250 --> 00:40:58,500
about more than just the
pillars of observability,

1185
00:40:58,500 --> 00:41:00,900
but a bunch of different signals of data

1186
00:41:00,900 --> 00:41:03,270
which you can use for
unified observability data.

1187
00:41:03,270 --> 00:41:05,194
And seriously, read up Charity Majors,

1188
00:41:05,194 --> 00:41:07,320
read everything she writes about

1189
00:41:07,320 --> 00:41:09,330
and talks about observability.

1190
00:41:09,330 --> 00:41:11,760
She's an absolute guru and amazing.

1191
00:41:11,760 --> 00:41:13,230
So you've got things like metrics

1192
00:41:13,230 --> 00:41:15,300
and logs and traces and
events and profiling

1193
00:41:15,300 --> 00:41:17,490
and they can all be part of these signals

1194
00:41:17,490 --> 00:41:18,993
and, you know, other
kind of business things

1195
00:41:18,993 --> 00:41:19,920
that can come in.

1196
00:41:19,920 --> 00:41:21,573
Observability is a big thing.

1197
00:41:22,440 --> 00:41:23,640
But the thing I want you to take away

1198
00:41:23,640 --> 00:41:27,240
is you wanna really focus on
embedding business context

1199
00:41:27,240 --> 00:41:29,190
in all of the signals for
rapid troubleshooting.

1200
00:41:29,190 --> 00:41:31,200
So this isn't just a
technical monitoring thing.

1201
00:41:31,200 --> 00:41:32,400
This is observability,

1202
00:41:32,400 --> 00:41:34,890
and the business is very much part of it.

1203
00:41:34,890 --> 00:41:35,940
So this shifts your thinking

1204
00:41:35,940 --> 00:41:37,950
from not sort of only monitoring failures

1205
00:41:37,950 --> 00:41:40,770
to being able to answer
questions about individual things

1206
00:41:40,770 --> 00:41:42,243
to understand your business.

1207
00:41:43,230 --> 00:41:46,406
Again, Powertools is your
friend here for observability.

1208
00:41:46,406 --> 00:41:48,090
It just makes things so much easier.

1209
00:41:48,090 --> 00:41:49,710
You can import the
libraries really quickly,

1210
00:41:49,710 --> 00:41:51,420
configure up your service names,

1211
00:41:51,420 --> 00:41:54,390
and then it's easy to set up the logging

1212
00:41:54,390 --> 00:41:55,770
and the metrics as well.

1213
00:41:55,770 --> 00:41:56,970
And this is then gonna use

1214
00:41:56,970 --> 00:41:58,320
structured logging behind the scenes

1215
00:41:58,320 --> 00:42:00,660
and using the embedded metrics format

1216
00:42:00,660 --> 00:42:03,360
to create metrics from the
logs entries automatically.

1217
00:42:03,360 --> 00:42:05,674
A single log write creates
both searchable logs

1218
00:42:05,674 --> 00:42:06,630
and CloudWatch Metrics

1219
00:42:06,630 --> 00:42:08,539
and this is also faster and cheaper.

1220
00:42:08,539 --> 00:42:09,372
And this is a great way then

1221
00:42:09,372 --> 00:42:11,520
to incorporate that business
context I was talking about

1222
00:42:11,520 --> 00:42:13,290
like an order ID for the logs

1223
00:42:13,290 --> 00:42:15,290
without having it as a metric dimension,

1224
00:42:16,170 --> 00:42:18,150
which is gonna be pricing.

1225
00:42:18,150 --> 00:42:20,580
So here we see both entries
written to CloudWatch logs

1226
00:42:20,580 --> 00:42:21,750
in a single log stream.

1227
00:42:21,750 --> 00:42:23,820
The regular logs are queryable

1228
00:42:23,820 --> 00:42:25,620
with CloudWatch Insights for debugging.

1229
00:42:25,620 --> 00:42:26,850
And then the EMF entry

1230
00:42:26,850 --> 00:42:30,420
lets CloudWatch automatically
extract the metrics from it,

1231
00:42:30,420 --> 00:42:31,623
a powerful two for one.

1232
00:42:32,730 --> 00:42:34,470
CloudWatch Metrics also
has a bunch of new stuff

1233
00:42:34,470 --> 00:42:36,060
like powerful tag-based automation

1234
00:42:36,060 --> 00:42:37,650
with filtering on alarming

1235
00:42:37,650 --> 00:42:39,840
instead of configuring
individual function alarm tags,

1236
00:42:39,840 --> 00:42:41,340
and this makes it way easier.

1237
00:42:41,340 --> 00:42:42,173
And it's cheaper.

1238
00:42:42,173 --> 00:42:44,070
You've got one alarm
instead of, you know, 50

1239
00:42:44,070 --> 00:42:49,070
and individual alarms
reduce the CloudWatch costs.

1240
00:42:49,110 --> 00:42:51,360
There's really fancy querying
capabilities now available

1241
00:42:51,360 --> 00:42:52,770
and a cool new dynamic dashboard

1242
00:42:52,770 --> 00:42:54,630
where you can actually
build up your dashboard

1243
00:42:54,630 --> 00:42:56,220
and then you can just
select your environment

1244
00:42:56,220 --> 00:42:57,360
with a dropdown selection

1245
00:42:57,360 --> 00:42:58,960
and it changes all in real time.

1246
00:42:59,820 --> 00:43:01,860
The query generator
converts natural language

1247
00:43:01,860 --> 00:43:03,540
to CloudWatch Metrics SQL

1248
00:43:03,540 --> 00:43:05,250
and allows you to refine existing queries,

1249
00:43:05,250 --> 00:43:06,930
which I found so much easier.

1250
00:43:06,930 --> 00:43:09,150
The metrics history has also
been extended from three hours

1251
00:43:09,150 --> 00:43:12,750
all the way up to two weeks
for historical analysis.

1252
00:43:12,750 --> 00:43:14,650
CloudWatch Application Signals
is also something to look at,

1253
00:43:14,650 --> 00:43:16,410
a new observability solution

1254
00:43:16,410 --> 00:43:18,240
which is based on open telemetry,

1255
00:43:18,240 --> 00:43:20,820
and this can also
automatically discover services

1256
00:43:20,820 --> 00:43:22,380
and it's got a more advanced service map

1257
00:43:22,380 --> 00:43:24,720
and a whole bunch of other things.

1258
00:43:24,720 --> 00:43:26,580
So I can't do CloudWatch justice here

1259
00:43:26,580 --> 00:43:29,280
and I don't want to, you know,
flick over kind of things.

1260
00:43:29,280 --> 00:43:31,650
So the awesome Joe Alioto
has done a great video

1261
00:43:31,650 --> 00:43:32,483
on all the new things

1262
00:43:32,483 --> 00:43:33,316
and there are gonna be new things

1263
00:43:33,316 --> 00:43:34,290
announced this week as well.

1264
00:43:34,290 --> 00:43:36,180
So I just really suggest you taking a look

1265
00:43:36,180 --> 00:43:37,890
'cause there's a huge amount

1266
00:43:37,890 --> 00:43:40,290
that's gone on in observability this year,

1267
00:43:40,290 --> 00:43:43,230
which can really help
you with your business.

1268
00:43:43,230 --> 00:43:46,800
So talking about OTEL for Lambda.

1269
00:43:46,800 --> 00:43:48,540
Well, the X-Ray SDK is coming to an end.

1270
00:43:48,540 --> 00:43:49,860
X-Ray as as service continues,

1271
00:43:49,860 --> 00:43:51,750
but it's moving to OTEL
for instrumentation

1272
00:43:51,750 --> 00:43:54,870
using the AWS distro for open telemetry.

1273
00:43:54,870 --> 00:43:57,045
If you want to start
looking at this for Lambda,

1274
00:43:57,045 --> 00:43:58,020
there is an ADOT layer,

1275
00:43:58,020 --> 00:43:59,190
but we actually recommend using

1276
00:43:59,190 --> 00:44:01,260
the Application Signals Lambda layer.

1277
00:44:01,260 --> 00:44:04,200
It's actually an enhanced version of ADOT

1278
00:44:04,200 --> 00:44:06,240
specifically optimized for Lambda

1279
00:44:06,240 --> 00:44:09,660
with better performance and
lower resource consumption.

1280
00:44:09,660 --> 00:44:11,730
You can also configure
the Application Signals

1281
00:44:11,730 --> 00:44:14,640
to use only the OTEL
libraries for instrumentation

1282
00:44:14,640 --> 00:44:18,603
without all the other features
for Application Signals

1283
00:44:18,603 --> 00:44:20,670
by just setting an environment variable.

1284
00:44:20,670 --> 00:44:22,200
So moving to OTEL at the moment

1285
00:44:22,200 --> 00:44:23,520
does mean a little bit more money

1286
00:44:23,520 --> 00:44:25,050
and a small cold start penalty,

1287
00:44:25,050 --> 00:44:26,370
but if OTEL is your thing,

1288
00:44:26,370 --> 00:44:28,260
at least we've got some good options.

1289
00:44:28,260 --> 00:44:29,970
And, of course, we love
partnering with others.

1290
00:44:29,970 --> 00:44:32,822
So you can use your favorite
third-party tools with Lambda

1291
00:44:32,822 --> 00:44:34,140
to get the observability you need.

1292
00:44:34,140 --> 00:44:37,560
And so Amelie did a lot
of observability work,

1293
00:44:37,560 --> 00:44:38,820
more than I can cover here in the time,

1294
00:44:38,820 --> 00:44:39,810
but it all pays off.

1295
00:44:39,810 --> 00:44:41,820
A transformation from disaster prone

1296
00:44:41,820 --> 00:44:43,260
to a highly reliable system.

1297
00:44:43,260 --> 00:44:46,503
Better reliability, recovery
times, less tail chasing.

1298
00:44:47,370 --> 00:44:49,710
You know, Black Friday
2024 had zero outages

1299
00:44:49,710 --> 00:44:50,910
during a peak traffic

1300
00:44:50,910 --> 00:44:52,530
and, you know, far less
customer complaints

1301
00:44:52,530 --> 00:44:53,780
to better error handling.

1302
00:44:55,200 --> 00:44:56,640
Next, let's talk about costs

1303
00:44:56,640 --> 00:45:01,203
and how Amelie optimized costs
using design while scaling.

1304
00:45:02,310 --> 00:45:05,130
But Amelie started the serverless
journey with excitement,

1305
00:45:05,130 --> 00:45:06,960
but faced unexpected cost challenges.

1306
00:45:06,960 --> 00:45:08,100
This can happen to you.

1307
00:45:08,100 --> 00:45:11,640
Monthly AWS bills jumping
from $200 to $2,000

1308
00:45:11,640 --> 00:45:12,810
without much warning.

1309
00:45:12,810 --> 00:45:14,070
During a growth phase,

1310
00:45:14,070 --> 00:45:16,260
Lambda invocations spiking crazily.

1311
00:45:16,260 --> 00:45:19,350
Maybe overprovisioned Lambda
functions with too much memory.

1312
00:45:19,350 --> 00:45:22,560
CloudWatch logs accumulating
gigabytes of data daily.

1313
00:45:22,560 --> 00:45:23,850
Each new restaurant adding

1314
00:45:23,850 --> 00:45:25,440
more and more infrastructure costs,

1315
00:45:25,440 --> 00:45:27,927
and data transfer and
storage costs all adding up.

1316
00:45:27,927 --> 00:45:30,180
And Amelie realized that in serverless,

1317
00:45:30,180 --> 00:45:33,213
architecture decisions
really impact costs.

1318
00:45:34,560 --> 00:45:36,510
Understanding serverless
pricing is crucial

1319
00:45:36,510 --> 00:45:38,430
for cost-effective
architectural decisions.

1320
00:45:38,430 --> 00:45:40,080
Lambda is on demand.

1321
00:45:40,080 --> 00:45:41,550
It uses pay per use model

1322
00:45:41,550 --> 00:45:44,460
with charges for requests and
then also your compute time.

1323
00:45:44,460 --> 00:45:46,950
So init time is charged
from this year to remember

1324
00:45:46,950 --> 00:45:49,770
and this was to avoid complexity
with some inits charging

1325
00:45:49,770 --> 00:45:51,750
and some inits not being charged.

1326
00:45:51,750 --> 00:45:53,760
So it's all across the
board, inits are charged.

1327
00:45:53,760 --> 00:45:56,190
And then per millisecond
billing is super powerful

1328
00:45:56,190 --> 00:45:57,030
with each invoke

1329
00:45:57,030 --> 00:45:58,920
and, of course, there's
inits charged separately.

1330
00:45:58,920 --> 00:46:00,690
And so this is cost-effective

1331
00:46:00,690 --> 00:46:02,640
when there are gaps and work to do.

1332
00:46:02,640 --> 00:46:05,160
And you can also consider
Lambda managed instances

1333
00:46:05,160 --> 00:46:06,780
for bigger steady state workloads

1334
00:46:06,780 --> 00:46:09,680
where you can run Lambda scale
in a cost-effective manner.

1335
00:46:10,740 --> 00:46:13,410
Now, while Fargate uses time-based model

1336
00:46:13,410 --> 00:46:15,660
with charges for CPU and memory for hour.

1337
00:46:15,660 --> 00:46:17,130
You pay for what you provision.

1338
00:46:17,130 --> 00:46:18,570
Now, I didn't have enough time today,

1339
00:46:18,570 --> 00:46:19,470
there was enough to talk about,

1340
00:46:19,470 --> 00:46:21,360
to go into all Fargate best practices,

1341
00:46:21,360 --> 00:46:23,880
but it is a super great
solution for many workloads

1342
00:46:23,880 --> 00:46:26,610
when you don't want to
use an event-driven model.

1343
00:46:26,610 --> 00:46:29,340
And it can also be cheaper
than Lambda at scale.

1344
00:46:29,340 --> 00:46:32,790
Continuous processing or
long-running predictive workloads

1345
00:46:32,790 --> 00:46:34,260
are gonna favor Fargate

1346
00:46:34,260 --> 00:46:37,590
and sporadic, event-driven
processing is gonna favor Lambda,

1347
00:46:37,590 --> 00:46:39,090
even though Lambda can scale up

1348
00:46:39,090 --> 00:46:41,040
to super high workloads.

1349
00:46:41,040 --> 00:46:43,323
This is cost things that you can work out.

1350
00:46:44,504 --> 00:46:45,510
For Step Functions,

1351
00:46:45,510 --> 00:46:48,300
express workflows can also
be significantly cheaper.

1352
00:46:48,300 --> 00:46:49,260
Amelie's using both,

1353
00:46:49,260 --> 00:46:51,330
but you can see how
pricing can be cheaper.

1354
00:46:51,330 --> 00:46:53,520
Many more invocations
for 1/20th of the cost

1355
00:46:53,520 --> 00:46:55,710
in this example using express workflows.

1356
00:46:55,710 --> 00:46:57,720
So also take a look at your
existing state machines

1357
00:46:57,720 --> 00:47:00,262
and see whether you can
potentially save some money

1358
00:47:00,262 --> 00:47:02,940
and then consider using
express workflows first

1359
00:47:02,940 --> 00:47:05,140
when you're building
your next applications.

1360
00:47:06,060 --> 00:47:08,755
And optimizing your logging
costs can be an easy win.

1361
00:47:08,755 --> 00:47:11,130
AWS rolled out tiered
pricing for logs this year.

1362
00:47:11,130 --> 00:47:12,360
Basically, the more you log,

1363
00:47:12,360 --> 00:47:14,010
the cheaper it gets per gigabytes.

1364
00:47:14,010 --> 00:47:15,810
The best part, it's automatic.

1365
00:47:15,810 --> 00:47:16,680
You don't have to do anything.

1366
00:47:16,680 --> 00:47:17,820
It's just gonna be cheaper.

1367
00:47:17,820 --> 00:47:20,310
Use structured logging
with smart log levels.

1368
00:47:20,310 --> 00:47:22,350
Maybe you're gonna set
info as your default.

1369
00:47:22,350 --> 00:47:24,540
Maybe you're gonna sample
debug logs sparingly.

1370
00:47:24,540 --> 00:47:26,610
And use Lambda Powertools
for efficient logging

1371
00:47:26,610 --> 00:47:28,650
without the custom code overhead.

1372
00:47:28,650 --> 00:47:30,720
You also want to match
your retention to reality.

1373
00:47:30,720 --> 00:47:33,210
Maybe you need 30 days
for dev, 90 for staging,

1374
00:47:33,210 --> 00:47:34,770
three to six months for production.

1375
00:47:34,770 --> 00:47:36,449
I don't know, it's gonna
depend on your workload,

1376
00:47:36,449 --> 00:47:40,620
but basically stop paying for
logs you're never gonna use.

1377
00:47:40,620 --> 00:47:42,180
Also, don't log what you don't need to.

1378
00:47:42,180 --> 00:47:43,886
Skip massive payloads in your logs.

1379
00:47:43,886 --> 00:47:46,020
Just log key identifiers instead.

1380
00:47:46,020 --> 00:47:48,630
You can also archive to S3
now for long-term storage

1381
00:47:48,630 --> 00:47:50,430
or log directly to Data Firehose

1382
00:47:50,430 --> 00:47:51,990
when you want high throughput

1383
00:47:51,990 --> 00:47:54,540
and maybe wanna send the logs
to some other destinations.

1384
00:47:54,540 --> 00:47:56,190
And lastly, monitor those costs.

1385
00:47:56,190 --> 00:47:57,900
You wanna set CloudWatch billing alerts

1386
00:47:57,900 --> 00:48:00,933
and use Cost Explorer to identify
your biggest log spenders.

1387
00:48:02,070 --> 00:48:04,500
So there's lots that Amelie
did which reduced costs,

1388
00:48:04,500 --> 00:48:06,810
which also we covered in
some other kind of sections.

1389
00:48:06,810 --> 00:48:08,310
Right-sizing was the biggest flex

1390
00:48:08,310 --> 00:48:09,720
to not pay unnecessarily.

1391
00:48:09,720 --> 00:48:12,240
Filtering is obvious to not
just have to work on things

1392
00:48:12,240 --> 00:48:13,200
that you don't need to,

1393
00:48:13,200 --> 00:48:15,180
but it's gonna be able to, you know, avoid

1394
00:48:15,180 --> 00:48:16,800
a whole bunch of costs unnecessarily.

1395
00:48:16,800 --> 00:48:18,870
And also using compute savings plans.

1396
00:48:18,870 --> 00:48:20,310
Amelie is looking to explore that as well.

1397
00:48:20,310 --> 00:48:21,240
She hasn't quite done that.

1398
00:48:21,240 --> 00:48:22,530
That's another kind of option.

1399
00:48:22,530 --> 00:48:24,570
And so with some simple optimizations,

1400
00:48:24,570 --> 00:48:26,883
you can make a big difference on costs.

1401
00:48:28,020 --> 00:48:30,930
Every architecture decision
has a direct cost impact.

1402
00:48:30,930 --> 00:48:32,460
Understand your pricing models,

1403
00:48:32,460 --> 00:48:33,990
monitor your usage patterns,

1404
00:48:33,990 --> 00:48:36,783
and then optimize based on
your actual requirements.

1405
00:48:39,090 --> 00:48:41,610
So next up, integrating AI capabilities

1406
00:48:41,610 --> 00:48:43,020
into their serverless architectures.

1407
00:48:43,020 --> 00:48:44,490
This is the year of 2025.

1408
00:48:44,490 --> 00:48:47,100
I don't think anybody at any
conference in IT in the world

1409
00:48:47,100 --> 00:48:49,470
is unable to talk about gen AI.

1410
00:48:49,470 --> 00:48:50,550
But it is an important

1411
00:48:50,550 --> 00:48:53,223
and I think an interesting
part that we can look at.

1412
00:48:54,360 --> 00:48:57,900
By this year, Amelie's 60 global
locations faced challenges

1413
00:48:57,900 --> 00:49:00,150
traditional automation
just couldn't solve.

1414
00:49:00,150 --> 00:49:02,460
If you've got 12,000 monthly inquiries,

1415
00:49:02,460 --> 00:49:04,140
that's gonna overwhelm your support team.

1416
00:49:04,140 --> 00:49:05,580
You know, if you need
to particularly handle

1417
00:49:05,580 --> 00:49:06,780
multiple languages.

1418
00:49:06,780 --> 00:49:09,990
You know, 4.3 million orders
with minimal personalization.

1419
00:49:09,990 --> 00:49:11,270
Maybe you can do some more with that,

1420
00:49:11,270 --> 00:49:13,260
so you're missing some
revenue opportunities.

1421
00:49:13,260 --> 00:49:16,140
And manual decision-making
for pricing and inventory.

1422
00:49:16,140 --> 00:49:18,810
Amelie starts to look
to AI to help with this

1423
00:49:18,810 --> 00:49:21,750
and maybe take their
business to the next level.

1424
00:49:21,750 --> 00:49:24,030
But Amelie also knows that AI is a journey

1425
00:49:24,030 --> 00:49:26,310
to also work out what's
real and what's hype.

1426
00:49:26,310 --> 00:49:28,110
And I know we're all probably
struggling here at the moment

1427
00:49:28,110 --> 00:49:29,600
trying to work out what
the best use case is

1428
00:49:29,600 --> 00:49:31,200
in your business.

1429
00:49:31,200 --> 00:49:33,390
And, of course, all of AI could
be a whole nother session.

1430
00:49:33,390 --> 00:49:35,970
But I'm pretty sure here
at re:Invent this year,

1431
00:49:35,970 --> 00:49:38,100
there are gonna be a few
other gen AI sessions

1432
00:49:38,100 --> 00:49:39,450
to help with whatever gen AI things

1433
00:49:39,450 --> 00:49:41,610
you're gonna be looking at.

1434
00:49:41,610 --> 00:49:43,830
So first thing they realized
in the investigations

1435
00:49:43,830 --> 00:49:46,230
is that gen AI is just another workload.

1436
00:49:46,230 --> 00:49:47,610
The same rules apply.

1437
00:49:47,610 --> 00:49:48,443
What do you need to do?

1438
00:49:48,443 --> 00:49:49,470
You need to protect endpoints.

1439
00:49:49,470 --> 00:49:50,850
You need to think about quotas.

1440
00:49:50,850 --> 00:49:52,110
You need to handle security.

1441
00:49:52,110 --> 00:49:53,730
You need to think about performance.

1442
00:49:53,730 --> 00:49:57,513
Everything you know and love
about serverless still applies.

1443
00:49:58,650 --> 00:50:00,270
There's less infrastructure to manage,

1444
00:50:00,270 --> 00:50:01,620
variable compute demands.

1445
00:50:01,620 --> 00:50:03,120
Pay per use are all good.

1446
00:50:03,120 --> 00:50:05,850
Using async event-driven
processing allows AI

1447
00:50:05,850 --> 00:50:07,380
to respond to business events

1448
00:50:07,380 --> 00:50:10,230
like orders, inquiries
and inventory changes.

1449
00:50:10,230 --> 00:50:12,240
Automatic scaling handles traffic spikes

1450
00:50:12,240 --> 00:50:13,770
without capacity planning.

1451
00:50:13,770 --> 00:50:16,500
And your existing serverless
skills transfer directly.

1452
00:50:16,500 --> 00:50:17,940
Event-driven architectures,

1453
00:50:17,940 --> 00:50:20,853
cost optimization, and observability.

1454
00:50:21,900 --> 00:50:24,000
And Bedrock, as an example,
is a serverless service.

1455
00:50:24,000 --> 00:50:26,460
You know, the pricing is
based on input/output tokens,

1456
00:50:26,460 --> 00:50:28,156
automatic scaling to demand

1457
00:50:28,156 --> 00:50:31,110
with enterprise security
and compliance built in.

1458
00:50:31,110 --> 00:50:33,420
And in fact, if you
look around the industry

1459
00:50:33,420 --> 00:50:34,530
at the gen AI landscape,

1460
00:50:34,530 --> 00:50:37,530
most of it is just good
old-fashioned serverless.

1461
00:50:37,530 --> 00:50:38,610
Any kind of model you're using,

1462
00:50:38,610 --> 00:50:41,340
it's all token-based
pricing, all pay per use.

1463
00:50:41,340 --> 00:50:42,780
So, yeah, serverless,

1464
00:50:42,780 --> 00:50:45,420
you know, is definitely very
applicable in the AI world.

1465
00:50:45,420 --> 00:50:47,310
And, of course, you can integrate Bedrock

1466
00:50:47,310 --> 00:50:49,470
with all other serverless services.

1467
00:50:49,470 --> 00:50:51,180
For compute, there are
plenty of different options

1468
00:50:51,180 --> 00:50:52,680
and Step Functions is great

1469
00:50:52,680 --> 00:50:55,770
if you wanna do some complex
multi-step AI workflows.

1470
00:50:55,770 --> 00:50:58,740
Event routing and response
distribution are needed

1471
00:50:58,740 --> 00:51:00,510
with EventBridge to help.

1472
00:51:00,510 --> 00:51:02,580
And DynamoDB and S3 can also be used

1473
00:51:02,580 --> 00:51:05,760
for storage of many, many things.

1474
00:51:05,760 --> 00:51:07,260
And then we wanna understand

1475
00:51:07,260 --> 00:51:09,480
the sort of main two parts
with AI that are relevant:

1476
00:51:09,480 --> 00:51:13,350
agents, the orchestrators,
and tools as the executors.

1477
00:51:13,350 --> 00:51:15,660
Agents run in an orchestrated loop

1478
00:51:15,660 --> 00:51:18,240
to do something you've
basically told it to do.

1479
00:51:18,240 --> 00:51:19,710
It calls the tools to do the things,

1480
00:51:19,710 --> 00:51:21,300
then figures out what the next stage is

1481
00:51:21,300 --> 00:51:22,620
to satisfy your request.

1482
00:51:22,620 --> 00:51:24,030
And they're often long-running,

1483
00:51:24,030 --> 00:51:26,670
needing to maintain state, the agents.

1484
00:51:26,670 --> 00:51:27,840
And so we could decide

1485
00:51:27,840 --> 00:51:29,400
whether agents are short or long-running

1486
00:51:29,400 --> 00:51:31,080
and pick the best serverless
service to do this,

1487
00:51:31,080 --> 00:51:33,360
whether that's gonna be Lambda or Fargate.

1488
00:51:33,360 --> 00:51:34,680
But what do we actually learn

1489
00:51:34,680 --> 00:51:36,210
from a decade worth of serverless

1490
00:51:36,210 --> 00:51:39,630
is rather use a service that
is purpose built for the job.

1491
00:51:39,630 --> 00:51:42,300
And so Bedrock AgentCore
is built for running agents

1492
00:51:42,300 --> 00:51:44,520
and it abstracts a lot
of the heavy lifting

1493
00:51:44,520 --> 00:51:47,100
so you can head up the
stack for a managed service

1494
00:51:47,100 --> 00:51:48,303
the true serverless way.

1495
00:51:49,320 --> 00:51:50,910
AgentCore's got a bunch of capabilities,

1496
00:51:50,910 --> 00:51:52,200
a secure managed runtime

1497
00:51:52,200 --> 00:51:54,420
to run your long-running
agents with pay per use.

1498
00:51:54,420 --> 00:51:55,440
And that actually runs

1499
00:51:55,440 --> 00:51:57,390
a Lambda Firecracker instance for you,

1500
00:51:57,390 --> 00:51:59,250
all abstracted and behind the scenes.

1501
00:51:59,250 --> 00:52:00,510
So we take in the learnings

1502
00:52:00,510 --> 00:52:02,430
from other serverless services like Lambda

1503
00:52:02,430 --> 00:52:04,140
and applying them to more things.

1504
00:52:04,140 --> 00:52:07,140
An MCP gateway to connect
to existing APIs and tools,

1505
00:52:07,140 --> 00:52:10,830
conversation memory, inbound
and outbound identity.

1506
00:52:10,830 --> 00:52:13,200
AgentCore is also framework
and protocol agnostic.

1507
00:52:13,200 --> 00:52:15,240
And the cool thing is it's
a bunch of capabilities

1508
00:52:15,240 --> 00:52:16,530
you don't need to build yourself.

1509
00:52:16,530 --> 00:52:17,820
And also you don't have to use them all.

1510
00:52:17,820 --> 00:52:19,020
You can just use one, use two,

1511
00:52:19,020 --> 00:52:20,880
use all of them, doesn't matter.

1512
00:52:20,880 --> 00:52:22,170
It's built very specifically

1513
00:52:22,170 --> 00:52:24,240
that you can pick and
choose the useful things

1514
00:52:24,240 --> 00:52:25,080
that you're gonna do.

1515
00:52:25,080 --> 00:52:26,580
So for identity, for example,

1516
00:52:26,580 --> 00:52:28,620
sure you could spin up
a DynamoDB instance,

1517
00:52:28,620 --> 00:52:30,710
store your conversation
history and manage that,

1518
00:52:30,710 --> 00:52:31,950
or you could just use Identity

1519
00:52:31,950 --> 00:52:33,510
and it's all built in with semantic search

1520
00:52:33,510 --> 00:52:35,370
as part of the package.

1521
00:52:35,370 --> 00:52:36,960
And then tools,

1522
00:52:36,960 --> 00:52:41,400
and these are what the agents
call to actually do something.

1523
00:52:41,400 --> 00:52:44,400
And the tools are actually
what you likely already have.

1524
00:52:44,400 --> 00:52:45,930
Maybe that's something behind an API

1525
00:52:45,930 --> 00:52:48,270
that reads or writes data from a database,

1526
00:52:48,270 --> 00:52:50,340
does some action, connects
to your private data,

1527
00:52:50,340 --> 00:52:53,340
something on-prem, something
within your own VPC.

1528
00:52:53,340 --> 00:52:55,800
Actually, the tools are
the deterministic part

1529
00:52:55,800 --> 00:52:56,880
within gen AI.

1530
00:52:56,880 --> 00:52:58,307
You know exactly how they work,

1531
00:52:58,307 --> 00:53:00,420
what functions they can perform,

1532
00:53:00,420 --> 00:53:03,030
and they're gonna avoiding hallucinations.

1533
00:53:03,030 --> 00:53:05,460
And in fact, I think you
actually wanna focus more

1534
00:53:05,460 --> 00:53:06,600
on the tool side.

1535
00:53:06,600 --> 00:53:08,730
Even if you don't have robust APIs yet,

1536
00:53:08,730 --> 00:53:10,950
but with MCP or something
like that in front of them,

1537
00:53:10,950 --> 00:53:12,180
you've actually got more freedom

1538
00:53:12,180 --> 00:53:15,150
to be able to experiment
as APIs are forever.

1539
00:53:15,150 --> 00:53:16,950
But because MCP abstracts them,

1540
00:53:16,950 --> 00:53:18,630
you can have things internal

1541
00:53:18,630 --> 00:53:20,190
that you can expose externally

1542
00:53:20,190 --> 00:53:21,720
without having to worry
too much about your APIs.

1543
00:53:21,720 --> 00:53:23,430
You want to worry about your APIs,

1544
00:53:23,430 --> 00:53:25,560
but having this MCP intermediary layer

1545
00:53:25,560 --> 00:53:27,270
can be super powerful.

1546
00:53:27,270 --> 00:53:29,490
And, of course, Lambda is great for tools,

1547
00:53:29,490 --> 00:53:31,350
standalone or behind an API,

1548
00:53:31,350 --> 00:53:32,490
because they've got fast startup,

1549
00:53:32,490 --> 00:53:33,840
they can connect to anything,

1550
00:53:33,840 --> 00:53:36,210
things in your VPC, things anywhere,

1551
00:53:36,210 --> 00:53:37,590
your data, your VPC,

1552
00:53:37,590 --> 00:53:38,890
and they are super secure.

1553
00:53:40,380 --> 00:53:43,170
So just two examples I've
got here, what Amelie builds.

1554
00:53:43,170 --> 00:53:44,700
A new customer service AI agent

1555
00:53:44,700 --> 00:53:46,830
to integrate with existing functionality.

1556
00:53:46,830 --> 00:53:49,807
The customer inquiry goes
into an agent using MCP.

1557
00:53:49,807 --> 00:53:53,520
AgentCore manages the AI
agent runtime with Bedrock

1558
00:53:53,520 --> 00:53:56,400
while using the Gateway to connect to APIs

1559
00:53:56,400 --> 00:54:01,400
and then to connect to Lambda
functions as status tools.

1560
00:54:02,065 --> 00:54:04,170
So AgentCore is gonna
handle the orchestration,

1561
00:54:04,170 --> 00:54:05,130
the state management,

1562
00:54:05,130 --> 00:54:06,990
and the observability automatically

1563
00:54:06,990 --> 00:54:08,850
and the team could then focus

1564
00:54:08,850 --> 00:54:10,230
on building the actual tools

1565
00:54:10,230 --> 00:54:13,890
and not having to worry too
much about building the agents.

1566
00:54:13,890 --> 00:54:17,550
Another intelligent recommendation
was a good idea they had.

1567
00:54:17,550 --> 00:54:19,290
This is based on an order created event

1568
00:54:19,290 --> 00:54:21,023
which spurs EventBridge into action.

1569
00:54:21,023 --> 00:54:23,040
Lambda processes context.

1570
00:54:23,040 --> 00:54:25,350
Bedrock is then gonna
provide some AI analysis

1571
00:54:25,350 --> 00:54:27,660
and DynamoDB is gonna store the profiles.

1572
00:54:27,660 --> 00:54:30,750
Recommendations can adapt during sessions.

1573
00:54:30,750 --> 00:54:32,880
So this is basically when
someone does a payment,

1574
00:54:32,880 --> 00:54:34,737
it's gonna pop up, "Ooh, do
you also want to buy this?"

1575
00:54:34,737 --> 00:54:37,890
And so it can use time
or weather or location

1576
00:54:37,890 --> 00:54:39,990
or cross-location insights.

1577
00:54:39,990 --> 00:54:41,670
Contextual, personalized upselling

1578
00:54:41,670 --> 00:54:44,553
is really good for customers
and also for their business.

1579
00:54:45,761 --> 00:54:48,510
And so far, Amelie's AI's
results are a good start

1580
00:54:48,510 --> 00:54:49,500
and they are welcome.

1581
00:54:49,500 --> 00:54:51,330
70% customer service automation

1582
00:54:51,330 --> 00:54:53,250
using AgentCore and Lambda tools.

1583
00:54:53,250 --> 00:54:55,530
12% revenue growth by
using their upselling

1584
00:54:55,530 --> 00:54:56,580
with what's in your cart.

1585
00:54:56,580 --> 00:54:58,590
And also a faster development,

1586
00:54:58,590 --> 00:54:59,820
you know, focusing on the tools

1587
00:54:59,820 --> 00:55:00,990
and not on the infrastructure,

1588
00:55:00,990 --> 00:55:02,400
and cost efficiency,

1589
00:55:02,400 --> 00:55:04,470
pay per use for the agents and the tools.

1590
00:55:04,470 --> 00:55:06,041
And haven't got time as well,

1591
00:55:06,041 --> 00:55:09,180
but there's not even including
the AI coding assistance

1592
00:55:09,180 --> 00:55:10,710
that the team is taking on

1593
00:55:10,710 --> 00:55:12,660
using Kiro for spectrum development,

1594
00:55:12,660 --> 00:55:14,100
which is literally speeding up everything

1595
00:55:14,100 --> 00:55:16,620
and building really good
production-grade applications,

1596
00:55:16,620 --> 00:55:18,720
and then using the serverless MCP server,

1597
00:55:18,720 --> 00:55:20,250
which you can connect into Kiro

1598
00:55:20,250 --> 00:55:21,930
and, in fact, any MCP client,

1599
00:55:21,930 --> 00:55:24,360
and this really uplifts your
serverless best practices.

1600
00:55:24,360 --> 00:55:25,440
You can use natural language.

1601
00:55:25,440 --> 00:55:27,030
You can query about MCPs.

1602
00:55:27,030 --> 00:55:28,907
You can query about what
infrastructures code tools,

1603
00:55:28,907 --> 00:55:31,260
Lambda optimizations, Step Functions,

1604
00:55:31,260 --> 00:55:33,660
all these kind of things
built into that MCP server.

1605
00:55:33,660 --> 00:55:35,373
I really suggest you take a look.

1606
00:55:36,990 --> 00:55:41,640
So after all that, I'd said
I'd cover a lot and I did.

1607
00:55:41,640 --> 00:55:43,710
Let's recap to see what
Amelie's journey revealed.

1608
00:55:43,710 --> 00:55:44,610
Is everybody okay?

1609
00:55:44,610 --> 00:55:46,110
You're hanging in there? Good.

1610
00:55:46,950 --> 00:55:48,210
Serverless works at any scale,

1611
00:55:48,210 --> 00:55:50,440
from 40 orders to 320,000.

1612
00:55:51,481 --> 00:55:53,100
Serverless architectures had to evolve

1613
00:55:53,100 --> 00:55:54,300
to support this growth.

1614
00:55:54,300 --> 00:55:57,180
But serverless, of course, was
the best way to achieve this.

1615
00:55:57,180 --> 00:55:58,470
We had some core principles.

1616
00:55:58,470 --> 00:56:00,270
Here's the QR code again
for the presentation

1617
00:56:00,270 --> 00:56:01,980
and more handy jumping off links.

1618
00:56:01,980 --> 00:56:03,930
Right-sizing, matching the function repo

1619
00:56:03,930 --> 00:56:05,700
and stack size to your workload,

1620
00:56:05,700 --> 00:56:06,990
async over sync,

1621
00:56:06,990 --> 00:56:08,490
EventBridge and queues for resiliency

1622
00:56:08,490 --> 00:56:10,530
with still faster customer updates,

1623
00:56:10,530 --> 00:56:13,470
avoiding work if you can using
direct service integrations,

1624
00:56:13,470 --> 00:56:16,590
Step Functions, filtering
early using batch operations,

1625
00:56:16,590 --> 00:56:17,790
performance optimization,

1626
00:56:17,790 --> 00:56:19,290
optimizing those cold starts,

1627
00:56:19,290 --> 00:56:21,120
function code optimization,

1628
00:56:21,120 --> 00:56:22,170
platform functionality

1629
00:56:22,170 --> 00:56:24,698
with provisioned
concurrency and SnapStart.

1630
00:56:24,698 --> 00:56:25,658
Failure handling.

1631
00:56:25,658 --> 00:56:27,210
We've got retries, deal queues,

1632
00:56:27,210 --> 00:56:29,732
circuit breakers, a whole bunch of things.

1633
00:56:29,732 --> 00:56:31,710
We've got observability, logs,

1634
00:56:31,710 --> 00:56:33,780
metrics, signals with business context

1635
00:56:33,780 --> 00:56:35,580
and lots of new CloudWatch goodness,

1636
00:56:35,580 --> 00:56:36,413
and cost.

1637
00:56:36,413 --> 00:56:39,180
Being architecture, every
decision has a price tag.

1638
00:56:39,180 --> 00:56:41,494
And gen AI, just another workload.

1639
00:56:41,494 --> 00:56:43,650
Your serverless skills apply.

1640
00:56:43,650 --> 00:56:46,383
And these principles
also apply at any scale.

1641
00:56:47,340 --> 00:56:48,900
So what could be your next steps?

1642
00:56:48,900 --> 00:56:51,120
Well, this week, maybe next week,

1643
00:56:51,120 --> 00:56:52,620
I think you're gonna be
broken from re:Invent,

1644
00:56:52,620 --> 00:56:54,960
but pick one Lambda function to optimize.

1645
00:56:54,960 --> 00:56:56,070
Add some structured logging,

1646
00:56:56,070 --> 00:56:57,750
including some business context.

1647
00:56:57,750 --> 00:57:00,840
Implement proper error
handling using retries and DLQs

1648
00:57:00,840 --> 00:57:02,220
if using SQS.

1649
00:57:02,220 --> 00:57:03,300
Check function sizing.

1650
00:57:03,300 --> 00:57:05,190
Make sure you correctly size your memory.

1651
00:57:05,190 --> 00:57:08,430
And use EventBridge,
not direct invocations.

1652
00:57:08,430 --> 00:57:11,070
And then once you start there,
you can build some momentum.

1653
00:57:11,070 --> 00:57:13,800
Migrate one sync workload to async.

1654
00:57:13,800 --> 00:57:16,680
Build robust workflows
considering Step Functions

1655
00:57:16,680 --> 00:57:18,600
or Lambda durable functions.

1656
00:57:18,600 --> 00:57:19,830
Implement some event filtering,

1657
00:57:19,830 --> 00:57:21,060
save yourself some money.

1658
00:57:21,060 --> 00:57:22,740
Review and optimize those costs.

1659
00:57:22,740 --> 00:57:24,390
And then also document your patterns

1660
00:57:24,390 --> 00:57:26,240
to be able to scale out your efforts.

1661
00:57:27,600 --> 00:57:29,532
There are lots of other
sessions going on this week.

1662
00:57:29,532 --> 00:57:30,960
Lambda managed instance

1663
00:57:30,960 --> 00:57:33,420
and durable functions I
mentioned in the talk as well.

1664
00:57:33,420 --> 00:57:34,860
There's also a leadership session,

1665
00:57:34,860 --> 00:57:37,050
Building the future with AWS Serverless.

1666
00:57:37,050 --> 00:57:39,360
And a shameless plug,
I'll be back tomorrow

1667
00:57:39,360 --> 00:57:41,130
with a Lambda principal engineer

1668
00:57:41,130 --> 00:57:42,660
and we are gonna be talking in detail

1669
00:57:42,660 --> 00:57:44,670
all about how events
travel through Lambda,

1670
00:57:44,670 --> 00:57:45,720
specifically with polling,

1671
00:57:45,720 --> 00:57:48,030
and that's gonna be back here tomorrow.

1672
00:57:48,030 --> 00:57:50,640
This link also provides a
bunch of additional information

1673
00:57:50,640 --> 00:57:52,200
to continuous serverless learning.

1674
00:57:52,200 --> 00:57:53,670
It's got links to Powertools,

1675
00:57:53,670 --> 00:57:54,900
a bunch of announcements,

1676
00:57:54,900 --> 00:57:55,950
Serverless Land patterns.

1677
00:57:55,950 --> 00:57:56,850
If you've ever used that,

1678
00:57:56,850 --> 00:57:58,320
that is infrastructure as code

1679
00:57:58,320 --> 00:58:00,840
and function code examples

1680
00:58:00,840 --> 00:58:02,670
for, you know, all the serverless services

1681
00:58:02,670 --> 00:58:03,630
and you just be able to do them

1682
00:58:03,630 --> 00:58:04,800
and a whole bunch of them you can use

1683
00:58:04,800 --> 00:58:06,210
directly in your ID as well.

1684
00:58:06,210 --> 00:58:07,200
Super useful.

1685
00:58:07,200 --> 00:58:09,990
Serverlessland.com as
well is the best website

1686
00:58:09,990 --> 00:58:12,360
to keep up to date with all
things serverless on AWS.

1687
00:58:12,360 --> 00:58:15,163
Lots of cool stuff that
you can read over there.

1688
00:58:15,163 --> 00:58:18,270
But most importantly, thank
you for joining me today.

1689
00:58:18,270 --> 00:58:20,430
I appreciate you taking out the time

1690
00:58:20,430 --> 00:58:21,690
if you've had to move across Vegas

1691
00:58:21,690 --> 00:58:23,010
and come to another venue.

1692
00:58:23,010 --> 00:58:23,970
Thank you so much.

1693
00:58:23,970 --> 00:58:25,350
I also really appreciate

1694
00:58:25,350 --> 00:58:27,350
if you could fill in the session survey.

1695
00:58:28,710 --> 00:58:29,543
If you're hungry

1696
00:58:29,543 --> 00:58:32,310
for, you know, more deep
technical contents, more broad,

1697
00:58:32,310 --> 00:58:33,143
and more kind of things,

1698
00:58:33,143 --> 00:58:34,367
this really lets us know

1699
00:58:34,367 --> 00:58:36,480
the kind of things you'd be interested in.

1700
00:58:36,480 --> 00:58:38,678
So I would appreciate, of course,

1701
00:58:38,678 --> 00:58:40,080
a nice five in that as well,

1702
00:58:40,080 --> 00:58:41,700
but also be honest with what you think.

1703
00:58:41,700 --> 00:58:42,660
So thank you really much.

1704
00:58:42,660 --> 00:58:44,550
I hope you enjoy the rest of
your day here at re:Invent.

1705
00:58:44,550 --> 00:58:47,670
Plenty more to learn, plenty
more people to connect to.

1706
00:58:47,670 --> 00:58:50,250
I encourage you to use
the hallway track as well.

1707
00:58:50,250 --> 00:58:51,540
Sit down with somebody at lunch

1708
00:58:51,540 --> 00:58:54,510
and find out and talk
about your kind of things,

1709
00:58:54,510 --> 00:58:55,456
talk about serverless,

1710
00:58:55,456 --> 00:58:56,730
and enjoy the rest of your week.

1711
00:58:56,730 --> 00:58:57,708
Thank you very much.

1712
00:58:57,708 --> 00:59:00,708
(audience applauds)

