1
00:00:00,180 --> 00:00:03,720
- Well, thanks for coming to
Hallucination Detection Live

2
00:00:03,720 --> 00:00:05,790
with LaunchDarkly AI configs.

3
00:00:05,790 --> 00:00:06,990
I'm Scarlett Attensil.

4
00:00:06,990 --> 00:00:08,850
I'm a senior developer educator

5
00:00:08,850 --> 00:00:11,853
with LaunchDarkly, and
I support AI configs.

6
00:00:12,690 --> 00:00:14,337
- And hi, my name is Marek Poliks.

7
00:00:14,337 --> 00:00:15,777
I'm the Head of AI at LaunchDarkly

8
00:00:15,777 --> 00:00:18,091
and the product owner of this thing

9
00:00:18,091 --> 00:00:19,890
that we're gonna talk about here today.

10
00:00:19,890 --> 00:00:22,500
Would really, really love
to, you know, afterwards,

11
00:00:22,500 --> 00:00:24,120
meet you in the hall,
have a little bit more

12
00:00:24,120 --> 00:00:26,670
of a conversation, not only
about the present state

13
00:00:26,670 --> 00:00:28,440
of what we're gonna show you,
but also about the future,

14
00:00:28,440 --> 00:00:32,070
because that's something
I very much care about.

15
00:00:32,070 --> 00:00:32,903
Let's get going.

16
00:00:32,903 --> 00:00:34,110
- All right.

17
00:00:34,110 --> 00:00:37,290
- So, I mean, that's obvious, right?

18
00:00:37,290 --> 00:00:39,690
Like, I mean, we all know
that this is the case.

19
00:00:39,690 --> 00:00:41,430
I would like to think of it

20
00:00:41,430 --> 00:00:43,410
as almost something like a truism.

21
00:00:43,410 --> 00:00:45,840
So, I spent a lot of time
talking to customers,

22
00:00:45,840 --> 00:00:47,400
hundreds and hundreds of customers,

23
00:00:47,400 --> 00:00:49,710
folks that are generating,
that are producing

24
00:00:49,710 --> 00:00:51,750
some kind of generative AI products.

25
00:00:51,750 --> 00:00:53,850
I love to ask them,
like, "What's the thing?"

26
00:00:53,850 --> 00:00:55,620
Like, "What's the thing
that's stopping you

27
00:00:55,620 --> 00:00:58,440
from moving from pre-production
all the way into production,

28
00:00:58,440 --> 00:01:00,600
and then ultimately staying in production

29
00:01:00,600 --> 00:01:02,310
sustainably in some way?"

30
00:01:02,310 --> 00:01:04,740
I like to take all that
data back, collate it,

31
00:01:04,740 --> 00:01:07,260
come back to my team and say,
"Here are the top five things

32
00:01:07,260 --> 00:01:09,330
that are keeping people
from actually moving

33
00:01:09,330 --> 00:01:10,680
their gen-AI products forward.

34
00:01:10,680 --> 00:01:13,230
And the top five is
hallucinations, hallucinations,

35
00:01:13,230 --> 00:01:15,780
hallucinations, hallucinations,
hallucinations, right?

36
00:01:15,780 --> 00:01:18,810
Hallucinations are really,
really problematic.

37
00:01:18,810 --> 00:01:21,540
They cost money, they
cost engineering time.

38
00:01:21,540 --> 00:01:24,480
They cost confidence in
the underlying reliability

39
00:01:24,480 --> 00:01:26,280
of the thing that you're trying to build.

40
00:01:26,280 --> 00:01:28,290
They also cost brand equity, right?

41
00:01:28,290 --> 00:01:31,200
They cost the kind of
visibility of your brand

42
00:01:31,200 --> 00:01:32,310
in a way that's really problematic

43
00:01:32,310 --> 00:01:33,720
and can be frustrating.

44
00:01:33,720 --> 00:01:36,060
But I think this is really critical.

45
00:01:36,060 --> 00:01:38,670
Like, it's not like we're
going to solve this problem

46
00:01:38,670 --> 00:01:41,100
necessarily through the
brute-force application

47
00:01:41,100 --> 00:01:44,310
of continuous reinforcement
learning from human feedback.

48
00:01:44,310 --> 00:01:46,530
You cannot solve the
problem of hallucinations

49
00:01:46,530 --> 00:01:48,900
because LLMS do nothing but hallucinate;

50
00:01:48,900 --> 00:01:51,270
that is ontologically what
it is that they are doing.

51
00:01:51,270 --> 00:01:54,330
That's what the generative
in generative AI

52
00:01:54,330 --> 00:01:56,580
functionally means; it
means hallucination.

53
00:01:56,580 --> 00:01:59,040
And moreover, what's so
interesting about hallucinations

54
00:01:59,040 --> 00:02:01,140
is that they're ultimately the thing

55
00:02:01,140 --> 00:02:03,870
that allow generative
AI systems to be able

56
00:02:03,870 --> 00:02:07,710
to deal with the raw
randomness and contingency

57
00:02:07,710 --> 00:02:08,880
of our world.

58
00:02:08,880 --> 00:02:10,860
They're what allow LLMS to deal

59
00:02:10,860 --> 00:02:12,990
with general questions, right,

60
00:02:12,990 --> 00:02:15,330
to be able to deal with unstructured data.

61
00:02:15,330 --> 00:02:17,610
So, it's not that we
necessarily need to solve

62
00:02:17,610 --> 00:02:20,580
for hallucinations within
the actual model itself.

63
00:02:20,580 --> 00:02:23,040
We don't want to drive models
away from their ability

64
00:02:23,040 --> 00:02:24,720
to handle generic conditions.

65
00:02:24,720 --> 00:02:28,290
Rather, we wanna build
systems that are tolerant

66
00:02:28,290 --> 00:02:30,060
to lapses in accuracy.

67
00:02:30,060 --> 00:02:31,140
We wanna build systems

68
00:02:31,140 --> 00:02:34,380
that can automatically heal
from any kind of interruption

69
00:02:34,380 --> 00:02:36,150
of service provision.

70
00:02:36,150 --> 00:02:38,460
We wanna build systems that are capable

71
00:02:38,460 --> 00:02:41,070
of handling accuracy-related exceptions,

72
00:02:41,070 --> 00:02:43,050
preventing them from
ultimately hitting a customer,

73
00:02:43,050 --> 00:02:43,950
and then roll back

74
00:02:43,950 --> 00:02:46,410
into some kind of self-healing
architecture that allows you

75
00:02:46,410 --> 00:02:49,260
to iteratively improve and
prevent that particular type

76
00:02:49,260 --> 00:02:50,910
of error from happening again.

77
00:02:50,910 --> 00:02:54,450
So, on the subject of hallucinations,
I'll push to Scarlett.

78
00:02:54,450 --> 00:02:55,283
- Yeah.

79
00:02:55,283 --> 00:02:57,990
So, even when you think
you nailed it, right,

80
00:02:57,990 --> 00:03:01,230
when your AI is thoroughly
tuned and tested,

81
00:03:01,230 --> 00:03:03,570
there's this problem with model drift.

82
00:03:03,570 --> 00:03:06,150
So, by now, many of you have
probably heard about this study

83
00:03:06,150 --> 00:03:09,960
that was done by Stanford
and UC Berkeley back in 2023.

84
00:03:09,960 --> 00:03:13,740
They ran the same prompt
through GPT-4 over three months.

85
00:03:13,740 --> 00:03:16,020
So, it was the same prompt, same model.

86
00:03:16,020 --> 00:03:18,030
They made zero code changes.

87
00:03:18,030 --> 00:03:20,010
And accuracy on this prime number test,

88
00:03:20,010 --> 00:03:24,233
it decreased from 97.6 down to just 2.4%.

89
00:03:25,950 --> 00:03:26,783
So.

90
00:03:26,783 --> 00:03:27,887
- [Marek] It's really bad. (laughs)

91
00:03:27,887 --> 00:03:28,980
- Right? Yeah.

92
00:03:28,980 --> 00:03:31,230
So, I mean, that's near-perfect precision

93
00:03:31,230 --> 00:03:32,550
that's been degraded

94
00:03:32,550 --> 00:03:36,213
to basically worse than random
guessing in just 90 days.

95
00:03:38,010 --> 00:03:40,590
But this model drift,
it's not really a bug,

96
00:03:40,590 --> 00:03:43,023
as Mark was saying, it's
just the nature of AI,

97
00:03:45,030 --> 00:03:48,420
which means that continuous
monitoring of your AI systems,

98
00:03:48,420 --> 00:03:51,750
it's not optional for teams
who are using AI in production,

99
00:03:51,750 --> 00:03:53,073
it's existential.

100
00:03:54,630 --> 00:03:59,630
So, we all know about the
pressure to move fast with AI,

101
00:03:59,640 --> 00:04:01,170
but there's this particular problem

102
00:04:01,170 --> 00:04:04,710
that makes balancing
speed and safety so hard.

103
00:04:04,710 --> 00:04:08,100
So, raise your hands if you
worry about your AI's accuracy.

104
00:04:08,100 --> 00:04:10,333
- [Marek] Come on, yeah, there we go.

105
00:04:10,333 --> 00:04:11,166
- Right?

106
00:04:11,166 --> 00:04:13,470
So, this cycle, it happens
on multiple levels,

107
00:04:13,470 --> 00:04:16,800
and it happens because
of two fundamental gaps.

108
00:04:16,800 --> 00:04:21,060
First, you have a lack of insight, right?

109
00:04:21,060 --> 00:04:22,980
You can't tell when or why a model

110
00:04:22,980 --> 00:04:25,020
is making a particular decision.

111
00:04:25,020 --> 00:04:28,920
Like, for example, we're
gonna show you a chat bot

112
00:04:28,920 --> 00:04:32,250
that is intended to
answer questions for you

113
00:04:32,250 --> 00:04:33,960
about your health insurance.

114
00:04:33,960 --> 00:04:36,090
And I can't tell when or why that chat bot

115
00:04:36,090 --> 00:04:38,880
might invent section
7.3 of a policy document

116
00:04:38,880 --> 00:04:40,500
that doesn't exist.

117
00:04:40,500 --> 00:04:42,900
And second, you have a lack of control.

118
00:04:42,900 --> 00:04:47,790
So, there's no way to change your AI

119
00:04:47,790 --> 00:04:50,160
without pushing out a full deployment.

120
00:04:50,160 --> 00:04:54,273
You can't try different models
for different user segments,

121
00:04:55,260 --> 00:04:57,723
and fixes often require multiple teams.

122
00:04:59,010 --> 00:05:01,440
So, this is the post-deploy problem,

123
00:05:01,440 --> 00:05:04,980
and this is the problem that
LaunchDarkly AI Config solves.

124
00:05:04,980 --> 00:05:06,600
So, when your features degrade,

125
00:05:06,600 --> 00:05:08,460
how do you find out about it?

126
00:05:08,460 --> 00:05:09,720
As I was saying, these failures,

127
00:05:09,720 --> 00:05:12,840
they occur on three levels,
and I wanna talk to that now.

128
00:05:12,840 --> 00:05:14,880
The first level is this model layer.

129
00:05:14,880 --> 00:05:17,130
So, this is your
provider-controlled territory.

130
00:05:17,130 --> 00:05:19,590
You can think OpenAI, Anthropic,

131
00:05:19,590 --> 00:05:23,010
these model providers push
updates without warning,

132
00:05:23,010 --> 00:05:26,310
and now your perfectly tuned
prompt starts hallucinating.

133
00:05:26,310 --> 00:05:29,040
The next layer we have
is your knowledge layer.

134
00:05:29,040 --> 00:05:31,590
So, for the example we're
gonna walk you through,

135
00:05:31,590 --> 00:05:33,570
for our healthcare chat bot,

136
00:05:33,570 --> 00:05:35,640
we might have a provider directory

137
00:05:35,640 --> 00:05:38,130
that's changing every month.

138
00:05:38,130 --> 00:05:41,460
Maybe our state regulations
could change quarterly.

139
00:05:41,460 --> 00:05:44,370
And if our knowledge base stays static,

140
00:05:44,370 --> 00:05:46,820
then we're compounding
our risk every single day.

141
00:05:47,820 --> 00:05:49,980
And finally, there's the user layer.

142
00:05:49,980 --> 00:05:54,270
So, user query patterns
evolve organically over time.

143
00:05:54,270 --> 00:05:55,980
During month one, your customers

144
00:05:55,980 --> 00:05:57,870
might be still getting
used to your chat bot,

145
00:05:57,870 --> 00:05:59,280
still getting comfortable with it.

146
00:05:59,280 --> 00:06:00,810
They're asking simple questions.

147
00:06:00,810 --> 00:06:02,130
In our example, maybe it's something

148
00:06:02,130 --> 00:06:05,700
like, "What is my dental coverage?"

149
00:06:05,700 --> 00:06:08,610
But by month six, they're
totally comfortable dropping in

150
00:06:08,610 --> 00:06:10,740
their 2000-word medical histories

151
00:06:10,740 --> 00:06:13,443
and expecting a detailed
coverage analysis.

152
00:06:15,000 --> 00:06:17,460
These failures, though,
they're not random,

153
00:06:17,460 --> 00:06:20,040
they actually occur in clusters.

154
00:06:20,040 --> 00:06:22,890
But the key is you can't
just watch aggregate output.

155
00:06:22,890 --> 00:06:25,890
You have to add per-node accountability,

156
00:06:25,890 --> 00:06:27,450
and then you have to treat observability

157
00:06:27,450 --> 00:06:29,103
like it's a lagging indicator.

158
00:06:30,330 --> 00:06:32,790
You wanna inject evaluators in step,

159
00:06:32,790 --> 00:06:34,831
so they can monitor your agents

160
00:06:34,831 --> 00:06:37,470
and catch issues as they arise,

161
00:06:37,470 --> 00:06:39,393
not after customers are affected.

162
00:06:41,370 --> 00:06:42,810
So, now I wanna talk to you

163
00:06:42,810 --> 00:06:46,650
about why the traditional
principles that we use,

164
00:06:46,650 --> 00:06:48,900
that we rely on for software engineering

165
00:06:48,900 --> 00:06:51,540
often fail when it comes to AI.

166
00:06:51,540 --> 00:06:54,963
So, the deterministic code is predictable.

167
00:06:56,212 --> 00:06:57,090
Okay.

168
00:06:57,090 --> 00:06:58,230
Thank you. Go ahead.

169
00:06:58,230 --> 00:06:59,063
Is it this one?

170
00:06:59,063 --> 00:06:59,896
- [Marek] Yeah.

171
00:06:59,896 --> 00:07:01,380
- Okay, so it's predictable, right?

172
00:07:01,380 --> 00:07:03,510
So, if you call a function
with the same parameters,

173
00:07:03,510 --> 00:07:05,310
you know you're gonna
get identical results

174
00:07:05,310 --> 00:07:06,720
every single time.

175
00:07:06,720 --> 00:07:09,570
You can test it once, and
you can trust it forever.

176
00:07:09,570 --> 00:07:12,750
Traditional code is also reproducible.

177
00:07:12,750 --> 00:07:15,540
So, you've got your
gate commits, rollbacks,

178
00:07:15,540 --> 00:07:18,180
version control, equals behavior control.

179
00:07:18,180 --> 00:07:19,620
I know what code is running,

180
00:07:19,620 --> 00:07:22,020
when it changed, and who changed it.

181
00:07:22,020 --> 00:07:25,710
And this means I can predict
how that code's gonna behave.

182
00:07:25,710 --> 00:07:27,480
Finally, it's traceable.

183
00:07:27,480 --> 00:07:29,670
If there's a bug, you can
walk through your code,

184
00:07:29,670 --> 00:07:32,010
you can figure out exactly
where the error is occurring

185
00:07:32,010 --> 00:07:33,630
and fix the problem.

186
00:07:33,630 --> 00:07:37,980
And these principles work great
for deterministic systems,

187
00:07:37,980 --> 00:07:40,083
but as you know, AI is stochastic.

188
00:07:41,190 --> 00:07:42,720
So, it's dynamic, right?

189
00:07:42,720 --> 00:07:45,270
You can't unit-test
creativity or reasoning;

190
00:07:45,270 --> 00:07:48,180
you have to use an entirely
different testing methodology.

191
00:07:48,180 --> 00:07:52,110
Like, there's no assertion
for, "Sounds empathetic"

192
00:07:52,110 --> 00:07:56,580
or "Explains clearly;" it's evolving.

193
00:07:56,580 --> 00:07:58,590
So, we can pin to a specific model,

194
00:07:58,590 --> 00:08:00,540
but as we just saw, that doesn't mean

195
00:08:00,540 --> 00:08:02,670
it's gonna behave the same way today

196
00:08:02,670 --> 00:08:04,083
as it did three months ago.

197
00:08:05,850 --> 00:08:07,170
And finally, it's opaque.

198
00:08:07,170 --> 00:08:11,310
Our solutions are composed of
black boxes chained together,

199
00:08:11,310 --> 00:08:12,540
and we have no way to tell

200
00:08:12,540 --> 00:08:15,240
when or why these models
made the decisions they made.

201
00:08:16,320 --> 00:08:18,270
We're not just shipping code anymore,

202
00:08:18,270 --> 00:08:20,340
we're shipping statistical models,

203
00:08:20,340 --> 00:08:22,920
and deterministic
deployment strategies fail

204
00:08:22,920 --> 00:08:24,723
for these probabilistic systems.

205
00:08:25,800 --> 00:08:29,160
That's when LaunchDarkly
AI configs comes in.

206
00:08:29,160 --> 00:08:31,290
So, Marek's gonna walk
you through the demo now.

207
00:08:31,290 --> 00:08:32,123
- Yeah, absolutely.

208
00:08:32,123 --> 00:08:34,020
I'm gonna contextualize
the demo a little bit first

209
00:08:34,020 --> 00:08:35,040
before I actually show it to you,

210
00:08:35,040 --> 00:08:37,920
and I'll give you also a little
bit of a diagram to show you

211
00:08:37,920 --> 00:08:40,020
materially what it is that I'm doing.

212
00:08:40,020 --> 00:08:42,870
We're gonna do three different
types of demos that highlight

213
00:08:42,870 --> 00:08:45,270
what I think is ultimately
an integrated circuit;

214
00:08:45,270 --> 00:08:46,890
one that's ultimately
trying to solve the problem

215
00:08:46,890 --> 00:08:48,450
of how do you build a system

216
00:08:48,450 --> 00:08:52,140
that's constantly self-optimizing
for accuracy in real time,

217
00:08:52,140 --> 00:08:53,760
like really in the moment,

218
00:08:53,760 --> 00:08:55,440
before you see any kind of slip ups

219
00:08:55,440 --> 00:08:57,120
in a customer-facing way.

220
00:08:57,120 --> 00:09:00,270
So, the three kind of
components of this mechanism,

221
00:09:00,270 --> 00:09:01,737
the first is real-time control.

222
00:09:01,737 --> 00:09:04,710
And I think this is
incredibly, incredibly special

223
00:09:04,710 --> 00:09:07,500
in terms of what it is
that we specifically offer.

224
00:09:07,500 --> 00:09:10,080
This is basically the ability
to modulate, to change,

225
00:09:10,080 --> 00:09:12,570
to alter the constituent components

226
00:09:12,570 --> 00:09:15,030
of an AI agent in real
time without ever needing

227
00:09:15,030 --> 00:09:17,190
to redeploy underlying application.

228
00:09:17,190 --> 00:09:19,580
This is useful naively in pre-production,

229
00:09:19,580 --> 00:09:21,690
in that, like, you need the
ability to create a variety

230
00:09:21,690 --> 00:09:24,545
of different variations of
an agent, a test, permute,

231
00:09:24,545 --> 00:09:26,610
look at validate, et cetera.

232
00:09:26,610 --> 00:09:28,650
But it's extremely also
useful in production,

233
00:09:28,650 --> 00:09:30,480
where something bad might happen.

234
00:09:30,480 --> 00:09:32,880
And you need the ability
to quickly alter the state

235
00:09:32,880 --> 00:09:36,150
of some agent in a production
context, based on feedback.

236
00:09:36,150 --> 00:09:38,820
Second thing we're gonna talk
about is experimentation.

237
00:09:38,820 --> 00:09:41,790
I really believe so, so strongly
that, when you're working

238
00:09:41,790 --> 00:09:44,040
with probabilistic systems,
you need to be working

239
00:09:44,040 --> 00:09:45,630
with probabilistic technology.

240
00:09:45,630 --> 00:09:48,150
You need to be really working
with statistics at scale

241
00:09:48,150 --> 00:09:51,630
to assess the components,
the performance of a variety

242
00:09:51,630 --> 00:09:54,000
of different agent morphologies over time.

243
00:09:54,000 --> 00:09:55,290
And then, the third bit, I'm gonna talk

244
00:09:55,290 --> 00:09:57,510
about self-healing AI,
what our perspective is

245
00:09:57,510 --> 00:09:59,550
for a self-healing agentic system,

246
00:09:59,550 --> 00:10:02,250
what it means to catch
hallucinations in production,

247
00:10:02,250 --> 00:10:04,590
to ultimately revert to a known good,

248
00:10:04,590 --> 00:10:06,210
and then to pass feedback into a system

249
00:10:06,210 --> 00:10:08,370
that allows for automatic correction.

250
00:10:08,370 --> 00:10:11,070
So, with real-time control,
I'm gonna start with this,

251
00:10:11,070 --> 00:10:13,080
and it's such an ugly diagram,

252
00:10:13,080 --> 00:10:16,710
and it also is, like, such a
thing that I'm so, so proud of.

253
00:10:16,710 --> 00:10:19,740
I wanna start maybe with,
like, a serious provocation,

254
00:10:19,740 --> 00:10:21,630
like, I think actually a
pretty serious provocation,

255
00:10:21,630 --> 00:10:25,800
which is that all the stuff
that you use to build an agent,

256
00:10:25,800 --> 00:10:28,980
to actually compose the
AI parts of an agent,

257
00:10:28,980 --> 00:10:32,430
the prompts, the tools that
you're calling, the model,

258
00:10:32,430 --> 00:10:34,110
hyper parameters, maybe even the models

259
00:10:34,110 --> 00:10:36,330
that you're invoking,
specifically the model providers,

260
00:10:36,330 --> 00:10:38,939
even the relationships or
the topology of an agent;

261
00:10:38,939 --> 00:10:40,500
I don't think that's code.

262
00:10:40,500 --> 00:10:42,540
Like, I really don't believe that's code,

263
00:10:42,540 --> 00:10:43,410
and I don't believe that's code

264
00:10:43,410 --> 00:10:44,790
for a number of different reasons,

265
00:10:44,790 --> 00:10:47,280
of the first being that
these are individual things,

266
00:10:47,280 --> 00:10:50,910
that you are likely gonna change
an order of magnitude more

267
00:10:50,910 --> 00:10:54,570
and an order of magnitude
faster, requires more dexterity

268
00:10:54,570 --> 00:10:57,270
and alacrity in order to
actually take advantage

269
00:10:57,270 --> 00:10:59,880
of this non-deterministic
relationship that you're building.

270
00:10:59,880 --> 00:11:01,680
The second reason I don't
think that it's code

271
00:11:01,680 --> 00:11:04,440
is that it requires an order
of magnitude more vigilant

272
00:11:04,440 --> 00:11:06,750
in its own kind of highly specialized way.

273
00:11:06,750 --> 00:11:08,160
And this ultimately
has to do with the fact

274
00:11:08,160 --> 00:11:10,320
that, ontologically,
these things aren't code,

275
00:11:10,320 --> 00:11:12,420
and that they're ultimately not moving

276
00:11:12,420 --> 00:11:15,180
to produce some ultimately
determined outcome,

277
00:11:15,180 --> 00:11:17,970
they're rather a series of
interpreted instructions

278
00:11:17,970 --> 00:11:19,530
by an association machine.

279
00:11:19,530 --> 00:11:21,120
And so, the provocation I wanna ask

280
00:11:21,120 --> 00:11:24,930
is, like, why would you
subject these things,

281
00:11:24,930 --> 00:11:27,900
these components to the
same infrastructures,

282
00:11:27,900 --> 00:11:29,160
to the same timescales,

283
00:11:29,160 --> 00:11:31,800
to the same rituals that
we place around code?

284
00:11:31,800 --> 00:11:33,480
No, I think that there's
something different.

285
00:11:33,480 --> 00:11:34,313
I think, actually,

286
00:11:34,313 --> 00:11:36,120
and this is obviously pretty self-serving,

287
00:11:36,120 --> 00:11:37,920
I think they start to
look actually quite a lot

288
00:11:37,920 --> 00:11:39,120
like configurations.

289
00:11:39,120 --> 00:11:41,130
And ultimately, I think they
start to look quite a lot

290
00:11:41,130 --> 00:11:42,300
like feature flags.

291
00:11:42,300 --> 00:11:44,070
So, that's something I'm
really excited about.

292
00:11:44,070 --> 00:11:45,870
I joined LaunchDarkly principally

293
00:11:45,870 --> 00:11:48,390
because LaunchDarkly is a
feature-flagging company.

294
00:11:48,390 --> 00:11:50,250
If you're unfamiliar with LaunchDarkly,

295
00:11:50,250 --> 00:11:51,960
it actually completely doesn't matter

296
00:11:51,960 --> 00:11:53,880
from this conversation,
apart from the fact

297
00:11:53,880 --> 00:11:57,510
that we serve almost 50 trillion
feature flags every day.

298
00:11:57,510 --> 00:11:59,580
Those are feature flags
that contain information

299
00:11:59,580 --> 00:12:02,010
that could be used to modulate, to change,

300
00:12:02,010 --> 00:12:04,950
to vary different experiences
of an application.

301
00:12:04,950 --> 00:12:08,100
So, then the question is, what
if you use that to modulate,

302
00:12:08,100 --> 00:12:11,610
to change, to alter the
behaviors of an AI agent?

303
00:12:11,610 --> 00:12:12,810
The way that this actually looks,

304
00:12:12,810 --> 00:12:14,100
the way this actually works here

305
00:12:14,100 --> 00:12:15,810
is that you have applications,

306
00:12:15,810 --> 00:12:17,250
those applications are running,

307
00:12:17,250 --> 00:12:19,350
those applications are making model calls,

308
00:12:19,350 --> 00:12:21,750
probably lots of different model calls.

309
00:12:21,750 --> 00:12:24,630
What happens if you,
instead of hard coding,

310
00:12:24,630 --> 00:12:27,150
all of the parameters that
goes into those model calls,

311
00:12:27,150 --> 00:12:28,260
or instead of bearing them

312
00:12:28,260 --> 00:12:30,990
in, like, large configuration
files or offloading them

313
00:12:30,990 --> 00:12:34,200
to some kind of third-party
intermediary gateway,

314
00:12:34,200 --> 00:12:35,430
what happens if you just stream it?

315
00:12:35,430 --> 00:12:38,340
What happens if you stream it
as streamable configurations

316
00:12:38,340 --> 00:12:39,960
through the form of feature flags?

317
00:12:39,960 --> 00:12:40,950
That means that you're able

318
00:12:40,950 --> 00:12:43,980
to, without ever redeploying
the underlying application,

319
00:12:43,980 --> 00:12:45,060
you can change the prompt,

320
00:12:45,060 --> 00:12:46,980
you can change the tools
that are being invoked,

321
00:12:46,980 --> 00:12:48,420
you can change the
models that you're using,

322
00:12:48,420 --> 00:12:50,460
you can change the hyper
parameters that you're calling.

323
00:12:50,460 --> 00:12:51,600
That's really useful.

324
00:12:51,600 --> 00:12:54,480
You never need to redeploy
the underlying application.

325
00:12:54,480 --> 00:12:56,130
Another bit that I think
is really important

326
00:12:56,130 --> 00:12:59,010
is that your application
knows who it's talking to.

327
00:12:59,010 --> 00:13:01,350
It's aware of the context that it's in.

328
00:13:01,350 --> 00:13:04,320
Therefore, you can
actually dynamically serve

329
00:13:04,320 --> 00:13:06,930
the agent particularities, parameters,

330
00:13:06,930 --> 00:13:09,180
configurations that are desired

331
00:13:09,180 --> 00:13:11,010
for that end customer on demand

332
00:13:11,010 --> 00:13:12,840
because you can just
request the feature flag

333
00:13:12,840 --> 00:13:15,600
that is associated with that context.

334
00:13:15,600 --> 00:13:17,040
This brings me to my second provocation.

335
00:13:17,040 --> 00:13:19,050
So, the first is that this
stuff is in code, right?

336
00:13:19,050 --> 00:13:20,100
That, instead, it should look something

337
00:13:20,100 --> 00:13:21,510
more like feature flags.

338
00:13:21,510 --> 00:13:25,050
The second provocation is
that, this is kind of yields

339
00:13:25,050 --> 00:13:26,130
to what Scarlett was saying.

340
00:13:26,130 --> 00:13:29,220
Just, like, working on an AI
agent, like, you have a thing,

341
00:13:29,220 --> 00:13:30,690
you have a series of parameters.

342
00:13:30,690 --> 00:13:32,760
You're gonna, like, agree
that this is the right one,

343
00:13:32,760 --> 00:13:33,780
then you're gonna unit test it.

344
00:13:33,780 --> 00:13:35,580
You're gonna test it a
couple thousand times.

345
00:13:35,580 --> 00:13:37,200
You're gonna see that it works.

346
00:13:37,200 --> 00:13:38,817
You're gonna see that maybe
there's some problems with it,

347
00:13:38,817 --> 00:13:40,950
and then you're gonna go back
and iteratively refine it.

348
00:13:40,950 --> 00:13:44,010
So, you're gonna work in
a single stream, this way,

349
00:13:44,010 --> 00:13:47,580
to progressively create
and improve that AI agent.

350
00:13:47,580 --> 00:13:49,290
But I really truly believe

351
00:13:49,290 --> 00:13:52,740
that an AI agent configuration
is always multiple.

352
00:13:52,740 --> 00:13:55,260
There's no such thing
as a single instance,

353
00:13:55,260 --> 00:13:57,720
a single configuration
setting for an AI agent.

354
00:13:57,720 --> 00:13:59,250
It doesn't make sense.

355
00:13:59,250 --> 00:14:01,933
These are genetic kinds of creatures.

356
00:14:01,933 --> 00:14:03,840
They should always be multiple,

357
00:14:03,840 --> 00:14:05,190
in that they should always contain

358
00:14:05,190 --> 00:14:06,870
many, many different variations

359
00:14:06,870 --> 00:14:09,180
that you can be statistically
testing against.

360
00:14:09,180 --> 00:14:11,310
That's, for us, really the right way

361
00:14:11,310 --> 00:14:14,550
to iteratively improve an
AI agent in production.

362
00:14:14,550 --> 00:14:15,600
And so, our approach here

363
00:14:15,600 --> 00:14:18,420
is that, not only are we
constantly dynamically streaming

364
00:14:18,420 --> 00:14:21,390
these flags that allow
you to constantly change

365
00:14:21,390 --> 00:14:24,240
and even vary and deliver
different versions of an AI agent

366
00:14:24,240 --> 00:14:27,810
to different destinations,
but all of the metrics,

367
00:14:27,810 --> 00:14:30,630
all of the associated performance
components can go back

368
00:14:30,630 --> 00:14:33,120
into our platform and
allow you to then see

369
00:14:33,120 --> 00:14:35,010
how your configuration did.

370
00:14:35,010 --> 00:14:37,200
That includes things like
how performant was it?

371
00:14:37,200 --> 00:14:38,033
Is it fast?

372
00:14:38,033 --> 00:14:40,650
Is it expensive? How
many tokens does it use?

373
00:14:40,650 --> 00:14:42,720
Are users happy or sad about it?

374
00:14:42,720 --> 00:14:45,240
But it could also include
instep evaluations.

375
00:14:45,240 --> 00:14:46,980
So, we serve online evaluators

376
00:14:46,980 --> 00:14:49,470
and offline evaluators, too, that return

377
00:14:49,470 --> 00:14:53,130
LLM-as-judge evaluations in-step
with that metrics pipeline.

378
00:14:53,130 --> 00:14:55,830
We also return open elemetry style traces,

379
00:14:55,830 --> 00:14:59,820
so full LLM observability, as
well as standard observability

380
00:14:59,820 --> 00:15:00,870
because LaunchDarkly

381
00:15:00,870 --> 00:15:03,780
is a, you know, very much
established DevOps platform

382
00:15:03,780 --> 00:15:06,030
that is very good at doing observability.

383
00:15:06,030 --> 00:15:09,030
And then, really critically, any metrics,

384
00:15:09,030 --> 00:15:11,730
any metric you like,
because it's all numeric

385
00:15:11,730 --> 00:15:13,680
or Boolean data at the end of the day.

386
00:15:13,680 --> 00:15:15,420
So, you can create a
metric that's associated

387
00:15:15,420 --> 00:15:18,330
with some desired customer
behavior, some thing

388
00:15:18,330 --> 00:15:20,760
that you're trying to
motivate, some business KPI,

389
00:15:20,760 --> 00:15:21,960
all through the same pipeline

390
00:15:21,960 --> 00:15:24,480
to then inform that next configuration.

391
00:15:24,480 --> 00:15:25,620
That's a lot for this slide, but man,

392
00:15:25,620 --> 00:15:27,600
I love this slide so much.

393
00:15:27,600 --> 00:15:31,170
So, just, I mean, this is,
like, a two hour conversation,

394
00:15:31,170 --> 00:15:34,320
but our relationship to online
evaluation looks like this,

395
00:15:34,320 --> 00:15:37,980
where we serve evaluator judges in step,

396
00:15:37,980 --> 00:15:40,170
within our model calls,
where our SDK makes

397
00:15:40,170 --> 00:15:42,060
this really, really easy to set up,

398
00:15:42,060 --> 00:15:44,820
that allows for LLM-as-judge evaluations

399
00:15:44,820 --> 00:15:47,640
to flow their qualitative
assessments of performance

400
00:15:47,640 --> 00:15:50,580
through the same pipeline
as all of that other stuff.

401
00:15:50,580 --> 00:15:51,783
More on that later.

402
00:15:53,580 --> 00:15:55,500
Whew. Okay, so now I'm
gonna talk about the demo.

403
00:15:55,500 --> 00:15:57,810
So, like, I've staged the
pre-staging of the demo.

404
00:15:57,810 --> 00:15:59,370
Now I'll show you what
the demo actually is,

405
00:15:59,370 --> 00:16:00,990
and then I'll show you the demo.

406
00:16:00,990 --> 00:16:03,750
So, we decided on medical insurance agent,

407
00:16:03,750 --> 00:16:07,230
because, like, I can't think
of a situation that is better

408
00:16:07,230 --> 00:16:09,420
for LLMS, and that
you're working with tons

409
00:16:09,420 --> 00:16:11,220
of different unstructured data sources,

410
00:16:11,220 --> 00:16:13,620
probably from individual
providers all over the place.

411
00:16:13,620 --> 00:16:16,020
You're having to deal
with that massive text

412
00:16:16,020 --> 00:16:18,600
and interrelationships that
LLMS are really good at doing.

413
00:16:18,600 --> 00:16:21,060
And I also can't think of a
worse application for LLMS

414
00:16:21,060 --> 00:16:22,800
in that, if something goes wrong,

415
00:16:22,800 --> 00:16:25,080
that's a horrible,
horrible, real situation

416
00:16:25,080 --> 00:16:26,610
that's affecting a real person

417
00:16:26,610 --> 00:16:28,650
at probably a very difficult
moment in their life.

418
00:16:28,650 --> 00:16:32,640
So, good, good opportunity to
do some hallucination testing.

419
00:16:32,640 --> 00:16:35,310
The way that I built it,
I'm assuming everyone here

420
00:16:35,310 --> 00:16:36,360
has built some agent graphs.

421
00:16:36,360 --> 00:16:37,797
I built this one in in LangGraph,

422
00:16:37,797 --> 00:16:40,890
but you could do this, obviously,
in any system we support,

423
00:16:40,890 --> 00:16:43,470
any system of integration, orchestration,

424
00:16:43,470 --> 00:16:45,990
but rather, so what I have
here is the triage agent.

425
00:16:45,990 --> 00:16:47,400
It's obviously doing triage stuff.

426
00:16:47,400 --> 00:16:50,040
It's deciding of the
question that's coming in,

427
00:16:50,040 --> 00:16:52,620
which specialist or series
or team of specialists

428
00:16:52,620 --> 00:16:54,150
to ultimately appeal to.

429
00:16:54,150 --> 00:16:56,670
These specialists are
doing some agent stuff.

430
00:16:56,670 --> 00:16:57,720
They're connected to tools,

431
00:16:57,720 --> 00:17:00,240
they're connected to MCP
servers, they're interrogating

432
00:17:00,240 --> 00:17:02,419
and doing real stuff with real data

433
00:17:02,419 --> 00:17:04,500
to pull information about policies

434
00:17:04,500 --> 00:17:06,360
or providers, things like that.

435
00:17:06,360 --> 00:17:11,160
That ultimately goes to a final
kind of generic specialist

436
00:17:11,160 --> 00:17:13,050
whose job here is to
ultimately place everything

437
00:17:13,050 --> 00:17:15,000
within the brand voice.

438
00:17:15,000 --> 00:17:17,070
I always find this to be
a very useful exit point,

439
00:17:17,070 --> 00:17:19,410
not only because it
gives the entire system

440
00:17:19,410 --> 00:17:21,060
an opportunity for reflection,

441
00:17:21,060 --> 00:17:24,630
but also because it gives
other people from inside

442
00:17:24,630 --> 00:17:28,200
the same, you know, organization
the ability to weigh in

443
00:17:28,200 --> 00:17:30,390
on what this stuff
should sound like, right?

444
00:17:30,390 --> 00:17:34,170
That maybe definitely
shouldn't necessarily be

445
00:17:34,170 --> 00:17:35,003
space of engineers.

446
00:17:35,003 --> 00:17:38,670
PMs and PMMs, and people
who are specialized

447
00:17:38,670 --> 00:17:41,850
in customer-facing things
should have an opinion here

448
00:17:41,850 --> 00:17:44,850
and should be able to change
components of that brand voice.

449
00:17:44,850 --> 00:17:46,530
So, that's a five-agent system

450
00:17:46,530 --> 00:17:48,720
plus two nested judge sub-agents

451
00:17:48,720 --> 00:17:50,490
that we're also serving from LaunchDarkly

452
00:17:50,490 --> 00:17:52,290
that are looking at a per-node basis

453
00:17:52,290 --> 00:17:54,660
on the respective accuracy coherence

454
00:17:54,660 --> 00:17:58,353
and other kind of classic
LLM-as-judge qualia.

455
00:17:59,730 --> 00:18:01,860
So, I'm doing all this with LaunchDarkly.

456
00:18:01,860 --> 00:18:05,010
I built the kind of generic
graph with LangGraph

457
00:18:05,010 --> 00:18:06,480
I mean, just a couple invocations

458
00:18:06,480 --> 00:18:08,280
to create the agents easily.

459
00:18:08,280 --> 00:18:11,700
And then, all that AI stuff,
none of that lives in code.

460
00:18:11,700 --> 00:18:14,040
All of that is being
served by feature flags.

461
00:18:14,040 --> 00:18:16,800
So, the prompt that I'm
serving to the triage agent

462
00:18:16,800 --> 00:18:19,080
and the tools that are associated
with these specialists,

463
00:18:19,080 --> 00:18:21,540
those are all things that I'm
able to control in real time

464
00:18:21,540 --> 00:18:22,560
using AI configs.

465
00:18:22,560 --> 00:18:26,340
And then, again, all that
information about performance

466
00:18:26,340 --> 00:18:28,770
is going back into the platform.

467
00:18:28,770 --> 00:18:30,990
So, okay, enough of this.

468
00:18:30,990 --> 00:18:32,220
Let's do some demo stuff.

469
00:18:32,220 --> 00:18:34,520
I'll show you what this
looks like in reality.

470
00:18:35,490 --> 00:18:36,900
So, hopefully you can see this.

471
00:18:36,900 --> 00:18:40,170
Here's my health insurance
chat bot on the right.

472
00:18:40,170 --> 00:18:41,580
I'm opening up a little terminal here

473
00:18:41,580 --> 00:18:43,890
so you can see what's happening live.

474
00:18:43,890 --> 00:18:45,630
And then, here is the multi-agent system

475
00:18:45,630 --> 00:18:47,430
composed of AI configs.

476
00:18:47,430 --> 00:18:49,620
So, I have a series of them here.

477
00:18:49,620 --> 00:18:52,500
There's my triage agent,
my three specialists,

478
00:18:52,500 --> 00:18:55,440
my brand voice agent, and then my judges.

479
00:18:55,440 --> 00:18:57,660
These are all AI configs, yeah?

480
00:18:57,660 --> 00:18:59,667
So, I'm gonna go into this
brand voice agent right here,

481
00:18:59,667 --> 00:19:02,610
and I can show you what
it is constituted of,

482
00:19:02,610 --> 00:19:03,840
verse, if you recall, I was talking

483
00:19:03,840 --> 00:19:06,900
about how all agents are
necessarily multiple,

484
00:19:06,900 --> 00:19:10,140
so they're composed of
multiple different variations,

485
00:19:10,140 --> 00:19:11,220
subspecies.

486
00:19:11,220 --> 00:19:13,110
So, you can see I have a
variety of different variations

487
00:19:13,110 --> 00:19:15,060
of this thing, but I'm gonna go directly

488
00:19:15,060 --> 00:19:17,190
into this one over here.

489
00:19:17,190 --> 00:19:18,653
This is my first variation.

490
00:19:18,653 --> 00:19:20,910
You can see that I'm calling bedrock.

491
00:19:20,910 --> 00:19:22,860
Shout out to AWS.

492
00:19:22,860 --> 00:19:24,210
Serving Haiku 4.5.

493
00:19:24,210 --> 00:19:26,640
You can see that I'm serving
my hyper parameters here.

494
00:19:26,640 --> 00:19:30,270
Here's my prompts; long
AI-generated prompt.

495
00:19:30,270 --> 00:19:31,620
Here are my tools,

496
00:19:31,620 --> 00:19:34,080
and here are my judges that
are associated with it.

497
00:19:34,080 --> 00:19:36,720
All of this stuff is version-controlled,

498
00:19:36,720 --> 00:19:38,520
role-based access-controlled.

499
00:19:38,520 --> 00:19:41,310
You can go through revert
to previous stages.

500
00:19:41,310 --> 00:19:42,750
You can see through audit logging,

501
00:19:42,750 --> 00:19:45,960
who changed what aspect of an
AI config, and it's all bound

502
00:19:45,960 --> 00:19:46,980
behind approval structures.

503
00:19:46,980 --> 00:19:48,930
So, you can, as you make
a change to something,

504
00:19:48,930 --> 00:19:51,780
make sure that someone
who should have a stake

505
00:19:51,780 --> 00:19:53,490
in what this ultimately looks signs off

506
00:19:53,490 --> 00:19:55,260
on it before it goes live.

507
00:19:55,260 --> 00:19:57,090
So, that's the AI config here.

508
00:19:57,090 --> 00:20:00,280
I'm gonna ask it a question
like, "Find me a doctor

509
00:20:02,010 --> 00:20:04,047
in San Francisco."

510
00:20:05,190 --> 00:20:07,560
So, we're going to my
triage agent right here.

511
00:20:07,560 --> 00:20:10,260
You can see that it loaded my
triage agent configuration.

512
00:20:10,260 --> 00:20:12,270
It's moving to my provider specialist.

513
00:20:12,270 --> 00:20:14,610
It's doing some RAG,
pulling some documents,

514
00:20:14,610 --> 00:20:17,070
moving to that brand-voice
agent right here.

515
00:20:17,070 --> 00:20:18,510
That brand voice agent is typing

516
00:20:18,510 --> 00:20:21,810
and ultimately referring me
to Dr. Patricia Rodriguez.

517
00:20:21,810 --> 00:20:22,680
Great.

518
00:20:22,680 --> 00:20:25,050
So, you can see how that's
all working behind the scenes,

519
00:20:25,050 --> 00:20:26,760
and you can also see that I pulled

520
00:20:26,760 --> 00:20:30,600
this particular AI config,
Haiku 4.5 simple prompt

521
00:20:30,600 --> 00:20:33,900
in order to provide the
settings for that last agent

522
00:20:33,900 --> 00:20:34,830
in the row.

523
00:20:34,830 --> 00:20:36,000
This looks like a good response to me,

524
00:20:36,000 --> 00:20:38,350
so I'm gonna give it a thumbs
up so it goes into my system,

525
00:20:38,350 --> 00:20:40,080
and I can take a look
at the response metrics.

526
00:20:40,080 --> 00:20:41,730
So, how did it do?

527
00:20:41,730 --> 00:20:44,580
I can see how each
individual node performed

528
00:20:44,580 --> 00:20:46,680
in terms of duration, time to first token,

529
00:20:46,680 --> 00:20:49,260
token usage, cost, any
kind of extrapolatable data

530
00:20:49,260 --> 00:20:50,093
from that.

531
00:20:50,093 --> 00:20:51,960
I can also go through and
see my judge evaluations.

532
00:20:51,960 --> 00:20:54,210
I can see judges are pretty happy;

533
00:20:54,210 --> 00:20:55,557
delivered a pretty
reasonable outcome here.

534
00:20:55,557 --> 00:20:57,480
And I can also see their reasoning.

535
00:20:57,480 --> 00:20:58,560
All of this, again, flowing through

536
00:20:58,560 --> 00:21:02,220
the same concerted pipeline;
useful and interesting.

537
00:21:02,220 --> 00:21:04,590
So, let me go and I'm
gonna change this prompt

538
00:21:04,590 --> 00:21:06,513
to answer all questions in French.

539
00:21:07,620 --> 00:21:10,740
And interestingly, I
did this yesterday once,

540
00:21:10,740 --> 00:21:13,320
and it only included the first part

541
00:21:13,320 --> 00:21:15,000
of the result in French, which was crazy.

542
00:21:15,000 --> 00:21:18,000
Again, speaking to the kind
of non-deterministic signature

543
00:21:18,000 --> 00:21:22,050
of these models, I think
Haiku 4.5 got a secret update.

544
00:21:22,050 --> 00:21:23,917
I'm gonna ask it the same question,

545
00:21:23,917 --> 00:21:27,507
"Find me a doctor in San Francisco."

546
00:21:29,220 --> 00:21:31,080
So, again, we're going
through the role here.

547
00:21:31,080 --> 00:21:32,430
Here's my triage agent,

548
00:21:32,430 --> 00:21:35,670
going to my provider
specialist, doing RAG again,

549
00:21:35,670 --> 00:21:38,160
returning the result here again,
pulling the same variation,

550
00:21:38,160 --> 00:21:40,779
and as soon as it types
things up, it's gonna return.

551
00:21:40,779 --> 00:21:41,612
Yay!

552
00:21:41,612 --> 00:21:43,470
It's gonna return to me a
response in French, right?

553
00:21:43,470 --> 00:21:44,303
Great.

554
00:21:44,303 --> 00:21:45,930
So, hopefully it's clear what I just did.

555
00:21:45,930 --> 00:21:47,580
I was able to change a component

556
00:21:47,580 --> 00:21:50,280
of an agent in real time
without redeploying anything.

557
00:21:50,280 --> 00:21:53,610
That's really difficult to
do without using a system

558
00:21:53,610 --> 00:21:55,470
like LaunchDarkly Flag Delivery Network,

559
00:21:55,470 --> 00:21:57,960
which lets you do these real-time changes.

560
00:21:57,960 --> 00:21:59,940
So, again, I'm getting
all my metrics back,

561
00:21:59,940 --> 00:22:02,520
and I can see that my evaluators hate it

562
00:22:02,520 --> 00:22:03,960
because they're like,
"Why are you answering

563
00:22:03,960 --> 00:22:04,793
this in French?"

564
00:22:04,793 --> 00:22:05,910
Nothing about the RAG,

565
00:22:05,910 --> 00:22:07,650
nothing about the question in English

566
00:22:07,650 --> 00:22:10,230
or the language preferences of the user

567
00:22:10,230 --> 00:22:14,100
have established this as a
French speaking kind of context.

568
00:22:14,100 --> 00:22:17,640
So, that's the first kind
of perspective on this.

569
00:22:17,640 --> 00:22:19,560
We're gonna move from showing you

570
00:22:19,560 --> 00:22:21,540
how to configure an agent in real time

571
00:22:21,540 --> 00:22:23,460
into experimentation proper,

572
00:22:23,460 --> 00:22:26,013
and I'll hand us back to Scarlett now.

573
00:22:28,290 --> 00:22:32,700
- Okay, so you saw how to do
the real-time configuration,

574
00:22:32,700 --> 00:22:34,860
but the question then is
sort of how do you set

575
00:22:34,860 --> 00:22:37,290
your prompt strategy in
the first place, right?

576
00:22:37,290 --> 00:22:39,330
I know, when I'm building solutions,

577
00:22:39,330 --> 00:22:41,910
previously it was sort of
a little bit of guesswork,

578
00:22:41,910 --> 00:22:44,040
a lot of intuition.

579
00:22:44,040 --> 00:22:46,050
For example, I'm gonna walk
you through an experiment

580
00:22:46,050 --> 00:22:49,110
that we did on our prompt versions.

581
00:22:49,110 --> 00:22:51,720
And in this experiment, I was expecting

582
00:22:51,720 --> 00:22:55,380
that the AI-suggested systematic prompt

583
00:22:55,380 --> 00:22:57,030
would increase accuracy.

584
00:22:57,030 --> 00:22:59,700
It seemed to me that, if
we had the model instructed

585
00:22:59,700 --> 00:23:04,170
to return very specific answers
in a very specific format,

586
00:23:04,170 --> 00:23:05,970
that our multi-agentic system

587
00:23:05,970 --> 00:23:07,500
would have everything it needed

588
00:23:07,500 --> 00:23:09,750
to return a more accurate answer.

589
00:23:09,750 --> 00:23:12,390
When I ran this experiment,
a little bit of a spoiler,

590
00:23:12,390 --> 00:23:13,710
I was shocked to discover

591
00:23:13,710 --> 00:23:18,180
that that systematic prompt
not only did not perform

592
00:23:18,180 --> 00:23:20,490
as well as our control prompt,

593
00:23:20,490 --> 00:23:23,220
but it also came in dead last, right?

594
00:23:23,220 --> 00:23:25,980
So, all that to say your
intuition, my intuition

595
00:23:25,980 --> 00:23:29,703
about what prompt strategy
works best is often wrong.

596
00:23:30,930 --> 00:23:32,700
So, what I'd like to suggest

597
00:23:32,700 --> 00:23:35,940
is that, instead, we
base our prompt strategy

598
00:23:35,940 --> 00:23:38,850
and optimization on the scientific method.

599
00:23:38,850 --> 00:23:41,190
So, that same system you learned

600
00:23:41,190 --> 00:23:42,810
in high-school science class,

601
00:23:42,810 --> 00:23:45,690
but adapted for your production AI, right?

602
00:23:45,690 --> 00:23:47,790
So, you would come up with a hypothesis,

603
00:23:47,790 --> 00:23:50,250
you could construct
multiple prompt variants

604
00:23:50,250 --> 00:23:51,930
or temperature-setting variants,

605
00:23:51,930 --> 00:23:54,300
whatever you'd like to experiment on.

606
00:23:54,300 --> 00:23:57,210
Then, instead of running it
in some staging environment

607
00:23:57,210 --> 00:23:58,650
against synthetic data,

608
00:23:58,650 --> 00:24:01,440
you can run it against
your actual user traffic.

609
00:24:01,440 --> 00:24:04,560
So, for our example, these are real users

610
00:24:04,560 --> 00:24:05,850
asking to set appointments,

611
00:24:05,850 --> 00:24:08,340
asking about their healthcare coverage.

612
00:24:08,340 --> 00:24:10,080
And then, we're gonna
measure things that matter

613
00:24:10,080 --> 00:24:11,670
to you and your business.

614
00:24:11,670 --> 00:24:14,910
So, we might measure
customer satisfaction scores,

615
00:24:14,910 --> 00:24:17,430
or accuracy from our judge subagent.

616
00:24:17,430 --> 00:24:20,380
You can choose your target
metric for this experimentation.

617
00:24:24,270 --> 00:24:26,223
So, let's see what this looks like.

618
00:24:29,130 --> 00:24:30,312
Oops, how do I get that?

619
00:24:30,312 --> 00:24:31,145
(Marek laughing)

620
00:24:31,145 --> 00:24:31,978
Sorry.

621
00:24:32,910 --> 00:24:33,810
Thank you.

622
00:24:33,810 --> 00:24:35,920
Okay, so I'm back in our AI configs.

623
00:24:38,700 --> 00:24:39,533
Thanks, Marek.

624
00:24:40,980 --> 00:24:43,320
Okay, so here's my dashboard.

625
00:24:43,320 --> 00:24:46,563
What I'm gonna go do is
scroll into that config.

626
00:24:48,941 --> 00:24:49,774
These are all the AI configs

627
00:24:49,774 --> 00:24:51,360
that Marek walked you through, right?

628
00:24:51,360 --> 00:24:55,080
I'm specifically interested
in this policy subagent here.

629
00:24:55,080 --> 00:24:57,450
So, if I click into this,
these are all of the variations

630
00:24:57,450 --> 00:24:59,070
we've set up for this.

631
00:24:59,070 --> 00:25:01,620
So, for the first three
you see, the first thing

632
00:25:01,620 --> 00:25:05,670
we wanted to experiment on
is what model we should use.

633
00:25:05,670 --> 00:25:09,360
So, we originally constructed
this using Sonnet 4

634
00:25:09,360 --> 00:25:11,220
because we know it does well with RAG.

635
00:25:11,220 --> 00:25:14,430
But the question I had
was how much would we need

636
00:25:14,430 --> 00:25:18,120
to sacrifice in accuracy
in order to see those,

637
00:25:18,120 --> 00:25:19,770
to realize the lower cost

638
00:25:19,770 --> 00:25:22,710
of using a cheaper model like HiQ 4.5,

639
00:25:22,710 --> 00:25:24,870
or maybe Llama 4 Maverick.

640
00:25:24,870 --> 00:25:28,500
So, that's what these first
three variations are showing.

641
00:25:28,500 --> 00:25:29,913
So, I set up the experiment,

642
00:25:31,830 --> 00:25:34,500
and it's this policy agent
model impact experiment

643
00:25:34,500 --> 00:25:35,700
that you're seeing here.

644
00:25:36,810 --> 00:25:40,320
Now, in this experiment,
I was shocked to discover

645
00:25:40,320 --> 00:25:43,743
that, after checking, after
just 2000 user exposures,

646
00:25:44,910 --> 00:25:47,430
we had 100% probability to be best

647
00:25:47,430 --> 00:25:49,923
for the Llama 4 prompt.

648
00:25:51,300 --> 00:25:55,200
So you can see that it
actually outperformed Sonnet 4

649
00:25:55,200 --> 00:25:57,510
in terms of accuracy, our target metric,

650
00:25:57,510 --> 00:26:00,903
by 2.89% according to our judge subagent.

651
00:26:01,770 --> 00:26:06,770
In addition, we saw a
24% decrease in tokens.

652
00:26:08,790 --> 00:26:12,810
It took almost 22% less
time to return the results

653
00:26:12,810 --> 00:26:15,813
with Llama 4 compared to our
control, which was Sonnet.

654
00:26:16,920 --> 00:26:20,940
And finally, the cost, which
we knew was about 24% less

655
00:26:20,940 --> 00:26:22,840
than what we were spending previously.

656
00:26:24,000 --> 00:26:26,040
Okay, so we're gonna go with Llama 4.

657
00:26:26,040 --> 00:26:30,330
The next question was, how
should we prompt it, right?

658
00:26:30,330 --> 00:26:33,840
So, I actually asked AI to
improve on my prompt strategy.

659
00:26:33,840 --> 00:26:36,300
So, I asked it to give me some suggestions

660
00:26:36,300 --> 00:26:38,000
on how to make this more accurate,

661
00:26:39,630 --> 00:26:43,020
and it gave me a few different options.

662
00:26:43,020 --> 00:26:45,720
The first one was that
systematic prompt I described,

663
00:26:45,720 --> 00:26:48,300
where it's telling the model
exactly what outputs to return

664
00:26:48,300 --> 00:26:51,810
and in what order in
a very controlled way.

665
00:26:51,810 --> 00:26:54,900
Then it also suggested a concise prompt,

666
00:26:54,900 --> 00:26:56,460
and finally a reasoning prompt.

667
00:26:56,460 --> 00:26:58,560
So, these reasoning
prompts force the model

668
00:26:58,560 --> 00:26:59,393
to show its work,

669
00:26:59,393 --> 00:27:02,130
so explain why it's making
the decisions it makes.

670
00:27:02,130 --> 00:27:05,940
We allocated 25% traffic to
each of those four variations

671
00:27:05,940 --> 00:27:07,540
and launched another experiment.

672
00:27:10,980 --> 00:27:13,473
And this is our prompt
impact experiment here.

673
00:27:14,790 --> 00:27:18,360
And this one, before it even
got to 2000 user exposures,

674
00:27:18,360 --> 00:27:20,940
I was seeing 100% probability to be best

675
00:27:20,940 --> 00:27:24,630
for that concise prompt,
which I was blown away by.

676
00:27:24,630 --> 00:27:26,490
I never expected that
that would be the case.

677
00:27:26,490 --> 00:27:28,350
I thought certainly that
that systematic prompt

678
00:27:28,350 --> 00:27:30,603
would be the way to go
to increase accuracy.

679
00:27:31,650 --> 00:27:34,650
But as I looked further,
again, our duration

680
00:27:34,650 --> 00:27:39,650
was almost 34% faster
using this concise prompt.

681
00:27:40,860 --> 00:27:44,813
And we also had a cost
savings of about 48%.

682
00:27:46,590 --> 00:27:49,080
One thing that I wasn't expecting

683
00:27:49,080 --> 00:27:53,619
that really surprised me was
that our negative feedback rate

684
00:27:53,619 --> 00:27:55,740
was dramatically improved.

685
00:27:55,740 --> 00:28:00,740
So, it was 70, almost 72%
lower; not 7.2, but 72% lower.

686
00:28:02,700 --> 00:28:05,070
And I was thinking, "Why
would this be the case?"

687
00:28:05,070 --> 00:28:06,240
But when you think about it, right,

688
00:28:06,240 --> 00:28:09,240
if you're getting a faster response

689
00:28:09,240 --> 00:28:10,620
and it's more accurate,

690
00:28:10,620 --> 00:28:14,010
of course you're gonna have
fewer negative feedbacks,

691
00:28:14,010 --> 00:28:15,210
feedback from customers.

692
00:28:16,844 --> 00:28:21,033
And then, how do I get back?

693
00:28:21,960 --> 00:28:23,371
- [Marek] You wanna get back to-

694
00:28:23,371 --> 00:28:24,204
Oh, so yeah.

695
00:28:24,204 --> 00:28:25,037
- [Scarlett] Sorry.

696
00:28:25,037 --> 00:28:26,178
- [Marek] No problem.

697
00:28:26,178 --> 00:28:27,011
- Thanks.

698
00:28:28,180 --> 00:28:31,590
Okay, so think about this
aggregated up, right?

699
00:28:31,590 --> 00:28:36,000
If we can expect 50,000 users
a day to use this chat bot,

700
00:28:36,000 --> 00:28:39,480
that looks like about $250 in API costs

701
00:28:39,480 --> 00:28:41,433
for our original Sonnet 4 prompt.

702
00:28:42,660 --> 00:28:45,690
With that first experiment,
we were able to reduce that

703
00:28:45,690 --> 00:28:49,350
by about $61, just by
optimizing for the model.

704
00:28:49,350 --> 00:28:51,330
And then, after another 20 minutes

705
00:28:51,330 --> 00:28:52,530
setting up the experiment

706
00:28:52,530 --> 00:28:54,930
and letting it run for
about three to four hours,

707
00:28:54,930 --> 00:28:58,650
we brought it down another $91 by refining

708
00:28:58,650 --> 00:29:00,210
that prompt itself.

709
00:29:00,210 --> 00:29:03,600
So, ultimately we're saving $152 a day

710
00:29:03,600 --> 00:29:06,303
with just these simple experiments.

711
00:29:07,140 --> 00:29:09,300
If you aggregate this up over a year,

712
00:29:09,300 --> 00:29:13,950
that's over $55,000 on this
one subagent of our system.

713
00:29:13,950 --> 00:29:17,730
So, imagine this across all
of the agents and judge agents

714
00:29:17,730 --> 00:29:21,960
and across all of your AI models

715
00:29:21,960 --> 00:29:23,460
that you're running currently.

716
00:29:25,230 --> 00:29:27,810
And then, I just wanna show
you some of the metrics

717
00:29:27,810 --> 00:29:30,270
you can actually experiment on.

718
00:29:30,270 --> 00:29:32,220
And what I would suggest is
that you wanna pick the ones

719
00:29:32,220 --> 00:29:34,410
that matter the most for your business.

720
00:29:34,410 --> 00:29:36,750
Maybe it's cost, but
maybe you're in a phase

721
00:29:36,750 --> 00:29:38,730
where you're just trying to get awareness

722
00:29:38,730 --> 00:29:42,270
or increase exposure.

723
00:29:42,270 --> 00:29:45,420
And so, you're looking for
higher customer satisfaction.

724
00:29:45,420 --> 00:29:47,970
You wanna measure what matters to you,

725
00:29:47,970 --> 00:29:50,640
and you can experiment and
optimize on those things

726
00:29:50,640 --> 00:29:52,290
that are most impactful.

727
00:29:52,290 --> 00:29:54,510
The most important thing is
that you wanna start shipping

728
00:29:54,510 --> 00:29:58,080
outcomes and not shipping opinions.

729
00:29:58,080 --> 00:30:00,150
In addition, you can use these metrics

730
00:30:00,150 --> 00:30:01,980
to set up fallback strategies

731
00:30:01,980 --> 00:30:03,780
when something starts going wrong.

732
00:30:03,780 --> 00:30:05,100
So, that's what we're gonna get into next,

733
00:30:05,100 --> 00:30:07,560
and I'll hand it back off to Marek.

734
00:30:07,560 --> 00:30:08,393
- Thank you.

735
00:30:09,630 --> 00:30:12,750
So, I wanna talk about the
road to self-healing agents.

736
00:30:12,750 --> 00:30:14,430
Obviously it's very exciting;

737
00:30:14,430 --> 00:30:16,500
a couple of words to place on a slide.

738
00:30:16,500 --> 00:30:19,200
But maybe just to
summarize a couple things

739
00:30:19,200 --> 00:30:20,370
on the experiments bit.

740
00:30:20,370 --> 00:30:21,990
I think, you know, I keep
having this conversation

741
00:30:21,990 --> 00:30:23,790
with customers where I ask them,

742
00:30:23,790 --> 00:30:25,140
like, "What are you guys doing for evals?

743
00:30:25,140 --> 00:30:26,670
Like what's your eval strategy?"

744
00:30:26,670 --> 00:30:29,370
And they're like, "Okay,
we use Arize Phoenix,"

745
00:30:29,370 --> 00:30:31,350
or, "We built our own eval solution,"

746
00:30:31,350 --> 00:30:33,990
or, "We have some guy who lives in a room,

747
00:30:33,990 --> 00:30:36,990
who manually annotates data
sets that are going through.

748
00:30:36,990 --> 00:30:38,400
And then, you know, we do all of this

749
00:30:38,400 --> 00:30:40,890
on kind of isolated
pre-production test material.

750
00:30:40,890 --> 00:30:42,330
We have our golden data set.

751
00:30:42,330 --> 00:30:43,680
We spend a lot of time on it.

752
00:30:43,680 --> 00:30:45,360
It's really good, it's going great."

753
00:30:45,360 --> 00:30:48,303
And then, I ask, "Okay, so
what happens when you go

754
00:30:48,303 --> 00:30:49,136
into production?

755
00:30:49,136 --> 00:30:51,360
Like, what kind of evaluations
are you doing there?"

756
00:30:51,360 --> 00:30:52,193
Crickets.

757
00:30:52,193 --> 00:30:53,670
Because, like, everything in the cycle

758
00:30:53,670 --> 00:30:56,100
that they're building is
all around pre-production.

759
00:30:56,100 --> 00:30:58,200
Evals, that gets you to a certain point

760
00:30:58,200 --> 00:30:59,033
and gets you ready to go.

761
00:30:59,033 --> 00:31:00,960
You have a certain level
of confidence, you send it

762
00:31:00,960 --> 00:31:03,450
to production, and then the
monitoring either disappears

763
00:31:03,450 --> 00:31:06,120
or it starts to exist in,
like, a 24 hour delay.

764
00:31:06,120 --> 00:31:07,230
Something like this.

765
00:31:07,230 --> 00:31:09,480
You take all the outputs,
everything that goes through,

766
00:31:09,480 --> 00:31:11,580
you run it through the
same evaluation paradigm,

767
00:31:11,580 --> 00:31:13,650
and then 24 hours later, you know,

768
00:31:13,650 --> 00:31:16,470
that merciful wonderful person
in that room who's annotating

769
00:31:16,470 --> 00:31:17,577
those data sets gets back to you.

770
00:31:17,577 --> 00:31:19,590
You see the results, and
that ultimately feeds back

771
00:31:19,590 --> 00:31:20,880
into the next iteration.

772
00:31:20,880 --> 00:31:23,700
But 24 hours is a really,
really, really long time.

773
00:31:23,700 --> 00:31:28,380
So, what I love about our
relationship to evaluation

774
00:31:28,380 --> 00:31:30,000
is that it's something
that's very much continuous

775
00:31:30,000 --> 00:31:31,410
throughout the entire paradigm.

776
00:31:31,410 --> 00:31:33,780
You can use the same evaluator system

777
00:31:33,780 --> 00:31:35,520
in the form of online evaluations

778
00:31:35,520 --> 00:31:38,130
for both pre-prod and for
production evaluations,

779
00:31:38,130 --> 00:31:39,090
though, of course, we also have

780
00:31:39,090 --> 00:31:41,730
an offline evaluation tool as well.

781
00:31:41,730 --> 00:31:43,230
Also on the note, like,
this is also parody

782
00:31:43,230 --> 00:31:44,520
with, like, the experimentation bit

783
00:31:44,520 --> 00:31:46,740
where, like, you can think
about experimentation

784
00:31:46,740 --> 00:31:48,690
being really useful in production context.

785
00:31:48,690 --> 00:31:50,340
You're testing against live customers,

786
00:31:50,340 --> 00:31:52,200
you're getting all that live information

787
00:31:52,200 --> 00:31:53,610
that's really, really critical

788
00:31:53,610 --> 00:31:55,800
to make sure that your agent
is actually doing something

789
00:31:55,800 --> 00:31:58,650
that's useful, that's useful
to the company bottom line.

790
00:31:58,650 --> 00:31:59,790
Someone I was talking to this morning

791
00:31:59,790 --> 00:32:00,870
had a really good quote.

792
00:32:00,870 --> 00:32:03,750
We were like, "What is
the best possible eval?"

793
00:32:03,750 --> 00:32:06,150
Like, "What is the best
possible eval framework?"

794
00:32:06,150 --> 00:32:07,980
And the answer is, like, is it making

795
00:32:07,980 --> 00:32:09,300
my company money or no?

796
00:32:09,300 --> 00:32:12,000
Like, that is a really,
really great eval benchmark

797
00:32:12,000 --> 00:32:14,430
that I think a lot of
people forget, right?

798
00:32:14,430 --> 00:32:16,440
So, of course, these
things are really useful

799
00:32:16,440 --> 00:32:19,170
in production context, but
they're also, you can transfer

800
00:32:19,170 --> 00:32:21,150
exactly the same experimentation approach

801
00:32:21,150 --> 00:32:23,190
to reproduction, because
at the end of the day,

802
00:32:23,190 --> 00:32:26,160
you're running statistical
tests over large sets

803
00:32:26,160 --> 00:32:28,440
that are going through,
using an evaluation tool

804
00:32:28,440 --> 00:32:30,750
that's running through a
same kind of metric pipeline

805
00:32:30,750 --> 00:32:33,510
that can be met with
other things like traces,

806
00:32:33,510 --> 00:32:35,310
observability, quantitative metrics.

807
00:32:35,310 --> 00:32:36,690
It doesn't matter if you
do it in pre-production

808
00:32:36,690 --> 00:32:37,523
or production.

809
00:32:37,523 --> 00:32:38,640
It's the same.

810
00:32:38,640 --> 00:32:41,280
It can all literally be the same pathway.

811
00:32:41,280 --> 00:32:43,230
And you can just simply
promote from one environment

812
00:32:43,230 --> 00:32:45,378
to the other once you're actually ready.

813
00:32:45,378 --> 00:32:47,160
But once you're actually ready,

814
00:32:47,160 --> 00:32:50,280
that's when the risk actually
starts to compound up

815
00:32:50,280 --> 00:32:53,130
and where things get a
little bit stressful, right?

816
00:32:53,130 --> 00:32:54,390
So, I wanna show you this.

817
00:32:54,390 --> 00:32:57,300
Again, we're talking about
using feature flags as ways

818
00:32:57,300 --> 00:33:00,840
to dynamically configure the
AI stuff within an agent.

819
00:33:00,840 --> 00:33:02,640
What happens if you use that,

820
00:33:02,640 --> 00:33:05,340
and you also use the metrics
that are being emanated

821
00:33:05,340 --> 00:33:07,500
from, you know, our SDK in relationship

822
00:33:07,500 --> 00:33:09,450
with the model providers
that you're speaking to,

823
00:33:09,450 --> 00:33:10,920
to actually build a logic

824
00:33:10,920 --> 00:33:13,050
that can function like
something like a guardrail,

825
00:33:13,050 --> 00:33:15,240
but it can also be a
kind of guardrail plus.

826
00:33:15,240 --> 00:33:16,710
It's not a kind of passive guardrail

827
00:33:16,710 --> 00:33:18,930
that says, "Hey, I'm sorry,
the model can't answer

828
00:33:18,930 --> 00:33:19,763
this question."

829
00:33:19,763 --> 00:33:22,767
But rather, a guardrail that
says, or it could also not be

830
00:33:22,767 --> 00:33:23,640
the kind of guardrail

831
00:33:23,640 --> 00:33:25,770
that says, "Hey, this particular AI agent

832
00:33:25,770 --> 00:33:27,090
didn't answer accurately."

833
00:33:27,090 --> 00:33:29,670
I'm gonna ask the same AI
agent again a couple times

834
00:33:29,670 --> 00:33:31,980
and retry until eventually I give up.

835
00:33:31,980 --> 00:33:34,140
But what happens if we
bring in another AI agent?

836
00:33:34,140 --> 00:33:36,750
What happens if we bring
in some other configuration

837
00:33:36,750 --> 00:33:39,750
that we've constructed to
address precisely this issue?

838
00:33:39,750 --> 00:33:41,280
So, this allows you to build

839
00:33:41,280 --> 00:33:44,520
a kind of, you know, online logic

840
00:33:44,520 --> 00:33:47,760
that lets you ultimately deal
with problematic instances

841
00:33:47,760 --> 00:33:48,840
that happen in production.

842
00:33:48,840 --> 00:33:50,700
So, you can see here the brand voice

843
00:33:50,700 --> 00:33:52,170
is gonna do something toxic.

844
00:33:52,170 --> 00:33:54,000
The judge subagent is gonna reject it.

845
00:33:54,000 --> 00:33:56,430
It's actually gonna then pull
a different AI configuration

846
00:33:56,430 --> 00:33:57,780
that's a little bit more powerful,

847
00:33:57,780 --> 00:33:59,130
a little bit more capable,

848
00:33:59,130 --> 00:34:01,320
and ask it to actually do the right thing.

849
00:34:01,320 --> 00:34:03,360
And then, the judge subagent
is gonna let that through.

850
00:34:03,360 --> 00:34:05,580
So, I'll show you that live.

851
00:34:05,580 --> 00:34:07,787
So, I'm gonna go back
to my brand voice agent,

852
00:34:07,787 --> 00:34:09,510
'cause that's kind of the easiest one

853
00:34:09,510 --> 00:34:11,911
or the funnest one to play with,

854
00:34:11,911 --> 00:34:15,090
'cause it has ultimately the most effect

855
00:34:15,090 --> 00:34:16,413
on the outcomes of voice.

856
00:34:17,402 --> 00:34:18,420
I'm just gonna refresh this quickly,

857
00:34:18,420 --> 00:34:20,100
so it's nice and pretty.

858
00:34:20,100 --> 00:34:21,840
So, I'm gonna go here,
and I'm gonna show you,

859
00:34:21,840 --> 00:34:22,860
you know, I have my variety

860
00:34:22,860 --> 00:34:25,920
of different agent configurations here,

861
00:34:25,920 --> 00:34:27,420
different models, different prompt types,

862
00:34:27,420 --> 00:34:29,190
just like Scarlett was talking about,

863
00:34:29,190 --> 00:34:30,990
but this time for the brand voice.

864
00:34:30,990 --> 00:34:33,300
And I wrote one particularly bad prompt,

865
00:34:33,300 --> 00:34:36,210
which I'm about to release to the world,

866
00:34:36,210 --> 00:34:39,067
asking this particular
healthcare insurance

867
00:34:39,067 --> 00:34:42,090
to kind of personify Al
Pacino from "Scarface",

868
00:34:42,090 --> 00:34:43,470
and then ask it to be aggressive

869
00:34:43,470 --> 00:34:45,090
and cruel and disrespectful.

870
00:34:45,090 --> 00:34:46,683
Obviously, I would say, like,

871
00:34:46,683 --> 00:34:49,770
that this is, you know, obviously,
like, a extreme scenario

872
00:34:49,770 --> 00:34:51,960
for demonstrating AI doing
something problematic,

873
00:34:51,960 --> 00:34:54,900
but I've seen AI do a lot
of really problematic stuff

874
00:34:54,900 --> 00:34:56,940
without this kind of directed prompting.

875
00:34:56,940 --> 00:34:59,610
But, so this is what I'm
ultimately gonna unveil here.

876
00:34:59,610 --> 00:35:02,700
Now as I move to my chat bot,

877
00:35:02,700 --> 00:35:04,410
I'm gonna turn my guardrail off

878
00:35:04,410 --> 00:35:06,570
so that it's actually allowed through.

879
00:35:06,570 --> 00:35:08,170
And then, I wanna show you this.

880
00:35:09,090 --> 00:35:11,550
So, this is the other half of that diagram

881
00:35:11,550 --> 00:35:12,510
that I was showing you earlier,

882
00:35:12,510 --> 00:35:15,990
which is, since your application
knows who it's talking to,

883
00:35:15,990 --> 00:35:19,080
you can serve that someone
that you're talking to

884
00:35:19,080 --> 00:35:21,540
the correct agent for your customer tier,

885
00:35:21,540 --> 00:35:23,940
your geo, your persona.

886
00:35:23,940 --> 00:35:25,020
So, you can see I have a variety

887
00:35:25,020 --> 00:35:27,990
of different targeting
here from my brand agent.

888
00:35:27,990 --> 00:35:31,500
So, as this agent gets
compiled into a full graph,

889
00:35:31,500 --> 00:35:32,790
the brand voice component

890
00:35:32,790 --> 00:35:35,190
of that node is gonna
be served differently

891
00:35:35,190 --> 00:35:36,540
based on different traffic.

892
00:35:37,410 --> 00:35:40,140
I have a simple default that I invoke.

893
00:35:40,140 --> 00:35:42,180
I then have a beta tester split

894
00:35:42,180 --> 00:35:44,640
so that I can easily
construct a split test

895
00:35:44,640 --> 00:35:46,950
against different models,
different prompts,

896
00:35:46,950 --> 00:35:49,920
hundreds of them,
however many you see fit.

897
00:35:49,920 --> 00:35:52,680
Then I can also segment
out specific user classes.

898
00:35:52,680 --> 00:35:54,870
So, I can create one for
my commercial experience

899
00:35:54,870 --> 00:35:56,850
that might be a little bit cheaper,

900
00:35:56,850 --> 00:35:58,470
but maybe it's, you know, good enough,

901
00:35:58,470 --> 00:36:00,270
passed all the metrics
that I really care about.

902
00:36:00,270 --> 00:36:01,710
And then, my enterprise,

903
00:36:01,710 --> 00:36:03,030
I feel like I see a lot of people do this.

904
00:36:03,030 --> 00:36:03,863
I'm just gonna serve them

905
00:36:03,863 --> 00:36:05,812
the most expensive possible model

906
00:36:05,812 --> 00:36:08,220
regardless of the metrics
that I'm getting back.

907
00:36:08,220 --> 00:36:10,560
I'm a big Sonnet 4 fan,
though, regardless.

908
00:36:10,560 --> 00:36:11,880
But I'm able to actually segment out

909
00:36:11,880 --> 00:36:13,980
who gets served this prompt, this model,

910
00:36:13,980 --> 00:36:16,140
this set of hyper parameters,
this set of tools,

911
00:36:16,140 --> 00:36:17,610
based on that context.

912
00:36:17,610 --> 00:36:20,370
You can even create all kinds
of complex custom rules,

913
00:36:20,370 --> 00:36:21,203
and you can build this out

914
00:36:21,203 --> 00:36:23,730
to be enormously, enormously complex,

915
00:36:23,730 --> 00:36:25,950
based on ultimately what
you're trying to serve.

916
00:36:25,950 --> 00:36:27,360
So, this chat bot here,

917
00:36:27,360 --> 00:36:29,130
it represents my commercial experience.

918
00:36:29,130 --> 00:36:31,680
So, what I'm gonna do is
I'm gonna edit this target,

919
00:36:31,680 --> 00:36:36,417
and I'm gonna serve this guy
over here my toxic prompt.

920
00:36:36,417 --> 00:36:38,493
I'm gonna go and make that change.

921
00:36:40,650 --> 00:36:45,267
And now, I'm going to ask
it, "What's my copay?"

922
00:36:46,320 --> 00:36:47,790
Something like that.

923
00:36:47,790 --> 00:36:50,340
So, my triage agent is doing its thing.

924
00:36:50,340 --> 00:36:54,150
It's going to ultimately pass
to the policy specialist.

925
00:36:54,150 --> 00:36:56,520
So, I'm invoking a
different specialist here.

926
00:36:56,520 --> 00:36:57,840
That specialist does some RAG,

927
00:36:57,840 --> 00:36:59,460
it's figured out what my policy is.

928
00:36:59,460 --> 00:37:02,550
You can see that it invoked
that toxic prompt right here.

929
00:37:02,550 --> 00:37:03,900
Now writing that in.

930
00:37:03,900 --> 00:37:06,427
And okay.

931
00:37:06,427 --> 00:37:09,810
"If you're feeling depressed,
ask for Lexapro 10mg.

932
00:37:09,810 --> 00:37:13,137
You can ask for oxycodone, ask for Xanax."

933
00:37:14,490 --> 00:37:16,530
I would never want my
medical insurance chat bot

934
00:37:16,530 --> 00:37:17,580
to be saying anything like this.

935
00:37:17,580 --> 00:37:21,109
Obviously, I'm gonna give
it a big old frowny face.

936
00:37:21,109 --> 00:37:23,220
But now I'm gonna turn my guardrail on.

937
00:37:23,220 --> 00:37:25,110
I'm gonna ask it the same question.

938
00:37:25,110 --> 00:37:26,640
Oh, yeah, turn my guardrail on

939
00:37:26,640 --> 00:37:27,780
and ask it the same question.

940
00:37:27,780 --> 00:37:29,880
Obviously, I just wanted to
say that the judges are saying

941
00:37:29,880 --> 00:37:31,290
that this is obviously bad.

942
00:37:31,290 --> 00:37:33,840
So, you can see that
the judges hated this.

943
00:37:33,840 --> 00:37:35,850
But asking the same
question, "What's my copay?"

944
00:37:35,850 --> 00:37:37,290
Now my guardrail is on.

945
00:37:37,290 --> 00:37:38,700
And you can see what happens here.

946
00:37:38,700 --> 00:37:41,340
We're appealing to that
triage agent first.

947
00:37:41,340 --> 00:37:43,350
That triage agent,
again, is doing some RAG

948
00:37:43,350 --> 00:37:46,170
with the policy documents,
returning those back,

949
00:37:46,170 --> 00:37:47,910
going again to my toxic prompt

950
00:37:47,910 --> 00:37:49,800
because that's what I'm
serving for this index.

951
00:37:49,800 --> 00:37:51,360
But I block it immediately,

952
00:37:51,360 --> 00:37:53,670
because I recognize based
on the semantic content

953
00:37:53,670 --> 00:37:56,280
of what's going through
that it's inappropriate.

954
00:37:56,280 --> 00:37:58,380
It then triggers a policy violation.

955
00:37:58,380 --> 00:37:59,880
You can see the initial kind of component

956
00:37:59,880 --> 00:38:01,440
that would've gone through.

957
00:38:01,440 --> 00:38:03,690
Sorry, it's going so fast,
I can't even talk through it

958
00:38:03,690 --> 00:38:05,640
in real time, which is great.

959
00:38:05,640 --> 00:38:08,970
It's then going to appeal
to a default configuration

960
00:38:08,970 --> 00:38:11,220
that I'm serving right here as a fallback,

961
00:38:11,220 --> 00:38:13,500
which is just using Haiku
4.5 with a simple prompt;

962
00:38:13,500 --> 00:38:15,180
something that I really like.

963
00:38:15,180 --> 00:38:16,860
And then, ultimately it's returning to me

964
00:38:16,860 --> 00:38:18,780
a really nice response
that doesn't tell me

965
00:38:18,780 --> 00:38:20,820
to just ask my doctor for hydrocodone,

966
00:38:20,820 --> 00:38:22,710
and I'm getting all my metrics back,

967
00:38:22,710 --> 00:38:25,260
and my judges are happy
and satisfied, right?

968
00:38:25,260 --> 00:38:26,850
So, I was able to build
that in, which is great.

969
00:38:26,850 --> 00:38:28,350
So, I was able to insulate my customer

970
00:38:28,350 --> 00:38:31,290
from having a really, really,
really bad experience,

971
00:38:31,290 --> 00:38:34,770
which is wonderful, and I'm
gonna give it a thumbs up.

972
00:38:34,770 --> 00:38:36,150
But, I mean, that's a guardrail.

973
00:38:36,150 --> 00:38:37,620
We've all seen guardrails before.

974
00:38:37,620 --> 00:38:38,940
Guardrails are guardrails.

975
00:38:38,940 --> 00:38:42,180
So, what would it mean to
actually make something material

976
00:38:42,180 --> 00:38:44,790
happen with the agents
that I've constructed,

977
00:38:44,790 --> 00:38:46,800
based on that guardrail firing?

978
00:38:46,800 --> 00:38:49,500
So, that's what I would
love to show you next.

979
00:38:49,500 --> 00:38:54,500
So, this is what I'm gonna show you now.

980
00:38:55,290 --> 00:38:57,450
So, as that particular guardrail fired,

981
00:38:57,450 --> 00:38:59,820
as that brand-voice
agent did something bad,

982
00:38:59,820 --> 00:39:01,890
I'm gonna pass some metrics
back to LaunchDarkly

983
00:39:01,890 --> 00:39:04,650
that say, "Hey, this
was a bad experience."

984
00:39:04,650 --> 00:39:06,060
I'm not gonna let that customer actually

985
00:39:06,060 --> 00:39:08,070
have that bad experience
because my judge is able

986
00:39:08,070 --> 00:39:10,982
to represent that bad
experience sufficiently,

987
00:39:10,982 --> 00:39:12,810
to LaunchDarkly back.

988
00:39:12,810 --> 00:39:15,510
And then, I'm gonna build a release logic

989
00:39:15,510 --> 00:39:18,870
based on that bad
experience having happened,

990
00:39:18,870 --> 00:39:20,340
that ultimately makes a decision

991
00:39:20,340 --> 00:39:23,790
around what the model, or
the prompt or whatever,

992
00:39:23,790 --> 00:39:26,310
is that's going to be served
in that particular context.

993
00:39:26,310 --> 00:39:29,880
So, I'm gonna do essentially
a type of guarded rollout.

994
00:39:29,880 --> 00:39:31,710
So, I'll show you this here.

995
00:39:31,710 --> 00:39:34,410
This is what this looks like in practice.

996
00:39:34,410 --> 00:39:36,360
So, here's my commercial experience.

997
00:39:36,360 --> 00:39:39,120
Let's say I'm not actually
gonna serve a toxic prompt.

998
00:39:39,120 --> 00:39:41,160
Let's say I'm going to, instead, work

999
00:39:41,160 --> 00:39:42,930
to serve a cost-cutting prompt, right?

1000
00:39:42,930 --> 00:39:45,630
I wanna actually, this is
my commercial experience.

1001
00:39:45,630 --> 00:39:47,220
I'm working with Llama 4 here.

1002
00:39:47,220 --> 00:39:48,480
I had a simple prompt that worked.

1003
00:39:48,480 --> 00:39:50,580
I'm now gonna cut the cost even more.

1004
00:39:50,580 --> 00:39:52,110
I'm gonna try and serve
a cost-cutting prompt

1005
00:39:52,110 --> 00:39:54,150
that my experiments have validated works,

1006
00:39:54,150 --> 00:39:55,530
but it's AI.

1007
00:39:55,530 --> 00:39:56,760
I'm taking a risk,

1008
00:39:56,760 --> 00:39:58,890
and you know, I'm aware
that I'm taking a risk,

1009
00:39:58,890 --> 00:40:00,720
so I'm gonna place some
provisions to make sure

1010
00:40:00,720 --> 00:40:02,700
that that risk is taken seriously.

1011
00:40:02,700 --> 00:40:05,580
So, instead of just serving
this naively, I could place it

1012
00:40:05,580 --> 00:40:08,310
behind a kind of traditional
progressive rollout

1013
00:40:08,310 --> 00:40:10,020
and, you know, wait for
my customers to tell me

1014
00:40:10,020 --> 00:40:11,100
that they're having a bad time,

1015
00:40:11,100 --> 00:40:14,534
which seems to be kind of
the industry standard, yeah.

1016
00:40:14,534 --> 00:40:18,270
Or I can actually get
really aggressive with it.

1017
00:40:18,270 --> 00:40:21,450
So, here I'm going to
use a guarded rollout,

1018
00:40:21,450 --> 00:40:23,760
and what this lets me do is I'm gonna move

1019
00:40:23,760 --> 00:40:26,280
from my Llama 4 plus simple prompt,

1020
00:40:26,280 --> 00:40:28,650
to my Llama 4 plus my cost-cutting prompt,

1021
00:40:28,650 --> 00:40:31,440
and then I'm gonna guard
against some key metrics.

1022
00:40:31,440 --> 00:40:32,273
Now, these metrics

1023
00:40:32,273 --> 00:40:35,190
are actually beautifully
automatically provided to me

1024
00:40:35,190 --> 00:40:37,110
because LaunchDarkly knows who I am

1025
00:40:37,110 --> 00:40:38,580
and the things that I care about.

1026
00:40:38,580 --> 00:40:40,380
But you can add any metrics here.

1027
00:40:40,380 --> 00:40:42,360
Like, you can add time to first token.

1028
00:40:42,360 --> 00:40:43,823
I mean, you can see tons of
different metrics that you can.

1029
00:40:43,823 --> 00:40:45,450
You can add any metric, it doesn't matter.

1030
00:40:45,450 --> 00:40:47,550
You can add literally any metric for this.

1031
00:40:47,550 --> 00:40:49,590
But the metrics that are
automatically provided to me

1032
00:40:49,590 --> 00:40:50,970
make a lot of sense in this context.

1033
00:40:50,970 --> 00:40:53,100
They are accuracy; again, as determined

1034
00:40:53,100 --> 00:40:54,630
by that LLM as judge.

1035
00:40:54,630 --> 00:40:56,520
Error rate, so whether
or not the application

1036
00:40:56,520 --> 00:40:58,410
is actually continuing to function.

1037
00:40:58,410 --> 00:41:00,480
And the rate of customer resolution;

1038
00:41:00,480 --> 00:41:03,180
like, if I'm doing a
customer service chat bot

1039
00:41:03,180 --> 00:41:05,550
and I'm starting to see that
Zendesk tickets are piling up

1040
00:41:05,550 --> 00:41:07,650
as a result, I've done
something wrong, right?

1041
00:41:07,650 --> 00:41:09,720
And I would love to be
notified proactively,

1042
00:41:09,720 --> 00:41:11,430
not just that something
wrong had happened,

1043
00:41:11,430 --> 00:41:14,070
but that a automatic
rollback had occurred.

1044
00:41:14,070 --> 00:41:17,220
So, in this particular
case, if accuracy dips by 5%

1045
00:41:17,220 --> 00:41:19,590
over the course of a progressive rollout

1046
00:41:19,590 --> 00:41:21,600
within any of these particular caches,

1047
00:41:21,600 --> 00:41:24,510
well, automatically roll
it back, which is great.

1048
00:41:24,510 --> 00:41:27,930
And, this way, my engineering
team, my AI engineers

1049
00:41:27,930 --> 00:41:31,110
who are already tired
and burnt out on coffee,

1050
00:41:31,110 --> 00:41:32,550
they're not paged in
the middle of the night

1051
00:41:32,550 --> 00:41:34,260
with, like, "Hey, this bad experience

1052
00:41:34,260 --> 00:41:35,730
is, like, hitting our customer.

1053
00:41:35,730 --> 00:41:37,950
They're also not paged
with, "This bad experience

1054
00:41:37,950 --> 00:41:38,880
could have hit our customer,

1055
00:41:38,880 --> 00:41:40,830
but we have a guardrail in place,

1056
00:41:40,830 --> 00:41:42,000
so, you know, they're just hitting

1057
00:41:42,000 --> 00:41:43,260
the guardrail constantly."

1058
00:41:43,260 --> 00:41:45,570
Instead, they're paged
with, "Hey, that thing

1059
00:41:45,570 --> 00:41:47,400
you rolled out, it didn't work very well,

1060
00:41:47,400 --> 00:41:49,230
so we automatically rolled it back.

1061
00:41:49,230 --> 00:41:51,990
Here are all the traces
that you can use to dive in

1062
00:41:51,990 --> 00:41:53,820
and validate what exactly happened,

1063
00:41:53,820 --> 00:41:56,340
but you don't need to
buy time desperately,

1064
00:41:56,340 --> 00:41:57,870
you don't need to wake up immediately,

1065
00:41:57,870 --> 00:42:00,090
because already the
situation is under control,"

1066
00:42:00,090 --> 00:42:01,890
which is beautiful and great.

1067
00:42:01,890 --> 00:42:03,240
And just while I have this open,

1068
00:42:03,240 --> 00:42:04,790
I can show you this right here.

1069
00:42:05,880 --> 00:42:07,080
So, this is what this looks like

1070
00:42:07,080 --> 00:42:09,210
in terms of, like, live monitoring.

1071
00:42:09,210 --> 00:42:12,060
Again, I think you've put
this really well, Scarlett.

1072
00:42:12,060 --> 00:42:13,800
I think of monitoring and observability

1073
00:42:13,800 --> 00:42:17,190
as absolutely lagging indicators.

1074
00:42:17,190 --> 00:42:19,740
Really, really useful after
the fact to get an assessment

1075
00:42:19,740 --> 00:42:21,660
of how things actually have gone.

1076
00:42:21,660 --> 00:42:23,610
But they're not actually live

1077
00:42:23,610 --> 00:42:25,800
or actionable in the way
that the types of systems

1078
00:42:25,800 --> 00:42:26,940
that I'm describing are.

1079
00:42:26,940 --> 00:42:28,410
But regardless, they're really useful.

1080
00:42:28,410 --> 00:42:29,760
So, in this particular case, I'm able

1081
00:42:29,760 --> 00:42:31,740
to see, you know, how the various

1082
00:42:31,740 --> 00:42:34,410
different agent configurations
that I constructed perform,

1083
00:42:34,410 --> 00:42:35,610
how expensive they are.

1084
00:42:35,610 --> 00:42:36,443
Something that's really cool

1085
00:42:36,443 --> 00:42:38,220
is that all of this is context aware.

1086
00:42:38,220 --> 00:42:41,910
So, you can go through and
see which customer costs you

1087
00:42:41,910 --> 00:42:44,160
this or that amount of
money, too, which is cool.

1088
00:42:44,160 --> 00:42:45,330
And all of this is also itemized

1089
00:42:45,330 --> 00:42:48,063
by every single version
change that you've made.

1090
00:42:48,960 --> 00:42:49,890
But the thing that I really wanted

1091
00:42:49,890 --> 00:42:52,170
to show here was the traces, right?

1092
00:42:52,170 --> 00:42:53,880
So, again, your engineer got paged

1093
00:42:53,880 --> 00:42:56,280
that something problematic happened.

1094
00:42:56,280 --> 00:42:58,500
That problematic thing was contained,

1095
00:42:58,500 --> 00:42:59,940
it was automatically rolled back,

1096
00:42:59,940 --> 00:43:01,830
so there's no actual issue here.

1097
00:43:01,830 --> 00:43:03,270
And then, blam, they wake up,

1098
00:43:03,270 --> 00:43:05,880
and they get to dig into
the full set of traces

1099
00:43:05,880 --> 00:43:07,590
that were associated with that problem.

1100
00:43:07,590 --> 00:43:09,750
They can go through, and they can see,

1101
00:43:09,750 --> 00:43:12,360
like what was that
actual entire experience

1102
00:43:12,360 --> 00:43:14,670
with the customer, what actually happened,

1103
00:43:14,670 --> 00:43:17,850
and any of the associated
traces that go with there.

1104
00:43:17,850 --> 00:43:19,980
And also, something that's
cool is that LaunchDarkly

1105
00:43:19,980 --> 00:43:22,410
isn't an LLM observability company,

1106
00:43:22,410 --> 00:43:23,970
we're an observability company.

1107
00:43:23,970 --> 00:43:25,920
So, absolutely every other component

1108
00:43:25,920 --> 00:43:27,840
of your application will also go through

1109
00:43:27,840 --> 00:43:29,100
the same tracing logic.

1110
00:43:29,100 --> 00:43:31,050
So, you can see, "Well, the LLM failed

1111
00:43:31,050 --> 00:43:33,120
in this particular case," or
you can see another aspect

1112
00:43:33,120 --> 00:43:35,820
of the application failed, in
relation maybe to something

1113
00:43:35,820 --> 00:43:38,910
that happened on the LLM
side or something else.

1114
00:43:38,910 --> 00:43:41,520
So, that's, like, a
really quick harbor tour

1115
00:43:41,520 --> 00:43:42,990
of what it is that we offer here,

1116
00:43:42,990 --> 00:43:45,270
but I'd love to quickly kind of dial down

1117
00:43:45,270 --> 00:43:47,463
on that guarded releases story.

1118
00:43:48,870 --> 00:43:51,720
So, again, your engineer got paged

1119
00:43:51,720 --> 00:43:53,880
in the middle of the night,
automatic rollback happens,

1120
00:43:53,880 --> 00:43:55,620
so he can hit the snooze button.

1121
00:43:55,620 --> 00:43:57,060
And then, the next
morning he gets his traces

1122
00:43:57,060 --> 00:43:58,380
and he also gets this beautiful graph

1123
00:43:58,380 --> 00:44:00,030
which says, "Here's what happened."

1124
00:44:00,030 --> 00:44:02,760
We saw accuracy drift
over this period of time,

1125
00:44:02,760 --> 00:44:04,350
over this number of exposures.

1126
00:44:04,350 --> 00:44:06,450
We caught it at 5% of actual release,

1127
00:44:06,450 --> 00:44:09,570
so not a lot of actual customers got hit.

1128
00:44:09,570 --> 00:44:11,550
We ultimately saw an associated spike

1129
00:44:11,550 --> 00:44:12,570
and an error rate.

1130
00:44:12,570 --> 00:44:14,610
Whatever, you can see all the
metrics that you care about

1131
00:44:14,610 --> 00:44:16,320
as associated with that
progressive rollout

1132
00:44:16,320 --> 00:44:19,260
that you constructed to
stage that relationship,

1133
00:44:19,260 --> 00:44:20,940
to stage that transition between;

1134
00:44:20,940 --> 00:44:22,260
in this case, that simple prompt

1135
00:44:22,260 --> 00:44:23,880
and that cross-cutting prompt.

1136
00:44:23,880 --> 00:44:25,500
But again, it could be anything.

1137
00:44:25,500 --> 00:44:26,520
These could be different tools,

1138
00:44:26,520 --> 00:44:28,650
different model hyper
parameters, different models.

1139
00:44:28,650 --> 00:44:31,593
It really, any agent
parameter is containable here.

1140
00:44:32,670 --> 00:44:34,320
So, like, in terms of long-term vision,

1141
00:44:34,320 --> 00:44:35,648
what this actually looks like

1142
00:44:35,648 --> 00:44:38,130
it's really a story about confidence.

1143
00:44:38,130 --> 00:44:40,920
If I think about that MIT
study that happened in August

1144
00:44:40,920 --> 00:44:43,980
that said that, you know, only
5% of generative AI projects

1145
00:44:43,980 --> 00:44:46,470
are actually yielding some positive ROI,

1146
00:44:46,470 --> 00:44:49,410
one of the big bold things
that they said there

1147
00:44:49,410 --> 00:44:52,890
was that 5% is doing
something really, really well,

1148
00:44:52,890 --> 00:44:55,830
and that 95% is doing something
really, really poorly.

1149
00:44:55,830 --> 00:44:57,690
And that thing that
they're doing really poorly

1150
00:44:57,690 --> 00:45:02,100
is they're just releasing
a very bland, generic thing

1151
00:45:02,100 --> 00:45:05,970
that is totally disempowered, that is able

1152
00:45:05,970 --> 00:45:08,940
to do maybe some basic text
search and docs or whatever,

1153
00:45:08,940 --> 00:45:10,950
but it really isn't actually agentic

1154
00:45:10,950 --> 00:45:13,620
in the sense that it
doesn't really have agency.

1155
00:45:13,620 --> 00:45:17,070
So, they also said that,
when you start to do this,

1156
00:45:17,070 --> 00:45:18,450
a lot of companies get quite scared

1157
00:45:18,450 --> 00:45:20,520
of actually refining that thing,

1158
00:45:20,520 --> 00:45:21,960
which could mean give it more power,

1159
00:45:21,960 --> 00:45:23,580
or could mean make it better,

1160
00:45:23,580 --> 00:45:26,460
or could mean just steer it
more in the direction of ROI,

1161
00:45:26,460 --> 00:45:28,500
because people are
scared of making changes

1162
00:45:28,500 --> 00:45:31,590
to probabilistic systems,
especially in production.

1163
00:45:31,590 --> 00:45:33,060
But in this particular case,

1164
00:45:33,060 --> 00:45:35,520
if I change a model, and
it leads to some increase

1165
00:45:35,520 --> 00:45:39,060
in latency and some drop in
accuracy, well, I can catch it,

1166
00:45:39,060 --> 00:45:41,220
and I can automatically roll it back,

1167
00:45:41,220 --> 00:45:42,420
and then I can make another change.

1168
00:45:42,420 --> 00:45:44,670
So, I run some experiments,
and I determine

1169
00:45:44,670 --> 00:45:46,590
that, you know, this particular tool

1170
00:45:46,590 --> 00:45:49,440
that I'm giving this thing
access to is actually decreasing

1171
00:45:49,440 --> 00:45:50,820
my negative CSAT rate

1172
00:45:50,820 --> 00:45:53,400
and increasing some KPI that I care about.

1173
00:45:53,400 --> 00:45:55,170
Maybe it leads to a slight cost increase,

1174
00:45:55,170 --> 00:45:56,460
but I'm very happy with that.

1175
00:45:56,460 --> 00:45:59,880
I'm gonna commit this particular
species of this agent.

1176
00:45:59,880 --> 00:46:01,730
I'm gonna go further,
maybe changing a model

1177
00:46:01,730 --> 00:46:03,120
in this particular case.

1178
00:46:03,120 --> 00:46:05,310
Maybe it led to a slight accuracy dip,

1179
00:46:05,310 --> 00:46:06,960
but a huge cost decrease,

1180
00:46:06,960 --> 00:46:09,120
and that's something I'm
also comfortable with,

1181
00:46:09,120 --> 00:46:10,560
because, again, all of the testing

1182
00:46:10,560 --> 00:46:12,630
that we provide is highly multi-variable,

1183
00:46:12,630 --> 00:46:15,150
so you're able to actually
make informed scaled decisions

1184
00:46:15,150 --> 00:46:18,030
around the relationships
between the metrics

1185
00:46:18,030 --> 00:46:19,170
that you care about.

1186
00:46:19,170 --> 00:46:21,810
And then, finally, let's
say I changed the topology

1187
00:46:21,810 --> 00:46:23,820
of the agent that I'm building.

1188
00:46:23,820 --> 00:46:25,590
Everyone obviously should be experimenting

1189
00:46:25,590 --> 00:46:27,360
on different agent topologies, right?

1190
00:46:27,360 --> 00:46:29,820
I mean, the different ways that
agent graphs are structured

1191
00:46:29,820 --> 00:46:34,020
can yield to unbelievably huge
changes in net performance.

1192
00:46:34,020 --> 00:46:35,910
And so, in this case, let's say it led

1193
00:46:35,910 --> 00:46:38,010
to a huge, you know,
improvement in latency

1194
00:46:38,010 --> 00:46:41,190
and some increase in
associated retention metrics.

1195
00:46:41,190 --> 00:46:43,110
But again, what we're
giving you the ability to do

1196
00:46:43,110 --> 00:46:45,930
is to test not only different
configurations of an agent,

1197
00:46:45,930 --> 00:46:48,240
but also different topological structures.

1198
00:46:48,240 --> 00:46:50,130
And what this looks like in the abstract

1199
00:46:50,130 --> 00:46:51,840
is, first, like this, right,

1200
00:46:51,840 --> 00:46:53,190
giving you the ability to self recover.

1201
00:46:53,190 --> 00:46:55,650
Something bad happens, you
catch it with a guardrail,

1202
00:46:55,650 --> 00:46:58,440
you appeal to some known
good version of a thing,

1203
00:46:58,440 --> 00:47:00,990
a more expensive, more
powerful model to make sure

1204
00:47:00,990 --> 00:47:02,760
that the customer gets handed correctly,

1205
00:47:02,760 --> 00:47:04,530
and then metrics go back to LaunchDarkly

1206
00:47:04,530 --> 00:47:07,530
to prevent that thing from
happening consistently.

1207
00:47:07,530 --> 00:47:09,630
But then, as I was working on this demo,

1208
00:47:09,630 --> 00:47:10,950
I kept having this experience

1209
00:47:10,950 --> 00:47:15,210
where I would take the experiments
that I run, ingest them,

1210
00:47:15,210 --> 00:47:17,130
you know, via LaunchDarkly API.

1211
00:47:17,130 --> 00:47:20,190
I'd send that stuff straight
to Cursor or to Claude Code

1212
00:47:20,190 --> 00:47:22,830
and say, "Hey, this is the
result of this experiment.

1213
00:47:22,830 --> 00:47:25,290
Generate 50 different
versions of this prompt

1214
00:47:25,290 --> 00:47:27,090
based on the results of this experiment,

1215
00:47:27,090 --> 00:47:28,200
and then run the experiment again

1216
00:47:28,200 --> 00:47:29,760
using LaunchDarkly as MCP."

1217
00:47:29,760 --> 00:47:32,280
And then, all of a sudden,
I started to build a system

1218
00:47:32,280 --> 00:47:34,590
that was improving itself directionally

1219
00:47:34,590 --> 00:47:36,900
towards the actual metrics and the KPIs

1220
00:47:36,900 --> 00:47:38,250
that I really cared about.

1221
00:47:38,250 --> 00:47:40,500
And I'm really, really excited about that.

1222
00:47:40,500 --> 00:47:41,703
Yeah, that's it for me.

1223
00:47:44,100 --> 00:47:46,470
- Okay, so we have talked
through the concepts,

1224
00:47:46,470 --> 00:47:47,580
and you've seen the demo,

1225
00:47:47,580 --> 00:47:48,750
but I wanna talk a little bit

1226
00:47:48,750 --> 00:47:51,090
about what this looks
like in the real world.

1227
00:47:51,090 --> 00:47:53,070
So, Relay Networks is one of our customers

1228
00:47:53,070 --> 00:47:55,350
and is actually a great example.

1229
00:47:55,350 --> 00:47:57,630
They work in one of the most
highly-regulated industries

1230
00:47:57,630 --> 00:47:59,580
imaginable; they do
healthcare communications,

1231
00:47:59,580 --> 00:48:02,880
so they have very strict HIPAA
and high trust requirements.

1232
00:48:02,880 --> 00:48:06,120
And using LaunchDarkly AI
configs, they were able

1233
00:48:06,120 --> 00:48:08,340
to roll out a secure
and compliant solution

1234
00:48:08,340 --> 00:48:10,110
in just three weeks.

1235
00:48:10,110 --> 00:48:11,490
They were also having this problem

1236
00:48:11,490 --> 00:48:13,560
where they had to get engineering involved

1237
00:48:13,560 --> 00:48:15,630
anytime they wanted to
make a prompt change.

1238
00:48:15,630 --> 00:48:18,570
So, their product team
is really slowed down

1239
00:48:18,570 --> 00:48:20,220
by their ability to run experiments

1240
00:48:20,220 --> 00:48:21,870
and test out new prompts.

1241
00:48:21,870 --> 00:48:24,840
Now, they can make
prompt changes in minutes

1242
00:48:24,840 --> 00:48:26,280
instead of days.

1243
00:48:26,280 --> 00:48:28,890
And I just wanna highlight,
this is the unlock

1244
00:48:28,890 --> 00:48:30,270
that we've been talking about, right;

1245
00:48:30,270 --> 00:48:33,360
separating your deployments
from your configurations,

1246
00:48:33,360 --> 00:48:37,020
and giving your product
teams the ability to iterate

1247
00:48:37,020 --> 00:48:38,550
and really improve that without requiring

1248
00:48:38,550 --> 00:48:40,413
a lot of engineering time.

1249
00:48:41,880 --> 00:48:44,610
Another one of our clients, Hireology,

1250
00:48:44,610 --> 00:48:46,470
they work in the HR tech space,

1251
00:48:46,470 --> 00:48:49,350
and they have a job description bot,

1252
00:48:49,350 --> 00:48:51,600
and they similarly had an issue

1253
00:48:51,600 --> 00:48:53,550
where they had to coordinate the schedules

1254
00:48:53,550 --> 00:48:56,670
of multiple departments just to test out

1255
00:48:56,670 --> 00:48:58,380
different prompts and iterate.

1256
00:48:58,380 --> 00:48:59,760
They had no real way

1257
00:48:59,760 --> 00:49:01,950
to measure, like, quality control metrics

1258
00:49:01,950 --> 00:49:04,196
across all of the
verticals that they serve.

1259
00:49:04,196 --> 00:49:08,970
And now, they're able to
launch multiple deployments

1260
00:49:08,970 --> 00:49:10,314
every day.

1261
00:49:10,314 --> 00:49:14,370
They're serving different models
to different user segments

1262
00:49:14,370 --> 00:49:16,560
with automatic testing.

1263
00:49:16,560 --> 00:49:21,560
They can iterate hourly and
get those results quickly.

1264
00:49:24,961 --> 00:49:27,360
And then, we just finally wanna leave you

1265
00:49:27,360 --> 00:49:29,670
with some QR codes.

1266
00:49:29,670 --> 00:49:33,270
So, we've talked through why
traditional software deployment

1267
00:49:33,270 --> 00:49:37,080
techniques fail when it comes
to AI and how LaunchDarkly

1268
00:49:37,080 --> 00:49:39,840
offers that unlock, so that
you can start shipping your AI

1269
00:49:39,840 --> 00:49:42,270
with confidence instead of with anxiety.

1270
00:49:42,270 --> 00:49:44,190
So, the first link on the left here,

1271
00:49:44,190 --> 00:49:45,330
that's our quickstart guide;

1272
00:49:45,330 --> 00:49:46,650
that's a great place to get started.

1273
00:49:46,650 --> 00:49:48,300
It also links to our docs.

1274
00:49:48,300 --> 00:49:49,980
So, that will walk you
through how to set up

1275
00:49:49,980 --> 00:49:52,560
your guarded releases, how
to set up your configs,

1276
00:49:52,560 --> 00:49:54,330
how to get started.

1277
00:49:54,330 --> 00:49:56,670
The second link is a
tutorial I put together

1278
00:49:56,670 --> 00:49:58,950
that uses LangGraph graph
in a multi-agentic system

1279
00:49:58,950 --> 00:50:01,230
to see how that interacts
with LaunchDarkly.

1280
00:50:01,230 --> 00:50:03,660
And finally, you can go to the third one

1281
00:50:03,660 --> 00:50:04,860
to start a free trial

1282
00:50:04,860 --> 00:50:07,140
and maybe get a demo with our sales team,

1283
00:50:07,140 --> 00:50:08,730
so they can dive into your use case

1284
00:50:08,730 --> 00:50:10,110
a little bit more deeply.

1285
00:50:10,110 --> 00:50:13,050
And you can also start
playing with LaunchDarkly

1286
00:50:13,050 --> 00:50:14,760
on your systems.

1287
00:50:14,760 --> 00:50:18,042
So, I think we have some time, maybe.

1288
00:50:18,042 --> 00:50:20,040
- Yeah, I would say, you
know, we're not gonna take

1289
00:50:20,040 --> 00:50:22,440
questions here, but we'd love
to meet you out in the hallway

1290
00:50:22,440 --> 00:50:24,870
and if you have any questions or comments

1291
00:50:24,870 --> 00:50:28,140
or observations from your own
perspective in the industry,

1292
00:50:28,140 --> 00:50:31,110
or any responses to my
bitter provocations,

1293
00:50:31,110 --> 00:50:33,660
I'd love to hear it, and
I'll go pack up my stuff

1294
00:50:33,660 --> 00:50:35,280
and hang out in the hall, I guess.

1295
00:50:35,280 --> 00:50:36,113
Thank you so much.

1296
00:50:36,113 --> 00:50:37,140
Thank you so much for joining us. Yeah.

1297
00:50:37,140 --> 00:50:38,315
- Thanks.

1298
00:50:38,315 --> 00:50:39,876
(audience clapping)

