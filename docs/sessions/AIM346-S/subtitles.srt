1
00:00:03,960 --> 00:00:05,550
- Hello everybody.

2
00:00:05,550 --> 00:00:09,090
Thank you very much for
coming all the way over

3
00:00:09,090 --> 00:00:13,233
to Mandalay Bay, to
hear me talk about logs.

4
00:00:14,640 --> 00:00:17,400
Now, this presentation

5
00:00:17,400 --> 00:00:22,290
partly is a story about what's
really possible with AI.

6
00:00:22,290 --> 00:00:26,283
And this picture here was
actually generated by Sora.

7
00:00:27,630 --> 00:00:30,600
It's actually me at the 1966 World Cup,

8
00:00:30,600 --> 00:00:33,746
which I am far too young
to have been a part of.

9
00:00:33,746 --> 00:00:35,370
(David laughing)

10
00:00:35,370 --> 00:00:38,400
Witnessing England's winning goal,

11
00:00:38,400 --> 00:00:40,830
which is the only time
we've ever seen success

12
00:00:40,830 --> 00:00:42,420
in a sport we invented, right?

13
00:00:42,420 --> 00:00:45,510
So, (David laughing)

14
00:00:45,510 --> 00:00:46,860
All right.

15
00:00:46,860 --> 00:00:49,590
So let's talk a little
bit about the problem

16
00:00:49,590 --> 00:00:51,513
that people are experiencing today.

17
00:00:53,340 --> 00:00:57,570
Observability today has been
laid down with a foundation

18
00:00:57,570 --> 00:01:00,450
of data collection,

19
00:01:00,450 --> 00:01:03,150
proprietary agents,

20
00:01:03,150 --> 00:01:08,150
and in a lot of ways we've
come to rely on dashboards

21
00:01:09,000 --> 00:01:11,340
and visualizations,

22
00:01:11,340 --> 00:01:15,090
but we're struggling to get answers.

23
00:01:15,090 --> 00:01:17,940
And we have a lot of
alerts out there today,

24
00:01:17,940 --> 00:01:20,250
also without context, right?

25
00:01:20,250 --> 00:01:24,030
So, you know, I get an alert
about some CPU usage issue

26
00:01:24,030 --> 00:01:25,200
on one of my boxes,

27
00:01:25,200 --> 00:01:29,310
or my Kubernetes cluster pods
are running out of memory.

28
00:01:29,310 --> 00:01:32,730
I get the alert, I just
don't have a lot of context,

29
00:01:32,730 --> 00:01:34,330
I don't know what to do with it.

30
00:01:35,520 --> 00:01:36,600
And this is a challenge

31
00:01:36,600 --> 00:01:40,413
that we see our customers
struggle with every day.

32
00:01:41,700 --> 00:01:45,720
And even when, you know,
you're looking at the outcomes,

33
00:01:45,720 --> 00:01:47,190
a lot of time

34
00:01:47,190 --> 00:01:52,190
is spent working on putting
together the data pipelines.

35
00:01:52,230 --> 00:01:56,130
Just to get the data into
the observability solution,

36
00:01:56,130 --> 00:02:01,130
you have to create lots of log pipelines,

37
00:02:01,170 --> 00:02:04,560
maybe you use a log stash
or something like that,

38
00:02:04,560 --> 00:02:05,970
and it takes a lot of work

39
00:02:05,970 --> 00:02:08,400
and a lot of data gets
lost during this process.

40
00:02:08,400 --> 00:02:10,620
So I talk to customers all the time

41
00:02:10,620 --> 00:02:12,457
and they say to me things like,

42
00:02:12,457 --> 00:02:13,533
"You know oh,

43
00:02:14,610 --> 00:02:15,750
half of the logs

44
00:02:15,750 --> 00:02:18,630
that I'm trying to get
into my solution here

45
00:02:18,630 --> 00:02:20,970
are just being dropped on the floor."

46
00:02:20,970 --> 00:02:23,580
And if you don't have the information,

47
00:02:23,580 --> 00:02:25,510
then you spend a lot of time

48
00:02:26,670 --> 00:02:29,370
wrestling with getting answers

49
00:02:29,370 --> 00:02:30,660
to the problems that you have,

50
00:02:30,660 --> 00:02:33,720
which increases the amount of downtime,

51
00:02:33,720 --> 00:02:38,670
customers are experiencing slow systems.

52
00:02:38,670 --> 00:02:42,183
I was trying to get onto
Claude earlier today actually,

53
00:02:43,212 --> 00:02:45,780
and it was currently down.

54
00:02:45,780 --> 00:02:49,350
So, you know, what do we do
while customers are suffering

55
00:02:49,350 --> 00:02:50,520
with these problems, right?

56
00:02:50,520 --> 00:02:53,280
We need to get things up and running again

57
00:02:53,280 --> 00:02:54,963
and get people back online.

58
00:02:56,340 --> 00:03:00,963
And part of the problem here,
is the why has been missing.

59
00:03:03,090 --> 00:03:05,733
I was an SRE long time ago,

60
00:03:06,870 --> 00:03:11,640
and you know, I used to work
with logs on a regular basis

61
00:03:11,640 --> 00:03:14,700
to figure out what's
wrong with my systems.

62
00:03:14,700 --> 00:03:17,760
And you know, back in those days,

63
00:03:17,760 --> 00:03:21,870
we only had five or six different servers,

64
00:03:21,870 --> 00:03:23,370
and I had a notepad file

65
00:03:23,370 --> 00:03:25,620
with a list of Grep and set patterns

66
00:03:25,620 --> 00:03:30,060
that we'd come up with together as a team.

67
00:03:30,060 --> 00:03:32,370
I copy and paste those into my terminal

68
00:03:32,370 --> 00:03:34,833
and try to find the logs that way.

69
00:03:35,790 --> 00:03:37,860
And then over time,

70
00:03:37,860 --> 00:03:42,030
we built systems that were
became very, very complex, right?

71
00:03:42,030 --> 00:03:44,943
Microservices is everywhere,
Kubernetes everything.

72
00:03:45,900 --> 00:03:50,670
We've basically built
enormously complex systems.

73
00:03:50,670 --> 00:03:54,690
And as a result of that, the
industry has been telling you

74
00:03:54,690 --> 00:03:59,460
to throw your logs away,
turn them into metrics,

75
00:03:59,460 --> 00:04:02,460
or put all that information into traces,

76
00:04:02,460 --> 00:04:04,230
but you can't rely on that, right?

77
00:04:04,230 --> 00:04:07,980
Because you can't instrument
everything with traces.

78
00:04:07,980 --> 00:04:10,620
You know, I think I looked
the other day at a survey

79
00:04:10,620 --> 00:04:13,350
that an observability
vendor had put out there,

80
00:04:13,350 --> 00:04:16,530
you can get to 50 or 60%

81
00:04:16,530 --> 00:04:19,020
of traces deployed out
in your environment,

82
00:04:19,020 --> 00:04:21,210
what do you do with the rest of it?

83
00:04:21,210 --> 00:04:25,470
So we shouldn't be throwing
away all this log data,

84
00:04:25,470 --> 00:04:28,440
we should just be being more intentional

85
00:04:28,440 --> 00:04:29,940
and more intelligent about it.

86
00:04:31,770 --> 00:04:34,470
So why aren't SREs using logs more?

87
00:04:34,470 --> 00:04:36,570
Because there's some great
information in there.

88
00:04:36,570 --> 00:04:40,140
And like I said, every time
I've solved the problem,

89
00:04:40,140 --> 00:04:42,453
it usually has come down to the logs.

90
00:04:43,920 --> 00:04:46,770
Well, they're messy and
unstructured, right?

91
00:04:46,770 --> 00:04:49,500
So, you know, the information in there,

92
00:04:49,500 --> 00:04:52,743
sometimes a little bit messy.

93
00:04:54,180 --> 00:04:58,290
Sometimes you can't do
analytics on that data

94
00:04:58,290 --> 00:04:59,883
because it's unstructured.

95
00:05:00,750 --> 00:05:02,640
You have to pause it first,

96
00:05:02,640 --> 00:05:05,730
which goes back to the pipeline
problem I was talking about.

97
00:05:05,730 --> 00:05:08,130
You need to write all
these grok expressions

98
00:05:08,130 --> 00:05:09,783
or regular expressions.

99
00:05:10,620 --> 00:05:14,160
Actually did a search

100
00:05:14,160 --> 00:05:14,993
and looked up

101
00:05:14,993 --> 00:05:18,240
like what was the most
complicated regex expression.

102
00:05:18,240 --> 00:05:19,770
And the first result on Google

103
00:05:19,770 --> 00:05:22,890
is somebody trying to use regex
to parse a log line, right?

104
00:05:22,890 --> 00:05:24,540
Which I thought was pretty funny.

105
00:05:26,220 --> 00:05:27,660
And there's a lot of logs, right?

106
00:05:27,660 --> 00:05:30,063
Millions, millions of logs or more.

107
00:05:31,080 --> 00:05:32,730
It's volumous,

108
00:05:32,730 --> 00:05:35,913
the most volumous observability
signal that we have.

109
00:05:37,620 --> 00:05:40,590
And finally, there's all sorts of logs

110
00:05:40,590 --> 00:05:43,080
from all over the place,
different systems.

111
00:05:43,080 --> 00:05:45,870
And that problem has gotten a lot worse.

112
00:05:45,870 --> 00:05:49,440
Again, when I was doing the SRE role

113
00:05:49,440 --> 00:05:51,333
about 10 years ago,

114
00:05:52,470 --> 00:05:55,830
things were starting to
get very complicated,

115
00:05:55,830 --> 00:06:00,830
but we still had a
manageable number of systems.

116
00:06:01,350 --> 00:06:06,300
We didn't have like a million
different AWS services,

117
00:06:06,300 --> 00:06:08,673
for example, sending us data,

118
00:06:09,510 --> 00:06:12,600
all these different SaaS
applications that we have now,

119
00:06:12,600 --> 00:06:14,250
it was kind of manageable.

120
00:06:14,250 --> 00:06:19,083
And every single system out
there has a different format.

121
00:06:21,390 --> 00:06:23,730
And it's a shame that we
can't rely on logs more

122
00:06:23,730 --> 00:06:26,310
because it's easy to collect logs.

123
00:06:26,310 --> 00:06:29,820
Even SaaS applications,
typically output logs.

124
00:06:32,040 --> 00:06:34,770
So you can get logs from
just about everywhere,

125
00:06:34,770 --> 00:06:38,790
even, you know, mainframes, put out logs,

126
00:06:38,790 --> 00:06:41,883
ATM machines, everything emits a log.

127
00:06:42,720 --> 00:06:44,010
And it has a log,

128
00:06:44,010 --> 00:06:47,370
logs typically have a
lot of great context.

129
00:06:47,370 --> 00:06:51,450
They were written for
humans to consume typically.

130
00:06:51,450 --> 00:06:56,450
And so, you know, things
like user IDs, explanations,

131
00:06:56,640 --> 00:06:58,410
you know, you can read them,

132
00:06:58,410 --> 00:07:00,240
they're like a novel in some ways, right?

133
00:07:00,240 --> 00:07:02,130
You can actually read logs

134
00:07:02,130 --> 00:07:04,680
if they're well written, of course,

135
00:07:04,680 --> 00:07:07,140
and you can get a lot of
really good information

136
00:07:07,140 --> 00:07:08,910
like you can see on the screen here,

137
00:07:08,910 --> 00:07:12,153
the server restarted a host for example.

138
00:07:14,993 --> 00:07:16,950
But what's needed to manage logs well?

139
00:07:16,950 --> 00:07:20,640
Well first of all, you
need a scalable platform

140
00:07:20,640 --> 00:07:24,003
because we've got a lot of logs out there,

141
00:07:25,080 --> 00:07:27,630
and you need to be able to,

142
00:07:27,630 --> 00:07:29,430
you need a platform that can handle

143
00:07:29,430 --> 00:07:31,080
all of that at scale, right?

144
00:07:31,080 --> 00:07:33,660
So, you know, if you want to write a query

145
00:07:33,660 --> 00:07:35,370
or you've got a dashboard,

146
00:07:35,370 --> 00:07:38,250
you want to be able to have
those things updating quickly.

147
00:07:38,250 --> 00:07:41,130
You don't want to have to
wait for hours and hours

148
00:07:41,130 --> 00:07:42,780
to get the results that you need

149
00:07:42,780 --> 00:07:45,450
when you're trying to solve a problem.

150
00:07:45,450 --> 00:07:46,830
And you need to be able to store

151
00:07:46,830 --> 00:07:49,470
this stuff fairly cost effectively.

152
00:07:49,470 --> 00:07:51,990
If you want to keep all your log data,

153
00:07:51,990 --> 00:07:53,820
instead of throwing it away,

154
00:07:53,820 --> 00:07:56,913
it needs to be in a way
that's cost effective.

155
00:07:58,050 --> 00:08:01,320
And so that's what you
need on the platform side.

156
00:08:01,320 --> 00:08:06,210
On the other side, we can
use AI-driven analysis

157
00:08:06,210 --> 00:08:10,320
to extract incredible insights from logs,

158
00:08:10,320 --> 00:08:13,623
and we'll talk a little bit
more about that later on.

159
00:08:14,730 --> 00:08:16,380
But first of all,

160
00:08:16,380 --> 00:08:17,213
let's talk a little bit

161
00:08:17,213 --> 00:08:20,760
about what Elastic does
well already today.

162
00:08:20,760 --> 00:08:23,910
We have a lot of integrations
out there in Elastic.

163
00:08:23,910 --> 00:08:26,640
So we can, you know, consume logs

164
00:08:26,640 --> 00:08:30,060
from a huge variety of different sources.

165
00:08:30,060 --> 00:08:32,400
The platform is very, very scalable.

166
00:08:32,400 --> 00:08:35,220
Customers tell me all the time

167
00:08:35,220 --> 00:08:39,270
that they moved to Elastic
or they adopted Elastic

168
00:08:39,270 --> 00:08:41,700
and they were very, very
impressed with the performance

169
00:08:41,700 --> 00:08:43,140
of the platform.

170
00:08:43,140 --> 00:08:45,390
They can get queries back in half the time

171
00:08:45,390 --> 00:08:47,100
that they could before,

172
00:08:47,100 --> 00:08:50,280
they can get information
on their dashboards

173
00:08:50,280 --> 00:08:52,680
and they can actually see
the information quickly

174
00:08:52,680 --> 00:08:55,530
without having to wait hours
for the dashboards to load.

175
00:08:55,530 --> 00:08:59,670
Elastic does these things very, very well,

176
00:08:59,670 --> 00:09:04,670
and we also have good
ML-driven patent analysis,

177
00:09:05,280 --> 00:09:07,980
classic ML algorithms, anomaly detection,

178
00:09:07,980 --> 00:09:10,380
patent analysis, things like that.

179
00:09:10,380 --> 00:09:13,650
We can do really great
search and filter things.

180
00:09:13,650 --> 00:09:16,650
We've had this new query
language called ES|QL.

181
00:09:16,650 --> 00:09:19,470
If you haven't seen it
already, check it out.

182
00:09:19,470 --> 00:09:21,570
We keep adding stuff to that all the time,

183
00:09:21,570 --> 00:09:24,630
we just GA joins, which is great.

184
00:09:24,630 --> 00:09:27,900
So now you can join Data
and Elastic with ES|QL

185
00:09:27,900 --> 00:09:30,480
and we keep adding ML functions

186
00:09:30,480 --> 00:09:32,460
to ES|QL as well, which is great.

187
00:09:32,460 --> 00:09:37,460
So you can actually query a
lot of data in Elastic now,

188
00:09:37,590 --> 00:09:39,990
and inside the query language,

189
00:09:39,990 --> 00:09:42,570
you can send the result to an LLM

190
00:09:42,570 --> 00:09:45,600
and get it to have a
look at what the issue

191
00:09:45,600 --> 00:09:48,540
is from the ES|QL query directly using

192
00:09:48,540 --> 00:09:49,710
the piped query language.

193
00:09:49,710 --> 00:09:53,250
You just pipe it to the LLM,
have the LLM do the analysis

194
00:09:53,250 --> 00:09:55,710
and it comes right back into your results.

195
00:09:55,710 --> 00:09:56,730
Really fantastic.

196
00:09:56,730 --> 00:10:01,730
So ES|QL has some very, very
powerful analytic capabilities.

197
00:10:05,280 --> 00:10:07,653
So how do we store log data efficiently?

198
00:10:08,700 --> 00:10:12,810
We recently released LogsDB index mode.

199
00:10:12,810 --> 00:10:15,243
And the way it works is, is it does,

200
00:10:16,590 --> 00:10:18,270
one of the main ways that it works

201
00:10:18,270 --> 00:10:21,810
is that we do some additional
compression on logs.

202
00:10:21,810 --> 00:10:23,880
We use index sorting,

203
00:10:23,880 --> 00:10:26,940
we also use a technique
called Synthetic Source.

204
00:10:26,940 --> 00:10:29,160
So now we're actually storing logs

205
00:10:29,160 --> 00:10:31,920
in a columnar data format.

206
00:10:31,920 --> 00:10:35,400
We can save a lot of disc space this way.

207
00:10:35,400 --> 00:10:40,080
In fact, many of our
customers are seeing up to 70%

208
00:10:40,080 --> 00:10:44,673
of their data storage costs
have gone down using LogsDB.

209
00:10:46,710 --> 00:10:49,350
And so, you know, that's one way

210
00:10:49,350 --> 00:10:52,950
we're making it so that customers
can store all their logs

211
00:10:52,950 --> 00:10:55,290
without having to throw them away.

212
00:10:55,290 --> 00:10:57,450
And we're going even further than that.

213
00:10:57,450 --> 00:10:59,400
We haven't actually released

214
00:10:59,400 --> 00:11:02,010
this particular part of Elastic yet,

215
00:11:02,010 --> 00:11:04,170
but we we're currently working

216
00:11:04,170 --> 00:11:06,270
on a compressed log processor as well.

217
00:11:06,270 --> 00:11:08,070
So if you've got repeated log lines,

218
00:11:08,070 --> 00:11:10,710
like you can see in the slide here,

219
00:11:10,710 --> 00:11:12,600
we'll template those, right?

220
00:11:12,600 --> 00:11:16,470
So we only store the bit of
the log line that changes.

221
00:11:16,470 --> 00:11:18,540
So we can save you another 50%

222
00:11:18,540 --> 00:11:21,360
in storage using this technique.

223
00:11:21,360 --> 00:11:24,360
And this is completely transparent to you,

224
00:11:24,360 --> 00:11:26,260
it's just happening in the background.

225
00:11:27,990 --> 00:11:32,850
So we have a great platform,
you can store all your logs,

226
00:11:32,850 --> 00:11:37,320
you can, you know, manually
analyze the data really nicely

227
00:11:37,320 --> 00:11:40,710
with a lot of the analytic
functions that we provide.

228
00:11:40,710 --> 00:11:42,790
If we take all of that power

229
00:11:43,650 --> 00:11:46,560
and we bring LLMs into the picture,

230
00:11:46,560 --> 00:11:48,270
we can do a lot more,

231
00:11:48,270 --> 00:11:50,940
we can do some really great things

232
00:11:50,940 --> 00:11:53,130
with all this log data that you have.

233
00:11:53,130 --> 00:11:56,580
That means that you don't have
to go hunting and pecking,

234
00:11:56,580 --> 00:11:58,593
you get the insights delivered to you.

235
00:11:59,700 --> 00:12:01,470
And if we do that,

236
00:12:01,470 --> 00:12:04,260
and give you those
actionable capabilities,

237
00:12:04,260 --> 00:12:07,560
rather than just having to
troll through all this data,

238
00:12:07,560 --> 00:12:10,950
you actually get insights
that you can action on,

239
00:12:10,950 --> 00:12:14,010
then we've actually
created something magical

240
00:12:14,010 --> 00:12:14,853
for you, right?

241
00:12:17,340 --> 00:12:20,487
So we already talked about how we store

242
00:12:20,487 --> 00:12:22,083
the data efficiently.

243
00:12:23,670 --> 00:12:27,780
Once we have the data stored
efficiently in Elastic,

244
00:12:27,780 --> 00:12:29,080
the first thing we do

245
00:12:30,150 --> 00:12:33,660
with this new AI-powered
log analytics engine

246
00:12:33,660 --> 00:12:36,360
that we're calling streams,

247
00:12:36,360 --> 00:12:38,850
is we identify the systems.

248
00:12:38,850 --> 00:12:42,900
So we send the logs into
Elastic, store them efficiently,

249
00:12:42,900 --> 00:12:46,420
and then the first thing we
do is we look at those logs

250
00:12:47,970 --> 00:12:50,400
with the assistance of an LLM

251
00:12:50,400 --> 00:12:52,563
to understand what systems they come from.

252
00:12:53,640 --> 00:12:55,260
And you may have tried this yourself.

253
00:12:55,260 --> 00:12:56,580
I've certainly tried it myself

254
00:12:56,580 --> 00:12:59,100
and it works phenomenally well,

255
00:12:59,100 --> 00:13:03,720
which is take a log line,
chuck it into an LLM.

256
00:13:03,720 --> 00:13:06,510
Quite often the LLM
will be able to tell you

257
00:13:06,510 --> 00:13:09,630
where that log came from,
what system it came from.

258
00:13:09,630 --> 00:13:12,780
It'll do some really good
analysis on the logs,

259
00:13:12,780 --> 00:13:14,853
might even help you with the root cause.

260
00:13:15,810 --> 00:13:19,530
So we can usually tell
just by looking at the logs

261
00:13:19,530 --> 00:13:22,200
or a sample of the logs,
not just one log line,

262
00:13:22,200 --> 00:13:24,510
but you know, we get a sample of the logs,

263
00:13:24,510 --> 00:13:27,180
we can tell what system
it comes from, right?

264
00:13:27,180 --> 00:13:29,610
So we can identify if it's Elasticsearch

265
00:13:29,610 --> 00:13:32,013
or Nginx per this example.

266
00:13:33,840 --> 00:13:37,140
Once we've identified
what system it comes from,

267
00:13:37,140 --> 00:13:40,710
we can then start to
partition them automatically.

268
00:13:40,710 --> 00:13:43,500
So we can send Hadoop blogs for example,

269
00:13:43,500 --> 00:13:46,050
into a special Hadoop index.

270
00:13:46,050 --> 00:13:49,440
We can send Spark logs
to a special Spark index.

271
00:13:49,440 --> 00:13:51,210
Or if you have Java applications

272
00:13:51,210 --> 00:13:52,980
and you've got an order
processing service,

273
00:13:52,980 --> 00:13:56,250
we identify that it's an
order processing service

274
00:13:56,250 --> 00:14:01,250
and we send those logs into
an order processing index.

275
00:14:02,130 --> 00:14:04,410
So now we've got your logs organized

276
00:14:04,410 --> 00:14:06,813
and this all happens automatically.

277
00:14:08,460 --> 00:14:10,980
You can see here,

278
00:14:10,980 --> 00:14:14,220
an example of how we organize that data.

279
00:14:14,220 --> 00:14:15,720
So it's not just going,

280
00:14:15,720 --> 00:14:18,240
one level deep like I described here.

281
00:14:18,240 --> 00:14:19,950
You can go multiple levels down.

282
00:14:19,950 --> 00:14:23,400
So if you've identified an
order processing system,

283
00:14:23,400 --> 00:14:25,080
you might wanna separate error logs

284
00:14:25,080 --> 00:14:28,590
from info logs for example, or
you may want to go higher up,

285
00:14:28,590 --> 00:14:30,780
you might wanna bucket
everything under Kubernetes

286
00:14:30,780 --> 00:14:32,160
and the things that are under (indistinct)

287
00:14:32,160 --> 00:14:37,160
So we have a hierarchical
organization system here in play,

288
00:14:37,260 --> 00:14:41,913
that enables you to organize
your logs, how you see fit,

289
00:14:42,750 --> 00:14:44,220
and like I said,

290
00:14:44,220 --> 00:14:47,613
using AI assistance to help you.

291
00:14:50,280 --> 00:14:51,113
So once

292
00:14:52,771 --> 00:14:53,880
we've got the systems,

293
00:14:53,880 --> 00:14:55,530
we've identified those,

294
00:14:55,530 --> 00:14:59,820
we've partitioned the data
into sensible streams,

295
00:14:59,820 --> 00:15:03,990
we then want to extract
meaning from your logs.

296
00:15:03,990 --> 00:15:07,500
So like at the moment most people

297
00:15:07,500 --> 00:15:10,320
are still using unstructured logs, right?

298
00:15:10,320 --> 00:15:13,260
I would say a good 90% of the logs

299
00:15:13,260 --> 00:15:16,830
I'm still seeing out there
are completely unstructured,

300
00:15:16,830 --> 00:15:20,193
and that is not good for
our analytics, right?

301
00:15:21,510 --> 00:15:22,770
So the next thing you wanna do

302
00:15:22,770 --> 00:15:25,890
is take that unstructured
data and structure it.

303
00:15:25,890 --> 00:15:28,200
So that's the next thing that we wanna do

304
00:15:28,200 --> 00:15:30,063
with our data processing pipeline.

305
00:15:31,680 --> 00:15:35,130
Once we've finally done all of that,

306
00:15:35,130 --> 00:15:37,680
we've actually processed your logs,

307
00:15:37,680 --> 00:15:40,860
structured them, made
it easy for analytics.

308
00:15:40,860 --> 00:15:44,310
The next thing we look for
is any significant events.

309
00:15:44,310 --> 00:15:45,330
So like I said, you know,

310
00:15:45,330 --> 00:15:48,000
we know if it's an order
processing service,

311
00:15:48,000 --> 00:15:51,660
we maybe know if it's
hosted on Kubernetes,

312
00:15:51,660 --> 00:15:53,730
we might know if it's a Spark system,

313
00:15:53,730 --> 00:15:55,500
whatever it happens to be,

314
00:15:55,500 --> 00:15:58,950
what we then do is with the
assistance again of an LLM,

315
00:15:58,950 --> 00:16:03,360
we say what kinds of problems
am I likely to encounter

316
00:16:03,360 --> 00:16:05,250
with this particular system?

317
00:16:05,250 --> 00:16:08,460
And here are some samples
of some logs as well.

318
00:16:08,460 --> 00:16:11,730
So it gets familiar with
what it's dealing with.

319
00:16:11,730 --> 00:16:14,130
And it will come back and say,

320
00:16:14,130 --> 00:16:17,070
you know what, this is a spark system,

321
00:16:17,070 --> 00:16:18,990
there might be out of memory errors

322
00:16:18,990 --> 00:16:20,590
that you're likely to encounter.

323
00:16:21,660 --> 00:16:24,480
And what it does then
is it generates a query

324
00:16:24,480 --> 00:16:26,400
that runs on a regular basis

325
00:16:26,400 --> 00:16:29,550
to see if there are any new occurrences

326
00:16:29,550 --> 00:16:34,500
of say this out of memory error.

327
00:16:34,500 --> 00:16:37,800
Now you don't have to do
any of this manually, right?

328
00:16:37,800 --> 00:16:40,833
This is AI doing a lot
of the work for you.

329
00:16:42,900 --> 00:16:46,620
And then, we can generate assets,

330
00:16:46,620 --> 00:16:50,070
we can generate dashboards,
we can generate SLOs,

331
00:16:50,070 --> 00:16:54,840
we can generate things
like visualizations,

332
00:16:54,840 --> 00:16:55,980
and we can even do things

333
00:16:55,980 --> 00:17:00,660
like put information about the
systems that we've detected

334
00:17:00,660 --> 00:17:03,210
into our knowledge base.

335
00:17:03,210 --> 00:17:05,220
So if you don't know about this already,

336
00:17:05,220 --> 00:17:09,150
Elastic has a Vector
database in it, right?

337
00:17:09,150 --> 00:17:11,910
It's one of the big
things that Elastic does.

338
00:17:11,910 --> 00:17:14,160
And all of our AI technology

339
00:17:14,160 --> 00:17:16,530
in the Elastic Observability platform

340
00:17:16,530 --> 00:17:20,400
is built on top of
Elasticsearch capabilities

341
00:17:20,400 --> 00:17:22,140
like the Vector database.

342
00:17:22,140 --> 00:17:23,730
And so we have this thing

343
00:17:23,730 --> 00:17:26,520
we talk about on a regular
basis here at Elastic,

344
00:17:26,520 --> 00:17:28,830
called Retrieval Augmented Generation.

345
00:17:28,830 --> 00:17:30,510
So what we can do is,

346
00:17:30,510 --> 00:17:33,330
is if we have the AI analyze your systems,

347
00:17:33,330 --> 00:17:36,630
we can say, you know,
this is a Spark system,

348
00:17:36,630 --> 00:17:38,160
these are the things you need

349
00:17:38,160 --> 00:17:41,730
to think about with Spark Systems.

350
00:17:41,730 --> 00:17:44,280
We can put that into the knowledge base.

351
00:17:44,280 --> 00:17:45,660
And so when you are dealing

352
00:17:45,660 --> 00:17:48,000
with problems within the AI assistant,

353
00:17:48,000 --> 00:17:52,170
you know, having natural
conversations about your problems,

354
00:17:52,170 --> 00:17:54,570
what is the root cause
of this problem and that,

355
00:17:55,458 --> 00:17:57,960
it can then refer to any information

356
00:17:57,960 --> 00:17:59,940
that it's recorded

357
00:17:59,940 --> 00:18:02,850
about the systems that it's working with,

358
00:18:02,850 --> 00:18:06,543
which will help you accelerate,
root cause analysis, right?

359
00:18:09,090 --> 00:18:11,700
And this is what it looks like end to end.

360
00:18:11,700 --> 00:18:13,260
There's a couple of things
that are interesting

361
00:18:13,260 --> 00:18:14,820
on this slide.

362
00:18:14,820 --> 00:18:16,263
Firstly, if you look at,

363
00:18:18,303 --> 00:18:21,660
the bottom left hand corner on this slide,

364
00:18:21,660 --> 00:18:25,020
we still support all the
existing elastic things

365
00:18:25,020 --> 00:18:27,900
like integrations that we have.

366
00:18:27,900 --> 00:18:31,530
If you're using Elastic Agent
or Logstash or whatever,

367
00:18:31,530 --> 00:18:34,290
we still support all of
those Nginx mechanisms

368
00:18:34,290 --> 00:18:36,273
with this new streams product.

369
00:18:37,291 --> 00:18:38,910
You can still use those.

370
00:18:38,910 --> 00:18:39,993
But naturally,

371
00:18:41,520 --> 00:18:44,190
when we send data in using an integration,

372
00:18:44,190 --> 00:18:46,170
we don't need to partition that data

373
00:18:46,170 --> 00:18:49,410
because the integration
already tells it where to go

374
00:18:49,410 --> 00:18:52,560
in Elastic and organizes
that data for you.

375
00:18:52,560 --> 00:18:53,393
So,

376
00:18:54,705 --> 00:18:56,255
it skips the partitioning step.

377
00:18:57,750 --> 00:19:00,360
That's kind of like the legacy approach.

378
00:19:00,360 --> 00:19:02,220
We'll still support that.

379
00:19:02,220 --> 00:19:05,100
The new approach that we're
producing over here at Elastic

380
00:19:05,100 --> 00:19:08,670
is to send your data in
using whatever you like.

381
00:19:08,670 --> 00:19:11,100
It could be the Vanilla Otel Collector,

382
00:19:11,100 --> 00:19:13,830
it could be Fluent Bit,

383
00:19:13,830 --> 00:19:17,310
it could be, you know, whatever
it is you are using today.

384
00:19:17,310 --> 00:19:19,740
It doesn't have to be Elastic Technology,

385
00:19:19,740 --> 00:19:22,990
you can send it into our new logs endpoint

386
00:19:24,030 --> 00:19:26,820
and we just put the logs in Elastic,

387
00:19:26,820 --> 00:19:29,220
you don't have to do any
edge processing anymore,

388
00:19:29,220 --> 00:19:32,520
move all of that over to Elastic,

389
00:19:32,520 --> 00:19:35,370
send all the data to the log stream

390
00:19:35,370 --> 00:19:38,763
and then we figure it out by
starting out with partitioning.

391
00:19:41,070 --> 00:19:44,220
So that's Elastic Streams.

392
00:19:44,220 --> 00:19:48,420
Now, you'll see when I show you the demo,

393
00:19:48,420 --> 00:19:49,890
which I'm gonna do in a minute,

394
00:19:49,890 --> 00:19:52,290
that it is like a magical experience.

395
00:19:52,290 --> 00:19:53,123
So let's do that.

396
00:19:53,123 --> 00:19:55,320
Let's switch over to the demo

397
00:19:55,320 --> 00:19:57,820
and I'll show you what
Elastic Streams looks like.

398
00:20:00,600 --> 00:20:01,503
So here we go.

399
00:20:02,490 --> 00:20:04,530
This is Discover,

400
00:20:04,530 --> 00:20:06,870
and I've got an ES|QL query here.

401
00:20:06,870 --> 00:20:11,580
And this is a log data going
back to the previous diagram

402
00:20:11,580 --> 00:20:12,540
that you just saw,

403
00:20:12,540 --> 00:20:17,190
that's coming in via the classic
elastic agent route, okay?

404
00:20:17,190 --> 00:20:20,580
And the first thing you'll
notice about this data

405
00:20:20,580 --> 00:20:23,490
is that it's completely
unstructured, okay?

406
00:20:23,490 --> 00:20:27,090
So it's just raw unstructured log data,

407
00:20:27,090 --> 00:20:30,213
which is what most of you are
probably dealing with today.

408
00:20:32,160 --> 00:20:34,050
At the moment, you
could probably recognize

409
00:20:34,050 --> 00:20:35,280
a little bit about this data.

410
00:20:35,280 --> 00:20:36,723
It looks like trading data.

411
00:20:38,220 --> 00:20:41,040
And what we want to do,

412
00:20:41,040 --> 00:20:43,650
is we want to structure this data.

413
00:20:43,650 --> 00:20:45,300
So we're gonna click
this little button here,

414
00:20:45,300 --> 00:20:47,850
pause content in Streams.

415
00:20:47,850 --> 00:20:49,170
And the first thing we're gonna do

416
00:20:49,170 --> 00:20:51,183
is we're gonna create a new processor.

417
00:20:52,770 --> 00:20:55,890
Now, as you can see here,

418
00:20:55,890 --> 00:20:57,660
we have this handy little button here

419
00:20:57,660 --> 00:20:59,310
called Generate Pattern.

420
00:20:59,310 --> 00:21:01,440
And it has that little AI logo

421
00:21:01,440 --> 00:21:04,080
that you're probably seeing
all over the place now,

422
00:21:04,080 --> 00:21:06,300
but if we click that button,

423
00:21:06,300 --> 00:21:09,450
you'll see that it generates
a grok pattern for us

424
00:21:09,450 --> 00:21:12,453
so you don't have to do it
manually anymore, right?

425
00:21:13,500 --> 00:21:16,710
And you can see that it's
like looked at the data

426
00:21:16,710 --> 00:21:20,100
and it's got some understanding
of the data, right?

427
00:21:20,100 --> 00:21:22,560
So it knows that this is a stock symbol

428
00:21:22,560 --> 00:21:24,870
and a quantity and a price,

429
00:21:24,870 --> 00:21:26,640
and it inferred this, you know,

430
00:21:26,640 --> 00:21:28,410
we didn't tell it any of that information,

431
00:21:28,410 --> 00:21:31,203
we just gave it the raw
logs that you see here.

432
00:21:32,610 --> 00:21:34,350
So if we accept that,

433
00:21:34,350 --> 00:21:38,820
now the data is going to be much easier

434
00:21:38,820 --> 00:21:40,350
to do analytics on, right?

435
00:21:40,350 --> 00:21:42,750
We can now start to look at how much

436
00:21:42,750 --> 00:21:46,740
did a particular stock trade
for over a 30 day window

437
00:21:46,740 --> 00:21:48,270
or something like that,

438
00:21:48,270 --> 00:21:50,220
which would've been much harder to do

439
00:21:50,220 --> 00:21:52,863
had we left this data in
an unstructured format.

440
00:21:54,480 --> 00:21:55,983
So let's save that.

441
00:21:57,150 --> 00:21:58,740
One thing you'll notice here,

442
00:21:58,740 --> 00:22:01,560
that's very interesting as well,

443
00:22:01,560 --> 00:22:05,220
is this little source thing
here that says OpenTelemetry.

444
00:22:05,220 --> 00:22:09,660
So when we do this with the
generating of the patterns,

445
00:22:09,660 --> 00:22:13,053
we try to use OpenTelemetry
semantic conventions.

446
00:22:14,040 --> 00:22:19,040
So we're using this open standard schema,

447
00:22:19,110 --> 00:22:22,230
we store that in that
standard schema format,

448
00:22:22,230 --> 00:22:24,120
natively and elastic,

449
00:22:24,120 --> 00:22:27,240
so you can be sure that all your data

450
00:22:27,240 --> 00:22:30,303
is being written in a standard format.

451
00:22:32,370 --> 00:22:34,413
We'll confirm those changes.

452
00:22:35,790 --> 00:22:39,663
And now if we go back to discover,

453
00:22:40,650 --> 00:22:42,540
we can now have a look and see

454
00:22:42,540 --> 00:22:45,660
how easy it is to run
analytics on that data, right?

455
00:22:45,660 --> 00:22:49,510
So now we can actually do
a sum of the quantities

456
00:22:50,610 --> 00:22:52,800
over the cloud regions,

457
00:22:52,800 --> 00:22:56,040
and we can see that our
workloads are pretty evenly split

458
00:22:56,040 --> 00:22:59,520
between all the different cloud
regions that we have there.

459
00:22:59,520 --> 00:23:02,520
And we can do, you know,
other types of analytics now,

460
00:23:02,520 --> 00:23:06,180
because we couldn't have done
that on the unstructured data,

461
00:23:06,180 --> 00:23:08,100
not very easily,

462
00:23:08,100 --> 00:23:10,800
or you would've had to
have manually created

463
00:23:10,800 --> 00:23:13,953
a processing pipeline
to deal with it, right?

464
00:23:16,110 --> 00:23:16,943
So,

465
00:23:19,860 --> 00:23:21,400
now we look to how you do it

466
00:23:23,568 --> 00:23:26,010
with the standard processing pipeline

467
00:23:26,010 --> 00:23:28,710
using Elastic Agent and all
of that sort of thing, right?

468
00:23:28,710 --> 00:23:33,360
Now we're gonna look at how
you do it in the new way,

469
00:23:33,360 --> 00:23:37,020
which is just sending your
data to the Elastic Log index.

470
00:23:37,020 --> 00:23:41,040
So this is a new special index called Logs

471
00:23:41,040 --> 00:23:43,680
and we manage all the
complexity behind the scenes

472
00:23:43,680 --> 00:23:46,680
like the Nginx pipelines
and things like that.

473
00:23:46,680 --> 00:23:49,180
You don't have to worry
about those, we manage it.

474
00:23:50,100 --> 00:23:54,840
So let's take a look at what
we wanna do with that first.

475
00:23:54,840 --> 00:23:56,160
So the first thing

476
00:23:56,160 --> 00:23:59,040
we would do with that
is partition the data.

477
00:23:59,040 --> 00:24:02,070
So we're gonna go into partitioning here,

478
00:24:02,070 --> 00:24:05,160
and of course we can
still do things manually.

479
00:24:05,160 --> 00:24:07,710
We don't always have to use AI, right?

480
00:24:07,710 --> 00:24:09,180
We want to give you that flexibility.

481
00:24:09,180 --> 00:24:11,760
The AI isn't always gonna get it right.

482
00:24:11,760 --> 00:24:14,553
And so we'll create a
partition manually first.

483
00:24:15,600 --> 00:24:17,460
We'll always call it Spark.

484
00:24:17,460 --> 00:24:20,020
We're gonna use a field attribute

485
00:24:22,628 --> 00:24:26,810
to split off Spark logs
from the Log Stream.

486
00:24:27,690 --> 00:24:30,290
So if we save that,
what's gonna happen now

487
00:24:30,290 --> 00:24:33,510
is you can see this hierarchy appearing

488
00:24:33,510 --> 00:24:35,430
where we've got logs at the top level,

489
00:24:35,430 --> 00:24:38,070
which is where we are
just sending all our data

490
00:24:38,070 --> 00:24:42,420
and now we've got a new index
that's being written to,

491
00:24:42,420 --> 00:24:44,673
which is gonna contain just Spark Logs.

492
00:24:45,870 --> 00:24:47,850
But we can also do this with AI.

493
00:24:47,850 --> 00:24:51,990
So we click suggest partitions with AI,

494
00:24:51,990 --> 00:24:54,750
and AI would've done a perfectly fine job.

495
00:24:54,750 --> 00:24:58,920
In fact, we had Hadoop and
Spark Logs in the log stream

496
00:24:58,920 --> 00:25:01,410
and it has split those out perfectly.

497
00:25:01,410 --> 00:25:03,783
So we're gonna just save those changes.

498
00:25:05,370 --> 00:25:09,360
And you can see here that we
have Spark and Hadoop logs

499
00:25:09,360 --> 00:25:12,960
and you can see that hierarchy
that I talked about before.

500
00:25:12,960 --> 00:25:15,720
So the log stream has
now been split into two.

501
00:25:15,720 --> 00:25:19,170
We've got Hadoop logs
and Spark logs separate

502
00:25:19,170 --> 00:25:20,163
from how they were.

503
00:25:21,720 --> 00:25:24,480
So once they're all separated out,

504
00:25:24,480 --> 00:25:28,590
we can then also do the same
processing that we did before

505
00:25:28,590 --> 00:25:29,700
on those logs.

506
00:25:29,700 --> 00:25:32,790
We can create a processor,

507
00:25:32,790 --> 00:25:36,270
we can use AI to generate
those grok patterns,

508
00:25:36,270 --> 00:25:39,750
and we can split out the
severity text using that,

509
00:25:39,750 --> 00:25:44,750
and again, do a better job
of analytics on this data.

510
00:25:45,960 --> 00:25:47,943
So let's confirm those changes.

511
00:25:48,960 --> 00:25:51,450
And then you'll see here,

512
00:25:51,450 --> 00:25:54,843
now I can run ES|QL against that data,

513
00:25:56,820 --> 00:25:58,650
much, much easier.

514
00:25:58,650 --> 00:26:00,000
Right.

515
00:26:00,000 --> 00:26:01,230
So what have we done so far?

516
00:26:01,230 --> 00:26:03,033
So we partitioned our logs,

517
00:26:05,890 --> 00:26:09,390
we structured our logs, we've
ran analytics on our logs.

518
00:26:09,390 --> 00:26:11,000
The next thing that we wanted to do,

519
00:26:11,000 --> 00:26:14,010
if you go back to thinking
about the diagram I showed you,

520
00:26:14,010 --> 00:26:17,790
is we wanna find problems
in our logs, right?

521
00:26:17,790 --> 00:26:19,893
So let's take a look at the Spark logs.

522
00:26:21,720 --> 00:26:25,650
And the first thing we'll
do here is we'll detect

523
00:26:25,650 --> 00:26:26,973
what systems are here.

524
00:26:28,410 --> 00:26:30,570
We know it's a Spark system,

525
00:26:30,570 --> 00:26:33,330
but what the AI has done in the background

526
00:26:33,330 --> 00:26:37,230
is it's added additional
information about the Spark system.

527
00:26:37,230 --> 00:26:39,570
So you can see that description field

528
00:26:39,570 --> 00:26:41,820
is kind of a cut off at the moment,

529
00:26:41,820 --> 00:26:45,600
but it's got a large
description about Spark Systems

530
00:26:45,600 --> 00:26:47,070
and what Spark Systems do

531
00:26:47,070 --> 00:26:49,230
and what types of problems
that you might encounter

532
00:26:49,230 --> 00:26:51,240
with Spark Systems.

533
00:26:51,240 --> 00:26:54,990
So once we've identified the system,

534
00:26:54,990 --> 00:26:57,960
and we've described the system to Elastic,

535
00:26:57,960 --> 00:27:00,580
we'll add that to streams

536
00:27:01,680 --> 00:27:06,660
and we'll generate suggestions
on the types of problems

537
00:27:06,660 --> 00:27:08,820
that can occur with Spark Systems.

538
00:27:08,820 --> 00:27:10,893
So you can see this list here,

539
00:27:11,880 --> 00:27:14,970
we've got Spark Debug events,

540
00:27:14,970 --> 00:27:18,900
Spark warnings, Spark
out of memory exceptions.

541
00:27:18,900 --> 00:27:21,270
These are all things that are specific,

542
00:27:21,270 --> 00:27:24,363
specific problems to Spark
Systems that might occur.

543
00:27:25,410 --> 00:27:27,963
And by bringing them into Elastic,

544
00:27:29,160 --> 00:27:31,170
we then keep track

545
00:27:31,170 --> 00:27:34,380
to see if any of these
problems occur over time.

546
00:27:34,380 --> 00:27:36,810
We're actually using a technique called,

547
00:27:36,810 --> 00:27:38,850
change point detection,

548
00:27:38,850 --> 00:27:40,260
which is a machine learning technique.

549
00:27:40,260 --> 00:27:42,850
It will detect whether there is a spike

550
00:27:45,257 --> 00:27:47,100
in the number of these logs over time,

551
00:27:47,100 --> 00:27:49,320
or if there's a dip in
the number of these logs

552
00:27:49,320 --> 00:27:52,800
or if there's a change in the
distribution in these logs.

553
00:27:52,800 --> 00:27:55,260
It'll detect all of these things,

554
00:27:55,260 --> 00:28:00,003
and you can get alerted to those problems,

555
00:28:00,960 --> 00:28:02,940
and I'll show you how to do that.

556
00:28:02,940 --> 00:28:04,380
So here what we do is,

557
00:28:04,380 --> 00:28:08,280
is we actually are gonna set
up an alert based on the query

558
00:28:08,280 --> 00:28:10,050
that the AI generated.

559
00:28:10,050 --> 00:28:11,580
So we go in here,

560
00:28:11,580 --> 00:28:12,413
and you can see

561
00:28:12,413 --> 00:28:15,603
we copied the query from
here into the alert.

562
00:28:17,040 --> 00:28:19,743
And now if we save that,

563
00:28:20,880 --> 00:28:23,280
we will get alerted
anytime there is an out

564
00:28:23,280 --> 00:28:26,583
of memory exception with our Spark system.

565
00:28:27,450 --> 00:28:29,700
I didn't have to create
this query manually,

566
00:28:29,700 --> 00:28:31,890
the LLM did it for me.

567
00:28:31,890 --> 00:28:35,100
It actually probably
knows the Spark System

568
00:28:35,100 --> 00:28:36,120
and the types of problems

569
00:28:36,120 --> 00:28:39,600
that can happen with Spark
Systems far better than I do,

570
00:28:39,600 --> 00:28:40,443
to be honest.

571
00:28:43,860 --> 00:28:46,683
And you can see that alert
in Elastic over here.

572
00:28:48,480 --> 00:28:51,180
So we've done significant events,

573
00:28:51,180 --> 00:28:53,550
now we've found problems in our logs.

574
00:28:53,550 --> 00:28:57,960
We've also made a lot of other
things much, much easier.

575
00:28:57,960 --> 00:29:01,080
That used to be kind of
tricky to do in Elastic.

576
00:29:01,080 --> 00:29:03,480
We've made things like retention

577
00:29:03,480 --> 00:29:06,270
and data quality issues much easier

578
00:29:06,270 --> 00:29:10,050
to see using the Streams UI.

579
00:29:10,050 --> 00:29:12,210
So the next thing we're gonna
dig into is I'm gonna show you

580
00:29:12,210 --> 00:29:14,250
how easy it's to do that now.

581
00:29:14,250 --> 00:29:15,960
So if you look in here,

582
00:29:15,960 --> 00:29:19,230
we can actually change the
retention pretty easily.

583
00:29:19,230 --> 00:29:23,010
So you know, if I want to change
it to 30 days, for example,

584
00:29:23,010 --> 00:29:24,990
for these particular Spark logs,

585
00:29:24,990 --> 00:29:27,840
I can change it very,
very quickly in here.

586
00:29:27,840 --> 00:29:30,450
I can adjust the ILM policies.

587
00:29:30,450 --> 00:29:33,610
So if I want to just put,
you know, two days of hot

588
00:29:34,560 --> 00:29:36,210
and 30 days of warm,

589
00:29:36,210 --> 00:29:38,520
or maybe I want to use the frozen tier

590
00:29:38,520 --> 00:29:41,310
to save even more money on storage,

591
00:29:41,310 --> 00:29:43,620
I could do that quickly in streams.

592
00:29:43,620 --> 00:29:46,833
We've just made these things
a little easier to do,

593
00:29:47,730 --> 00:29:50,223
hiding a lot of the
complexity behind the scenes.

594
00:29:51,660 --> 00:29:54,030
So we can set retention.

595
00:29:54,030 --> 00:29:57,330
And the other thing we can
do is we can actually look

596
00:29:57,330 --> 00:29:59,343
at data quality.

597
00:30:00,210 --> 00:30:04,200
So if we look at the data quality here,

598
00:30:04,200 --> 00:30:06,510
there's actually two types
of data quality things

599
00:30:06,510 --> 00:30:07,343
that we could do.

600
00:30:07,343 --> 00:30:09,900
So if you think about
data coming into Elastic,

601
00:30:09,900 --> 00:30:12,840
we can tell you if there is a problem

602
00:30:12,840 --> 00:30:15,960
with a particular field that's coming in,

603
00:30:15,960 --> 00:30:17,970
a specific piece of data,

604
00:30:17,970 --> 00:30:21,210
or we can tell you if the
entire document failed

605
00:30:21,210 --> 00:30:22,647
to come into Elastic

606
00:30:22,647 --> 00:30:26,130
and we store that information
in our failure store

607
00:30:26,130 --> 00:30:27,603
so you can dissect it.

608
00:30:28,680 --> 00:30:30,210
So I'll show you how that works.

609
00:30:30,210 --> 00:30:32,190
Again, we've made it much, much easier

610
00:30:32,190 --> 00:30:34,440
to do all these things in streams.

611
00:30:34,440 --> 00:30:36,690
So the first thing I'm gonna do,

612
00:30:36,690 --> 00:30:41,690
is I'm going to deliberately
break this regular expression.

613
00:30:43,500 --> 00:30:46,100
So you can see here I've
put some nonsense in there.

614
00:30:47,100 --> 00:30:50,490
We're gonna ignore failures
for this processor,

615
00:30:50,490 --> 00:30:52,320
and we're gonna update that.

616
00:30:52,320 --> 00:30:54,270
I'm gonna save the changes.

617
00:30:54,270 --> 00:30:57,243
And now what will happen is, is,

618
00:30:58,500 --> 00:31:01,503
Elastic has caught the problem.

619
00:31:02,520 --> 00:31:05,310
If you see here, the
entire document has failed

620
00:31:05,310 --> 00:31:07,440
because that regular expression is broken.

621
00:31:07,440 --> 00:31:09,840
So it failed to bring that document

622
00:31:09,840 --> 00:31:11,970
into Elastic successfully.

623
00:31:11,970 --> 00:31:13,650
And we can dig into what the problem is.

624
00:31:13,650 --> 00:31:16,230
So you can see here it
tells us straight away

625
00:31:16,230 --> 00:31:18,300
that the provider grok expression

626
00:31:18,300 --> 00:31:19,710
did not match the field value,

627
00:31:19,710 --> 00:31:22,140
which stopped the data from
coming into Elastic properly.

628
00:31:22,140 --> 00:31:25,170
So it's much easier now to
see when there are problems

629
00:31:25,170 --> 00:31:26,613
with the data stream.

630
00:31:28,290 --> 00:31:31,740
So that's one thing that's
the entire failed document,

631
00:31:31,740 --> 00:31:34,320
but we also can do it on a
field level as well, right?

632
00:31:34,320 --> 00:31:38,670
So if we go back here, we'll,
first of all we'll fix this

633
00:31:38,670 --> 00:31:40,143
so it's not broken anymore.

634
00:31:40,980 --> 00:31:45,483
And then what we'll do is we
will create a new processor,

635
00:31:46,350 --> 00:31:49,350
we will use the host name

636
00:31:49,350 --> 00:31:53,249
and we'll put a ridiculously
large host name for you,

637
00:31:53,249 --> 00:31:55,530
that is definitely not a host name

638
00:31:55,530 --> 00:32:00,270
that will fit any sort of validation logic

639
00:32:00,270 --> 00:32:01,549
that I'm aware of.

640
00:32:01,549 --> 00:32:02,580
(David laughing)

641
00:32:02,580 --> 00:32:05,010
So we'll do that,

642
00:32:05,010 --> 00:32:08,310
and then you can see what happens
in the data quality pages.

643
00:32:08,310 --> 00:32:10,590
Now, instead of a failed document,

644
00:32:10,590 --> 00:32:12,990
I've got a degraded document,

645
00:32:12,990 --> 00:32:16,710
and that's because it's
ignoring the host name field.

646
00:32:16,710 --> 00:32:19,620
And if you look at the
host name field in here,

647
00:32:19,620 --> 00:32:22,080
it's because the size

648
00:32:22,080 --> 00:32:25,050
of the host name is too big.

649
00:32:25,050 --> 00:32:28,950
So we basically just
ignored that specific field.

650
00:32:28,950 --> 00:32:30,660
We're still ingesting the document

651
00:32:30,660 --> 00:32:33,090
and we're still ingesting
all the other data

652
00:32:33,090 --> 00:32:34,980
or the other attributes that are in there.

653
00:32:34,980 --> 00:32:36,240
We're just ignoring the host name

654
00:32:36,240 --> 00:32:39,385
because that field's being corrupted.

655
00:32:39,385 --> 00:32:44,385
It's got a completely ineligible
host name in it, right?

656
00:32:45,840 --> 00:32:49,440
So we'll go back and
we'll fix these problems,

657
00:32:49,440 --> 00:32:50,913
we'll delete this,

658
00:32:52,050 --> 00:32:53,880
we'll save the changes,

659
00:32:53,880 --> 00:32:55,263
and then if we go back in,

660
00:32:56,190 --> 00:32:58,090
everything's working again now, right?

661
00:33:02,246 --> 00:33:03,510
So that's streams.

662
00:33:03,510 --> 00:33:05,190
Those are all the new goodies

663
00:33:05,190 --> 00:33:06,903
that we've released in Elastic.

664
00:33:09,330 --> 00:33:10,590
I'll switch back to the slides.

665
00:33:10,590 --> 00:33:11,423
Here we go.

666
00:33:13,290 --> 00:33:14,123
So,

667
00:33:15,480 --> 00:33:20,370
you can see now why we
need a good solid platform.

668
00:33:20,370 --> 00:33:22,450
We need scale to billions of logs

669
00:33:23,643 --> 00:33:26,400
and do that in a way
that's cost effective.

670
00:33:26,400 --> 00:33:28,530
We wanna simplify, Nginx, right?

671
00:33:28,530 --> 00:33:31,830
We don't want people to have
to wrestle with pipelines

672
00:33:31,830 --> 00:33:34,590
and drop data on the floor.

673
00:33:34,590 --> 00:33:37,290
We want to automate as
much of that as possible.

674
00:33:37,290 --> 00:33:41,520
And we now have AI that can really help us

675
00:33:41,520 --> 00:33:42,933
with this automation.

676
00:33:44,550 --> 00:33:48,000
And now you can spend
more time solving problems

677
00:33:48,000 --> 00:33:49,120
instead of working

678
00:33:50,573 --> 00:33:52,470
on log processing pipelines,

679
00:33:52,470 --> 00:33:55,353
which I know is a massive
headache for a lot of you.

680
00:33:56,850 --> 00:34:01,440
And we then use AI to do
a lot of analytics work

681
00:34:01,440 --> 00:34:04,740
to surface problems in
your logs automatically,

682
00:34:04,740 --> 00:34:05,970
to give you the ability

683
00:34:05,970 --> 00:34:08,280
to, you know, find problems, for example,

684
00:34:08,280 --> 00:34:10,170
that you may not even be aware of

685
00:34:10,170 --> 00:34:12,153
in your Spark logs, for example.

686
00:34:15,660 --> 00:34:17,350
And you know, at some point

687
00:34:18,390 --> 00:34:23,390
our vision is to bring
this beyond logs, right?

688
00:34:23,520 --> 00:34:26,763
Bring it to traces and metrics
at some point in the future.

689
00:34:27,990 --> 00:34:30,570
So thank you very much everybody.

690
00:34:30,570 --> 00:34:33,840
Thanks for joining me on
this talk about streams,

691
00:34:33,840 --> 00:34:36,990
and I appreciate you coming
all the way down here

692
00:34:36,990 --> 00:34:38,580
from the Venetian.

693
00:34:38,580 --> 00:34:40,347
Thank you very much.

694
00:34:40,347 --> 00:34:43,097
(audience clapping)

