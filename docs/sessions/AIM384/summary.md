# AWS re:Invent 2025 Slack AI 会议总结

## 会议概述

本次会议由 AWS 首席解决方案架构师 Jean Ting 以及 Slack 的 Austin Bell 和 Sharia Kathready 共同主讲，重点介绍了 Slack 如何利用 Amazon Bedrock 和 Nova 为全球数百万用户提供生成式 AI 功能。会议深入探讨了将生成式 AI 应用从概念验证阶段推进到生产级别所面临的挑战，以及 Slack 在这一过程中的实践经验。

演讲者首先介绍了生成式 AI 应用生产化的基本框架，包括用户需求定义、模型选择、成本管理、安全性和可靠性等关键要素。随后，Slack 团队分享了他们从 SageMaker 迁移到 Amazon Bedrock 的完整历程，展示了如何在保持高安全标准和合规性要求的同时，实现大规模部署。特别值得关注的是，通过这次迁移，Slack 不仅将可用的 LLM 模型数量从 1 个增加到 15 个以上，还实现了超过 90% 的成本节省（超过 2000 万美元），同时显著提升了系统的可靠性和灵活性。

会议还详细介绍了 Slack 内部开发的实验框架，该框架能够客观地衡量生成式 AI 输出的质量。通过结合自动化程序指标、LLM 评判器和护栏机制，Slack 能够在大规模数据集上持续优化 AI 功能的质量，确保为用户提供安全、准确且相关的响应。

## 详细时间线

### 开场与背景介绍 (0:00-5:30)

0:00 - 会议开始，Jean Ting 向观众问候并询问有多少人已经构建了生成式 AI 应用

1:15 - 回顾生成式 AI 行业的快速发展历程，从早期研究到如今能够搜索信息、生成内容和代码的工具

2:45 - Jean Ting 自我介绍，她是 AWS 的首席解决方案架构师，并介绍来自 Slack 的两位嘉宾 Austin Bell 和 Sharia Kathready

3:20 - 介绍将生成式 AI 应用推向生产环境所面临的挑战

### 生产化框架概述 (5:30-12:00)

5:30 - 讲解生成式 AI 应用生产化的高层次步骤，包括用户需求、可靠性、运营卓越性、成本管理和安全性

6:45 - 强调生成式 AI 应用的独特之处：确保系统以安全方式响应用户，并只处理设计范围内的工作流

7:30 - 介绍定义用户需求时需要考虑的关键因素：静态 LLM 工作流 vs 动态代理工作流、任务复杂度、延迟敏感性

8:50 - 介绍 Amazon Bedrock 如何提供多种模型选择来满足不同复杂度的任务需求

9:40 - 讲解 Bedrock 的不同推理选项：预置吞吐量、优先级层、标准层、批量推理和弹性层

10:30 - 介绍模型升级和优化策略，包括 Bedrock 的提示管理、提示调优和 LLM 评判功能

### 规模化运营 (12:00-17:30)

12:00 - 讨论规模化运营的关键决策：区域选择、监控策略、性能调优

13:00 - 介绍监控指标：首个令牌时间、端到端延迟、错误率、限流情况，使用 CloudWatch 进行日志记录

14:00 - 讲解性能调优策略，特别是显式提示缓存功能，可以节省时间和成本

15:00 - 介绍模型灵活性如何帮助提高弹性，包括在模型出现问题时快速切换的能力

16:00 - 介绍跨区域推理功能，允许将请求发送到其他区域以获取容量

### 安全设计 (17:30-21:00)

17:30 - 强调安全设计是生产化应用的核心要素

18:00 - 介绍 Bedrock 的基本安全保障：模型提供商无法访问托管环境，AWS 不会共享输入输出数据

19:00 - 讲解 IAM 策略、服务控制策略和 VPC 端点的使用

20:00 - 介绍跨区域推理的地理位置限制功能，满足数据主权需求

20:30 - 详细介绍 Amazon Bedrock 护栏功能，可以过滤关键词、拒绝危险内容、自定义主题限制，并在输出端进行自动推理检查

### Slack AI 实践介绍 (21:00-25:00)

21:00 - Austin Bell 开始介绍 Slack 如何有效、安全、高效地交付 Slack AI 功能

22:00 - 强调达到 80% 的生成式 AI 功能相对容易，但最后 20% 的成本效率和质量提升才是关键

23:00 - 介绍演讲的三个主要部分：基础设施层、实验框架、产品集成

24:00 - Austin Bell 和 Sharia Kathready 自我介绍

24:30 - 介绍 Slack AI 的功能范围：AI 摘要、问答系统、每日摘要等十几种不同的生成式 AI 功能

### Slack 基础设施扩展 (25:00-40:00)

25:00 - Sharia 开始介绍如何将 Slack AI 扩展到数百万日活跃用户

26:00 - 强调 Slack 不仅关注速度，更关注信任，确保保护客户数据

27:00 - 介绍 Slack 的三大产品承诺支柱：信任、安全、可靠性

28:00 - 信任支柱：不使用客户数据训练模型、不记录客户数据、管理员可选择退出功能、零数据保留

29:00 - 安全支柱：符合 FedRAMP Moderate 合规标准、保持在信任边界内、技术访问控制

30:00 - 可靠性支柱：高可用性、上下文相关性、透明度（提供引用来源）

31:00 - 展示 Slack 的运营规模：每周处理 10-50 亿条消息、1-5 亿个文件、10-50 亿次搜索

32:30 - 介绍 2023 年中至 2024 年中的早期架构：使用 SageMaker，选项有限、成本高、灵活性低

34:00 - 讲解早期架构：通过 VPC 端点连接 SageMaker，使用并发检查器管理负载

35:30 - 介绍遇到的问题：流量峰值明显、难以获取 GPU、无法轻松扩缩容

37:00 - 问题影响：大部分时间过度配置、基础设施按峰值流量扩展、固定成本限制了 LLM 多样性

38:30 - 介绍迁移愿景：寻找托管服务、模型多样性

39:00 - 2024 年中 Bedrock 获得 FedRAMP Moderate 认证，成为理想选择

### 迁移到 Bedrock (40:00-50:00)

40:00 - 介绍 Bedrock 的优势：在信任边界内、不共享输入输出、前沿模型集合、快速添加新模型

41:30 - 迁移步骤第一步：了解 Bedrock 基础设施，选择预置吞吐量以简化迁移

42:30 - 迁移步骤第二步：内部负载测试和计算换算，科学地确定 Bedrock 模型单元与 SageMaker 计算的等效关系

43:30 - 展示负载测试示例：Claude Instant 和 Claude Haiku 在 SageMaker 和 Bedrock 上的对比

44:30 - 迁移步骤第三步：运行影子流量，向 Bedrock 发送重复请求以验证延迟和指标

45:30 - 迁移步骤第四步：分阶段完全切换（1%、5%、10%、100%）

46:30 - 迁移后发现的改进空间：预置吞吐量仍无法扩展、需要备用模型、需要紧急停止功能

48:00 - 介绍平台增强功能：备用模型、紧急停止、支持工具、提示缓存、护栏等 Bedrock 功能

49:00 - 决定从预置吞吐量迁移到按需模式

### 按需模式迁移 (50:00-55:00)

50:00 - 介绍按需模式基于配额而非实例：每分钟令牌数（TPM）和每分钟请求数（RPM）

51:00 - 利用预置吞吐量时期的元数据计算 RPM 和 TPM，向 Bedrock 请求配额

52:00 - 展示新架构：使用 RPM/TPM 检查器替代并发检查器，隔离不同功能

53:00 - 介绍美国跨区域推理配置文件的优势：符合 FedRAMP Moderate 要求、可使用 US-West-2、更快获取计算资源

54:00 - 总结迁移成果：从 1 个 LLM 增加到 15+ 个、提高可靠性、利用率效率提升超过 90%、节省超过 2000 万美元

### 质量评估框架 (55:00-65:00)

55:00 - Austin 开始介绍内部实验框架，用于客观衡量生成式 AI 输出质量

56:00 - 引用工程师的困境：修复一个问题导致另一个问题，陷入"打地鼠"循环

57:00 - 解释评估生成式输出的困难：与传统机器学习不同，输出高度主观

58:30 - 强调核心理念：只能改进可以衡量的内容

59:00 - 介绍两大评估支柱：质量和安全

60:00 - 质量支柱细分：客观测量（格式化、JSON/XML 解析、ID 和链接格式）和主观测量（事实准确性、答案相关性、归因准确性）

61:30 - 安全支柱细分：危害（毒性、偏见）和安全性（防止提示注入攻击、搜索投毒）

62:30 - 介绍评估器生成的演进历程：从手动审查小数据集开始

63:00 - 引入自动化程序指标捕获客观测量

64:00 - 当前状态：使用基于 LLM 的质量指标衡量事实准确性和答案相关性，使用护栏捕获安全和危害问题

64:30 - 目标：通过组合自动化指标、LLM 评判器和护栏，在更大规模、更具代表性的数据集上评估质量

会议在此处字幕截断，但已经涵盖了 Slack AI 从基础设施扩展到质量保障的完整实践经验。