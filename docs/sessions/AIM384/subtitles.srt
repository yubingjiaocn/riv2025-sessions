1
00:00:01,050 --> 00:00:03,420
- Hey, good afternoon everyone.

2
00:00:03,420 --> 00:00:06,150
Hope everyone's enjoying
their re:Invent 2025

3
00:00:06,150 --> 00:00:07,353
so far this week.

4
00:00:08,190 --> 00:00:12,033
Yeah, thank you so much for coming today.

5
00:00:13,140 --> 00:00:14,970
Before we actually kick things off,

6
00:00:14,970 --> 00:00:16,830
I just want to take a quick show of hands,

7
00:00:16,830 --> 00:00:19,140
how many of you guys have already

8
00:00:19,140 --> 00:00:22,600
built a generative AI
application too, actually helps

9
00:00:23,580 --> 00:00:25,480
some of your users out there trying to

10
00:00:26,722 --> 00:00:29,703
make their lives more effective
or more productive, so far?

11
00:00:31,140 --> 00:00:33,750
A quick show of hands, don't be shy.

12
00:00:33,750 --> 00:00:35,010
It's a good showing actually.

13
00:00:35,010 --> 00:00:38,040
So, and you know, that's
really a testament

14
00:00:38,040 --> 00:00:41,310
of how the amazing the
industry has transformed

15
00:00:41,310 --> 00:00:42,270
the last couple of years.

16
00:00:42,270 --> 00:00:45,150
You know, if you think
about it, so it all started

17
00:00:45,150 --> 00:00:47,550
with like some early
research and development

18
00:00:47,550 --> 00:00:50,760
that actually led to the first
set of in frontier models

19
00:00:50,760 --> 00:00:52,950
that actually showed us,
showed the entire world

20
00:00:52,950 --> 00:00:57,950
the capability or the
possible for letting you help

21
00:00:58,740 --> 00:01:02,010
letting users interface
with a system in their own

22
00:01:02,010 --> 00:01:05,760
natural language and
creatively responding and kind.

23
00:01:05,760 --> 00:01:08,100
And, you know, with
that then it all led to

24
00:01:08,100 --> 00:01:10,080
a whole lot of tools today that, you know,

25
00:01:10,080 --> 00:01:13,500
not only it could actually
go and search for information

26
00:01:13,500 --> 00:01:15,480
for you, it can answer
the question yourself.

27
00:01:15,480 --> 00:01:18,420
It generates new rich
media content based on

28
00:01:18,420 --> 00:01:21,240
a set of descriptions or
captions that you feed it.

29
00:01:21,240 --> 00:01:23,400
It can even generate
brand new code for you

30
00:01:23,400 --> 00:01:28,400
that actually can like
either just prototype

31
00:01:29,130 --> 00:01:30,840
for you on a new concept you want,

32
00:01:30,840 --> 00:01:33,150
you wanna share with people,
or you want to fill a gap

33
00:01:33,150 --> 00:01:35,754
or do some small repair
on your applications.

34
00:01:35,754 --> 00:01:40,020
So that's how quickly the industry evolved

35
00:01:40,020 --> 00:01:41,340
in the last couple years.

36
00:01:41,340 --> 00:01:43,080
But I think for everyone who has said

37
00:01:43,080 --> 00:01:44,910
that they deployed something out there,

38
00:01:44,910 --> 00:01:47,700
they probably also seen
all the challenges involved

39
00:01:47,700 --> 00:01:50,964
and actually getting
that type of application

40
00:01:50,964 --> 00:01:55,200
to a production level quality
that's you feel comfortable

41
00:01:55,200 --> 00:01:56,793
giving to your users over time.

42
00:01:58,512 --> 00:02:01,590
So, with that, my name is Gene Ting,

43
00:02:01,590 --> 00:02:05,310
I'm a principal solutions
architect at Amazon Web Services.

44
00:02:05,310 --> 00:02:07,440
And joining me today is Austin Bell

45
00:02:07,440 --> 00:02:09,600
and Shaurya Kethireddy from Slack,

46
00:02:09,600 --> 00:02:12,150
as we gonna talk about their
journey on overcoming some

47
00:02:12,150 --> 00:02:14,580
of those challenges so
that they can actually

48
00:02:14,580 --> 00:02:17,280
delight millions of Slack
users around the world

49
00:02:17,280 --> 00:02:18,933
with Amazon Bedrock and Nova.

50
00:02:22,291 --> 00:02:27,291
So, to start things off making
a generative AI application

51
00:02:28,170 --> 00:02:31,170
production worthy follow
some similar steps

52
00:02:31,170 --> 00:02:33,090
as what you would've
thought about in the past

53
00:02:33,090 --> 00:02:35,690
for more traditional
applications on the high level.

54
00:02:37,440 --> 00:02:39,290
You're gonna have to think hard about

55
00:02:40,800 --> 00:02:42,180
who you're building towards,
the types of personas they are,

56
00:02:42,180 --> 00:02:43,620
which defines the user requirements

57
00:02:43,620 --> 00:02:47,250
and the functions that you're
gonna be thinking about

58
00:02:47,250 --> 00:02:48,600
to design your application.

59
00:02:49,650 --> 00:02:51,480
Once those decisions are made,

60
00:02:51,480 --> 00:02:54,270
you're gonna have to
think about reliability,

61
00:02:54,270 --> 00:02:57,660
operational excellence, cost management,

62
00:02:57,660 --> 00:03:00,010
especially as your
application grows over time.

63
00:03:01,260 --> 00:03:03,090
You always wanna make
sure that you can protect

64
00:03:03,090 --> 00:03:08,090
the application, protect 'em
from harmful damage to it,

65
00:03:08,250 --> 00:03:10,300
protecting your user session, their data.

66
00:03:11,910 --> 00:03:15,090
But what's kind of new
and an interesting nuance

67
00:03:15,090 --> 00:03:17,730
that comes with generative
AI applications as well,

68
00:03:17,730 --> 00:03:21,720
how to make sure that your system

69
00:03:21,720 --> 00:03:25,050
can actually respond to
your users in a safe manner

70
00:03:25,050 --> 00:03:27,060
and how to make sure
that it's only handling

71
00:03:27,060 --> 00:03:29,610
the types of workflows
and types of requests

72
00:03:29,610 --> 00:03:31,160
that you design the system for.

73
00:03:35,070 --> 00:03:38,010
So some key factors to think about when

74
00:03:38,010 --> 00:03:41,310
you defining your user requirements

75
00:03:41,310 --> 00:03:44,700
and designing your
generative AI application

76
00:03:44,700 --> 00:03:46,350
includes a couple of facets here.

77
00:03:48,210 --> 00:03:49,982
Sometimes you're gonna have to think about

78
00:03:49,982 --> 00:03:52,581
whether or not you're
going to build functions

79
00:03:52,581 --> 00:03:54,180
through static LLM workflows.

80
00:03:54,180 --> 00:03:55,680
Sometimes you're going want to see

81
00:03:55,680 --> 00:03:58,710
how you could apply more
dynamic agenetic workflows.

82
00:03:58,710 --> 00:04:01,755
Well, but the common thing about these

83
00:04:01,755 --> 00:04:04,800
is that they all involve
different levels of complexity

84
00:04:04,800 --> 00:04:07,503
based on the tasks that
you're asking to accomplish.

85
00:04:08,700 --> 00:04:10,740
Sometimes you wanna
respond instantaneously

86
00:04:10,740 --> 00:04:12,450
or as quick as possible in real time,

87
00:04:12,450 --> 00:04:17,421
such as answering a search
phrase and giving back

88
00:04:17,421 --> 00:04:20,340
a rich summarization of search results.

89
00:04:20,340 --> 00:04:23,361
Sometimes you wanna take a whole lot of

90
00:04:23,361 --> 00:04:26,280
conversational thread and text

91
00:04:26,280 --> 00:04:28,860
and summarize that on a daily basis.

92
00:04:28,860 --> 00:04:30,630
Those are more latency insensitive types

93
00:04:30,630 --> 00:04:33,120
of workflows over there.

94
00:04:33,120 --> 00:04:34,830
But the main thing is also it's like,

95
00:04:34,830 --> 00:04:36,360
just like traditional applications,

96
00:04:36,360 --> 00:04:38,550
you always have to be cognizant

97
00:04:38,550 --> 00:04:40,500
and aware around latency sensitivity

98
00:04:40,500 --> 00:04:42,330
for your users based
on the type of use case

99
00:04:42,330 --> 00:04:44,463
that's being considered at the time.

100
00:04:45,911 --> 00:04:48,990
This is where Amazon Bedrock comes in

101
00:04:48,990 --> 00:04:51,330
where we talked about different complexity

102
00:04:51,330 --> 00:04:53,550
for different tasks, you
have a lot of selections

103
00:04:53,550 --> 00:04:55,492
and models within Amazon
Bedrock to choose from

104
00:04:55,492 --> 00:04:58,561
to actually able to
accomplish those tasks.

105
00:04:58,561 --> 00:05:01,960
Making those choices helps
you having that select option

106
00:05:02,822 --> 00:05:05,520
and making those choices helps
you see which ones are best

107
00:05:05,520 --> 00:05:07,800
for the type of use case
you're trying to accomplish.

108
00:05:07,800 --> 00:05:12,800
And also helps with managing
costs over time, as well.

109
00:05:12,900 --> 00:05:16,200
And similarly with
figuring out is real time

110
00:05:16,200 --> 00:05:19,170
or asynchronous response,
you have also choices

111
00:05:19,170 --> 00:05:20,003
with Bedrock.

112
00:05:20,003 --> 00:05:25,003
You could choose what provision,
throughput, priority tiers

113
00:05:25,440 --> 00:05:28,260
or just the standard tier
to answer most questions.

114
00:05:28,260 --> 00:05:31,470
You could defer some inference
to either batch inference

115
00:05:31,470 --> 00:05:33,780
or to the flex tier if you wanted,

116
00:05:33,780 --> 00:05:36,870
if you're not in such a
rush to actually respond

117
00:05:36,870 --> 00:05:37,830
to those questions.

118
00:05:37,830 --> 00:05:40,863
And making those choices also
helps with managing costs.

119
00:05:43,170 --> 00:05:46,410
As your application matures
and improves over time,

120
00:05:46,410 --> 00:05:48,390
you want have a strategy
on how you're gonna upgrade

121
00:05:48,390 --> 00:05:50,316
and replace those models.

122
00:05:50,316 --> 00:05:53,940
Either upgrade to a new
version of that same model,

123
00:05:53,940 --> 00:05:57,581
upgrade to a different model that you see

124
00:05:57,581 --> 00:06:00,360
that it could also
accomplish the same task.

125
00:06:00,360 --> 00:06:03,270
You also have to have a way
to optimize model responses

126
00:06:03,270 --> 00:06:05,163
over and over iteratively over time.

127
00:06:06,210 --> 00:06:08,780
This is where Bedrock prompt management

128
00:06:08,780 --> 00:06:09,900
and prompt tuning comes into play.

129
00:06:09,900 --> 00:06:12,210
You could have, you
could use LN as a judge

130
00:06:12,210 --> 00:06:15,285
and also have a a set of
prompts that you catalog

131
00:06:15,285 --> 00:06:17,490
so you could actually apply
them to different models

132
00:06:17,490 --> 00:06:19,020
to compare over time.

133
00:06:19,020 --> 00:06:21,330
You could use prompt tuning
and use the Prompt Optimizer

134
00:06:21,330 --> 00:06:23,769
to get suggestions on how you, you could

135
00:06:23,769 --> 00:06:27,681
basically submit different
prompts and context

136
00:06:27,681 --> 00:06:30,003
and see what the results are.

137
00:06:33,270 --> 00:06:35,430
So after all those fundamental
application decisions

138
00:06:35,430 --> 00:06:38,190
are made, you wanna think
about how you're gonna operate

139
00:06:38,190 --> 00:06:40,833
and basically run at scale.

140
00:06:42,600 --> 00:06:44,910
So starting with a choice
around which region

141
00:06:44,910 --> 00:06:48,030
you're gonna pick to actually
host your application,

142
00:06:48,030 --> 00:06:51,420
based on your user requirements
or regulatory needs,

143
00:06:51,420 --> 00:06:53,130
that's gonna define
where you're gonna start

144
00:06:53,130 --> 00:06:54,843
your interactions with Bedrock.

145
00:06:56,160 --> 00:06:59,010
You wanna establish a monitoring strategy

146
00:06:59,010 --> 00:07:00,651
immediately from there.

147
00:07:00,651 --> 00:07:03,700
You'll be either monitoring for time

148
00:07:04,555 --> 00:07:06,960
to first token for your for your request.

149
00:07:06,960 --> 00:07:08,370
You may be using CloudWatch metrics

150
00:07:08,370 --> 00:07:11,850
to actually view end
to end-to-end latency.

151
00:07:11,850 --> 00:07:13,320
How often are you getting errors,

152
00:07:13,320 --> 00:07:14,730
how often you're throttling.

153
00:07:14,730 --> 00:07:16,560
You wanna use CloudWatch logs for example,

154
00:07:16,560 --> 00:07:18,810
or your own login system to actually see

155
00:07:18,810 --> 00:07:21,630
how the agents are
responding or what the LM

156
00:07:21,630 --> 00:07:22,830
is doing on the backend.

157
00:07:23,906 --> 00:07:26,733
You wanna able tune for performance.

158
00:07:27,687 --> 00:07:29,973
So you wanna have you tune
performance always comes

159
00:07:29,973 --> 00:07:32,430
into play some sort of caching strategy.

160
00:07:32,430 --> 00:07:35,370
You could use explicit prompt
caching for example on Bedrock

161
00:07:35,370 --> 00:07:38,640
to actually able to cache common

162
00:07:38,640 --> 00:07:42,210
and frequently frequent used inputs

163
00:07:42,210 --> 00:07:44,400
so that you could save time
and costs on transforming

164
00:07:44,400 --> 00:07:46,383
that input into to input tokens.

165
00:07:47,640 --> 00:07:51,180
And we talked about how
Bedrock had model flexibility

166
00:07:51,180 --> 00:07:52,800
to help with cost management.

167
00:07:52,800 --> 00:07:55,230
It also very much comes
into play to help you

168
00:07:55,230 --> 00:07:56,760
with resiliency as well.

169
00:07:56,760 --> 00:07:59,970
So that if for ever reason, yeah the model

170
00:07:59,970 --> 00:08:04,230
starts misbehaving or you have
some availability concerns

171
00:08:04,230 --> 00:08:07,110
around that, around that model,

172
00:08:07,110 --> 00:08:11,520
you should always establish
a deeper model strategy

173
00:08:11,520 --> 00:08:13,683
to able have your application covered.

174
00:08:17,070 --> 00:08:19,470
As your application really
grows out over time,

175
00:08:19,470 --> 00:08:21,660
you wanna be able to use other
regions that has capacity

176
00:08:21,660 --> 00:08:23,360
that's also helps in those models.

177
00:08:24,480 --> 00:08:27,150
With Bedrock cross region
inference, you could ask it

178
00:08:27,150 --> 00:08:30,210
to actually send your
request to another region

179
00:08:30,210 --> 00:08:33,660
that's hosting that model
what is on the same continent

180
00:08:33,660 --> 00:08:35,760
or even halfway across the world, as well.

181
00:08:38,760 --> 00:08:40,860
Secured by design is
always an essential part

182
00:08:40,860 --> 00:08:42,660
of productionizing your application.

183
00:08:44,310 --> 00:08:47,280
Starting with that basic
blueprint we talked about before,

184
00:08:47,280 --> 00:08:50,922
the first thing you
could rest assure is that

185
00:08:50,922 --> 00:08:54,821
Bedrock has been designed
so that model providers

186
00:08:54,821 --> 00:08:56,340
cannot access environment
that's actually hosting

187
00:08:56,340 --> 00:08:57,750
their model.

188
00:08:57,750 --> 00:09:01,410
And on the flip side as well,
we don't by design share

189
00:09:01,410 --> 00:09:05,310
any of the inputs and
outputs that's being fed in

190
00:09:05,310 --> 00:09:06,720
and coming out from the models

191
00:09:06,720 --> 00:09:08,320
that you use on Bedrock as well.

192
00:09:09,900 --> 00:09:12,130
You can imply IAM policies to make sure

193
00:09:14,092 --> 00:09:16,110
that only the specific
parts and specific users

194
00:09:16,110 --> 00:09:19,350
in your organization and
you and in your environment

195
00:09:19,350 --> 00:09:21,360
can actually access the models

196
00:09:21,360 --> 00:09:25,320
that you have already started
leveraging on Bedrock.

197
00:09:25,320 --> 00:09:27,480
You can use service control
policies to make sure

198
00:09:27,480 --> 00:09:29,040
that only the specific models

199
00:09:29,040 --> 00:09:31,470
that your organization has approved

200
00:09:31,470 --> 00:09:33,920
is actually available
within your infrastructure.

201
00:09:35,070 --> 00:09:38,850
You even have the added
control with VPC endpoints

202
00:09:38,850 --> 00:09:42,600
so that reaching Amazon Bedrock
through your infrastructure

203
00:09:42,600 --> 00:09:45,003
has to go through a specific path.

204
00:09:47,310 --> 00:09:49,860
And remember how we talked
about using cross region

205
00:09:52,386 --> 00:09:53,250
inference to able to reach capacity

206
00:09:54,255 --> 00:09:56,070
wherever is available on Bedrock?

207
00:09:56,070 --> 00:09:59,460
Based on your specific
regulatory requirements

208
00:09:59,460 --> 00:10:02,430
or data sovereignty needs,
you can even restrict

209
00:10:02,430 --> 00:10:04,500
that cross region inference behavior

210
00:10:04,500 --> 00:10:07,380
to only certain geographic locations

211
00:10:07,380 --> 00:10:08,630
of your choosing as well.

212
00:10:12,870 --> 00:10:14,790
So coming back to this nuance that we have

213
00:10:14,790 --> 00:10:17,130
around making sure your
application responds

214
00:10:17,130 --> 00:10:19,140
in a safe manner and only handling things

215
00:10:19,140 --> 00:10:21,140
that you have specifically designed for.

216
00:10:22,470 --> 00:10:26,673
Let's talk about coming from
the basic response workflow.

217
00:10:27,690 --> 00:10:29,490
Looking at the user input side of things,

218
00:10:29,490 --> 00:10:33,030
you can apply Amazon Bedrock
guardrails to make sure

219
00:10:33,030 --> 00:10:37,320
that you could filter out
for a certain keyword.

220
00:10:37,320 --> 00:10:41,160
You could basically
deny dangerous content.

221
00:10:41,160 --> 00:10:43,830
You could actually pick
other types of custom topics

222
00:10:43,830 --> 00:10:46,440
that you don't want the system to handle.

223
00:10:46,440 --> 00:10:48,750
And if any of those
policies are triggered,

224
00:10:48,750 --> 00:10:50,730
you can have Bedrock respond back over

225
00:10:50,730 --> 00:10:52,353
custom moderator response.

226
00:10:54,030 --> 00:10:55,630
On the other end of the spectrum

227
00:10:56,620 --> 00:10:59,760
after your initial inputs have been vetted

228
00:10:59,760 --> 00:11:01,650
and then fed into the model,

229
00:11:01,650 --> 00:11:03,600
you can have Bedrock guardrails again,

230
00:11:03,600 --> 00:11:06,468
apply those tapes, same types of policies

231
00:11:06,468 --> 00:11:10,650
and add the additional
automated reasoning checks

232
00:11:10,650 --> 00:11:12,750
for correctness before sending

233
00:11:12,750 --> 00:11:14,250
the response back to the user.

234
00:11:17,610 --> 00:11:20,208
So now that we talked
about a basic blueprint on

235
00:11:20,208 --> 00:11:25,208
how to go from ideation to production,

236
00:11:25,320 --> 00:11:28,500
let's see how one of our
biggest customers, Slack,

237
00:11:28,500 --> 00:11:31,350
apply some of these
principles and rigor so

238
00:11:31,350 --> 00:11:33,750
that they can actually
deliver Slack AI effectively,

239
00:11:33,750 --> 00:11:35,940
securely and efficiently.

240
00:11:35,940 --> 00:11:37,640
All right, so take it away Austin.

241
00:11:43,710 --> 00:11:46,680
- Great, thank you for
that introduction Gene

242
00:11:46,680 --> 00:11:48,213
and thank you all for coming.

243
00:11:49,050 --> 00:11:51,660
We are gonna spend this time talking about

244
00:11:51,660 --> 00:11:55,320
how we at Slack are able
deliver our Slack AI features

245
00:11:55,320 --> 00:11:57,993
effectively, securely and efficiently.

246
00:11:59,190 --> 00:12:01,440
Specifically, our goal here is to be able

247
00:12:01,440 --> 00:12:03,720
to enable generative AI to permeate

248
00:12:03,720 --> 00:12:05,460
throughout the Slack product.

249
00:12:05,460 --> 00:12:08,850
And it's very easy to
sort of get to that 80%

250
00:12:08,850 --> 00:12:11,700
of a generative AI feature,
but really sets that moat

251
00:12:11,700 --> 00:12:12,990
and sets you apart.

252
00:12:12,990 --> 00:12:16,800
Is that extra 20% of
becoming more cost efficient,

253
00:12:16,800 --> 00:12:19,290
high or having higher quality?

254
00:12:19,290 --> 00:12:21,480
We're gonna spend this presentation going

255
00:12:21,480 --> 00:12:24,180
through our journey of
how we were able to sort

256
00:12:24,180 --> 00:12:27,873
of meet our scale and meet our
quality needs here at Slack.

257
00:12:28,920 --> 00:12:33,360
Specifically, we're gonna talk
about three different areas.

258
00:12:33,360 --> 00:12:35,130
We're gonna discuss how we developed

259
00:12:35,130 --> 00:12:37,020
our infrastructure layer that allowed us

260
00:12:37,020 --> 00:12:39,900
to sort of meet our scaling needs in a way

261
00:12:39,900 --> 00:12:42,330
that was highly secure and met all

262
00:12:42,330 --> 00:12:44,370
of our compliance requirements.

263
00:12:44,370 --> 00:12:46,350
We're gonna talk about how we developed

264
00:12:46,350 --> 00:12:49,680
an internal experimentation
framework that allowed us

265
00:12:49,680 --> 00:12:51,540
to objectively measure the quality

266
00:12:51,540 --> 00:12:54,870
of our generative AI outputs
and give us the confidence

267
00:12:54,870 --> 00:12:56,850
that we are actually
tangibly moving the needle

268
00:12:56,850 --> 00:12:58,350
in that quality.

269
00:12:58,350 --> 00:13:00,030
Lastly, we're gonna talk about how those

270
00:13:00,030 --> 00:13:03,480
two different sections come
together to actually enable us

271
00:13:03,480 --> 00:13:06,810
to integrate generative
AI more seamlessly across

272
00:13:06,810 --> 00:13:07,653
the product.

273
00:13:08,880 --> 00:13:11,310
But first, a little bit about who we are.

274
00:13:11,310 --> 00:13:13,170
My name is Austin Bell.

275
00:13:13,170 --> 00:13:15,690
I am a director here at Slack,

276
00:13:15,690 --> 00:13:18,300
responsible across machine
learning search and AI

277
00:13:18,300 --> 00:13:19,983
and I'm joined by my colleague.

278
00:13:21,653 --> 00:13:23,880
- Hey everyone, my name is Shaurya.

279
00:13:23,880 --> 00:13:25,590
I'm an engineer here at Slack working on

280
00:13:25,590 --> 00:13:27,633
our infrastructure and platform team.

281
00:13:30,210 --> 00:13:31,920
- So just to level set a little bit,

282
00:13:31,920 --> 00:13:35,340
I'm gonna assume some
familiarity of what Slack is,

283
00:13:35,340 --> 00:13:39,330
but for anybody who's not
aware of our Slack AI offering,

284
00:13:39,330 --> 00:13:43,980
we support over a dozen
different generative AI features

285
00:13:43,980 --> 00:13:47,700
and tools that kind of spanned
the spectrum of complexity

286
00:13:47,700 --> 00:13:49,260
and use cases.

287
00:13:49,260 --> 00:13:51,120
For example, we offer the ability

288
00:13:51,120 --> 00:13:53,520
to do AI summaries across a variety

289
00:13:53,520 --> 00:13:55,920
of different product surface areas.

290
00:13:55,920 --> 00:13:59,430
We have our in-house QA systems
where you can ask questions

291
00:13:59,430 --> 00:14:02,280
and find information related
to the data that you house

292
00:14:02,280 --> 00:14:05,850
within Slack, we have the
ability to create daily digest

293
00:14:05,850 --> 00:14:07,770
to get up to date very quickly on

294
00:14:07,770 --> 00:14:09,480
everything that's going on.

295
00:14:09,480 --> 00:14:11,910
These are just a small number of examples

296
00:14:11,910 --> 00:14:15,753
and we continue to add more
on a very, very regular basis.

297
00:14:16,920 --> 00:14:19,830
Now for our first topic,
I'm gonna pass it over

298
00:14:19,830 --> 00:14:21,840
to my colleague Sharia,
who's gonna discuss

299
00:14:21,840 --> 00:14:24,693
how we have built our
infrastructure layer here at Slack.

300
00:14:29,070 --> 00:14:31,170
- Thank you Austin for the introduction.

301
00:14:31,170 --> 00:14:34,710
Hey everyone, I'm here to talk
about how we scale Slack AI

302
00:14:34,710 --> 00:14:38,010
to the millions of daily
active users that we have.

303
00:14:38,010 --> 00:14:40,560
So generally when we
talk about generative AI,

304
00:14:40,560 --> 00:14:42,480
it usually is around speed.

305
00:14:42,480 --> 00:14:46,110
How fast are tokens coming,
how fast is a product moving

306
00:14:46,110 --> 00:14:49,200
and how fast is the
market moving, as well.

307
00:14:49,200 --> 00:14:53,670
But here at Slack we
also add factor of trust.

308
00:14:53,670 --> 00:14:56,710
We ensure that the services
that we use have the trust

309
00:14:58,039 --> 00:14:59,430
that we need to protect
the customer's data

310
00:14:59,430 --> 00:15:01,350
and their trust in us to ensure that

311
00:15:01,350 --> 00:15:03,633
they're able to work within our platform.

312
00:15:05,310 --> 00:15:08,580
So one quote that we usually
go by is the real test

313
00:15:08,580 --> 00:15:11,250
of scalable infrastructure
isn't just how fast it grows,

314
00:15:11,250 --> 00:15:14,880
but it's also how well it
protects what matters as it grows.

315
00:15:14,880 --> 00:15:17,190
So as you can imagine,
a lot of people have

316
00:15:17,190 --> 00:15:19,860
all their working content within Slack.

317
00:15:19,860 --> 00:15:23,010
We want to ensure that whichever
features that we do add,

318
00:15:23,010 --> 00:15:26,130
it enables them to keep working with Slack

319
00:15:26,130 --> 00:15:28,530
while not having to add
any roadblocks in there.

320
00:15:30,138 --> 00:15:35,031
So before going into the technical aspect

321
00:15:35,031 --> 00:15:37,410
of how we are scaling Slack AI is,

322
00:15:37,410 --> 00:15:40,025
I like to set a little
bit of context about

323
00:15:40,025 --> 00:15:43,800
what product promises that
we have for our features.

324
00:15:43,800 --> 00:15:46,140
The first pillar is trust.

325
00:15:46,140 --> 00:15:50,950
So whenever we started
scaling out Slack AI question

326
00:15:51,887 --> 00:15:54,090
that we've gotten a lot from customers is,

327
00:15:54,090 --> 00:15:57,720
is our data being used to
train generative AI models?

328
00:15:57,720 --> 00:15:59,640
And the answer is no.

329
00:15:59,640 --> 00:16:03,270
So we don't train generative
models on customer data,

330
00:16:03,270 --> 00:16:05,460
we don't log customer data.

331
00:16:05,460 --> 00:16:08,730
We also allow the admins
of the Slack workspaces

332
00:16:08,730 --> 00:16:10,260
to opt out of features.

333
00:16:10,260 --> 00:16:14,250
So this means that the admins
can opt into a certain feature

334
00:16:14,250 --> 00:16:16,680
or opt out a certain feature as well.

335
00:16:16,680 --> 00:16:19,410
We also have zero retention of the data

336
00:16:19,410 --> 00:16:22,020
and the data that's being
sent to the LM providers

337
00:16:22,020 --> 00:16:23,190
is not being shared.

338
00:16:23,190 --> 00:16:25,680
So as Gene mentioned,
the inputs and outputs

339
00:16:25,680 --> 00:16:26,730
are not being shared.

340
00:16:28,440 --> 00:16:30,660
The second pillar is security.

341
00:16:30,660 --> 00:16:34,650
We operate in a FedRAMP
moderate compliance space.

342
00:16:34,650 --> 00:16:37,470
So we ensure that the
services that we use meet

343
00:16:37,470 --> 00:16:42,000
that rigorous standard set
by the federal guidelines.

344
00:16:42,000 --> 00:16:43,890
We also make sure that the services

345
00:16:43,890 --> 00:16:45,990
that we do use stay
within our trust boundary

346
00:16:45,990 --> 00:16:48,370
as there's a lot of
data being moved around

347
00:16:49,290 --> 00:16:52,080
and we also have
technical access controls.

348
00:16:52,080 --> 00:16:54,300
So this means that for example,

349
00:16:54,300 --> 00:16:56,430
if there's message in a private channel,

350
00:16:56,430 --> 00:16:58,360
we don't want people who are not in

351
00:16:59,879 --> 00:17:01,110
that channel to have access to it.

352
00:17:01,110 --> 00:17:03,480
So we ensure that there's
security of the messages

353
00:17:03,480 --> 00:17:05,553
and even the answers being shared.

354
00:17:07,950 --> 00:17:11,130
The third pillar I'd like
to cover is reliability.

355
00:17:11,130 --> 00:17:13,950
So having trust and
security is not enough.

356
00:17:13,950 --> 00:17:17,730
We also need to have the
features set be highly available.

357
00:17:17,730 --> 00:17:19,410
This is what enables the customers

358
00:17:19,410 --> 00:17:22,470
to actually use it in their daily process.

359
00:17:22,470 --> 00:17:25,920
On top of that, we also want
to have contextual relevance.

360
00:17:25,920 --> 00:17:29,730
So this means that whenever a
customer is sending a request,

361
00:17:29,730 --> 00:17:30,990
we want to make sure that the answer

362
00:17:30,990 --> 00:17:32,640
that they're getting is actually relevant

363
00:17:32,640 --> 00:17:36,360
to what they're asking for
and transparency as well.

364
00:17:36,360 --> 00:17:39,750
So this means that whenever
they do get an answer,

365
00:17:39,750 --> 00:17:42,650
we're able to add
citations wherever possible

366
00:17:42,650 --> 00:17:46,110
so the customers can backtrack
to see the main message

367
00:17:46,110 --> 00:17:49,113
that the LLM may have used
to generate that particular.

368
00:17:52,410 --> 00:17:54,990
So having all these pillars,
it may be pretty easy

369
00:17:54,990 --> 00:17:57,090
to do it when we have 10 users,

370
00:17:57,090 --> 00:18:00,510
but let's take a look at how
Slack is operating right now.

371
00:18:00,510 --> 00:18:02,850
Every week we are processing more than

372
00:18:02,850 --> 00:18:07,710
one to 5 billion messages, a
hundred to 500 million files

373
00:18:07,710 --> 00:18:10,260
and one to 5 billion searches.

374
00:18:10,260 --> 00:18:13,620
So as you can see, many
of like Slack is operating

375
00:18:13,620 --> 00:18:15,030
at a massive scale.

376
00:18:15,030 --> 00:18:18,330
We can't just plug in our AI
features into a public API

377
00:18:18,330 --> 00:18:19,830
and hope for the best.

378
00:18:19,830 --> 00:18:21,720
We need to ensure that the services

379
00:18:21,720 --> 00:18:24,480
that we use are properly
tested behind the scenes

380
00:18:24,480 --> 00:18:27,300
and can handle our millions
and billions of requests

381
00:18:27,300 --> 00:18:29,820
without breaking a sweat.

382
00:18:29,820 --> 00:18:34,263
As well, as ensuring our
security and trust postures.

383
00:18:36,990 --> 00:18:40,500
So now that we've set the
context of the products

384
00:18:40,500 --> 00:18:42,093
and also the scale of Slack,

385
00:18:42,990 --> 00:18:46,410
let's start with the past
journey of how we used

386
00:18:46,410 --> 00:18:51,410
to serve Slack AI in mid
2023 up until mid 2024,

387
00:18:51,720 --> 00:18:54,480
at the time we had limited LLM options,

388
00:18:54,480 --> 00:18:57,450
we had high costs, low flexibility,

389
00:18:57,450 --> 00:19:00,250
and we're running in a provision
throughput environment.

390
00:19:01,290 --> 00:19:03,930
From there we have now gone to the future

391
00:19:03,930 --> 00:19:08,130
where we have high flexibility,
high utilization efficiency,

392
00:19:08,130 --> 00:19:11,193
and as well as increased reliability.

393
00:19:14,040 --> 00:19:17,683
So when we first started in mid 2023,

394
00:19:17,683 --> 00:19:19,710
we were looking for services
that Amazon offered,

395
00:19:19,710 --> 00:19:22,710
which were within the FedRAMP
moderate compliance space

396
00:19:22,710 --> 00:19:27,710
as well as compliant with our
security and trust postures.

397
00:19:28,500 --> 00:19:32,400
So we started with SageMaker
and for context too,

398
00:19:32,400 --> 00:19:35,610
Slack mostly operates within
the US East one region

399
00:19:35,610 --> 00:19:39,453
and we have our VPC which
contains the Slack instance.

400
00:19:40,380 --> 00:19:43,560
From there, the requests go
through the VPC endpoint,

401
00:19:43,560 --> 00:19:46,465
like the Slack AI requests,
which is basically

402
00:19:46,465 --> 00:19:48,810
allowing the requests to
go through the internal

403
00:19:48,810 --> 00:19:50,490
networking of AWS.

404
00:19:50,490 --> 00:19:53,190
And that request goes to
the SageMaker endpoint,

405
00:19:53,190 --> 00:19:58,190
which is the basically like
the wrapper around the model.

406
00:19:58,620 --> 00:20:01,590
And when we first started
we only had one model

407
00:20:01,590 --> 00:20:04,623
and this model was served in
a provision throughput manner.

408
00:20:04,623 --> 00:20:08,222
And you may also see
a small box in our VPC

409
00:20:08,222 --> 00:20:09,210
called the concurrency checker.

410
00:20:09,210 --> 00:20:11,760
And this is what we use to maintain load.

411
00:20:11,760 --> 00:20:14,670
So at times of peak load we were able

412
00:20:14,670 --> 00:20:17,100
to shed the lower priority requests.

413
00:20:17,100 --> 00:20:20,130
So for example, we have
three priority internally,

414
00:20:20,130 --> 00:20:23,220
the highest priority, which
is the most latency sensitive,

415
00:20:23,220 --> 00:20:25,470
the medium priority,
which can be done within

416
00:20:25,470 --> 00:20:27,240
a five to 10 minute SLA.

417
00:20:27,240 --> 00:20:29,340
And then our third tier priority,

418
00:20:29,340 --> 00:20:32,100
which is like our batch
jobs, which are on overnight.

419
00:20:32,100 --> 00:20:34,710
And during the times of high concurrency

420
00:20:34,710 --> 00:20:37,920
when we have our cluster
pretty much at peak utilization

421
00:20:37,920 --> 00:20:40,680
instead of having to scale
up, which could take more than

422
00:20:40,680 --> 00:20:43,290
an hour at that time we
were able to load shed

423
00:20:43,290 --> 00:20:44,123
pretty quickly.

424
00:20:46,800 --> 00:20:49,380
Though this worked during
the initial days of Slack AI,

425
00:20:49,380 --> 00:20:51,850
when we were still scaling up, we noticed

426
00:20:53,966 --> 00:20:55,848
that Slack AI was also getting added on

427
00:20:55,848 --> 00:20:57,870
pretty exponential exponentially
to the customer basis.

428
00:20:57,870 --> 00:20:59,760
So we noticed a couple of problems.

429
00:20:59,760 --> 00:21:02,220
The first one was peaky traffic.

430
00:21:02,220 --> 00:21:05,010
We had two different types
of requests coming in,

431
00:21:05,010 --> 00:21:08,100
the time sensitive ones
and the batch workloads.

432
00:21:08,100 --> 00:21:11,160
So these are pretty predictable
in our day-to-day scenarios.

433
00:21:11,160 --> 00:21:14,590
But these had different
consistencies of throughput

434
00:21:15,870 --> 00:21:19,002
and also it was difficult to obtain GPUs

435
00:21:19,002 --> 00:21:21,210
at the time it was the GPU crunch.

436
00:21:21,210 --> 00:21:23,040
So it was taking us weeks to get the GPUs

437
00:21:23,040 --> 00:21:25,332
that we need to scale up
for our new customer bases

438
00:21:25,332 --> 00:21:27,960
who are added to Slack AI.

439
00:21:27,960 --> 00:21:30,540
And because of the GPU
crunch, we weren't able

440
00:21:30,540 --> 00:21:32,280
to scale up and down easily.

441
00:21:32,280 --> 00:21:35,417
We were having to
maintain our GPU instances

442
00:21:35,417 --> 00:21:38,190
in our on demand capacity reserve.

443
00:21:38,190 --> 00:21:40,230
So then we were able to hold onto those

444
00:21:40,230 --> 00:21:42,993
during the non-peak times.

445
00:21:43,860 --> 00:21:46,080
So the impact of these two problems

446
00:21:46,080 --> 00:21:47,970
were that we were over-provisioned

447
00:21:47,970 --> 00:21:50,160
for the majority of the day and this was

448
00:21:50,160 --> 00:21:53,313
because of having to keep
our GPUs up and running.

449
00:21:54,330 --> 00:21:57,030
And because of this we had
our infrastructure scaled

450
00:21:57,030 --> 00:22:01,320
to support the peak traffic,
which isn't ideal as this cost,

451
00:22:01,320 --> 00:22:04,020
this basically causes cost inefficiency.

452
00:22:04,020 --> 00:22:07,320
We're paying for instances and GPU time,

453
00:22:07,320 --> 00:22:11,220
which we aren't actively using
to serve customer requests.

454
00:22:11,220 --> 00:22:14,280
And because of this
fixed cost that we have,

455
00:22:14,280 --> 00:22:16,890
we weren't able to diversify our LLMs.

456
00:22:16,890 --> 00:22:19,530
We only had one or maybe even two.

457
00:22:19,530 --> 00:22:23,700
But this also slowed us
down when experimenting

458
00:22:23,700 --> 00:22:25,923
with new models or even
adding new features.

459
00:22:27,270 --> 00:22:29,790
So keeping these problems in mind,

460
00:22:29,790 --> 00:22:31,980
we had a vision about
where we wanted to go.

461
00:22:31,980 --> 00:22:35,400
We wanted to look for a new
service which is managed

462
00:22:35,400 --> 00:22:39,360
but also gives us a diversity of models.

463
00:22:39,360 --> 00:22:43,830
So this is when we were
able to come across Bedrock.

464
00:22:43,830 --> 00:22:48,830
So in mid 2024, Bedrock became
FedRAMP moderate compliant

465
00:22:48,990 --> 00:22:50,850
and we were able to take a more serious

466
00:22:50,850 --> 00:22:52,740
look into the service.

467
00:22:52,740 --> 00:22:54,870
We were able to serve all these requests

468
00:22:54,870 --> 00:22:57,030
within our trust boundary.

469
00:22:57,030 --> 00:23:00,240
And Bedrock also promises
that the input outputs

470
00:23:00,240 --> 00:23:02,610
don't get shared with the providers

471
00:23:02,610 --> 00:23:05,970
so that they're not able to
train on our inputs and outputs.

472
00:23:05,970 --> 00:23:09,540
Bedrock also has a whole
collection of frontier models.

473
00:23:09,540 --> 00:23:12,900
So this was able, this was
enabling our product engineers

474
00:23:12,900 --> 00:23:15,033
to add more features.

475
00:23:15,900 --> 00:23:18,270
And with Bedrock two we were able

476
00:23:18,270 --> 00:23:21,180
to see models getting
added to the model registry

477
00:23:21,180 --> 00:23:22,320
at a faster rate.

478
00:23:22,320 --> 00:23:26,160
So within a day of whenever
these models get published,

479
00:23:26,160 --> 00:23:27,960
they're within the Bedrock platform.

480
00:23:29,940 --> 00:23:33,982
Once we essentially set Bedrock
as our service to migrate to

481
00:23:33,982 --> 00:23:36,750
we a couple of migration steps to ensure

482
00:23:36,750 --> 00:23:40,590
that we have a very reliable
migration while keeping

483
00:23:40,590 --> 00:23:42,483
customer trust and no downtime.

484
00:23:43,320 --> 00:23:46,680
The first aspect was to
understand Bedrock infrastructure

485
00:23:46,680 --> 00:23:48,870
and the two differences were the provision

486
00:23:48,870 --> 00:23:50,910
throughput and the on demand.

487
00:23:50,910 --> 00:23:53,700
And because in the SageMaker
world we were in the provision

488
00:23:53,700 --> 00:23:55,620
throughput infrastructure type,

489
00:23:55,620 --> 00:23:57,420
we decided to keep the same in Bedrock

490
00:23:57,420 --> 00:23:59,550
to make the migration
easier and then handle

491
00:23:59,550 --> 00:24:03,003
the on demand transition the next stage.

492
00:24:04,470 --> 00:24:07,290
The second part was to do
some internal load testing

493
00:24:07,290 --> 00:24:10,020
and doing some compute calculation.

494
00:24:10,020 --> 00:24:13,950
So this means that we're working
on scientifically figuring

495
00:24:13,950 --> 00:24:16,560
out what is the equivalent
of Bedrock Mall units

496
00:24:16,560 --> 00:24:18,920
to SageMaker compute that we already had.

497
00:24:18,920 --> 00:24:21,420
So then we were able to
get the equivalent compute

498
00:24:22,589 --> 00:24:24,510
without having to guess and check.

499
00:24:24,510 --> 00:24:27,960
So here's an example of
one of our load tests.

500
00:24:27,960 --> 00:24:29,760
We were able to run Claude Instant

501
00:24:29,760 --> 00:24:32,910
and Claude Haiku on both
SageMaker and Bedrock.

502
00:24:32,910 --> 00:24:35,250
And as you can see, for Claude Instant,

503
00:24:35,250 --> 00:24:37,050
it's like a one-to-one mapping.

504
00:24:37,050 --> 00:24:39,924
So when we did run SageMaker on a P4D,

505
00:24:39,924 --> 00:24:43,710
it was equivalent to a Bedrock Mall unit.

506
00:24:43,710 --> 00:24:46,140
And when we ran a Claude Haiku instance

507
00:24:46,140 --> 00:24:49,890
on a P5 instance type, it was two P5's

508
00:24:49,890 --> 00:24:52,203
were equivalent to one Bedrock Mall unit.

509
00:24:53,490 --> 00:24:55,350
So by using these ratios we were able

510
00:24:55,350 --> 00:24:57,627
to get the equivalent
compute and we worked with

511
00:24:57,627 --> 00:25:00,300
the Amazon Bedrock service
team to get that delivered

512
00:25:00,300 --> 00:25:01,133
to our account.

513
00:25:03,000 --> 00:25:05,130
Once we got the compute delivered to us,

514
00:25:05,130 --> 00:25:07,290
we were able to start
running shadow traffic.

515
00:25:07,290 --> 00:25:11,070
So this means that whenever a
request was sent to SageMaker,

516
00:25:11,070 --> 00:25:13,620
we were sending a duplicate
request to Bedrock.

517
00:25:13,620 --> 00:25:15,540
So then we were able
to get an understanding

518
00:25:15,540 --> 00:25:17,760
of the internals of the service,

519
00:25:17,760 --> 00:25:20,680
and this also helped us
out while building out

520
00:25:20,680 --> 00:25:22,410
our monitoring dashboards as well.

521
00:25:22,410 --> 00:25:24,796
So we were able to verify the latencies,

522
00:25:24,796 --> 00:25:26,700
the time of first token
and the other metrics

523
00:25:26,700 --> 00:25:28,353
that we had in place.

524
00:25:29,940 --> 00:25:33,840
Once the shadow traffic was
running a 100% for two weeks,

525
00:25:33,840 --> 00:25:36,420
we started to do a full cutover process.

526
00:25:36,420 --> 00:25:39,476
So this was in branches
too, so we started with

527
00:25:39,476 --> 00:25:42,323
a 1%, 5%, 10% and 100%.

528
00:25:42,323 --> 00:25:45,420
And this essentially means that
instead of the shadow mode,

529
00:25:45,420 --> 00:25:48,120
now we're serving the
response that was sent

530
00:25:48,120 --> 00:25:50,920
by the Bedrock service instead
of the SageMaker service.

531
00:25:52,800 --> 00:25:57,450
So by going to Bedrock,
this helped us save money

532
00:25:57,450 --> 00:26:00,480
and we were also able to experiment more,

533
00:26:00,480 --> 00:26:03,060
but we noticed a couple
more gaps that we could

534
00:26:03,060 --> 00:26:06,003
kind of work towards to make
this service more efficient.

535
00:26:07,350 --> 00:26:09,960
The first one was we weren't
able to scale PT still,

536
00:26:09,960 --> 00:26:13,230
it was still a static compute cluster

537
00:26:13,230 --> 00:26:16,410
and because of this we
were still experiencing

538
00:26:16,410 --> 00:26:19,023
some cost inefficiencies.

539
00:26:20,160 --> 00:26:22,980
And then within the platform
on our side, we noticed

540
00:26:22,980 --> 00:26:24,630
that it would be more
helpful if we're able

541
00:26:24,630 --> 00:26:26,070
to add backup models.

542
00:26:26,070 --> 00:26:29,340
So during the times when
Bedrock models had regressions

543
00:26:29,340 --> 00:26:32,040
or we were noticing some
features not performing

544
00:26:32,040 --> 00:26:35,797
as expected, we were able to
add backup models for them

545
00:26:35,797 --> 00:26:38,420
so that we can essentially reroute them

546
00:26:38,420 --> 00:26:42,420
without having to do any code
changes during incidents.

547
00:26:42,420 --> 00:26:46,080
We also added features for emergency stops

548
00:26:46,080 --> 00:26:48,720
for the particular
features and the models.

549
00:26:48,720 --> 00:26:50,760
So this is like basically it goes in play

550
00:26:50,760 --> 00:26:52,762
with the backup models.

551
00:26:52,762 --> 00:26:54,510
Whenever emergency stop is turned on,

552
00:26:54,510 --> 00:26:57,000
a whole feature can be
potentially turned off.

553
00:26:57,000 --> 00:26:59,730
And if a model is turned off on our side,

554
00:26:59,730 --> 00:27:02,823
then all the requests are
rerouted to our backup models.

555
00:27:03,960 --> 00:27:07,180
And when using Amazon
Bedrock two, we were exposed

556
00:27:07,180 --> 00:27:11,610
to being able to use different
tools that models have.

557
00:27:11,610 --> 00:27:16,380
So for example, tools,
prompt caching, guardrails,

558
00:27:16,380 --> 00:27:18,780
and other features that
the models do have.

559
00:27:18,780 --> 00:27:22,950
And we were able to expand
our internal AI platform

560
00:27:22,950 --> 00:27:26,283
to enable our developers
to have access to these.

561
00:27:28,440 --> 00:27:31,170
So from going from provision throughput,

562
00:27:31,170 --> 00:27:33,630
we noticed that we
weren't able to scale PT.

563
00:27:33,630 --> 00:27:35,730
So this was when we
decided to move towards

564
00:27:35,730 --> 00:27:37,500
the on-demand world.

565
00:27:37,500 --> 00:27:39,810
And this is a different
infrastructure type

566
00:27:39,810 --> 00:27:42,822
as it's now based more
on quotas rather than

567
00:27:42,822 --> 00:27:45,270
the Bayer instances.

568
00:27:45,270 --> 00:27:47,673
And this is being done
on a tokens per minute,

569
00:27:48,540 --> 00:27:51,240
the input tokens per minute
and request per minute basis.

570
00:27:51,240 --> 00:27:54,120
We also had a lot of metadata on our side

571
00:27:54,120 --> 00:27:55,830
during the provision throughput era.

572
00:27:55,830 --> 00:27:58,770
So we were able to
essentially calculate the RPM

573
00:27:58,770 --> 00:28:03,030
and TPMS from our metadata
and pass that compute request

574
00:28:03,030 --> 00:28:04,980
to Amazon Bedrock to get those quotas

575
00:28:04,980 --> 00:28:07,050
delivered to our accounts.

576
00:28:07,050 --> 00:28:09,810
So similar to how it was
within the SageMaker world,

577
00:28:09,810 --> 00:28:13,380
we had our Slack app within
the US East one region,

578
00:28:13,380 --> 00:28:16,050
the VPC holds a Slack instances,

579
00:28:16,050 --> 00:28:17,940
the request goes to the VPC endpoint,

580
00:28:17,940 --> 00:28:20,070
which does all the internal routing goes

581
00:28:20,070 --> 00:28:21,540
through the Bedrock service.

582
00:28:21,540 --> 00:28:24,840
And then based on the model
ID that we add to the request,

583
00:28:24,840 --> 00:28:26,943
it will point it to that particular model.

584
00:28:28,290 --> 00:28:31,590
One difference here from the
SageMaker architecture diagram

585
00:28:31,590 --> 00:28:33,720
is instead of a concurrency checker,

586
00:28:33,720 --> 00:28:36,780
we had a RPM and TPM checker on our side

587
00:28:36,780 --> 00:28:39,420
and the RPM TPM is for
the requests per minute

588
00:28:39,420 --> 00:28:40,740
and tokens per minute.

589
00:28:40,740 --> 00:28:43,859
And this enabled us to
essentially like keep our

590
00:28:43,859 --> 00:28:46,800
different features under control.

591
00:28:46,800 --> 00:28:50,580
And this means like we're able
to isolate certain features

592
00:28:50,580 --> 00:28:53,973
from taking over our
entire Bedrock cluster.

593
00:28:57,300 --> 00:29:00,480
And a different benefit that
we also got from Bedrock

594
00:29:00,480 --> 00:29:02,740
on Demand is now we're able to use

595
00:29:04,757 --> 00:29:06,374
US cross region inference profiles.

596
00:29:06,374 --> 00:29:08,040
And because we are a
FedRAMP moderate shop,

597
00:29:08,040 --> 00:29:10,140
we needed to keep it
within the US boundaries

598
00:29:10,140 --> 00:29:13,092
and Bedrock provides that ability

599
00:29:13,092 --> 00:29:16,380
and by going to US West two as well,

600
00:29:16,380 --> 00:29:18,780
we're able to get our
compute delivered to us

601
00:29:18,780 --> 00:29:21,813
at a much faster rate as we
had two regions to choose from.

602
00:29:24,270 --> 00:29:26,850
So with all of this,
we had a bunch of wins

603
00:29:26,850 --> 00:29:28,770
at the end of the migration.

604
00:29:28,770 --> 00:29:31,200
When we first started, we had one LLM

605
00:29:31,200 --> 00:29:33,240
to start to choose from.

606
00:29:33,240 --> 00:29:35,190
And by the end of the migration

607
00:29:35,190 --> 00:29:37,440
and current state right
now we're experimenting

608
00:29:37,440 --> 00:29:40,950
and serving greater than
15 LLMs in production.

609
00:29:40,950 --> 00:29:42,960
We also have increased reliability.

610
00:29:42,960 --> 00:29:46,597
So because of the higher
flexibility of LLMs,

611
00:29:46,597 --> 00:29:48,995
we were able to fall
back to certain models,

612
00:29:48,995 --> 00:29:51,658
we were able to quickly switch
models during incident times

613
00:29:51,658 --> 00:29:56,130
and even experiment around
to see what is the best model

614
00:29:56,130 --> 00:29:58,563
to serve for quality and cost.

615
00:29:59,970 --> 00:30:03,870
On top of that, our biggest number win

616
00:30:03,870 --> 00:30:06,480
was the utilization efficiency.

617
00:30:06,480 --> 00:30:08,850
During the whole migration,
we were still experiencing

618
00:30:08,850 --> 00:30:12,022
an exponential scale
increase of Slack AI usage

619
00:30:12,022 --> 00:30:14,460
as customers were still onboarding,

620
00:30:14,460 --> 00:30:17,220
but even with that
increased customer base,

621
00:30:17,220 --> 00:30:21,720
we saw greater than 90%
savings and in dollar values,

622
00:30:21,720 --> 00:30:23,943
this was greater than 20 million, as well.

623
00:30:25,530 --> 00:30:28,140
So to close it off, we started
off with a quote saying

624
00:30:28,140 --> 00:30:32,220
that it's not about how fast
you can scale infrastructure,

625
00:30:32,220 --> 00:30:35,220
but it's about how you
can secure your data

626
00:30:35,220 --> 00:30:36,720
as well as scaling it.

627
00:30:36,720 --> 00:30:39,480
And by using the services that Amazon has,

628
00:30:39,480 --> 00:30:42,916
we were able to deliver all
of our AI features at scale

629
00:30:42,916 --> 00:30:46,200
by collaborating with our internal cloud

630
00:30:46,200 --> 00:30:47,910
and platform teams as
well as collaborating

631
00:30:47,910 --> 00:30:51,210
with the AWS side, as well.

632
00:30:51,210 --> 00:30:52,920
Thank you, and now I'll pass it to Austin,

633
00:30:52,920 --> 00:30:56,283
talk about how we were able
to serve it with high quality.

634
00:30:58,357 --> 00:31:01,107
(audience claps)

635
00:31:02,010 --> 00:31:03,570
- Thank you for that Shaurya.

636
00:31:03,570 --> 00:31:06,840
We're gonna switch gears here a little bit

637
00:31:06,840 --> 00:31:09,390
and we're gonna talk
about how we've developed

638
00:31:09,390 --> 00:31:12,720
kind of an internal experimentation
framework that allows us

639
00:31:12,720 --> 00:31:14,730
to more objectively measure the quality

640
00:31:14,730 --> 00:31:17,370
of our generative AI outputs
so we can actually start

641
00:31:17,370 --> 00:31:20,070
to move the needle on this quality.

642
00:31:20,070 --> 00:31:22,920
Now this is a quote from
an engineer at Slack,

643
00:31:22,920 --> 00:31:25,710
and I don't know if you've
ever been in this situation,

644
00:31:25,710 --> 00:31:28,770
but you demo a generative AI feature.

645
00:31:28,770 --> 00:31:30,960
You get it really nice looking

646
00:31:30,960 --> 00:31:33,042
and you're really excited it comes down to

647
00:31:33,042 --> 00:31:35,160
time to productionize it.

648
00:31:35,160 --> 00:31:36,780
But you just have to kind of, you know,

649
00:31:36,780 --> 00:31:40,020
refine a couple edge
cases in terms of quality.

650
00:31:40,020 --> 00:31:43,230
You make prompt changes,
you make pipeline changes

651
00:31:43,230 --> 00:31:45,213
and you actually start to get that out.

652
00:31:46,975 --> 00:31:48,055
But after a few days you start

653
00:31:48,055 --> 00:31:49,980
to notice you actually
regressed in certain areas.

654
00:31:49,980 --> 00:31:51,810
This turns into kind of a cycle

655
00:31:51,810 --> 00:31:53,730
and a bit of a whack-a-mole situation

656
00:31:53,730 --> 00:31:55,290
where you're trying to fix one issue,

657
00:31:55,290 --> 00:31:57,840
but leading to another issue with actually

658
00:31:57,840 --> 00:32:01,113
no quality improvements
over the course of weeks.

659
00:32:02,310 --> 00:32:03,750
So why is this the case?

660
00:32:03,750 --> 00:32:08,400
Well, evaluating generative
outputs, it's difficult.

661
00:32:08,400 --> 00:32:11,520
We are kind of, or have been
in a bit of a paradigm shift

662
00:32:11,520 --> 00:32:12,990
from classical machine learning

663
00:32:12,990 --> 00:32:15,660
where in online settings
you could leverage

664
00:32:15,660 --> 00:32:18,120
engagement metrics to evaluate quality

665
00:32:18,120 --> 00:32:20,850
or in offline settings
you could leverage things

666
00:32:20,850 --> 00:32:22,890
like precision or recall.

667
00:32:22,890 --> 00:32:25,560
The outputs of generative
AI are significantly

668
00:32:25,560 --> 00:32:27,120
more subjective.

669
00:32:27,120 --> 00:32:29,700
What I consider a good
output may be very different

670
00:32:29,700 --> 00:32:31,490
than what you consider a good output.

671
00:32:31,490 --> 00:32:35,340
It may also be dependent
on the product surface area

672
00:32:35,340 --> 00:32:38,580
that you actually want to display
this generative AI output.

673
00:32:38,580 --> 00:32:41,640
So the question becomes
how do you actually start

674
00:32:41,640 --> 00:32:44,310
to measure this in a way
that meets your goals

675
00:32:44,310 --> 00:32:47,698
and actually continues
to allow you to improve.

676
00:32:47,698 --> 00:32:51,540
Now a key thing that we
believe here at Slack

677
00:32:51,540 --> 00:32:53,940
is that you can really only improve

678
00:32:53,940 --> 00:32:57,150
what you actually have
the ability to measure.

679
00:32:57,150 --> 00:32:59,760
So we first went on a journey
of actually trying to define

680
00:32:59,760 --> 00:33:01,620
what we wanted to measure.

681
00:33:01,620 --> 00:33:04,770
We defined by two pillars, quality.

682
00:33:04,770 --> 00:33:06,570
This is what we kind of refer to as like,

683
00:33:06,570 --> 00:33:09,720
is the answer giving you
what you actually wanted?

684
00:33:09,720 --> 00:33:12,120
Is it accurate to safety?

685
00:33:12,120 --> 00:33:15,060
Are we fostering the correct environment

686
00:33:15,060 --> 00:33:16,230
that we want at Slack?

687
00:33:16,230 --> 00:33:18,810
Are we ensuring that your data's as secure

688
00:33:18,810 --> 00:33:20,583
as it possibly can be?

689
00:33:21,510 --> 00:33:24,300
Within each of these, we
further broke these down.

690
00:33:24,300 --> 00:33:26,340
On the quality side, we
broke it down into two

691
00:33:26,340 --> 00:33:28,050
separate categories.

692
00:33:28,050 --> 00:33:29,910
First objective measurements.

693
00:33:29,910 --> 00:33:31,560
This is what we consider kind of more

694
00:33:31,560 --> 00:33:33,900
of our deterministic outputs.

695
00:33:33,900 --> 00:33:35,970
Things that are somewhat table stakes,

696
00:33:35,970 --> 00:33:37,710
being able to render it properly,

697
00:33:37,710 --> 00:33:41,070
being able to parse a JSON
output or an XML output.

698
00:33:41,070 --> 00:33:43,170
Are we formatting IDs correctly?

699
00:33:43,170 --> 00:33:44,550
Are we formatting links correctly

700
00:33:44,550 --> 00:33:47,280
so you can navigate Slack properly?

701
00:33:47,280 --> 00:33:49,980
Things that if we don't
have the user's gonna notice

702
00:33:49,980 --> 00:33:52,080
and it doesn't actually matter the content

703
00:33:52,080 --> 00:33:54,720
of your generative AI because
they're not gonna be able

704
00:33:54,720 --> 00:33:56,673
to look past some of these issues.

705
00:33:57,720 --> 00:34:01,050
On the harder side, we have
the subjective measurements.

706
00:34:01,050 --> 00:34:04,080
These are things like factual accuracy.

707
00:34:04,080 --> 00:34:06,870
Are you in fact telling
the truth based off

708
00:34:06,870 --> 00:34:09,150
of your grounded context?

709
00:34:09,150 --> 00:34:10,470
Answer relevancy.

710
00:34:10,470 --> 00:34:14,253
Are you answering the question
that the user actually asked?

711
00:34:14,253 --> 00:34:17,910
Attribution accuracy, this is
a big problem for us at Slack.

712
00:34:17,910 --> 00:34:20,340
Are we attributing the correct content

713
00:34:20,340 --> 00:34:23,193
to the correct user or
to the correct file?

714
00:34:24,300 --> 00:34:26,850
On the safety side, we
also broke this down

715
00:34:26,850 --> 00:34:28,860
into two separate categories.

716
00:34:28,860 --> 00:34:30,810
Harm and security.

717
00:34:30,810 --> 00:34:35,810
Harm refers to what we do
around measure around toxicity.

718
00:34:35,850 --> 00:34:37,650
Are we capturing bias?

719
00:34:37,650 --> 00:34:40,100
Are we ensuring that it
is a good environment

720
00:34:40,100 --> 00:34:42,660
and that our LLM is responding
in the way that aligns

721
00:34:42,660 --> 00:34:44,640
with our values?

722
00:34:44,640 --> 00:34:45,903
Second security.

723
00:34:46,754 --> 00:34:49,110
Are we protecting against
prompt injection attacks

724
00:34:49,110 --> 00:34:50,850
to the extent that we can?

725
00:34:50,850 --> 00:34:55,007
Are we preventing even search poisoning

726
00:34:56,490 --> 00:34:58,530
that you don't want to
do that is unintended

727
00:34:58,530 --> 00:35:00,993
or even is intended at search poisoning.

728
00:35:02,580 --> 00:35:04,710
Now that's kind of step one,

729
00:35:04,710 --> 00:35:08,790
defining what you
actually want to measure.

730
00:35:08,790 --> 00:35:12,090
The next part is actually
generating the evaluators

731
00:35:12,090 --> 00:35:15,300
that will give you the ability
to actually measure this.

732
00:35:15,300 --> 00:35:17,700
Now this has been sort of a journey.

733
00:35:17,700 --> 00:35:20,340
Starting at the beginning we
wanted to do a few things.

734
00:35:20,340 --> 00:35:22,500
We wanted to ensure that
our product engineers

735
00:35:22,500 --> 00:35:25,590
had the ability to very
quickly manually review

736
00:35:25,590 --> 00:35:28,101
the outputs across small data sets

737
00:35:28,101 --> 00:35:32,040
based off of their prompt changes
or their pipeline changes.

738
00:35:32,040 --> 00:35:34,200
We also wanted to introduce a variety

739
00:35:34,200 --> 00:35:37,080
of automated programmatic
metrics that allowed us

740
00:35:37,080 --> 00:35:39,600
to capture in an automated fashion

741
00:35:39,600 --> 00:35:42,360
these objective measurements
within the quality pillar.

742
00:35:42,360 --> 00:35:43,650
Things that allow us to ensure

743
00:35:43,650 --> 00:35:46,050
that we're not regressing on formatting

744
00:35:46,050 --> 00:35:48,960
or rendering capabilities that we consider

745
00:35:48,960 --> 00:35:50,703
at table stakes here at Slack.

746
00:35:51,600 --> 00:35:53,763
We increasing complexity in this journey,

747
00:35:54,630 --> 00:35:58,500
and this is kind of where
we are today here at Slack.

748
00:35:58,500 --> 00:36:01,680
We start to tackle some of
those more complex definitions

749
00:36:01,680 --> 00:36:02,610
of quality.

750
00:36:02,610 --> 00:36:04,800
We leverage LLM based quality metrics

751
00:36:04,800 --> 00:36:07,200
to be able to measure factual accuracy

752
00:36:07,200 --> 00:36:10,530
or to be able to measure answer relevancy.

753
00:36:10,530 --> 00:36:15,180
We leverage guardrails to
capture safety and harm issues.

754
00:36:15,180 --> 00:36:17,790
The overall goal here is that by combining

755
00:36:17,790 --> 00:36:21,900
these automated programmatic
metrics alongside LLM judges

756
00:36:21,900 --> 00:36:25,770
and guardrails, we can start
to evaluate the quality

757
00:36:25,770 --> 00:36:28,830
of our generative AI on
much larger data sets

758
00:36:28,830 --> 00:36:31,500
that are much more
representative of production.

759
00:36:31,500 --> 00:36:33,960
This gives us the ability
to run much larger

760
00:36:33,960 --> 00:36:36,183
scale experiments and tests.

761
00:36:37,260 --> 00:36:38,970
Now where do we go from here?

762
00:36:38,970 --> 00:36:42,270
Well, the goal is to
essentially start to develop

763
00:36:42,270 --> 00:36:45,900
kind of CICD for the generative
AI, giving us the ability

764
00:36:45,900 --> 00:36:49,650
to define verified outputs
where we can automate

765
00:36:49,650 --> 00:36:52,560
a series of tests so we
can capture regressions

766
00:36:52,560 --> 00:36:55,740
and lead to quality
improvements in a much quicker

767
00:36:55,740 --> 00:36:57,783
and more efficient way.

768
00:36:59,640 --> 00:37:02,460
Let me take a step back here
and talk a little bit about

769
00:37:02,460 --> 00:37:04,710
some of the things that I just mentioned.

770
00:37:04,710 --> 00:37:08,280
At Slack we run dozens of different task

771
00:37:08,280 --> 00:37:10,320
specific LLM judges.

772
00:37:10,320 --> 00:37:12,270
We work with each product team

773
00:37:12,270 --> 00:37:15,330
that develops product AI
features here at Slack

774
00:37:15,330 --> 00:37:18,540
to come up with a rubric on
what actually is the definition

775
00:37:18,540 --> 00:37:21,270
of quality for their particular feature.

776
00:37:21,270 --> 00:37:24,030
This changes for every single feature.

777
00:37:24,030 --> 00:37:26,940
So given the ability to our engineers

778
00:37:26,940 --> 00:37:29,820
to only define kind of a
rubric without actually

779
00:37:29,820 --> 00:37:32,880
having to write any code
to deploy these allows us

780
00:37:32,880 --> 00:37:35,370
to get these out very quickly.

781
00:37:35,370 --> 00:37:37,950
We also leverage Amazon
Bedrock guardrails,

782
00:37:37,950 --> 00:37:41,160
which gives us the ability
to measure toxicity,

783
00:37:41,160 --> 00:37:44,520
harm prompt injection
in a very, very easy way

784
00:37:44,520 --> 00:37:47,433
on both inputs and outputs of our LLMs.

785
00:37:49,920 --> 00:37:53,220
So we've kind of
highlighted two steps here.

786
00:37:53,220 --> 00:37:55,671
We first defined what we wanted to measure

787
00:37:55,671 --> 00:37:59,160
and then we defined how we measured it.

788
00:37:59,160 --> 00:38:01,230
But what really allows our engineers

789
00:38:01,230 --> 00:38:03,780
to be productive is kind
of developing the workflow

790
00:38:03,780 --> 00:38:06,570
that allows 'em to
utilize these capabilities

791
00:38:06,570 --> 00:38:10,140
in their day-to-day
development experience.

792
00:38:10,140 --> 00:38:13,620
We kind of, if you have a
machine learning background,

793
00:38:13,620 --> 00:38:16,110
this may seem somewhat familiar to you.

794
00:38:16,110 --> 00:38:19,170
We first do a series of
offline experimentation.

795
00:38:19,170 --> 00:38:21,660
We start on what we call
kind of golden sets.

796
00:38:21,660 --> 00:38:25,521
These are verified outputs
from our internal data

797
00:38:25,521 --> 00:38:28,530
that allow users to kind
of make prompt changes

798
00:38:28,530 --> 00:38:31,200
to their LLMs and perform manual review

799
00:38:31,200 --> 00:38:34,473
on just like small data
sets of like 10 to 20.

800
00:38:35,640 --> 00:38:37,650
If they feel confident with that,

801
00:38:37,650 --> 00:38:39,930
they can move to running their experiments

802
00:38:39,930 --> 00:38:41,850
on what we call validation sets.

803
00:38:41,850 --> 00:38:44,190
These are much larger in size

804
00:38:44,190 --> 00:38:45,810
and are much more representative

805
00:38:45,810 --> 00:38:47,970
of our production data.

806
00:38:47,970 --> 00:38:51,150
This way using the
combination of these automated

807
00:38:51,150 --> 00:38:54,342
programmatic metrics as well
as these quality metrics,

808
00:38:54,342 --> 00:38:57,840
they can look to capture
large scale regressions

809
00:38:57,840 --> 00:38:59,760
as well as ensure that
they're actually making

810
00:38:59,760 --> 00:39:02,730
the quality improvement
that they intended to.

811
00:39:02,730 --> 00:39:06,450
Now a key thing here is
that at each step we want

812
00:39:06,450 --> 00:39:10,020
to provide the fastest
feedback loop possible.

813
00:39:10,020 --> 00:39:12,090
This gives our engineers
the ability to fail

814
00:39:12,090 --> 00:39:14,850
as quickly as possible, go
back to the drawing board

815
00:39:14,850 --> 00:39:17,460
and ship features more quickly.

816
00:39:17,460 --> 00:39:21,510
Now, while our goal here
is to ultimately make this

817
00:39:21,510 --> 00:39:25,380
as automated as possible,
leveraging human in the loop

818
00:39:25,380 --> 00:39:27,270
is key to this.

819
00:39:27,270 --> 00:39:30,120
We aren't perfect in a
lot of our development

820
00:39:30,120 --> 00:39:31,380
of these evaluators.

821
00:39:31,380 --> 00:39:33,210
So giving our engineers the ability

822
00:39:33,210 --> 00:39:36,209
to actually see the
impact of their changes

823
00:39:36,209 --> 00:39:40,260
in the LLM response
very quickly across data

824
00:39:40,260 --> 00:39:42,993
will allow them to move
significantly faster.

825
00:39:44,190 --> 00:39:46,260
Now, after you've sort of validated

826
00:39:46,260 --> 00:39:48,270
a lot of this in your offline setting,

827
00:39:48,270 --> 00:39:50,580
we move into online evaluation.

828
00:39:50,580 --> 00:39:53,160
This is where we start to run AB tests

829
00:39:53,160 --> 00:39:55,950
and we integrate all of those evaluators

830
00:39:55,950 --> 00:39:58,530
that I've mentioned into
our AB tests as well

831
00:39:58,530 --> 00:40:01,200
so that you can actually
measure both across

832
00:40:01,200 --> 00:40:05,280
these quality metrics as
well as user feedback metrics

833
00:40:05,280 --> 00:40:07,710
before you actually make the decision

834
00:40:07,710 --> 00:40:09,840
to roll out to production.

835
00:40:09,840 --> 00:40:14,310
Now, the key thing here
is that you actually have

836
00:40:14,310 --> 00:40:17,640
the confidence that you are
making the quality change.

837
00:40:17,640 --> 00:40:19,800
You are not regressing in certain areas

838
00:40:19,800 --> 00:40:23,493
before too many of your
users actually see it.

839
00:40:25,140 --> 00:40:27,390
Stepping back, I'm gonna
talk about a few things

840
00:40:27,390 --> 00:40:28,770
that I just mentioned as part of that,

841
00:40:28,770 --> 00:40:30,870
we have three different types of data sets

842
00:40:30,870 --> 00:40:33,750
that we typically operate
within golden sets,

843
00:40:33,750 --> 00:40:35,850
which have been sort of manually vetted.

844
00:40:35,850 --> 00:40:38,790
These are very small 10 to 50 samples

845
00:40:38,790 --> 00:40:40,470
and give our engineers the ability

846
00:40:40,470 --> 00:40:43,110
to manually review the outputs,

847
00:40:43,110 --> 00:40:45,630
validation sets ranging typically

848
00:40:45,630 --> 00:40:48,180
between a hundred and a thousand samples.

849
00:40:48,180 --> 00:40:50,310
This is where those automated quality

850
00:40:50,310 --> 00:40:52,481
and programmatic metrics
really start to shine,

851
00:40:52,481 --> 00:40:57,481
as you can imagine, reviewing
say a thousand samples

852
00:40:57,750 --> 00:41:00,390
as kind of impossible for a human.

853
00:41:00,390 --> 00:41:04,260
Then the AB testing where we
typically run these experiments

854
00:41:04,260 --> 00:41:08,460
on somewhere between one
to 25% of Slack AI queries

855
00:41:08,460 --> 00:41:10,233
for that particular feature.

856
00:41:11,700 --> 00:41:15,780
So that's all nice, but is it worth it?

857
00:41:15,780 --> 00:41:18,510
I've just kind of picked
three examples here

858
00:41:18,510 --> 00:41:20,970
that kind of showcase some
of the different areas

859
00:41:20,970 --> 00:41:23,910
that we look to tackle here at Slack.

860
00:41:23,910 --> 00:41:27,270
On the prompt engineering
side, we recently changed

861
00:41:27,270 --> 00:41:31,684
how we serialize the content that we send

862
00:41:31,684 --> 00:41:34,826
to the LLM resulting in
a 5 and 6% improvement

863
00:41:34,826 --> 00:41:39,630
in both factual and user
attribution accuracy respectively.

864
00:41:39,630 --> 00:41:42,810
We oftentimes run model
upgrades when a new version

865
00:41:42,810 --> 00:41:45,347
of a model or a new LLM comes out.

866
00:41:45,347 --> 00:41:48,681
A recent upgrade resulted in 11% increase

867
00:41:48,681 --> 00:41:53,681
in user satisfaction as
well as a 3 to 5% increase

868
00:41:54,030 --> 00:41:55,980
in key quality metrics.

869
00:41:55,980 --> 00:41:59,610
Now we run that flow in a
much more automated fashion

870
00:41:59,610 --> 00:42:02,430
for every single LLM upgrade that we do

871
00:42:02,430 --> 00:42:05,670
because we've actually seen a
new version lead to regression

872
00:42:05,670 --> 00:42:08,040
and decided to not roll it out.

873
00:42:08,040 --> 00:42:13,040
Outside of quality, cost
management is a key area for us.

874
00:42:13,080 --> 00:42:15,180
Sometimes you want to
just maintain quality

875
00:42:15,180 --> 00:42:18,300
while reducing costs or improving latency.

876
00:42:18,300 --> 00:42:23,300
A recent change allowed us
to move a similar quality LLM

877
00:42:24,120 --> 00:42:28,263
or change resulting in
a 60% cost reduction.

878
00:42:31,920 --> 00:42:34,230
So we've talked about-

879
00:42:34,230 --> 00:42:36,300
- [Audience Member] Can
you talk more about that?

880
00:42:36,300 --> 00:42:37,950
- I will, yeah, yeah, yeah, yeah.

881
00:42:40,170 --> 00:42:43,950
We've talked about two different things,

882
00:42:43,950 --> 00:42:47,700
independently, our how we
built the infrastructure layer,

883
00:42:47,700 --> 00:42:50,670
which gave us the ability
to utilize our LMS

884
00:42:50,670 --> 00:42:54,930
much more efficiently,
choose between different LLMs

885
00:42:54,930 --> 00:42:58,170
and give us the ability
to switch seamlessly.

886
00:42:58,170 --> 00:43:00,540
We then talked about how we developed kind

887
00:43:00,540 --> 00:43:04,440
of this internal experimentation
framework that allows us

888
00:43:04,440 --> 00:43:07,290
to actually measure the
quality of our AI outputs

889
00:43:07,290 --> 00:43:09,680
and have confidence that the changes

890
00:43:09,680 --> 00:43:12,690
we want to make are the
changes we are making.

891
00:43:12,690 --> 00:43:16,050
In this section, I'm gonna
go through how we utilize

892
00:43:16,050 --> 00:43:20,640
both of those to more seamlessly
integrate generative AI

893
00:43:20,640 --> 00:43:22,680
across the application in a way

894
00:43:22,680 --> 00:43:25,500
that is cost efficient while
maintaining the quality

895
00:43:25,500 --> 00:43:27,453
and scale that our users expect.

896
00:43:28,860 --> 00:43:32,016
To do that I'm gonna walk
through a couple a use case,

897
00:43:32,016 --> 00:43:35,010
but before that I just wanna
highlight there's a a spectrum

898
00:43:35,010 --> 00:43:37,560
of generative AI complexity.

899
00:43:37,560 --> 00:43:39,780
On the low complexity
side, you have things

900
00:43:39,780 --> 00:43:41,220
that maybe in the past were done

901
00:43:41,220 --> 00:43:44,520
by traditional machine learning
models, classification,

902
00:43:44,520 --> 00:43:48,810
converting unstructured to
structured data, parsing data.

903
00:43:48,810 --> 00:43:50,310
You jump to the medium complexity

904
00:43:50,310 --> 00:43:54,210
where it's summarization,
very basic image generation,

905
00:43:54,210 --> 00:43:56,557
content editing capabilities.

906
00:43:56,557 --> 00:43:59,010
Then on the high complexity side,

907
00:43:59,010 --> 00:44:01,830
a little bit more of what the
media likes to talk about.

908
00:44:01,830 --> 00:44:05,327
Agenetic workflows, video
generation, tool use,

909
00:44:05,327 --> 00:44:07,320
these bigger things.

910
00:44:07,320 --> 00:44:10,830
Now here at Slack we have a large number

911
00:44:10,830 --> 00:44:13,380
of generative AI applications

912
00:44:13,380 --> 00:44:17,130
and they all span this spectrum
ranging from low complexity

913
00:44:17,130 --> 00:44:19,350
all the way to high complexity.

914
00:44:19,350 --> 00:44:23,730
You don't want to use your
state-of-the-art frontier LLM

915
00:44:23,730 --> 00:44:27,090
for every single use case,
but it's a tough question

916
00:44:27,090 --> 00:44:30,240
to answer like what is the exact right LLM

917
00:44:30,240 --> 00:44:33,480
to be able to utilize
that will meet your goals

918
00:44:33,480 --> 00:44:36,753
without sacrificing quality to your users.

919
00:44:38,100 --> 00:44:40,680
So we're gonna step
through a specific use case

920
00:44:40,680 --> 00:44:44,880
tackling this low complexity
area that we refer to

921
00:44:44,880 --> 00:44:47,973
as search query
understanding here at Slack.

922
00:44:49,260 --> 00:44:51,780
But first I'm gonna
revisit the scale slide

923
00:44:51,780 --> 00:44:54,780
that Shaurya has shown
highlighting the search area.

924
00:44:54,780 --> 00:44:57,870
Now we run a lot of
searches here at Slack.

925
00:44:57,870 --> 00:45:01,560
While the bulk of those may
just be finding a person,

926
00:45:01,560 --> 00:45:04,320
finding a channel, a fraction of those

927
00:45:04,320 --> 00:45:07,170
are highly complex searches
where you want to ask questions

928
00:45:07,170 --> 00:45:10,890
to your data or run searches
across a large number

929
00:45:10,890 --> 00:45:12,450
of messages.

930
00:45:12,450 --> 00:45:16,800
So even a fraction of this
number can result in high cost

931
00:45:16,800 --> 00:45:18,633
if you are not running efficiently.

932
00:45:19,981 --> 00:45:23,250
So what is search query understanding?

933
00:45:23,250 --> 00:45:26,940
So let's say a user comes into Slack

934
00:45:26,940 --> 00:45:30,217
and adds the following search term.

935
00:45:30,217 --> 00:45:33,780
"Can you find me the FY25 sales deck

936
00:45:33,780 --> 00:45:36,360
that John Doe sent me recently?"

937
00:45:36,360 --> 00:45:38,190
Now we may not want to send that

938
00:45:38,190 --> 00:45:41,760
to our search cluster directly,
it might not be helpful,

939
00:45:41,760 --> 00:45:44,250
but there's a lot of
information hidden in there

940
00:45:44,250 --> 00:45:47,010
that allows us to more target our searches

941
00:45:47,010 --> 00:45:49,440
and improve the chances
that we're actually finding

942
00:45:49,440 --> 00:45:53,550
the files or the messages that
you are actually looking for.

943
00:45:53,550 --> 00:45:56,880
So we run it through kind of a
large language model pipeline

944
00:45:56,880 --> 00:46:01,530
that allows us to generate
JSON, that includes filters

945
00:46:01,530 --> 00:46:04,860
and other information that
you might be relevant to you.

946
00:46:04,860 --> 00:46:05,960
Let's break this down.

947
00:46:07,140 --> 00:46:09,840
We can see that the actual
query you might wanna search

948
00:46:09,840 --> 00:46:10,920
isn't that full thing.

949
00:46:10,920 --> 00:46:15,090
Maybe it's just FY25
sales PowerPoint deck.

950
00:46:15,090 --> 00:46:17,520
Now this is just one query, but oftentimes

951
00:46:17,520 --> 00:46:19,470
you run multiple queries in parallel.

952
00:46:19,470 --> 00:46:23,010
So there might be multiple
versions of this search term.

953
00:46:23,010 --> 00:46:24,750
You use the term recently.

954
00:46:24,750 --> 00:46:27,750
So this gives us the indication
that we might be able

955
00:46:27,750 --> 00:46:30,180
to condense this to a specific time range

956
00:46:30,180 --> 00:46:32,850
in the past three months
to increase the chances

957
00:46:32,850 --> 00:46:34,955
of finding what we're looking for.

958
00:46:34,955 --> 00:46:37,500
You said that you're
looking for a sales deck,

959
00:46:37,500 --> 00:46:40,350
now that likely includes a type of a file

960
00:46:40,350 --> 00:46:42,030
called a presentation.

961
00:46:42,030 --> 00:46:43,890
So can we limit to only threads

962
00:46:43,890 --> 00:46:47,910
or conversations that
include presentations?

963
00:46:47,910 --> 00:46:50,700
You also mentioned that it was
John Doe that sent it to you,

964
00:46:50,700 --> 00:46:52,350
so let's only look at conversations

965
00:46:52,350 --> 00:46:54,423
or threads that included him.

966
00:46:56,220 --> 00:46:58,650
So what does this actually look
like in our search pipeline

967
00:46:58,650 --> 00:47:02,610
user sends of search, we
have machine learning models

968
00:47:02,610 --> 00:47:05,700
that allow us to quickly
and very quickly determine

969
00:47:05,700 --> 00:47:08,021
is this an informational search

970
00:47:08,021 --> 00:47:09,450
or is this a navigational search?

971
00:47:09,450 --> 00:47:10,950
If it's a navigational search,

972
00:47:10,950 --> 00:47:12,750
you go through a very specific search

973
00:47:12,750 --> 00:47:15,240
and machine learning ranking pipeline.

974
00:47:15,240 --> 00:47:17,550
If it's an informational search,

975
00:47:17,550 --> 00:47:20,490
we then go through our
query understanding pipeline

976
00:47:20,490 --> 00:47:23,670
to try to better target your actual search

977
00:47:23,670 --> 00:47:27,660
before then going to the
rest of our search retrieval

978
00:47:27,660 --> 00:47:31,383
machine learning and then
eventually LLM response.

979
00:47:33,390 --> 00:47:35,100
So where did we start here?

980
00:47:35,100 --> 00:47:38,970
So we had a problem our
existing LLM that we were using

981
00:47:38,970 --> 00:47:42,426
for this case, it worked,
it met our quality goals,

982
00:47:42,426 --> 00:47:45,360
but it exceeded our search latency budget

983
00:47:45,360 --> 00:47:48,030
and it was extremely
high cost given the scale

984
00:47:48,030 --> 00:47:50,400
that we needed to operate at.

985
00:47:50,400 --> 00:47:53,280
So what we wanted to do in
this particular use case

986
00:47:53,280 --> 00:47:56,910
is we wanted to switch or
leverage prompt engineering

987
00:47:56,910 --> 00:48:00,630
that allowed us to maintain
quality while simultaneously

988
00:48:00,630 --> 00:48:04,020
reducing latency as well as our costs.

989
00:48:04,020 --> 00:48:06,390
Now the benefit is that we had both

990
00:48:06,390 --> 00:48:08,100
of these infrastructure layer changes

991
00:48:08,100 --> 00:48:10,560
as well as this experimentation framework.

992
00:48:10,560 --> 00:48:13,110
So we wanted to see how we
could utilize both of these

993
00:48:13,110 --> 00:48:15,960
to ensure that we could
select the right LLM

994
00:48:15,960 --> 00:48:19,710
for this particular problem,
while simultaneously,

995
00:48:19,710 --> 00:48:21,790
having the confidence that we are meeting

996
00:48:23,467 --> 00:48:24,353
all three of those goals.

997
00:48:25,656 --> 00:48:27,210
So I'm gonna walk you through
how this actually looks

998
00:48:27,210 --> 00:48:30,780
in our internal experimentation framework.

999
00:48:30,780 --> 00:48:34,050
This is just a small summary
of what it might look like

1000
00:48:34,050 --> 00:48:37,230
when we run an offline experiment.

1001
00:48:37,230 --> 00:48:41,070
We compare the LLM in this case Nova-lite

1002
00:48:41,070 --> 00:48:44,100
to the original LLM that we were using

1003
00:48:44,100 --> 00:48:47,490
to determine if this is actually
leading to the improvements

1004
00:48:47,490 --> 00:48:49,260
that we want it to be.

1005
00:48:49,260 --> 00:48:51,420
Most specifically, we
were looking at latency

1006
00:48:51,420 --> 00:48:53,790
as well as key quality metrics.

1007
00:48:53,790 --> 00:48:55,800
This is a rolled up summary.

1008
00:48:55,800 --> 00:48:58,440
We actually have dozens
of different evaluators

1009
00:48:58,440 --> 00:49:01,020
that then aggregate up
into these summaries.

1010
00:49:01,020 --> 00:49:03,720
Now this is run on just a
very, very small sample set,

1011
00:49:03,720 --> 00:49:07,260
but as you can see, we saw
significant reductions in latency

1012
00:49:07,260 --> 00:49:10,860
as well as improvements in
quality on this small sample.

1013
00:49:10,860 --> 00:49:13,102
Now with these kind of like offline tests,

1014
00:49:13,102 --> 00:49:16,620
we felt confident that
we were actually making

1015
00:49:16,620 --> 00:49:18,720
the improvements that we wanted to.

1016
00:49:18,720 --> 00:49:20,850
So we moved to an online evaluation.

1017
00:49:20,850 --> 00:49:23,010
We started to run an AB test,

1018
00:49:23,010 --> 00:49:26,310
and this is just a couple
of photos from that

1019
00:49:26,310 --> 00:49:29,400
tackling the key metrics
that we were concerned about.

1020
00:49:29,400 --> 00:49:32,490
Specifically we wanted to look at latency

1021
00:49:32,490 --> 00:49:35,520
as well as quality metrics in terms of our

1022
00:49:35,520 --> 00:49:39,600
automated quality metrics as
well as user satisfaction.

1023
00:49:39,600 --> 00:49:43,590
What we saw is that we
significantly improved latency

1024
00:49:43,590 --> 00:49:46,620
and we had no significant
change across both

1025
00:49:46,620 --> 00:49:51,573
user satisfaction as well as
our automated quality metrics.

1026
00:49:52,961 --> 00:49:55,470
So what did this mean?

1027
00:49:55,470 --> 00:49:58,230
Well, for query understanding,
we ultimately switched

1028
00:49:58,230 --> 00:50:01,920
to Nova-lite and the result
was that we were able to do so

1029
00:50:01,920 --> 00:50:04,500
after a series of prompt changes with

1030
00:50:04,500 --> 00:50:08,610
no user visible regression in quality,

1031
00:50:08,610 --> 00:50:13,610
while simultaneously
reducing P50 latency by 46%

1032
00:50:14,407 --> 00:50:17,957
and reducing our cost to serve

1033
00:50:17,957 --> 00:50:20,633
for this particular feature by 70%.

1034
00:50:24,000 --> 00:50:25,920
So what have we unlocked here?

1035
00:50:25,920 --> 00:50:28,830
We had these two independent sections,

1036
00:50:28,830 --> 00:50:31,950
these infrastructure improvements,
which gave us the ability

1037
00:50:31,950 --> 00:50:35,430
to select the right LLM for the feature,

1038
00:50:35,430 --> 00:50:38,770
to be able to run our
infrastructure at a much more

1039
00:50:39,917 --> 00:50:40,883
efficient utilization.

1040
00:50:41,822 --> 00:50:43,560
We also had our quality
and evaluation system,

1041
00:50:43,560 --> 00:50:45,630
which gave us the ability to perform

1042
00:50:45,630 --> 00:50:48,300
more objective benchmarking and evaluation

1043
00:50:48,300 --> 00:50:51,990
of the quality of our
generative AI outputs.

1044
00:50:51,990 --> 00:50:54,270
And so with this, we are
able to select the right LLM

1045
00:50:54,270 --> 00:50:57,060
for the job, maximize the
time we actually spend

1046
00:50:57,060 --> 00:51:00,060
prompt engineering while
minimizing engineering time

1047
00:51:00,060 --> 00:51:04,530
that goes into this,
run our infrastructure

1048
00:51:04,530 --> 00:51:07,320
much more efficiently and
actually have the confidence

1049
00:51:07,320 --> 00:51:09,030
that when we communicate
to the improvements

1050
00:51:09,030 --> 00:51:12,754
that we are making to
other teams, to leadership,

1051
00:51:12,754 --> 00:51:15,603
that those improvements are actual.

1052
00:51:17,490 --> 00:51:20,527
And so we do this across dozens and dozens

1053
00:51:20,527 --> 00:51:23,010
of different features.

1054
00:51:23,010 --> 00:51:25,860
What has been the result
of all of these changes?

1055
00:51:25,860 --> 00:51:29,220
In the past year, we've
seen a 90% reduction

1056
00:51:29,220 --> 00:51:32,820
in the cost to serve Slack
AI by monthly active user,

1057
00:51:32,820 --> 00:51:35,910
simultaneously, we've increased
the scale that we operate at

1058
00:51:35,910 --> 00:51:38,070
by nearly 5X.

1059
00:51:38,070 --> 00:51:41,790
We did this all while
increasing user satisfaction

1060
00:51:41,790 --> 00:51:45,540
and user feedback by
ranging from 15 to 30%

1061
00:51:45,540 --> 00:51:49,113
across Slack AI's marquee features.

1062
00:51:51,180 --> 00:51:52,013
Great.

1063
00:51:52,013 --> 00:51:56,280
Thank you all for taking
the time to watch us today.

1064
00:51:56,280 --> 00:51:59,013
Happy to answer any questions.

