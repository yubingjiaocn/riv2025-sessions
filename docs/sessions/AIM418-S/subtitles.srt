1
00:00:02,700 --> 00:00:04,050
- Good afternoon, everyone.

2
00:00:04,950 --> 00:00:08,260
I thank you for coming at
this late afternoon meeting

3
00:00:09,120 --> 00:00:12,810
that today I'm going to talk
about discussion control plane.

4
00:00:12,810 --> 00:00:15,330
I'm going to talk about how
the operations will change

5
00:00:15,330 --> 00:00:16,920
in the next two years.

6
00:00:16,920 --> 00:00:20,220
Like whatever you are
doing for running software,

7
00:00:20,220 --> 00:00:23,820
you will dealing it differently
like either yourself

8
00:00:23,820 --> 00:00:25,173
or by using a product.

9
00:00:26,250 --> 00:00:28,680
I'd just like to talk about this quote

10
00:00:28,680 --> 00:00:31,590
from Dario Amodei, CEO of Anthropic.

11
00:00:31,590 --> 00:00:36,517
But in March 2025, like
eight months ago, he'd said,

12
00:00:36,517 --> 00:00:40,620
"AI will be writing 90% of
the code software developers

13
00:00:40,620 --> 00:00:41,790
were in charge of."

14
00:00:41,790 --> 00:00:44,100
So like, I don't know if you
agree with this sentiment.

15
00:00:44,100 --> 00:00:45,960
I don't think that we are there now,

16
00:00:45,960 --> 00:00:47,430
but I think we are coming to them.

17
00:00:47,430 --> 00:00:51,720
If not 90%, it'll be
like around 50% today.

18
00:00:51,720 --> 00:00:55,020
And like considering that
where we were two years ago,

19
00:00:55,020 --> 00:00:56,520
I think this is quite a leap.

20
00:00:56,520 --> 00:00:59,520
And like even if we
are not at the 90% now,

21
00:00:59,520 --> 00:01:02,640
I think we will be in a
very, very near feature.

22
00:01:02,640 --> 00:01:05,010
So that actually sounds super exciting

23
00:01:05,010 --> 00:01:07,800
so that like now we are not, you know,

24
00:01:07,800 --> 00:01:10,380
doing the daunting task of writing code,

25
00:01:10,380 --> 00:01:13,680
but this means that you'll be responsible

26
00:01:13,680 --> 00:01:16,290
of the alerts that you got at 2:00 AM

27
00:01:16,290 --> 00:01:19,260
about the quote that for the 90% of it,

28
00:01:19,260 --> 00:01:20,700
you have no context,

29
00:01:20,700 --> 00:01:23,130
even like maybe less
context on the issues.

30
00:01:23,130 --> 00:01:25,680
And you also will have no
context about the issues

31
00:01:25,680 --> 00:01:29,130
that like the blast radius of this issue

32
00:01:29,130 --> 00:01:30,090
on the other systems

33
00:01:30,090 --> 00:01:31,980
that are probably written by AI again.

34
00:01:31,980 --> 00:01:36,030
So like this area requires something

35
00:01:36,030 --> 00:01:39,780
that is like a new operational
patterning for SREs

36
00:01:39,780 --> 00:01:41,970
and also for DevOps engines

37
00:01:41,970 --> 00:01:46,860
and whoever is responsible
for waking up at 2 AM.

38
00:01:46,860 --> 00:01:48,780
So even today, like, you know,

39
00:01:48,780 --> 00:01:52,950
without even any AI getting into space,

40
00:01:52,950 --> 00:01:55,970
the modern ops is already very siloed.

41
00:01:55,970 --> 00:01:58,140
So like people are
responsible of different parts

42
00:01:58,140 --> 00:02:00,510
of the systems and they
don't care about the rest.

43
00:02:00,510 --> 00:02:03,930
They're always reactively
coming up with action steps

44
00:02:03,930 --> 00:02:07,680
and even if the wrong books
are coming like piling up,

45
00:02:07,680 --> 00:02:10,620
it's about the lessons
learned from the other stuff.

46
00:02:10,620 --> 00:02:14,040
So it's like the people
are just like looking

47
00:02:14,040 --> 00:02:16,620
at the dashboard, trying to query data,

48
00:02:16,620 --> 00:02:19,380
just acknowledging the errors.

49
00:02:19,380 --> 00:02:22,110
And then this is all with
the old school tools.

50
00:02:22,110 --> 00:02:23,100
Here I'm saying

51
00:02:23,100 --> 00:02:26,370
that like you cannot
resolve tomorrow's problems

52
00:02:26,370 --> 00:02:27,663
with yesterday's tools.

53
00:02:29,040 --> 00:02:32,220
That's why you'll be
introducing today something

54
00:02:32,220 --> 00:02:34,500
we are calling as discussion control plane

55
00:02:34,500 --> 00:02:37,800
where your agentic SRE
teammates or any other teammates

56
00:02:37,800 --> 00:02:40,920
will be talking with your systems.

57
00:02:40,920 --> 00:02:42,450
Before jumping it in more,

58
00:02:42,450 --> 00:02:45,060
let me just introduce myself a little bit.

59
00:02:45,060 --> 00:02:47,100
So this is my like

60
00:02:47,100 --> 00:02:51,180
and almost a decade, I'm
working on observability

61
00:02:51,180 --> 00:02:53,656
and first, I started my
journey with observability,

62
00:02:53,656 --> 00:02:55,050
with serverless observability,

63
00:02:55,050 --> 00:02:58,293
and that's how I got become a
Serverless Hero back in 2020.

64
00:02:59,130 --> 00:03:01,230
And I'm actually an ex-developer

65
00:03:01,230 --> 00:03:03,450
who become a product manager accidentally.

66
00:03:03,450 --> 00:03:07,530
Now I'm becoming an developer
again with the powers of AI.

67
00:03:07,530 --> 00:03:10,110
And I'm at my seventh
AWS re:Invent this year.

68
00:03:10,110 --> 00:03:13,560
And I'm really, really
feeling the energy here

69
00:03:13,560 --> 00:03:18,090
about like how AI changes
everything about like every role

70
00:03:18,090 --> 00:03:22,110
and how it'll be changing
in the next year.

71
00:03:22,110 --> 00:03:24,270
So I'm already full of questions

72
00:03:24,270 --> 00:03:26,373
about like how it will be for next year.

73
00:03:27,480 --> 00:03:30,090
So agenda for the rest of today

74
00:03:30,090 --> 00:03:32,970
is that we will talk about
like what I mean by teammates.

75
00:03:32,970 --> 00:03:35,220
So we will talk about what is assistants

76
00:03:35,220 --> 00:03:36,660
and then what is teammates,

77
00:03:36,660 --> 00:03:39,150
and then we will then
talk about the challenges

78
00:03:39,150 --> 00:03:42,090
of running LLMs on streaming data.

79
00:03:42,090 --> 00:03:44,970
And also, we will link it

80
00:03:44,970 --> 00:03:47,790
with the guardrails on
data on telemetry pipelines

81
00:03:47,790 --> 00:03:51,750
because you just cannot send
everything to an LLM, right?

82
00:03:51,750 --> 00:03:53,940
And then we will talk about cultural shift

83
00:03:53,940 --> 00:03:55,650
while after AI teammates.

84
00:03:55,650 --> 00:03:57,120
Like, okay, AI teammate's here.

85
00:03:57,120 --> 00:04:01,050
So what I'm going to do
is like a normal teammate.

86
00:04:01,050 --> 00:04:06,050
So let me talk about assistants
versus teammates first.

87
00:04:06,510 --> 00:04:09,810
So AI assistants are
just like the, you know,

88
00:04:09,810 --> 00:04:11,460
you see them everywhere in a website.

89
00:04:11,460 --> 00:04:14,730
Like, so in every website
there is now an AI teammate

90
00:04:14,730 --> 00:04:17,490
that's great at fetching
relevant information.

91
00:04:17,490 --> 00:04:20,010
For SRE use cases,

92
00:04:20,010 --> 00:04:23,490
it's always like, okay,
I'm asking a question,

93
00:04:23,490 --> 00:04:24,750
I'm curious about something.

94
00:04:24,750 --> 00:04:29,130
Okay, it's helping me to
query my ask in PromptQL

95
00:04:29,130 --> 00:04:31,320
or in whatever query
language that you're using.

96
00:04:31,320 --> 00:04:34,050
It's good at like retrieving
data from a dashboard,

97
00:04:34,050 --> 00:04:36,120
but it's not like, you know,

98
00:04:36,120 --> 00:04:38,550
you need to be following
up with the discussion.

99
00:04:38,550 --> 00:04:41,730
You need to be telling that,
"Okay, I got my answer,

100
00:04:41,730 --> 00:04:43,830
but is this actually helping me?"

101
00:04:43,830 --> 00:04:47,100
So they're always like by
design passive helpers.

102
00:04:47,100 --> 00:04:50,790
So they're not just there to
actually to resolve the issue

103
00:04:50,790 --> 00:04:52,620
for you, but they are just like there

104
00:04:52,620 --> 00:04:55,560
to answer your question so
that you can resolve the issue.

105
00:04:55,560 --> 00:04:57,330
But, you know, this is good.

106
00:04:57,330 --> 00:04:59,430
Like, this is like what
we have been using AI for

107
00:04:59,430 --> 00:05:02,250
for quite a lot time of time, right?

108
00:05:02,250 --> 00:05:04,680
But like this is kind of passive.

109
00:05:04,680 --> 00:05:07,140
So the actual difference
between a teammate

110
00:05:07,140 --> 00:05:11,250
and assistant is rooted from
an action versus information

111
00:05:11,250 --> 00:05:13,830
with AI teammate versus an assistant.

112
00:05:13,830 --> 00:05:15,240
As I said, like the AI assistants

113
00:05:15,240 --> 00:05:17,130
are like response to prompts

114
00:05:17,130 --> 00:05:19,860
and just like, you know, gives an answer

115
00:05:19,860 --> 00:05:21,030
and wait for the next question

116
00:05:21,030 --> 00:05:24,150
while AI teammates can
proactively initiate the actions

117
00:05:24,150 --> 00:05:27,570
and start, how can I say?

118
00:05:27,570 --> 00:05:30,330
Doing the research about the issue

119
00:05:30,330 --> 00:05:32,730
while you didn't even ask about it.

120
00:05:32,730 --> 00:05:35,370
And for AI, the scope

121
00:05:35,370 --> 00:05:37,500
and the memory is all
about like the questions

122
00:05:37,500 --> 00:05:38,333
that you're asking.

123
00:05:38,333 --> 00:05:39,780
It only remembers the previous questions

124
00:05:39,780 --> 00:05:41,400
and the previous discussions.

125
00:05:41,400 --> 00:05:44,130
And the AI team makes
is like the multi-turn

126
00:05:44,130 --> 00:05:46,110
and they're like the,
"Okay, I did something

127
00:05:46,110 --> 00:05:47,370
but it didn't have the solution

128
00:05:47,370 --> 00:05:49,050
so I need to do something else."

129
00:05:49,050 --> 00:05:52,020
And from that perspective,
like we can see AI assistants

130
00:05:52,020 --> 00:05:54,510
as helpers while you can look

131
00:05:54,510 --> 00:05:56,160
at the AI teammates, your collaborators,

132
00:05:56,160 --> 00:05:59,250
like almost like your junior
engineer that gets better

133
00:05:59,250 --> 00:06:01,860
and better with everything
that they are learning.

134
00:06:01,860 --> 00:06:06,330
Again, the context of it is
like the AI teammates can know

135
00:06:06,330 --> 00:06:08,520
like something other than the prompts

136
00:06:08,520 --> 00:06:09,810
that you are giving to it.

137
00:06:09,810 --> 00:06:13,590
You can learn about like how
is your service map looking at,

138
00:06:13,590 --> 00:06:16,530
how is your dependency map is looking at?

139
00:06:16,530 --> 00:06:17,760
And like you can understand

140
00:06:17,760 --> 00:06:22,590
that like the AI team must
know your systems more than you

141
00:06:22,590 --> 00:06:25,090
and then this's how actually
you can trust on them

142
00:06:25,999 --> 00:06:29,460
and they can act anonymously
between the boundaries

143
00:06:29,460 --> 00:06:30,630
that you put them for it.

144
00:06:30,630 --> 00:06:32,520
So you can say that, "Hey,
I want this AI teammate

145
00:06:32,520 --> 00:06:34,920
to do this stuff, but not that much."

146
00:06:34,920 --> 00:06:36,750
Maybe you can just admit some actions

147
00:06:36,750 --> 00:06:37,583
and you can say that,

148
00:06:37,583 --> 00:06:41,100
"Hey, I want certainly
an approval permission

149
00:06:41,100 --> 00:06:42,543
for these type of things.

150
00:06:43,470 --> 00:06:47,010
So that's like, let's talk
about a real-life scenario

151
00:06:47,010 --> 00:06:49,860
that we have been having a Edge Delta

152
00:06:49,860 --> 00:06:51,510
that like we are
anonymizing the names here.

153
00:06:51,510 --> 00:06:54,450
But, you know, this is
like an a way to start,

154
00:06:54,450 --> 00:06:57,810
one of the ways to start a
discussion with AI teammates.

155
00:06:57,810 --> 00:07:00,810
Like as a human, you can just
go and ask a question to it,

156
00:07:00,810 --> 00:07:01,983
but there might be other ways,

157
00:07:01,983 --> 00:07:04,740
like a monitor can trigger
a discussion before

158
00:07:04,740 --> 00:07:06,150
or like an external event

159
00:07:06,150 --> 00:07:09,180
or a periodic task can
trigger a discussion.

160
00:07:09,180 --> 00:07:11,460
This time, this is like
a human asking a question

161
00:07:11,460 --> 00:07:15,300
about, "Okay, I'm seeing something wrong.

162
00:07:15,300 --> 00:07:17,400
I'm not sure if it's a incident."

163
00:07:17,400 --> 00:07:20,910
I'm just like, I just want
to check something happening.

164
00:07:20,910 --> 00:07:22,230
So like this is normally

165
00:07:22,230 --> 00:07:24,090
how you would behave on Slack, right?

166
00:07:24,090 --> 00:07:26,760
So you will say that, "Okay,
yeah, something seems off,

167
00:07:26,760 --> 00:07:28,770
like can we just check?"

168
00:07:28,770 --> 00:07:31,050
And then it starts here.

169
00:07:31,050 --> 00:07:34,530
So there is an orchestrator
for this all AI teammate

170
00:07:34,530 --> 00:07:37,590
that knows all the AI
teammates in your system.

171
00:07:37,590 --> 00:07:40,140
So let's say you have like 10 AI teammate

172
00:07:40,140 --> 00:07:43,170
and then one is responsible
for SRE, one is responsible

173
00:07:43,170 --> 00:07:46,050
for writing code, one is
responsible for design, let's say.

174
00:07:46,050 --> 00:07:49,080
And like orchestrators, in our case,

175
00:07:49,080 --> 00:07:50,910
it's OnCall AI at Edge Delta,

176
00:07:50,910 --> 00:07:53,490
like looks at all these teammates,
looks at all their tools,

177
00:07:53,490 --> 00:07:54,780
looks at all their permissions

178
00:07:54,780 --> 00:07:56,880
and looks at it all their memory

179
00:07:56,880 --> 00:07:58,260
and picks a teammate

180
00:07:58,260 --> 00:08:00,960
that is the most suited for this question.

181
00:08:00,960 --> 00:08:02,970
In this case, it is like the SRE

182
00:08:02,970 --> 00:08:04,567
and it's giving it a prompt about like,

183
00:08:04,567 --> 00:08:06,540
"Hey, I want you to run this analysis

184
00:08:06,540 --> 00:08:08,160
to answer this question."

185
00:08:08,160 --> 00:08:09,960
And it comes with these questions,

186
00:08:09,960 --> 00:08:13,440
but I like to take your
attention to this approved thing

187
00:08:13,440 --> 00:08:16,050
because like it is asking for
a permission before moving on.

188
00:08:16,050 --> 00:08:20,160
This is like how you actually
can configure a system

189
00:08:20,160 --> 00:08:24,030
to act in the boundaries that you set.

190
00:08:24,030 --> 00:08:27,720
It's asking humans to
get an approval for this,

191
00:08:27,720 --> 00:08:29,490
and then it starts its analysis

192
00:08:29,490 --> 00:08:32,400
and it pulls the first
chart it can retrieve

193
00:08:32,400 --> 00:08:34,290
right into the thread.

194
00:08:34,290 --> 00:08:36,570
Then it goes back to the orchestrator

195
00:08:36,570 --> 00:08:39,450
and orchestrator says,
"Yeah, I did some analysis.

196
00:08:39,450 --> 00:08:41,760
Maybe like there can be some other stuff,

197
00:08:41,760 --> 00:08:43,920
but like, do you want me to go for it?"

198
00:08:43,920 --> 00:08:45,963
And then human user says, "Oh,

199
00:08:47,078 --> 00:08:49,560
I have some idea what
can be happening now.

200
00:08:49,560 --> 00:08:51,547
So I'd like to ask more questions about it

201
00:08:51,547 --> 00:08:54,690
and like, can I also like
look at the log patterns?"

202
00:08:54,690 --> 00:08:59,690
And again, the orchestrator
pulls up the other teammates

203
00:08:59,910 --> 00:09:03,930
just to look at this question.

204
00:09:03,930 --> 00:09:07,500
And then the other teammate
is asking for another approval

205
00:09:07,500 --> 00:09:09,210
and then pulls this data.

206
00:09:09,210 --> 00:09:12,810
But like I really want to
take your attention to here

207
00:09:12,810 --> 00:09:16,200
that like normally if
you would be, you know,

208
00:09:16,200 --> 00:09:19,020
this person who's trying
to resolve this issue,

209
00:09:19,020 --> 00:09:20,550
you would switch between tabs.

210
00:09:20,550 --> 00:09:22,830
You would say, "Okay, now
I need to look at patterns.

211
00:09:22,830 --> 00:09:23,970
Now I need to look like..."

212
00:09:23,970 --> 00:09:25,470
You will go to some dashboards

213
00:09:25,470 --> 00:09:28,110
and like all of a sudden,
you will find yourself

214
00:09:28,110 --> 00:09:31,560
like the 10 browser tabs
open at the same time.

215
00:09:31,560 --> 00:09:34,290
But here everything is
actually in a context suite

216
00:09:34,290 --> 00:09:35,283
just in a thread.

217
00:09:36,630 --> 00:09:38,880
And then, you know, this
conversation takes long

218
00:09:38,880 --> 00:09:40,533
and then after some time,

219
00:09:43,110 --> 00:09:46,080
some other teammates came
in, like it's re-reported

220
00:09:46,080 --> 00:09:48,390
and then colonize or identify
the problematic comment

221
00:09:48,390 --> 00:09:49,980
in the latest deployment.

222
00:09:49,980 --> 00:09:52,920
And then it's asked for like,
do you want me to add rollback

223
00:09:52,920 --> 00:09:55,380
and user requested the
rollback and it's the rollback.

224
00:09:55,380 --> 00:10:00,270
And then the next step
would be like, okay,

225
00:10:00,270 --> 00:10:02,160
you can maybe continue analysis,

226
00:10:02,160 --> 00:10:03,960
but then you can continue
foraging for issues.

227
00:10:03,960 --> 00:10:06,720
So it is like the issue is resolved

228
00:10:06,720 --> 00:10:08,700
like with minimum human involvement

229
00:10:08,700 --> 00:10:10,800
and this human doesn't need to know

230
00:10:10,800 --> 00:10:13,050
about like how to query data,

231
00:10:13,050 --> 00:10:15,390
how to orchestrate the actions.

232
00:10:15,390 --> 00:10:20,390
It's just like asking questions in any way

233
00:10:21,150 --> 00:10:24,123
and then let the AI
teammates do their work.

234
00:10:26,280 --> 00:10:29,160
So I just wanted to talk
about one more thing that here

235
00:10:29,160 --> 00:10:31,680
that like, as you can see
now, like in this thread,

236
00:10:31,680 --> 00:10:34,050
it's all preserved, all
the charts that are,

237
00:10:34,050 --> 00:10:36,240
all the data that is
retreived for this purpose,

238
00:10:36,240 --> 00:10:39,120
all the charts that are drawn

239
00:10:39,120 --> 00:10:41,340
for this purpose are now
preserved in this thread.

240
00:10:41,340 --> 00:10:43,140
You can always go back as a human user

241
00:10:43,140 --> 00:10:45,660
and see what actually happened,

242
00:10:45,660 --> 00:10:48,570
but like more importantly,
your AI teammates can go back

243
00:10:48,570 --> 00:10:50,253
and see what happened there.

244
00:10:51,810 --> 00:10:56,343
But running LLMs on
streaming data is, you know,

245
00:10:57,600 --> 00:10:58,953
more difficult than said.

246
00:11:00,046 --> 00:11:02,550
So like some of the
telemetry data sets are kind

247
00:11:02,550 --> 00:11:06,030
of more suitable for running with the AI.

248
00:11:06,030 --> 00:11:08,880
So logs and events have starts on them now

249
00:11:08,880 --> 00:11:11,063
because like they are
more compatible with LLMs

250
00:11:11,063 --> 00:11:14,190
because by their nature,
they are like diverse

251
00:11:14,190 --> 00:11:15,660
that are tokenized by LLMs

252
00:11:15,660 --> 00:11:19,710
and that like any log line
actually can mean something

253
00:11:19,710 --> 00:11:20,587
to an LLM.

254
00:11:21,600 --> 00:11:24,210
Similarly and like events
like the Kubernetes events,

255
00:11:24,210 --> 00:11:25,920
such events actually means something

256
00:11:25,920 --> 00:11:28,713
and like the LLM has
enough context about those.

257
00:11:29,790 --> 00:11:33,540
But like they are easy
to understand for LLMs,

258
00:11:33,540 --> 00:11:37,110
but they are not easy to,
you know, just feed LLMs for

259
00:11:37,110 --> 00:11:39,990
because like they comes
in the petabytes of scale

260
00:11:39,990 --> 00:11:43,320
and you need to actually
find a way to like, you know,

261
00:11:43,320 --> 00:11:47,100
allow it to feed the
LLM in a distilled way.

262
00:11:47,100 --> 00:11:49,590
Matrix entries are also,
you know, interesting,

263
00:11:49,590 --> 00:11:51,840
like you cannot just like expect LLM

264
00:11:51,840 --> 00:11:54,300
to reveal the trends for you,

265
00:11:54,300 --> 00:11:55,950
so you will need to do a boring job

266
00:11:55,950 --> 00:11:58,230
of like, okay, there is a metric anomaly

267
00:11:58,230 --> 00:12:00,150
so I need to pass it to LLM

268
00:12:00,150 --> 00:12:01,830
and like similarly at traces,

269
00:12:01,830 --> 00:12:05,100
I will do a distributed
tail something on traces

270
00:12:05,100 --> 00:12:07,080
and pass it to the LLM so that the LLM

271
00:12:07,080 --> 00:12:08,280
can do analysis after all.

272
00:12:08,280 --> 00:12:12,930
You cannot just dump all the
metrics and traces to LLM

273
00:12:12,930 --> 00:12:15,780
because in this case,
you will go bankrupt.

274
00:12:15,780 --> 00:12:18,270
So it is, as I say,

275
00:12:18,270 --> 00:12:21,690
like it's efficient data
processing requires for terabytes

276
00:12:21,690 --> 00:12:22,950
or petabytes of data,

277
00:12:22,950 --> 00:12:25,800
but it allows you to run
the queries in English.

278
00:12:25,800 --> 00:12:27,360
But the scalability of course

279
00:12:27,360 --> 00:12:29,730
is not the best thing that can happen.

280
00:12:29,730 --> 00:12:33,810
So for that thing, actually,
like I see people that, "Eh,

281
00:12:33,810 --> 00:12:35,490
tokens are getting cheaper, like, ou know,

282
00:12:35,490 --> 00:12:38,280
we can maybe in the future
just feed everything to LLM."

283
00:12:38,280 --> 00:12:39,960
You know, no matter
how cheap they will be,

284
00:12:39,960 --> 00:12:42,450
like it will be still
very expensive at scale.

285
00:12:42,450 --> 00:12:45,268
And like the training,
like the fine tuning models

286
00:12:45,268 --> 00:12:50,040
with your own metrics is
infeasible for most of the data.

287
00:12:50,040 --> 00:12:53,580
And actually not very spoken
truth about data flowing

288
00:12:53,580 --> 00:12:56,460
to AI is that we are just
letting everything out.

289
00:12:56,460 --> 00:12:59,550
So we are just like, okay,
LLM, just read everything,

290
00:12:59,550 --> 00:13:01,440
read my code, read my data,

291
00:13:01,440 --> 00:13:04,320
read like all my intellectual
property, it's all yours

292
00:13:04,320 --> 00:13:06,270
and it's like exposing a risk.

293
00:13:06,270 --> 00:13:07,860
And teams are now busy

294
00:13:07,860 --> 00:13:10,620
with managing the data following to LLM.

295
00:13:10,620 --> 00:13:14,070
And at this point, like we
come to telemetry pipelines.

296
00:13:14,070 --> 00:13:16,530
So when telemetry pipelines are first out,

297
00:13:16,530 --> 00:13:18,300
they were being used for like, hey,

298
00:13:18,300 --> 00:13:20,220
like let me just compress this data,

299
00:13:20,220 --> 00:13:21,780
archive it in a destination

300
00:13:21,780 --> 00:13:24,600
and it'll just like make me, you know,

301
00:13:24,600 --> 00:13:29,280
store my telemetry datasets
like in an S3 bucket.

302
00:13:29,280 --> 00:13:33,150
And then it is just like
keeping the data safe

303
00:13:33,150 --> 00:13:36,600
and then, like we discovered nicer ways

304
00:13:36,600 --> 00:13:38,250
of like the seeing data

305
00:13:38,250 --> 00:13:41,280
and we like did cost actually,
you know, go by like another,

306
00:13:41,280 --> 00:13:42,900
like 100 times.

307
00:13:42,900 --> 00:13:45,350
So we could send the data to Splunk

308
00:13:45,350 --> 00:13:47,250
or Datadog, but you know,

309
00:13:47,250 --> 00:13:49,380
like if you just ingest
all this data to those,

310
00:13:49,380 --> 00:13:51,660
so you will also go another bankruptcy.

311
00:13:51,660 --> 00:13:55,260
And then that's why telemetry
pipelines came into space

312
00:13:55,260 --> 00:13:58,722
to reduce the data, to filter the data

313
00:13:58,722 --> 00:14:01,473
in an intelligent way and to mask data.

314
00:14:02,790 --> 00:14:05,010
And this was actually what we know

315
00:14:05,010 --> 00:14:06,870
as telemetry pipelines today.

316
00:14:06,870 --> 00:14:10,110
This is good value but
it is not strategic.

317
00:14:10,110 --> 00:14:14,400
It is about like, okay,
now I'm reducing the cost,

318
00:14:14,400 --> 00:14:18,510
but like I don't still make
the data speaking to itself.

319
00:14:18,510 --> 00:14:20,820
And for that value, now we have evidence.

320
00:14:20,820 --> 00:14:23,340
Now using telemetry pipelines now,

321
00:14:23,340 --> 00:14:26,640
like, which is also
costing 100 times more,

322
00:14:26,640 --> 00:14:27,990
using telemetry pipelines,

323
00:14:27,990 --> 00:14:30,030
you can just provide high value to AI

324
00:14:30,030 --> 00:14:33,393
but while still keeping
the data under control.

325
00:14:34,650 --> 00:14:38,220
And there's also, you know,
I know a lot of companies

326
00:14:38,220 --> 00:14:41,730
that just like complying with
all these security things,

327
00:14:41,730 --> 00:14:43,882
but like security certificates,

328
00:14:43,882 --> 00:14:45,870
but they are just
sharing their source code

329
00:14:45,870 --> 00:14:50,103
and their architecture
just bluntly with LLMs.

330
00:14:50,103 --> 00:14:52,830
And then like with telemetry
pipelines actually,

331
00:14:52,830 --> 00:14:55,920
you can say that, "Hey, I
like to really mask the data

332
00:14:55,920 --> 00:14:58,170
that goes to, you know, LLM."

333
00:14:58,170 --> 00:14:59,670
I can just say that, "Hey, I don't want

334
00:14:59,670 --> 00:15:01,980
to send my, you know, anything about,

335
00:15:01,980 --> 00:15:05,790
like any information about my users,

336
00:15:05,790 --> 00:15:09,150
any information about their PII

337
00:15:09,150 --> 00:15:13,080
and also about like my own
code, my own service topology,

338
00:15:13,080 --> 00:15:17,040
I don't want to just leak
the important details."

339
00:15:17,040 --> 00:15:19,140
And actually when you have the masking

340
00:15:19,140 --> 00:15:20,460
and filtering capabilities

341
00:15:20,460 --> 00:15:23,400
before the data goes to
telemetry pipeline, sorry,

342
00:15:23,400 --> 00:15:25,230
before the data goes to LLM,

343
00:15:25,230 --> 00:15:27,090
it's actually protecting not only you

344
00:15:27,090 --> 00:15:28,680
but also your customers

345
00:15:28,680 --> 00:15:32,790
and also your new hires that
you will hire in next month

346
00:15:32,790 --> 00:15:35,583
about like, they will
not be blunt using it.

347
00:15:37,890 --> 00:15:39,483
So you may say that like,

348
00:15:41,383 --> 00:15:45,330
now agentic SRE is here now,
like, are they taking our jobs?

349
00:15:45,330 --> 00:15:48,510
Like what's the role of
us in this new paradigm?

350
00:15:48,510 --> 00:15:50,880
I believe this is more like an elevation

351
00:15:50,880 --> 00:15:53,640
rather than just like replacing them.

352
00:15:53,640 --> 00:15:55,200
So as human people,

353
00:15:55,200 --> 00:15:57,570
you will still be responsible
about the risk management.

354
00:15:57,570 --> 00:15:59,850
You are still responsible of the mistakes

355
00:15:59,850 --> 00:16:01,440
that LLMs can make

356
00:16:01,440 --> 00:16:05,130
and then LLMs can
actually create problems.

357
00:16:05,130 --> 00:16:08,370
So you will need to
focus on how they behave.

358
00:16:08,370 --> 00:16:10,290
You will need to change their behavior

359
00:16:10,290 --> 00:16:12,570
by just changing the
tools that they're using,

360
00:16:12,570 --> 00:16:14,427
changing the models that they are using

361
00:16:14,427 --> 00:16:16,650
and changing the data that they are using

362
00:16:16,650 --> 00:16:19,020
and you'll need to
actually be managing them.

363
00:16:19,020 --> 00:16:21,330
And you'll also move
in like an upper layer

364
00:16:21,330 --> 00:16:23,910
in the organization or risk
management perspective.

365
00:16:23,910 --> 00:16:26,130
And then you'll focus on more complex case

366
00:16:26,130 --> 00:16:29,040
and you will just like maybe go shift left

367
00:16:29,040 --> 00:16:31,860
on the development flow

368
00:16:31,860 --> 00:16:33,790
about like how to respond issues

369
00:16:34,770 --> 00:16:37,410
before they actually push the production.

370
00:16:37,410 --> 00:16:39,690
And you'll be the ultimate decision makers

371
00:16:39,690 --> 00:16:41,790
about like how LLMs behave

372
00:16:41,790 --> 00:16:46,260
and like how humans should
be focusing on these jobs.

373
00:16:46,260 --> 00:16:50,400
So this brings me to the
end of my presentation.

374
00:16:50,400 --> 00:16:52,110
So if you have any questions,

375
00:16:52,110 --> 00:16:54,420
we have a booth over
there, Edge Delta booth.

376
00:16:54,420 --> 00:16:56,790
So we will be happy

377
00:16:56,790 --> 00:16:58,560
to talk about these questions there.

378
00:16:58,560 --> 00:17:00,191
Thanks a lot for listening to me today.

379
00:17:00,191 --> 00:17:02,894
(audience applauding)

