1
00:00:00,151 --> 00:00:01,413
- Welcome to our session.

2
00:00:01,413 --> 00:00:03,750
My name is Felix, I'm the product owner

3
00:00:03,750 --> 00:00:06,587
for the Accelerate and Compute at Nvidia.

4
00:00:06,587 --> 00:00:08,760
I'm gonna get started.

5
00:00:08,760 --> 00:00:10,470
So today we have a full agenda,

6
00:00:10,470 --> 00:00:12,150
there's lots of stuff to talk about.

7
00:00:12,150 --> 00:00:13,530
We're gonna introduce a little bit

8
00:00:13,530 --> 00:00:17,015
about the RAPIDS Accelerator first, right?

9
00:00:17,015 --> 00:00:21,720
And then my friend here from
FINRA is gonna talk about FINRA

10
00:00:21,720 --> 00:00:24,990
and their story around people
who leverage the technology

11
00:00:24,990 --> 00:00:27,780
and accelerate their workload.

12
00:00:27,780 --> 00:00:31,080
So without further ado,
just go get quickly

13
00:00:31,080 --> 00:00:32,790
into the context here, right?

14
00:00:32,790 --> 00:00:34,912
Apache Spark is one of the most popular

15
00:00:34,912 --> 00:00:37,230
data processing framework out there.

16
00:00:37,230 --> 00:00:40,530
It's basically used by almost
every single enterprise

17
00:00:40,530 --> 00:00:42,180
and organization out there

18
00:00:42,180 --> 00:00:44,460
for a number of different
kind of needs, right?

19
00:00:44,460 --> 00:00:48,540
Whether it is just doing
analytics, reportings,

20
00:00:48,540 --> 00:00:50,730
or even machine learning, you see.

21
00:00:50,730 --> 00:00:53,820
But more importantly, data is growing

22
00:00:53,820 --> 00:00:57,660
at this crazy exceptional
rate in the last couple years.

23
00:00:57,660 --> 00:00:59,670
So obviously in the last year,

24
00:00:59,670 --> 00:01:01,680
maybe more for AI and all of that,

25
00:01:01,680 --> 00:01:04,950
but it's definitely growing
at a exceptional pace.

26
00:01:04,950 --> 00:01:08,906
So in order for enterprises, organizations

27
00:01:08,906 --> 00:01:13,260
to be able to kind of handle
this exceptional growth,

28
00:01:13,260 --> 00:01:16,740
we're introducing kind of
the GPU acceleration factor.

29
00:01:16,740 --> 00:01:19,290
So what you see on the
right hand side here

30
00:01:19,290 --> 00:01:20,123
is a stack, right?

31
00:01:20,123 --> 00:01:22,044
So you have the Apache Spark workload,

32
00:01:22,044 --> 00:01:26,250
basically Spark application
running here as is.

33
00:01:26,250 --> 00:01:30,030
We are introducing a plugin
that you can kind of install

34
00:01:30,030 --> 00:01:33,060
into the workflow itself,
with no code change,

35
00:01:33,060 --> 00:01:35,070
and then leveraging the
hardware acceleration layer,

36
00:01:35,070 --> 00:01:38,790
the GPU you see, running
on the cloud or on premise,

37
00:01:38,790 --> 00:01:43,790
and then get you the cost
saving and faster result.

38
00:01:44,310 --> 00:01:48,806
So even though the GPU might
cost some amount of money,

39
00:01:48,806 --> 00:01:50,392
it's not really a whole lot,

40
00:01:50,392 --> 00:01:52,890
but the acceleration is so much

41
00:01:52,890 --> 00:01:57,510
that basically you get
your data, your result,

42
00:01:57,510 --> 00:01:59,466
faster and cheaper.

43
00:01:59,466 --> 00:02:01,686
And so that's kind of what we focus on.

44
00:02:01,686 --> 00:02:05,010
And to give you an idea, these
are the kind of companies

45
00:02:05,010 --> 00:02:07,770
and enterprise organization
that's kind of gone public

46
00:02:07,770 --> 00:02:10,320
to talk about their
use of this technology.

47
00:02:10,320 --> 00:02:11,790
In the last two years or so

48
00:02:11,790 --> 00:02:13,020
there are definitely more of that,

49
00:02:13,020 --> 00:02:15,390
but, so you can kind of see a pattern,

50
00:02:15,390 --> 00:02:19,680
like online services,
retail, financial services,

51
00:02:19,680 --> 00:02:22,470
and that's kind of more
relevant today as well.

52
00:02:22,470 --> 00:02:24,090
So I can give you a little bit more,

53
00:02:24,090 --> 00:02:27,750
one other example here,
especially around fraud detection.

54
00:02:27,750 --> 00:02:29,400
So with fraud detection,

55
00:02:29,400 --> 00:02:30,960
oftentimes you have the process

56
00:02:30,960 --> 00:02:32,820
like billions and
billions of record, right?

57
00:02:32,820 --> 00:02:35,732
You go back like five
years, 10 years, 15 years,

58
00:02:35,732 --> 00:02:38,160
kind of looking at patterns.

59
00:02:38,160 --> 00:02:39,812
That pattern is actually using

60
00:02:39,812 --> 00:02:42,960
time series windowing
operation and analysis.

61
00:02:42,960 --> 00:02:46,246
And that's actually something
that's really good with GPU.

62
00:02:46,246 --> 00:02:49,800
And so as a result, we're
able to get 14 times speed up,

63
00:02:49,800 --> 00:02:53,821
in this particular case,
this type of workload at 80%,

64
00:02:53,821 --> 00:02:56,300
well 90% saving.

65
00:02:56,300 --> 00:02:59,370
And if you want to take a look at this,

66
00:02:59,370 --> 00:03:03,780
we actually partnered with
AWS FSI team last year

67
00:03:03,780 --> 00:03:04,680
to make this post.

68
00:03:04,680 --> 00:03:07,650
You're welcome to use this
QR code, open up the webpage,

69
00:03:07,650 --> 00:03:09,840
take a look at it yourself,
kind of what it looks like,

70
00:03:09,840 --> 00:03:12,078
and we'd love to kind of be
in touch with you as well.

71
00:03:12,078 --> 00:03:15,570
So without further ado,
we'd like to introduce

72
00:03:15,570 --> 00:03:18,690
our friend from FINRA here to
talk about their experience

73
00:03:18,690 --> 00:03:20,160
working in this technology.

74
00:03:20,160 --> 00:03:22,050
I'm super excited to actually talk here,

75
00:03:22,050 --> 00:03:25,140
because last year it's our presentation

76
00:03:25,140 --> 00:03:27,090
actually what got them interested in this,

77
00:03:27,090 --> 00:03:30,270
and this is how we were working
together to work on this.

78
00:03:30,270 --> 00:03:31,620
So.

79
00:03:31,620 --> 00:03:33,030
- Perfect.

80
00:03:33,030 --> 00:03:33,863
Thank you, thank you Felix.

81
00:03:33,863 --> 00:03:37,770
Thank you for the great
introduction on Apache Spark RAPIDS

82
00:03:37,770 --> 00:03:41,700
running on Nvidia GPU chipset.

83
00:03:41,700 --> 00:03:46,570
I'm Alain Menguy, the
senior director of big data

84
00:03:47,439 --> 00:03:50,806
and performance engineering at FINRA.

85
00:03:50,806 --> 00:03:55,806
I've been working for FINRA
for more than 20 plus years,

86
00:03:56,080 --> 00:03:59,370
and I can tell you that I've been involved

87
00:03:59,370 --> 00:04:04,200
in many transformative
and innovative project

88
00:04:04,200 --> 00:04:07,423
and this one, Spark RAPIDS on GPU

89
00:04:10,770 --> 00:04:12,483
is absolutely one of them.

90
00:04:15,655 --> 00:04:18,570
Yeah, so first, who is FINRA?

91
00:04:19,660 --> 00:04:22,350
FINRA has been around for a while.

92
00:04:22,350 --> 00:04:27,330
We were celebrating our
85th anniversary last year.

93
00:04:27,330 --> 00:04:31,440
Our mission is twofold, market integrity

94
00:04:31,440 --> 00:04:33,672
and investor protection.

95
00:04:33,672 --> 00:04:36,306
We are making sure that
all the participants

96
00:04:36,306 --> 00:04:38,770
can trade with confidence,

97
00:04:38,770 --> 00:04:43,050
and that the market operate
free from market manipulation

98
00:04:43,050 --> 00:04:44,460
and fraud.

99
00:04:44,460 --> 00:04:47,370
Basically, what we are
doing, we are protecting you,

100
00:04:47,370 --> 00:04:52,370
as investor, to ensure that
your money is safe and sound.

101
00:04:56,130 --> 00:04:58,620
So the nature of our business

102
00:04:58,620 --> 00:05:02,820
make us an AI and a big data company.

103
00:05:02,820 --> 00:05:07,820
We are operating over
1000 petabytes of storage

104
00:05:08,460 --> 00:05:11,190
in the AWS cloud.

105
00:05:11,190 --> 00:05:15,960
In March this year, we
process a peak volume

106
00:05:15,960 --> 00:05:20,960
of 1.5 trillion market
event for a single day.

107
00:05:21,450 --> 00:05:26,450
Yes, I repeat, 1.5 trillion
market event in a single day.

108
00:05:27,810 --> 00:05:29,610
Processing this volume

109
00:05:29,610 --> 00:05:34,020
require a massive
technological infrastructure

110
00:05:34,020 --> 00:05:35,913
and sophisticated system.

111
00:05:38,130 --> 00:05:40,770
So quickly, our business flow,

112
00:05:40,770 --> 00:05:45,150
after the market closed
at 4:00 PM Eastern time,

113
00:05:45,150 --> 00:05:48,300
the member firms have the obligation

114
00:05:48,300 --> 00:05:50,385
to report their trading activity

115
00:05:50,385 --> 00:05:53,043
before eight o'clock the next day.

116
00:05:54,060 --> 00:05:56,430
From there, we have only few hours

117
00:05:56,430 --> 00:05:58,743
to reconstruct the market activity.

118
00:05:58,743 --> 00:06:03,390
Then we are gonna run
hundreds of pattern model

119
00:06:03,390 --> 00:06:06,930
looking for market manipulation and fraud.

120
00:06:06,930 --> 00:06:10,830
It generates alert if
it finds some anomalies,

121
00:06:10,830 --> 00:06:14,332
and those alerts are sent
to our investigator team,

122
00:06:14,332 --> 00:06:18,270
then they can interactively
query the system

123
00:06:18,270 --> 00:06:21,093
to get a better understanding
of those alerts.

124
00:06:23,890 --> 00:06:26,853
Market volume keep going up,

125
00:06:28,740 --> 00:06:32,790
and over the year has
been growing dramatically,

126
00:06:32,790 --> 00:06:36,330
and the trend, that is still gonna go up

127
00:06:36,330 --> 00:06:38,130
the next couple of years.

128
00:06:38,130 --> 00:06:40,950
Modern electronic trading platform

129
00:06:40,950 --> 00:06:43,950
and high frequency trading system

130
00:06:43,950 --> 00:06:46,503
generate far more transaction than before.

131
00:06:47,610 --> 00:06:49,620
And as volume scales up,

132
00:06:49,620 --> 00:06:53,733
so does the computing need
to handle that volume.

133
00:06:54,690 --> 00:06:57,990
Our challenge is not only business logic,

134
00:06:57,990 --> 00:07:01,650
it's also scale and cost efficiency.

135
00:07:01,650 --> 00:07:06,243
And that led us to evaluate
the GPU acceleration option.

136
00:07:09,150 --> 00:07:14,150
As you can imagine, FINRA
needs to constantly adopt.

137
00:07:15,690 --> 00:07:18,960
We cannot stand still.

138
00:07:18,960 --> 00:07:22,975
We are always looking for
ways of new capabilities

139
00:07:22,975 --> 00:07:27,535
that will reduce the cost,
improve the performance

140
00:07:27,535 --> 00:07:32,310
and the concurrency, as well
as strengthening our resiliency

141
00:07:32,310 --> 00:07:36,000
and security, and whatever we adopt

142
00:07:36,000 --> 00:07:39,213
must make performance and economic sense.

143
00:07:42,660 --> 00:07:46,650
So as you know, AWS and
the open source community

144
00:07:46,650 --> 00:07:51,600
are constantly releasing
new hardware, new services,

145
00:07:51,600 --> 00:07:54,000
and new execution model.

146
00:07:54,000 --> 00:07:57,540
So for us, evaluating this innovation

147
00:07:57,540 --> 00:08:01,260
is not something that we do once a year,

148
00:08:01,260 --> 00:08:04,080
it's part of our DNA.

149
00:08:04,080 --> 00:08:07,233
This is something that
we are constantly doing.

150
00:08:08,250 --> 00:08:11,220
It's not about chasing shiny object,

151
00:08:11,220 --> 00:08:16,220
it's about deciding which
EMR version, or Java,

152
00:08:16,260 --> 00:08:18,900
or Spark we will be using.

153
00:08:18,900 --> 00:08:23,130
This ongoing benchmarking
guide our architecture roadmap

154
00:08:23,130 --> 00:08:24,603
and platform decision.

155
00:08:27,090 --> 00:08:31,200
So at FINRA we rely on
the benchmarking suite

156
00:08:31,200 --> 00:08:33,570
that blends industry standard

157
00:08:33,570 --> 00:08:36,840
and real FINRA production workload.

158
00:08:36,840 --> 00:08:41,220
We use TeraSort to stress large scale IO

159
00:08:41,220 --> 00:08:46,220
and TPC-DS to evaluate SQL
planning, joins, and aggregation.

160
00:08:47,520 --> 00:08:49,350
We are also benchmarking things,

161
00:08:49,350 --> 00:08:51,886
six of our regulatory workload

162
00:08:51,886 --> 00:08:56,886
with very large data set, heavy shuffle,

163
00:08:57,270 --> 00:08:59,103
and complex join pattern.

164
00:09:03,570 --> 00:09:06,180
So for the sake of time,
I will skip that slide,

165
00:09:06,180 --> 00:09:09,776
but understand that we are
doing a fair comparison

166
00:09:09,776 --> 00:09:11,970
between the runs.

167
00:09:11,970 --> 00:09:14,220
We have the same input data dataset,

168
00:09:14,220 --> 00:09:16,230
we have the same Spark configuration,

169
00:09:16,230 --> 00:09:19,110
the same cluster
configuration between runs.

170
00:09:19,110 --> 00:09:21,720
And of course we validate the output.

171
00:09:21,720 --> 00:09:24,690
We are making sure that
the result does not change

172
00:09:24,690 --> 00:09:25,713
between the runs.

173
00:09:28,620 --> 00:09:33,030
So this is really the
Moore's law in action.

174
00:09:33,030 --> 00:09:37,080
Look at the performance
improvement over the years.

175
00:09:37,080 --> 00:09:41,430
Just from the natural
evolution of EMR, Spark,

176
00:09:41,430 --> 00:09:44,640
Java and underlying hardware.

177
00:09:44,640 --> 00:09:49,640
Those are the results for our
TPC-DS nine terabyte benchmark

178
00:09:50,007 --> 00:09:53,523
that we've been running
through all those years.

179
00:09:54,690 --> 00:09:58,770
No code change, same data, same logic,

180
00:09:58,770 --> 00:10:02,190
just newer Spark version, newer engine,

181
00:10:02,190 --> 00:10:04,860
and newer instance families.

182
00:10:04,860 --> 00:10:08,370
We went down from nine
hours five or six years ago

183
00:10:08,370 --> 00:10:11,040
when we were using EMR 5.24,

184
00:10:11,040 --> 00:10:15,210
to one hour and 45 with
the latest EMR 7.10

185
00:10:15,210 --> 00:10:17,550
that was released just few weeks ago.

186
00:10:17,550 --> 00:10:20,550
That's a five times
performance improvement.

187
00:10:20,550 --> 00:10:22,650
And you can imagine the cost reduction

188
00:10:22,650 --> 00:10:26,643
by jumping on those new technology.

189
00:10:28,020 --> 00:10:31,110
This is really the Moore's laws in action,

190
00:10:31,110 --> 00:10:35,133
and that's where Spark
GPU entered the stream.

191
00:10:36,210 --> 00:10:39,540
So this is a slide that I
believe I saw a few months ago

192
00:10:39,540 --> 00:10:42,420
on the presentation from the Nvidia team,

193
00:10:42,420 --> 00:10:46,080
and it really caught my eyes.

194
00:10:46,080 --> 00:10:49,452
We are early adopter in the cloud,

195
00:10:49,452 --> 00:10:54,452
our first workload on
the cloud was in 2013.

196
00:10:54,750 --> 00:10:58,710
Back in the Hadoop era
we were using Apache Hive

197
00:10:58,710 --> 00:11:02,520
in order to run our query, our SQL query.

198
00:11:02,520 --> 00:11:04,860
Then Apache Spark came along

199
00:11:04,860 --> 00:11:08,040
and changed the game in memory processing,

200
00:11:08,040 --> 00:11:12,273
gave us a major leap in
speed and efficiency.

201
00:11:13,290 --> 00:11:16,500
So then when I saw that
slide, I said, "Oh, oh, oh,

202
00:11:16,500 --> 00:11:18,510
this is really speaking to me."

203
00:11:18,510 --> 00:11:23,510
So is GPU accelerated spark
the new leap in technology?

204
00:11:25,080 --> 00:11:26,973
So that's what we want to try out.

205
00:11:29,550 --> 00:11:32,403
So let's use TPC-DS nine terabyte,

206
00:11:32,403 --> 00:11:36,483
and wow, breaking the one hour mark.

207
00:11:38,160 --> 00:11:41,580
From 53 minutes exactly

208
00:11:41,580 --> 00:11:43,890
in order to run the exact same workload

209
00:11:43,890 --> 00:11:48,890
that was running for one hour
and 45 minutes on EMR 7.10

210
00:11:49,174 --> 00:11:51,903
and the latest RHGD instance type.

211
00:11:52,920 --> 00:11:57,870
That's roughly 50%
performance reduction in time,

212
00:11:59,580 --> 00:12:00,900
processing time.

213
00:12:00,900 --> 00:12:03,543
And it's also roughly 50% in cost saving,

214
00:12:04,650 --> 00:12:07,093
because of course the cluster is running

215
00:12:07,093 --> 00:12:12,093
for a shorter period of time, so faster,

216
00:12:12,630 --> 00:12:15,582
cheaper, without doing a code change.

217
00:12:15,582 --> 00:12:19,950
So that moment I say, "Well, let's do it

218
00:12:19,950 --> 00:12:22,737
on some real FINRA production workload."

219
00:12:24,300 --> 00:12:27,990
So that's one of the case
that we are having here.

220
00:12:27,990 --> 00:12:29,310
This is not synthetic,

221
00:12:29,310 --> 00:12:33,210
this is real FINRA production workload.

222
00:12:33,210 --> 00:12:37,950
The total input of this
data set is 11 terabyte.

223
00:12:37,950 --> 00:12:41,520
And this pipeline is really a join heavy

224
00:12:41,520 --> 00:12:43,650
and shuffle intensive.

225
00:12:43,650 --> 00:12:46,967
We are joining two very large tables,

226
00:12:46,967 --> 00:12:49,830
50 billion record in each of them.

227
00:12:49,830 --> 00:12:54,810
One of them has 125 rows,
columns, and the other one has 25.

228
00:12:54,810 --> 00:12:57,750
And we are joining the
key by processing debt

229
00:12:57,750 --> 00:13:00,810
and some row ID which is
one of the business logic

230
00:13:00,810 --> 00:13:01,803
in our application.

231
00:13:04,650 --> 00:13:08,193
Here, of course, the data is
partitioned by trading debt.

232
00:13:09,053 --> 00:13:11,310
We didn't change the code,

233
00:13:11,310 --> 00:13:13,803
so this application is return in Scala,

234
00:13:15,240 --> 00:13:20,240
and then we run this
application on CPU and on GPU.

235
00:13:24,150 --> 00:13:28,290
And again, what we see blew our mind.

236
00:13:28,290 --> 00:13:30,960
Again, 50% performance reduction

237
00:13:30,960 --> 00:13:34,763
and lower cost of around 45%.

238
00:13:37,410 --> 00:13:39,180
No code change.

239
00:13:39,180 --> 00:13:42,900
And this is really where
we start to say to ourself,

240
00:13:42,900 --> 00:13:45,150
there is something that
we really need to explore,

241
00:13:45,150 --> 00:13:47,010
because this is mind blowing

242
00:13:47,010 --> 00:13:49,533
from a performance and
cost saving opportunity.

243
00:13:51,180 --> 00:13:54,450
So the one of the key takeaway

244
00:13:54,450 --> 00:13:57,271
that I want you to remember is,

245
00:13:57,271 --> 00:14:00,407
we didn't just switch the flip,

246
00:14:00,407 --> 00:14:03,420
flip the switch in order to enable

247
00:14:03,420 --> 00:14:05,947
that massive performance gain.

248
00:14:05,947 --> 00:14:10,947
On our first try, the GPU
run ran for five hours.

249
00:14:11,760 --> 00:14:14,100
So then we did contact the Nvidia team,

250
00:14:14,100 --> 00:14:15,840
we had a very good collaboration

251
00:14:15,840 --> 00:14:19,470
between the two engineering
team in order to identify

252
00:14:19,470 --> 00:14:21,360
what was the bottleneck.

253
00:14:21,360 --> 00:14:25,080
We explored the plan.

254
00:14:25,080 --> 00:14:29,370
We saw that sometimes the GPU
was falling back on the CPU,

255
00:14:29,370 --> 00:14:32,130
identify a bunch of bottleneck,

256
00:14:32,130 --> 00:14:37,130
and the Nvidia team came
up with a new plugin

257
00:14:37,650 --> 00:14:41,708
based on those finding
on this special workload.

258
00:14:41,708 --> 00:14:46,380
So a key takeaway, I
say yes, no code change,

259
00:14:46,380 --> 00:14:50,013
but it's not plug and play, not always.

260
00:14:54,930 --> 00:14:56,280
Then another workload.

261
00:14:56,280 --> 00:14:58,260
So this is the key data extraction.

262
00:14:58,260 --> 00:15:00,270
I've been telling you
that the member firms

263
00:15:00,270 --> 00:15:02,790
are submitting their file to us.

264
00:15:02,790 --> 00:15:05,580
So they're sending CSV.bzip2 file.

265
00:15:05,580 --> 00:15:08,940
We need to decompress
them, we need to read them,

266
00:15:08,940 --> 00:15:10,890
parse into type columns,

267
00:15:10,890 --> 00:15:13,900
and then from there we
need to convert them

268
00:15:14,794 --> 00:15:18,953
to a Parquet format
using the Snappy protocol

269
00:15:18,953 --> 00:15:21,754
compression format.

270
00:15:21,754 --> 00:15:25,110
Firms are sending us a
hundreds of thousands of files

271
00:15:25,110 --> 00:15:26,010
on the daily basis.

272
00:15:26,010 --> 00:15:29,220
So this is an application
that is gonna run

273
00:15:29,220 --> 00:15:32,163
quite a few times over a single day.

274
00:15:34,170 --> 00:15:37,833
So here again, iteration process,

275
00:15:39,090 --> 00:15:42,510
at first CPU, GPU, we were even,

276
00:15:42,510 --> 00:15:44,880
and then again we collaborate
with the Nvidia team

277
00:15:44,880 --> 00:15:46,860
in order to identify the bottleneck.

278
00:15:46,860 --> 00:15:51,690
And what we've seen is that
the original application

279
00:15:51,690 --> 00:15:54,087
is using the dataset API,

280
00:15:54,087 --> 00:15:58,410
and that was not really a
good use case for the GPU.

281
00:15:58,410 --> 00:16:02,370
So we had to transform
to a data frame API.

282
00:16:02,370 --> 00:16:05,430
And by doing this, by
doing multiple iteration,

283
00:16:05,430 --> 00:16:08,190
we were able to reduce the time by two,

284
00:16:08,190 --> 00:16:10,470
and then at the end of the day by 10.

285
00:16:10,470 --> 00:16:12,750
So, which was a major improvement,

286
00:16:12,750 --> 00:16:15,390
but again here, very strong collaboration

287
00:16:15,390 --> 00:16:19,683
with the Nvidia team in order
to arrive to this result.

288
00:16:21,420 --> 00:16:26,420
So key takeaway, we look at
benchmark, industry benchmark,

289
00:16:26,790 --> 00:16:29,721
and we look at some FINRA
production workload,

290
00:16:29,721 --> 00:16:34,721
and what worked very well was that yes,

291
00:16:36,120 --> 00:16:40,453
we were able to get two times
faster join at lower cost

292
00:16:40,453 --> 00:16:43,473
on some of the workload that we try.

293
00:16:44,550 --> 00:16:46,950
RAPIDS integrate very well with SQL

294
00:16:46,950 --> 00:16:48,483
and data frame pipeline,

295
00:16:49,350 --> 00:16:54,350
no code change in some case
were needed for some pipeline.

296
00:16:54,780 --> 00:16:58,740
And yes, the run time and the
cost saving were consistent

297
00:16:58,740 --> 00:17:00,663
on switching up the configuration.

298
00:17:01,661 --> 00:17:05,881
What was hard was the GPU
memory has some limitation.

299
00:17:05,881 --> 00:17:10,621
The spill to this needs to
be managed very carefully.

300
00:17:10,621 --> 00:17:14,114
Some operator fall back to CPU,

301
00:17:14,114 --> 00:17:17,190
creating mixed execution plan,

302
00:17:17,190 --> 00:17:21,060
and the RAPID configuration
is not straightforward.

303
00:17:21,060 --> 00:17:24,840
You know, open source is a
little bit finicky and tricky.

304
00:17:24,840 --> 00:17:27,090
So one change in one parameter

305
00:17:27,090 --> 00:17:31,260
can do some dramatic
performance improvement

306
00:17:31,260 --> 00:17:34,983
or sometimes performance down.

307
00:17:35,820 --> 00:17:40,820
And also one of the issue was
the GPU instance availability.

308
00:17:41,190 --> 00:17:45,061
It's not always easy to get them,

309
00:17:45,061 --> 00:17:49,590
especially if you require,
if you need hundreds of them.

310
00:17:49,590 --> 00:17:52,860
So we need to establish
some relation with Nvidia

311
00:17:52,860 --> 00:17:55,893
in order to be able to get our capacity

312
00:17:55,893 --> 00:17:58,083
for those type of workload here.

313
00:18:00,120 --> 00:18:02,550
So where are we heading?

314
00:18:02,550 --> 00:18:07,550
So I think this is the end
of the first proof of concept

315
00:18:07,650 --> 00:18:11,856
that we did with the GPU
and Apache Spark Rapid.

316
00:18:11,856 --> 00:18:16,350
So not every workload get advantage,

317
00:18:16,350 --> 00:18:18,933
or you are gonna need to spend lot of time

318
00:18:18,933 --> 00:18:21,660
in order to identify
what is the bottleneck.

319
00:18:21,660 --> 00:18:24,378
Now we have a process
where we can validate

320
00:18:24,378 --> 00:18:29,130
the CPU versus GPU execution time.

321
00:18:29,130 --> 00:18:31,578
So we have a process to test more.

322
00:18:31,578 --> 00:18:36,578
We are hopeful that GPU
evolve from experimentation

323
00:18:37,040 --> 00:18:39,180
to a strategic performance level,

324
00:18:39,180 --> 00:18:42,480
based on the performance
benefits and cost reduction

325
00:18:42,480 --> 00:18:44,250
that we've seen.

326
00:18:44,250 --> 00:18:46,530
This is an evolution for us.

327
00:18:46,530 --> 00:18:49,740
We are gonna add GPU and Spark Rapids

328
00:18:49,740 --> 00:18:53,958
into our big data workload stack.

329
00:18:53,958 --> 00:18:58,650
However, today CPU remain our default,

330
00:18:58,650 --> 00:19:03,573
but GPU certainly will be
our future accelerator path.

331
00:19:06,990 --> 00:19:08,608
And I'm right on time.

332
00:19:08,608 --> 00:19:09,540
(Alain laughs)

333
00:19:09,540 --> 00:19:12,480
Perfect, so thank you,

334
00:19:12,480 --> 00:19:14,343
and we are happy to take question.

335
00:19:16,980 --> 00:19:17,910
Oh, there's one more one.

336
00:19:17,910 --> 00:19:19,189
Here we go.

337
00:19:19,189 --> 00:19:20,022
- [Felix] You're welcome to try it out.

338
00:19:20,022 --> 00:19:21,090
This is the URL.

339
00:19:21,090 --> 00:19:23,460
You can go in and welcome to contact us

340
00:19:23,460 --> 00:19:24,560
in this email as well.

