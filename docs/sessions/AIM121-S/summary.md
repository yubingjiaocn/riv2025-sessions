# AWS re:Invent 2025 - AI 可观测性最佳实践

## 会议概述

本次会议由 Chronosphere 的销售工程师 Ryan 主讲,重点介绍了 AI 可观测性的核心概念和实践应用。Chronosphere 是一家专注于开源数据收集、大规模性能和可靠性的可观测性公司,近期在 AI 公司和 AI 用例中取得了显著成功。

会议深入探讨了 AI 可观测性面临的独特挑战,包括传统的大规模云原生问题(如海量规模、关键任务可靠性、分布式系统故障排查复杂性、可观测性成本控制)以及 AI 特有的挑战(如模型行为准确性、Token 经济学管理、复杂依赖关系理解、GPU 基础设施管理)。演讲者将市场分为四类参与者:模型构建者、GPU 提供商、AI 原生公司和功能构建者,并针对每类场景提供了具体的可观测性解决方案。

会议涵盖了三个主要用例:模型训练/微调、模型推理托管,以及 AI 原生产品的推理健康监控。通过 Chronosphere 平台的实际演示,展示了如何利用开源工具(OpenTelemetry、Prometheus、NVIDIA DCGM 等)实现端到端的 AI 系统可观测性,帮助团队最大化训练效率、确保推理服务可靠性,并通过数据驱动的方式优化 Token 成本和模型性能。

## 详细时间线

### 开场介绍
[00:00 - 01:30] - 演讲者自我介绍,介绍 Chronosphere 公司背景及其在 AI 可观测性领域的定位

[01:30 - 02:15] - 会议议程概览:AI 可观测性简介、不同使用模式和用例、如何利用可观测性避免常见陷阱

[02:15 - 02:45] - 鼓励参会者访问 Chronosphere 展位,体验 AI 引导功能和新特性演示

### 市场格局分析
[02:45 - 04:00] - AI 市场四大类参与者分析:
- 模型构建者(构建基础模型)
- GPU 提供商(为 AI 推理、训练和微调定制 GPU 基础设施)
- AI 原生公司(从零开始围绕 AI 技术构建产品)
- 功能构建者(在现有产品中添加 AI 功能)

[04:00 - 05:30] - 可观测性挑战分析:
- 传统挑战:海量规模、关键任务可靠性、高性能要求、分布式系统故障排查、成本和数据量控制、基数管理
- AI 特有挑战:模型行为准确性、Token 经济学、复杂依赖关系(MCP、RAG、Agent 架构)、GPU 基础设施管理

### 用例一:模型训练与微调
[05:30 - 06:15] - 模型训练用例关键要素:训练效率、模型性能(最终产出)、GPU 利用率

[06:15 - 07:30] - 模型开发生命周期概述:大数据集 → GPU 加速计算基础设施 → 分布式训练作业 → 训练完成的模型 → 推理服务部署

[07:30 - 08:45] - 训练架构的规模、可靠性和性能要求,强调训练周期越多、算力越大,模型质量越好,高效训练成为竞争优势

[08:45 - 10:30] - 问题模式识别:
- 数据集:少量不准确或无效数据可能污染整个训练周期,需要理解数据集元数据及其对结果的影响
- 数据摄取服务:速度慢或错误峰值会成为训练管道瓶颈
- 训练作业:需要关联基础设施问题与训练结果
- GPU:停机或低利用率不仅浪费资金,还会延缓产品上市时间

[10:30 - 12:00] - Chronosphere 解决方案演示:
- 服务页面展示 GPU 指标(NVIDIA DCGM Prometheus exporter)
- 训练指标(通过 OpenTelemetry Python SDK):训练准确度、梯度范数、每秒样本数
- 通过标签策略关联 GPU 和训练作业
- 低延迟告警:XID 错误检测到运维人员响应的时间至关重要

[12:00 - 13:00] - 分布式追踪展示:
- 依赖关系图自动生成
- 错误峰值和数据摄取服务延迟可视化
- 所有遥测数据(日志、事件、指标、追踪)集中管理
- 最小化训练停机时间,最大化 GPU 利用率

### 用例二:模型推理托管
[13:00 - 14:00] - 推理托管关键要素:服务可靠性、响应速度、可扩展性

[14:00 - 15:30] - 推理架构分析:
- 类似传统云原生服务架构,后端接入推理服务
- 用户需要跨多客户端设备获得快速准确的响应
- 服务依赖推理,正常运行时间和性能至关重要
- 事故和中断可能产生高影响和高可见度(避免因 AI 提供错误或有害信息而上新闻)

[15:30 - 16:30] - 问题模式:前端 UI 问题、上游依赖、支持服务可靠性、网络问题、GPU 性能(对推理影响相对较小但仍需监控)

[16:30 - 18:30] - Chronosphere 推理监控演示:
- 平台团队自托管推理的视角
- RED 指标(请求、错误、持续时间)监控
- 推理健康评估指标:幻觉率、偏见响应率、有毒响应率
- 异常检测功能(差异诊断):点击任何图表快速识别与异常最相关的标签(构建版本、集群版本、容器等)

### 用例三:AI 原生产品与推理健康
[18:30 - 20:00] - AI 原生定义:从第一天起就围绕 AI 技术设计构建的产品,测试方法:"如果我们构建一个 X,但使用 AI"

[20:00 - 22:00] - 传统架构 vs AI 架构对比:
- 传统:严格模式和数据模型、CRUD REST 架构、逐个实现端点功能
- AI:数据模型不需要那么严格、LLM 具有推理能力可动态处理未预实现的请求、可使用非结构化数据
- AI 架构围绕推理和 Token、推理和 RAG 能力、提示和上下文工程优化

[22:00 - 23:30] - 关键概念定义:
- Token:进出 LLM 的字数统计,用于衡量吞吐量和计算定价
- 评估(Evaluations):检查 LLM 输入输出是否符合预期和健康标准
- RAG(检索增强生成):通过外部数据集扩展基础模型的知识

[23:30 - 24:30] - 推理健康关注点:模型准确性/性能 vs Token 经济学和成本,通过 AI 实现产品差异化

### 推理健康问题与解决方案
[24:30 - 26:30] - 常见推理健康问题示例:
- 幻觉:LLM 自信地陈述错误信息(例如将某物错误识别为 3D 打印望远镜)
- 偏见:在招聘等场景中产生有害偏见(例如"只雇佣冰球迷")
- 过度 Token 消耗:简单问题得到冗长回答,大规模下造成成本浪费

[26:30 - 29:00] - 问题根源分析:
- 幻觉原因:提示问题、训练数据不准确、缺少 RAG 工具或更新信息
- 偏见原因:训练数据存在偏见、缺少评估或防护栏、提示模糊
- 过度 Token 消耗:Agent 工作流中的无限循环请求、缺少输出过滤、未指定响应格式
- 自托管推理:温度设置、推理和模型配置、训练数据质量、GPU 性能影响模型行为

[29:00 - 32:00] - Chronosphere 推理健康监控演示:
- OpenTelemetry 追踪(使用 Arise AI 的 Open Inference 库)
- LLM 特定属性:模型版本、提示输入输出、Token 计数、幻觉/偏见/有毒性评估属性
- 追踪中任何服务错误或幻觉/偏见问题会标红,快速定位问题
- 可将数据输入 Phoenix 等评估系统或自建评估系统

[32:00 - 34:00] - 数据驱动决策示例:
- 按模型分解的每请求平均成本对比
- 跨时间比较,识别提示变化对不同模型成本的影响
- 幻觉率监控:检测模型或提供商变更导致的幻觉峰值,快速从生产环境撤出问题版本
- 强调 SRE 和支持运维人员快速响应的重要性

### 总结与技术栈
[34:00 - 35:30] - 强调所有演示数据来自开源工具:
- OpenTelemetry SDK 和 Collector
- NVIDIA DCGM Prometheus Exporter
- Kube State Metrics
- Prometheus Node Exporter
- Open Inference SDK(Arise AI)
- Phoenix AI
- Fluent Bit(日志)
- Chronosphere 不使用专有 Agent

[35:30 - 36:00] - 会议结束,感谢参会者,祝愿大家在 re:Invent 期间收获满满