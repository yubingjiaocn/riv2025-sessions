1
00:00:00,510 --> 00:00:02,970
- Good morning everyone,
and welcome to the day one

2
00:00:02,970 --> 00:00:04,590
of re:Invent.

3
00:00:04,590 --> 00:00:06,630
For those who had celebrated Thanksgiving,

4
00:00:06,630 --> 00:00:09,630
I hope you had a great Thanksgiving
with family and friends.

5
00:00:11,580 --> 00:00:13,830
If you have traveled
here, I hope your journey

6
00:00:13,830 --> 00:00:18,830
was smoother than mine because
my travel was a disaster.

7
00:00:22,140 --> 00:00:24,243
Let me tell you what happened.

8
00:00:25,200 --> 00:00:27,720
I flew in right in the
middle of the winter storm

9
00:00:27,720 --> 00:00:29,370
hitting the Midwest.

10
00:00:29,370 --> 00:00:31,440
So due to that, due to the bad weather,

11
00:00:31,440 --> 00:00:33,840
lots of the flights were canceled.

12
00:00:33,840 --> 00:00:35,850
It's hundreds and thousands
of flights were canceled

13
00:00:35,850 --> 00:00:37,200
and it was a mess.

14
00:00:37,200 --> 00:00:40,560
When I reached airport, I got not one,

15
00:00:40,560 --> 00:00:43,050
three different gate updates.

16
00:00:43,050 --> 00:00:48,050
My phone said B12, whereas
the email alert said B17

17
00:00:48,120 --> 00:00:51,090
and the monitor in front
of me at the airport

18
00:00:51,090 --> 00:00:52,533
confidently said C7.

19
00:00:53,670 --> 00:00:56,790
So I was standing at the
terminal understanding,

20
00:00:56,790 --> 00:00:59,710
having the moment of clarity saying that

21
00:01:00,660 --> 00:01:04,003
even if something as
simple as a gate number

22
00:01:04,003 --> 00:01:07,950
can be become your adventure story,

23
00:01:07,950 --> 00:01:12,150
that exactly the challenge that
we are going to unpack today

24
00:01:12,150 --> 00:01:15,960
from fragmented systems to unified data,

25
00:01:15,960 --> 00:01:20,940
to power, accelerate and
transform AML experiences

26
00:01:20,940 --> 00:01:22,533
across your organizations.

27
00:01:24,090 --> 00:01:28,950
So if that was your
travel journey, welcome.

28
00:01:28,950 --> 00:01:33,060
We are around the friends,
however, we are finally here

29
00:01:33,060 --> 00:01:35,310
and we are together in this room

30
00:01:35,310 --> 00:01:39,420
and we are going to kick off
the day one of re:Invent.

31
00:01:39,420 --> 00:01:43,023
Sorry, we are going to kick
off the re:Invent in this room.

32
00:01:44,370 --> 00:01:48,840
So along with me today,
I've joined by two speakers,

33
00:01:48,840 --> 00:01:51,393
one, Garima Sharma, VP of data Rocket,

34
00:01:52,500 --> 00:01:55,203
and Ilia Fisher, director
of engineering at Rocket.

35
00:01:56,965 --> 00:02:00,060
You're going to hear directly
from them what transformation,

36
00:02:00,060 --> 00:02:03,900
innovation, the architecture, the wins,

37
00:02:03,900 --> 00:02:06,720
and the hard earned
lessons that can help you

38
00:02:06,720 --> 00:02:08,703
to accelerate your journey.

39
00:02:10,200 --> 00:02:12,030
So let's start with a simple question.

40
00:02:12,030 --> 00:02:15,360
What happens when your data grows faster

41
00:02:15,360 --> 00:02:17,103
than you actually uses it?

42
00:02:18,450 --> 00:02:20,373
I will give a second for you to think.

43
00:02:22,320 --> 00:02:25,863
Honestly, that's every
enterprise right now.

44
00:02:27,270 --> 00:02:28,110
That's true.

45
00:02:28,110 --> 00:02:32,550
Every enterprise is accumulating
or collecting the data

46
00:02:32,550 --> 00:02:35,730
more than ever from the more systems

47
00:02:35,730 --> 00:02:38,820
and in different, many formats,

48
00:02:38,820 --> 00:02:42,510
faster than their teams can organize it.

49
00:02:42,510 --> 00:02:45,150
So what is the side effect of it?

50
00:02:45,150 --> 00:02:47,670
Your data may be gets richer,

51
00:02:47,670 --> 00:02:51,660
but turning the data into insights,

52
00:02:51,660 --> 00:02:56,373
into actions into AI, that gets poorer.

53
00:02:57,750 --> 00:03:01,470
So why? Because that
would be your constraint.

54
00:03:01,470 --> 00:03:03,633
So that's when the
fragmentation creeps in.

55
00:03:05,310 --> 00:03:08,010
So how does this actually
happens in the organization?

56
00:03:09,120 --> 00:03:10,980
Is it like organization sit out

57
00:03:10,980 --> 00:03:13,170
and say that, "Hey,
let's create a fragmented

58
00:03:13,170 --> 00:03:15,120
mess organization?"

59
00:03:15,120 --> 00:03:16,410
Definitely no.

60
00:03:16,410 --> 00:03:19,353
So they'll set a goal
to say that move faster.

61
00:03:21,030 --> 00:03:25,121
The three main things that
every organization set the goals

62
00:03:25,121 --> 00:03:29,880
to their team is one,
innovate and scale out faster

63
00:03:29,880 --> 00:03:32,730
and the third reduce the time to market.

64
00:03:32,730 --> 00:03:35,220
These are the three
things that we always say,

65
00:03:35,220 --> 00:03:39,000
but what happens with
every product launch?

66
00:03:39,000 --> 00:03:41,280
When you stand up, every new use case,

67
00:03:41,280 --> 00:03:43,880
you are going to solve a
problem from the local team

68
00:03:44,970 --> 00:03:46,773
and you celebrate the wins.

69
00:03:47,670 --> 00:03:50,313
But what happens? Fragmentation creeps in.

70
00:03:51,870 --> 00:03:56,310
So under the hood, when
you launch a new product

71
00:03:56,310 --> 00:04:00,360
or launch a new use case,
it comes with its own

72
00:04:00,360 --> 00:04:03,570
customer data and it
comes with own schema,

73
00:04:03,570 --> 00:04:06,243
own pipeline and own data assets.

74
00:04:07,200 --> 00:04:10,140
Because of that, you
will be having different

75
00:04:10,140 --> 00:04:11,820
multiple source of truth.

76
00:04:11,820 --> 00:04:14,610
Individually they are successful.

77
00:04:14,610 --> 00:04:18,540
But when you collate all
these things together,

78
00:04:18,540 --> 00:04:20,583
collectively this is fragmentation.

79
00:04:22,410 --> 00:04:25,320
And what happens when it
happens over the period of time

80
00:04:25,320 --> 00:04:27,780
when you have this innovation
cycle, keep going on,

81
00:04:27,780 --> 00:04:32,310
keep going on to your
organization that will accumulate,

82
00:04:32,310 --> 00:04:35,098
you are ended up having
multiple source of truth

83
00:04:35,098 --> 00:04:37,743
as I've given in my introduction,

84
00:04:41,190 --> 00:04:43,530
the same flight from the same airline,

85
00:04:43,530 --> 00:04:46,440
I'm getting the different gate updates.

86
00:04:46,440 --> 00:04:47,717
Why?

87
00:04:47,717 --> 00:04:49,240
Because you have multiple source

88
00:04:49,240 --> 00:04:50,130
of truth which can derive the gate number,

89
00:04:50,130 --> 00:04:52,345
the same that's going to happen

90
00:04:52,345 --> 00:04:53,520
because of the multiple source of truth

91
00:04:53,520 --> 00:04:55,860
that's going to be coexist
in your organization.

92
00:04:55,860 --> 00:04:57,843
You don't know who owns the data

93
00:04:57,843 --> 00:05:00,570
and what part of data is useful

94
00:05:00,570 --> 00:05:03,210
and how the data going to be cleaned

95
00:05:03,210 --> 00:05:06,450
or whether is it the golden
source of truth or not,

96
00:05:06,450 --> 00:05:09,003
see ended up in those situations.

97
00:05:10,980 --> 00:05:14,425
So let's take a quick poll
check, I want to test the room.

98
00:05:14,425 --> 00:05:19,425
Whatever I spoke about until
now, the fragmented systems,

99
00:05:19,530 --> 00:05:21,963
the inconsistency in the metrics.

100
00:05:23,160 --> 00:05:24,960
Who resonates with this?

101
00:05:24,960 --> 00:05:26,940
Please raise your hands when I call it out

102
00:05:26,940 --> 00:05:28,170
few of the things.

103
00:05:28,170 --> 00:05:30,150
Is your data spread across dozens

104
00:05:30,150 --> 00:05:31,750
of systems in your organization.

105
00:05:33,750 --> 00:05:35,760
Okay, I see a few hands raised.

106
00:05:35,760 --> 00:05:38,550
And the next ones does
different teams define

107
00:05:38,550 --> 00:05:40,353
the same metrics differently?

108
00:05:42,270 --> 00:05:43,103
Awesome.

109
00:05:45,185 --> 00:05:49,800
And the third one, how harder
it is you to find the data

110
00:05:49,800 --> 00:05:53,730
or in a way, how much time
that you are finding the data

111
00:05:53,730 --> 00:05:54,933
rather than using it?

112
00:05:57,119 --> 00:05:58,419
Awesome. See, hand raised.

113
00:05:59,725 --> 00:06:01,770
And all of our favorite
questions, all of the above?

114
00:06:01,770 --> 00:06:03,270
When we don't know the answer,

115
00:06:03,270 --> 00:06:05,553
all of the above is the perfect answer.

116
00:06:06,660 --> 00:06:08,820
So everything we talked about,

117
00:06:08,820 --> 00:06:11,010
the fragmentation has a problem

118
00:06:11,010 --> 00:06:13,380
and is a side effect of the innovation,

119
00:06:13,380 --> 00:06:16,440
or you can call it as a
tax that we are paying out

120
00:06:16,440 --> 00:06:18,240
of the innovation that we have done.

121
00:06:19,320 --> 00:06:22,383
So what exactly and why
does it even matter?

122
00:06:24,090 --> 00:06:25,770
Think of this.

123
00:06:25,770 --> 00:06:27,540
When the fragmentation system happen,

124
00:06:27,540 --> 00:06:31,470
it's not just one of the
team is getting bottleneck,

125
00:06:31,470 --> 00:06:34,440
it is across how you
turn the accumulated data

126
00:06:34,440 --> 00:06:35,760
into insights.

127
00:06:35,760 --> 00:06:38,520
How much value we are
deriving from the data,

128
00:06:38,520 --> 00:06:41,880
that's where your downstream
systems are getting impacted.

129
00:06:41,880 --> 00:06:45,000
And when do you realize that
you have a fragmented systems

130
00:06:45,000 --> 00:06:46,290
in your organization?

131
00:06:46,290 --> 00:06:48,750
When your teams are spending
more data in finding

132
00:06:48,750 --> 00:06:50,490
the right data to use.

133
00:06:50,490 --> 00:06:53,850
When your AML teams are waiting for you

134
00:06:53,850 --> 00:06:56,670
to get your data cleanliness
and have the data ready

135
00:06:56,670 --> 00:06:58,620
for them to try the models.

136
00:06:58,620 --> 00:07:02,190
And as the AI is becoming a
biggest thing in the market

137
00:07:02,190 --> 00:07:04,350
when you want to adopt AI,

138
00:07:04,350 --> 00:07:06,780
and everyone complains about accuracy

139
00:07:06,780 --> 00:07:09,660
because why the accuracy get matter

140
00:07:09,660 --> 00:07:13,650
when you talk about rack
patterns, augmented generation?

141
00:07:13,650 --> 00:07:15,761
Every time when you want to give

142
00:07:15,761 --> 00:07:17,580
and if the data is not cleanse enough

143
00:07:17,580 --> 00:07:20,460
and it's not clean enough,
you are going to impact

144
00:07:20,460 --> 00:07:21,780
your accuracy.

145
00:07:21,780 --> 00:07:24,030
That's why it is matter.

146
00:07:24,030 --> 00:07:27,660
And different orations
fields have this pressure

147
00:07:27,660 --> 00:07:29,550
at a different magnitude.

148
00:07:29,550 --> 00:07:33,180
Let's imagine you are the
number one mortgage provider

149
00:07:33,180 --> 00:07:34,623
in the United States.

150
00:07:36,270 --> 00:07:40,533
And you are serving one in
six mortgages in the country.

151
00:07:42,030 --> 00:07:45,870
And if all these work things are happening

152
00:07:45,870 --> 00:07:48,360
that your data is flowing
from hundreds of systems

153
00:07:48,360 --> 00:07:52,713
and thousands of processes and
build one top of each other.

154
00:07:54,180 --> 00:07:56,340
And now imagine you are the leader

155
00:07:56,340 --> 00:07:58,653
to unify all these systems together.

156
00:07:59,940 --> 00:08:02,580
How do you feel and what
are the things comes

157
00:08:02,580 --> 00:08:03,430
to your thoughts?

158
00:08:04,800 --> 00:08:07,200
To share this firsthand information

159
00:08:07,200 --> 00:08:10,440
Rocket is going through
the same transformation

160
00:08:10,440 --> 00:08:14,400
right now at their organization
and to share directly

161
00:08:14,400 --> 00:08:18,718
the inputs, let's welcome Garima Shema,

162
00:08:18,718 --> 00:08:20,700
the VP of Data of Rocket
Companies to talk about

163
00:08:20,700 --> 00:08:23,520
how the transformation
is going on at Rocket.

164
00:08:23,520 --> 00:08:25,136
- Thank you.

165
00:08:25,136 --> 00:08:27,886
(audience claps)

166
00:08:31,200 --> 00:08:33,270
Hi, good morning everyone.

167
00:08:33,270 --> 00:08:36,390
I'm Garima Sharma from Rocket Companies.

168
00:08:36,390 --> 00:08:40,950
Rocket's mission is simple to
help everyone experience home

169
00:08:40,950 --> 00:08:44,160
with which comes stability, hope,

170
00:08:44,160 --> 00:08:46,170
and that sense of belonging.

171
00:08:46,170 --> 00:08:48,510
Every architecture decision we make,

172
00:08:48,510 --> 00:08:50,640
every automation we deploy

173
00:08:50,640 --> 00:08:53,730
and every capability we build with AWS,

174
00:08:53,730 --> 00:08:58,200
ultimately serves that mission
because for us in the end,

175
00:08:58,200 --> 00:09:00,840
it is not about building technology,

176
00:09:00,840 --> 00:09:03,453
it is about creating pathways to home.

177
00:09:05,700 --> 00:09:08,460
Rocket has always been
an innovative company,

178
00:09:08,460 --> 00:09:11,700
leading the transformation
in housing industry.

179
00:09:11,700 --> 00:09:16,700
We were the first to launch
online mortgages in 1998.

180
00:09:19,560 --> 00:09:24,540
We were the first to launch
fully digital mortgages in 2015.

181
00:09:26,550 --> 00:09:29,610
We were also the first to take the mobile

182
00:09:29,610 --> 00:09:34,320
and the very first to
deliver full e-closing

183
00:09:34,320 --> 00:09:37,830
for all 50 states in 2019.

184
00:09:37,830 --> 00:09:41,910
And just about two years ago,
we were the first in our space

185
00:09:41,910 --> 00:09:46,230
to declare an AI fueled
home ownership strategy.

186
00:09:46,230 --> 00:09:48,933
When you innovate at that pace,

187
00:09:50,760 --> 00:09:55,440
naturally new systems,
new data and new tools

188
00:09:55,440 --> 00:09:57,033
come with each milestone.

189
00:09:58,380 --> 00:10:03,380
And here's the important part
in that growth and progress,

190
00:10:04,500 --> 00:10:08,700
we realized that we
needed a data foundation

191
00:10:08,700 --> 00:10:11,970
because for mission this
big, you need a foundation

192
00:10:11,970 --> 00:10:13,053
that can match.

193
00:10:14,400 --> 00:10:18,900
All that innovation was
incredible for business.

194
00:10:18,900 --> 00:10:21,900
We were moving fast and
we were opening doors

195
00:10:21,900 --> 00:10:23,910
to new capabilities.

196
00:10:23,910 --> 00:10:26,400
But as teams were innovating and building,

197
00:10:26,400 --> 00:10:29,670
we began noticing three distinct patterns

198
00:10:29,670 --> 00:10:31,770
in our data landscape.

199
00:10:31,770 --> 00:10:35,730
Not problems, but signals that
we were outgrowing the way

200
00:10:35,730 --> 00:10:38,010
things were done before.

201
00:10:38,010 --> 00:10:43,010
The first siloed and duplicated data.

202
00:10:43,901 --> 00:10:47,790
As teams were building great solutions,

203
00:10:47,790 --> 00:10:50,820
you can imagine they were
doing it often independently,

204
00:10:50,820 --> 00:10:55,820
which meant similar data sets
existed in multiple places.

205
00:10:57,210 --> 00:11:02,210
Second, as our data grew
in volume in variety,

206
00:11:04,260 --> 00:11:07,410
finding the right data
for the right use case

207
00:11:07,410 --> 00:11:10,173
became harder and time consuming.

208
00:11:11,250 --> 00:11:16,080
And the third, as our data grew in volume,

209
00:11:16,080 --> 00:11:19,380
we had definitely very rich data sets.

210
00:11:19,380 --> 00:11:24,380
However, utilizing them
at scale for analytics,

211
00:11:24,390 --> 00:11:28,353
machine learning and AI became
harder than it needed to be.

212
00:11:29,250 --> 00:11:31,560
None of this was unique to Rocket.

213
00:11:31,560 --> 00:11:35,103
Every high growth company
and innovative company

214
00:11:35,103 --> 00:11:38,973
finds itself in such a situation.

215
00:11:39,990 --> 00:11:44,310
But the key point here
for us was we recognized

216
00:11:44,310 --> 00:11:48,150
that we needed a strong foundation

217
00:11:48,150 --> 00:11:50,793
if we were to be
successful in our mission.

218
00:11:56,040 --> 00:12:01,040
And once we recognized what
rapid innovation brings,

219
00:12:01,080 --> 00:12:05,130
we were able to make a core decision,

220
00:12:05,130 --> 00:12:08,340
which is move everything
to one unified data lake,

221
00:12:08,340 --> 00:12:13,320
build on open table formats,
standardized ingestion,

222
00:12:13,320 --> 00:12:14,883
and shared governance.

223
00:12:15,750 --> 00:12:18,900
This was more than an
infrastructure choice for us,

224
00:12:18,900 --> 00:12:22,533
it was an operating model
choice and a speed choice.

225
00:12:23,640 --> 00:12:27,480
At our scale of about 10 plus
petabytes of data back then

226
00:12:27,480 --> 00:12:30,423
and 30 plus petabytes of data now,

227
00:12:31,350 --> 00:12:35,790
we knew that innovation
cannot happen rapidly

228
00:12:35,790 --> 00:12:38,673
if our data was living on islands.

229
00:12:39,630 --> 00:12:44,630
So we made a very simple

230
00:12:44,640 --> 00:12:47,580
and most very simple decision

231
00:12:47,580 --> 00:12:49,950
to adopt a most universal pattern,

232
00:12:49,950 --> 00:12:54,480
bring everything into one
place, clean, consistent,

233
00:12:54,480 --> 00:12:57,960
governed, and make it usable for everyone.

234
00:12:57,960 --> 00:12:59,913
Let me show you what that looked like.

235
00:13:02,730 --> 00:13:04,890
When we talk about
unified data foundation,

236
00:13:04,890 --> 00:13:06,570
this is what we mean.

237
00:13:06,570 --> 00:13:10,020
Rockets data comes in
every shape, structured,

238
00:13:10,020 --> 00:13:12,720
semi-structured, unstructured,

239
00:13:12,720 --> 00:13:16,653
and traditionally each data
type lived in its own system.

240
00:13:17,580 --> 00:13:21,750
Once we got the lake, that gave us a place

241
00:13:21,750 --> 00:13:25,110
where everything flew in,
in a standardized way,

242
00:13:25,110 --> 00:13:27,793
regardless of the format,
everything followed

243
00:13:27,793 --> 00:13:31,410
a standard pattern in architecture.

244
00:13:31,410 --> 00:13:36,390
The data lake gave us a place
where we now finally had

245
00:13:36,390 --> 00:13:37,800
a unified foundation.

246
00:13:37,800 --> 00:13:41,250
And with this foundation you can imagine

247
00:13:41,250 --> 00:13:43,890
the value explodes upward.

248
00:13:43,890 --> 00:13:47,970
BI now has standard
definitions, machine learning

249
00:13:47,970 --> 00:13:52,970
and AI teams now have
clean features at scale

250
00:13:56,130 --> 00:13:59,973
and analytics and streaming become faster.

251
00:14:04,140 --> 00:14:07,530
When we began modernizing
our data landscape,

252
00:14:07,530 --> 00:14:11,070
our vision was simple,
build a platform that powers

253
00:14:11,070 --> 00:14:16,070
every workload, real time
batch, machine learning, AI.

254
00:14:18,540 --> 00:14:23,540
And we knew that we needed to
have one foundation, one lake.

255
00:14:25,770 --> 00:14:30,450
So how we went about it,
step one was hydration.

256
00:14:30,450 --> 00:14:35,450
We brought data from about
12 different OLTP systems

257
00:14:37,020 --> 00:14:41,043
and brought them and
landed it into one lake.

258
00:14:43,290 --> 00:14:48,290
The step two, and that lake
was of course on S3 based

259
00:14:48,410 --> 00:14:51,810
Amazon technology.

260
00:14:51,810 --> 00:14:55,500
Step two was automation.

261
00:14:55,500 --> 00:15:00,450
We built standardized ingestion
pipelines using EMR and Glue

262
00:15:00,450 --> 00:15:04,800
and adopted Parquet as our
default storage format.

263
00:15:04,800 --> 00:15:06,270
Step three was security.

264
00:15:06,270 --> 00:15:09,690
From day one, we encrypted everything,

265
00:15:09,690 --> 00:15:14,690
disc file field, at rest,
in transit, and in use.

266
00:15:19,770 --> 00:15:23,730
Aggregating 10 plus petabytes of data

267
00:15:23,730 --> 00:15:27,300
into single lake gives access.

268
00:15:27,300 --> 00:15:30,843
But as you all know, access
doesn't mean understanding.

269
00:15:32,280 --> 00:15:35,820
And the next step for us was curation

270
00:15:35,820 --> 00:15:40,050
to give the entire company a
shared language around data.

271
00:15:40,050 --> 00:15:43,500
Instead of 30 teams
building their own versions

272
00:15:43,500 --> 00:15:48,367
of customer data, we created Customer 360,

273
00:15:52,108 --> 00:15:54,750
a single governed and
trusted view of the client.

274
00:15:54,750 --> 00:15:59,190
The same happened for Mortgage
360 and Transaction 360,

275
00:15:59,190 --> 00:16:03,480
which are single governed,
trusted view of mortgage loan

276
00:16:03,480 --> 00:16:05,956
and a client's transaction.

277
00:16:05,956 --> 00:16:08,880
Each one has standardized definitions,

278
00:16:08,880 --> 00:16:11,433
consistent metrics, and clear ownership.

279
00:16:12,660 --> 00:16:15,900
And this wasn't just
a data project for us,

280
00:16:15,900 --> 00:16:18,870
it was actually a cultural shift for us

281
00:16:18,870 --> 00:16:23,250
because we moved from duplication

282
00:16:23,250 --> 00:16:28,250
to standardization, from tribal
knowledge to data products,

283
00:16:28,290 --> 00:16:31,710
from searching for data to discovering it.

284
00:16:31,710 --> 00:16:33,690
And once we curated our data layer,

285
00:16:33,690 --> 00:16:36,960
we finally had the
foundation to operationalize

286
00:16:36,960 --> 00:16:39,930
machine learning, AI and analytics

287
00:16:39,930 --> 00:16:42,183
and real time decisioning at scale.

288
00:16:43,440 --> 00:16:46,080
We not only improved how we used data,

289
00:16:46,080 --> 00:16:49,533
but we also unlocked what
we could build on top of it.

290
00:16:51,810 --> 00:16:54,390
And with this strong data foundation,

291
00:16:54,390 --> 00:16:57,153
we created the runway
for something bigger,

292
00:16:58,740 --> 00:17:01,683
to reimagine home ownership end to end.

293
00:17:02,550 --> 00:17:06,090
Traditionally, the housing
industry works in silos.

294
00:17:06,090 --> 00:17:09,240
You have one company that
helps you find a home,

295
00:17:09,240 --> 00:17:12,180
another company helps you finance it,

296
00:17:12,180 --> 00:17:15,573
and another company helps
you own and manage it.

297
00:17:17,250 --> 00:17:22,250
But home ownership isn't three
distinct disconnected stages.

298
00:17:22,380 --> 00:17:24,420
It's one journey.

299
00:17:24,420 --> 00:17:29,100
And this is exactly how now
Rocket is able to treat it.

300
00:17:29,100 --> 00:17:33,540
By bringing Redfin, Rocket
and Mr. Cooper together,

301
00:17:33,540 --> 00:17:36,120
we are now able to support a client

302
00:17:36,120 --> 00:17:38,043
across the entire journey.

303
00:17:40,260 --> 00:17:45,260
And so we can help them find
home finance and own the home.

304
00:17:49,200 --> 00:17:53,970
This foundation was important for us

305
00:17:53,970 --> 00:17:58,970
because now it helped us create
solutions, impact clients

306
00:18:02,520 --> 00:18:06,120
and business performance in
ways that we hadn't imagined.

307
00:18:06,120 --> 00:18:08,970
And results showed up immediately.

308
00:18:08,970 --> 00:18:11,871
During the early phase of integration,

309
00:18:11,871 --> 00:18:14,640
40,000 servicing leads
seamlessly flowed into

310
00:18:14,640 --> 00:18:19,640
our unified platform within
just nine days, clean, governed

311
00:18:19,770 --> 00:18:21,900
and ready for activation.

312
00:18:21,900 --> 00:18:25,530
On day 12, a client from the integration

313
00:18:25,530 --> 00:18:29,190
went from application to
closing in just three days,

314
00:18:29,190 --> 00:18:32,673
something that previously
would've taken the industry weeks,

315
00:18:33,870 --> 00:18:37,920
our engagement engine began
analyzing conversations,

316
00:18:37,920 --> 00:18:42,920
client behavior, and started
surfacing the right outreach

317
00:18:42,990 --> 00:18:44,460
at the right moment.

318
00:18:44,460 --> 00:18:49,023
And that drove up up to nine
points our banker follow ups.

319
00:18:49,920 --> 00:18:54,920
we saw a 10% lift in conversion
across daily credit polls

320
00:18:55,170 --> 00:18:56,910
and applications.

321
00:18:56,910 --> 00:19:01,140
And when in September
the refi wave came in,

322
00:19:01,140 --> 00:19:06,120
our systems were able to
instantly pull qualified clients

323
00:19:06,120 --> 00:19:09,780
and activate personalized
outreach for them.

324
00:19:09,780 --> 00:19:13,740
And that led to 20% increase
in refinance pipeline

325
00:19:13,740 --> 00:19:14,823
just overnight.

326
00:19:17,580 --> 00:19:22,580
We now can unify servicing
data, client intent

327
00:19:23,670 --> 00:19:28,530
and behavioral signals and
that has led us to achieve

328
00:19:28,530 --> 00:19:32,523
three x recapture rate as
compared to the industry.

329
00:19:35,340 --> 00:19:39,276
When you have a such a strong foundation,

330
00:19:39,276 --> 00:19:42,990
nothing is impossible for you.

331
00:19:42,990 --> 00:19:47,370
But it took hard work,
it took a lot of effort

332
00:19:47,370 --> 00:19:51,630
across the team and it took a
lot of alignment across teams

333
00:19:51,630 --> 00:19:55,920
to show the value of a
unified data foundation.

334
00:19:55,920 --> 00:20:00,920
And whatever I've shared so
far, the platform, the AI impact

335
00:20:01,440 --> 00:20:06,210
and the data foundation
was not accidental at all.

336
00:20:06,210 --> 00:20:09,720
It was the result of three things,

337
00:20:09,720 --> 00:20:14,223
intentional architecture,
repeatable patterns,

338
00:20:15,287 --> 00:20:17,313
and governance.

339
00:20:19,290 --> 00:20:23,820
So let me now hand it off to my colleague,

340
00:20:23,820 --> 00:20:27,730
Ilia Fisher, who will walk
you through how we made

341
00:20:28,580 --> 00:20:29,933
all that possible under the hood.

342
00:20:30,983 --> 00:20:32,213
So please welcome Ilia Fisher.

343
00:20:33,307 --> 00:20:36,300
(audience claps)

344
00:20:36,300 --> 00:20:39,300
- Thank you Grima. Whoa,
the microphone is working.

345
00:20:39,300 --> 00:20:40,500
Good morning, re:Invent.

346
00:20:41,940 --> 00:20:44,721
Well, it's so much energy in this room.

347
00:20:44,721 --> 00:20:47,460
Do you feel excited for
this whole week of events?

348
00:20:47,460 --> 00:20:50,880
I am excited, I am excited
to see so many folks here

349
00:20:50,880 --> 00:20:55,740
willing to hear about the
modern data architecture

350
00:20:55,740 --> 00:20:59,400
because it means to me that
whatever we'll share with you

351
00:20:59,400 --> 00:21:02,490
today, you you'll take the
home to your companies,

352
00:21:02,490 --> 00:21:05,160
to businesses and you'll evolve.

353
00:21:05,160 --> 00:21:06,483
You'll evolve.

354
00:21:06,483 --> 00:21:08,670
So in the years to come,
we'll be coming to re:Invent,

355
00:21:08,670 --> 00:21:10,860
we'll be seeing new
patterns, new technologies

356
00:21:10,860 --> 00:21:14,283
that maybe just one of
you will keep driving.

357
00:21:15,660 --> 00:21:19,230
Let me start the presentation
with some personal story.

358
00:21:19,230 --> 00:21:22,470
About 10 years ago I
was buying my own home

359
00:21:22,470 --> 00:21:25,200
and I had to secure Rocket.

360
00:21:25,200 --> 00:21:27,630
Sorry, I had to secure mortgage.

361
00:21:27,630 --> 00:21:30,360
I didn't go to Rocket, I
went to a different broker.

362
00:21:30,360 --> 00:21:34,149
So had a very interesting experience.

363
00:21:34,149 --> 00:21:37,560
Maybe many of you went
through it buying a home,

364
00:21:37,560 --> 00:21:40,950
collective pay stubs for
six months backwards.

365
00:21:40,950 --> 00:21:42,630
You know, the bank statements,

366
00:21:42,630 --> 00:21:46,290
the investment statements, the tax forms,

367
00:21:46,290 --> 00:21:49,125
scan every document as a PDF.

368
00:21:49,125 --> 00:21:51,450
And I had this HP scanner, so
I couldn't put a bulk of them,

369
00:21:51,450 --> 00:21:53,430
just had to scan page by page

370
00:21:53,430 --> 00:21:55,620
and then then email to my mortgage broker.

371
00:21:55,620 --> 00:21:58,440
And I was very much
worried for two things.

372
00:21:58,440 --> 00:22:02,130
Firstly, I hope that I'll get
the mortgage to buy the home.

373
00:22:02,130 --> 00:22:05,010
I did about two weeks
later, received the answer

374
00:22:05,010 --> 00:22:06,870
that I get the mortgage.

375
00:22:06,870 --> 00:22:09,288
And, secondly, I was
very much worried that

376
00:22:09,288 --> 00:22:12,890
there is not anyone else
who is buying their own home

377
00:22:12,890 --> 00:22:15,600
with my identity since
I was sharing all these

378
00:22:15,600 --> 00:22:17,703
confidential documents through an email.

379
00:22:18,730 --> 00:22:21,426
And 10 years later, seems
like that hasn't happened yet,

380
00:22:21,426 --> 00:22:23,006
who knows?

381
00:22:23,006 --> 00:22:28,006
But I'm sharing this story
because maybe not as insecure,

382
00:22:28,290 --> 00:22:31,560
but in terms of the latency
and the manual labor,

383
00:22:31,560 --> 00:22:34,590
this is what Rocket Mortgage
was doing decades ago

384
00:22:34,590 --> 00:22:37,470
and it's entirely different story today.

385
00:22:37,470 --> 00:22:39,780
Decisions like this,
actually millions of them

386
00:22:39,780 --> 00:22:41,460
happen in under eight minutes.

387
00:22:41,460 --> 00:22:46,380
Everything through secure portal,
through secure connections

388
00:22:46,380 --> 00:22:51,380
and people get, get the data, get, an idea

389
00:22:51,990 --> 00:22:55,140
if they can buy home literally in minutes

390
00:22:55,140 --> 00:22:58,110
or in in rare situations in days.

391
00:22:58,110 --> 00:22:59,820
And how did it happen?

392
00:22:59,820 --> 00:23:02,770
How did we went through waiting for weeks

393
00:23:05,026 --> 00:23:05,859
until knowing that you can
get that loan to buy a home

394
00:23:06,970 --> 00:23:09,810
to two hours or maximum few days.

395
00:23:09,810 --> 00:23:14,810
And it happened because we
threw away the old playbook

396
00:23:15,060 --> 00:23:17,932
of treating data as a call center,

397
00:23:17,932 --> 00:23:20,220
sorry for the call
centers, hopefully no one

398
00:23:20,220 --> 00:23:22,080
from a call center here.

399
00:23:22,080 --> 00:23:27,080
And we turned the data to the
engine that runs the company.

400
00:23:28,194 --> 00:23:33,194
210 machine learning models,
thousands of data pipelines

401
00:23:36,126 --> 00:23:39,480
and all that runs in full automation

402
00:23:39,480 --> 00:23:42,063
with zero human intervention.

403
00:23:45,330 --> 00:23:49,080
This is what Rocket Companies do today.

404
00:23:49,080 --> 00:23:51,480
This is what we achieved by building

405
00:23:51,480 --> 00:23:55,020
a modernized data platform without hiring

406
00:23:55,020 --> 00:23:58,260
hundreds of engineers, without purchasing

407
00:23:58,260 --> 00:24:00,360
any silver bullet software.

408
00:24:00,360 --> 00:24:04,230
All that by building that
modernized data platform,

409
00:24:04,230 --> 00:24:06,690
which I'm going to share with you.

410
00:24:06,690 --> 00:24:09,000
So buckle up in the next 20 minutes or so,

411
00:24:09,000 --> 00:24:12,780
we'll go through the steps
of building that platform

412
00:24:12,780 --> 00:24:14,370
through the architecture patterns,

413
00:24:14,370 --> 00:24:17,910
through the design patterns
and I hope you'll find it

414
00:24:17,910 --> 00:24:19,260
interesting and insightful.

415
00:24:20,400 --> 00:24:24,000
So before a deep dive
into the actual services

416
00:24:24,000 --> 00:24:27,106
that run the platform, let me step back

417
00:24:27,106 --> 00:24:32,070
and share with you how
typical data platform,

418
00:24:32,070 --> 00:24:33,570
modern data platform looks like,

419
00:24:33,570 --> 00:24:36,360
which most companies converge on.

420
00:24:36,360 --> 00:24:41,360
And it real boils down to
three clean decoupled layers.

421
00:24:42,360 --> 00:24:45,030
And the first layer you
can see on your screens,

422
00:24:45,030 --> 00:24:46,350
it's called ingestion.

423
00:24:46,350 --> 00:24:49,971
It's goal is to get everything
in fast as it drives,

424
00:24:49,971 --> 00:24:54,971
real time streams, APIs
file drops, SaaS connectors,

425
00:24:56,460 --> 00:24:58,620
the Swiss knife of tool link, if you wish,

426
00:24:58,620 --> 00:25:02,310
is supported to get the data
quickly into your platform

427
00:25:02,310 --> 00:25:04,590
in a raw immutable format.

428
00:25:04,590 --> 00:25:07,380
Ideally, like Garmima was mentioning,

429
00:25:07,380 --> 00:25:11,670
using open formats like
Parkette, so that easy

430
00:25:11,670 --> 00:25:15,180
to integrate the data
with plethora of tools.

431
00:25:15,180 --> 00:25:18,510
So this is the first layer which
is called again, ingestion.

432
00:25:18,510 --> 00:25:21,270
Just land your data as fast as it arrives,

433
00:25:21,270 --> 00:25:25,380
reliably, securely in the raw format.

434
00:25:25,380 --> 00:25:29,340
The next comes the processing layer,

435
00:25:29,340 --> 00:25:32,430
transfer once but serve many.

436
00:25:32,430 --> 00:25:34,690
So our data lake intends to support

437
00:25:35,530 --> 00:25:38,280
so many different use
cases and we don't want to

438
00:25:38,280 --> 00:25:40,170
ad hoc process data for each of them.

439
00:25:40,170 --> 00:25:44,280
So we want to transfer once
this is the key statement once,

440
00:25:44,280 --> 00:25:46,980
but then again, the data will serve many.

441
00:25:46,980 --> 00:25:51,980
The overall data lake is
partitioned in three zones.

442
00:25:52,080 --> 00:25:54,120
The first zone is raw.

443
00:25:54,120 --> 00:25:57,810
This is a where the ingested data lands.

444
00:25:57,810 --> 00:26:01,260
It's exactly what's
landed forever, unchanged.

445
00:26:01,260 --> 00:26:04,260
The next level is process data.

446
00:26:04,260 --> 00:26:08,424
So the process data is enriched clean,

447
00:26:08,424 --> 00:26:12,213
the PII managed according
to these standards,

448
00:26:13,260 --> 00:26:16,193
standardized to to the
standards you define

449
00:26:16,193 --> 00:26:18,120
in your platform, right?

450
00:26:18,120 --> 00:26:20,690
And this is the process
layer still not ready

451
00:26:20,690 --> 00:26:22,470
for your business consumption.

452
00:26:22,470 --> 00:26:26,610
So the conform layer is the third one

453
00:26:26,610 --> 00:26:29,700
and it's data gets the
shape of business level,

454
00:26:29,700 --> 00:26:31,710
domain aligned form.

455
00:26:31,710 --> 00:26:36,710
So that many consuming
applications from a platform,

456
00:26:37,439 --> 00:26:40,560
many consuming use cases
will find it useful

457
00:26:40,560 --> 00:26:43,770
for their specific business case.

458
00:26:43,770 --> 00:26:46,860
Now again, remember
transfer one, serve many.

459
00:26:46,860 --> 00:26:48,720
So right, there are many use cases,

460
00:26:48,720 --> 00:26:51,011
there are many business use cases.

461
00:26:51,011 --> 00:26:52,500
However, we want to ensure
that our transformation

462
00:26:52,500 --> 00:26:55,890
goes only once and produces
the data that we need.

463
00:26:55,890 --> 00:27:00,120
All those transformations are
event driven when possible,

464
00:27:00,120 --> 00:27:04,170
scheduled, when necessary,
declarative version controlled.

465
00:27:04,170 --> 00:27:06,873
So you always know what's going on.

466
00:27:07,950 --> 00:27:11,640
You have a solid audit trail,
how these transformations go.

467
00:27:11,640 --> 00:27:15,540
And this is what platform's
goal is to provide.

468
00:27:15,540 --> 00:27:19,230
And well, last but not least
is the consumption layer.

469
00:27:19,230 --> 00:27:21,579
Call it a gateway to your platform,

470
00:27:21,579 --> 00:27:25,410
one version of truth for
every possible consumer.

471
00:27:25,410 --> 00:27:27,750
So again, one version of truth is the key.

472
00:27:27,750 --> 00:27:30,210
We don't want to have silo data.

473
00:27:30,210 --> 00:27:34,024
We don't want our the users
of our platform to think

474
00:27:34,024 --> 00:27:36,450
which version is the right one.

475
00:27:36,450 --> 00:27:39,270
So our platform provides
a unified service,

476
00:27:39,270 --> 00:27:43,980
a unified data set for variety
of those business use cases,

477
00:27:43,980 --> 00:27:48,150
including analysts doing
business intelligence,

478
00:27:48,150 --> 00:27:50,970
genetic applications,
data scientists training

479
00:27:50,970 --> 00:27:54,240
their models, all of them
point to the very same

480
00:27:54,240 --> 00:27:58,740
well-governed data set
that our platform provides.

481
00:27:58,740 --> 00:28:03,740
All that, this kind of overarching
schema of architecture,

482
00:28:04,590 --> 00:28:07,620
if you wish not the architecture
model schema gives you

483
00:28:07,620 --> 00:28:11,234
a highly scalable infrastructure,

484
00:28:11,234 --> 00:28:16,234
gives you pay only for
what you use economics,

485
00:28:16,830 --> 00:28:18,720
and we'll talk more about that,

486
00:28:18,720 --> 00:28:21,870
and drastically over
lowering operational overhead

487
00:28:21,870 --> 00:28:24,870
because you have like a
well formatted three phases

488
00:28:24,870 --> 00:28:26,430
that your data goes through.

489
00:28:26,430 --> 00:28:29,880
Your processing takes care
about so many use cases,

490
00:28:29,880 --> 00:28:31,830
your ingestion, your consumption,

491
00:28:31,830 --> 00:28:33,960
a Swiss knives easily extendable

492
00:28:33,960 --> 00:28:35,610
to support new ingestion patterns,

493
00:28:35,610 --> 00:28:38,730
new ingestion systems if you wish.

494
00:28:38,730 --> 00:28:41,640
And on the consumption
all the new scenarios

495
00:28:41,640 --> 00:28:44,913
that your platform must
support in your business.

496
00:28:45,870 --> 00:28:49,080
But now let's go a bit more to the details

497
00:28:49,080 --> 00:28:50,970
about each of those layers, how it works.

498
00:28:50,970 --> 00:28:55,970
So today it's not about picking
a tool to make data work.

499
00:28:57,930 --> 00:29:02,250
It's about building a
clean modular pipeline

500
00:29:02,250 --> 00:29:06,600
to ingest process and consume
that is supporting so many

501
00:29:06,600 --> 00:29:07,770
of those use cases.

502
00:29:07,770 --> 00:29:12,270
And again, going back to the
ingestion, get any data in fast

503
00:29:12,270 --> 00:29:15,900
and reliably, we would
support the API gateway,

504
00:29:15,900 --> 00:29:18,330
running on elastic Kubernetes service.

505
00:29:18,330 --> 00:29:21,540
We would support replication of data

506
00:29:21,540 --> 00:29:25,320
between south services into S3 buckets.

507
00:29:25,320 --> 00:29:30,240
We would support streaming
data with Amazon Kinesis.

508
00:29:30,240 --> 00:29:32,940
All that is included
in our ingestion layer,

509
00:29:32,940 --> 00:29:35,460
supporting all the business use cases.

510
00:29:35,460 --> 00:29:37,770
And as soon as new business case arrives

511
00:29:37,770 --> 00:29:41,010
to source data from a new
platform, from a new source,

512
00:29:41,010 --> 00:29:44,723
we can easily extend the
ingestion and accommodate it.

513
00:29:46,620 --> 00:29:51,620
And a bonus is that that
ingested data is landing in

514
00:29:52,140 --> 00:29:55,773
an S3 bucket, which is our raw data lake,

515
00:29:56,853 --> 00:29:58,320
raw data zone in the lake.

516
00:29:58,320 --> 00:30:02,400
And as a result we get
out of the box versioning,

517
00:30:02,400 --> 00:30:04,800
audit trailing of the data.

518
00:30:04,800 --> 00:30:09,270
We built a resilient data lake
with geographical replication

519
00:30:09,270 --> 00:30:11,250
and availability we needed.

520
00:30:11,250 --> 00:30:14,655
And that's like an extra bonus that we get

521
00:30:14,655 --> 00:30:17,237
when relying on AWS.

522
00:30:18,480 --> 00:30:21,720
Next is the processing layer.

523
00:30:21,720 --> 00:30:23,970
Transformed ones used everywhere.

524
00:30:23,970 --> 00:30:26,400
So again, you see those
three standardized buckets

525
00:30:26,400 --> 00:30:29,040
at the bottom, the raw, the processed,

526
00:30:29,040 --> 00:30:30,930
and the conformed data.

527
00:30:30,930 --> 00:30:34,380
Some people you might might
have heard, call them bronze,

528
00:30:34,380 --> 00:30:36,000
silver, and gold.

529
00:30:36,000 --> 00:30:39,091
So, and on top of them the
data processing orchestration

530
00:30:39,091 --> 00:30:43,260
is event driven when
possible and time-based.

531
00:30:43,260 --> 00:30:47,520
For heavy ETL workflows, we rely on Glue

532
00:30:47,520 --> 00:30:51,510
as the most general use
case for highly scalable

533
00:30:51,510 --> 00:30:53,370
compute situations.

534
00:30:53,370 --> 00:30:58,140
With provision EMR, Amazon
managed Flink service helps us

535
00:30:58,140 --> 00:31:00,690
to deal with streaming data.

536
00:31:00,690 --> 00:31:05,370
And for a very lightweight
workflows, we find lambda

537
00:31:05,370 --> 00:31:08,400
and step functions giving
very good economics

538
00:31:08,400 --> 00:31:11,928
to process to make lightweight
data transformations

539
00:31:11,928 --> 00:31:15,060
a personal hint, if you
rely on step functions,

540
00:31:15,060 --> 00:31:19,800
it's an amazing tool, very
addictive with its friendly UI.

541
00:31:19,800 --> 00:31:24,800
But if your data ends up
being virtually streaming,

542
00:31:25,522 --> 00:31:28,260
what I noticed in my experience,

543
00:31:28,260 --> 00:31:30,480
I had a situation where my step functions

544
00:31:30,480 --> 00:31:33,510
became to run a thousand times in a minute

545
00:31:33,510 --> 00:31:36,240
and bills spikes very quickly.

546
00:31:36,240 --> 00:31:40,830
So it's always important to
monitor very well your expenses,

547
00:31:40,830 --> 00:31:45,120
your costs, and using scholastic
services in your compute,

548
00:31:45,120 --> 00:31:46,800
but also being scholastic yourself

549
00:31:46,800 --> 00:31:49,830
to see how you restructure
your architecture

550
00:31:49,830 --> 00:31:53,970
to send data via the right channels.

551
00:31:53,970 --> 00:31:56,940
And so again, this is the processing layer

552
00:31:56,940 --> 00:31:58,860
using all these tools.

553
00:31:58,860 --> 00:32:01,572
There is no religion, like I mentioned.

554
00:32:01,572 --> 00:32:03,030
You should be smart to
say which tool works

555
00:32:03,030 --> 00:32:06,420
for which use case, what
gives you the right economics

556
00:32:06,420 --> 00:32:10,710
and they all are elastically
scalable services.

557
00:32:10,710 --> 00:32:13,740
So you know, in the data world,

558
00:32:13,740 --> 00:32:17,883
you have certain periods of
the year, month, week, a day,

559
00:32:18,759 --> 00:32:20,610
when you have high scalability needs,

560
00:32:20,610 --> 00:32:23,940
you have other times when you
need lower scalability needs.

561
00:32:23,940 --> 00:32:26,786
So using KWS, we are
getting that elasticity

562
00:32:26,786 --> 00:32:30,720
of compute of the
processing and that gives us

563
00:32:30,720 --> 00:32:34,050
near perfect economics
so that we pay exactly

564
00:32:34,050 --> 00:32:35,640
for what we need and when we need

565
00:32:35,640 --> 00:32:38,010
and we get that compute when we need

566
00:32:38,010 --> 00:32:42,300
and with not much overdue.

567
00:32:42,300 --> 00:32:44,340
Lastly, the consumption layer.

568
00:32:44,340 --> 00:32:48,870
One platform, many consumers, same data.

569
00:32:48,870 --> 00:32:50,430
That's that's the importance here.

570
00:32:50,430 --> 00:32:53,430
The same data served for
every possible consumer

571
00:32:53,430 --> 00:32:54,930
without duplication.

572
00:32:54,930 --> 00:32:58,950
And this is what I saw so
many times in businesses that,

573
00:32:58,950 --> 00:33:02,370
well, our data scientists
read from their data set

574
00:33:02,370 --> 00:33:05,220
and our application
reads from their database

575
00:33:05,220 --> 00:33:07,290
and they're not the same.

576
00:33:07,290 --> 00:33:10,530
So my data scientist trained
models on something one

577
00:33:10,530 --> 00:33:15,270
and eventually they're machine
learning based application

578
00:33:15,270 --> 00:33:17,640
doesn't work as expected with real data

579
00:33:17,640 --> 00:33:18,960
used in a different application.

580
00:33:18,960 --> 00:33:21,690
So this is what platforms knows to solve.

581
00:33:21,690 --> 00:33:25,350
Well by again, having that
one platform, one data set

582
00:33:25,350 --> 00:33:29,910
for so many use cases, analytics,
BI, operational use cases,

583
00:33:29,910 --> 00:33:32,400
machine learning, SageMaker feature store

584
00:33:32,400 --> 00:33:34,470
pulling straight from the lake,

585
00:33:34,470 --> 00:33:36,600
sometimes external sharing for integration

586
00:33:36,600 --> 00:33:38,670
with other platforms like Redshift,

587
00:33:38,670 --> 00:33:40,770
data sharing could just S3.

588
00:33:40,770 --> 00:33:45,630
As a result, all those teams
work on the very same governed

589
00:33:45,630 --> 00:33:50,308
data set without again
worrying that the user

590
00:33:50,308 --> 00:33:54,810
something different, something wrong.

591
00:33:54,810 --> 00:33:59,340
And so building this
platform with plethora

592
00:33:59,340 --> 00:34:03,960
of those AWS services is
call it table stakes now,

593
00:34:03,960 --> 00:34:07,446
but how do you make sure you
can actually operate at scale?

594
00:34:07,446 --> 00:34:10,620
Because you could see
there are so many services

595
00:34:10,620 --> 00:34:13,098
and I was saying, well guess,

596
00:34:13,098 --> 00:34:14,760
we need to take care about the governance,

597
00:34:14,760 --> 00:34:17,490
make sure that all our
data, all our resources

598
00:34:17,490 --> 00:34:20,370
are tagged properly for right economics,

599
00:34:20,370 --> 00:34:22,350
for right ownership identification.

600
00:34:22,350 --> 00:34:24,870
We need to be flexible to
add new ingestion patterns,

601
00:34:24,870 --> 00:34:28,530
new consumption patterns to we
locate like a hundred people

602
00:34:28,530 --> 00:34:30,630
to paint in the toll?

603
00:34:30,630 --> 00:34:31,500
No we do not.

604
00:34:31,500 --> 00:34:36,500
All that drills down to very
well defined MLOps and DevOps

605
00:34:36,600 --> 00:34:40,713
system behind the scenes
that automates that all

606
00:34:42,780 --> 00:34:45,697
running it on autopilot,
repeatable, governed

607
00:34:45,697 --> 00:34:47,970
and self-healing.

608
00:34:47,970 --> 00:34:50,550
There are no tickets, there
is no co console click,

609
00:34:50,550 --> 00:34:53,040
no exceptions, no heroics.

610
00:34:53,040 --> 00:34:56,580
Otherwise we just wouldn't
be able to scale it for

611
00:34:56,580 --> 00:35:00,420
so many consumptions, for so
many resources and use cases.

612
00:35:00,420 --> 00:35:04,080
The moment the new data
lands in the S3 bucket,

613
00:35:04,080 --> 00:35:06,660
in the ingestion on your left,

614
00:35:06,660 --> 00:35:10,530
all the downstream is already
pre-provisioned the events,

615
00:35:10,530 --> 00:35:15,090
the ETL pipelines, the
machine learning models,

616
00:35:15,090 --> 00:35:17,397
which you expecting data to train

617
00:35:17,397 --> 00:35:21,840
and basically runs seamlessly.

618
00:35:21,840 --> 00:35:26,840
And what we did actually,
and this is a good pattern

619
00:35:28,053 --> 00:35:32,016
that I recommend to you, I
want to note specifically,

620
00:35:32,016 --> 00:35:36,240
initially we started with standard DevOps

621
00:35:36,240 --> 00:35:38,610
source code first of course
infrastructure is code

622
00:35:38,610 --> 00:35:41,490
and you provision your Glue, your lambda,

623
00:35:41,490 --> 00:35:44,520
your step function, all that.

624
00:35:44,520 --> 00:35:47,820
But then you realize that
the challenge still persists.

625
00:35:47,820 --> 00:35:51,240
Like, okay, so I provision
my Glue job, so what about

626
00:35:51,240 --> 00:35:53,010
that lambda that needs to trigger it,

627
00:35:53,010 --> 00:35:54,270
I need to provision as well.

628
00:35:54,270 --> 00:35:56,070
And then I need permissions from Lambda

629
00:35:56,070 --> 00:35:57,690
to Glue jobs to execute it.

630
00:35:57,690 --> 00:35:59,650
And then my Lambda needs to listen

631
00:36:00,623 --> 00:36:01,913
to an event coming from S3 bucket,

632
00:36:02,848 --> 00:36:03,890
so I need those permissions,
et cetera, et cetera.

633
00:36:03,890 --> 00:36:06,925
That takes days even
so partially automated,

634
00:36:06,925 --> 00:36:11,040
it takes a day sometimes
week for a team member

635
00:36:11,040 --> 00:36:13,680
to build all this architecture pattern.

636
00:36:13,680 --> 00:36:17,040
So with the well-defined
architecture that you saw,

637
00:36:17,040 --> 00:36:20,280
we realized that we can
identify those patterns

638
00:36:20,280 --> 00:36:23,670
and give engineers literally the blocks

639
00:36:23,670 --> 00:36:26,416
of deployment patterns
which they can reuse.

640
00:36:26,416 --> 00:36:29,850
And those blocks would
give them right away

641
00:36:29,850 --> 00:36:33,930
the event listener, the
lambda, the permissions,

642
00:36:33,930 --> 00:36:38,250
the Glue job, all the tags,
all the governance in place

643
00:36:38,250 --> 00:36:40,320
for personal information processing,

644
00:36:40,320 --> 00:36:42,150
for general information processing,

645
00:36:42,150 --> 00:36:46,200
all that as a single package
so that they come take it

646
00:36:46,200 --> 00:36:50,010
and deploy literally in
under 10 minutes instead

647
00:36:50,010 --> 00:36:55,010
of spending weeks sprints,
build all that infrastructure.

648
00:36:56,280 --> 00:36:58,560
Data scientists, they treat the model

649
00:36:58,560 --> 00:37:01,080
exactly the same as the code.

650
00:37:01,080 --> 00:37:04,020
When they merge the
changes to their model,

651
00:37:04,020 --> 00:37:06,136
it automatically gets deployed

652
00:37:06,136 --> 00:37:09,990
and using blue print deployment style

653
00:37:09,990 --> 00:37:14,340
and with the right metrics,
with the right monitoring,

654
00:37:14,340 --> 00:37:19,090
so that if there is a model drift,

655
00:37:19,090 --> 00:37:20,850
in other words model
starts to produce results

656
00:37:20,850 --> 00:37:23,040
which weren't initially expected

657
00:37:23,040 --> 00:37:26,430
or any other software
issues with that model,

658
00:37:26,430 --> 00:37:28,860
the role book happens automatically

659
00:37:28,860 --> 00:37:32,670
and they just get an event, an
email that, well guess what?

660
00:37:32,670 --> 00:37:34,920
The change that you tried to
make didn't go as expected.

661
00:37:34,920 --> 00:37:36,450
We're all back for you.

662
00:37:36,450 --> 00:37:38,430
Go to take a look tomorrow,
you don't need to go

663
00:37:38,430 --> 00:37:41,340
to pager duty alert in
the middle of the night.

664
00:37:41,340 --> 00:37:44,310
And same counts for
application API developers

665
00:37:44,310 --> 00:37:47,130
building KPIs using canard deployments

666
00:37:47,130 --> 00:37:51,316
so they can safely check how
things work in a lower volume.

667
00:37:51,316 --> 00:37:55,110
And later on, again,
having all the automation

668
00:37:55,110 --> 00:37:59,633
and metrics embedded into the
CICD pipelines deployments

669
00:37:59,633 --> 00:38:03,120
which help them identify any issues early

670
00:38:03,120 --> 00:38:05,640
before the impact customers
and automatically,

671
00:38:05,640 --> 00:38:08,130
automatically heal the pipeline,

672
00:38:08,130 --> 00:38:12,063
the changes that, sorry,
the issues which occurred.

673
00:38:13,170 --> 00:38:17,220
So this is about the
platform and we covered

674
00:38:17,220 --> 00:38:18,540
quite many areas.

675
00:38:18,540 --> 00:38:22,090
The overall structure,
we covered the specific

676
00:38:22,090 --> 00:38:26,070
implementation details with
those great AWS services

677
00:38:26,070 --> 00:38:29,070
that you all know the open data format,

678
00:38:29,070 --> 00:38:31,770
the MLOps and DevOps behind the scenes.

679
00:38:31,770 --> 00:38:36,770
But how does this work, right?

680
00:38:37,170 --> 00:38:39,150
This foundation isn't just sitting there,

681
00:38:39,150 --> 00:38:41,790
it's supporting so many
enterprise grade solutions

682
00:38:41,790 --> 00:38:43,380
in our company.

683
00:38:43,380 --> 00:38:47,796
And those include, again, 360
APIs, well not 360 of those,

684
00:38:47,796 --> 00:38:51,420
but few APIs, which
give you roundabout view

685
00:38:51,420 --> 00:38:52,800
of your data models.

686
00:38:52,800 --> 00:38:55,080
They support the data scientists,

687
00:38:55,080 --> 00:38:59,820
they support the business
analyst with static dashboards.

688
00:38:59,820 --> 00:39:03,030
But one of the most
exciting features to me,

689
00:39:03,030 --> 00:39:04,170
and well guess what?

690
00:39:04,170 --> 00:39:07,110
It's almost 2026. So how about it?

691
00:39:07,110 --> 00:39:09,197
It's the agenda applications,

692
00:39:09,197 --> 00:39:12,270
which is in my humble
opinion is the future

693
00:39:12,270 --> 00:39:15,330
of business intelligence,
which will replace

694
00:39:15,330 --> 00:39:18,780
static typical BI dashboards.

695
00:39:18,780 --> 00:39:23,550
And what I'm going to try to
do now, I'll make a quick demo

696
00:39:23,550 --> 00:39:28,550
to show how that Gentech
application works at Rocket.

697
00:39:28,830 --> 00:39:32,370
Exactly the same one that I'm demoing you

698
00:39:32,370 --> 00:39:35,920
is is being used by many of
our executive stakeholders

699
00:39:36,930 --> 00:39:40,830
except yeah, except that
the data you'll see,

700
00:39:40,830 --> 00:39:43,740
and that's a legal disclaimer,
it's synthetic data,

701
00:39:43,740 --> 00:39:47,580
but everything else is true
and valid exactly as it works.

702
00:39:47,580 --> 00:39:52,580
So let me see how we make it work here.

703
00:39:54,900 --> 00:39:57,234
Just do one, just do one click.

704
00:39:57,234 --> 00:39:58,470
Okay.

705
00:39:58,470 --> 00:40:02,149
That's, we'll be playing video and okay,

706
00:40:02,149 --> 00:40:03,573
it's actually working.

707
00:40:04,440 --> 00:40:06,026
We'll be playing video.

708
00:40:06,026 --> 00:40:09,780
I'll be posing to catch up
and tell what I'm showing.

709
00:40:09,780 --> 00:40:13,320
So what you are seeing in this
browser window on your screen

710
00:40:13,320 --> 00:40:17,253
is the system, I wouldn't
call it platform,

711
00:40:19,087 --> 00:40:22,158
it's the system we developed enabling

712
00:40:22,158 --> 00:40:27,158
not so technical users of our platform

713
00:40:27,330 --> 00:40:32,330
to quickly stand up new agents
for their business use cases.

714
00:40:32,553 --> 00:40:37,255
The interface is very intuitive.

715
00:40:37,255 --> 00:40:39,337
You don't need to be a
too technical person.

716
00:40:39,337 --> 00:40:42,810
You explain the data sources
that you want to use,

717
00:40:42,810 --> 00:40:46,110
you explain the context
that you want to get,

718
00:40:46,110 --> 00:40:47,960
the information you want to retrieve,

719
00:40:50,290 --> 00:40:52,920
and then the system allows
you to create a new bot,

720
00:40:52,920 --> 00:40:55,200
a new application which we'll talk

721
00:40:55,200 --> 00:40:58,530
and with the very same
data set behind the scenes

722
00:40:58,530 --> 00:41:00,300
and produce for you the results.

723
00:41:00,300 --> 00:41:03,395
Like I'm going to share with you.

724
00:41:03,395 --> 00:41:06,990
One note probably all of you are familiar

725
00:41:06,990 --> 00:41:10,893
with the ChatGPT and Gronk
and many similar systems.

726
00:41:13,643 --> 00:41:17,160
It's not the same, it's
not the same as the chatbot

727
00:41:17,160 --> 00:41:19,200
that helps to polish your text

728
00:41:19,200 --> 00:41:23,790
or some fine general
information on the internet

729
00:41:23,790 --> 00:41:27,360
or elsewhere and telling you,
"Well please verify the data

730
00:41:27,360 --> 00:41:29,820
because you know, it's my best effort."

731
00:41:29,820 --> 00:41:32,640
In this case, all the data is sourced

732
00:41:32,640 --> 00:41:37,640
from the same data platform,
which we've just showed.

733
00:41:37,800 --> 00:41:41,460
So the APIs, the BI
dashboards, and these agents

734
00:41:41,460 --> 00:41:43,980
get it from the very same data source

735
00:41:43,980 --> 00:41:47,190
and you can rely on the
data as the true source

736
00:41:47,190 --> 00:41:49,555
of truth for your analytics and for making

737
00:41:49,555 --> 00:41:51,855
executive decisions.

738
00:41:51,855 --> 00:41:54,843
So let's continue the playback.

739
00:41:56,520 --> 00:42:01,520
Well, in this case, we'll
ask a very simple question.

740
00:42:02,429 --> 00:42:06,810
It is just so little on the
screen, hardly can see it.

741
00:42:06,810 --> 00:42:07,950
We'll ask a simple question.

742
00:42:07,950 --> 00:42:12,950
So what is the average
mortgage amount right now?

743
00:42:12,990 --> 00:42:17,990
And the agent will go and
the agent will go on behind

744
00:42:23,040 --> 00:42:25,530
the scenes, will will
fetch this information.

745
00:42:25,530 --> 00:42:28,320
We'll fetch you the data
in a matter of seconds.

746
00:42:28,320 --> 00:42:31,140
You don't need to think how
you run those SQL queries.

747
00:42:31,140 --> 00:42:33,120
It's all very much interactive.

748
00:42:33,120 --> 00:42:38,073
And the average loan amount
in September was $288,000.

749
00:42:40,110 --> 00:42:44,820
And if you're very curious
how the agent receive the data

750
00:42:44,820 --> 00:42:47,520
or you just want to get
some technical insights,

751
00:42:47,520 --> 00:42:50,670
you go to the SQL tab and it
shows you exactly what data

752
00:42:50,670 --> 00:42:54,603
was made to our analytical
data stores to fetch the data.

753
00:42:55,530 --> 00:42:58,920
Okay, but what if I
want to split this data

754
00:42:58,920 --> 00:43:01,590
and reorganize it by state?

755
00:43:01,590 --> 00:43:05,190
And if in in the old
world, I would need to go

756
00:43:05,190 --> 00:43:07,680
and think how do I build my queries,

757
00:43:07,680 --> 00:43:11,550
how to test multiple times,
then how do I visualize that?

758
00:43:11,550 --> 00:43:13,980
And here you go, using
some natural language,

759
00:43:13,980 --> 00:43:16,560
you quickly get the table
of all the low average,

760
00:43:16,560 --> 00:43:19,950
low on amounts per state,
the number of those loans,

761
00:43:19,950 --> 00:43:20,783
if you wish.

762
00:43:20,783 --> 00:43:24,900
You can visualize this data
in a variety of graphs,

763
00:43:24,900 --> 00:43:28,410
the pie, the line charts depending.

764
00:43:28,410 --> 00:43:30,690
You can embed this data
in any of your reports

765
00:43:30,690 --> 00:43:34,110
and obviously also have that SQL tab

766
00:43:34,110 --> 00:43:38,730
to see how it works.

767
00:43:38,730 --> 00:43:41,430
Well, but what if I want
a bit more analysis?

768
00:43:41,430 --> 00:43:42,960
Tell me what it needs.

769
00:43:42,960 --> 00:43:46,140
So I can ask agent, well
analyze this data for me

770
00:43:46,140 --> 00:43:49,920
and the agent will right
away give you key insights.

771
00:43:49,920 --> 00:43:53,700
They'll give you the premium markets

772
00:43:53,700 --> 00:43:56,070
where people take most mortgages.

773
00:43:56,070 --> 00:43:58,590
They give you the regional trends,

774
00:43:58,590 --> 00:44:01,260
all kinds of information
that can be valuable

775
00:44:01,260 --> 00:44:02,910
for you if you are a business analyst

776
00:44:02,910 --> 00:44:07,230
or just a team member
associate generating a report

777
00:44:07,230 --> 00:44:09,810
about the last week, the last month, which

778
00:44:09,810 --> 00:44:13,053
by the way can also be
automated using this platform.

779
00:44:13,053 --> 00:44:16,290
And what if you want to
cross reference those loans

780
00:44:16,290 --> 00:44:19,535
with the interest rates in ease, right?

781
00:44:19,535 --> 00:44:22,590
So you don't need to
think how you write query,

782
00:44:22,590 --> 00:44:26,091
how you test query, get right
away all the information

783
00:44:26,091 --> 00:44:30,120
and the agent spits it out in seconds.

784
00:44:30,120 --> 00:44:32,850
Again, you can verify
the query it provides,

785
00:44:32,850 --> 00:44:37,850
you can filter, you can
visualize in different graphs

786
00:44:38,340 --> 00:44:41,160
how again, those loans
amounts, the numbers

787
00:44:41,160 --> 00:44:44,850
and the interest rates all
correlate to each other.

788
00:44:44,850 --> 00:44:47,670
I can go ahead and ask to analyze deeper

789
00:44:47,670 --> 00:44:51,450
all this information and
at the end of this analysis

790
00:44:51,450 --> 00:44:56,100
I'll get a full fledged
report for my mortgages

791
00:44:56,100 --> 00:44:58,650
and for the interest rates in September,

792
00:44:58,650 --> 00:45:02,455
I can learn from that
report about the states

793
00:45:02,455 --> 00:45:06,060
with high mortgages where
people take high mortgages

794
00:45:06,060 --> 00:45:08,988
and how it affects the interest rates.

795
00:45:08,988 --> 00:45:11,610
The agent will tell you, well
the interest rates are low

796
00:45:11,610 --> 00:45:15,000
in those states and the
opposite happens in those states

797
00:45:15,000 --> 00:45:19,290
where the mortgages are
the loans bit smallest.

798
00:45:19,290 --> 00:45:22,260
So all that gives you
that kind of interaction,

799
00:45:22,260 --> 00:45:25,080
which you can again just grab this report,

800
00:45:25,080 --> 00:45:27,660
sharing your newsletter,
sharing your presentation,

801
00:45:27,660 --> 00:45:30,360
sharing in your narrative updating

802
00:45:30,360 --> 00:45:33,030
your business stakeholders
and also the insights,

803
00:45:33,030 --> 00:45:36,390
the insights based on
the very long experience

804
00:45:36,390 --> 00:45:41,390
of how a company, how these
trends affect the trends

805
00:45:44,097 --> 00:45:45,810
and affect the company.

806
00:45:45,810 --> 00:45:50,400
So again, all that is
not some kind of a story

807
00:45:50,400 --> 00:45:51,480
or fairy tale I'm sharing.

808
00:45:51,480 --> 00:45:54,333
It's not a POC running on this laptop,

809
00:45:54,333 --> 00:45:56,220
it's actually running in production today.

810
00:45:56,220 --> 00:45:59,280
Actually tens of agents like
this running in production

811
00:45:59,280 --> 00:46:02,310
based on the very same data platform

812
00:46:02,310 --> 00:46:04,890
that I presented to
you guys and helping us

813
00:46:04,890 --> 00:46:07,833
to drive the business efficiently.

814
00:46:09,510 --> 00:46:13,695
With saying that I'll pause
the microphone back to Sijan

815
00:46:13,695 --> 00:46:16,500
to our senior architect and to wrap up

816
00:46:16,500 --> 00:46:19,050
and tell more about some of those services

817
00:46:19,050 --> 00:46:21,240
behind the scenes that drive the toll.

818
00:46:21,240 --> 00:46:22,073
Thank you guys.

819
00:46:23,152 --> 00:46:24,330
(audience claps)

820
00:46:24,330 --> 00:46:25,630
- Awesome. Thank you Ilia.

821
00:46:29,680 --> 00:46:32,940
Okay, so far what you
heard about from Rocket is

822
00:46:32,940 --> 00:46:35,140
how they have transformed
their organization

823
00:46:36,449 --> 00:46:37,740
to be ready for the air readiness

824
00:46:37,740 --> 00:46:40,230
and as the executors at Rocket had said

825
00:46:40,230 --> 00:46:44,370
AI fuel home ownership and how
the foundation has been built

826
00:46:44,370 --> 00:46:46,440
to power the vision that executives

827
00:46:46,440 --> 00:46:49,110
has been set for our organization to take.

828
00:46:49,110 --> 00:46:51,300
So let's see how they have got it there.

829
00:46:51,300 --> 00:46:54,630
So we always see a three patterns at AWS

830
00:46:54,630 --> 00:46:57,780
that most of the successful
enterprises have followed.

831
00:46:57,780 --> 00:47:01,950
So, and the first one is aggregate.

832
00:47:01,950 --> 00:47:05,730
So the principle here is
you have to unify your data

833
00:47:05,730 --> 00:47:07,890
from the different data sources.

834
00:47:07,890 --> 00:47:10,700
So either it is
structured, semi-structured

835
00:47:10,700 --> 00:47:12,330
or unstructured data.

836
00:47:12,330 --> 00:47:15,450
Bring all the data together
into a single place

837
00:47:15,450 --> 00:47:18,429
and choose one standardized
format as an open table format.

838
00:47:18,429 --> 00:47:22,890
Here in this case Rocket
was using an Apache iceberg

839
00:47:22,890 --> 00:47:25,353
on an Amazon S3 tables.

840
00:47:26,880 --> 00:47:29,220
So what happens when you do an aggregation

841
00:47:29,220 --> 00:47:33,120
or unify your data, it
removes the fragmentation

842
00:47:33,120 --> 00:47:36,270
and it'll help accelerate
your downstream systems

843
00:47:36,270 --> 00:47:37,533
and downstream teams.

844
00:47:38,760 --> 00:47:40,743
What is the second one? Curate.

845
00:47:41,820 --> 00:47:45,570
So once you unify the data,
as Garima rightly said,

846
00:47:45,570 --> 00:47:49,800
instead of multiple teams
billing the same data

847
00:47:49,800 --> 00:47:53,730
again and again you
became the data products

848
00:47:53,730 --> 00:47:57,030
like curate the data into the dimensions

849
00:47:57,030 --> 00:47:59,130
or the dissect the data, unified data,

850
00:47:59,130 --> 00:48:01,710
into the right dimension
that can be helpful

851
00:48:01,710 --> 00:48:03,960
for your teams to take advantage of

852
00:48:03,960 --> 00:48:06,300
rather than the rebuilding it.

853
00:48:06,300 --> 00:48:09,570
So, the second one is curate.

854
00:48:09,570 --> 00:48:11,653
So, and which services
can be helpful in having

855
00:48:11,653 --> 00:48:16,410
curation stage, Amazon, EMR, Glue catalog

856
00:48:16,410 --> 00:48:17,460
and the data zones.

857
00:48:17,460 --> 00:48:19,920
You can get all these different services.

858
00:48:19,920 --> 00:48:22,860
I'm just naming few, but
you can build your systems

859
00:48:22,860 --> 00:48:25,230
into modern the curation systems.

860
00:48:25,230 --> 00:48:28,440
So what happens when you would love to go

861
00:48:28,440 --> 00:48:30,420
one more step ahead.

862
00:48:30,420 --> 00:48:33,540
Hey, I don't want just curate
and have that data lake,

863
00:48:33,540 --> 00:48:36,330
one more other table within our data lab

864
00:48:36,330 --> 00:48:38,670
or the data warehouse or the data lake.

865
00:48:38,670 --> 00:48:40,470
Like why should you do that?

866
00:48:40,470 --> 00:48:43,500
Okay, let's have the
data sits into curation

867
00:48:43,500 --> 00:48:46,020
into one of the data formats because

868
00:48:46,020 --> 00:48:49,740
as Garima said about the
data products Multi 360,

869
00:48:49,740 --> 00:48:52,860
Customer 360 and the Transaction 360.

870
00:48:52,860 --> 00:48:56,400
So once you have these
data products curated,

871
00:48:56,400 --> 00:49:01,400
make them accessible via
APIs so that it's readily,

872
00:49:01,690 --> 00:49:04,590
easily consumable for all
your downstream systems,

873
00:49:04,590 --> 00:49:07,770
even the agents demo that Ilia showcased

874
00:49:07,770 --> 00:49:10,680
because those APIs are readily available,

875
00:49:10,680 --> 00:49:13,930
you can accelerate your agentic systems

876
00:49:14,880 --> 00:49:15,713
in your organization.

877
00:49:15,713 --> 00:49:18,183
How many of you here are
building AI agents right now?

878
00:49:19,440 --> 00:49:20,610
Okay, good.

879
00:49:20,610 --> 00:49:21,870
So if you want to accelerate,

880
00:49:21,870 --> 00:49:24,510
you need to have this
data foundation ready

881
00:49:24,510 --> 00:49:27,753
and have your data products
as an extended APIs.

882
00:49:29,010 --> 00:49:30,690
These are our LinkedIn profile QR code.

883
00:49:30,690 --> 00:49:32,940
If you're allowed to
scan, please free to scan.

884
00:49:32,940 --> 00:49:36,031
If there is any follow-up
question that you would like

885
00:49:36,031 --> 00:49:38,280
to have after this, we are happy to help.

886
00:49:38,280 --> 00:49:40,800
And once again, thank
you for choosing to spend

887
00:49:40,800 --> 00:49:43,653
first hour with us and happy re:Invent.

