# AWS re:Invent 2025 会议总结：优化生成式 AI 工作负载的可持续性与成本

## 会议概述

本次分会由 AWS 高级解决方案架构师 Dwa 主讲，重点探讨了生成式 AI 和代理式 AI 带来的环境挑战以及优化策略。演讲者指出，尽管人工智能承诺解决人类面临的重大挑战，但它也带来了严重的环境危机。自 2021 年生成式 AI 爆发以来，数据中心的能源消耗急剧增加，模型训练规模在过去 10 年中增长了约 35 万倍。

根据高盛研究院 2025 年 8 月的预测，数据中心电力需求增长的约 60% 将通过燃烧化石燃料来满足，这将向大气中排放约 2.15 至 2.2 亿吨二氧化碳。世界经济论坛 2024 年的文章也指出，这些工作负载的计算能力每 100 天翻一番。面对这些严峻的数据，演讲者强调开发者、架构师和超大规模云服务提供商必须采取负责任的创新方式，在系统中构建干预措施以减轻不断膨胀的碳足迹。

演讲者介绍了 AWS 在可持续性方面的承诺，包括 2019 年签署的气候承诺，并在 2023 年实现了 100% 可再生能源运营的目标。在 AWS 上构建生成式 AI 工作负载比本地部署节能 4.1 倍，可减少高达 90% 的碳足迹。这得益于 AWS 在可再生能源、硬件效率、托管服务优化、冷却效率、低碳混凝土以及针对模型训练和推理优化的芯片等方面的投入。

## 详细时间线与关键要点

[00:00 - 02:30] 开场与问题背景
- 演讲者 Dwa 自我介绍，在 AWS 担任高级解决方案架构师约 6 年
- 指出过去两天大会主要讨论代理式 AI 和生成式 AI
- 强调 AI 在解决人类挑战的同时带来了严重的环境危机

[02:30 - 05:00] 数据中心能源消耗现状
- 过去十年数据中心电力需求保持在约 100 太瓦时的稳定水平
- 2021 年生成式 AI 爆发后，能源消耗急剧增加
- 模型训练规模在过去 10 年增长了约 35 万倍

[05:00 - 07:30] 环境影响数据
- 高盛研究院预测：60% 的数据中心电力需求增长将通过化石燃料满足
- 预计排放 2.15-2.2 亿吨二氧化碳
- 对比：驾驶燃油车行驶 5000 英里产生 1 吨二氧化碳
- 世界经济论坛 2024 年报告：计算能力每 100 天翻一番

[07:30 - 10:00] AWS 可持续性承诺
- 2019 年签署气候承诺
- 2023 年实现 100% 可再生能源运营目标
- AWS 比本地部署节能 4.1 倍
- 可减少高达 90% 的碳足迹

[10:00 - 12:00] AWS 能效优势来源
- 100% 可再生能源运营
- 硬件效率优化
- 托管服务的可持续性优化
- 蒸发冷却技术
- 使用低碳混凝土建设数据中心
- 针对模型训练和推理优化的芯片

[12:00 - 14:30] 建议一：使用托管服务
- 托管服务将可持续性优化责任转移给 AWS
- 推荐服务：Amazon Bedrock（200+ 基础模型的无服务器访问）
- SageMaker 用于模型训练
- SageMaker HyperPod（支持 Slurm 编排）
- EKS（Kubernetes 编排）
- Bedrock Agents（构建代理系统）

[14:30 - 17:30] 建议二：模型选择策略
- 需要考虑的问题：业务成果、开源 vs 专有、语言需求、通用 vs 领域特定
- 使用 Bedrock Evaluations 评估和比较模型
- 三种评估技术：LLM 作为评判者、传统指标（BLEU、F1 分数）、人工评估
- 关键原则：不需要最大最亮的模型，较小模型有时就足够

[17:30 - 19:00] 模型大小对比示例
- ChatGPT 3.5：1750 亿参数
- Alpaca 模型：70 亿参数
- 两者行为质量相似，因为 Alpaca 是 ChatGPT 3.5 的蒸馏版本
- 更大的模型意味着更多资源、更高成本、更大碳足迹

[19:00 - 23:00] 建议三：模型定制技术层次
- 按能源消耗递增顺序：
  1. 提示工程（最低成本和碳足迹）
  2. 检索增强生成（RAG）- 添加专有信息和内部文档
  3. 参数高效微调（PEFT）- 如 LoRA、前缀调优、P-tuning
  4. 完全微调
  5. 持续预训练
  6. 从头训练（最高成本和碳排放，仅在其他方法都失败时使用）

[23:00 - 26:00] 建议四：选择正确的芯片
- 训练芯片：
  - Trainium 1：比同类 EC2 实例节能 25%
  - Trainium 2：节能 3 倍
  - Trainium 3：即将推出，预计更节能
- 推理芯片：
  - Inferentia 实例：每瓦性能提升 50%
  - Graviton 实例（用于小型模型）：比同类 EC2 实例节能 60%
  - Graviton 自 2018 年推出以来性能提升 4 倍

[26:00 - 29:00] 建议五：推理优化技术
- 可用的库：DeepSpeed、Hugging Face Accelerate、Faster Transformer
- 优化技术：
  - 模型压缩
  - 分布式处理优化
  - 剪枝不必要的权重
  - 模型蒸馏（将大模型知识转移到小模型，精度损失很小）
  - 量化（尝试不同精度类型）
  - 针对特定硬件优化模型

[29:00 - 32:00] 建议六：全生命周期可观测性
- 在整个生命周期的每个阶段构建可观测性
- 持续监控和优化已部署的模型
- 监控内容：
  - 数据漂移和模型漂移
  - 输出在真实场景中的相关性
  - 潜在危害、偏见和有害内容
- 构建负责任的 AI 实践

[32:00 - 34:00] 监控工具介绍
- CloudWatch：监控 CPU、内存、磁盘和其他指标
- SageMaker Profiler 和 Neuron Monitor：
  - 适用于模型训练
  - 提供数据分布、训练指标、损失和准确性信息
  - 深入了解训练黑盒
- NVIDIA 系统管理接口（SMI）：
  - 用于 NVIDIA 实例（如 P 系列）
  - 监控 GPU 使用率和其他指标

[34:00 - 36:00] 总结与关键要点回顾
- 使用托管服务
- 根据业务成果和用例选择基础模型
- 按顺序应用模型定制技术（提示工程 → RAG → PEFT → 完全微调 → 从头训练）
- 应用推理优化技术（剪枝、蒸馏、量化）
- 选择节能芯片（Trainium、Inferentia、Graviton）
- 在每个阶段构建可观测性并持续改进

[36:00 - 37:00] 结束语
- 提供相关资源链接
- 包括生成式 AI 生命周期各阶段的优化指南
- AWS 上 MLOps 生态系统的可持续性优化指南
- 开放问答环节