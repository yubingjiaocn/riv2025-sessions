1
00:00:00,480 --> 00:00:01,829
- Thank you. Hi.

2
00:00:01,829 --> 00:00:02,700
(audience applauds)

3
00:00:02,700 --> 00:00:05,430
Hi everyone. My name's Ammar.

4
00:00:05,430 --> 00:00:09,270
I'm an engineer at Netflix
on the data platform team,

5
00:00:09,270 --> 00:00:12,360
and today we're gonna talk
about how we moved dozens

6
00:00:12,360 --> 00:00:13,710
of databases that were on

7
00:00:13,710 --> 00:00:18,710
a third party Postgres-compatible
distributed database

8
00:00:18,960 --> 00:00:21,963
to Amazon Aurora Postgres.

9
00:00:22,980 --> 00:00:25,440
We'll start by talking
a bit about the problem,

10
00:00:25,440 --> 00:00:28,290
like what we actually did and why.

11
00:00:28,290 --> 00:00:30,900
Then we'll spend some time
talking about how we did it.

12
00:00:30,900 --> 00:00:32,640
And I'll leave you with some, you know,

13
00:00:32,640 --> 00:00:33,750
where we are right now

14
00:00:33,750 --> 00:00:36,660
and what learnings we have so far.

15
00:00:36,660 --> 00:00:39,420
There's a ton of content
and not as much time.

16
00:00:39,420 --> 00:00:41,280
So I'm gonna blast through a lot of this,

17
00:00:41,280 --> 00:00:43,170
also because I'm slightly
over caffeinated,

18
00:00:43,170 --> 00:00:45,390
but fine at the end if you have questions

19
00:00:45,390 --> 00:00:47,990
or I can explain things a
little bit more in detail.

20
00:00:49,170 --> 00:00:50,659
So I'm on a platform team.

21
00:00:50,659 --> 00:00:53,760
We work with a whole
bunch of application teams

22
00:00:53,760 --> 00:00:55,470
and these application teams that,

23
00:00:55,470 --> 00:00:56,550
you know, run the business.

24
00:00:56,550 --> 00:00:58,470
They like work on the, "Stranger Things,"

25
00:00:58,470 --> 00:01:01,470
season five, you know, production

26
00:01:01,470 --> 00:01:02,629
and a whole bunch of other things.

27
00:01:02,629 --> 00:01:04,620
No spoilers please.

28
00:01:04,620 --> 00:01:08,640
And a lot of these teams
like run, you know,

29
00:01:08,640 --> 00:01:10,149
they use our databases

30
00:01:10,149 --> 00:01:14,970
and particularly a lot of them ran on

31
00:01:14,970 --> 00:01:16,629
this third party distributed

32
00:01:16,629 --> 00:01:18,780
Postgres-compatible data store.

33
00:01:18,780 --> 00:01:19,920
It's, you know, self-manage,

34
00:01:19,920 --> 00:01:23,400
which really means our team
manages it or managed it.

35
00:01:23,400 --> 00:01:26,070
And you know, that kinda
comes with its straight offs.

36
00:01:26,070 --> 00:01:27,090
It's, you know, self-managed.

37
00:01:27,090 --> 00:01:29,220
So we do a lot of work on managing it,

38
00:01:29,220 --> 00:01:30,990
upgrading and those sorts of things.

39
00:01:30,990 --> 00:01:32,970
And instance replacements.

40
00:01:32,970 --> 00:01:35,550
It's Postgres-compatible,
which means, you know,

41
00:01:35,550 --> 00:01:38,010
it's usually Postgres, but until it's not.

42
00:01:38,010 --> 00:01:40,680
And so, you know, we found
that those trade offs

43
00:01:40,680 --> 00:01:43,110
plus it can get kind of
pricey running such a powerful

44
00:01:43,110 --> 00:01:44,730
database across the entire fleet.

45
00:01:44,730 --> 00:01:46,950
And in most cases you don't need it.

46
00:01:46,950 --> 00:01:48,360
And in the past few years,

47
00:01:48,360 --> 00:01:51,180
especially Aurora Postgres
has come a long way in terms

48
00:01:51,180 --> 00:01:54,900
of feature set, reliability
and even, you know, cost.

49
00:01:54,900 --> 00:01:58,050
And so we made the decision
as the central platform teams

50
00:01:58,050 --> 00:02:01,680
to move all these databases
over to Aurora Postgres.

51
00:02:01,680 --> 00:02:03,173
And it's important to note that, you know,

52
00:02:03,173 --> 00:02:05,280
this is the platform
team making a decision

53
00:02:05,280 --> 00:02:07,140
for all application teams.

54
00:02:07,140 --> 00:02:09,420
And so the more work we
can do, the better it is,

55
00:02:09,420 --> 00:02:11,400
because we're asking teams to move.

56
00:02:11,400 --> 00:02:14,810
Additionally, it's better for
the company if we take as much

57
00:02:14,810 --> 00:02:16,616
of the cycles and execute them ourselves

58
00:02:16,616 --> 00:02:18,543
and not make the app teams do it.

59
00:02:19,740 --> 00:02:21,660
And so we're gonna go
over like how it works.

60
00:02:21,660 --> 00:02:23,550
But you know, in general,
database migrations,

61
00:02:23,550 --> 00:02:24,720
there's solved problem.

62
00:02:24,720 --> 00:02:25,800
You know, you copy the schema

63
00:02:25,800 --> 00:02:28,950
and the data, you validate
it, make sure it's all there.

64
00:02:28,950 --> 00:02:30,720
And then all you have to do in theory

65
00:02:30,720 --> 00:02:33,240
is to point the application
to new database.

66
00:02:33,240 --> 00:02:35,340
And AWS has tooling to help with this.

67
00:02:35,340 --> 00:02:37,800
There's something called the
database migration service

68
00:02:37,800 --> 00:02:39,900
that we used as a building block.

69
00:02:39,900 --> 00:02:43,290
And it can do, you know,
schema conversion and copy.

70
00:02:43,290 --> 00:02:45,690
It can migrate data, it
can even validate the data.

71
00:02:45,690 --> 00:02:47,520
And then again, in theory, all you have

72
00:02:47,520 --> 00:02:48,782
to do is point the application.

73
00:02:48,782 --> 00:02:52,860
However, when it comes to
the real world, things start

74
00:02:52,860 --> 00:02:54,570
to get a bit more tricky.

75
00:02:54,570 --> 00:02:57,230
So like in our case, we had
over a hundred applications

76
00:02:57,230 --> 00:02:58,590
of different data sizes.

77
00:02:58,590 --> 00:02:59,790
And each application,

78
00:02:59,790 --> 00:03:02,640
at least from the database
perspective, is very different.

79
00:03:02,640 --> 00:03:04,980
The data shape is very different.

80
00:03:04,980 --> 00:03:06,450
The data access patterns are different.

81
00:03:06,450 --> 00:03:09,720
And like if you do this naively

82
00:03:09,720 --> 00:03:11,760
you can run into different
issues per database

83
00:03:11,760 --> 00:03:12,593
and you don't wanna do that.

84
00:03:12,593 --> 00:03:15,390
You wanna catch issues
early as much as you can.

85
00:03:15,390 --> 00:03:18,240
And these applications are
written in different languages.

86
00:03:18,240 --> 00:03:19,830
We can't just build something for Java

87
00:03:19,830 --> 00:03:22,410
and like ignore all the
other use cases out there.

88
00:03:22,410 --> 00:03:24,480
You have to make sure you build
something, at least for us,

89
00:03:24,480 --> 00:03:25,655
for all the languages,

90
00:03:25,655 --> 00:03:28,710
that'll work in all languages instead of-

91
00:03:28,710 --> 00:03:29,970
And you don't wanna like build something

92
00:03:29,970 --> 00:03:31,320
individually in each language.

93
00:03:31,320 --> 00:03:32,610
You wanna find a way to support all

94
00:03:32,610 --> 00:03:34,440
these use cases together.

95
00:03:34,440 --> 00:03:36,330
Additionally, these different applications

96
00:03:36,330 --> 00:03:38,190
have different downtime requirements.

97
00:03:38,190 --> 00:03:40,170
Some are okay with being
down for, you know,

98
00:03:40,170 --> 00:03:41,340
four hours in a weekend.

99
00:03:41,340 --> 00:03:42,300
They're cool with that.

100
00:03:42,300 --> 00:03:44,280
Others are like, oh, we
can't support more than,

101
00:03:44,280 --> 00:03:45,660
you know, two minutes of downtime

102
00:03:45,660 --> 00:03:46,980
or 30 seconds of downtime.

103
00:03:46,980 --> 00:03:48,180
So it gets kinda tricky.

104
00:03:49,050 --> 00:03:51,660
And these applications also
don't exist in a vacuum.

105
00:03:51,660 --> 00:03:54,390
For every application database is often,

106
00:03:54,390 --> 00:03:55,950
you know, streaming connectors,

107
00:03:55,950 --> 00:03:57,840
there's analytics connectors.

108
00:03:57,840 --> 00:03:59,310
And again, if you did this naively,

109
00:03:59,310 --> 00:04:00,810
we'd migrate the database

110
00:04:00,810 --> 00:04:02,250
and then say, "Hey app
team, you're on your own

111
00:04:02,250 --> 00:04:04,710
to migrate your like iceberg exports

112
00:04:04,710 --> 00:04:05,970
or like Flink jobs and things."

113
00:04:05,970 --> 00:04:07,620
And we have a lot of this in Netflix.

114
00:04:07,620 --> 00:04:09,450
So you have a whole data platform

115
00:04:09,450 --> 00:04:10,950
of all these pieces that you can just plug

116
00:04:10,950 --> 00:04:12,063
into your database.

117
00:04:13,038 --> 00:04:14,553
And so that's where it
gets kind of tricky.

118
00:04:14,553 --> 00:04:18,060
And so now we've talked
enough I think about like

119
00:04:18,060 --> 00:04:19,530
what we try to solve and why.

120
00:04:19,530 --> 00:04:21,360
Let's talk about how we solved it.

121
00:04:21,360 --> 00:04:25,110
And we're gonna go over this
in sort of three sections.

122
00:04:25,110 --> 00:04:26,790
First we'll talk about what we did

123
00:04:26,790 --> 00:04:28,590
before even involving app teams.

124
00:04:28,590 --> 00:04:31,230
Like what sorts of pre-work we can do

125
00:04:31,230 --> 00:04:33,720
before talking to this team

126
00:04:33,720 --> 00:04:35,580
so we can minimize the back and forth.

127
00:04:35,580 --> 00:04:37,830
We'll talk about how we
copy the schema and the data

128
00:04:37,830 --> 00:04:41,010
and you know, spoiler alert,
we leaned heavily on DMS

129
00:04:41,010 --> 00:04:43,320
for this with some additional
tooling on our end.

130
00:04:43,320 --> 00:04:45,480
And then we'll talk about
how we validated the

131
00:04:45,480 --> 00:04:46,560
data as you know, all there.

132
00:04:46,560 --> 00:04:49,530
And then did, you know,
a liveish cut-over,

133
00:04:49,530 --> 00:04:52,500
live to the point where,
you know, reads continue

134
00:04:52,500 --> 00:04:54,600
to work throughout the entire process.

135
00:04:54,600 --> 00:04:56,943
Writes is what we took
down for a few minutes.

136
00:04:57,990 --> 00:05:00,510
So let's jump right into
the pre-flight checks.

137
00:05:00,510 --> 00:05:03,660
So this is something we did
before even involving app teams.

138
00:05:03,660 --> 00:05:05,370
And we did this across the
entire fleet, you know,

139
00:05:05,370 --> 00:05:06,507
hundreds of databases.

140
00:05:06,507 --> 00:05:07,830
We did this upfront just

141
00:05:07,830 --> 00:05:09,810
so we can minimize it back and forth.

142
00:05:09,810 --> 00:05:11,940
We used AWS DMS tooling.

143
00:05:11,940 --> 00:05:14,790
They have this thing called
a schema conversion tool.

144
00:05:14,790 --> 00:05:17,580
And you can run across on a database

145
00:05:17,580 --> 00:05:20,427
and say, "Hey, generate a
report, like will this work

146
00:05:20,427 --> 00:05:23,580
with this as a source
and this as a target."

147
00:05:23,580 --> 00:05:25,976
So our target obviously
is Aurora Postgres.

148
00:05:25,976 --> 00:05:29,550
AWS actually did not support
the source that we use,

149
00:05:29,550 --> 00:05:31,860
'cause it's a third
party piece of software.

150
00:05:31,860 --> 00:05:33,903
And so we work with
them to build in support

151
00:05:33,903 --> 00:05:35,760
so we could run this migration.

152
00:05:35,760 --> 00:05:37,620
So this generated a report with, you know,

153
00:05:37,620 --> 00:05:41,160
this index is not gonna
work in Aurora Postgres

154
00:05:41,160 --> 00:05:45,090
or you know, these hidden
columns become visible columns.

155
00:05:45,090 --> 00:05:46,260
And so we were able to take these

156
00:05:46,260 --> 00:05:47,670
and work with the app teams to make sure

157
00:05:47,670 --> 00:05:50,607
their schema would work
on the source and target.

158
00:05:50,607 --> 00:05:52,318
And so that was pretty helpful.

159
00:05:52,318 --> 00:05:55,980
The second piece is, so now
that you have the schema done,

160
00:05:55,980 --> 00:05:57,480
you have to worry about the data.

161
00:05:57,480 --> 00:05:58,800
So application teams, you know,

162
00:05:58,800 --> 00:06:01,740
are executing SQL
statements against all of

163
00:06:01,740 --> 00:06:04,050
their like third party databases.

164
00:06:04,050 --> 00:06:07,020
We were able to take those, sample them

165
00:06:07,020 --> 00:06:09,217
and again, without app team involvement

166
00:06:09,217 --> 00:06:12,450
and run them against Postgres
basically to sort of see

167
00:06:12,450 --> 00:06:13,800
what this actually work.

168
00:06:13,800 --> 00:06:15,769
And so this gave us some confidence that

169
00:06:15,769 --> 00:06:17,430
the schema will work

170
00:06:17,430 --> 00:06:20,550
and the sort of schema
changes will work as well.

171
00:06:20,550 --> 00:06:23,100
We also did the work of
looking at their source cluster

172
00:06:23,100 --> 00:06:26,910
upfront and provisioning the
target clusters based on the

173
00:06:26,910 --> 00:06:28,080
traffic patterns we saw.

174
00:06:28,080 --> 00:06:29,970
So we knew how to size this, you know,

175
00:06:29,970 --> 00:06:32,610
Postgres database instance
on the target end.

176
00:06:32,610 --> 00:06:34,950
We copied over all the ownership metadata.

177
00:06:34,950 --> 00:06:37,890
We copied over all the
authorization information

178
00:06:37,890 --> 00:06:38,723
that we needed

179
00:06:38,723 --> 00:06:41,910
to make sure their apps
could talk to the database.

180
00:06:41,910 --> 00:06:45,540
And not only did we
provision these, you know,

181
00:06:45,540 --> 00:06:49,170
final clusters, we provisioned
a temporary cluster as well.

182
00:06:49,170 --> 00:06:50,310
And we took that temporary cluster,

183
00:06:50,310 --> 00:06:51,600
we copied over the schema and data,

184
00:06:51,600 --> 00:06:53,850
which we'll go over in a second
and, you know, validated it

185
00:06:53,850 --> 00:06:55,800
and we handed those off
to your application teams,

186
00:06:55,800 --> 00:06:57,690
because there's only so much you can do

187
00:06:57,690 --> 00:06:59,520
by looking at the schema and the data.

188
00:06:59,520 --> 00:07:01,290
You actually have to run
your application to see,

189
00:07:01,290 --> 00:07:03,750
you know, how does the database perform,

190
00:07:03,750 --> 00:07:05,940
how does all the SQL actually work?

191
00:07:05,940 --> 00:07:08,430
And we found a whole bunch
of issues with, you know,

192
00:07:08,430 --> 00:07:10,350
implicit cast working in the old database

193
00:07:10,350 --> 00:07:11,280
but not working in the new.

194
00:07:11,280 --> 00:07:13,620
And so those are sort
of the three, you know,

195
00:07:13,620 --> 00:07:16,320
ways we tested this and
again, across the entire fleet

196
00:07:16,320 --> 00:07:19,503
before even talking to application teams.

197
00:07:20,910 --> 00:07:22,830
The next thing we have to do is the

198
00:07:22,830 --> 00:07:24,030
actual schema and data copy.

199
00:07:24,030 --> 00:07:26,460
And this is where, you
know, let's say schema copy,

200
00:07:26,460 --> 00:07:30,960
we built tooling around AWS DMS.

201
00:07:30,960 --> 00:07:33,450
The schema copy is any,
you know, database object.

202
00:07:33,450 --> 00:07:35,850
So tables, views, sequences

203
00:07:35,850 --> 00:07:38,190
and because we're
switching database engines,

204
00:07:38,190 --> 00:07:39,870
some of these have to be converted.

205
00:07:39,870 --> 00:07:41,190
You know, it's not always one to one.

206
00:07:41,190 --> 00:07:43,380
So this is an example of conversion

207
00:07:43,380 --> 00:07:47,550
that we didn't do ourselves,
you know, AWS DMS did it

208
00:07:47,550 --> 00:07:50,250
for us and just copy things over.

209
00:07:50,250 --> 00:07:53,040
We did however build in verification,

210
00:07:53,040 --> 00:07:54,030
'cause you know, especially

211
00:07:54,030 --> 00:07:55,650
because they didn't
technically support this.

212
00:07:55,650 --> 00:07:59,070
We found all these edge cases
where things were, you know,

213
00:07:59,070 --> 00:08:00,660
getting mangled using copying.

214
00:08:00,660 --> 00:08:04,020
So for example, we had
VARCHAR(3) that was converted

215
00:08:04,020 --> 00:08:07,350
to VARCHAR(1), which, you know,
what's the impact of that?

216
00:08:07,350 --> 00:08:08,430
These were country codes.

217
00:08:08,430 --> 00:08:10,200
You can't really have a
country code that is just C,

218
00:08:10,200 --> 00:08:11,130
that's not gonna work.

219
00:08:11,130 --> 00:08:12,750
And so we found these sorts of things

220
00:08:12,750 --> 00:08:15,210
by writing our own schema
verification tooling to sort

221
00:08:15,210 --> 00:08:17,040
of double check the work
that you know, we were doing,

222
00:08:17,040 --> 00:08:18,060
'cause you know, we were on the hook for

223
00:08:18,060 --> 00:08:19,801
this with our app teams.

224
00:08:19,801 --> 00:08:23,445
The next thing to do is
the actual data copy.

225
00:08:23,445 --> 00:08:26,093
And this is again where we
built tooling around DMS

226
00:08:26,093 --> 00:08:29,670
and DMS does, you know, a
full load, which is a sort

227
00:08:29,670 --> 00:08:32,130
of point in time copy over of all the data

228
00:08:32,130 --> 00:08:33,660
and the source to the target.

229
00:08:33,660 --> 00:08:36,900
It also does CDC, which is
change data capture, which is,

230
00:08:36,900 --> 00:08:38,520
you know, as the application is writing

231
00:08:38,520 --> 00:08:39,810
to the source database,

232
00:08:39,810 --> 00:08:41,607
it's being replicated over to the target.

233
00:08:41,607 --> 00:08:44,220
Keep in mind this is a live migration

234
00:08:44,220 --> 00:08:46,200
and so we are not shutting
down the application

235
00:08:46,200 --> 00:08:47,370
and then like copying the data.

236
00:08:47,370 --> 00:08:51,120
We're doing this live, so
applications running the database

237
00:08:51,120 --> 00:08:52,890
is run, the source database
is running and the target.

238
00:08:52,890 --> 00:08:55,860
And so we copy data and
then start replicating it.

239
00:08:55,860 --> 00:08:57,840
We did, similar to earlier,

240
00:08:57,840 --> 00:08:59,580
we did build in some tooling on our end.

241
00:08:59,580 --> 00:09:01,337
So we built in specifically
some monitoring tooling,

242
00:09:01,337 --> 00:09:03,390
'cause we found that in some cases

243
00:09:03,390 --> 00:09:06,000
the task would fail part way through

244
00:09:06,000 --> 00:09:07,140
because of a transient issue.

245
00:09:07,140 --> 00:09:09,605
And we wouldn't realize
until it's time to cut over

246
00:09:09,605 --> 00:09:11,340
and we're like, "Oh no,
database replication

247
00:09:11,340 --> 00:09:12,720
isn't happening, we need to abort."

248
00:09:12,720 --> 00:09:15,690
And so this helped us find
issues earlier instead of

249
00:09:15,690 --> 00:09:17,133
during a cut-over.

250
00:09:18,510 --> 00:09:20,190
Now just because the data is all there

251
00:09:20,190 --> 00:09:21,630
doesn't mean you're done.

252
00:09:21,630 --> 00:09:23,934
You actually have to validate
the data is all there.

253
00:09:23,934 --> 00:09:26,040
And remember at this
point we have a source,

254
00:09:26,040 --> 00:09:28,590
we have a target, we have
data being replicated from the

255
00:09:28,590 --> 00:09:30,960
source to target, but
effectively we have two different

256
00:09:30,960 --> 00:09:33,510
databases with very similar data sets.

257
00:09:33,510 --> 00:09:34,650
Maybe not exactly the same,

258
00:09:34,650 --> 00:09:36,690
but that's what we need to find out.

259
00:09:36,690 --> 00:09:38,370
And so we kind of learned
this the hard way,

260
00:09:38,370 --> 00:09:39,946
like we found issues up front.

261
00:09:39,946 --> 00:09:44,946
So DMS does support validation except one,

262
00:09:45,060 --> 00:09:46,440
it wasn't built for

263
00:09:46,440 --> 00:09:49,935
this custom, the third
party source database.

264
00:09:49,935 --> 00:09:52,170
And two, you know, we wanted to do it

265
00:09:52,170 --> 00:09:54,090
without putting pressure
on the source and target,

266
00:09:54,090 --> 00:09:55,320
because these are live databases,

267
00:09:55,320 --> 00:09:57,144
at least the source is initially.

268
00:09:57,144 --> 00:09:59,280
And we also wanted to
make sure this works well

269
00:09:59,280 --> 00:10:01,650
for both large and small
data sets in, you know,

270
00:10:01,650 --> 00:10:03,050
a reasonable amount of time.

271
00:10:03,900 --> 00:10:05,610
And so what we did is we used a lot

272
00:10:05,610 --> 00:10:08,580
of existing infrastructure
and we took those source

273
00:10:08,580 --> 00:10:11,850
and target databases and
attach them to a data feed.

274
00:10:11,850 --> 00:10:14,160
So, you know, think Kafka for example,

275
00:10:14,160 --> 00:10:15,480
actually Kafka not in this case,

276
00:10:15,480 --> 00:10:19,200
but so we connected to a data feed so

277
00:10:19,200 --> 00:10:21,180
that we sort of dumped the data

278
00:10:21,180 --> 00:10:23,700
and then also do CDC from there

279
00:10:23,700 --> 00:10:26,670
and we sent that to our data warehouse.

280
00:10:26,670 --> 00:10:27,900
Now it's in a data warehouse,

281
00:10:27,900 --> 00:10:30,270
we don't have to worry about
putting pressure on the source

282
00:10:30,270 --> 00:10:33,630
database, because we can
write a giant distributed SQL

283
00:10:33,630 --> 00:10:37,081
join across two tables to say,
"Hey, for these two tables

284
00:10:37,081 --> 00:10:39,180
what data is different."

285
00:10:39,180 --> 00:10:41,700
Like we need to make sure
that everything matches.

286
00:10:41,700 --> 00:10:44,850
But again, since this data is
changing over time, we sort

287
00:10:44,850 --> 00:10:47,100
of cut off, you know, let's
say five minutes in the past

288
00:10:47,100 --> 00:10:49,337
and see if any records have
come within five minutes,

289
00:10:49,337 --> 00:10:50,760
ignore them, everything else,

290
00:10:50,760 --> 00:10:52,680
let's validate the entire data set.

291
00:10:52,680 --> 00:10:53,957
And so we can do this faster

292
00:10:53,957 --> 00:10:56,100
and we can do this sort
of out of band as well

293
00:10:56,100 --> 00:11:00,390
because we're taking a cutoff
at a certain point anyway.

294
00:11:00,390 --> 00:11:02,370
But obviously that's
only part of the problem.

295
00:11:02,370 --> 00:11:04,590
The second part is how
do we validate that data

296
00:11:04,590 --> 00:11:07,069
that's still coming through
and also the most recent data

297
00:11:07,069 --> 00:11:09,750
is still correct.

298
00:11:09,750 --> 00:11:11,766
And so this is where
live validation comes in.

299
00:11:11,766 --> 00:11:14,130
So we take those same two data streams,

300
00:11:14,130 --> 00:11:16,860
which again are processing
data coming into the source

301
00:11:16,860 --> 00:11:18,643
from the user and to the target

302
00:11:18,643 --> 00:11:20,940
from the source through replication.

303
00:11:20,940 --> 00:11:24,360
And we pipe them into
a validator, under foot

304
00:11:24,360 --> 00:11:26,520
this is a flink job but a validator

305
00:11:26,520 --> 00:11:28,140
that is taking the two streams,

306
00:11:28,140 --> 00:11:29,940
source and target and comparing them.

307
00:11:29,940 --> 00:11:31,828
So for each record we see in one stream,

308
00:11:31,828 --> 00:11:33,736
let's make sure we see the same,

309
00:11:33,736 --> 00:11:36,540
the equivalent record on the other stream.

310
00:11:36,540 --> 00:11:39,420
And this is a lot faster than
doing the entire data set

311
00:11:39,420 --> 00:11:40,950
in one fell swoop.

312
00:11:40,950 --> 00:11:44,670
And so this lets us, you'll
see later on, do the cut-over

313
00:11:44,670 --> 00:11:47,190
much faster, because we can
be confident that the data

314
00:11:47,190 --> 00:11:50,673
before is valid and the data
that is going through is valid.

315
00:11:52,404 --> 00:11:56,460
Oh the last bit detail is
like this is the full picture

316
00:11:56,460 --> 00:11:57,480
of our validation.

317
00:11:57,480 --> 00:11:59,027
We have the data streams going

318
00:11:59,027 --> 00:12:01,860
and both the batch validation

319
00:12:01,860 --> 00:12:05,100
and online validation
happening at the same time.

320
00:12:05,100 --> 00:12:06,480
Okay, we have validated our data,

321
00:12:06,480 --> 00:12:08,400
let's talk a bit about the cut-over.

322
00:12:08,400 --> 00:12:12,120
And the way we realized we
can do this is, you know,

323
00:12:12,120 --> 00:12:14,970
zero downtime for reads and
minimal downtime for writes.

324
00:12:14,970 --> 00:12:17,220
But to do this, there's
sort of two pieces of this.

325
00:12:17,220 --> 00:12:19,511
There's one is we need
to take the application

326
00:12:19,511 --> 00:12:22,290
out of the loop as much as possible.

327
00:12:22,290 --> 00:12:24,420
We don't really wanna be sitting
on a call with the app team

328
00:12:24,420 --> 00:12:26,970
and coordinating things back
and forth and it's just a mess

329
00:12:26,970 --> 00:12:28,290
'cause they have to shut off the app.

330
00:12:28,290 --> 00:12:30,960
So we realize if we can move
them out a little faster,

331
00:12:30,960 --> 00:12:32,560
we can do a lot more things on our end.

332
00:12:32,560 --> 00:12:36,090
And then the second part
is just automate, you know,

333
00:12:36,090 --> 00:12:38,880
it's one thing to do
one migration by hand,

334
00:12:38,880 --> 00:12:42,120
it's another thing to do like
a hundred plus migrations.

335
00:12:42,120 --> 00:12:45,750
And so let's go over both of these.

336
00:12:45,750 --> 00:12:47,230
So for the cover process

337
00:12:48,570 --> 00:12:50,400
before I get into it, one thing we tend

338
00:12:50,400 --> 00:12:53,931
to do at Netflix is we run
proxies in front of data stores.

339
00:12:53,931 --> 00:12:56,848
And we do this in a lot of
cases just to do auth end

340
00:12:56,848 --> 00:12:58,410
authentication authorization,

341
00:12:58,410 --> 00:13:00,210
but depending on the data
store it might have some

342
00:13:00,210 --> 00:13:04,140
other functionality as well.

343
00:13:04,140 --> 00:13:08,610
So we realize that in order
to get the application out

344
00:13:08,610 --> 00:13:11,010
of the loop, if we handle
things the proxy layer

345
00:13:11,010 --> 00:13:13,680
that lets us take the connection,
you know, from the proxy

346
00:13:13,680 --> 00:13:16,381
to Postgres and like re-point it to Aurora

347
00:13:16,381 --> 00:13:19,020
without the application being involved.

348
00:13:19,020 --> 00:13:20,340
To do that you have to make sure

349
00:13:20,340 --> 00:13:21,840
that the application will work

350
00:13:21,840 --> 00:13:23,640
with both the source and the target.

351
00:13:23,640 --> 00:13:25,560
And so at this point we have to make sure

352
00:13:25,560 --> 00:13:27,420
that their previous schema checks

353
00:13:27,420 --> 00:13:29,728
and SQL checks are passing,
everything there is good,

354
00:13:29,728 --> 00:13:32,231
they've done all their
testing with a temp cluster

355
00:13:32,231 --> 00:13:36,240
and then we make sure that
they're talking to the proxy.

356
00:13:36,240 --> 00:13:39,090
And this makes it so that, you
know, say if they're talking

357
00:13:39,090 --> 00:13:40,470
to the proxy, we control the proxy,

358
00:13:40,470 --> 00:13:42,420
we can control the cut-over
time and like shut off writes

359
00:13:42,420 --> 00:13:43,890
and do all that fun stuff without the

360
00:13:43,890 --> 00:13:45,480
application team being involved.

361
00:13:45,480 --> 00:13:48,000
And we also to make sure that
using the correct credentials,

362
00:13:48,000 --> 00:13:50,000
credentials all work on
the source and target.

363
00:13:50,000 --> 00:13:53,040
So this lets us do cut-over without the

364
00:13:53,040 --> 00:13:54,390
application being involved.

365
00:13:55,860 --> 00:13:57,930
Now let's go over the
actual cut-over process.

366
00:13:57,930 --> 00:13:59,100
And this is a lot of steps.

367
00:13:59,100 --> 00:14:00,840
So again, I'm gonna blaze through it,

368
00:14:00,840 --> 00:14:03,570
but let's see how far we can get.

369
00:14:03,570 --> 00:14:06,570
So remember at this point we have source

370
00:14:06,570 --> 00:14:07,860
and target replications happening

371
00:14:07,860 --> 00:14:09,020
from the source to the target.

372
00:14:09,020 --> 00:14:12,960
The first step is, hey, how
long is that replication delay?

373
00:14:12,960 --> 00:14:14,580
We don't wanna attempt

374
00:14:14,580 --> 00:14:16,650
to cut over if the replication
delay is like an hour,

375
00:14:16,650 --> 00:14:18,270
because then you're sitting

376
00:14:18,270 --> 00:14:20,550
and waiting for an hour for
replication to catch up.

377
00:14:20,550 --> 00:14:24,840
And so what we do is we write
a record into the source

378
00:14:24,840 --> 00:14:26,550
and wait for it to show up in the target.

379
00:14:26,550 --> 00:14:27,660
It's that simple.

380
00:14:27,660 --> 00:14:30,330
And we're using an internal
metadata table that we create

381
00:14:30,330 --> 00:14:32,130
so that we don't mess
with the user's data.

382
00:14:32,130 --> 00:14:33,540
And we set an

383
00:14:33,540 --> 00:14:35,250
arbitrary ish cutoff as a minute.

384
00:14:35,250 --> 00:14:37,640
But in reality we saw
in most cases it was 15,

385
00:14:37,640 --> 00:14:39,090
16 seconds of replication delay.

386
00:14:39,090 --> 00:14:40,200
And you know, we measure this

387
00:14:40,200 --> 00:14:41,450
so we know it's accurate.

388
00:14:42,590 --> 00:14:45,420
Once we know that the
replication is happening,

389
00:14:45,420 --> 00:14:47,190
we have to run our validation.

390
00:14:47,190 --> 00:14:48,960
And I talked about this
earlier, we have batch

391
00:14:48,960 --> 00:14:50,280
and online validation.

392
00:14:50,280 --> 00:14:51,870
You know, at this point, remember the

393
00:14:51,870 --> 00:14:53,040
application's still running,

394
00:14:53,040 --> 00:14:54,420
everything is up, nothing's down.

395
00:14:54,420 --> 00:14:55,950
And so we just run our batch validation,

396
00:14:55,950 --> 00:14:59,403
make sure that the data we
copied over is accurate.

397
00:15:00,750 --> 00:15:01,740
Assuming it's accurate,

398
00:15:01,740 --> 00:15:04,170
we have to make sure
that the user is still

399
00:15:04,170 --> 00:15:05,880
actually talking to the proxy.

400
00:15:05,880 --> 00:15:07,260
And this is, you know, us trying

401
00:15:07,260 --> 00:15:09,810
to remove all the foot guns from the user,

402
00:15:09,810 --> 00:15:13,080
because we found cases
where users were not talking

403
00:15:13,080 --> 00:15:14,737
to the proxy or they had
some app that was running,

404
00:15:14,737 --> 00:15:16,650
you know, on a batch schedule.

405
00:15:16,650 --> 00:15:18,900
And you know, we ran some checks

406
00:15:18,900 --> 00:15:20,340
to make sure no one is talking directly,

407
00:15:20,340 --> 00:15:22,110
but we also removed direct access

408
00:15:22,110 --> 00:15:24,206
and made it so they had
to talk to their proxy.

409
00:15:24,206 --> 00:15:26,430
And this made it more difficult for users

410
00:15:26,430 --> 00:15:28,380
to like mess things up
later on by, you know,

411
00:15:28,380 --> 00:15:31,830
sort of a split brain scenario
talking to the wrong cluster.

412
00:15:31,830 --> 00:15:34,023
And then the last step
is, I talked about how

413
00:15:34,023 --> 00:15:38,010
databases don't exist by themselves.

414
00:15:38,010 --> 00:15:41,010
And so we take all that streaming
and batch infrastructure

415
00:15:41,010 --> 00:15:43,507
and copy it over from
the source to the target,

416
00:15:43,507 --> 00:15:46,770
'cause at this point we know
that the data is all there.

417
00:15:46,770 --> 00:15:48,450
It's correct, replication's happening.

418
00:15:48,450 --> 00:15:49,530
So it's safe to do that.

419
00:15:49,530 --> 00:15:51,900
Like all you're doing is
introducing another few seconds.

420
00:15:51,900 --> 00:15:54,000
You know, the replication
delays 15 seconds,

421
00:15:54,000 --> 00:15:55,170
you're adding 15 seconds of delay

422
00:15:55,170 --> 00:15:56,520
to their downstream pipelines.

423
00:15:56,520 --> 00:15:58,080
That's fine. No one's gonna care.

424
00:15:58,080 --> 00:15:59,820
No one's gonna notice.

425
00:15:59,820 --> 00:16:02,370
This gets us to the critical section.

426
00:16:02,370 --> 00:16:04,110
At this point we shut off writes

427
00:16:04,110 --> 00:16:08,370
and we just sort of did this
by modifying permissions

428
00:16:08,370 --> 00:16:10,740
for the user on the source database.

429
00:16:10,740 --> 00:16:13,710
And so read is still working
so the application is still up

430
00:16:13,710 --> 00:16:15,300
but it's, you know, it's slightly degraded

431
00:16:15,300 --> 00:16:16,890
because writes aren't working anymore.

432
00:16:16,890 --> 00:16:17,970
And we did, you know, coordinate

433
00:16:17,970 --> 00:16:19,980
with app teams on a
downtime window for this.

434
00:16:19,980 --> 00:16:23,013
But you know, applications
again up and writes are blocked.

435
00:16:24,450 --> 00:16:26,700
And then I talked about
sequences briefly earlier.

436
00:16:26,700 --> 00:16:30,240
Sequences are basically stateful
counters in the database

437
00:16:30,240 --> 00:16:32,520
and you know, they don't get
copied over our replication.

438
00:16:32,520 --> 00:16:34,830
So you have to take the
sequences from the source,

439
00:16:34,830 --> 00:16:35,853
then make sure the ones in the target

440
00:16:35,853 --> 00:16:38,550
that the values match up to
what they're supposed to be.

441
00:16:38,550 --> 00:16:41,130
Sort of a small detail.

442
00:16:41,130 --> 00:16:43,470
And then I mentioned earlier

443
00:16:43,470 --> 00:16:45,120
we measured replication delay.

444
00:16:45,120 --> 00:16:47,340
We now have to wait for
replication to catch up

445
00:16:47,340 --> 00:16:49,710
and it's the same process as before,

446
00:16:49,710 --> 00:16:51,960
we push a record into the source,

447
00:16:51,960 --> 00:16:53,550
wait for the show up at the target.

448
00:16:53,550 --> 00:16:54,630
At this point we have blocked write

449
00:16:54,630 --> 00:16:57,720
so we know that the record we
pushed in is the last record.

450
00:16:57,720 --> 00:16:59,910
So when it comes through,
we know we're done

451
00:16:59,910 --> 00:17:03,540
with replication and then
we look at our validation.

452
00:17:03,540 --> 00:17:06,013
But this time we look at
only online validation,

453
00:17:06,013 --> 00:17:08,640
'cause we previously
verified batch validation

454
00:17:08,640 --> 00:17:10,110
and also online at the time.

455
00:17:10,110 --> 00:17:11,250
At this point we're just saying, "Hey,

456
00:17:11,250 --> 00:17:12,420
is the data still correct?

457
00:17:12,420 --> 00:17:14,320
Is it still not mangled?

458
00:17:14,320 --> 00:17:17,370
Do we have confidence that
we can cut-over safely?"

459
00:17:17,370 --> 00:17:20,100
Assuming that's true,
we're essentially done,

460
00:17:20,100 --> 00:17:23,580
we take the proxy, we update
it to point to the target,

461
00:17:23,580 --> 00:17:25,920
which is this case Aurora Postgres.

462
00:17:25,920 --> 00:17:29,864
And we also terminate connections
on the source database

463
00:17:29,864 --> 00:17:32,040
just to make sure there are
no lingering connections,

464
00:17:32,040 --> 00:17:33,420
'cause a lot of our
stuff is done, you know,

465
00:17:33,420 --> 00:17:35,199
our connection time and
so we kill all connections

466
00:17:35,199 --> 00:17:37,500
and now the application's
talking to the target,

467
00:17:37,500 --> 00:17:39,560
we never turned off write
to the target and so

468
00:17:39,560 --> 00:17:41,850
it just magically starts working

469
00:17:41,850 --> 00:17:43,590
and that was a cut-over process.

470
00:17:43,590 --> 00:17:44,880
That was a lot.

471
00:17:44,880 --> 00:17:46,590
And so I'll leave you with how far we are,

472
00:17:46,590 --> 00:17:49,200
'cause we're not done and
learnings we have from

473
00:17:49,200 --> 00:17:51,683
this adventure that we've been on.

474
00:17:51,683 --> 00:17:55,020
This has been, you know, we
started writing the automation

475
00:17:55,020 --> 00:17:56,310
I think December last year.

476
00:17:56,310 --> 00:17:57,720
So almost a full year.

477
00:17:57,720 --> 00:17:59,430
But we ran our first migration in April.

478
00:17:59,430 --> 00:18:02,460
It was kind of janky, some
manual code running on my laptop,

479
00:18:02,460 --> 00:18:04,770
some running on Jupyter
Notebooks but we made it work

480
00:18:04,770 --> 00:18:06,810
and then we built pieces as we went.

481
00:18:06,810 --> 00:18:09,150
But at this point we're over 90% complete.

482
00:18:09,150 --> 00:18:10,560
There's a handful of cases

483
00:18:10,560 --> 00:18:13,290
that require some special
handling, you know,

484
00:18:13,290 --> 00:18:15,560
large data sets that
we'll be finishing off

485
00:18:15,560 --> 00:18:16,500
in the next few weeks.

486
00:18:16,500 --> 00:18:17,880
But like I'm pretty impressed

487
00:18:17,880 --> 00:18:19,800
with this progress on our team.

488
00:18:19,800 --> 00:18:22,770
We did not think we'd
be this far right now.

489
00:18:22,770 --> 00:18:25,295
Additionally we've seen
this in some cases, not all,

490
00:18:25,295 --> 00:18:29,190
we saw in some cases latencies drop.

491
00:18:29,190 --> 00:18:30,360
And this is, you know, partly

492
00:18:30,360 --> 00:18:32,640
because this is no longer
a distributed store.

493
00:18:32,640 --> 00:18:35,550
We're not doing raft
consensus for reads and writes

494
00:18:35,550 --> 00:18:37,155
and you know, Aurora Postgres,

495
00:18:37,155 --> 00:18:38,670
'cause you're generally
reading from memory.

496
00:18:38,670 --> 00:18:39,870
So you know, there are reasons for that.

497
00:18:39,870 --> 00:18:42,300
But this is sort of a side benefit

498
00:18:42,300 --> 00:18:43,890
is latencies are generally lower

499
00:18:43,890 --> 00:18:45,687
in a lot of cases

500
00:18:45,687 --> 00:18:47,880
and you can kinda see it's
pretty obvious from this graph

501
00:18:47,880 --> 00:18:48,900
when the cut-over happened.

502
00:18:48,900 --> 00:18:50,733
There's a very clear demarcation.

503
00:18:52,380 --> 00:18:54,450
And this is critical for us

504
00:18:54,450 --> 00:18:56,910
as the data platform
team is we found a number

505
00:18:56,910 --> 00:18:58,290
of data corruption bugs

506
00:18:58,290 --> 00:19:00,840
and we fixed them all
as far as we can tell.

507
00:19:00,840 --> 00:19:04,239
I mentioned earlier that we ran
our attempt cluster creation

508
00:19:04,239 --> 00:19:06,480
across the entire fleet.

509
00:19:06,480 --> 00:19:08,040
That included validation

510
00:19:08,040 --> 00:19:10,230
and that found so well, you know,

511
00:19:10,230 --> 00:19:12,420
10 ish data corruption bugs somewhere

512
00:19:12,420 --> 00:19:14,550
with null handling of columns.

513
00:19:14,550 --> 00:19:17,730
Some were encoding, getting mangled,

514
00:19:17,730 --> 00:19:20,940
some were generated columns not
being copied over correctly.

515
00:19:20,940 --> 00:19:23,700
Some were data truncation,
column is just being dropped.

516
00:19:23,700 --> 00:19:26,400
And we found all these
upfront, well almost all,

517
00:19:26,400 --> 00:19:27,750
there were a couple we found later on.

518
00:19:27,750 --> 00:19:31,620
There's one weird one with
statements in a transaction being

519
00:19:31,620 --> 00:19:33,720
ordered differently depending
on when they came into

520
00:19:33,720 --> 00:19:36,750
replication that we found
when we were doing like test

521
00:19:36,750 --> 00:19:39,060
cut-overs because that was CDC related.

522
00:19:39,060 --> 00:19:43,140
Our initial process was less
CDC and more just full load.

523
00:19:43,140 --> 00:19:44,940
And so I am pretty happy with this

524
00:19:44,940 --> 00:19:47,010
'cause we, you know, we
found all these bugs early on

525
00:19:47,010 --> 00:19:48,630
without going back and
forth with the app teams.

526
00:19:48,630 --> 00:19:50,370
We worked with the DMS folks,

527
00:19:50,370 --> 00:19:52,740
'cause sometimes it was
edge cases on their end.

528
00:19:52,740 --> 00:19:54,780
Sometimes it was engine differences

529
00:19:54,780 --> 00:19:56,130
that we had to work around.

530
00:19:56,130 --> 00:19:58,770
But we got all these sorted
and validation passed

531
00:19:58,770 --> 00:20:00,559
and after all that,
you know, no rollbacks,

532
00:20:00,559 --> 00:20:02,580
which is, you know, I'm pretty proud of.

533
00:20:02,580 --> 00:20:04,290
We had a couple of fix forward use cases.

534
00:20:04,290 --> 00:20:06,212
It wasn't all, you know,
sunshine and rainbows,

535
00:20:06,212 --> 00:20:08,400
some of them were just
under scale clusters.

536
00:20:08,400 --> 00:20:10,710
Some of them were, you know,

537
00:20:10,710 --> 00:20:12,000
metadata didn't get copied over correctly.

538
00:20:12,000 --> 00:20:15,450
We fixed all of those but
you know, everything happened

539
00:20:15,450 --> 00:20:16,850
and no one had to roll back.

540
00:20:17,820 --> 00:20:19,890
And so the last thing
I'll leave you with is

541
00:20:19,890 --> 00:20:22,890
that initial fleet wide
dry run we did was amazing,

542
00:20:22,890 --> 00:20:24,750
'cause it saved months of time.

543
00:20:24,750 --> 00:20:27,330
Not even exaggerating by doing
this all up front across the

544
00:20:27,330 --> 00:20:29,370
fleet, working with the DMS folks,

545
00:20:29,370 --> 00:20:31,560
building resiliency into our tooling

546
00:20:31,560 --> 00:20:33,810
and that all, you know,
worked really well.

547
00:20:33,810 --> 00:20:36,570
We built on top of a lot of
existing tooling, you know,

548
00:20:36,570 --> 00:20:38,910
DMS some of our own tooling as well,

549
00:20:38,910 --> 00:20:41,670
but we got a lot of value
outta building verification

550
00:20:41,670 --> 00:20:44,910
and making sure things
actually worked correctly.

551
00:20:44,910 --> 00:20:48,210
And the last piece is I
mentioned earlier, you know,

552
00:20:48,210 --> 00:20:51,330
we focused on the lowest
common denominator of sorts

553
00:20:51,330 --> 00:20:52,350
and build things that would work

554
00:20:52,350 --> 00:20:54,701
across applications, across technologies.

555
00:20:54,701 --> 00:20:58,320
But we did spend some
cycles building, you know,

556
00:20:58,320 --> 00:21:00,630
specific tooling for like Java Flyway

557
00:21:00,630 --> 00:21:02,820
because turned out a lot of
folks in Netflix use that.

558
00:21:02,820 --> 00:21:05,910
And so we built some specific
tooling just for that,

559
00:21:05,910 --> 00:21:09,544
even though it's not really
applicable to all use cases.

560
00:21:09,544 --> 00:21:12,269
But yeah, that's all I have. Thank you.

561
00:21:12,269 --> 00:21:14,460
(audience applauds)

562
00:21:14,460 --> 00:21:15,783
- Thank you very much.

