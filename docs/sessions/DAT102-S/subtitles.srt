1
00:00:01,260 --> 00:00:04,713
- Hi, everyone, a very good
afternoon to all of you.

2
00:00:05,700 --> 00:00:08,050
Hope you all have been
having a good day today,

3
00:00:08,970 --> 00:00:10,530
lot of sessions.

4
00:00:10,530 --> 00:00:13,860
And today, we are here to discuss

5
00:00:13,860 --> 00:00:17,793
about how at CCC Intelligent Solutions,

6
00:00:18,630 --> 00:00:21,630
we were able to achieve and improve

7
00:00:21,630 --> 00:00:26,630
our operational efficiency
with database modernization

8
00:00:26,760 --> 00:00:31,113
from Oracle to AWS Aurora Postgres.

9
00:00:33,570 --> 00:00:36,840
We have a very nice agenda
over here for today.

10
00:00:36,840 --> 00:00:41,160
The first one, I will try to explain

11
00:00:41,160 --> 00:00:44,790
why data is critical for CCC.

12
00:00:44,790 --> 00:00:48,300
I'll give you brief
background on what is CCC,

13
00:00:48,300 --> 00:00:51,240
and then I will explain or take you all

14
00:00:51,240 --> 00:00:54,030
through our cloud journey.

15
00:00:54,030 --> 00:00:56,520
But to understand all of these,

16
00:00:56,520 --> 00:01:01,170
we need to understand why we did it,

17
00:01:01,170 --> 00:01:04,320
how we did it, and what did we get

18
00:01:04,320 --> 00:01:07,503
after achieving or
completing this journey?

19
00:01:11,640 --> 00:01:16,640
So to start with, CCC is a
leader as a SaaS provider

20
00:01:18,300 --> 00:01:22,680
to provide solutions for auto claims

21
00:01:22,680 --> 00:01:25,230
and collision repair industry.

22
00:01:25,230 --> 00:01:27,900
We help all the auto insurers,

23
00:01:27,900 --> 00:01:30,300
we help repair shops,

24
00:01:30,300 --> 00:01:33,630
automakers, parts suppliers,

25
00:01:33,630 --> 00:01:36,630
and other vendors

26
00:01:36,630 --> 00:01:40,563
to automate and streamline
their claim processing workflow.

27
00:01:41,880 --> 00:01:44,470
We build solutions for the repair shops

28
00:01:45,390 --> 00:01:47,550
to streamline their repair process,

29
00:01:47,550 --> 00:01:50,883
and we provide, as part
of our SaaS platform,

30
00:01:51,720 --> 00:01:56,070
AI-enabled advanced solutions
so that they can maintain

31
00:01:56,070 --> 00:02:00,363
and manage their claim
cycle for their customers.

32
00:02:01,230 --> 00:02:03,480
Now, when we talk about our customers,

33
00:02:03,480 --> 00:02:07,260
maybe the customers are auto
insurers or repair shops,

34
00:02:07,260 --> 00:02:08,670
but at the end of the day,

35
00:02:08,670 --> 00:02:10,890
their customers are our customers.

36
00:02:10,890 --> 00:02:14,130
So what we try to do or
how we help our customers

37
00:02:14,130 --> 00:02:18,390
is to turn all these
data points into actions,

38
00:02:18,390 --> 00:02:22,233
and all their key moments
into intelligent experiences.

39
00:02:23,207 --> 00:02:27,060
It's never a great experience
to be in an auto accident

40
00:02:27,060 --> 00:02:29,730
and go through that journey,

41
00:02:29,730 --> 00:02:33,690
so we try our best to
provide simplified solutions

42
00:02:33,690 --> 00:02:35,940
to make their journey
through that claim cycle

43
00:02:35,940 --> 00:02:37,443
better and smoother.

44
00:02:41,370 --> 00:02:44,070
Kind of what sets CCC apart

45
00:02:44,070 --> 00:02:47,400
is we are industry leader
in collision estimating

46
00:02:47,400 --> 00:02:48,930
solution provider,

47
00:02:48,930 --> 00:02:51,120
and also, what we do is as part

48
00:02:51,120 --> 00:02:52,890
of helping our customer's customer,

49
00:02:52,890 --> 00:02:57,450
we also help our customers
to fuel their growth.

50
00:02:57,450 --> 00:03:01,230
Now, if I talk about data, what we do

51
00:03:01,230 --> 00:03:03,660
or what we process in
through our SaaS platform

52
00:03:03,660 --> 00:03:08,263
is we process around 18
million claims over a year,

53
00:03:09,960 --> 00:03:13,653
which is close to 50,000
to 55,000 per day.

54
00:03:14,580 --> 00:03:16,200
I may be talking about claims,

55
00:03:16,200 --> 00:03:20,670
but think about these from
a human point of view,

56
00:03:20,670 --> 00:03:24,330
like 50,000 accidents or
50,000 claims every day,

57
00:03:24,330 --> 00:03:28,170
that means we are touching
at least 50,000 human lives

58
00:03:28,170 --> 00:03:30,810
at any point of time in a particular day.

59
00:03:30,810 --> 00:03:33,570
So who do we work with?

60
00:03:33,570 --> 00:03:37,710
We have top 300 auto insurers.

61
00:03:37,710 --> 00:03:41,670
Our solutions are implemented
by 35,000 repair shops

62
00:03:41,670 --> 00:03:44,220
across United States.

63
00:03:44,220 --> 00:03:47,670
We have our integration
with 5,000 suppliers

64
00:03:47,670 --> 00:03:50,040
through which we do parts procurement,

65
00:03:50,040 --> 00:03:54,960
and then we integrate with
top 12 of top 15 automakers

66
00:03:54,960 --> 00:03:58,143
to do our integration with OEM providers.

67
00:03:59,070 --> 00:04:03,210
Now, all through this, CCC's primary focus

68
00:04:03,210 --> 00:04:08,100
is when an auto collision
estimating processing,

69
00:04:08,100 --> 00:04:10,860
the claim is going
through all its process,

70
00:04:10,860 --> 00:04:13,950
it generates a huge amount of data.

71
00:04:13,950 --> 00:04:17,250
Now, how we differentiate from others

72
00:04:17,250 --> 00:04:19,320
are we take these data points,

73
00:04:19,320 --> 00:04:22,620
we create intelligent reasons out of it,

74
00:04:22,620 --> 00:04:24,630
we create prediction out of it,

75
00:04:24,630 --> 00:04:29,630
and then all these data turns
into a predictive solution,

76
00:04:29,940 --> 00:04:33,510
and we have AI-enabled
analytics on top of it

77
00:04:33,510 --> 00:04:35,013
to help our customers.

78
00:04:36,210 --> 00:04:39,780
As I mentioned, we have
around 5,000 parts suppliers

79
00:04:39,780 --> 00:04:42,420
who do we direct integration with them.

80
00:04:42,420 --> 00:04:45,390
We have 12 OEM providers
who we are directly

81
00:04:45,390 --> 00:04:46,263
integrated with.

82
00:04:50,580 --> 00:04:53,220
Now, this is all I said about what

83
00:04:53,220 --> 00:04:56,913
was CCC Intelligent Solution
and its SaaS platform,

84
00:04:58,440 --> 00:05:02,550
but we started this journey back in 2022

85
00:05:02,550 --> 00:05:06,150
to support our growth, to scale ourselves,

86
00:05:06,150 --> 00:05:08,850
to provide our customers the platform

87
00:05:08,850 --> 00:05:10,113
for their growth, also.

88
00:05:12,240 --> 00:05:15,480
Now, what we did, how did we achieve

89
00:05:15,480 --> 00:05:18,660
our cloud modernization journey?

90
00:05:18,660 --> 00:05:21,930
Initially, what we did
is we broke that apart

91
00:05:21,930 --> 00:05:23,670
into three cycles.

92
00:05:23,670 --> 00:05:28,020
The first cycle we did was we
focused on the infrastructure.

93
00:05:28,020 --> 00:05:31,950
The second, we looked
into the application tier,

94
00:05:31,950 --> 00:05:34,713
and then the third and the
last one was the database.

95
00:05:35,580 --> 00:05:38,340
You can ask why database was the last one,

96
00:05:38,340 --> 00:05:42,600
because our core business
sits inside that data.

97
00:05:42,600 --> 00:05:45,750
As I mentioned, we rely
on data very heavily,

98
00:05:45,750 --> 00:05:48,240
so we kept that towards the end.

99
00:05:48,240 --> 00:05:50,730
First, we looked into the infrastructure,

100
00:05:50,730 --> 00:05:53,700
we modernized the infrastructure layer

101
00:05:53,700 --> 00:05:58,700
from migrating all our workload
from on-prem to AWS Cloud

102
00:06:00,420 --> 00:06:03,990
so that we can have more freedom,

103
00:06:03,990 --> 00:06:08,990
and we can have a better
and faster scalable options.

104
00:06:09,060 --> 00:06:11,850
Then what we did is we
went through a complete

105
00:06:11,850 --> 00:06:13,620
application modernization.

106
00:06:13,620 --> 00:06:16,680
Earlier, we were pretty kind of tied up

107
00:06:16,680 --> 00:06:18,480
with vendor solutions,

108
00:06:18,480 --> 00:06:22,170
so what we did is we went for
a more open-source approach

109
00:06:22,170 --> 00:06:25,440
where we migrated our
Oracle WebLogic workloads

110
00:06:25,440 --> 00:06:28,890
into microservices and JBUS,

111
00:06:28,890 --> 00:06:32,310
then we migrated our service
with components from Oracle

112
00:06:32,310 --> 00:06:33,840
to MuleSoft,

113
00:06:33,840 --> 00:06:37,260
and then we leveraged
more of Kafka architecture

114
00:06:37,260 --> 00:06:39,603
to get into even giving
architecture design.

115
00:06:40,800 --> 00:06:43,650
Then came last as the database part,

116
00:06:43,650 --> 00:06:45,780
which was the heavy lift for us.

117
00:06:45,780 --> 00:06:48,120
As I already mentioned, that we process

118
00:06:48,120 --> 00:06:50,730
close to 50,000 claims per day,

119
00:06:50,730 --> 00:06:54,030
which turns into around 18 million,

120
00:06:54,030 --> 00:06:57,750
and then on top of that, if
you look at every transaction

121
00:06:57,750 --> 00:07:00,780
or every action which happens
on a particular claim,

122
00:07:00,780 --> 00:07:03,270
it turns out that our databases process

123
00:07:03,270 --> 00:07:06,330
almost 50,000 transactions per second,

124
00:07:06,330 --> 00:07:09,330
so you can imagine how
busy that database is.

125
00:07:09,330 --> 00:07:12,030
There are tons of applications,
small applications,

126
00:07:12,030 --> 00:07:14,760
talking to each other, tightly integrated

127
00:07:14,760 --> 00:07:18,180
so that it can manage each
touching point smoothly

128
00:07:18,180 --> 00:07:19,320
without any failure,

129
00:07:19,320 --> 00:07:21,240
so there are, like,
hundreds of applications.

130
00:07:21,240 --> 00:07:24,510
We need to think that how
we will simplify them.

131
00:07:24,510 --> 00:07:28,200
So we took this approach,
infrastructure was done,

132
00:07:28,200 --> 00:07:29,310
then as a second step,

133
00:07:29,310 --> 00:07:32,820
we finished the application modernization,

134
00:07:32,820 --> 00:07:36,480
and then we started the journey
of database modernization.

135
00:07:36,480 --> 00:07:39,300
Earlier, like when we were on on-prem,

136
00:07:39,300 --> 00:07:42,660
and even after we moved
to AWS Cloud initially

137
00:07:42,660 --> 00:07:44,250
with our infrastructure,

138
00:07:44,250 --> 00:07:47,433
our applications were
running on Oracle Database.

139
00:07:49,650 --> 00:07:53,190
So what we said is that we looked

140
00:07:53,190 --> 00:07:56,070
into our database modernation as a project

141
00:07:56,070 --> 00:07:59,887
or as a full challenge,
and then we said that,

142
00:07:59,887 --> 00:08:03,330
"Let's go through what
are the challenges we have

143
00:08:03,330 --> 00:08:05,700
to accomplish this journey?"

144
00:08:05,700 --> 00:08:08,250
So the first one was that we had to look

145
00:08:08,250 --> 00:08:09,840
into all our databases,

146
00:08:09,840 --> 00:08:12,480
and I think it'll be
applicable for all of you

147
00:08:12,480 --> 00:08:14,160
if you're going through that journey.

148
00:08:14,160 --> 00:08:18,210
You need to identify all
the integration points

149
00:08:18,210 --> 00:08:19,043
into that database.

150
00:08:19,043 --> 00:08:20,670
You need to identify all the sources

151
00:08:20,670 --> 00:08:22,650
which are going to put data into it.

152
00:08:22,650 --> 00:08:25,800
You need to identify all
the downstream applications

153
00:08:25,800 --> 00:08:27,480
which are going to
either read or come back

154
00:08:27,480 --> 00:08:29,793
and update those data points.

155
00:08:30,990 --> 00:08:32,730
We went through that analysis,

156
00:08:32,730 --> 00:08:37,080
and we identified that
what are all the dependency

157
00:08:37,080 --> 00:08:38,400
between each of them,

158
00:08:38,400 --> 00:08:42,360
so that one, I can do a
divide and rule approach.

159
00:08:42,360 --> 00:08:44,640
I say that all the components

160
00:08:44,640 --> 00:08:46,260
or applications which are independent,

161
00:08:46,260 --> 00:08:49,470
I will first put them into
a phase one deliverable,

162
00:08:49,470 --> 00:08:53,280
and then I will look into
all the dependency-heavy

163
00:08:53,280 --> 00:08:55,953
applications and see
how I can them further.

164
00:08:57,090 --> 00:09:01,530
When I'm talking about
hundreds of applications,

165
00:09:01,530 --> 00:09:06,530
I'm talking about close to,
like, more than 50,000 SQLs,

166
00:09:07,740 --> 00:09:10,920
which were kind of implemented
through those applications

167
00:09:10,920 --> 00:09:13,500
which were running when
all these applications

168
00:09:13,500 --> 00:09:15,840
generate workload into the databases.

169
00:09:15,840 --> 00:09:18,480
So what we did is we
went through analyzing

170
00:09:18,480 --> 00:09:22,110
all those Oracle-native SQLs,

171
00:09:22,110 --> 00:09:24,960
and we tried to analyze them,

172
00:09:24,960 --> 00:09:27,840
how many of them are easily transportable

173
00:09:27,840 --> 00:09:31,683
or can be migrated into
a PostgreSQL structure?

174
00:09:32,550 --> 00:09:35,490
And then also we looked into a lot of SQLs

175
00:09:35,490 --> 00:09:38,790
which we could kind of tune or modify

176
00:09:38,790 --> 00:09:42,063
so that it can fit easily into
the Postgres architecture.

177
00:09:43,380 --> 00:09:46,740
The next challenge was the data migration,

178
00:09:46,740 --> 00:09:48,930
where you can work
through your application,

179
00:09:48,930 --> 00:09:51,000
you can work through your SQLs,

180
00:09:51,000 --> 00:09:53,970
but our data migration was the next step,

181
00:09:53,970 --> 00:09:57,780
and on an average, the
amount of data we have

182
00:09:57,780 --> 00:10:02,160
in all our databases was
more than 150 terabytes,

183
00:10:02,160 --> 00:10:04,620
and one of them was even
close to 100 terabytes.

184
00:10:04,620 --> 00:10:09,150
So what we had to do is we
needed to find a solution

185
00:10:09,150 --> 00:10:11,520
where I can take in all
these hundred terabytes

186
00:10:11,520 --> 00:10:14,700
and fit it into a Oracle
Postgres engine architecture,

187
00:10:14,700 --> 00:10:17,403
which was a tough challenge for all of us,

188
00:10:18,750 --> 00:10:21,510
but while we were going through
all these modernization,

189
00:10:21,510 --> 00:10:24,600
we are trying to find solution
for all these challenges,

190
00:10:24,600 --> 00:10:28,650
there was one constant
factor in all of those.

191
00:10:28,650 --> 00:10:33,240
There was no compromise
on our SLOs or SLAs,

192
00:10:33,240 --> 00:10:35,760
because business has to run as is.

193
00:10:35,760 --> 00:10:38,370
So that was our primary goal number one,

194
00:10:38,370 --> 00:10:41,460
that any simplification,
any modernization,

195
00:10:41,460 --> 00:10:45,870
any improvement, any tuning
we are trying to come up with

196
00:10:45,870 --> 00:10:48,933
has to meet those SLAs or even better.

197
00:10:50,070 --> 00:10:51,930
So what we did?

198
00:10:51,930 --> 00:10:54,540
Now, with all this journey,

199
00:10:54,540 --> 00:10:56,730
there always has to be a risk, correct?

200
00:10:56,730 --> 00:10:59,550
As I mentioned, there are tons of data,

201
00:10:59,550 --> 00:11:02,280
there are tons of different data types,

202
00:11:02,280 --> 00:11:05,910
there are different applications
accessing those data

203
00:11:05,910 --> 00:11:09,870
more or less at the same
point in time concurrently.

204
00:11:09,870 --> 00:11:12,270
So what we had to do?

205
00:11:12,270 --> 00:11:15,007
Now, I cannot come and
go to business, say that,

206
00:11:15,007 --> 00:11:17,700
"Okay, all these products applications

207
00:11:17,700 --> 00:11:19,620
will not be available for six months,

208
00:11:19,620 --> 00:11:21,420
because we are going
through this journey,"

209
00:11:21,420 --> 00:11:24,090
so how I can mitigate that risk?

210
00:11:24,090 --> 00:11:26,820
So what we did is we created a isolated,

211
00:11:26,820 --> 00:11:30,060
separated test environment
where we were able

212
00:11:30,060 --> 00:11:32,160
to take the production, like data,

213
00:11:32,160 --> 00:11:35,760
we are able to generate
production, like a load volume,

214
00:11:35,760 --> 00:11:38,790
so that I can not only
test my compatibility

215
00:11:38,790 --> 00:11:41,760
of my applications into
the Postgres architecture,

216
00:11:41,760 --> 00:11:44,760
but also ensure that the
performance efficiency

217
00:11:44,760 --> 00:11:46,263
hasn't been compromised.

218
00:11:47,250 --> 00:11:51,510
So what we did is we built
a separate test environment

219
00:11:51,510 --> 00:11:54,390
for all of us to try out
all these code changes,

220
00:11:54,390 --> 00:11:56,970
all these SQL changes,
test our compatibility,

221
00:11:56,970 --> 00:11:59,100
test our performance,

222
00:11:59,100 --> 00:12:01,710
and then the next challenge,
as I mentioned earlier,

223
00:12:01,710 --> 00:12:04,950
was how to fit into 100 terabytes

224
00:12:04,950 --> 00:12:07,230
or more than 100 terabytes of data worth

225
00:12:07,230 --> 00:12:08,430
into Aurora Postgres.

226
00:12:08,430 --> 00:12:12,720
As we all know, Aurora
Postgres has a limitation

227
00:12:12,720 --> 00:12:15,660
of maximum amount of storage it can hold.

228
00:12:15,660 --> 00:12:20,660
So what we did was we went
back to our Oracle Database,

229
00:12:20,850 --> 00:12:23,610
and we started to analyze
different types of data

230
00:12:23,610 --> 00:12:25,740
which we were storing in that database,

231
00:12:25,740 --> 00:12:28,260
which I think is always
going to be critical part

232
00:12:28,260 --> 00:12:31,380
if you're going through that
database modernization journey.

233
00:12:31,380 --> 00:12:34,410
We went through, we first identified

234
00:12:34,410 --> 00:12:36,600
that if there are any binary data,

235
00:12:36,600 --> 00:12:39,540
which were stored on the Oracle Database,

236
00:12:39,540 --> 00:12:42,900
if those can be migrated
out of a database.

237
00:12:42,900 --> 00:12:46,140
Again, these databases were used by a lot

238
00:12:46,140 --> 00:12:47,280
of legacy applications,

239
00:12:47,280 --> 00:12:49,920
so we identified there's a lot of storage

240
00:12:49,920 --> 00:12:53,040
is used by this binary
data, such as BLOB and CLOB,

241
00:12:53,040 --> 00:12:55,530
where we used to store our documents,

242
00:12:55,530 --> 00:12:58,650
images related to,
associated to those claims.

243
00:12:58,650 --> 00:13:00,450
So we came up with our approach.

244
00:13:00,450 --> 00:13:02,880
We built automation to migrate

245
00:13:02,880 --> 00:13:07,050
close to 30 to 40 terabytes of data,

246
00:13:07,050 --> 00:13:10,320
all this binary data out
of these Oracle Database

247
00:13:10,320 --> 00:13:12,450
and migrate it over to S3 first

248
00:13:12,450 --> 00:13:15,780
before even we migrate rest
of the data into Postgres.

249
00:13:15,780 --> 00:13:18,750
So if you try to understand the breakdown

250
00:13:18,750 --> 00:13:20,970
of these database migration journey,

251
00:13:20,970 --> 00:13:24,060
first we analyze the database types,

252
00:13:24,060 --> 00:13:27,000
like all the data types,
we analyze the data,

253
00:13:27,000 --> 00:13:30,420
we analyze what is binary
and what is non-binary,

254
00:13:30,420 --> 00:13:32,760
we identified and marked down,

255
00:13:32,760 --> 00:13:34,440
and came up with an approach to migrate

256
00:13:34,440 --> 00:13:38,670
all those binary data from
our Oracle Database to S3.

257
00:13:38,670 --> 00:13:42,660
Now, what I have been left
with is all the non-binary data

258
00:13:42,660 --> 00:13:45,060
which are sitting on the Oracle.

259
00:13:45,060 --> 00:13:50,040
There comes how I will transfer
all those non-binary data

260
00:13:50,040 --> 00:13:51,420
from Oracle to Postgres,

261
00:13:51,420 --> 00:13:54,910
so challenge is Postgres doesn't
support all the data types

262
00:13:55,890 --> 00:13:57,453
in comparison to Oracle.

263
00:13:58,410 --> 00:14:00,990
Thus, the team came up with an approach.

264
00:14:00,990 --> 00:14:03,420
There were different tools available to us

265
00:14:03,420 --> 00:14:06,660
to run some analysis
and generate a report,

266
00:14:06,660 --> 00:14:10,530
and we did a comparison
between a data type,

267
00:14:10,530 --> 00:14:13,710
let's say, a number data
type in Oracle Database

268
00:14:13,710 --> 00:14:15,540
to a numeric data type in Postgres,

269
00:14:15,540 --> 00:14:17,640
how that conversion can happen.

270
00:14:17,640 --> 00:14:19,740
You will always end up with challenges

271
00:14:19,740 --> 00:14:24,150
when there are precisions
kind of included in your data.

272
00:14:24,150 --> 00:14:25,440
If you have decimal values,

273
00:14:25,440 --> 00:14:30,440
if you have, like,
non-full numeric values,

274
00:14:30,540 --> 00:14:32,940
that is always a risk
when you migrate it over

275
00:14:32,940 --> 00:14:35,163
to your Postgres engine.

276
00:14:36,390 --> 00:14:38,940
So with all these challenges,

277
00:14:38,940 --> 00:14:43,940
once we were done with all
this risk mitigation approach,

278
00:14:43,980 --> 00:14:47,790
we went to our solution details.

279
00:14:47,790 --> 00:14:50,790
So all those database workloads

280
00:14:50,790 --> 00:14:53,340
which were independent of each other,

281
00:14:53,340 --> 00:14:56,400
what we did is we simplified
them out of these databases

282
00:14:56,400 --> 00:14:59,310
from the central core
database, Oracle Database,

283
00:14:59,310 --> 00:15:02,850
and we created individual
dedicated Postgres databases

284
00:15:02,850 --> 00:15:05,010
for those application workload.

285
00:15:05,010 --> 00:15:08,460
That help us to trim the fat out of it.

286
00:15:08,460 --> 00:15:10,920
Now, comes to the core meat, correct?

287
00:15:10,920 --> 00:15:14,550
That still has close to 60
terabytes of data for me

288
00:15:14,550 --> 00:15:17,673
with all the complex data type columns.

289
00:15:18,690 --> 00:15:20,100
Not to worry.

290
00:15:20,100 --> 00:15:22,830
The next set of challenge was
there were a lot of tables

291
00:15:22,830 --> 00:15:27,240
which were used by ETLs,
staging tables, no primary key.

292
00:15:27,240 --> 00:15:29,370
So good luck with your data transfer

293
00:15:29,370 --> 00:15:33,630
when any tool doesn't understand
what is the reliability

294
00:15:33,630 --> 00:15:36,510
of the transfer of these datasets

295
00:15:36,510 --> 00:15:38,460
which do not have any primary key,

296
00:15:38,460 --> 00:15:40,770
whether it has been successfully migrated

297
00:15:40,770 --> 00:15:42,303
to the destination or not.

298
00:15:43,560 --> 00:15:48,000
So to achieve this data transfer
from Oracle to Postgres,

299
00:15:48,000 --> 00:15:52,620
we leveraged AWS's data DMS solution.

300
00:15:52,620 --> 00:15:56,160
The tool comes in kind of a lot

301
00:15:56,160 --> 00:15:58,230
of out-of-the-box configuration,

302
00:15:58,230 --> 00:16:00,990
but for us, we had to do
a lot of customization,

303
00:16:00,990 --> 00:16:04,230
particularly on busy
transactional databases

304
00:16:04,230 --> 00:16:07,410
such as which was supporting
50,000 transactions per second.

305
00:16:07,410 --> 00:16:12,410
It was always a battle with
how to configure the DMS

306
00:16:12,429 --> 00:16:16,260
up to a kinda maximum and correct point

307
00:16:16,260 --> 00:16:18,030
so that it supports that transaction rate,

308
00:16:18,030 --> 00:16:21,090
because, one, I cannot put a
load on my source database,

309
00:16:21,090 --> 00:16:23,370
because I cannot impact my business.

310
00:16:23,370 --> 00:16:26,820
It has to run as usual,

311
00:16:26,820 --> 00:16:28,560
but at the same time, I need to maintain

312
00:16:28,560 --> 00:16:32,910
my DMS transfer rate so that
I'm not lagging behind a lot

313
00:16:32,910 --> 00:16:35,550
to try move that data into the Postgres,

314
00:16:35,550 --> 00:16:37,590
because if your lag is too much,

315
00:16:37,590 --> 00:16:40,980
then we all all know
that the DMS will break,

316
00:16:40,980 --> 00:16:44,310
because it will find it
difficult to keep track

317
00:16:44,310 --> 00:16:47,039
of those thinking points,
and at one point of time,

318
00:16:47,039 --> 00:16:48,770
it will just just break up,

319
00:16:48,770 --> 00:16:51,299
and it will say, "I am way behind in this,

320
00:16:51,299 --> 00:16:54,120
so please come back and start me again."

321
00:16:54,120 --> 00:16:55,470
So there were certain challenges

322
00:16:55,470 --> 00:16:57,963
on that data transfer exercise.

323
00:16:58,950 --> 00:17:03,630
And I already mentioned there
were not a one-to-one mapping

324
00:17:03,630 --> 00:17:05,670
when we were talking about converting

325
00:17:05,670 --> 00:17:09,120
a table data type column
from Oracle to Postgres.

326
00:17:09,120 --> 00:17:10,440
There were certain data types

327
00:17:10,440 --> 00:17:12,090
which were not supported on Postgres,

328
00:17:12,090 --> 00:17:15,330
there were certain data
types which were created

329
00:17:15,330 --> 00:17:17,490
or designed in a little bit
different way in Postgres

330
00:17:17,490 --> 00:17:19,170
compared to Oracle.

331
00:17:19,170 --> 00:17:24,170
So how can I know when I
have close to 50 schemas

332
00:17:24,397 --> 00:17:26,400
across 15 databases,

333
00:17:26,400 --> 00:17:28,770
and if I take all my lifecycle

334
00:17:28,770 --> 00:17:30,300
of that particular environment,

335
00:17:30,300 --> 00:17:33,330
if I duplicate it across
five different environments,

336
00:17:33,330 --> 00:17:37,440
I'm talking about close
to 100 database instances

337
00:17:37,440 --> 00:17:40,200
with 200 to 300 schemas.

338
00:17:40,200 --> 00:17:43,893
So what we did is we,
again, went back to AWS,

339
00:17:45,060 --> 00:17:47,110
we used their SCT tool

340
00:17:48,360 --> 00:17:51,090
to do initial schema conversion.

341
00:17:51,090 --> 00:17:53,160
Now, what this tool will do,

342
00:17:53,160 --> 00:17:55,860
it will look into your source schemas,

343
00:17:55,860 --> 00:17:59,070
it will go through and
analyze and translate

344
00:17:59,070 --> 00:18:02,280
all the individual database objects

345
00:18:02,280 --> 00:18:03,900
which are on Oracle side.

346
00:18:03,900 --> 00:18:07,530
You can take tables, be
it views, your procedures,

347
00:18:07,530 --> 00:18:10,680
your Oracle functions,

348
00:18:10,680 --> 00:18:13,620
and then what it will do
is it'll run an assessment,

349
00:18:13,620 --> 00:18:18,620
and it will generate a report
at the end of the execution.

350
00:18:18,660 --> 00:18:21,930
And what it will do is it
will basically provide you

351
00:18:21,930 --> 00:18:22,830
three information.

352
00:18:22,830 --> 00:18:27,240
One, how much out-of-the-box
conversion can be done.

353
00:18:27,240 --> 00:18:29,850
For example, it can
generate a script for you,

354
00:18:29,850 --> 00:18:33,577
and it will tell that out of
these 100 database objects,

355
00:18:33,577 --> 00:18:36,810
"70 of them are straightaway
one-to-one conversion,

356
00:18:36,810 --> 00:18:38,820
I will generate a script for you."

357
00:18:38,820 --> 00:18:40,710
And then for stored procedures,

358
00:18:40,710 --> 00:18:43,410
or for some other complex data types,

359
00:18:43,410 --> 00:18:45,660
it will give you an option,

360
00:18:45,660 --> 00:18:48,210
but you have to manually create a script

361
00:18:48,210 --> 00:18:50,190
or customize your configuration

362
00:18:50,190 --> 00:18:52,110
to support that transformation.

363
00:18:52,110 --> 00:18:53,790
And then for the rest third,

364
00:18:53,790 --> 00:18:56,550
which is the most challenging group,

365
00:18:56,550 --> 00:19:01,290
is it will say that it cannot
be converted by this SCT tool.

366
00:19:01,290 --> 00:19:03,570
So what you have to do, you have to go,

367
00:19:03,570 --> 00:19:05,220
you have to review them manually,

368
00:19:05,220 --> 00:19:10,140
and then rewrote all
those database objects,

369
00:19:10,140 --> 00:19:11,190
procedures as function,

370
00:19:11,190 --> 00:19:15,753
or all those logic in a
Postgres-compatible SQL.

371
00:19:16,830 --> 00:19:21,000
So again, what we did is we
did not do everything at once.

372
00:19:21,000 --> 00:19:24,030
We did all these in an iterative manner,

373
00:19:24,030 --> 00:19:28,020
and then we did all this,
we did this in phases,

374
00:19:28,020 --> 00:19:30,873
and then slowly moved one pace at a time,

375
00:19:32,220 --> 00:19:35,550
which is very critical if
your database is complex,

376
00:19:35,550 --> 00:19:37,323
if your database size is larger.

377
00:19:42,180 --> 00:19:45,693
Now, this journey took
us close to three years,

378
00:19:46,650 --> 00:19:49,860
and I'm sure everyone
would be interested to know

379
00:19:49,860 --> 00:19:52,440
that three years of investment

380
00:19:52,440 --> 00:19:54,300
of working through this modernization,

381
00:19:54,300 --> 00:19:56,760
working through this cloud transformation,

382
00:19:56,760 --> 00:20:00,150
at the end of the day, what
it gives back to us, correct?

383
00:20:00,150 --> 00:20:03,750
That's where the buck
stops, is the dollar value,

384
00:20:03,750 --> 00:20:07,530
but sometimes the dollar
value is not direct, correct?

385
00:20:07,530 --> 00:20:10,140
It can be direct, it can be indirect,

386
00:20:10,140 --> 00:20:11,943
particularly in the operation world.

387
00:20:12,810 --> 00:20:14,400
Most of the time, it is indirect.

388
00:20:14,400 --> 00:20:18,693
So how we can justify that
return value out of this journey?

389
00:20:19,890 --> 00:20:22,290
So what are all the benefits we received

390
00:20:22,290 --> 00:20:24,483
as part of this journey?

391
00:20:25,410 --> 00:20:27,540
What we have observed so far,

392
00:20:27,540 --> 00:20:30,090
after working through this modernization

393
00:20:30,090 --> 00:20:32,370
and completing this journey,

394
00:20:32,370 --> 00:20:37,370
that we have achieved and
improved operational efficiency.

395
00:20:37,980 --> 00:20:41,670
It helped us to sustain the
same performance throughput,

396
00:20:41,670 --> 00:20:44,100
in some cases, even better.

397
00:20:44,100 --> 00:20:48,150
It helped us to have a
optimized cost strategy.

398
00:20:48,150 --> 00:20:52,530
As we started using a lot of
managed database solutions

399
00:20:52,530 --> 00:20:54,120
provided by AWS,

400
00:20:54,120 --> 00:20:56,820
it was easy for our FinOps team to come up

401
00:20:56,820 --> 00:20:59,850
with a better prediction and forecast

402
00:20:59,850 --> 00:21:02,283
so that we can manage
our cost in a better way.

403
00:21:03,990 --> 00:21:07,500
We were able to engage
our site reliability team

404
00:21:07,500 --> 00:21:10,320
and DevOps team to focus
on more automation,

405
00:21:10,320 --> 00:21:14,550
more scripting, so that
we can leverage those

406
00:21:14,550 --> 00:21:17,250
and optimize our cloud management.

407
00:21:17,250 --> 00:21:19,590
Those procedures were
pretty proven procedures.

408
00:21:19,590 --> 00:21:22,530
Some we were able to
leverage provided by AWS,

409
00:21:22,530 --> 00:21:26,010
some were built and
developed by our own teams,

410
00:21:26,010 --> 00:21:29,490
and that help us to enforce and expedite

411
00:21:29,490 --> 00:21:32,193
our automation track.

412
00:21:33,030 --> 00:21:36,300
The next one, what I'm
going to highlight over here

413
00:21:36,300 --> 00:21:39,570
is with all this,

414
00:21:39,570 --> 00:21:42,150
we were not only able to achieve

415
00:21:42,150 --> 00:21:45,180
an improved RTO and RPO capabilities,

416
00:21:45,180 --> 00:21:49,830
but we were also able to have
a better sustainable strategy.

417
00:21:49,830 --> 00:21:51,420
How we were able to achieve it?

418
00:21:51,420 --> 00:21:55,290
We were able to create
a plan in some cases

419
00:21:55,290 --> 00:22:00,290
where we can use multi-ASE
across the same region.

420
00:22:00,510 --> 00:22:04,980
And then we created a kind
of East-West region strategy

421
00:22:04,980 --> 00:22:08,460
where we can use West as our
disaster recovery solution

422
00:22:08,460 --> 00:22:10,260
or sustainable recovery solution,

423
00:22:10,260 --> 00:22:13,083
and then use East region
as our primary production.

424
00:22:14,940 --> 00:22:18,330
And then by leveraging
AWS-managed solution,

425
00:22:18,330 --> 00:22:20,790
what we were able to
achieve is an improved

426
00:22:20,790 --> 00:22:22,380
security footprint.

427
00:22:22,380 --> 00:22:25,140
We did not have to worry a lot

428
00:22:25,140 --> 00:22:27,960
the way we used to think
about our security compliance

429
00:22:27,960 --> 00:22:30,423
when we were running
an on-prem with Oracle.

430
00:22:31,740 --> 00:22:36,740
Scaling opportunity, again,
it enabled us to grow faster

431
00:22:36,840 --> 00:22:39,330
if there were any faster
go-to-market requirements,

432
00:22:39,330 --> 00:22:42,300
so that we are able to scale faster there,

433
00:22:42,300 --> 00:22:45,270
and then we were able to
support our business growth.

434
00:22:45,270 --> 00:22:49,320
There was no pushback from
technology to business,

435
00:22:49,320 --> 00:22:52,620
saying that if there is a new
requirement or new workload,

436
00:22:52,620 --> 00:22:54,060
or it is going to take us months.

437
00:22:54,060 --> 00:22:55,230
No, it's days.

438
00:22:55,230 --> 00:22:56,700
You give us the requirement,

439
00:22:56,700 --> 00:22:59,370
we run through the new workload pattern

440
00:22:59,370 --> 00:23:01,320
in our performance test environment.

441
00:23:01,320 --> 00:23:03,930
Once it gets certified,
you are ready to go

442
00:23:03,930 --> 00:23:06,780
to move it to production
as long as you are hitting

443
00:23:06,780 --> 00:23:08,940
all the checklist and check marks

444
00:23:08,940 --> 00:23:10,440
for your production readiness.

445
00:23:11,670 --> 00:23:15,210
And then the next best thing
which happened, as I mentioned,

446
00:23:15,210 --> 00:23:19,170
that every time it is
not direct dollar impact,

447
00:23:19,170 --> 00:23:23,910
with operation is a better and
proactive incident management

448
00:23:23,910 --> 00:23:27,420
with a lot of automation
and auto-remediation.

449
00:23:27,420 --> 00:23:31,410
So when everything was
running on managed solutions,

450
00:23:31,410 --> 00:23:34,110
we were able to focus on
our incident management,

451
00:23:34,110 --> 00:23:35,250
on our runbook,

452
00:23:35,250 --> 00:23:38,010
we were able to automate our SOPs.

453
00:23:38,010 --> 00:23:40,960
In a lot of cases, we are
able to eliminate those SOPs

454
00:23:41,940 --> 00:23:44,640
with a better modernization
on the application

455
00:23:44,640 --> 00:23:45,603
and database layer.

456
00:23:49,680 --> 00:23:53,520
Now, I would like to highlight
what kind of infrastructure

457
00:23:53,520 --> 00:23:57,390
we were running before we were
running on Aurora Postgres,

458
00:23:57,390 --> 00:24:02,280
and I am trying to say
that it is cost effective,

459
00:24:02,280 --> 00:24:05,250
it is operational efficiency effective,

460
00:24:05,250 --> 00:24:08,010
why it gives us a better sleep at night

461
00:24:08,010 --> 00:24:11,070
than those sleepless days in on-prem.

462
00:24:11,070 --> 00:24:13,050
So before Aurora Postgres,

463
00:24:13,050 --> 00:24:15,540
we were running our database workload

464
00:24:15,540 --> 00:24:17,463
on a Oracle RAC cluster.

465
00:24:18,480 --> 00:24:21,030
Earlier, we were running
on the extra data.

466
00:24:21,030 --> 00:24:22,560
Now, after we moved to AWS,

467
00:24:22,560 --> 00:24:26,040
we are running on the
custom Oracle RAC on AWS.

468
00:24:26,040 --> 00:24:29,010
We were running on a RAC
cluster with nine nodes,

469
00:24:29,010 --> 00:24:32,760
and all those nodes at that point of time,

470
00:24:32,760 --> 00:24:35,040
were running on the largest instance type,

471
00:24:35,040 --> 00:24:37,650
whatever AWS was providing at that time.

472
00:24:37,650 --> 00:24:40,620
All of them were 32xlarge.

473
00:24:40,620 --> 00:24:43,290
This is just to manage
the workload distribution

474
00:24:43,290 --> 00:24:45,870
so that, one, I'm not creating
a single point of failure,

475
00:24:45,870 --> 00:24:48,210
and second, is the load
is well-distributed.

476
00:24:48,210 --> 00:24:51,180
We all know, with Oracle,
you can create services,

477
00:24:51,180 --> 00:24:53,400
distribute your application workload,

478
00:24:53,400 --> 00:24:56,100
but this is a huge server.

479
00:24:56,100 --> 00:25:00,270
Not only your monthly billing
meter goes high with this,

480
00:25:00,270 --> 00:25:02,310
but also, it's a operational headache.

481
00:25:02,310 --> 00:25:03,990
You think about patching,

482
00:25:03,990 --> 00:25:08,070
you think about managing all
these nine-node RAC cluster

483
00:25:08,070 --> 00:25:08,910
day in, day out,

484
00:25:08,910 --> 00:25:11,973
it's a nightmare for support engineers.

485
00:25:13,110 --> 00:25:16,470
I mentioned about
application modernization.

486
00:25:16,470 --> 00:25:19,680
To support all those
hundreds of applications,

487
00:25:19,680 --> 00:25:23,523
we are running 100-some
WebLogic VMs there.

488
00:25:24,750 --> 00:25:28,290
The Oracle rules engine
cluster, the Oracle Service Bus,

489
00:25:28,290 --> 00:25:30,540
those were complex and high maintenance.

490
00:25:30,540 --> 00:25:32,730
It required a lot of customization

491
00:25:32,730 --> 00:25:34,950
to support our application requirements,

492
00:25:34,950 --> 00:25:38,280
and support engineers were
having a lot of challenges

493
00:25:38,280 --> 00:25:39,633
to manage and support them.

494
00:25:42,150 --> 00:25:44,310
With moving to AWS Cloud,

495
00:25:44,310 --> 00:25:47,220
there were kind of technical limitation

496
00:25:47,220 --> 00:25:50,520
around how many nodes you can include

497
00:25:50,520 --> 00:25:53,197
or add in a new Oracle RAC cluster on AWS.

498
00:25:54,400 --> 00:25:56,520
Let's say, tomorrow, we got requirement

499
00:25:56,520 --> 00:25:58,470
for a new application, new workload,

500
00:25:58,470 --> 00:26:00,900
or existing workload is going to increase

501
00:26:00,900 --> 00:26:03,030
because of new customer rollout,

502
00:26:03,030 --> 00:26:05,490
it was always a challenge
and hustle with AWS

503
00:26:05,490 --> 00:26:07,320
to understand how I can manage

504
00:26:07,320 --> 00:26:09,183
that new workload requirement.

505
00:26:10,050 --> 00:26:12,333
Now, the life after Aurora Postgres.

506
00:26:14,220 --> 00:26:19,220
What we did is we designed
these Aurora Postgres instances

507
00:26:19,440 --> 00:26:22,290
with a writer and reader instance concept.

508
00:26:22,290 --> 00:26:24,990
We not only were able to
reduce the number of nodes,

509
00:26:24,990 --> 00:26:27,630
or in this case, number of instances

510
00:26:27,630 --> 00:26:29,910
after we moved to Aurora Postgres,

511
00:26:29,910 --> 00:26:33,240
we were able to reduce the number of nodes

512
00:26:33,240 --> 00:26:36,030
of, like, nine-nodes Oracle RAC cluster

513
00:26:36,030 --> 00:26:40,620
to a two-instance Aurora
Postgres database instance.

514
00:26:40,620 --> 00:26:45,620
So we had one writer instance
where all the writer, like,

515
00:26:45,810 --> 00:26:48,310
insert update, delete workload was going

516
00:26:49,260 --> 00:26:50,640
from all the applications,

517
00:26:50,640 --> 00:26:52,590
and then we created a reader instance

518
00:26:52,590 --> 00:26:54,240
to support any cache refresh,

519
00:26:54,240 --> 00:26:56,550
any reporting type of requirements,

520
00:26:56,550 --> 00:26:59,520
or any application workload requirements

521
00:26:59,520 --> 00:27:02,270
where there are selector read
operations were required.

522
00:27:03,390 --> 00:27:05,730
Again, tomorrow, if a workload comes in,

523
00:27:05,730 --> 00:27:07,530
more reporting requirement,

524
00:27:07,530 --> 00:27:10,230
more businesses need more analytical data,

525
00:27:10,230 --> 00:27:12,690
we can easily horizontally
scale this cluster

526
00:27:12,690 --> 00:27:15,660
by adding more reader instances.

527
00:27:15,660 --> 00:27:19,050
It is not going to take us
five different discussions

528
00:27:19,050 --> 00:27:20,046
with AWS,

529
00:27:20,046 --> 00:27:23,490
10 different discussions for weeks.

530
00:27:23,490 --> 00:27:27,210
It's just only going to need
a select click of a button

531
00:27:27,210 --> 00:27:30,870
or run a pipeline to add one
more reader instance there,

532
00:27:30,870 --> 00:27:33,570
so horizontal scaling was never an issue

533
00:27:33,570 --> 00:27:35,470
after we moved to our Aurora Postgres.

534
00:27:36,480 --> 00:27:39,720
Headless DR, that's a very nice feature

535
00:27:39,720 --> 00:27:42,210
provided by Aurora Postgres.

536
00:27:42,210 --> 00:27:47,210
Now, what we did is these Aurora
Postgres database instances

537
00:27:47,280 --> 00:27:49,743
were huge to support this workload.

538
00:27:50,820 --> 00:27:52,740
Again, when we moved to Aurora Postgres,

539
00:27:52,740 --> 00:27:54,570
we were running on the largest instance

540
00:27:54,570 --> 00:27:56,340
provided by AWS on this.

541
00:27:56,340 --> 00:28:01,340
So creating another compute
and the storage on DR

542
00:28:01,620 --> 00:28:04,470
just to support the DR environment

543
00:28:04,470 --> 00:28:08,100
was a cost of business for us.

544
00:28:08,100 --> 00:28:11,430
So what we did is we created or designed

545
00:28:11,430 --> 00:28:14,310
our DR solutions wherever
possible and applicable

546
00:28:14,310 --> 00:28:18,930
with minimizing our availability risk

547
00:28:18,930 --> 00:28:20,820
to use the headless DR.

548
00:28:20,820 --> 00:28:22,440
So what is headless DR?

549
00:28:22,440 --> 00:28:25,023
Headless DR is, as we
all know, like Aurora,

550
00:28:25,860 --> 00:28:29,730
you can replicate or clone the storage

551
00:28:29,730 --> 00:28:32,400
onto your DR recovery region.

552
00:28:32,400 --> 00:28:35,550
In our case, it was, let's
say, on the west side,

553
00:28:35,550 --> 00:28:38,550
but you don't need to add
any compute over there,

554
00:28:38,550 --> 00:28:41,220
so you save a huge cost.

555
00:28:41,220 --> 00:28:45,630
Whenever you need your
compute for a failover reason

556
00:28:45,630 --> 00:28:47,010
or for any other reason,

557
00:28:47,010 --> 00:28:50,190
you can add a compute to those instances

558
00:28:50,190 --> 00:28:51,420
in a click of button,

559
00:28:51,420 --> 00:28:53,880
and your DR database
instances will be ready

560
00:28:53,880 --> 00:28:55,173
and available in minutes.

561
00:28:56,160 --> 00:28:58,170
So that's what we did to manage the cost.

562
00:28:58,170 --> 00:29:00,570
Wherever DR instance
compute is not needed,

563
00:29:00,570 --> 00:29:03,633
we used all headless DR for those.

564
00:29:04,530 --> 00:29:06,960
And with Aurora Postgres, what we achieved

565
00:29:06,960 --> 00:29:09,720
was subsequent data sync lag.

566
00:29:09,720 --> 00:29:11,943
It's in milliseconds,

567
00:29:12,780 --> 00:29:16,920
so if you are talking about
doing a insert operation

568
00:29:16,920 --> 00:29:18,450
in your writer instance,

569
00:29:18,450 --> 00:29:21,870
and doing a read operation in reader,

570
00:29:21,870 --> 00:29:24,540
it's less than a second,
it's in milliseconds.

571
00:29:24,540 --> 00:29:29,160
So unless you have very
time-critical operations,

572
00:29:29,160 --> 00:29:32,250
you can very easily isolate
your write and read,

573
00:29:32,250 --> 00:29:34,290
and move them into two
different instances,

574
00:29:34,290 --> 00:29:37,080
or as many instances as you want.

575
00:29:37,080 --> 00:29:38,730
Same goes for DR.

576
00:29:38,730 --> 00:29:41,102
If you need it, you can
run a lot of reporting

577
00:29:41,102 --> 00:29:42,390
from your DR engine.

578
00:29:42,390 --> 00:29:43,950
You add a computer over there,

579
00:29:43,950 --> 00:29:47,853
even the cross-region replication
is pretty quick there.

580
00:29:49,260 --> 00:29:51,180
So that reduced a lot of headache.

581
00:29:51,180 --> 00:29:52,740
When we were on Oracle RAC,

582
00:29:52,740 --> 00:29:55,320
I had to maintain a primary instance,

583
00:29:55,320 --> 00:29:57,630
I had to maintain a local standby,

584
00:29:57,630 --> 00:30:00,410
and then I had to maintain
a remote standby for my DR,

585
00:30:00,410 --> 00:30:03,390
so we reduced a lot of
redundancy over that

586
00:30:03,390 --> 00:30:06,993
with the advantage of AWS automation.

587
00:30:08,490 --> 00:30:12,330
Now, before Aurora Postgres,
when we were running on Oracle,

588
00:30:12,330 --> 00:30:15,720
I had mentioned earlier,
patching was a nightmare,

589
00:30:15,720 --> 00:30:18,750
even on the extra data era,

590
00:30:18,750 --> 00:30:22,080
or even when we were running
on AWS Custom Oracle RAC.

591
00:30:22,080 --> 00:30:24,450
It used to be a weekend business.

592
00:30:24,450 --> 00:30:26,220
It used to run for hours,

593
00:30:26,220 --> 00:30:28,500
and you don't know until
that whole patching ends

594
00:30:28,500 --> 00:30:30,260
whether you are successful
or not successful,

595
00:30:30,260 --> 00:30:32,703
so it was a use risk for our business.

596
00:30:33,720 --> 00:30:38,370
Runbooks to manage operational
risk and operational task.

597
00:30:38,370 --> 00:30:40,410
So we were having very limited option

598
00:30:40,410 --> 00:30:42,120
to create runbooks when we were running

599
00:30:42,120 --> 00:30:44,313
on our Oracle RAC clusters.

600
00:30:45,757 --> 00:30:50,520
It was giving us very
limited options over there.

601
00:30:50,520 --> 00:30:51,720
License management,

602
00:30:51,720 --> 00:30:54,990
we all know Oracle is
not cheap in the market.

603
00:30:54,990 --> 00:30:57,210
You have to do annual license renewal,

604
00:30:57,210 --> 00:30:59,070
you need to do annual support renewal,

605
00:30:59,070 --> 00:31:01,353
so it has its own cost implication,

606
00:31:02,370 --> 00:31:04,350
and you get locked down
with vendor software.

607
00:31:04,350 --> 00:31:05,970
You don't have any freedoms,

608
00:31:05,970 --> 00:31:08,250
you wait for their patches,

609
00:31:08,250 --> 00:31:12,750
you have their lockdown with Oracle,

610
00:31:12,750 --> 00:31:15,570
so Oracle basically manages and drive

611
00:31:15,570 --> 00:31:18,120
what you have to do with
your application workload.

612
00:31:20,820 --> 00:31:25,050
To meet our compliance, we
used to do annual DR test,

613
00:31:25,050 --> 00:31:29,610
and with all these, like,
remote standby failover,

614
00:31:29,610 --> 00:31:31,860
bringing up that data,

615
00:31:31,860 --> 00:31:33,810
it used to take hours, as I mentioned,

616
00:31:33,810 --> 00:31:37,470
so we were having, like,
larger RTO and RPOs.

617
00:31:38,520 --> 00:31:42,240
Now, after Aurora Postgres,
what we were able to do,

618
00:31:42,240 --> 00:31:45,120
we all know, with AWS-managed solution,

619
00:31:45,120 --> 00:31:48,030
we can enable our automated patching.

620
00:31:48,030 --> 00:31:51,900
In a lot of cases, all the minor
patchings are auto-enabled,

621
00:31:51,900 --> 00:31:54,153
which means if you use those flags,

622
00:31:56,160 --> 00:31:58,650
how you have defined
your maintenance window,

623
00:31:58,650 --> 00:32:02,310
AWS is going to run automated
patching on your databases

624
00:32:02,310 --> 00:32:04,413
depending on how your business runs.

625
00:32:05,340 --> 00:32:09,060
Now, all our DBAs, data architects,

626
00:32:09,060 --> 00:32:11,730
they can really focus on
application designing,

627
00:32:11,730 --> 00:32:13,860
their application database designing,

628
00:32:13,860 --> 00:32:15,900
rather than focusing on how to manage

629
00:32:15,900 --> 00:32:18,510
those Oracle RAC clusters.

630
00:32:18,510 --> 00:32:22,170
That gave them a good platform
to learn new technologies,

631
00:32:22,170 --> 00:32:25,983
to bring development in their
professional growth, also.

632
00:32:27,360 --> 00:32:28,980
And with what we were able to do,

633
00:32:28,980 --> 00:32:32,040
is as we were able to
implement site reliability

634
00:32:32,040 --> 00:32:34,740
engineering practices on
our application stack,

635
00:32:34,740 --> 00:32:38,160
we were able to also roll
out our database reliability

636
00:32:38,160 --> 00:32:39,060
engineering practice.

637
00:32:39,060 --> 00:32:44,010
So DBAs were not considered as
those orthodox DBAs anymore.

638
00:32:44,010 --> 00:32:46,350
They were working as reliability engineers

639
00:32:46,350 --> 00:32:50,160
to ensure reliability and
stability of the database

640
00:32:50,160 --> 00:32:52,020
rather than thinking about patching

641
00:32:52,020 --> 00:32:54,870
and all those regular operational tasks,

642
00:32:54,870 --> 00:32:58,290
which we used to do on
Oracle RAC clusters.

643
00:32:58,290 --> 00:33:00,060
Easy to scale, as I mentioned.

644
00:33:00,060 --> 00:33:04,230
Horizontal scaling is a button away.

645
00:33:04,230 --> 00:33:06,960
You click a button or you
run your automation pipeline,

646
00:33:06,960 --> 00:33:08,940
you can scale it out horizontally

647
00:33:08,940 --> 00:33:10,653
as per your application need.

648
00:33:11,520 --> 00:33:14,100
Significant reduction in RPO and RTO.

649
00:33:14,100 --> 00:33:16,050
As I said, with Aurora Postgres,

650
00:33:16,050 --> 00:33:19,350
your replication sync lag is very minimum.

651
00:33:19,350 --> 00:33:23,010
You have a better cross-region
replication solution

652
00:33:23,010 --> 00:33:24,930
available by AWS itself,

653
00:33:24,930 --> 00:33:28,413
so it reduced our RPO and RTO factors.

654
00:33:30,060 --> 00:33:32,760
When we tried to compare how much effort

655
00:33:32,760 --> 00:33:36,300
we were putting on Oracle
patching and managing Oracle

656
00:33:36,300 --> 00:33:38,220
versus Aurora Postgres,

657
00:33:38,220 --> 00:33:42,210
we have observed more than 30%
reduction in patching effort,

658
00:33:42,210 --> 00:33:46,350
which is huge, not only from
a business point of view,

659
00:33:46,350 --> 00:33:49,080
but also for a database team,

660
00:33:49,080 --> 00:33:52,020
how much challenges it reduce,

661
00:33:52,020 --> 00:33:54,810
how much sleepless nights it can reduce

662
00:33:54,810 --> 00:33:57,150
out of your calendar.

663
00:33:57,150 --> 00:34:00,510
And then as I mentioned,
for security compliance

664
00:34:00,510 --> 00:34:02,910
and for other compliances,
regulatory compliances,

665
00:34:02,910 --> 00:34:07,290
we used to test our DR
capability every year.

666
00:34:07,290 --> 00:34:11,310
What we have achieved is we
have observed that even similar,

667
00:34:11,310 --> 00:34:15,330
more than 30% reduction
in those DR exercises

668
00:34:15,330 --> 00:34:16,530
across the teams.

669
00:34:16,530 --> 00:34:19,060
Not only database team, be
it site reliability team,

670
00:34:19,060 --> 00:34:21,720
be it DevOps, be it application team,

671
00:34:21,720 --> 00:34:24,273
we saw a huge improvement over those.

672
00:34:27,090 --> 00:34:30,360
Now, we completed this journey,

673
00:34:30,360 --> 00:34:31,890
now, the platform is there,

674
00:34:31,890 --> 00:34:33,780
the foundation migration was done,

675
00:34:33,780 --> 00:34:38,780
foundation is running stable
and well on AWS Cloud,

676
00:34:39,180 --> 00:34:42,150
we are using most of
the AWS-managed solution

677
00:34:42,150 --> 00:34:45,360
to enable us to move faster to achieve

678
00:34:45,360 --> 00:34:47,610
a better go-to-market strategy.

679
00:34:47,610 --> 00:34:51,813
Now, how are we scaling with AWS?

680
00:34:52,920 --> 00:34:54,900
So as mentioned earlier,

681
00:34:54,900 --> 00:34:58,020
we are still able to manage the same,

682
00:34:58,020 --> 00:35:01,560
or even, in some cases, more
as we are growing more than

683
00:35:01,560 --> 00:35:04,680
or close to 50,000
transactions per second.

684
00:35:04,680 --> 00:35:08,460
This database in total
handles close to 5 billion

685
00:35:08,460 --> 00:35:10,110
database transactions per day,

686
00:35:10,110 --> 00:35:12,183
so you can imagine how busy is this.

687
00:35:13,650 --> 00:35:17,370
So we needed a hyperscaler enabler for us

688
00:35:17,370 --> 00:35:22,370
to support this workload,
and we found it with AWS.

689
00:35:23,670 --> 00:35:27,480
We started this journey with
a very defined strategy.

690
00:35:27,480 --> 00:35:30,810
There was no opponents,
there are no loose endpoints.

691
00:35:30,810 --> 00:35:33,480
We knew exactly what was the challenge.

692
00:35:33,480 --> 00:35:35,520
We knew exactly what was the risk.

693
00:35:35,520 --> 00:35:39,030
We knew exactly what risk
mitigation path we have,

694
00:35:39,030 --> 00:35:41,130
and what are the solution
we are going to use

695
00:35:41,130 --> 00:35:42,483
for each of them.

696
00:35:45,227 --> 00:35:48,600
We were able to get freed
out from this vendor locking.

697
00:35:48,600 --> 00:35:53,100
We were more open-source
technology adapter right now.

698
00:35:53,100 --> 00:35:56,670
We were able to modernize
all our application stack.

699
00:35:56,670 --> 00:36:00,423
We were able to leverage
event-driven Kafka architecture,

700
00:36:01,500 --> 00:36:05,580
which basically opened up a
lot of other options for us.

701
00:36:05,580 --> 00:36:07,680
And then what, at the end of the day,

702
00:36:07,680 --> 00:36:09,270
it helped us to achieve,

703
00:36:09,270 --> 00:36:12,030
is our core strength is data management,

704
00:36:12,030 --> 00:36:16,230
because we get so many data
points from so many claims,

705
00:36:16,230 --> 00:36:17,910
so many different types of data,

706
00:36:17,910 --> 00:36:22,290
because every claim is
different by its nature.

707
00:36:22,290 --> 00:36:24,390
And with the complexity slowly increasing

708
00:36:24,390 --> 00:36:25,770
with all the automobiles,

709
00:36:25,770 --> 00:36:27,670
I think it is going to grow even more.

710
00:36:28,680 --> 00:36:32,280
So we focused on our
database modernization,

711
00:36:32,280 --> 00:36:33,510
we identified each layer,

712
00:36:33,510 --> 00:36:36,270
we created a solution and
strategy for each of them,

713
00:36:36,270 --> 00:36:37,923
and we did them in phases.

714
00:36:39,960 --> 00:36:43,260
And after the initial
binary data migration done,

715
00:36:43,260 --> 00:36:46,080
we took the non-binary data,
and in the second phase,

716
00:36:46,080 --> 00:36:49,980
we migrated the databases
from our AWS Oracle

717
00:36:49,980 --> 00:36:53,763
custom RAC cluster to AWS
Aurora Postgres cluster.

718
00:36:55,320 --> 00:36:57,840
As I said, it not only enabled us

719
00:36:57,840 --> 00:36:59,670
to achieve operational efficiency,

720
00:36:59,670 --> 00:37:01,920
better cost mitigation strategy,

721
00:37:01,920 --> 00:37:03,780
it also help us to simplify

722
00:37:03,780 --> 00:37:06,903
both our application architecture
and database architecture.

723
00:37:13,500 --> 00:37:17,340
So this journey was for
years, more than three years.

724
00:37:17,340 --> 00:37:20,370
There were lots of partners with us

725
00:37:20,370 --> 00:37:23,730
who kind of help us to
enable this journey.

726
00:37:23,730 --> 00:37:27,690
We were never being able
to do this by ourselves.

727
00:37:27,690 --> 00:37:31,290
So we leveraged AWS's expertise

728
00:37:31,290 --> 00:37:34,560
whenever we were trying to
implement their solutions.

729
00:37:34,560 --> 00:37:38,490
Then we also leveraged
Percona's Postgres expertise

730
00:37:38,490 --> 00:37:41,010
when we were towards the
end of this migration,

731
00:37:41,010 --> 00:37:43,740
and we needed help on their SQL expertise

732
00:37:43,740 --> 00:37:45,840
and Postgres expertise,

733
00:37:45,840 --> 00:37:49,325
but nonetheless, there was
another critical partner for us,

734
00:37:49,325 --> 00:37:50,400
(Subrat clearing his throat)

735
00:37:50,400 --> 00:37:51,993
which was Cognizant.

736
00:37:53,010 --> 00:37:55,593
They were our strategic
partner from day one.

737
00:37:56,610 --> 00:37:59,700
I have Chandana with me from Cognizant,

738
00:37:59,700 --> 00:38:03,540
so she will be able to give
more details around it,

739
00:38:03,540 --> 00:38:06,150
but nonetheless, it was a joint effort.

740
00:38:06,150 --> 00:38:10,050
We all worked together, we all
worked with a concrete plan,

741
00:38:10,050 --> 00:38:12,870
and at the end of the day,
we were able to achieve

742
00:38:12,870 --> 00:38:15,573
all the accomplishment,
whatever we aimed for.

743
00:38:16,740 --> 00:38:17,733
Thank you.

744
00:38:17,733 --> 00:38:18,566
Chandana?

745
00:38:18,566 --> 00:38:19,866
(audience applauding)

746
00:38:19,866 --> 00:38:21,480
- Thank you, Subrat.

747
00:38:21,480 --> 00:38:23,550
That was tremendous, actually.

748
00:38:23,550 --> 00:38:27,450
So I'm Chandana Paul, and as
you said, I'm from Cognizant.

749
00:38:27,450 --> 00:38:31,620
I manage the relationship
between Cognizant and CCC,

750
00:38:31,620 --> 00:38:36,620
and based out of Chicago, I
almost did not make it here.

751
00:38:36,690 --> 00:38:39,120
You know, the Midwest is slammed, right?

752
00:38:39,120 --> 00:38:43,530
We had 14 inches of snow,
but here I am. (chuckles)

753
00:38:43,530 --> 00:38:47,013
So as a strategic partner of CCC,

754
00:38:48,060 --> 00:38:51,783
we were involved end to
end in this whole journey,

755
00:38:53,130 --> 00:38:55,623
across the phases, you know,

756
00:38:56,700 --> 00:39:00,072
starting from planning all
the way to production cutover.

757
00:39:00,072 --> 00:39:04,980
And this continuity basically
helped with minimizing risk,

758
00:39:04,980 --> 00:39:06,870
with quick issue resolution.

759
00:39:06,870 --> 00:39:10,620
We were, and we are, intimately
aware of the landscape

760
00:39:10,620 --> 00:39:12,900
and the applications, et cetera,

761
00:39:12,900 --> 00:39:15,630
so it's really been a wonderful journey.

762
00:39:15,630 --> 00:39:18,030
And I'd like to give a, maybe, you know,

763
00:39:18,030 --> 00:39:21,153
just a brief about how we were engaged.

764
00:39:22,260 --> 00:39:24,390
This slide actually gives a sampling

765
00:39:24,390 --> 00:39:28,833
of the various areas
that we were involved in.

766
00:39:30,960 --> 00:39:33,750
Some highlights that I
would like to talk about

767
00:39:33,750 --> 00:39:37,440
is one Subrat mentioned,
close teamwork, right?

768
00:39:37,440 --> 00:39:39,960
So there was AWS,

769
00:39:39,960 --> 00:39:43,230
CCC involved, you know,

770
00:39:43,230 --> 00:39:46,980
and Cognizant was involved
together as a single team.

771
00:39:46,980 --> 00:39:50,070
There wasn't any difference,
so it was a one-team effort.

772
00:39:50,070 --> 00:39:52,140
Cognizant was able to bring in mindshare

773
00:39:52,140 --> 00:39:56,010
from similar engagements
that we have accomplished

774
00:39:56,010 --> 00:40:01,010
with other customers, and so together,

775
00:40:01,290 --> 00:40:02,580
we were able to, you know,

776
00:40:02,580 --> 00:40:07,230
minimize risk, resolve issues quickly.

777
00:40:07,230 --> 00:40:08,223
Secondly,

778
00:40:10,860 --> 00:40:14,580
because we were so intimately, you know,

779
00:40:14,580 --> 00:40:17,400
we had so much in-depth
knowledge of the applications

780
00:40:17,400 --> 00:40:20,010
and the various objects,

781
00:40:20,010 --> 00:40:24,000
we were able to put together a strategy.

782
00:40:24,000 --> 00:40:25,440
I mean, you know, do the analysis,

783
00:40:25,440 --> 00:40:29,433
put together a systematic
strategy for remediation,

784
00:40:30,660 --> 00:40:35,430
and that remediation of the applications,

785
00:40:35,430 --> 00:40:36,513
the queries,

786
00:40:37,800 --> 00:40:40,110
the stored procedures, et cetera,

787
00:40:40,110 --> 00:40:43,230
helped in achieving this, you know,

788
00:40:43,230 --> 00:40:47,040
with minimal risk, minimal challenges.

789
00:40:47,040 --> 00:40:49,170
And we were able to resolve issues

790
00:40:49,170 --> 00:40:51,153
as they came up along the way.

791
00:40:52,290 --> 00:40:53,793
On the DevOps front,

792
00:40:55,380 --> 00:41:00,380
the DevOps strategy
was key to accelerating

793
00:41:00,420 --> 00:41:04,170
migration timelines and reducing risk,

794
00:41:04,170 --> 00:41:06,610
and this was achieved through automation

795
00:41:07,890 --> 00:41:10,650
using parallel EKS environment,

796
00:41:10,650 --> 00:41:14,160
bulk deployment across 30 clusters.

797
00:41:14,160 --> 00:41:17,130
And all of this helped to
enable seamless deployment

798
00:41:17,130 --> 00:41:19,593
of over 300 applications,

799
00:41:22,380 --> 00:41:24,300
you know, using these applications

800
00:41:24,300 --> 00:41:28,020
as well as portals, actually,
across EKS environments

801
00:41:28,020 --> 00:41:30,150
with zero downtime.

802
00:41:30,150 --> 00:41:31,650
So this was, you know,

803
00:41:31,650 --> 00:41:33,690
that really helped along the way.

804
00:41:33,690 --> 00:41:35,490
And then Cognizant, of course,

805
00:41:35,490 --> 00:41:38,430
collaborated on the
actual migration efforts.

806
00:41:38,430 --> 00:41:42,120
Subrat talked about using the SCT,

807
00:41:42,120 --> 00:41:45,870
you know, the schema conversion
tool wherever possible,

808
00:41:45,870 --> 00:41:50,700
but there was a large
number of metadata objects

809
00:41:50,700 --> 00:41:52,800
that needed manual intervention, you know,

810
00:41:52,800 --> 00:41:55,770
manual remediation,
customization, et cetera,

811
00:41:55,770 --> 00:41:56,910
so that was achieved,

812
00:41:56,910 --> 00:42:00,330
and then working, you
know, collaboratively

813
00:42:00,330 --> 00:42:04,113
on the actual data migration
using the DMS tool.

814
00:42:05,880 --> 00:42:09,270
On application modernization,

815
00:42:09,270 --> 00:42:14,270
more than 200 applications
were modernized to JBUS, EKS,

816
00:42:14,490 --> 00:42:18,060
over 500 SQL queries were remediated,

817
00:42:18,060 --> 00:42:19,590
validated, et cetera.

818
00:42:19,590 --> 00:42:23,250
So again, you know, large
volumes of stored procedures

819
00:42:23,250 --> 00:42:25,443
were updated to PostgreSQL.

820
00:42:26,340 --> 00:42:29,370
Load testing, performance
testing throughout that whole,

821
00:42:29,370 --> 00:42:31,023
you know, intense process,

822
00:42:31,920 --> 00:42:36,600
bringing in quick fixes,
helping through deployment,

823
00:42:36,600 --> 00:42:41,073
and then of course, once
moved into production,

824
00:42:41,970 --> 00:42:44,130
on the SRE standpoint,

825
00:42:44,130 --> 00:42:49,130
the Cognizant team supported
all the necessary SRE functions

826
00:42:49,350 --> 00:42:52,230
through 24/7 support,

827
00:42:52,230 --> 00:42:56,433
and that included CloudWatch
deployment, dashboard setup,

828
00:42:57,900 --> 00:43:01,470
you know, all to ensure
the reliable platform

829
00:43:01,470 --> 00:43:06,240
that CCC is known to provide
for their end customers.

830
00:43:06,240 --> 00:43:08,643
So again, a great journey together,

831
00:43:09,480 --> 00:43:12,000
a lot of learnings, lot of best practices,

832
00:43:12,000 --> 00:43:16,320
which, after completion of
this, we were actually able

833
00:43:16,320 --> 00:43:19,590
to take to our other customers as well.

834
00:43:19,590 --> 00:43:22,890
And we are really happy to
have you as an audience here.

835
00:43:22,890 --> 00:43:24,840
I'm sure you've either embarked on

836
00:43:24,840 --> 00:43:28,890
or are thinking about this
similar transformation journey.

837
00:43:28,890 --> 00:43:31,740
We'd be happy to share, you know,

838
00:43:31,740 --> 00:43:35,490
some of the best practices,
lessons learned, et cetera,

839
00:43:35,490 --> 00:43:38,640
what to watch out for,
and so on and so forth,

840
00:43:38,640 --> 00:43:40,233
so thank you very much.

841
00:43:41,253 --> 00:43:44,503
(audience applauding)

842
00:43:46,290 --> 00:43:49,383
- So before I conclude this session today,

843
00:43:50,490 --> 00:43:53,760
if you are planning for
a database modernization,

844
00:43:53,760 --> 00:43:55,980
if you are in the path of migrating

845
00:43:55,980 --> 00:43:57,780
from a native database solution

846
00:43:57,780 --> 00:44:01,380
to any cloud-managed database solutions,

847
00:44:01,380 --> 00:44:04,110
if you're working on modifying
your application workload

848
00:44:04,110 --> 00:44:06,543
from one database engine
type to another one,

849
00:44:08,520 --> 00:44:11,130
don't be overwhelmed by it.

850
00:44:11,130 --> 00:44:14,160
There are solutions,
there are tools available.

851
00:44:14,160 --> 00:44:17,520
As long as you have identified
all your challenges,

852
00:44:17,520 --> 00:44:20,880
you have a better plan and
you execute through it,

853
00:44:20,880 --> 00:44:22,650
your journey will be successful.

854
00:44:22,650 --> 00:44:23,880
It has happened to us,

855
00:44:23,880 --> 00:44:25,890
and I'm sure it'll happen to all of you.

856
00:44:25,890 --> 00:44:27,390
Thank you for your time today.

