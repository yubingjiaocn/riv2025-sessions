1
00:00:03,780 --> 00:00:05,080
- Good afternoon everyone.

2
00:00:07,410 --> 00:00:11,103
Today we are gonna talk about
innovations in data analytics,

3
00:00:12,780 --> 00:00:14,910
and my name is Kinshuk Pahare.

4
00:00:14,910 --> 00:00:18,300
I'm head of product for
our analytics portfolio.

5
00:00:18,300 --> 00:00:20,730
That's Glue, EMR, Athena, and Redshift.

6
00:00:20,730 --> 00:00:25,730
With me is Neil Mukerje and Anjali

7
00:00:25,800 --> 00:00:29,040
who is engineering manager at Netflix.

8
00:00:29,040 --> 00:00:32,460
And today the topic is
gonna be around innovation

9
00:00:32,460 --> 00:00:33,723
in data processing.

10
00:00:34,890 --> 00:00:36,333
So let's dive right in.

11
00:00:37,200 --> 00:00:38,370
This is gonna be our agenda.

12
00:00:38,370 --> 00:00:42,060
We are gonna just talk
about latest innovations,

13
00:00:42,060 --> 00:00:43,440
specifically touch on

14
00:00:43,440 --> 00:00:46,560
some of the agentic
innovations we have done.

15
00:00:46,560 --> 00:00:48,540
We are gonna talk about Iceberg

16
00:00:48,540 --> 00:00:53,100
and ease of use, innovations
that we have done.

17
00:00:53,100 --> 00:00:55,200
And finally, Anjali is gonna share

18
00:00:55,200 --> 00:00:57,210
how Netflix is leveraging EMR

19
00:00:57,210 --> 00:00:59,163
for their data processing workloads.

20
00:01:02,070 --> 00:01:05,340
So within data processing services,

21
00:01:05,340 --> 00:01:08,040
you're familiar with Glue ETL,

22
00:01:08,040 --> 00:01:10,713
which is used for
serverless data integration.

23
00:01:12,000 --> 00:01:13,200
You're obviously familiar with EMR.

24
00:01:13,200 --> 00:01:15,450
EMR is one of those services

25
00:01:15,450 --> 00:01:17,610
which had been around since 2009.

26
00:01:17,610 --> 00:01:19,410
It is used primarily for the purpose

27
00:01:19,410 --> 00:01:22,410
of open source data processing.

28
00:01:22,410 --> 00:01:27,030
Customers use EMR and multiple
engines that EMR supports

29
00:01:27,030 --> 00:01:29,130
for their data workloads.

30
00:01:29,130 --> 00:01:33,150
And Athena, which is our
interactive query service

31
00:01:33,150 --> 00:01:36,793
that is primarily used for
querying your data lake in S3.

32
00:01:38,970 --> 00:01:43,970
Together, you know, customers
are using these services

33
00:01:44,010 --> 00:01:48,270
to author their, you know, data pipelines,

34
00:01:48,270 --> 00:01:52,597
querying their data for
interactive workload on,

35
00:01:52,597 --> 00:01:56,580
you know, 300 million plus
Redshift Serverless queries

36
00:01:56,580 --> 00:01:58,113
are executed per week.

37
00:01:59,640 --> 00:02:02,733
There's a 19 years of
innovation that goes behind it.

38
00:02:03,840 --> 00:02:07,413
There are a billion plus Athena queries

39
00:02:07,413 --> 00:02:11,370
that are executed against
the data lake every week.

40
00:02:11,370 --> 00:02:14,880
And then EMR Serverless, one
of the fastest growing service,

41
00:02:14,880 --> 00:02:18,063
executes about 300 million Spark jobs.

42
00:02:19,140 --> 00:02:23,190
Together, these services query
about a hundred petabyte plus

43
00:02:23,190 --> 00:02:27,780
of data from S3 during
the course of the week.

44
00:02:27,780 --> 00:02:32,780
And these data workloads
are primarily built

45
00:02:33,690 --> 00:02:35,943
on S3 as the data lake.

46
00:02:37,440 --> 00:02:41,670
So over years, S3 has
emerged as the best place

47
00:02:41,670 --> 00:02:42,870
to build your data lake.

48
00:02:44,220 --> 00:02:47,640
There are exabytes of,
you know, Parquet data

49
00:02:47,640 --> 00:02:50,370
that is stored in S3 for the purpose

50
00:02:50,370 --> 00:02:52,200
of data lake and querying.

51
00:02:52,200 --> 00:02:55,260
Now Parquet, as you're
familiar, is a columnar format

52
00:02:55,260 --> 00:02:58,440
that is typically used
for building data lakes

53
00:02:58,440 --> 00:03:01,860
because it is much more
efficient for reads and writes.

54
00:03:01,860 --> 00:03:03,990
And the engines that are writing

55
00:03:03,990 --> 00:03:06,690
and reading from S3 are, you know,

56
00:03:06,690 --> 00:03:08,463
behind Glue EMR and Athena.

57
00:03:09,900 --> 00:03:14,900
There are 25 million
plus requests per second

58
00:03:15,780 --> 00:03:17,880
against those data lakes.

59
00:03:17,880 --> 00:03:20,550
So the reason I'm sharing
this, these numbers,

60
00:03:20,550 --> 00:03:22,530
these are exciting numbers.

61
00:03:22,530 --> 00:03:27,210
It's actually a testament of
how customers are trusting

62
00:03:27,210 --> 00:03:28,920
these data processing services

63
00:03:28,920 --> 00:03:33,750
and our S3 for building their, you know,

64
00:03:33,750 --> 00:03:36,033
mission critical data applications.

65
00:03:39,270 --> 00:03:43,020
Now, every year we
gather here to talk about

66
00:03:43,020 --> 00:03:45,363
how we are gonna make it even better.

67
00:03:47,400 --> 00:03:51,960
This is, you know, my fourth
re:Invent, but, you know,

68
00:03:51,960 --> 00:03:55,170
the Athena service was
launched nine years ago

69
00:03:55,170 --> 00:03:56,760
in one of these re:Invent.

70
00:03:56,760 --> 00:03:59,280
Glue was launched in 2017, August.

71
00:03:59,280 --> 00:04:01,410
So these are all the
services where, you know,

72
00:04:01,410 --> 00:04:05,880
they have years of
innovation behind these,

73
00:04:05,880 --> 00:04:08,370
exactly for the purpose of data processing

74
00:04:08,370 --> 00:04:09,960
and querying these data.

75
00:04:09,960 --> 00:04:14,960
So, the first and foremost,
price performance is key for us.

76
00:04:16,830 --> 00:04:19,983
This is a recurring theme year after year.

77
00:04:21,600 --> 00:04:22,860
This year also,

78
00:04:22,860 --> 00:04:27,300
our engineering has delivered
the AWS Spark performance,

79
00:04:27,300 --> 00:04:31,323
which exceed open source
Spark performance by 4.5x.

80
00:04:33,090 --> 00:04:35,410
It's 2x better for writes

81
00:04:36,600 --> 00:04:41,010
and about 5.4x better
if you use a Spark 4.0

82
00:04:41,010 --> 00:04:43,863
which is the new innovation
that we have done.

83
00:04:46,080 --> 00:04:49,080
The reason why performance
becomes important

84
00:04:49,080 --> 00:04:53,430
is that faster the Spark engine,

85
00:04:53,430 --> 00:04:56,250
better you're able to
meet your SLA targets

86
00:04:56,250 --> 00:05:00,420
and together you're also able
to actually realize pricing

87
00:05:00,420 --> 00:05:01,470
and the cost gains.

88
00:05:01,470 --> 00:05:03,030
Because if a compute

89
00:05:03,030 --> 00:05:05,940
or if a data processing
job finishes faster,

90
00:05:05,940 --> 00:05:08,140
you're actually paying
lower price for that.

91
00:05:10,860 --> 00:05:13,170
So with the price
performance out of the way,

92
00:05:13,170 --> 00:05:15,990
there are four innovation themes this year

93
00:05:15,990 --> 00:05:17,313
that the team is driving.

94
00:05:18,420 --> 00:05:23,420
Iceberg has emerged as the
standard for building data lakes.

95
00:05:25,800 --> 00:05:29,130
The key advantage is that
you can build a data lake

96
00:05:29,130 --> 00:05:33,453
in Iceberg and use any engine
to query that data lake.

97
00:05:35,340 --> 00:05:39,270
AI agents in the context of
data processing have emerged

98
00:05:39,270 --> 00:05:42,720
and they're solving some
of the most gnarly problems

99
00:05:42,720 --> 00:05:45,180
of data engineering.

100
00:05:45,180 --> 00:05:47,910
And I'm gonna talk about
some of these in my session.

101
00:05:47,910 --> 00:05:49,635
And then Neil is gonna talk about

102
00:05:49,635 --> 00:05:51,300
how we are using data agent

103
00:05:51,300 --> 00:05:54,333
for some of the ease of use, use cases.

104
00:05:55,530 --> 00:05:57,330
And finally, governance.

105
00:05:57,330 --> 00:06:00,900
Governance is always a topic of interest

106
00:06:00,900 --> 00:06:02,430
for a lot of our customers.

107
00:06:02,430 --> 00:06:04,170
How do you govern data lake?

108
00:06:04,170 --> 00:06:07,110
How do you actually ensure
fine grained access control?

109
00:06:07,110 --> 00:06:08,850
How do you ensure that the identity

110
00:06:08,850 --> 00:06:11,640
of an end user is carried forward

111
00:06:11,640 --> 00:06:15,390
for all your enforcement decision
when it comes to querying

112
00:06:15,390 --> 00:06:16,773
and writing data lakes?

113
00:06:18,150 --> 00:06:19,170
So with that out of the way,

114
00:06:19,170 --> 00:06:21,870
let's start with the AI agents in data.

115
00:06:21,870 --> 00:06:25,050
Now, AI agents have many applications.

116
00:06:25,050 --> 00:06:30,050
This had been a year of AI
agent in various contexts.

117
00:06:30,300 --> 00:06:32,340
So, for this talk, we are gonna focus

118
00:06:32,340 --> 00:06:37,340
on one of the hardest problem
that data engineers face

119
00:06:37,710 --> 00:06:41,220
which is upgrading a Spark runtime

120
00:06:41,220 --> 00:06:44,973
from version X.Y to version P.Q.

121
00:06:46,020 --> 00:06:49,860
Now, the Spark version upgrade
always had been a challenge.

122
00:06:49,860 --> 00:06:53,430
The reason it is getting
exasperated is because of the fact

123
00:06:53,430 --> 00:06:58,170
that all the data lake
innovations depend on the latest

124
00:06:58,170 --> 00:06:59,670
and the greatest Spark runtime.

125
00:06:59,670 --> 00:07:02,280
Now, obviously as a practitioner,

126
00:07:02,280 --> 00:07:05,040
everyone should run the
latest Spark version

127
00:07:05,040 --> 00:07:08,100
to get the best performance out of it,

128
00:07:08,100 --> 00:07:09,990
get the best data lake
performance out of it,

129
00:07:09,990 --> 00:07:12,750
as well as, you know,
the features out of it.

130
00:07:12,750 --> 00:07:17,750
But it's not usually a
easier upgrade to achieve.

131
00:07:19,560 --> 00:07:24,560
There are two problems that
compound the Spark upgrade.

132
00:07:24,600 --> 00:07:27,993
Number one, there is a code upgrade part,

133
00:07:28,980 --> 00:07:31,593
but then there is also
a data consistency part.

134
00:07:32,430 --> 00:07:34,860
So it's not just about you
can upgrade the Spark code,

135
00:07:34,860 --> 00:07:38,910
it compiles well, but is
it actually the same thing

136
00:07:38,910 --> 00:07:41,490
with the data that your
previous version was doing?

137
00:07:41,490 --> 00:07:43,800
So there are third elements.

138
00:07:43,800 --> 00:07:45,840
And in practice what end up happening is

139
00:07:45,840 --> 00:07:50,460
that the data engineers have to iterate

140
00:07:50,460 --> 00:07:52,170
while they're upgrading the code.

141
00:07:52,170 --> 00:07:55,410
So you upgrade the code,
see some new error messages,

142
00:07:55,410 --> 00:07:57,813
edit the code, rinse and repeat.

143
00:07:58,950 --> 00:08:02,670
That process end up becoming
not only the time consuming,

144
00:08:02,670 --> 00:08:05,100
it is also not a foolproof process

145
00:08:05,100 --> 00:08:09,060
because it end up happening
that you miss a transform,

146
00:08:09,060 --> 00:08:12,810
which is defined differently
in the latest Spark version.

147
00:08:12,810 --> 00:08:16,740
To solve this, we have
unleashed the AI agent

148
00:08:16,740 --> 00:08:17,943
for Spark upgrade.

149
00:08:19,920 --> 00:08:23,040
The way this agent works is similar to

150
00:08:23,040 --> 00:08:25,530
how a data engineer works.

151
00:08:25,530 --> 00:08:29,610
It actually builds a plan for the upgrade.

152
00:08:29,610 --> 00:08:30,900
And to build the plan,

153
00:08:30,900 --> 00:08:33,300
it actually reads through
your project structure

154
00:08:34,290 --> 00:08:36,840
and it generates an upgrade plan.

155
00:08:36,840 --> 00:08:37,673
It tells you that,

156
00:08:37,673 --> 00:08:40,020
hey, this is what I'm planning to achieve.

157
00:08:40,020 --> 00:08:42,330
You can look at the plan, edit the plan,

158
00:08:42,330 --> 00:08:45,210
you can provide your own
upgrade plan depending on,

159
00:08:45,210 --> 00:08:47,310
you know, how your pipeline is structured.

160
00:08:48,600 --> 00:08:51,240
The next stage is to build and execute.

161
00:08:51,240 --> 00:08:55,560
And this is the stage where
it will execute this plan,

162
00:08:55,560 --> 00:08:57,780
see the error messages,

163
00:08:57,780 --> 00:09:01,500
edit the code based on the
error message, rinse and repeat.

164
00:09:01,500 --> 00:09:02,730
So it will do the same thing,

165
00:09:02,730 --> 00:09:04,710
which a data engineer is expected to do,

166
00:09:04,710 --> 00:09:06,963
but it will do it in an automatic fashion.

167
00:09:09,150 --> 00:09:12,480
Finally, when the code is
upgraded and it's compiling well,

168
00:09:12,480 --> 00:09:14,760
so all the test plans are passing,

169
00:09:14,760 --> 00:09:17,463
it's also gonna do a data quality check.

170
00:09:18,540 --> 00:09:21,150
And what I mean by data quality check?

171
00:09:21,150 --> 00:09:26,010
It was, the AI agent will actually look

172
00:09:26,010 --> 00:09:30,900
at your data structure
before and after upgrade,

173
00:09:30,900 --> 00:09:34,080
whether the schema matches,
whether the column type matches,

174
00:09:34,080 --> 00:09:39,080
whether the data, you know,
the size of the data matches.

175
00:09:40,050 --> 00:09:41,730
It will do all those checks

176
00:09:41,730 --> 00:09:45,000
and then it will say that,
okay, this is ready for your,

177
00:09:45,000 --> 00:09:47,853
you know, this code is finally upgraded.

178
00:09:50,010 --> 00:09:53,640
Throughout the process,
you are in full control.

179
00:09:53,640 --> 00:09:57,240
There is a guardrail
which can be configured.

180
00:09:57,240 --> 00:09:58,080
You can actually say

181
00:09:58,080 --> 00:10:01,800
that I want to manually
execute every single step

182
00:10:01,800 --> 00:10:05,760
in the plan, or I want
agent, I trust the agent

183
00:10:05,760 --> 00:10:09,093
and I trust it to upgrade
the code properly.

184
00:10:10,140 --> 00:10:15,140
And during all the runs and all the build

185
00:10:15,180 --> 00:10:18,060
and all these, you know,
the entire process,

186
00:10:18,060 --> 00:10:21,030
you have access to the observability logs.

187
00:10:21,030 --> 00:10:24,270
So you can actually see
what changes it has done.

188
00:10:24,270 --> 00:10:25,680
And I'm gonna show you a demo

189
00:10:25,680 --> 00:10:28,893
which actually talks about how
this works in the practice.

190
00:10:32,520 --> 00:10:37,410
It also, like, what I
didn't mention before,

191
00:10:37,410 --> 00:10:40,980
you can actually upgrade
your entire pipeline

192
00:10:40,980 --> 00:10:42,840
or multiple pipelines.

193
00:10:42,840 --> 00:10:44,820
The task that used to take six months,

194
00:10:44,820 --> 00:10:47,403
sometimes six to 12 months, in minutes.

195
00:10:49,320 --> 00:10:52,080
Because this agent is actually, you know,

196
00:10:52,080 --> 00:10:55,260
automatically able to
achieve some of these,

197
00:10:55,260 --> 00:10:58,680
you know, very hard to find
problems, the Spark problems

198
00:10:58,680 --> 00:11:00,093
and act upon it.

199
00:11:03,030 --> 00:11:05,640
So in this case, here's a little demo,

200
00:11:05,640 --> 00:11:08,550
help me upgrade from,
you know, 2.4 to 3.5.

201
00:11:08,550 --> 00:11:10,530
It will say, okay, you
want me to upgrade this?

202
00:11:10,530 --> 00:11:11,850
I read through the file,

203
00:11:11,850 --> 00:11:13,830
I read through your project structure.

204
00:11:13,830 --> 00:11:15,450
It will actually build a plan.

205
00:11:15,450 --> 00:11:17,910
It will say, hey, do you like this plan?

206
00:11:17,910 --> 00:11:20,913
When you say I like
this plan, you can say,

207
00:11:22,650 --> 00:11:25,560
and it is gonna actually say,

208
00:11:25,560 --> 00:11:27,963
here are the steps which
I'm about to execute.

209
00:11:29,220 --> 00:11:33,573
And as part of the execution
you can save plan and proceed.

210
00:11:34,470 --> 00:11:37,620
And finally it will say,
okay, I found this diff

211
00:11:37,620 --> 00:11:39,870
between the previous
version and new version.

212
00:11:39,870 --> 00:11:42,930
Here are all the changes
which I'm about to execute.

213
00:11:42,930 --> 00:11:44,610
Do you approve?

214
00:11:44,610 --> 00:11:49,610
It's gonna hit yes and then
it's gonna keep doing this

215
00:11:50,250 --> 00:11:51,690
for every single process.

216
00:11:51,690 --> 00:11:53,910
And finally it will tell you exactly

217
00:11:53,910 --> 00:11:56,820
what changes it made, a diff doc.

218
00:11:56,820 --> 00:11:58,830
And once you're satisfied
with this upgrade,

219
00:11:58,830 --> 00:12:00,980
you can actually execute
this into product.

220
00:12:03,480 --> 00:12:07,560
FINRA is using this upgrade
agent for the purpose of,

221
00:12:07,560 --> 00:12:10,683
you know, upgrading their
multiple data pipelines.

222
00:12:13,410 --> 00:12:15,300
The second innovation,
which I talked about

223
00:12:15,300 --> 00:12:17,610
is support for Iceberg V3.

224
00:12:17,610 --> 00:12:20,850
Iceberg V3 is the latest table format.

225
00:12:20,850 --> 00:12:23,283
It is based out of Iceberg version 1.10.

226
00:12:24,210 --> 00:12:26,310
It supports some of the new capabilities

227
00:12:26,310 --> 00:12:29,190
around Deletion Vectors, Row-Lineage.

228
00:12:29,190 --> 00:12:30,840
All of these are available to you

229
00:12:30,840 --> 00:12:33,303
as part of EMR Spark runtime 7.12.

230
00:12:34,500 --> 00:12:36,960
In case you're not aware,

231
00:12:36,960 --> 00:12:41,160
EMR Spark runtime is
also shared with Glue ETL

232
00:12:41,160 --> 00:12:42,183
as well as Athena.

233
00:12:43,170 --> 00:12:44,820
These are all the Spark runtimes

234
00:12:44,820 --> 00:12:47,730
which benefit from the same
innovations that are delivered

235
00:12:47,730 --> 00:12:49,140
with the EMR 7.12.

236
00:12:49,140 --> 00:12:50,940
So you get the same
performance characteristics,

237
00:12:50,940 --> 00:12:54,900
you get the same feature
set as part of Iceberg V3.

238
00:12:54,900 --> 00:12:57,990
And what Deletion Vectors
and Row-lineage allows you

239
00:12:57,990 --> 00:13:00,780
to do is it actually makes it easier

240
00:13:00,780 --> 00:13:02,613
for you to manage your data lake.

241
00:13:03,660 --> 00:13:07,110
It reduces your right
amplification problems,

242
00:13:07,110 --> 00:13:09,510
it eliminates smart delete problems

243
00:13:09,510 --> 00:13:13,680
and, you know, it is, it
makes it very, very efficient

244
00:13:13,680 --> 00:13:16,130
for you to actually build
and maintain your data.

245
00:13:18,420 --> 00:13:23,400
We're also announcing
EMR Spark version 4.0.

246
00:13:23,400 --> 00:13:25,350
It is supporting, you know,

247
00:13:25,350 --> 00:13:28,110
the latest greatest Iceberg
version as just talked about.

248
00:13:28,110 --> 00:13:33,110
It is also 5.4 times faster
than the open source Spark,

249
00:13:33,660 --> 00:13:37,587
which means you are
able to use a Spark 4.0

250
00:13:38,460 --> 00:13:40,590
and achieve your SLA

251
00:13:40,590 --> 00:13:43,920
or reduce your cost of
operating a pipeline

252
00:13:43,920 --> 00:13:45,213
by an equivalent amount.

253
00:13:48,900 --> 00:13:52,620
Finally, there is another
launch that we did this year,

254
00:13:52,620 --> 00:13:54,603
which is Iceberg Materialized View.

255
00:13:55,440 --> 00:13:58,200
Materialized View, as
you're familiar with,

256
00:13:58,200 --> 00:14:03,200
allows you to define a view in the SQL

257
00:14:03,330 --> 00:14:08,330
and Iceberg Materialized
Views are Iceberg tables

258
00:14:08,370 --> 00:14:10,863
that are result of that
SQL code execution.

259
00:14:11,700 --> 00:14:14,013
So it automatically speeds up your,

260
00:14:14,940 --> 00:14:17,403
you know, query from your Spark engine.

261
00:14:18,360 --> 00:14:20,490
It is compatible with the Iceberg APIs,

262
00:14:20,490 --> 00:14:24,390
which means when they are
executed as part of SQL queries,

263
00:14:24,390 --> 00:14:27,000
they manifest as a catalog object.

264
00:14:27,000 --> 00:14:28,650
And against that catalog object,

265
00:14:28,650 --> 00:14:30,273
you can run your Athena, Spark or,

266
00:14:30,273 --> 00:14:33,360
you know, EMR Spark or Glue ETL queries.

267
00:14:33,360 --> 00:14:35,100
And more importantly,

268
00:14:35,100 --> 00:14:38,130
you can actually do incremental refresh

269
00:14:38,130 --> 00:14:39,660
to the Iceberg views.

270
00:14:39,660 --> 00:14:42,480
So what that means is
that instead of having

271
00:14:42,480 --> 00:14:47,070
to configure an ETL pipeline
and configure all the triggers

272
00:14:47,070 --> 00:14:50,970
and all the scheduling,

273
00:14:50,970 --> 00:14:53,310
you can just define a materialized view,

274
00:14:53,310 --> 00:14:55,890
which is refreshed automatically based

275
00:14:55,890 --> 00:14:57,340
on frequency that you choose

276
00:14:58,440 --> 00:15:01,470
or based on, you know, as
soon as the data is available,

277
00:15:01,470 --> 00:15:03,070
it automatically gets refreshed.

278
00:15:06,420 --> 00:15:10,560
This is available as part
of the Glue data catalog.

279
00:15:10,560 --> 00:15:13,260
The view definitions are
stored in the catalog.

280
00:15:13,260 --> 00:15:18,180
They can be, you know, accessed from EMR,

281
00:15:18,180 --> 00:15:20,913
Spark, Glue ETL Spark or Athena Spark.

282
00:15:21,900 --> 00:15:25,620
And it is an announcement
we did yesterday.

283
00:15:25,620 --> 00:15:27,480
So with that I'm gonna hand off to Neil

284
00:15:27,480 --> 00:15:29,230
who's gonna talk about ease of use.

285
00:15:34,920 --> 00:15:38,190
- Everyone, let's focus on features

286
00:15:38,190 --> 00:15:41,790
that focus on ease of use, right?

287
00:15:41,790 --> 00:15:43,680
So the most shiny feature

288
00:15:43,680 --> 00:15:46,260
that we have is SageMaker notebooks.

289
00:15:46,260 --> 00:15:47,400
Now it is important to note

290
00:15:47,400 --> 00:15:50,820
that SageMaker Studio is the
new UI, the new front end

291
00:15:50,820 --> 00:15:53,400
for all data processing as well as AI ML

292
00:15:53,400 --> 00:15:56,730
which means whether you're
using EMR or Glue or Athena,

293
00:15:56,730 --> 00:16:00,060
your main UI front end for
users is SageMaker Studio.

294
00:16:00,060 --> 00:16:02,490
And SageMaker notebooks
is the newest addition

295
00:16:02,490 --> 00:16:03,870
to SageMaker Studio.

296
00:16:03,870 --> 00:16:07,290
It's a new modern notebook experience

297
00:16:07,290 --> 00:16:12,060
that comes with a purpose-built AI agent.

298
00:16:12,060 --> 00:16:14,160
You can get started in seconds.

299
00:16:14,160 --> 00:16:17,070
And you can choose the language
best suited for your task,

300
00:16:17,070 --> 00:16:20,940
which can be SQL, which
can be Python or Spark.

301
00:16:20,940 --> 00:16:24,660
And what is great about it is
that you can start with Python

302
00:16:24,660 --> 00:16:27,810
and you can seamlessly scale
to, you know, Spark workloads

303
00:16:27,810 --> 00:16:30,270
to handle large data sets.

304
00:16:30,270 --> 00:16:32,430
Now let's see how that works.

305
00:16:32,430 --> 00:16:36,510
Behind the scene, SageMaker Notebook steps

306
00:16:36,510 --> 00:16:38,670
into Athena for Apache Spark

307
00:16:38,670 --> 00:16:41,190
to deliver the Spark capabilities.

308
00:16:41,190 --> 00:16:43,320
Now, Athena for Apache Spark

309
00:16:43,320 --> 00:16:46,320
uses the same performance
runtime engine as EMR.

310
00:16:46,320 --> 00:16:50,160
It actually uses EMR
5.12, which is 4.7x faster

311
00:16:50,160 --> 00:16:51,930
than open source Spark.

312
00:16:51,930 --> 00:16:53,490
Being serverless, of course,

313
00:16:53,490 --> 00:16:57,240
it eliminates all management overhead

314
00:16:57,240 --> 00:17:00,750
and it starts up and scales
in a low order of seconds,

315
00:17:00,750 --> 00:17:03,600
which means if you go into
notebook and type Spark,

316
00:17:03,600 --> 00:17:06,480
within seconds, you have a
massive spark cluster ready

317
00:17:06,480 --> 00:17:09,540
for you for data processing.

318
00:17:09,540 --> 00:17:11,883
And what is unique about it is that,

319
00:17:13,560 --> 00:17:18,420
the architecture allows a
unified authoring execution

320
00:17:18,420 --> 00:17:22,740
and debugging experience across
Python and Spark workloads.

321
00:17:22,740 --> 00:17:24,190
Now let's see how that works.

322
00:17:25,740 --> 00:17:27,360
So SageMaker Notebook connects

323
00:17:27,360 --> 00:17:30,180
to Athena Spark using Spark Connect.

324
00:17:30,180 --> 00:17:33,360
So Spark Connect is a new
client server architecture

325
00:17:33,360 --> 00:17:36,240
in Spark that separates
the client application

326
00:17:36,240 --> 00:17:40,170
from the remote Spark driver
while establishing a protocol

327
00:17:40,170 --> 00:17:43,260
to seamlessly exchange
data between the two,

328
00:17:43,260 --> 00:17:45,750
facilitating remote execution which means

329
00:17:45,750 --> 00:17:48,990
that really when you type
Spark on a notebook cell,

330
00:17:48,990 --> 00:17:51,450
it appears that Spark is
running locally in the notebook

331
00:17:51,450 --> 00:17:54,330
while it's actually running
on a massive cluster.

332
00:17:54,330 --> 00:17:57,450
Behind the scene, you can
debug Spark variables just

333
00:17:57,450 --> 00:17:59,223
as you can debug Python variables.

334
00:18:00,210 --> 00:18:03,210
And which means that now
you can tap into Python

335
00:18:03,210 --> 00:18:05,460
and SQL, maybe Python
packages like, you know,

336
00:18:05,460 --> 00:18:08,970
pandas and duckDB etc. for
small to medium data sets

337
00:18:08,970 --> 00:18:10,440
and you can tap into Spark

338
00:18:10,440 --> 00:18:14,613
for large scale data
processing at the same time.

339
00:18:15,840 --> 00:18:19,687
Now let's have a deeper
look at the AI agent that,

340
00:18:19,687 --> 00:18:22,590
you know, comes with the notebook.

341
00:18:22,590 --> 00:18:24,930
Now the AI agent experience is embedded

342
00:18:24,930 --> 00:18:26,520
in the notebook interface, right?

343
00:18:26,520 --> 00:18:28,860
It's on the right hand
side of your notebook.

344
00:18:28,860 --> 00:18:30,600
It understands your data catalog.

345
00:18:30,600 --> 00:18:32,700
It understands your business metadata.

346
00:18:32,700 --> 00:18:34,920
It can generate SQL of the right dialect,

347
00:18:34,920 --> 00:18:38,160
which is Spark SQL or Athena
SQL if you're using Athena.

348
00:18:38,160 --> 00:18:40,230
It can generate Python code, Spark code,

349
00:18:40,230 --> 00:18:41,610
which is PySpark code.

350
00:18:41,610 --> 00:18:43,620
And it can also do more
than generate data.

351
00:18:43,620 --> 00:18:45,360
It can actually generate an entire plan,

352
00:18:45,360 --> 00:18:48,420
a full notebook filled up itself

353
00:18:48,420 --> 00:18:49,770
to achieve a specific outcome.

354
00:18:49,770 --> 00:18:51,480
So you can really talk to it.

355
00:18:51,480 --> 00:18:55,683
So, let's have a quick demo
of how the data agent works.

356
00:18:56,550 --> 00:18:59,700
So, in this example,
in my data agent panel,

357
00:18:59,700 --> 00:19:03,480
I ask a question, show me
tables in my sample database.

358
00:19:03,480 --> 00:19:05,850
Now it'll go and discover
the tables I have.

359
00:19:05,850 --> 00:19:08,820
Now I have three tables here,
the churn table, the ltv table

360
00:19:08,820 --> 00:19:10,890
and the New York taxi trips table.

361
00:19:10,890 --> 00:19:14,010
Now I can type SQL to
actually query it myself

362
00:19:14,010 --> 00:19:16,170
or I can go to the table cell

363
00:19:16,170 --> 00:19:18,960
and I can visualize the data I have,

364
00:19:18,960 --> 00:19:21,150
but what's even better, I
can just talk to it, say,

365
00:19:21,150 --> 00:19:24,450
can you please help me
visualize data in my ltv table?

366
00:19:24,450 --> 00:19:25,283
Right?

367
00:19:25,283 --> 00:19:28,770
And so, it then generates
the corresponding Python code

368
00:19:28,770 --> 00:19:31,080
to build visualizations

369
00:19:31,080 --> 00:19:33,840
and gives me a
comprehensive visualizations

370
00:19:33,840 --> 00:19:36,210
of the data in my ltv table.

371
00:19:36,210 --> 00:19:39,090
So I hope you like the
convenience and the power of it

372
00:19:39,090 --> 00:19:41,340
and you use it.

373
00:19:41,340 --> 00:19:45,840
So moving on to a highly
asked for feature,

374
00:19:45,840 --> 00:19:48,390
we have managed Airflow now

375
00:19:48,390 --> 00:19:50,340
in an entirely serverless
deployment, right?

376
00:19:50,340 --> 00:19:51,510
Of course Airflow is, you know,

377
00:19:51,510 --> 00:19:54,180
extremely popular in the
data orchestration space.

378
00:19:54,180 --> 00:19:57,000
And now we have it in the
serverless deployment model.

379
00:19:57,000 --> 00:19:59,970
The UI for this again is
SageMaker Unified studio.

380
00:19:59,970 --> 00:20:04,500
So you can author, create,
managed workflows in a single UI.

381
00:20:04,500 --> 00:20:07,230
Now it comes with an
enhanced security model

382
00:20:07,230 --> 00:20:10,230
where now per workflow you
can specify an IAM role,

383
00:20:10,230 --> 00:20:11,970
which means each workflow can now

384
00:20:11,970 --> 00:20:14,760
have a unique permission profile.

385
00:20:14,760 --> 00:20:15,990
And of course, being serverless,

386
00:20:15,990 --> 00:20:17,490
you only pay for what you use.

387
00:20:18,810 --> 00:20:20,460
So let's do a comparison

388
00:20:20,460 --> 00:20:24,120
of what we already have
within a provisioned airflow

389
00:20:24,120 --> 00:20:26,820
and compare it with serverless.

390
00:20:26,820 --> 00:20:29,790
With provisioned airflow, you
have a fixed infrastructure,

391
00:20:29,790 --> 00:20:30,750
you size it first

392
00:20:30,750 --> 00:20:33,000
and then, you can have
a micro size and so on.

393
00:20:33,000 --> 00:20:34,863
And it's always on,

394
00:20:35,790 --> 00:20:38,820
regardless of whether
workflows are running or not.

395
00:20:38,820 --> 00:20:40,380
There are certain capacity limits based

396
00:20:40,380 --> 00:20:41,790
on the size you chose,

397
00:20:41,790 --> 00:20:43,770
and yes, you have to
manage the environment.

398
00:20:43,770 --> 00:20:44,880
But on the serverless side,

399
00:20:44,880 --> 00:20:46,740
there's of course no
infrastructure anymore.

400
00:20:46,740 --> 00:20:47,970
It's pay per use.

401
00:20:47,970 --> 00:20:51,900
It's unlimited capacity and
it's entirely service managed.

402
00:20:51,900 --> 00:20:53,250
From the security perspective,

403
00:20:53,250 --> 00:20:58,250
there are some, you know, key
changes in the provision mode.

404
00:20:59,250 --> 00:21:01,470
Your entire Airflow environment

405
00:21:01,470 --> 00:21:03,000
has a single security model, right?

406
00:21:03,000 --> 00:21:04,740
So all the workflows
have the same, you know,

407
00:21:04,740 --> 00:21:06,180
permission profiles.

408
00:21:06,180 --> 00:21:07,980
If you wanna separate these workflows,

409
00:21:07,980 --> 00:21:11,040
you have to launch separate environments,

410
00:21:11,040 --> 00:21:12,600
which means the authorization is shared

411
00:21:12,600 --> 00:21:14,430
and isolation is complex.

412
00:21:14,430 --> 00:21:17,850
On the serverless side, you
have workflow level security.

413
00:21:17,850 --> 00:21:20,880
So per workflow you can define, you know,

414
00:21:20,880 --> 00:21:22,470
granular access control,

415
00:21:22,470 --> 00:21:26,730
which means you can now have
the least privileged IAM roles

416
00:21:26,730 --> 00:21:28,800
for each workflow,
which really simplifies,

417
00:21:28,800 --> 00:21:30,150
you know, compliance.

418
00:21:30,150 --> 00:21:35,150
Now all of this is embedded in the UI,

419
00:21:35,400 --> 00:21:39,570
workflow UI in SageMaker Unified Studio.

420
00:21:39,570 --> 00:21:41,910
You can easily drag and
drop these activities

421
00:21:41,910 --> 00:21:45,960
to build a gag yourself
without any prior expertise on,

422
00:21:45,960 --> 00:21:48,393
you know, Python or Airflow.

423
00:21:49,500 --> 00:21:51,150
And then once you have built them,

424
00:21:51,150 --> 00:21:53,970
you can submit these workflows,
you can monitor these jobs

425
00:21:53,970 --> 00:21:55,570
and so on, all from a single UI.

426
00:21:57,780 --> 00:22:01,500
Now what's also available
is a 1-click scheduler

427
00:22:01,500 --> 00:22:03,630
for Visual ETL.

428
00:22:03,630 --> 00:22:08,070
This allows you to easily
streamline your ETL flows

429
00:22:08,070 --> 00:22:10,070
and, you know, queries
directly from the same UI

430
00:22:10,070 --> 00:22:13,050
in SageMaker Unified studio.

431
00:22:13,050 --> 00:22:16,230
And you can create, you can
modify, you can monitor these,

432
00:22:16,230 --> 00:22:17,160
you know, schedules.

433
00:22:17,160 --> 00:22:18,060
Behind the scene,

434
00:22:18,060 --> 00:22:21,330
it's integrated with Amazon
EventBridge Scheduler

435
00:22:21,330 --> 00:22:22,880
to make these schedules happen.

436
00:22:24,750 --> 00:22:28,380
Now, customers have told us
that it's not easy to onboard

437
00:22:28,380 --> 00:22:29,550
to SageMaker Unified studio

438
00:22:29,550 --> 00:22:31,200
and that is because of
certain dependencies.

439
00:22:31,200 --> 00:22:34,110
You have to configure IAM
Identity Center and so on.

440
00:22:34,110 --> 00:22:37,770
We have listened and we
have made the onboarding

441
00:22:37,770 --> 00:22:40,710
to SageMaker Unified
Studio, a one click step

442
00:22:40,710 --> 00:22:44,310
from existing services like,
you know, Athena console,

443
00:22:44,310 --> 00:22:46,800
from S3 Tables, from
Amazon Redshift console.

444
00:22:46,800 --> 00:22:49,290
Now from those pages with a single click,

445
00:22:49,290 --> 00:22:51,420
you can go into SageMaker Unified studio.

446
00:22:51,420 --> 00:22:53,400
We will auto create

447
00:22:53,400 --> 00:22:56,970
a default SageMaker
Unified Studio workspace.

448
00:22:56,970 --> 00:22:58,950
The entire process takes a minute.

449
00:22:58,950 --> 00:23:01,650
You don't have to configure
IAM Identity Center anymore.

450
00:23:01,650 --> 00:23:05,160
It uses the IAM rules
you are already using

451
00:23:05,160 --> 00:23:06,300
in those consoles

452
00:23:06,300 --> 00:23:08,550
and you'll land in a much
sophisticated interface

453
00:23:08,550 --> 00:23:11,310
which allows you to use notebook
capabilities, query editors

454
00:23:11,310 --> 00:23:13,893
and so on, on those same data sets.

455
00:23:15,120 --> 00:23:17,730
Alright, so let's move into
a very exciting feature.

456
00:23:17,730 --> 00:23:22,730
This is an industry first
where EMR Serverless

457
00:23:23,009 --> 00:23:23,970
with serverless storage,

458
00:23:23,970 --> 00:23:27,240
entirely eliminates
local disc provisioning

459
00:23:27,240 --> 00:23:28,860
for Spark workloads.

460
00:23:28,860 --> 00:23:32,940
So with this you can achieve
up to 20% reduced costs

461
00:23:32,940 --> 00:23:35,160
as per our benchmarks.

462
00:23:35,160 --> 00:23:37,050
So here's how it works.

463
00:23:37,050 --> 00:23:39,840
So Spark workers have local discs,

464
00:23:39,840 --> 00:23:42,480
and these local discs have,
you know, shuffle in them.

465
00:23:42,480 --> 00:23:45,360
When this disc run out of
capacity, your jobs fail.

466
00:23:45,360 --> 00:23:48,240
If the disc are your
constraint, your jobs slow down.

467
00:23:48,240 --> 00:23:49,380
And often what happens,

468
00:23:49,380 --> 00:23:51,120
you have certain tasks
which are stragglers.

469
00:23:51,120 --> 00:23:53,400
So some workers keep
running for a long time

470
00:23:53,400 --> 00:23:55,500
and all workers have to be retained

471
00:23:55,500 --> 00:23:56,700
when some workers are working

472
00:23:56,700 --> 00:23:58,620
because the discs have
shuffled data in them.

473
00:23:58,620 --> 00:24:02,683
This makes Spark scaling
inherently inefficient.

474
00:24:04,230 --> 00:24:05,340
Now with serverless storage,

475
00:24:05,340 --> 00:24:07,350
these problems are entirely eliminated.

476
00:24:07,350 --> 00:24:09,600
Serverless storage offloads shuffle

477
00:24:09,600 --> 00:24:11,850
to a high performance storage layer,

478
00:24:11,850 --> 00:24:13,860
which means you never
run out of disk space,

479
00:24:13,860 --> 00:24:17,220
you don't face any job failures,
your jobs never slow down.

480
00:24:17,220 --> 00:24:20,610
And as the shuffle is outside the cluster,

481
00:24:20,610 --> 00:24:22,440
Spark can be far more elastic,

482
00:24:22,440 --> 00:24:25,800
scaling out and scaling in
as per the needs of a stage.

483
00:24:25,800 --> 00:24:30,800
So the combination of
the compute efficiencies

484
00:24:31,290 --> 00:24:32,910
that serverless storage brings along

485
00:24:32,910 --> 00:24:33,807
with the fact that they don't have to,

486
00:24:33,807 --> 00:24:36,330
you know, provision this anymore

487
00:24:36,330 --> 00:24:38,280
is an absolute game changer in this space.

488
00:24:38,280 --> 00:24:41,580
And you can save up to
20% as per all benchmarks

489
00:24:41,580 --> 00:24:42,960
and maybe even more depending

490
00:24:42,960 --> 00:24:44,510
on the shape of your workloads.

491
00:24:46,500 --> 00:24:49,950
All right, so let's now move
to the topic of security

492
00:24:49,950 --> 00:24:52,200
and you know, governance
and a very important topic

493
00:24:52,200 --> 00:24:53,650
for enterprise and customers.

494
00:24:54,900 --> 00:24:55,733
Of course the main, you know,

495
00:24:55,733 --> 00:24:59,373
concern is data access control, right?

496
00:25:00,300 --> 00:25:02,580
Customers want, you know,
consistent data access control

497
00:25:02,580 --> 00:25:04,380
across the various
formats they want to use.

498
00:25:04,380 --> 00:25:06,270
Of course Apache Iceberg
is very important,

499
00:25:06,270 --> 00:25:07,977
but so is Delta Lake and Hudi.

500
00:25:07,977 --> 00:25:11,794
And of course, traditional
formats like Parquet, CSV

501
00:25:11,794 --> 00:25:13,620
and JSON, right?

502
00:25:13,620 --> 00:25:15,900
For most use cases, coarse
grained access control

503
00:25:15,900 --> 00:25:17,150
is what is needed where you want

504
00:25:17,150 --> 00:25:18,900
to secure certain S3 locations,

505
00:25:18,900 --> 00:25:21,060
you wanna secure certain tables,

506
00:25:21,060 --> 00:25:22,800
ensure that the users get access to them.

507
00:25:22,800 --> 00:25:23,730
And for some use cases,

508
00:25:23,730 --> 00:25:25,170
you also need fine grained access control

509
00:25:25,170 --> 00:25:30,090
where you have column, row and
cell level security as well.

510
00:25:30,090 --> 00:25:32,100
So let's see how these,

511
00:25:32,100 --> 00:25:34,560
we have made a lot of launches this year,

512
00:25:34,560 --> 00:25:37,110
you know, focused on
security and governance.

513
00:25:37,110 --> 00:25:40,410
So let's see how all of this,
you know, comes together.

514
00:25:40,410 --> 00:25:45,000
So the first we'll focus
on is coarse grained access

515
00:25:45,000 --> 00:25:47,340
for S3 locations.

516
00:25:47,340 --> 00:25:49,337
Now, for this, the recommendation is

517
00:25:49,337 --> 00:25:51,150
to use S3 Access Grants of course,

518
00:25:51,150 --> 00:25:53,610
where you can have simple
SQL like permissions

519
00:25:53,610 --> 00:25:55,410
where you can have read,
write, you know, permissions

520
00:25:55,410 --> 00:25:59,730
to S3 buckets, prefixes
as well as objects.

521
00:25:59,730 --> 00:26:04,290
So how it works is that when
you submit a job, your job,

522
00:26:04,290 --> 00:26:06,510
the engine simply looks
up S3 Access Grants,

523
00:26:06,510 --> 00:26:10,260
gets scoped down credentials
and uses those credentials

524
00:26:10,260 --> 00:26:11,310
to talk to S3.

525
00:26:11,310 --> 00:26:14,130
Now this has been in EMR for a while from,

526
00:26:14,130 --> 00:26:18,057
you know, 6.15+. It is
now also in Glue with 5.0

527
00:26:18,057 --> 00:26:20,370
and it works, you know, consistently

528
00:26:20,370 --> 00:26:22,683
across all table formats.

529
00:26:23,550 --> 00:26:26,670
Now let's look at coarse grained access

530
00:26:26,670 --> 00:26:29,880
for actually tables in your catalog.

531
00:26:29,880 --> 00:26:33,450
This is definitely the most
common, you know, scenario

532
00:26:33,450 --> 00:26:36,390
where what you want is
you don't wanna deal

533
00:26:36,390 --> 00:26:38,430
with in a fine grained
access control overhead,

534
00:26:38,430 --> 00:26:42,360
but you want to grant users
access to certain tables, right?

535
00:26:42,360 --> 00:26:46,770
So this is, this has
no performance overhead

536
00:26:46,770 --> 00:26:49,080
because it's just filtering of metadata

537
00:26:49,080 --> 00:26:51,570
and the expectation is,
of course, full support,

538
00:26:51,570 --> 00:26:52,680
which means read, write, you know,

539
00:26:52,680 --> 00:26:54,240
support for all the use cases.

540
00:26:54,240 --> 00:26:56,040
It's a fairly easy implementation.

541
00:26:56,040 --> 00:26:58,860
So let's look how, how this is achieved.

542
00:26:58,860 --> 00:27:00,780
So the first step is
that the admin will go

543
00:27:00,780 --> 00:27:04,230
to Lake formation and will
grant, you know, permissions

544
00:27:04,230 --> 00:27:05,670
for, you know, specific users for,

545
00:27:05,670 --> 00:27:07,200
you know, specific tables.

546
00:27:07,200 --> 00:27:09,360
Now when a job is, you know, submitted,

547
00:27:09,360 --> 00:27:12,420
then it talks to the Glue catalog,

548
00:27:12,420 --> 00:27:14,430
which redirects to lake formation

549
00:27:14,430 --> 00:27:17,010
and gets scoped down credentials back.

550
00:27:17,010 --> 00:27:19,770
And those credentials
are used to access S3.

551
00:27:19,770 --> 00:27:23,280
Now this is available in EMR from 7.9+

552
00:27:23,280 --> 00:27:26,298
as well as is available
in the Glue from 5.0+

553
00:27:26,298 --> 00:27:28,950
and it supports reads and
writes from Iceberg Delta

554
00:27:28,950 --> 00:27:30,123
as well as Hudi tables.

555
00:27:31,290 --> 00:27:33,540
Alright, so let's now look at
fine grained access control

556
00:27:33,540 --> 00:27:37,140
where you want column, row as
well as cell level security,

557
00:27:37,140 --> 00:27:38,100
of course enterprise, you know,

558
00:27:38,100 --> 00:27:40,710
customers want, you know, audit
trails so that you can have,

559
00:27:40,710 --> 00:27:42,180
you know, compliance reporting.

560
00:27:42,180 --> 00:27:44,760
And of course this should work typically

561
00:27:44,760 --> 00:27:48,780
within a granular R&D based,
you know, controls as well.

562
00:27:48,780 --> 00:27:51,270
So let's see how this is implemented.

563
00:27:51,270 --> 00:27:54,030
Here, the admin will set
a fine grained permissions

564
00:27:54,030 --> 00:27:57,330
in lake formation for, you
know, specific tables, rows

565
00:27:57,330 --> 00:28:01,140
and you know, column cell as needed.

566
00:28:01,140 --> 00:28:04,470
Here at the engine level, you
have to enable lake formation

567
00:28:04,470 --> 00:28:06,900
and now the user submits a job,

568
00:28:06,900 --> 00:28:08,520
the engine communicates
with lake formation,

569
00:28:08,520 --> 00:28:10,020
gets, you know, scope down credentials

570
00:28:10,020 --> 00:28:11,970
and it accesses S3 as well as the right,

571
00:28:11,970 --> 00:28:13,380
you know, catalog tables.

572
00:28:13,380 --> 00:28:17,610
So note that from 7.7 onwards on EMR,

573
00:28:17,610 --> 00:28:21,090
we support reads on all deployments.

574
00:28:21,090 --> 00:28:23,313
Glue is supported 5.0 onwards.

575
00:28:24,240 --> 00:28:25,260
I'm very excited to announce

576
00:28:25,260 --> 00:28:27,090
that we support writes now as well, right?

577
00:28:27,090 --> 00:28:31,710
So 7.12 which is almost
the most recent launch,

578
00:28:31,710 --> 00:28:32,610
we support writes, you know,

579
00:28:32,610 --> 00:28:36,660
consistently on all EMR
deployments as well as in Glue 5.1,

580
00:28:36,660 --> 00:28:39,300
with, you know, Iceberg
Delta as well as Hudi.

581
00:28:39,300 --> 00:28:41,430
Now this is a very unique
architecture we have here

582
00:28:41,430 --> 00:28:44,250
where we separate the
system and the user drivers.

583
00:28:44,250 --> 00:28:47,640
The system drivers is where
all of the filtering happens

584
00:28:47,640 --> 00:28:50,190
and the user drivers where
your Spark code runs,

585
00:28:50,190 --> 00:28:53,400
and this isolation is
what you know creates the,

586
00:28:53,400 --> 00:28:56,130
you know, security model that ensures

587
00:28:56,130 --> 00:28:58,320
that you cannot run a Spark code

588
00:28:58,320 --> 00:28:59,973
to bypass the controls.

589
00:29:02,190 --> 00:29:04,080
Okay, so let's move to
a very interesting topic

590
00:29:04,080 --> 00:29:06,430
and a very important
topic for enterprise unit,

591
00:29:07,500 --> 00:29:10,980
customers of, you know,
trusted identity propagation.

592
00:29:10,980 --> 00:29:13,440
So the expectation of enterprise customers

593
00:29:13,440 --> 00:29:15,060
is that the user will authenticate

594
00:29:15,060 --> 00:29:18,210
against established identity providers

595
00:29:18,210 --> 00:29:20,490
like Active Directory, Okta and so on.

596
00:29:20,490 --> 00:29:22,770
And those identity has to be pushed down

597
00:29:22,770 --> 00:29:24,750
to the various services, right?

598
00:29:24,750 --> 00:29:26,937
And of course, the
expectation is end-to-end,

599
00:29:26,937 --> 00:29:30,093
you know, tracing of all users actions.

600
00:29:31,710 --> 00:29:35,970
So now this is possible
with IAM Identity Center.

601
00:29:35,970 --> 00:29:40,230
So you will enable single sign
on with IAM Identity Center

602
00:29:40,230 --> 00:29:43,560
which allows end-to-end, you
know, data access traceability,

603
00:29:43,560 --> 00:29:45,450
you have fine grained permissions there

604
00:29:45,450 --> 00:29:48,150
and you'll access based
on the user's identity.

605
00:29:48,150 --> 00:29:50,790
Now we support this from
both interactive sessions

606
00:29:50,790 --> 00:29:54,480
and jobs from SageMaker Unified studio

607
00:29:54,480 --> 00:29:56,760
and on the specific EMR, you know,

608
00:29:56,760 --> 00:29:59,340
versions mentioned, from your 7.11,

609
00:29:59,340 --> 00:30:01,500
it is supported on all EMR deployments

610
00:30:01,500 --> 00:30:04,803
and this works in Glue 5 as well.

611
00:30:06,390 --> 00:30:07,740
So let's look at how this works.

612
00:30:07,740 --> 00:30:09,840
So I have two users
here, you know, Charlie

613
00:30:09,840 --> 00:30:11,160
and, you know, Elle.

614
00:30:11,160 --> 00:30:14,040
Now Charlie wants to
access a specific S3 table

615
00:30:14,040 --> 00:30:17,910
and Elle wants to access another S3 table.

616
00:30:17,910 --> 00:30:20,400
So, when they log into SageMaker Studio,

617
00:30:20,400 --> 00:30:23,400
they will authenticate
with IAM Identity Center,

618
00:30:23,400 --> 00:30:24,600
which federates the identity

619
00:30:24,600 --> 00:30:28,620
from Active Directory, Okta, etc.

620
00:30:28,620 --> 00:30:31,890
And that identity and
authorization is then pushed down

621
00:30:31,890 --> 00:30:33,330
to EMR when they submit a job

622
00:30:33,330 --> 00:30:35,520
or start an interactive session

623
00:30:35,520 --> 00:30:37,350
and that will talk to lake formation

624
00:30:37,350 --> 00:30:38,820
and can give them access.

625
00:30:38,820 --> 00:30:41,700
So I hope this was
useful of how, you know,

626
00:30:41,700 --> 00:30:44,460
trust identity propagation

627
00:30:44,460 --> 00:30:46,710
as well as fine grained access control,

628
00:30:46,710 --> 00:30:49,380
delivers the enterprise,
you know, security.

629
00:30:49,380 --> 00:30:51,420
Alright, so now I would like

630
00:30:51,420 --> 00:30:55,320
to invite Anjali Norwood from
Netflix to share their story

631
00:30:55,320 --> 00:30:56,767
of evaluating EMR.

632
00:31:01,980 --> 00:31:02,830
- Thank you Neil.

633
00:31:06,030 --> 00:31:08,940
I'm here to share our Netflix story

634
00:31:08,940 --> 00:31:13,860
of POCing Spark EMR.

635
00:31:13,860 --> 00:31:16,110
Stick around with me to find out

636
00:31:16,110 --> 00:31:17,640
whether this has a happy ending.

637
00:31:17,640 --> 00:31:19,290
Was this a drama?

638
00:31:19,290 --> 00:31:21,990
Was this a story of
collaboration and teamwork

639
00:31:21,990 --> 00:31:24,360
or was there fighting, action?

640
00:31:24,360 --> 00:31:26,010
Stick around and you'll find out.

641
00:31:27,030 --> 00:31:30,720
I'm Anjali. I lead the big
data analytics platform

642
00:31:30,720 --> 00:31:31,950
at Netflix.

643
00:31:31,950 --> 00:31:35,700
My team is responsible
for management of data

644
00:31:35,700 --> 00:31:37,620
in our warehouse.

645
00:31:37,620 --> 00:31:42,620
It's an S3 table maintenance,
Iceberg table format,

646
00:31:44,580 --> 00:31:46,950
governance, security around that,

647
00:31:46,950 --> 00:31:49,380
engines to access the data,

648
00:31:49,380 --> 00:31:52,530
Spark being one of the big workhorses.

649
00:31:52,530 --> 00:31:56,580
Other engines like Trino, Druid,
solutions around Snowflake,

650
00:31:56,580 --> 00:32:00,510
new addition, LanceDB and
Orchestration Technologies.

651
00:32:00,510 --> 00:32:02,613
This is something we have built in-house.

652
00:32:06,270 --> 00:32:09,393
Netflix is first and foremost
an entertainment company,

653
00:32:10,560 --> 00:32:14,670
but almost all the
decisions are data driven.

654
00:32:14,670 --> 00:32:18,270
So when you log on to
your Netflix account,

655
00:32:18,270 --> 00:32:22,290
TV, cell phone, you
start seeing these rows.

656
00:32:22,290 --> 00:32:24,420
This is recommended content just for you.

657
00:32:24,420 --> 00:32:26,403
That decision is very much data driven.

658
00:32:27,870 --> 00:32:30,810
Which rows you see, in
what order you see them,

659
00:32:30,810 --> 00:32:34,140
which cover art you see,
this is all personalized

660
00:32:34,140 --> 00:32:35,880
and data driven decision.

661
00:32:35,880 --> 00:32:37,470
These are user facing decisions.

662
00:32:37,470 --> 00:32:39,630
And then there are decisions
we make internally,

663
00:32:39,630 --> 00:32:42,810
like when we run Spark on EC2,

664
00:32:42,810 --> 00:32:44,520
which instance type should we use?

665
00:32:44,520 --> 00:32:46,650
Is R7 ready for adoption

666
00:32:46,650 --> 00:32:49,350
or should we stay on
R5 for a little while?

667
00:32:49,350 --> 00:32:51,550
All these decisions are
also made with data.

668
00:32:54,570 --> 00:32:59,370
That is our streaming video on
demand, our legacy business.

669
00:32:59,370 --> 00:33:04,080
But now we started offering
ads plans in some countries.

670
00:33:04,080 --> 00:33:07,020
We started carrying live content.

671
00:33:07,020 --> 00:33:10,473
Anybody watching NFL
on Christmas Day? I am.

672
00:33:11,640 --> 00:33:16,293
Games, Netflix House, those
are consumer experiences.

673
00:33:17,280 --> 00:33:20,553
And also we will start carrying podcasts.

674
00:33:21,390 --> 00:33:22,530
This is all to say

675
00:33:22,530 --> 00:33:24,960
all of these new business
initiatives result

676
00:33:24,960 --> 00:33:27,210
in more and more data in our warehouse

677
00:33:27,210 --> 00:33:30,453
and more and more of a need to
get insight out of that data.

678
00:33:34,260 --> 00:33:36,390
This is a bit of our Spark story.

679
00:33:36,390 --> 00:33:38,883
This started more than seven years ago.

680
00:33:39,900 --> 00:33:43,470
Netflix decided to do storage
and compute decoupling.

681
00:33:43,470 --> 00:33:46,170
That was a big bet we took.

682
00:33:46,170 --> 00:33:47,133
It paid off.

683
00:33:48,480 --> 00:33:52,500
Since then, we have been
operating Spark on Hadoop.

684
00:33:52,500 --> 00:33:56,880
We cut a fork from open source
Spark, we hyper optimized it

685
00:33:56,880 --> 00:34:01,260
and customized it for Netflix
scale for performance,

686
00:34:01,260 --> 00:34:03,933
for a variety of use cases that emerge.

687
00:34:05,250 --> 00:34:07,710
We operate multiple Hadoop clusters

688
00:34:07,710 --> 00:34:11,250
and some stats are there just
to give you an idea of scale.

689
00:34:11,250 --> 00:34:13,230
Compared to what Kinshuk shared,

690
00:34:13,230 --> 00:34:16,917
this doesn't look like a lot,
but 11,000 unique workflows,

691
00:34:16,917 --> 00:34:21,917
250,000 jobs, more than exabyte
size data in our warehouse.

692
00:34:22,560 --> 00:34:26,610
That's some pretty good
scale that we operate at.

693
00:34:26,610 --> 00:34:31,173
We have about 8,000, 8,500 or so R7 nodes.

694
00:34:33,600 --> 00:34:35,010
What do we use Spark for?

695
00:34:35,010 --> 00:34:36,540
It's a variety of things.

696
00:34:36,540 --> 00:34:38,220
Getting insight out of data,

697
00:34:38,220 --> 00:34:43,140
ETL, that's a very popular engine for ETL,

698
00:34:43,140 --> 00:34:47,343
large scale analytics,
recommendations and more.

699
00:34:51,420 --> 00:34:54,840
Then you would ask, if we
have this nice platform

700
00:34:54,840 --> 00:34:57,810
and we have been operating
for all this time,

701
00:34:57,810 --> 00:35:00,243
why would we even think of something else?

702
00:35:01,350 --> 00:35:04,710
Turns out things are changing
a little bit for Netflix.

703
00:35:04,710 --> 00:35:07,590
When we started out, our
warehouse was open by default.

704
00:35:07,590 --> 00:35:08,580
There was no security.

705
00:35:08,580 --> 00:35:11,430
Anybody could access any piece of data.

706
00:35:11,430 --> 00:35:13,500
Netflix is a very transparent culture.

707
00:35:13,500 --> 00:35:15,930
We still are a transparent culture,

708
00:35:15,930 --> 00:35:18,840
but addition of new
businesses like ads mean

709
00:35:18,840 --> 00:35:20,820
that some of the data is sensitive.

710
00:35:20,820 --> 00:35:22,113
It has to be protected.

711
00:35:23,760 --> 00:35:28,110
Our current platform that we
operate on, Hadoop and Yarn,

712
00:35:28,110 --> 00:35:30,630
it's kind of hard to get
the level of security

713
00:35:30,630 --> 00:35:32,283
and isolation that we need.

714
00:35:33,450 --> 00:35:36,870
Adopting new technologies is
hard because of the scale,

715
00:35:36,870 --> 00:35:39,390
the number of jobs and workflows we run.

716
00:35:39,390 --> 00:35:41,460
A Scala migration can take a long time.

717
00:35:41,460 --> 00:35:43,860
Python version change takes a long time.

718
00:35:43,860 --> 00:35:45,900
Spark migration takes a long time.

719
00:35:45,900 --> 00:35:47,673
It's a lot of work.

720
00:35:50,400 --> 00:35:52,650
There is a lot of operational overhead

721
00:35:52,650 --> 00:35:54,030
and cost to our teams.

722
00:35:54,030 --> 00:35:57,180
We call that undifferentiated
heavy lifting.

723
00:35:57,180 --> 00:35:59,880
As in, this work is important,

724
00:35:59,880 --> 00:36:02,670
but it's not moving the
needle for business.

725
00:36:02,670 --> 00:36:05,520
We would like to see how
we can offload this work,

726
00:36:05,520 --> 00:36:07,293
reduce this work for our teams.

727
00:36:08,370 --> 00:36:10,590
Limited support for specialized hardware,

728
00:36:10,590 --> 00:36:13,890
with Gen AI use cases
emerging, we need GPU support,

729
00:36:13,890 --> 00:36:17,043
which is again, harder to
do with our current setup.

730
00:36:18,330 --> 00:36:19,860
Our environment is good,

731
00:36:19,860 --> 00:36:22,500
but it's a little bit
difficult to use and debug.

732
00:36:22,500 --> 00:36:26,613
Ease of use is not where
we would like it to be.

733
00:36:30,240 --> 00:36:33,480
And this is where we started
thinking about Amazon EMR.

734
00:36:33,480 --> 00:36:38,220
This journey started Q4
of 2024 where we said,

735
00:36:38,220 --> 00:36:40,860
you know what, let's check out EMR.

736
00:36:40,860 --> 00:36:43,590
And turns out it provides isolation,

737
00:36:43,590 --> 00:36:45,693
it provides improved security,

738
00:36:46,620 --> 00:36:51,540
it provides frequent releases
every 90 days, every quarter.

739
00:36:51,540 --> 00:36:55,110
Along with that comes new
Iceberg, new Python, Scala,

740
00:36:55,110 --> 00:37:00,110
all the nice support,
reduces operational overhead,

741
00:37:00,690 --> 00:37:05,303
things scale up, auto scale,
up, down, supports GPUs,

742
00:37:06,660 --> 00:37:08,940
integrates very well with the rest

743
00:37:08,940 --> 00:37:13,083
of the AWS services like
S3, IAM and other things.

744
00:37:16,560 --> 00:37:18,390
So what did we test?

745
00:37:18,390 --> 00:37:20,400
EMR comes in various flavors.

746
00:37:20,400 --> 00:37:23,073
We started with testing EMR on EC2.

747
00:37:24,120 --> 00:37:26,730
Couple reasons. One is it is very close to

748
00:37:26,730 --> 00:37:29,010
how we operate our current system.

749
00:37:29,010 --> 00:37:32,190
While we want managed services,

750
00:37:32,190 --> 00:37:34,650
we also want to kind get there slowly.

751
00:37:34,650 --> 00:37:38,100
We still wanna be able to
tune the knobs, you know,

752
00:37:38,100 --> 00:37:41,763
change the configs, make sure
things work for our users.

753
00:37:43,410 --> 00:37:45,510
We tested feature compatibility.

754
00:37:45,510 --> 00:37:47,580
This was important because as I mentioned,

755
00:37:47,580 --> 00:37:51,810
we have customized our
Spark for Netflix users.

756
00:37:51,810 --> 00:37:53,313
We tested performance.

757
00:37:54,150 --> 00:37:57,240
We started with TPC-DS, checks out,

758
00:37:57,240 --> 00:38:00,450
and then we decided to
bring in our user workflows.

759
00:38:00,450 --> 00:38:02,280
These are representative user workflows.

760
00:38:02,280 --> 00:38:05,160
We literally reached out and
said, give us the workflows

761
00:38:05,160 --> 00:38:07,980
that represent your team the best.

762
00:38:07,980 --> 00:38:09,510
And they fall in various categories.

763
00:38:09,510 --> 00:38:13,803
We have SQL, we have
PySpark, we have Scala, Java.

764
00:38:14,910 --> 00:38:16,293
Then we tested for scale.

765
00:38:17,670 --> 00:38:22,670
We basically mimicked our
production runs on EMR

766
00:38:23,280 --> 00:38:25,590
to see if EMR holds up.

767
00:38:25,590 --> 00:38:28,230
How is the resource consumption like?

768
00:38:28,230 --> 00:38:31,500
Second part of scale testing
was saturation testing.

769
00:38:31,500 --> 00:38:35,790
Can we just, you know, overload
the system, use up memory,

770
00:38:35,790 --> 00:38:39,240
throw all memory heavy, I/O
heavy, CPU heavy workloads

771
00:38:39,240 --> 00:38:40,443
and see how it holds up.

772
00:38:41,280 --> 00:38:43,140
And then we look for
operational complexity.

773
00:38:43,140 --> 00:38:45,450
As platform providers, my team has to know

774
00:38:45,450 --> 00:38:46,710
how to operate this platform

775
00:38:46,710 --> 00:38:49,050
and this has to be easy for them.

776
00:38:49,050 --> 00:38:50,940
And then cost, of course, is important.

777
00:38:50,940 --> 00:38:52,473
So we check that as well.

778
00:38:54,780 --> 00:38:56,700
Here are some numbers.

779
00:38:56,700 --> 00:38:58,440
If you look at the P90 column,

780
00:38:58,440 --> 00:39:03,060
you'll see that PySpark
by far saw the most gains

781
00:39:03,060 --> 00:39:05,073
followed by SQL and Scala.

782
00:39:06,360 --> 00:39:07,890
Again, I would like to remind you

783
00:39:07,890 --> 00:39:10,230
that our platform is hyper optimized,

784
00:39:10,230 --> 00:39:13,083
so seeing these kind of gains is amazing.

785
00:39:16,380 --> 00:39:17,880
Here is another slide.

786
00:39:17,880 --> 00:39:20,103
This talks about resource consumption.

787
00:39:22,200 --> 00:39:26,090
Again, PySpark shines
with most vCore gains

788
00:39:27,900 --> 00:39:32,900
at P90, followed by SQL and then Scala.

789
00:39:35,310 --> 00:39:39,390
Know that the consumption of
vCPU is directly correlated

790
00:39:39,390 --> 00:39:40,223
to cost.

791
00:39:40,223 --> 00:39:43,128
So the more, the less resources you use,

792
00:39:43,128 --> 00:39:44,403
that's a good thing.

793
00:39:49,800 --> 00:39:51,543
It's a bit of an eyeful chart.

794
00:39:53,040 --> 00:39:56,910
We have a pretty long doc
detailing our POC and evaluation.

795
00:39:56,910 --> 00:39:59,010
It's kind of hard to talk through it all,

796
00:39:59,010 --> 00:40:01,173
but some salient points
I wanna touch upon.

797
00:40:02,460 --> 00:40:04,110
Future proofing.

798
00:40:04,110 --> 00:40:07,050
We are looking to be on a platform

799
00:40:07,050 --> 00:40:08,580
that would just work for us

800
00:40:08,580 --> 00:40:11,830
and will grow with us
as our business grows

801
00:40:12,810 --> 00:40:17,810
and we see AWS investing
heavily, have strong roadmap

802
00:40:17,880 --> 00:40:21,510
and have continuous
innovation in this area.

803
00:40:21,510 --> 00:40:23,730
This was important for us.

804
00:40:23,730 --> 00:40:25,350
Security, I already mentioned,

805
00:40:25,350 --> 00:40:29,610
security and isolation
is really great with EMR.

806
00:40:29,610 --> 00:40:32,370
Cost structure, we found to be competitive

807
00:40:32,370 --> 00:40:33,363
and as transparent.

808
00:40:34,530 --> 00:40:36,093
User experience is good.

809
00:40:37,020 --> 00:40:38,943
Notebook integrations are supported.

810
00:40:40,200 --> 00:40:42,030
Our operational burden will go down.

811
00:40:42,030 --> 00:40:43,623
This is a managed service.

812
00:40:45,150 --> 00:40:46,533
Performance and scale.

813
00:40:47,880 --> 00:40:49,860
This worked out. I shared
performance results.

814
00:40:49,860 --> 00:40:52,650
Scale results were also equivalent

815
00:40:52,650 --> 00:40:54,450
or better than our current platform.

816
00:40:58,200 --> 00:41:01,590
This platform is pretty extensible
through bootstrap actions

817
00:41:01,590 --> 00:41:03,030
and so on.

818
00:41:03,030 --> 00:41:06,180
It's not going to be as
flexible as do it yourself,

819
00:41:06,180 --> 00:41:08,130
but that is to be expected.

820
00:41:08,130 --> 00:41:10,620
But taking that into account,
we still found this platform

821
00:41:10,620 --> 00:41:11,823
to be pretty nimble.

822
00:41:13,080 --> 00:41:15,420
Observability and debugging.

823
00:41:15,420 --> 00:41:17,040
We did not try CloudWatch.

824
00:41:17,040 --> 00:41:19,710
We are bringing our own observability,

825
00:41:19,710 --> 00:41:21,540
but rest of the integrations

826
00:41:21,540 --> 00:41:23,040
and the overall story looks good.

827
00:41:23,040 --> 00:41:24,960
We have a little bit of work to do here,

828
00:41:24,960 --> 00:41:27,123
but the story still looks good.

829
00:41:29,790 --> 00:41:33,390
Going back to the slide that
was shown earlier by Kinshuk,

830
00:41:33,390 --> 00:41:35,970
we checked out three of the four boxes

831
00:41:35,970 --> 00:41:38,610
and find them to be really great.

832
00:41:38,610 --> 00:41:41,850
Price, performance, ease of use, security.

833
00:41:41,850 --> 00:41:44,460
AI acceleration is something
we haven't tried yet.

834
00:41:44,460 --> 00:41:46,143
This is something we'll try next.

835
00:41:48,510 --> 00:41:51,360
And this is how this story ends.

836
00:41:51,360 --> 00:41:54,060
But maybe the new story begins, right?

837
00:41:54,060 --> 00:41:58,500
So we have decided to move
Netflix Spark workflows gradually

838
00:41:58,500 --> 00:42:00,360
to EMR.

839
00:42:00,360 --> 00:42:02,410
The way we plan to do that is

840
00:42:03,530 --> 00:42:06,750
in the first half of next
year, we have some work to do.

841
00:42:06,750 --> 00:42:09,930
We are paying off tech debt,
building new control plane,

842
00:42:09,930 --> 00:42:12,003
job routing infrastructure and so on.

843
00:42:13,020 --> 00:42:14,310
In second half,

844
00:42:14,310 --> 00:42:17,580
we'll be migrating our
own platform workflows,

845
00:42:17,580 --> 00:42:19,893
dogfooding, checking things out.

846
00:42:20,730 --> 00:42:24,060
We'll fine tune based on our learnings

847
00:42:24,060 --> 00:42:28,053
and in 2027, we'll be
migrating user workflows.

848
00:42:29,520 --> 00:42:32,520
We plan to test EMR serverless.

849
00:42:32,520 --> 00:42:35,490
During the POC, we kind
of touched upon it,

850
00:42:35,490 --> 00:42:36,930
saw early positive results,

851
00:42:36,930 --> 00:42:39,843
but didn't have a time
to do a deep dive there.

852
00:42:41,040 --> 00:42:43,440
AI acceleration, as I mentioned.

853
00:42:43,440 --> 00:42:44,883
Adding cost attribution.

854
00:42:49,470 --> 00:42:54,470
I want to thank the EMR account
team, Vedi, Manju, Karthik.

855
00:42:54,570 --> 00:42:55,830
It was one team.

856
00:42:55,830 --> 00:43:00,830
EMR product team, June, Geo,
of course Kinshuk, Neil.

857
00:43:02,700 --> 00:43:06,600
It's been, it's like an
extension of Netflix team.

858
00:43:06,600 --> 00:43:08,340
It never felt like they're outsiders.

859
00:43:08,340 --> 00:43:12,270
We were working together,
was a great experience.

860
00:43:12,270 --> 00:43:16,530
Big thank you to Netflix
Spark team, Anurag and team,

861
00:43:16,530 --> 00:43:19,620
DSI, security, other BDAP teams.

862
00:43:19,620 --> 00:43:23,160
It was a huge effort that
went on for three quarters.

863
00:43:23,160 --> 00:43:27,630
At the end of Q2 this
year, we finished our POC

864
00:43:27,630 --> 00:43:29,100
and then there were more follow ups

865
00:43:29,100 --> 00:43:32,370
because we have custom
patches, we have customizations

866
00:43:32,370 --> 00:43:33,780
that we needed to talk through

867
00:43:33,780 --> 00:43:36,120
and all that is coming together.

868
00:43:36,120 --> 00:43:41,120
I'm super excited for
our journey with AWS EMR.

869
00:43:41,370 --> 00:43:43,593
The best is yet to come. Thank you.

