1
00:00:03,450 --> 00:00:04,009
Thank you.

2
00:00:04,509 --> 00:00:06,610
Now the shipping forecast issued by the Met Office

3
00:00:06,610 --> 00:00:08,829
on behalf of the Maritime and Coast Guard Agency

4
00:00:08,829 --> 00:00:10,970
at 5:05 on Friday,

5
00:00:10,978 --> 00:00:12,189
the 4th of July.

6
00:00:12,689 --> 00:00:15,089
There are warnings of gales in Fitzroy, Shannon,

7
00:00:15,198 --> 00:00:17,359
Rockwall, Malin and Hebrides.

8
00:00:18,019 --> 00:00:20,350
The general synopsis at midnight, Atlantic

9
00:00:20,350 --> 00:00:22,429
low 997, moving rather

10
00:00:22,429 --> 00:00:23,350
quickly eastwards,

11
00:00:23,629 --> 00:00:25,629
expected Pharaohs, 998

12
00:00:25,629 --> 00:00:26,690
by midnight tonight.

13
00:00:27,548 --> 00:00:29,708
Low, Iberia, 1016,

14
00:00:29,908 --> 00:00:31,969
slow moving with little change.

15
00:00:32,707 --> 00:00:35,097
The area forecast for the next 24 hours.

16
00:00:35,429 --> 00:00:37,667
Viking, south or southwest, 4

17
00:00:37,667 --> 00:00:40,027
to 6, becoming cyclonic 3 to 5

18
00:00:40,027 --> 00:00:41,768
later. Rain or showers,

19
00:00:42,027 --> 00:00:43,508
good, occasionally poor.

20
00:00:44,439 --> 00:00:46,770
North at irra, south atirra, southerly

21
00:00:46,770 --> 00:00:48,880
or southwesterly, 4 to 6.

22
00:00:49,130 --> 00:00:51,459
Rain or showers, good, occasionally

23
00:00:51,459 --> 00:00:53,679
poor. 40s

24
00:00:53,679 --> 00:00:56,039
chromaty 4th Tyne dogger.

25
00:00:56,859 --> 00:00:59,219
West, 4 to 6, backing southwest,

26
00:00:59,340 --> 00:01:00,259
5 to 7.

27
00:01:00,539 --> 00:01:01,439
Occasional rain,

28
00:01:01,700 --> 00:01:03,118
good, occasionally moderate.

29
00:01:04,768 --> 00:01:06,259
Fisher, German Bight

30
00:01:06,808 --> 00:01:09,319
West or southwest, 4 to 6.

31
00:01:09,649 --> 00:01:12,289
Showers, rain later, good, occasionally

32
00:01:12,289 --> 00:01:13,268
moderate later.

33
00:01:16,519 --> 00:01:18,558
Hi everyone, good morning, thank you for joining us for

34
00:01:18,558 --> 00:01:19,180
this session.

35
00:01:19,799 --> 00:01:21,959
Uh, my name's Ed Steele, I'm IT Fellow

36
00:01:21,959 --> 00:01:24,000
for Data Science at the Met Office, the

37
00:01:24,000 --> 00:01:26,120
UK's National Weather Service. And what

38
00:01:26,120 --> 00:01:28,269
you've just listened to is the shipping

39
00:01:28,269 --> 00:01:30,439
forecast, the world's oldest

40
00:01:30,439 --> 00:01:32,629
and longest running forecast, in

41
00:01:32,629 --> 00:01:35,180
this case, on the anniversary of its broadcast

42
00:01:35,588 --> 00:01:37,819
of its centenary earlier this year.

43
00:01:38,359 --> 00:01:40,359
This is a cornerstone of regional

44
00:01:40,359 --> 00:01:42,400
maritime safety and a British

45
00:01:42,400 --> 00:01:43,680
cultural institution.

46
00:01:44,129 --> 00:01:46,290
It's issued by the Met Office on behalf

47
00:01:46,290 --> 00:01:48,769
of the Maritime Coast Guard Agency, the MCA,

48
00:01:49,040 --> 00:01:51,088
and it's broadcast to mariners 4 times

49
00:01:51,088 --> 00:01:52,409
a day via Navtex

50
00:01:52,808 --> 00:01:55,168
and via BBC Radio 4

51
00:01:55,168 --> 00:01:55,829
as well.

52
00:01:56,638 --> 00:01:58,689
The forecast, as you've just heard an excerpt

53
00:01:58,689 --> 00:02:01,168
of, uh, comprises predicted

54
00:02:01,168 --> 00:02:03,168
conditions for 31 different

55
00:02:03,168 --> 00:02:04,120
sea areas

56
00:02:04,448 --> 00:02:07,510
that must be analyzed by human meteorologists

57
00:02:07,659 --> 00:02:10,008
and condensed into text sentences

58
00:02:10,169 --> 00:02:12,189
with a very specific length

59
00:02:12,189 --> 00:02:13,050
and format,

60
00:02:13,368 --> 00:02:15,508
following a very specific

61
00:02:15,569 --> 00:02:17,550
and strict set of rules.

62
00:02:20,808 --> 00:02:23,050
So the shipping forecast

63
00:02:23,050 --> 00:02:23,868
itself though,

64
00:02:24,558 --> 00:02:26,610
is really a pioneer in

65
00:02:26,610 --> 00:02:29,058
the communication of meteorological

66
00:02:29,058 --> 00:02:31,250
information. And it can and it can trace its

67
00:02:31,250 --> 00:02:34,368
roots back to October 1859,

68
00:02:34,569 --> 00:02:36,750
when the steam clipper, the Royal Charter,

69
00:02:36,969 --> 00:02:39,169
foundered in a violent storm off the west coast

70
00:02:39,169 --> 00:02:39,909
of the UK.

71
00:02:41,719 --> 00:02:43,808
Over 450 lives

72
00:02:43,808 --> 00:02:46,129
were lost associated with just this ship

73
00:02:46,129 --> 00:02:48,210
alone, and the storm itself

74
00:02:48,409 --> 00:02:50,610
went on to claim at least a further

75
00:02:50,610 --> 00:02:52,849
800 lives in total, and

76
00:02:52,849 --> 00:02:55,189
133 ships were lost,

77
00:02:55,649 --> 00:02:57,770
with a further 90 badly

78
00:02:57,770 --> 00:02:58,409
damaged.

79
00:02:59,278 --> 00:03:00,770
The founder of the Met Office,

80
00:03:01,250 --> 00:03:03,349
Admiral Robert Fitzroy, as a direct result

81
00:03:03,349 --> 00:03:03,960
of this,

82
00:03:04,569 --> 00:03:05,669
introduced the first,

83
00:03:06,008 --> 00:03:08,210
uh, British storm warning service

84
00:03:08,210 --> 00:03:08,750
for shipping

85
00:03:09,008 --> 00:03:11,669
in February 1861.

86
00:03:12,169 --> 00:03:14,288
And in appreciation of the valuable help

87
00:03:14,288 --> 00:03:16,729
from, uh, the meteorological Service,

88
00:03:16,929 --> 00:03:19,008
um, by the radio reports that were

89
00:03:19,008 --> 00:03:21,368
received from ships, a weather bulletin

90
00:03:21,368 --> 00:03:22,508
called Weather Shipping

91
00:03:22,889 --> 00:03:25,349
was started, and this became,

92
00:03:25,449 --> 00:03:27,689
uh, the shipping forecast as we know

93
00:03:27,689 --> 00:03:29,909
today. So

94
00:03:29,909 --> 00:03:32,349
fast forward to present times and the Met Office

95
00:03:32,349 --> 00:03:34,949
continues to push the boundaries of science and

96
00:03:34,949 --> 00:03:37,229
technology, helping people make better

97
00:03:37,229 --> 00:03:39,639
decisions to stay safe and thrive

98
00:03:39,830 --> 00:03:41,949
by delivering the most trusted weather

99
00:03:41,949 --> 00:03:44,788
and climate intelligence in a radically

100
00:03:44,788 --> 00:03:45,788
changing world.

101
00:03:47,360 --> 00:03:49,588
We're one of the few, if not the only

102
00:03:49,588 --> 00:03:52,278
organizations in the world that crosses a huge diversity

103
00:03:52,278 --> 00:03:53,199
of boundaries,

104
00:03:53,558 --> 00:03:55,679
covering weather and climate across

105
00:03:55,679 --> 00:03:56,800
all timescales,

106
00:03:57,159 --> 00:03:59,338
drawing between science and services,

107
00:03:59,520 --> 00:04:00,300
to both government,

108
00:04:01,000 --> 00:04:03,278
industry, across civilian and military,

109
00:04:03,550 --> 00:04:06,069
both nationally and internationally,

110
00:04:06,278 --> 00:04:08,679
and predicting meteorology and extending

111
00:04:08,679 --> 00:04:10,990
this now into hazards.

112
00:04:11,169 --> 00:04:13,618
And this all happens in one place.

113
00:04:15,349 --> 00:04:17,588
But actually pulling this together, a

114
00:04:17,588 --> 00:04:19,750
recent report issued last year,

115
00:04:19,980 --> 00:04:22,269
um, from an independent analysis

116
00:04:22,670 --> 00:04:24,189
showed that the Met Office is

117
00:04:24,869 --> 00:04:27,350
poised to deliver 56 billion

118
00:04:27,350 --> 00:04:29,649
pounds of benefit to the UK economy

119
00:04:29,809 --> 00:04:32,389
and society over the next 10 years.

120
00:04:32,670 --> 00:04:34,670
And this gives a return on investment

121
00:04:34,670 --> 00:04:36,750
from the taxpayer, um, in the

122
00:04:36,750 --> 00:04:39,048
UK of 19 to 1

123
00:04:39,178 --> 00:04:40,500
on our public money.

124
00:04:40,819 --> 00:04:42,829
So a hugely important

125
00:04:42,949 --> 00:04:43,809
organization and

126
00:04:44,389 --> 00:04:44,988
service.

127
00:04:46,428 --> 00:04:48,829
But what does a 100 year old forecast

128
00:04:48,829 --> 00:04:50,230
have to do with AI?

129
00:04:51,230 --> 00:04:52,309
Well, in this talk,

130
00:04:52,790 --> 00:04:54,910
Dinesh and I are going to give an introduction

131
00:04:54,910 --> 00:04:56,069
to weather forecasting.

132
00:04:56,389 --> 00:04:58,509
We're going to talk about the last mile in weather

133
00:04:58,509 --> 00:04:59,350
forecasting

134
00:04:59,709 --> 00:05:02,338
before proceeding to talk about actually how we architect

135
00:05:02,338 --> 00:05:04,509
these solutions, uh, to be able

136
00:05:04,509 --> 00:05:07,069
to deliver these vital services.

137
00:05:08,100 --> 00:05:10,858
We've conducted some unique experiments

138
00:05:10,858 --> 00:05:13,209
er involving fine tuning of the Amazon

139
00:05:13,209 --> 00:05:15,379
Nova Foundation model, which we'll present

140
00:05:15,379 --> 00:05:17,379
some initial results from and talk

141
00:05:17,379 --> 00:05:19,420
a bit about some of the evaluation of these

142
00:05:19,619 --> 00:05:20,720
so that they can be adopted

143
00:05:21,059 --> 00:05:23,149
er by other um

144
00:05:23,290 --> 00:05:25,540
by other interested er groups.

145
00:05:27,309 --> 00:05:29,709
So how does the Met Office forecast the weather?

146
00:05:30,069 --> 00:05:32,189
Well, it first starts by understanding

147
00:05:32,189 --> 00:05:33,869
what the weather is doing now.

148
00:05:34,230 --> 00:05:36,309
So we're taking in observations from

149
00:05:36,309 --> 00:05:38,309
all over the world. In fact, we're ingesting about

150
00:05:38,309 --> 00:05:40,790
215 billion observations

151
00:05:40,980 --> 00:05:43,670
every day that provide the starting

152
00:05:43,670 --> 00:05:45,689
conditions for the numerical models that we run

153
00:05:46,069 --> 00:05:48,108
to predict how the conditions are

154
00:05:48,108 --> 00:05:49,199
going to evolve.

155
00:05:50,028 --> 00:05:52,290
This takes place on our supercomputer,

156
00:05:52,588 --> 00:05:55,059
um, where we're computing, using the equations

157
00:05:55,059 --> 00:05:57,369
of motion, uh, the evolution

158
00:05:57,509 --> 00:05:59,660
of those conditions, um,

159
00:05:59,670 --> 00:06:01,730
using mathematical equations

160
00:06:01,949 --> 00:06:04,149
um to uh to to then

161
00:06:04,149 --> 00:06:05,399
give us a,

162
00:06:05,790 --> 00:06:06,569
a prediction

163
00:06:06,910 --> 00:06:08,949
of what the conditions are going to be.

164
00:06:09,449 --> 00:06:11,699
The data from this is provided both

165
00:06:11,699 --> 00:06:13,980
directly to some users, but also passed

166
00:06:13,980 --> 00:06:16,170
to some of our operational meteorologists,

167
00:06:16,178 --> 00:06:18,699
who add further value by interpreting

168
00:06:18,699 --> 00:06:20,699
some of these complicated fields to

169
00:06:20,699 --> 00:06:22,778
be able to distill this information into value

170
00:06:22,778 --> 00:06:23,838
added services

171
00:06:24,220 --> 00:06:25,000
for the public.

172
00:06:25,394 --> 00:06:28,053
And finally, the accurate weather and climate

173
00:06:28,053 --> 00:06:30,875
information is then is then shared

174
00:06:30,875 --> 00:06:33,035
more broadly, benefiting all

175
00:06:33,035 --> 00:06:35,884
of the different stakeholders with whom we worker

176
00:06:35,884 --> 00:06:37,915
from defense right through to

177
00:06:37,915 --> 00:06:40,504
aviation, er marine and

178
00:06:40,535 --> 00:06:41,314
and others.

179
00:06:43,059 --> 00:06:45,338
Now when you step back and think about it,

180
00:06:45,619 --> 00:06:47,819
uh, weather forecasting is actually

181
00:06:47,819 --> 00:06:49,420
a remarkable achievement.

182
00:06:49,738 --> 00:06:51,399
And over the years, we've undergone

183
00:06:51,819 --> 00:06:54,619
what's really termed a quiet a revolution

184
00:06:54,778 --> 00:06:57,040
in terms of our ability to predict the weather.

185
00:06:58,048 --> 00:07:00,250
So physics-based numerical models

186
00:07:00,250 --> 00:07:02,850
have seen an improvement in accuracy of

187
00:07:02,850 --> 00:07:04,970
between about half a day and 1

188
00:07:04,970 --> 00:07:05,988
day per decade.

189
00:07:06,488 --> 00:07:09,629
So for example, our forecast in 2013,

190
00:07:11,059 --> 00:07:13,720
uh, 5 days ahead, was as accurate

191
00:07:13,720 --> 00:07:15,910
as our forecast in

192
00:07:15,910 --> 00:07:18,189
2003, for example,

193
00:07:18,369 --> 00:07:20,410
uh, at 4 days ahead lead

194
00:07:20,410 --> 00:07:22,488
time. And these improvements have

195
00:07:22,488 --> 00:07:24,928
been driven by upgrades to the

196
00:07:25,079 --> 00:07:27,290
observations and the assimilation of those

197
00:07:27,290 --> 00:07:29,759
data. The dynamics and physical

198
00:07:30,178 --> 00:07:32,519
representation of these processes

199
00:07:32,519 --> 00:07:34,759
within the maths, and also

200
00:07:34,759 --> 00:07:37,040
improvements in model resolution er

201
00:07:37,040 --> 00:07:39,119
that have er been ongoing through

202
00:07:39,119 --> 00:07:39,899
this time.

203
00:07:40,399 --> 00:07:43,149
And these physics-based models

204
00:07:43,160 --> 00:07:44,738
remain an essential component

205
00:07:45,160 --> 00:07:47,559
of our forecasting um

206
00:07:47,559 --> 00:07:50,079
approach. But

207
00:07:50,079 --> 00:07:52,278
obviously it's remiss not to mention

208
00:07:52,358 --> 00:07:54,678
er machine learning and the accompanying

209
00:07:54,678 --> 00:07:56,720
AI revolution that we've er

210
00:07:56,720 --> 00:07:57,439
experienced.

211
00:07:57,869 --> 00:08:00,459
Now the Met Office has always been a big data

212
00:08:00,459 --> 00:08:02,588
organization and AI and machine learning

213
00:08:02,588 --> 00:08:04,920
activities have likewise always been ongoing

214
00:08:04,920 --> 00:08:07,079
for many years, although these haven't

215
00:08:07,079 --> 00:08:09,160
always been er perhaps

216
00:08:09,160 --> 00:08:11,399
acknowledged under that specific label.

217
00:08:12,059 --> 00:08:13,238
But undeniably,

218
00:08:13,540 --> 00:08:15,608
advances in deep learning technologies

219
00:08:15,608 --> 00:08:17,939
in particular, are really rewriting

220
00:08:17,939 --> 00:08:20,500
the rules of entire industries and

221
00:08:20,500 --> 00:08:22,160
including how we predict the weather

222
00:08:22,569 --> 00:08:23,600
um as well.

223
00:08:23,858 --> 00:08:25,939
And what's really brought us to this point

224
00:08:26,129 --> 00:08:28,298
are the unprecedented volumes of data,

225
00:08:28,660 --> 00:08:30,738
some of which from observations, some of which

226
00:08:30,738 --> 00:08:32,139
from numerical predictions.

227
00:08:32,690 --> 00:08:35,070
Increasing availability of powerful

228
00:08:35,070 --> 00:08:36,629
er compute capacity,

229
00:08:37,090 --> 00:08:39,750
including on services such as AWS

230
00:08:40,210 --> 00:08:42,570
and then also equipping our expert

231
00:08:42,570 --> 00:08:44,808
scientists with the data science

232
00:08:44,808 --> 00:08:47,308
tools to really be able to exploit

233
00:08:47,450 --> 00:08:49,759
these insights um and

234
00:08:49,759 --> 00:08:51,250
er and information.

235
00:08:52,899 --> 00:08:56,099
And the rise in machine learning weather models

236
00:08:56,099 --> 00:08:57,908
has been relatively profound.

237
00:08:58,340 --> 00:09:00,379
So what I'm showing here is a

238
00:09:00,379 --> 00:09:02,859
graph showing the performance

239
00:09:02,859 --> 00:09:05,000
on the Y axis of

240
00:09:05,000 --> 00:09:07,219
a particular metric for

241
00:09:07,219 --> 00:09:08,759
a data-driven model

242
00:09:09,210 --> 00:09:09,960
as a function

243
00:09:10,298 --> 00:09:13,379
of the lead time when it was published.

244
00:09:14,279 --> 00:09:16,359
And the key threshold that we're looking at

245
00:09:16,359 --> 00:09:17,548
is this blue line,

246
00:09:17,840 --> 00:09:19,979
which is uh the reference level

247
00:09:19,979 --> 00:09:22,399
of uh a physics-based

248
00:09:22,399 --> 00:09:23,538
state of the art model.

249
00:09:23,879 --> 00:09:25,580
And what we can see is that from

250
00:09:26,359 --> 00:09:28,859
September 2017 onwards,

251
00:09:28,918 --> 00:09:31,139
we're seeing. A steady rise in our ability

252
00:09:31,139 --> 00:09:33,279
to predict weather conditions for a subset

253
00:09:33,279 --> 00:09:34,219
of parameters

254
00:09:34,840 --> 00:09:37,178
using entirely data-driven

255
00:09:37,849 --> 00:09:39,960
processes, so involving

256
00:09:39,960 --> 00:09:42,038
effectively pattern recognition rather

257
00:09:42,038 --> 00:09:44,279
than directly solving the equations of motion.

258
00:09:45,149 --> 00:09:47,389
Around this time in the Met Office, uh,

259
00:09:47,450 --> 00:09:49,460
we were starting to formulate

260
00:09:49,460 --> 00:09:51,519
some of our data science framework,

261
00:09:52,109 --> 00:09:54,308
so we were keeping an eye on some of these developments,

262
00:09:54,519 --> 00:09:56,678
but also laying out how we would develop

263
00:09:56,678 --> 00:09:58,759
our own capabilities uh in

264
00:09:58,759 --> 00:10:01,038
this deep learning space to complement

265
00:10:01,038 --> 00:10:03,359
our physics-based approaches, but also

266
00:10:03,359 --> 00:10:05,739
how we would work uh with partners

267
00:10:05,918 --> 00:10:07,918
and evolve er some of our

268
00:10:07,918 --> 00:10:09,759
people uh to support this.

269
00:10:10,879 --> 00:10:13,080
But the world changed when we hit

270
00:10:13,080 --> 00:10:13,899
December

271
00:10:14,500 --> 00:10:15,788
2022.

272
00:10:16,119 --> 00:10:18,440
And my boss often refers to this as the missing

273
00:10:18,440 --> 00:10:19,048
Christmas,

274
00:10:19,399 --> 00:10:21,979
because there was a stream of publications

275
00:10:22,200 --> 00:10:24,139
of really exciting capability

276
00:10:24,599 --> 00:10:26,099
around data-driven approaches

277
00:10:26,519 --> 00:10:29,048
um in weather prediction that came out

278
00:10:29,048 --> 00:10:31,219
literally uh between about

279
00:10:31,798 --> 00:10:33,869
20th of December, um,

280
00:10:33,879 --> 00:10:35,558
to, to the sort of Christmas period.

281
00:10:36,129 --> 00:10:38,229
And what we can see importantly here

282
00:10:38,450 --> 00:10:41,129
is that for the first time for some of these parameters,

283
00:10:41,450 --> 00:10:43,570
then we're starting to see the performance of these

284
00:10:43,570 --> 00:10:45,808
data-driven approaches exceeding

285
00:10:45,808 --> 00:10:48,288
that of our physics-based

286
00:10:48,288 --> 00:10:48,928
models.

287
00:10:49,928 --> 00:10:52,279
And if we just look at some of the er

288
00:10:52,279 --> 00:10:54,500
the references here, we can also

289
00:10:54,500 --> 00:10:56,599
see that these exciting developments

290
00:10:56,658 --> 00:10:59,090
were largely driven by some advances

291
00:10:59,090 --> 00:11:01,298
er from the likes of DeepMind, from the likes

292
00:11:01,298 --> 00:11:03,500
of Nvidia, from the likes of Huawei,

293
00:11:03,779 --> 00:11:05,979
so not typical players, but being able

294
00:11:05,979 --> 00:11:08,798
to exploit some of these data science capabilities.

295
00:11:11,168 --> 00:11:11,928
However,

296
00:11:14,308 --> 00:11:15,558
A forecast

297
00:11:16,058 --> 00:11:18,340
only actually has value if

298
00:11:18,340 --> 00:11:20,739
a user can derive some sort of

299
00:11:20,739 --> 00:11:22,269
decision making benefit from it.

300
00:11:22,580 --> 00:11:24,700
And so while we've had this er

301
00:11:24,700 --> 00:11:27,320
really exciting development in, in the underpinning

302
00:11:27,658 --> 00:11:30,229
er opportunities for predicting

303
00:11:30,229 --> 00:11:32,298
what the weather will be in different

304
00:11:32,298 --> 00:11:34,798
ways, actually, there's been remarkably

305
00:11:34,798 --> 00:11:35,759
less attention

306
00:11:36,129 --> 00:11:38,219
on what the weather is going to

307
00:11:38,219 --> 00:11:40,428
do. And arguably this last

308
00:11:40,428 --> 00:11:42,590
mile in weather forecasting, a bit

309
00:11:42,590 --> 00:11:45,298
like in delivery, um, is actually

310
00:11:45,298 --> 00:11:47,889
potentially even more significant

311
00:11:48,029 --> 00:11:50,029
in terms of the opportunities for

312
00:11:50,029 --> 00:11:50,570
adding,

313
00:11:50,940 --> 00:11:53,349
Uh, value and adding benefit

314
00:11:53,349 --> 00:11:54,349
for our users,

315
00:11:54,639 --> 00:11:56,649
because it enables us to provide even more

316
00:11:56,649 --> 00:11:58,960
personalized services. It enables

317
00:11:58,960 --> 00:12:01,330
us to provide even more multi-modal

318
00:12:01,330 --> 00:12:03,729
approaches, uh, for our data delivery,

319
00:12:04,080 --> 00:12:06,369
so that we can have data and accompanying

320
00:12:06,369 --> 00:12:08,969
narrative, uh, while reducing

321
00:12:08,969 --> 00:12:11,250
some of the burden that currently these

322
00:12:11,250 --> 00:12:13,489
processes are, are reliant on, on

323
00:12:13,489 --> 00:12:14,609
human interpretation.

324
00:12:15,678 --> 00:12:17,840
So we're super excited about some of this

325
00:12:17,840 --> 00:12:19,279
stuff, and.

326
00:12:21,038 --> 00:12:23,460
To articulate this, just with a single

327
00:12:23,460 --> 00:12:25,719
product example, so in this case, uh

328
00:12:25,719 --> 00:12:27,719
we're going to use the shipping forecast as an

329
00:12:27,719 --> 00:12:29,058
example that you heard at the start.

330
00:12:30,139 --> 00:12:32,298
We need to take the complicated data

331
00:12:32,298 --> 00:12:34,899
that's output from our weather models

332
00:12:35,058 --> 00:12:37,460
and turn this into text

333
00:12:37,460 --> 00:12:38,399
at scale.

334
00:12:38,979 --> 00:12:41,090
And this isn't necessarily a trivial problem

335
00:12:41,418 --> 00:12:43,619
because these are very large arrays of data

336
00:12:43,619 --> 00:12:44,678
that we're dealing with.

337
00:12:46,210 --> 00:12:48,548
That span multiple dimensions,

338
00:12:48,769 --> 00:12:51,119
including both in space and time,

339
00:12:51,369 --> 00:12:53,690
but also span multiple parameters.

340
00:12:54,009 --> 00:12:56,090
So not only, for example, pressure, as I've

341
00:12:56,090 --> 00:12:56,729
shown here,

342
00:12:57,009 --> 00:12:59,090
but also we heard things like sea state,

343
00:12:59,250 --> 00:13:00,009
visibility,

344
00:13:00,288 --> 00:13:02,609
weather, wind speed, wind direction,

345
00:13:02,849 --> 00:13:04,788
and all of this has to be

346
00:13:05,269 --> 00:13:07,349
framed within the very specific

347
00:13:07,349 --> 00:13:09,239
and strict wording and format

348
00:13:09,769 --> 00:13:11,330
of the shipping forecast.

349
00:13:11,710 --> 00:13:13,710
So this was, this was the use case that

350
00:13:13,710 --> 00:13:15,788
we're going to uh explore in more

351
00:13:15,788 --> 00:13:16,570
detail here.

352
00:13:18,399 --> 00:13:20,479
Now just to represent this in terms of some

353
00:13:20,479 --> 00:13:22,599
of the technology challenges associated with

354
00:13:22,599 --> 00:13:24,798
this, uh this is really just an

355
00:13:24,798 --> 00:13:26,859
example demonstrator of

356
00:13:27,000 --> 00:13:29,080
the types of products and services

357
00:13:29,080 --> 00:13:31,908
that we're looking to transform using some of these technologies.

358
00:13:32,359 --> 00:13:34,399
The reason the Shipping Forecast was chosen

359
00:13:34,399 --> 00:13:36,469
was partly because it was iconic status,

360
00:13:36,759 --> 00:13:38,918
but actually because it enables us to test

361
00:13:38,918 --> 00:13:39,820
a variety of different

362
00:13:40,349 --> 00:13:42,558
uh aspects and a variety

363
00:13:42,558 --> 00:13:44,469
of different data characteristics.

364
00:13:44,840 --> 00:13:46,889
So. The forecast itself and the

365
00:13:46,889 --> 00:13:48,908
underpinning data comprises both

366
00:13:48,908 --> 00:13:50,969
atmosphere and ocean model

367
00:13:50,969 --> 00:13:51,548
output,

368
00:13:51,879 --> 00:13:54,070
so for example, things like wind speeds, but also

369
00:13:54,070 --> 00:13:54,928
wave heights.

370
00:13:55,840 --> 00:13:58,158
The format of this data is both

371
00:13:58,158 --> 00:13:59,178
probabilistic

372
00:13:59,440 --> 00:14:01,500
and deterministic, depending on which

373
00:14:01,500 --> 00:14:04,109
of those variables from which of the models

374
00:14:04,109 --> 00:14:05,960
uh have been uh

375
00:14:06,320 --> 00:14:07,219
been considered.

376
00:14:07,639 --> 00:14:09,960
And it is multidimensional, so we're

377
00:14:09,960 --> 00:14:10,519
considering

378
00:14:10,840 --> 00:14:13,038
that spatial region, so latitude,

379
00:14:13,149 --> 00:14:15,038
longitude and time.

380
00:14:16,070 --> 00:14:18,190
And what's particularly exciting about this is

381
00:14:18,190 --> 00:14:19,989
really some of the data volumes involved.

382
00:14:20,340 --> 00:14:22,729
So considering just the output

383
00:14:22,729 --> 00:14:24,830
from a single forecast

384
00:14:24,830 --> 00:14:27,080
run from one of our one of our numerical

385
00:14:27,080 --> 00:14:28,139
weather prediction models,

386
00:14:28,428 --> 00:14:30,908
then we're typically generating upwards of 45

387
00:14:30,908 --> 00:14:33,070
gigabytes of data per

388
00:14:33,070 --> 00:14:35,418
individual forecast um from the atmospheric

389
00:14:35,418 --> 00:14:37,609
models and about 5 gigabytes of data

390
00:14:37,950 --> 00:14:40,029
per individual forecast, uh, from our

391
00:14:40,029 --> 00:14:41,070
ocean models.

392
00:14:42,119 --> 00:14:42,940
When we then

393
00:14:43,558 --> 00:14:45,960
consider the shipping forecast, we're able to make some efficiencies

394
00:14:45,960 --> 00:14:48,119
and we're able to uh crop this down

395
00:14:48,119 --> 00:14:50,379
to just the particular variables that we need

396
00:14:50,519 --> 00:14:52,639
and particular area that we're

397
00:14:52,639 --> 00:14:54,759
considering, that Northwest European shelf

398
00:14:54,759 --> 00:14:57,590
region. And this results in about

399
00:14:57,590 --> 00:14:59,769
152 megabytes

400
00:14:59,769 --> 00:15:02,168
for our atmospheric data and

401
00:15:02,168 --> 00:15:03,399
about 7 megabytes

402
00:15:03,950 --> 00:15:06,168
for our ocean data. But still

403
00:15:06,168 --> 00:15:06,928
fairly large,

404
00:15:07,250 --> 00:15:09,649
particularly considering when we're

405
00:15:09,649 --> 00:15:12,330
considering fine tuning foundation

406
00:15:12,330 --> 00:15:14,769
models, then this is about sort of 85

407
00:15:14,769 --> 00:15:15,750
gigabytes of data

408
00:15:16,489 --> 00:15:17,769
for a three month period.

409
00:15:19,019 --> 00:15:21,269
An additional challenge from a data science perspective

410
00:15:21,269 --> 00:15:23,489
is that some of the formats that we're using here

411
00:15:23,629 --> 00:15:25,269
are based on net Network Comm form.

412
00:15:26,349 --> 00:15:28,899
Uh, so this is largely an atmospheric and

413
00:15:28,899 --> 00:15:30,070
an ocean convention.

414
00:15:30,340 --> 00:15:32,500
Um, there are very good libraries for being able to,

415
00:15:32,649 --> 00:15:34,859
to, to read this in, but it's,

416
00:15:34,869 --> 00:15:36,979
I guess, not a typical form for that,

417
00:15:36,989 --> 00:15:39,048
uh, many outside the weather community,

418
00:15:39,428 --> 00:15:41,450
uh, will necessarily have, uh,

419
00:15:41,500 --> 00:15:43,750
encountered, but it's quite a convenient form

420
00:15:43,750 --> 00:15:44,889
because it's effectively

421
00:15:45,320 --> 00:15:47,548
a pandas or X-array type, uh.

422
00:15:48,340 --> 00:15:50,658
Format, uh, in, in memory.

423
00:15:53,009 --> 00:15:55,509
So the solution that we were then exploring then

424
00:15:55,509 --> 00:15:56,950
was could we provide

425
00:15:57,570 --> 00:15:59,678
this information as output from our weather

426
00:15:59,678 --> 00:16:01,969
models and then serve this

427
00:16:02,168 --> 00:16:02,750
into

428
00:16:03,210 --> 00:16:05,408
uh a large language model er

429
00:16:05,408 --> 00:16:07,570
to be able to er to to

430
00:16:07,570 --> 00:16:09,070
be able to actually write this text.

431
00:16:10,250 --> 00:16:12,330
We were using both a combination of the Amazon

432
00:16:12,330 --> 00:16:15,009
Nova Lite and the Nova Pro 1.0

433
00:16:15,009 --> 00:16:15,869
models, uh,

434
00:16:16,168 --> 00:16:17,479
two hadn't yet come out.

435
00:16:17,889 --> 00:16:20,009
And just to frame, uh, this

436
00:16:20,009 --> 00:16:22,210
task, it usually takes an expert

437
00:16:22,210 --> 00:16:24,489
meteorologist a few hours a day to

438
00:16:24,489 --> 00:16:25,658
produce this forecast.

439
00:16:25,969 --> 00:16:28,000
And in just 4 weeks of prototyping with

440
00:16:28,000 --> 00:16:29,000
the team at Amazon,

441
00:16:29,288 --> 00:16:31,450
we taught Amazonova to do this with

442
00:16:31,450 --> 00:16:33,750
between 52 and 62% accuracy

443
00:16:34,168 --> 00:16:35,529
in less than 5 minutes.

444
00:16:37,548 --> 00:16:39,548
So before I hand over to Dinesh, I

445
00:16:39,548 --> 00:16:41,590
was just going to say a few words about the

446
00:16:41,590 --> 00:16:43,750
process of how we, we went about this and

447
00:16:43,750 --> 00:16:44,369
some of the

448
00:16:44,739 --> 00:16:46,849
background, some of the experiments that we conducted.

449
00:16:47,389 --> 00:16:49,668
So we're going to talk about two different approaches

450
00:16:49,668 --> 00:16:52,048
that we, that we, uh, explored.

451
00:16:52,710 --> 00:16:53,609
Both of which

452
00:16:53,940 --> 00:16:56,190
start with taking the weather sensor

453
00:16:56,190 --> 00:16:58,349
data and, uh, and

454
00:16:58,349 --> 00:17:00,469
then that gets run with our models, both

455
00:17:00,469 --> 00:17:02,629
deterministic and probabilistic models, to produce

456
00:17:02,629 --> 00:17:03,308
outputs.

457
00:17:04,878 --> 00:17:07,118
Those outputs are typically, as I say, very

458
00:17:07,118 --> 00:17:09,307
large arrays, um, and those consi

459
00:17:09,307 --> 00:17:11,548
consist of the raw input gridded

460
00:17:11,548 --> 00:17:14,159
data. So we're dealing with latitude, longitude

461
00:17:14,159 --> 00:17:15,317
and time in this.

462
00:17:15,750 --> 00:17:17,799
Um, and we did for the large language model

463
00:17:17,799 --> 00:17:18,670
implementation,

464
00:17:18,939 --> 00:17:21,239
we interpreted these through a text intermediary,

465
00:17:21,509 --> 00:17:24,118
so we're extracting statistics corresponding

466
00:17:24,118 --> 00:17:26,160
to the different, uh, regions of

467
00:17:26,160 --> 00:17:28,239
the shipping forecast, uh, itself,

468
00:17:28,279 --> 00:17:29,338
as you heard at the start,

469
00:17:29,750 --> 00:17:32,000
er, to summarize those and then pass

470
00:17:32,000 --> 00:17:33,059
those into,

471
00:17:33,439 --> 00:17:35,759
uh, the Nova LLM.

472
00:17:36,939 --> 00:17:39,098
And this, uh, this really provides

473
00:17:39,098 --> 00:17:40,400
us with a benchmark.

474
00:17:41,170 --> 00:17:43,250
What we also explored, and a lot of the

475
00:17:43,250 --> 00:17:46,049
focus of our experiments

476
00:17:46,588 --> 00:17:48,809
was around using video capabilities of these

477
00:17:48,809 --> 00:17:50,568
types uh types of model.

478
00:17:50,959 --> 00:17:53,209
So that same gridded data was

479
00:17:53,209 --> 00:17:54,588
first converted

480
00:17:54,880 --> 00:17:56,489
and encoded to video.

481
00:17:56,939 --> 00:17:58,979
Um, and this was done on a sea area

482
00:17:58,979 --> 00:18:01,328
basis typically for most of the parameters.

483
00:18:01,618 --> 00:18:03,618
And then we fine-tuned this using

484
00:18:04,509 --> 00:18:05,578
the bulletins

485
00:18:06,108 --> 00:18:07,529
that had been issued

486
00:18:08,118 --> 00:18:10,209
previously by our meteorologists,

487
00:18:10,390 --> 00:18:13,029
uh, to be able to uh interpret

488
00:18:13,029 --> 00:18:15,189
these um as inputs. And

489
00:18:15,189 --> 00:18:17,318
then that was hosted uh to enable

490
00:18:17,318 --> 00:18:19,868
us to to then run inference

491
00:18:19,868 --> 00:18:22,029
on that um in real time.

492
00:18:22,848 --> 00:18:24,868
But to dive deeper into the

493
00:18:24,868 --> 00:18:26,930
uh architecture of this, I'm gonna hand

494
00:18:26,930 --> 00:18:29,009
over to to Dinesh uh to

495
00:18:29,009 --> 00:18:29,689
talk about that.

496
00:18:30,130 --> 00:18:31,029
Thank you so much, Ed.

497
00:18:32,368 --> 00:18:34,380
Uh hello. Good morning. My name

498
00:18:34,380 --> 00:18:35,529
is Ad Dinesh.

499
00:18:35,930 --> 00:18:38,088
I'm based out of Luxembourg and I work

500
00:18:38,088 --> 00:18:39,989
with Worldwide super tapping team.

501
00:18:40,449 --> 00:18:42,150
I work as an applied scientist over there.

502
00:18:42,568 --> 00:18:44,568
Before jumping into that, I want to ask a

503
00:18:44,568 --> 00:18:47,000
question. How many of you were in replay?

504
00:18:47,170 --> 00:18:48,009
Can you raise your hand?

505
00:18:49,549 --> 00:18:51,549
Kudos to you. You were able to make the

506
00:18:51,549 --> 00:18:52,410
morning session.

507
00:18:52,930 --> 00:18:55,049
My manager has to push me out of the

508
00:18:55,049 --> 00:18:57,108
replay to come to the session, you know, like

509
00:18:57,108 --> 00:18:57,979
have a good night's sleep.

510
00:18:58,309 --> 00:19:00,108
OK, let's dive deep into it now. OK,

511
00:19:00,469 --> 00:19:02,529
so I'm gonna talk,

512
00:19:02,549 --> 00:19:04,750
uh, what deployment strategies

513
00:19:04,750 --> 00:19:06,750
and what provision we did for

514
00:19:06,750 --> 00:19:07,519
fine-tuned model.

515
00:19:08,618 --> 00:19:10,650
We'll talk about the architecture deep dive.

516
00:19:10,900 --> 00:19:11,689
There are some

517
00:19:12,318 --> 00:19:14,380
222 main modules. One is

518
00:19:14,380 --> 00:19:16,019
large language model, a foundation,

519
00:19:16,299 --> 00:19:17,549
and another one is vision,

520
00:19:19,578 --> 00:19:20,279
language model.

521
00:19:20,739 --> 00:19:23,420
And third one, we'll talk about one of

522
00:19:23,420 --> 00:19:25,818
the notebook sample where you can actually

523
00:19:25,818 --> 00:19:26,689
see the code, uh,

524
00:19:27,019 --> 00:19:29,019
especially the machine learning engineer, they want

525
00:19:29,019 --> 00:19:31,130
to see how easy it is to fine tune a

526
00:19:31,130 --> 00:19:33,199
model. So let's do that one, OK?

527
00:19:33,779 --> 00:19:35,500
So. Good.

528
00:19:35,818 --> 00:19:38,059
So there are 2 choices or 2 flavors you can

529
00:19:38,059 --> 00:19:39,650
fine tune a Nova model.

530
00:19:39,939 --> 00:19:41,939
One is on sage maker training jobs, and

531
00:19:41,939 --> 00:19:44,009
the second one is sage maker hyperpod.

532
00:19:44,848 --> 00:19:47,049
So which one to choose, that depends

533
00:19:47,049 --> 00:19:48,150
on use case

534
00:19:48,529 --> 00:19:50,989
and also how much

535
00:19:51,170 --> 00:19:53,140
data and how long you're going to train the job.

536
00:19:53,809 --> 00:19:55,890
For our experiments, our training

537
00:19:55,890 --> 00:19:56,670
jobs were

538
00:19:57,009 --> 00:19:58,868
running for 3 hours on

539
00:19:59,689 --> 00:20:02,049
P548X large GPU

540
00:20:02,318 --> 00:20:03,469
using 4 GPUs,

541
00:20:03,848 --> 00:20:04,779
and

542
00:20:05,650 --> 00:20:07,759
we didn't want to use the cluster. We

543
00:20:07,759 --> 00:20:09,930
wanted to make like quick dirty runs like super

544
00:20:09,930 --> 00:20:12,049
fast, so we used sage maker training jobs.

545
00:20:12,739 --> 00:20:14,750
But in production, if you have data for

546
00:20:14,750 --> 00:20:17,029
like 5 years and you want to train on

547
00:20:17,029 --> 00:20:19,568
like several GPUs, several 100 GPUs,

548
00:20:19,949 --> 00:20:21,170
and for weeks,

549
00:20:21,469 --> 00:20:23,588
so Sage Maker Hyperpod is the right tool for

550
00:20:23,588 --> 00:20:25,729
you. There's one small

551
00:20:26,868 --> 00:20:29,229
change like only Sagemaker hyperpod

552
00:20:29,229 --> 00:20:30,068
supports PPO

553
00:20:30,670 --> 00:20:32,959
rates everything is supported by the Sagemaker

554
00:20:32,959 --> 00:20:33,630
training jobs.

555
00:20:36,140 --> 00:20:38,219
As Aid explained, just to recap

556
00:20:38,219 --> 00:20:40,660
this one for large first approach, LLM,

557
00:20:41,019 --> 00:20:43,019
how we tackle the forecasting

558
00:20:43,019 --> 00:20:44,328
using foundation model.

559
00:20:44,699 --> 00:20:45,949
We use two LLMs.

560
00:20:46,380 --> 00:20:47,338
First one was

561
00:20:47,818 --> 00:20:49,509
Cloud Sonnet 3.7,

562
00:20:50,059 --> 00:20:51,598
and we compared

563
00:20:51,920 --> 00:20:54,259
the output with NOAA Pro 1 model.

564
00:20:54,578 --> 00:20:56,818
OK. NOAA Pro 1

565
00:20:56,818 --> 00:20:57,559
has better

566
00:20:57,979 --> 00:21:00,019
accuracy for this weather forecast, and it

567
00:21:00,019 --> 00:21:01,519
was not fine tuned. It was just

568
00:21:01,779 --> 00:21:03,318
doing the simple prompts

569
00:21:03,578 --> 00:21:04,699
and process data.

570
00:21:05,160 --> 00:21:07,019
So if I show you here

571
00:21:07,439 --> 00:21:08,818
the weather sensor data,

572
00:21:10,000 --> 00:21:12,380
it is being picked up over here, the raw input

573
00:21:12,380 --> 00:21:14,400
of the grid data, what does that mean, raw

574
00:21:14,400 --> 00:21:15,618
input grid data for us

575
00:21:15,920 --> 00:21:16,880
is you have a

576
00:21:17,199 --> 00:21:18,420
multi-dimensional array

577
00:21:18,680 --> 00:21:21,719
and that xy will say like core latitude

578
00:21:21,719 --> 00:21:22,338
longitude.

579
00:21:22,779 --> 00:21:25,019
And one value will be maybe

580
00:21:25,019 --> 00:21:27,098
one of the weather attributes will have a value like amplitude.

581
00:21:27,420 --> 00:21:29,709
So you have sea wave height, right?

582
00:21:30,180 --> 00:21:32,358
So at certain geographical

583
00:21:32,358 --> 00:21:34,380
location you will see the latitude, longitude and height

584
00:21:34,380 --> 00:21:35,199
of that wave.

585
00:21:35,539 --> 00:21:37,739
So that is one of the raw grid data. I'll

586
00:21:37,739 --> 00:21:39,818
just give you one example that there are different

587
00:21:39,818 --> 00:21:40,439
aspects.

588
00:21:40,739 --> 00:21:43,059
So let's stick to that. And that data preparation

589
00:21:43,059 --> 00:21:44,680
is done as Edward mentioned,

590
00:21:44,949 --> 00:21:47,259
and we are passing that raw

591
00:21:47,259 --> 00:21:48,160
process data

592
00:21:48,459 --> 00:21:49,318
to the

593
00:21:49,979 --> 00:21:50,699
foundation model.

594
00:21:51,160 --> 00:21:53,318
There's one more thing, if in a sea

595
00:21:53,318 --> 00:21:55,259
region, there is a sea,

596
00:21:55,519 --> 00:21:56,140
um

597
00:21:56,920 --> 00:21:58,500
sorry, there is a weather attribute

598
00:21:58,920 --> 00:21:59,900
to wind direction.

599
00:22:00,588 --> 00:22:02,838
So if in this particular sea region,

600
00:22:03,039 --> 00:22:05,209
if 90% of wind is flowing to

601
00:22:05,209 --> 00:22:07,358
the north direction and 10% is

602
00:22:07,358 --> 00:22:08,199
going northeast,

603
00:22:08,519 --> 00:22:10,939
we'll select that entire region as a north

604
00:22:12,009 --> 00:22:14,358
direction. So that is the

605
00:22:14,358 --> 00:22:15,380
raw data

606
00:22:15,759 --> 00:22:17,759
to the data preparation, and

607
00:22:17,759 --> 00:22:19,459
that we are passing to the foundation model.

608
00:22:21,118 --> 00:22:22,338
Let's dive deep into architecture.

609
00:22:22,630 --> 00:22:24,709
So this architecture is built within 4 weeks

610
00:22:24,709 --> 00:22:26,420
of prototyping phase.

611
00:22:26,868 --> 00:22:27,818
So we have

612
00:22:28,799 --> 00:22:30,640
the raw input grid data.

613
00:22:31,279 --> 00:22:32,618
It's stored in 3 bucket,

614
00:22:32,959 --> 00:22:35,239
as Edward said, it was one run was 45

615
00:22:35,239 --> 00:22:36,269
GB around,

616
00:22:36,559 --> 00:22:38,299
but we had a subset of that,

617
00:22:38,680 --> 00:22:40,959
and for 3 months of data it was roughly

618
00:22:40,959 --> 00:22:42,989
around 86 GB of

619
00:22:43,059 --> 00:22:45,358
numerical data. That's it, 86

620
00:22:45,358 --> 00:22:47,519
GB just for three months and even for a subset.

621
00:22:48,400 --> 00:22:50,930
That's we actually process with ECS

622
00:22:52,078 --> 00:22:53,269
because of a parallel processing.

623
00:22:53,598 --> 00:22:55,719
If someone is running 86

624
00:22:55,719 --> 00:22:58,130
GB of processing on a single code, uh,

625
00:22:58,400 --> 00:22:59,539
it will take like months.

626
00:23:00,279 --> 00:23:02,019
These two ones are optional,

627
00:23:02,660 --> 00:23:04,500
uh, EKS or AWS patch.

628
00:23:04,880 --> 00:23:06,880
It is just for telling like data

629
00:23:06,880 --> 00:23:09,318
parallel processing can be done choosing

630
00:23:09,318 --> 00:23:10,059
one of the three.

631
00:23:10,598 --> 00:23:12,828
Once the data is processed, we are calling the Bedrock

632
00:23:12,828 --> 00:23:13,660
Foundation model

633
00:23:14,160 --> 00:23:16,358
and output we are storing into the S3 bucket.

634
00:23:17,479 --> 00:23:19,479
Here I want to highlight this architecture is

635
00:23:19,479 --> 00:23:21,799
really, really good to process large volume

636
00:23:21,799 --> 00:23:22,699
of historic data.

637
00:23:23,769 --> 00:23:24,750
Foundation model

638
00:23:25,449 --> 00:23:26,949
through the bedrock mostly

639
00:23:27,568 --> 00:23:28,930
supports the batch in front,

640
00:23:29,368 --> 00:23:31,529
but you need to check which model supports the cloud

641
00:23:31,529 --> 00:23:33,318
family supports, Nova family supports.

642
00:23:33,650 --> 00:23:35,689
So this saves a lot of time and it

643
00:23:35,709 --> 00:23:37,299
it it saves a lot of cost as well.

644
00:23:37,769 --> 00:23:40,170
There is one small caveat

645
00:23:40,170 --> 00:23:42,449
when you are using fine-tuned model, but I will

646
00:23:42,449 --> 00:23:44,338
explain that when we go to the VLM one,

647
00:23:44,930 --> 00:23:46,809
OK, just keep that in your note.

648
00:23:47,519 --> 00:23:49,598
This is very flexible because you are

649
00:23:49,598 --> 00:23:51,640
using ECS and under the hood you can

650
00:23:51,640 --> 00:23:52,750
use EC2 instead a target based on

651
00:23:54,160 --> 00:23:55,818
how much complete power you want to give,

652
00:23:56,358 --> 00:23:56,979
and

653
00:23:57,598 --> 00:23:59,809
there's one thing in this architecture that is lacking,

654
00:24:00,279 --> 00:24:02,180
it is the orchestration is manual

655
00:24:02,598 --> 00:24:04,608
as this was a prototype, we had to do like

656
00:24:04,608 --> 00:24:06,650
very quick runs and all, so that's what

657
00:24:07,118 --> 00:24:08,699
we proposed this one

658
00:24:09,160 --> 00:24:09,939
for the production,

659
00:24:10,420 --> 00:24:12,759
OK, and of course for customer

660
00:24:12,759 --> 00:24:14,318
to customer, it changes.

661
00:24:15,199 --> 00:24:17,229
For for this use case, we

662
00:24:17,229 --> 00:24:19,430
had the S3 bucket here. Sorry, my

663
00:24:19,430 --> 00:24:20,890
bad, uh,

664
00:24:21,809 --> 00:24:23,959
yes, I wanted to use the pointer. So

665
00:24:23,959 --> 00:24:25,529
for the S3 bucket we have the data.

666
00:24:26,108 --> 00:24:28,140
We have the AWS glue catalog and

667
00:24:28,140 --> 00:24:29,358
AWS lick formation.

668
00:24:29,699 --> 00:24:30,818
As the organization grows,

669
00:24:31,130 --> 00:24:32,529
teams want to access the data

670
00:24:32,949 --> 00:24:35,150
and to give the fine granular level

671
00:24:35,150 --> 00:24:37,130
access to the data, this data plan

672
00:24:37,489 --> 00:24:39,739
will be really helpful for AWS uh land lake

673
00:24:39,739 --> 00:24:40,949
formation with glue catalog.

674
00:24:41,680 --> 00:24:43,759
Glue catalog is really good if you

675
00:24:43,759 --> 00:24:45,920
want to get a subset of data for

676
00:24:45,920 --> 00:24:48,199
a certain time frame to X to Y,

677
00:24:48,640 --> 00:24:51,219
you can run the query and you can get data very easily.

678
00:24:51,680 --> 00:24:53,789
We are decoupling this architecture using

679
00:24:53,789 --> 00:24:54,868
Amazon SQS.

680
00:24:55,189 --> 00:24:57,390
Decoupling is very important in production

681
00:24:57,838 --> 00:24:59,838
because what if something goes wrong over

682
00:24:59,838 --> 00:25:01,989
here? We need to know

683
00:25:02,769 --> 00:25:03,729
which data is failed,

684
00:25:04,059 --> 00:25:05,848
which, which forecast is not being processed.

685
00:25:06,170 --> 00:25:08,289
So this SQS is very important. We can attach

686
00:25:08,289 --> 00:25:10,568
a dead letter queue also, so if the record

687
00:25:10,568 --> 00:25:12,348
is failed, that's been added to the queue

688
00:25:12,739 --> 00:25:14,809
and the team can go deeper into that.

689
00:25:16,559 --> 00:25:18,189
Data preprocessing block is

690
00:25:19,098 --> 00:25:21,439
The Data preprocesing block is the same, so ECS,

691
00:25:21,519 --> 00:25:22,818
EKS, and batch,

692
00:25:23,239 --> 00:25:25,309
you can use any one of these based on your

693
00:25:25,309 --> 00:25:27,390
need. Some customers use their

694
00:25:27,390 --> 00:25:29,519
workload mostly on EKS, so they, they

695
00:25:29,519 --> 00:25:31,739
prefer to run the processing on EKS. Some

696
00:25:31,739 --> 00:25:32,618
use ECS.

697
00:25:35,809 --> 00:25:37,890
There are other applications, other architecture

698
00:25:37,890 --> 00:25:39,289
I wanted to just walk through it.

699
00:25:39,568 --> 00:25:40,910
What if the

700
00:25:41,328 --> 00:25:43,390
use case uses streaming data?

701
00:25:43,930 --> 00:25:46,279
So we can use Amazon Kinesis data stream.

702
00:25:46,650 --> 00:25:48,469
A lambda is attached for processing.

703
00:25:48,729 --> 00:25:50,769
It depends on how many lambdas

704
00:25:50,769 --> 00:25:51,930
you're going to use. If,

705
00:25:52,209 --> 00:25:54,410
if it's a different processing, one lambda

706
00:25:54,410 --> 00:25:56,828
is output dependent on another, you might use a batch.

707
00:25:57,680 --> 00:25:59,799
And once the data is

708
00:25:59,799 --> 00:26:02,039
processed, which is formatted for the foundation

709
00:26:02,039 --> 00:26:04,118
model, you just call the model. You

710
00:26:04,118 --> 00:26:06,160
store the results in nano DBT and

711
00:26:06,160 --> 00:26:06,880
open search.

712
00:26:07,250 --> 00:26:09,318
Why we are using this, if you want to

713
00:26:09,318 --> 00:26:11,059
do the real-time analysis as well.

714
00:26:13,469 --> 00:26:14,789
Yeah, I want to,

715
00:26:15,789 --> 00:26:17,868
and the last one is erroles event driven

716
00:26:17,868 --> 00:26:18,449
architecture.

717
00:26:19,250 --> 00:26:20,858
Uh, in this one, everything is serless.

718
00:26:21,920 --> 00:26:23,279
When the data is uploaded,

719
00:26:23,568 --> 00:26:25,680
um, once the data is available in S3, it

720
00:26:25,680 --> 00:26:26,709
will trigger a lambda.

721
00:26:27,039 --> 00:26:29,279
It will do the processing using AWS

722
00:26:29,279 --> 00:26:29,858
step function,

723
00:26:30,838 --> 00:26:32,920
and then it will go to the Amazon bedrock and data will be

724
00:26:32,920 --> 00:26:35,858
stored. OK,

725
00:26:36,068 --> 00:26:37,959
now the second module, VLM,

726
00:26:38,269 --> 00:26:39,108
is much more fun.

727
00:26:40,199 --> 00:26:41,140
And

728
00:26:41,750 --> 00:26:43,858
I'm going to tell you the data, how it looks.

729
00:26:44,150 --> 00:26:45,328
So every day

730
00:26:45,828 --> 00:26:47,289
there are 4 times

731
00:26:47,630 --> 00:26:49,108
the forecast is being generated

732
00:26:49,430 --> 00:26:50,670
for 31 sea regions.

733
00:26:51,650 --> 00:26:53,799
In the first forecast

734
00:26:53,799 --> 00:26:56,219
we'll have 31 sea region and

735
00:26:56,219 --> 00:26:58,420
each region will have 5 attributes, 4

736
00:26:58,420 --> 00:27:00,269
or 5 attributes, weather attributes,

737
00:27:00,650 --> 00:27:02,838
and for that one we have to create a video.

738
00:27:03,259 --> 00:27:05,459
So we have and every hour

739
00:27:05,779 --> 00:27:07,838
we are recording the

740
00:27:09,818 --> 00:27:11,759
we are taking the snapshot of the sensor data.

741
00:27:12,219 --> 00:27:14,640
So for 24 hours we have 24

742
00:27:14,640 --> 00:27:16,650
frames like it's converted the raw

743
00:27:16,650 --> 00:27:18,209
grided data to images.

744
00:27:18,779 --> 00:27:20,939
So we have 24 images

745
00:27:20,939 --> 00:27:21,880
for one day

746
00:27:22,180 --> 00:27:24,420
of the call, uh, one day of the,

747
00:27:24,430 --> 00:27:25,489
um, records,

748
00:27:25,858 --> 00:27:28,039
and that we are appending after each other,

749
00:27:28,049 --> 00:27:30,039
and we are creating 1 2nd video

750
00:27:30,380 --> 00:27:31,539
with 24 frames.

751
00:27:31,890 --> 00:27:32,559
That is our

752
00:27:32,939 --> 00:27:33,959
input data,

753
00:27:34,420 --> 00:27:35,338
OK, for training.

754
00:27:37,400 --> 00:27:38,390
And after that

755
00:27:40,289 --> 00:27:42,410
We we're going to walk

756
00:27:42,410 --> 00:27:43,799
through about the architecture diagram.

757
00:27:44,130 --> 00:27:46,430
So this raw input grid data here.

758
00:27:47,318 --> 00:27:48,818
It's stored into S3 bucket

759
00:27:49,439 --> 00:27:52,118
that is being processed, the similar logic, data

760
00:27:52,118 --> 00:27:53,098
parallel processing.

761
00:27:54,160 --> 00:27:56,199
It it created from 86 GB

762
00:27:56,199 --> 00:27:57,578
of raw graded data,

763
00:27:58,039 --> 00:27:58,848
numerical

764
00:27:59,318 --> 00:27:59,858
to

765
00:28:00,358 --> 00:28:01,539
like 56

766
00:28:02,078 --> 00:28:04,098
videos, 56 GBs of video

767
00:28:04,098 --> 00:28:05,199
for 3 months of data.

768
00:28:06,199 --> 00:28:08,920
And also we had to clean some of the data, so

769
00:28:08,920 --> 00:28:10,818
there are some of the incidents where

770
00:28:11,680 --> 00:28:13,880
the two sea regions forecasts were combined. That's

771
00:28:13,880 --> 00:28:15,920
how the expert does it. So we have to skip

772
00:28:15,920 --> 00:28:17,160
that for training purpose.

773
00:28:17,719 --> 00:28:19,799
So that's how in this one we have

774
00:28:19,799 --> 00:28:22,078
the raw training bucket

775
00:28:22,078 --> 00:28:23,920
data in which we have the training data.

776
00:28:24,318 --> 00:28:26,420
Now we use Amazon Sagemaker AI

777
00:28:26,828 --> 00:28:29,098
to submit a NOAA fine tuning job.

778
00:28:31,318 --> 00:28:32,900
Once the training job is completed,

779
00:28:33,299 --> 00:28:35,420
we get the output bucket with Model wax,

780
00:28:36,180 --> 00:28:38,180
and we are hosting that using

781
00:28:38,180 --> 00:28:39,140
Amazon Bedrock.

782
00:28:39,519 --> 00:28:41,539
Now I want to talk to you about two types

783
00:28:41,539 --> 00:28:42,358
of

784
00:28:42,660 --> 00:28:44,680
fine tuning that we experimented.

785
00:28:45,259 --> 00:28:46,309
First one is Lora.

786
00:28:47,219 --> 00:28:49,029
What we do in Lora, we have

787
00:28:49,608 --> 00:28:50,568
a foundation model.

788
00:28:51,608 --> 00:28:53,670
We put the weights like adapter,

789
00:28:53,719 --> 00:28:55,439
and we just fine tune that adapter,

790
00:28:55,769 --> 00:28:57,500
right? That's it, nothing else.

791
00:28:58,009 --> 00:29:00,209
But when you do a full fine tuning

792
00:29:00,209 --> 00:29:02,608
of a foundation model in which you

793
00:29:02,608 --> 00:29:04,410
update the each uh weights of each layer.

794
00:29:05,239 --> 00:29:07,279
And that takes a longer time also, but

795
00:29:07,279 --> 00:29:09,358
it has additional benefit that the model learns

796
00:29:09,358 --> 00:29:12,019
more. Here

797
00:29:12,239 --> 00:29:13,559
when you are using Lora,

798
00:29:14,039 --> 00:29:16,239
you can use the on-demand

799
00:29:16,239 --> 00:29:16,838
inference.

800
00:29:18,019 --> 00:29:20,279
What does it mean on-demand inference for Laura,

801
00:29:20,809 --> 00:29:22,969
because the actual model weight is in

802
00:29:22,969 --> 00:29:23,890
service bucket.

803
00:29:24,529 --> 00:29:26,529
The lower adapter weight is in your

804
00:29:26,529 --> 00:29:27,049
bucket,

805
00:29:27,489 --> 00:29:28,509
in your account,

806
00:29:29,049 --> 00:29:31,160
and when we deploy it, the

807
00:29:31,160 --> 00:29:33,170
lower weights will be appended and the model

808
00:29:33,170 --> 00:29:33,939
will be deployed.

809
00:29:34,209 --> 00:29:36,209
So you can use on-demand inference. You

810
00:29:36,209 --> 00:29:38,289
only pay how many you only

811
00:29:38,289 --> 00:29:40,559
pay for how many input tokens and output tokens

812
00:29:40,559 --> 00:29:41,209
are generated.

813
00:29:42,368 --> 00:29:43,949
However, if you're using

814
00:29:44,650 --> 00:29:45,828
full fine tuning,

815
00:29:46,328 --> 00:29:48,410
what's gonna happen in full fine tuning is entire

816
00:29:48,410 --> 00:29:50,130
model weights is gonna update it.

817
00:29:50,489 --> 00:29:52,588
So that model weight is gonna be stored somewhere

818
00:29:52,959 --> 00:29:54,170
from the original foundation model.

819
00:29:55,150 --> 00:29:57,430
And now the new models which are there, that

820
00:29:57,430 --> 00:29:59,289
needs to be hosted 24 hours

821
00:29:59,949 --> 00:30:00,789
on instance.

822
00:30:01,229 --> 00:30:03,469
So that's why you need to go for a provisional throughput.

823
00:30:04,549 --> 00:30:06,588
You need to buy the provisional throughput so that you can

824
00:30:06,588 --> 00:30:07,130
deploy.

825
00:30:07,709 --> 00:30:09,670
So this is a key difference I wanted to highlight.

826
00:30:10,098 --> 00:30:12,150
So for this one, when we are

827
00:30:12,150 --> 00:30:14,170
using and using this Lora,

828
00:30:14,309 --> 00:30:16,618
we were able to use on-demand in France,

829
00:30:17,150 --> 00:30:19,180
but when we use full fine tuning, we

830
00:30:19,180 --> 00:30:20,509
had to do the provisional throughput.

831
00:30:21,670 --> 00:30:23,680
This architecture was built for

832
00:30:23,680 --> 00:30:24,789
the prototype in 4 weeks.

833
00:30:25,390 --> 00:30:26,328
It is very simple.

834
00:30:26,640 --> 00:30:28,680
It lacks a lot of functionality that can

835
00:30:28,680 --> 00:30:29,640
be used in the production.

836
00:30:30,000 --> 00:30:32,160
So let's go to the production architecture,

837
00:30:32,239 --> 00:30:32,920
how it looks.

838
00:30:35,630 --> 00:30:37,709
We have different planes, data

839
00:30:37,709 --> 00:30:39,318
plane, compute plane, um,

840
00:30:39,828 --> 00:30:41,828
and Sagemer training job plane also.

841
00:30:42,229 --> 00:30:44,269
So data plane, I want to

842
00:30:44,269 --> 00:30:45,410
highlight over here,

843
00:30:45,709 --> 00:30:46,650
just bring your attention.

844
00:30:46,910 --> 00:30:48,969
So we have uh Amazon FS 6

845
00:30:48,969 --> 00:30:50,068
for Lester here also.

846
00:30:50,769 --> 00:30:52,769
So this is added, this is like super

847
00:30:52,769 --> 00:30:54,930
fast access if

848
00:30:54,930 --> 00:30:57,088
you have a requirement that

849
00:30:57,088 --> 00:30:58,848
requires low latency.

850
00:31:01,739 --> 00:31:03,779
The FSX Amazon FXSX cluster

851
00:31:03,779 --> 00:31:04,439
can be used

852
00:31:05,219 --> 00:31:06,239
for caching purpose,

853
00:31:06,500 --> 00:31:07,838
and raises the similar

854
00:31:09,500 --> 00:31:11,539
logic for what we showed

855
00:31:11,539 --> 00:31:12,660
in LLM architecture.

856
00:31:15,068 --> 00:31:17,519
Compute plane is being uh decoupled with SQS

857
00:31:17,519 --> 00:31:19,598
and we are doing computing on ECS

858
00:31:19,598 --> 00:31:21,358
or EKS or AWS patch.

859
00:31:21,989 --> 00:31:23,400
Once the data is being.

860
00:31:24,680 --> 00:31:25,650
Process

861
00:31:25,949 --> 00:31:27,900
all the videos will be stored over here.

862
00:31:28,640 --> 00:31:29,660
So coming back

863
00:31:30,039 --> 00:31:32,479
for one forecast, we have 24

864
00:31:32,479 --> 00:31:33,479
hours of record

865
00:31:34,160 --> 00:31:36,420
pass, so we are creating 24

866
00:31:36,559 --> 00:31:37,500
images.

867
00:31:38,118 --> 00:31:40,130
We are creating one second video out of

868
00:31:40,130 --> 00:31:42,539
the 24 images,

869
00:31:42,880 --> 00:31:44,358
and we have 31 regions.

870
00:31:44,640 --> 00:31:46,338
So we have 31 videos

871
00:31:46,598 --> 00:31:47,959
for one weather attribute

872
00:31:48,719 --> 00:31:49,920
and for one day.

873
00:31:50,318 --> 00:31:51,838
So this is quite big data.

874
00:31:52,809 --> 00:31:54,049
Like a large amount of data,

875
00:31:54,368 --> 00:31:56,539
we use the SageMaker AI.

876
00:31:56,930 --> 00:31:59,289
In this case, you can use SageMaker hyperpods

877
00:31:59,289 --> 00:32:01,650
as well as Sage Maker training jobs in production

878
00:32:01,650 --> 00:32:02,789
based on the need.

879
00:32:03,568 --> 00:32:05,479
Do we really need to go for SageMaker hyperpo?

880
00:32:06,328 --> 00:32:07,930
And then once the model is trained,

881
00:32:08,229 --> 00:32:10,269
the weight should be stored into Amazon S3,

882
00:32:10,328 --> 00:32:12,900
and then we are deploying it on the bedrock.

883
00:32:14,088 --> 00:32:16,338
Why we are deploying it on bedrock, the first question comes,

884
00:32:16,719 --> 00:32:19,039
why what it gives additionally, because

885
00:32:19,039 --> 00:32:20,500
once the model is on bedrock,

886
00:32:21,000 --> 00:32:23,000
all the, almost all the features

887
00:32:23,000 --> 00:32:25,199
available for bedrock, you can use it on that model.

888
00:32:25,239 --> 00:32:27,400
You can use the uh you can use

889
00:32:27,400 --> 00:32:29,640
the bedrock converse APA

890
00:32:30,160 --> 00:32:32,439
without changing anything. You just need to change the

891
00:32:32,439 --> 00:32:33,180
idea of a model,

892
00:32:33,439 --> 00:32:34,549
and it, it works.

893
00:32:34,828 --> 00:32:36,838
If you, if you want to use the guard rails, you

894
00:32:36,838 --> 00:32:37,729
can use that also.

895
00:32:38,000 --> 00:32:40,000
So that gives that undifferentiated heavy

896
00:32:40,000 --> 00:32:40,578
lifting

897
00:32:40,920 --> 00:32:41,539
over here.

898
00:32:42,709 --> 00:32:43,338
Uh

899
00:32:46,239 --> 00:32:48,309
There are alternate architecture. I just wanted to

900
00:32:48,309 --> 00:32:48,969
walk through it.

901
00:32:49,598 --> 00:32:50,699
The first one is

902
00:32:51,439 --> 00:32:53,059
model fine tuning.

903
00:32:53,479 --> 00:32:54,259
It's simple.

904
00:32:54,969 --> 00:32:57,279
UR data in S3 bucket. Why we are taking

905
00:32:57,279 --> 00:32:59,309
everything in S3 bucket because it scales really

906
00:32:59,309 --> 00:33:00,890
well and

907
00:33:01,368 --> 00:33:03,680
uh with AKS and ECS, it

908
00:33:03,680 --> 00:33:05,680
actually pulls it pretty cool, like

909
00:33:05,680 --> 00:33:06,338
pretty fast.

910
00:33:07,118 --> 00:33:08,769
Once the data is there, uh,

911
00:33:09,680 --> 00:33:11,750
the data is processed, it stores into the

912
00:33:11,750 --> 00:33:12,660
S3 bucket again,

913
00:33:12,930 --> 00:33:15,229
and we are using the Sage maker or hyperpo

914
00:33:15,229 --> 00:33:17,368
to train. And we are deploying it on

915
00:33:17,368 --> 00:33:18,390
the inference.

916
00:33:19,779 --> 00:33:21,900
Second one, how we will be calling

917
00:33:21,900 --> 00:33:22,848
that model.

918
00:33:23,380 --> 00:33:25,459
Because in VLM there is a huge amount of

919
00:33:25,459 --> 00:33:26,219
data processing.

920
00:33:27,660 --> 00:33:28,380
For example,

921
00:33:28,640 --> 00:33:29,578
when the data is in.

922
00:33:31,809 --> 00:33:33,578
This might take at least

923
00:33:33,920 --> 00:33:36,400
a minute to process, to create a video,

924
00:33:37,219 --> 00:33:39,279
because the the raw graded data needs to be

925
00:33:39,279 --> 00:33:41,180
picked up, it needs to be cleaned,

926
00:33:41,559 --> 00:33:42,959
then it's converted to the images,

927
00:33:43,309 --> 00:33:44,759
and then it it goes to the video.

928
00:33:45,500 --> 00:33:46,959
So it takes a little bit of time,

929
00:33:47,299 --> 00:33:48,239
and once it is done,

930
00:33:48,739 --> 00:33:50,469
we will call the Amazon Bedrock

931
00:33:51,699 --> 00:33:53,180
to to get the entrance.

932
00:33:54,368 --> 00:33:56,420
The easiest way, this this part,

933
00:33:56,939 --> 00:33:58,939
why not we host it on Amazon Sagemaker

934
00:33:58,939 --> 00:34:00,900
endpoint? Why you are using Bedrock,

935
00:34:01,180 --> 00:34:03,338
or why we can't host it on Amazon EKS?

936
00:34:03,689 --> 00:34:05,739
If you have an open source model like

937
00:34:05,739 --> 00:34:06,269
from huggingpa Sumol V3,

938
00:34:08,780 --> 00:34:11,119
you can do that one. You can fine tune that model

939
00:34:11,300 --> 00:34:13,409
and you can host it on Sagemaker

940
00:34:13,409 --> 00:34:14,000
Endpoint,

941
00:34:14,260 --> 00:34:16,239
or you can host it on Amazon EKS also.

942
00:34:20,789 --> 00:34:22,958
We had the LLM and VLM part.

943
00:34:23,360 --> 00:34:25,039
Now how we wanted to show it on UI

944
00:34:25,769 --> 00:34:26,570
is real quick.

945
00:34:27,728 --> 00:34:30,389
If you

946
00:34:30,389 --> 00:34:32,829
are if the application is hosted

947
00:34:32,829 --> 00:34:35,010
and accessible to the public

948
00:34:35,010 --> 00:34:37,159
internet, you use a cloud front with API

949
00:34:37,159 --> 00:34:38,889
gateway and AWS Lambda,

950
00:34:39,409 --> 00:34:41,869
and decouple architecture with SQS

951
00:34:41,869 --> 00:34:44,010
and it will call the Amazon Bedrock or Sage

952
00:34:44,010 --> 00:34:45,090
Maker or EKS.

953
00:34:47,177 --> 00:34:49,458
The another approach is, uh, you put cloud

954
00:34:49,458 --> 00:34:51,688
front, you have application load balancer,

955
00:34:51,818 --> 00:34:54,998
your static website or um your

956
00:34:54,998 --> 00:34:57,018
easy to insert website is is is behind

957
00:34:57,018 --> 00:34:57,599
that one.

958
00:34:58,217 --> 00:34:59,657
sorry, Amazon Container Service,

959
00:34:59,918 --> 00:35:00,518
and then

960
00:35:00,818 --> 00:35:03,018
through that one you will call the um

961
00:35:03,018 --> 00:35:05,367
Sage maker AI or EKS or Bedrock

962
00:35:05,367 --> 00:35:06,197
for press.

963
00:35:06,748 --> 00:35:09,217
We are the Amazon app runner as also Amazon Amplified

964
00:35:09,217 --> 00:35:10,157
to host your website.

965
00:35:12,969 --> 00:35:14,958
Now let's go into the notebooks.

966
00:35:15,579 --> 00:35:17,619
I will ask very difficult question

967
00:35:17,619 --> 00:35:18,628
now. How

968
00:35:19,329 --> 00:35:21,418
2 questions. How many of, how

969
00:35:21,418 --> 00:35:22,639
many of you

970
00:35:22,898 --> 00:35:24,898
has fine tuned a large language model or

971
00:35:24,898 --> 00:35:25,938
any foundation model?

972
00:35:26,409 --> 00:35:27,320
Raise your hand, don't shy.

973
00:35:28,289 --> 00:35:29,938
OK, now a difficult question.

974
00:35:32,090 --> 00:35:34,250
Were you able to get it in the first attempt

975
00:35:34,250 --> 00:35:34,909
to fine tune

976
00:35:35,909 --> 00:35:37,409
or, uh, no, cool,

977
00:35:37,728 --> 00:35:39,599
that's it. So now I'm telling you

978
00:35:39,969 --> 00:35:40,739
this one,

979
00:35:41,119 --> 00:35:42,909
I ran like 25 experiments

980
00:35:43,699 --> 00:35:45,728
and one or two was failed because

981
00:35:45,728 --> 00:35:47,849
I had to stop it because the training data was missed from

982
00:35:47,849 --> 00:35:50,219
my side. It runs very smoothly,

983
00:35:50,369 --> 00:35:52,590
OK? Now how the training data

984
00:35:52,590 --> 00:35:53,309
looks like.

985
00:35:53,570 --> 00:35:55,708
So I just want you to pay attention over here.

986
00:35:56,090 --> 00:35:58,409
The system is a system prompt, OK,

987
00:35:58,489 --> 00:35:59,929
it says you are an expert.

988
00:36:00,938 --> 00:36:02,938
In meteorological specialized in UK

989
00:36:02,938 --> 00:36:04,019
maritime. OK,

990
00:36:04,389 --> 00:36:05,688
this is our

991
00:36:06,550 --> 00:36:08,628
user prompt. We should analyze this one

992
00:36:08,628 --> 00:36:10,659
second video and do this, this,

993
00:36:10,668 --> 00:36:12,668
this. And this

994
00:36:12,668 --> 00:36:13,610
is our input data.

995
00:36:14,659 --> 00:36:16,739
Uh, this is the, you say

996
00:36:16,739 --> 00:36:18,978
Biscay is the sea region, and this is

997
00:36:18,978 --> 00:36:21,329
one of the one of the weather attributes.

998
00:36:21,539 --> 00:36:22,949
This is our input video,

999
00:36:23,340 --> 00:36:24,119
and this is

1000
00:36:24,898 --> 00:36:26,260
the ground route, our output.

1001
00:36:27,320 --> 00:36:27,938
That's it.

1002
00:36:28,280 --> 00:36:30,360
If you have 3000 examples like

1003
00:36:30,360 --> 00:36:32,760
this, it creates one JOL file

1004
00:36:32,898 --> 00:36:34,179
with 3000 lines,

1005
00:36:34,519 --> 00:36:36,820
and this is getting flattened as one line.

1006
00:36:37,119 --> 00:36:38,500
So that is our training data.

1007
00:36:39,199 --> 00:36:41,300
We create training data set as well as

1008
00:36:41,300 --> 00:36:43,739
the validation data set when you are fine tuning

1009
00:36:43,840 --> 00:36:47,378
a model. OK.

1010
00:36:49,418 --> 00:36:51,590
How you will configure for fine tuning a model

1011
00:36:51,590 --> 00:36:53,500
like what is this?

1012
00:36:54,760 --> 00:36:56,989
What is the recipe file? So people who doesn't

1013
00:36:56,989 --> 00:36:59,228
know the recipe file is just like a configuration

1014
00:36:59,228 --> 00:37:00,429
for hyperparameterss.

1015
00:37:00,789 --> 00:37:03,050
So in this one, if you see, we give

1016
00:37:03,050 --> 00:37:04,559
them which model we want to fine tune.

1017
00:37:04,909 --> 00:37:07,110
Replicas is around how many GPUs

1018
00:37:07,110 --> 00:37:07,878
you want to train.

1019
00:37:08,628 --> 00:37:11,300
We say the training configuration, how many epochs

1020
00:37:11,958 --> 00:37:12,938
if there are

1021
00:37:13,800 --> 00:37:16,099
you want to control the

1022
00:37:16,478 --> 00:37:17,878
regulation parameters.

1023
00:37:18,530 --> 00:37:20,570
Uh, Optimizers, there are a bunch of

1024
00:37:20,570 --> 00:37:22,728
optimizers available out of the box.

1025
00:37:22,809 --> 00:37:25,208
You can use it straight away, and this is a Lora

1026
00:37:25,208 --> 00:37:26,389
configuration adapter.

1027
00:37:26,860 --> 00:37:28,918
That's it. And you need

1028
00:37:28,918 --> 00:37:31,139
this YAML file and a training script.

1029
00:37:31,639 --> 00:37:33,938
Now let's see how difficult is the training script.

1030
00:37:34,039 --> 00:37:36,119
OK? For me it was very hard, but

1031
00:37:36,119 --> 00:37:37,159
let's see. Cool.

1032
00:37:37,869 --> 00:37:38,849
It's very simple

1033
00:37:39,989 --> 00:37:42,070
Here we define our Yamal file.

1034
00:37:43,780 --> 00:37:45,909
OK. Input

1035
00:37:45,909 --> 00:37:48,260
data, as I told you, there is trend.json

1036
00:37:48,260 --> 00:37:50,519
file, for example, I select 3000 lines

1037
00:37:50,519 --> 00:37:51,289
just on file.

1038
00:37:51,750 --> 00:37:53,869
We output bucket where we want to store

1039
00:37:53,869 --> 00:37:54,949
the model weights.

1040
00:37:55,869 --> 00:37:56,750
Validation is 3

1041
00:37:57,458 --> 00:37:58,809
validation, Jason. OK,

1042
00:37:59,110 --> 00:38:01,469
I want to pay attention over here a little bit on validation.

1043
00:38:02,179 --> 00:38:04,099
Whenever you are fine tuning a model,

1044
00:38:04,519 --> 00:38:06,438
que validation data set.

1045
00:38:06,938 --> 00:38:08,938
Why? Because the model will

1046
00:38:08,938 --> 00:38:09,840
try to

1047
00:38:10,148 --> 00:38:11,739
optimize on a training data set.

1048
00:38:12,179 --> 00:38:14,329
It will always go the training loss low,

1049
00:38:14,378 --> 00:38:16,418
low, low, but on actual data it

1050
00:38:16,418 --> 00:38:17,239
might go up.

1051
00:38:17,619 --> 00:38:19,860
So you need to find a sweet spot where

1052
00:38:19,860 --> 00:38:20,679
training data,

1053
00:38:21,199 --> 00:38:23,329
training loss, as well as validation loss is

1054
00:38:23,329 --> 00:38:23,949
going down.

1055
00:38:24,260 --> 00:38:26,409
But as soon as validation loss is going up,

1056
00:38:26,739 --> 00:38:27,938
you need to stop your training.

1057
00:38:28,280 --> 00:38:30,458
OK? Just keep that in mind.

1058
00:38:31,429 --> 00:38:33,469
And this one is image URI.

1059
00:38:33,599 --> 00:38:34,739
This is a docker image,

1060
00:38:35,079 --> 00:38:37,239
OK? And it runs very smoothly. There

1061
00:38:37,239 --> 00:38:40,208
is no issues on the library

1062
00:38:40,719 --> 00:38:41,500
configuration.

1063
00:38:41,840 --> 00:38:43,378
It runs as smooth as possible.

1064
00:38:44,019 --> 00:38:46,378
Then instance type is P548 X large.

1065
00:38:46,918 --> 00:38:48,458
For NA fine tuning, you can use

1066
00:38:48,878 --> 00:38:50,458
P5 and P6 as well,

1067
00:38:51,000 --> 00:38:53,159
and there is a recommendation in instance count is

1068
00:38:53,159 --> 00:38:55,320
48, and 16.

1069
00:38:56,688 --> 00:38:58,219
You can add tensor bird output

1070
00:38:58,668 --> 00:39:00,110
when you're fine tuning a model as well.

1071
00:39:02,449 --> 00:39:03,688
Now we create an estimator.

1072
00:39:05,780 --> 00:39:07,820
This is just like giving all the

1073
00:39:07,820 --> 00:39:08,659
information what was there,

1074
00:39:08,978 --> 00:39:10,280
and here we get the input,

1075
00:39:11,378 --> 00:39:12,260
the training input,

1076
00:39:12,539 --> 00:39:14,728
validation input, and we say fit

1077
00:39:14,728 --> 00:39:16,769
the model. Within

1078
00:39:16,769 --> 00:39:18,148
3 hours, the model will be trained.

1079
00:39:18,610 --> 00:39:19,688
That was for our data.

1080
00:39:20,938 --> 00:39:22,199
Now how to

1081
00:39:22,458 --> 00:39:23,219
deploy that model.

1082
00:39:24,599 --> 00:39:25,260
We have

1083
00:39:25,949 --> 00:39:28,139
here Bedrock line create custom model, so

1084
00:39:28,139 --> 00:39:30,300
it will create a custom model as in

1085
00:39:30,500 --> 00:39:32,099
it will get the model weights,

1086
00:39:32,559 --> 00:39:34,918
OK? And this one create custom

1087
00:39:34,918 --> 00:39:37,300
deployment means it will deploy on Bedrock.

1088
00:39:37,800 --> 00:39:40,090
It will take an hour or so to

1089
00:39:40,119 --> 00:39:40,878
to do everything.

1090
00:39:41,179 --> 00:39:43,320
So I want you to pay attention to the custom deployment

1091
00:39:43,320 --> 00:39:45,309
ARN. This is kind of model ARN

1092
00:39:47,320 --> 00:39:48,679
or model ID.

1093
00:39:49,478 --> 00:39:51,699
Now, how we will get the inference?

1094
00:39:53,280 --> 00:39:54,199
This is a magic codes.

1095
00:39:54,478 --> 00:39:56,489
So we have the Bedrock runtime converse API.

1096
00:39:56,610 --> 00:39:57,708
We give the model ID.

1097
00:39:58,010 --> 00:39:59,688
This is a custom deployment AN

1098
00:40:00,208 --> 00:40:02,449
system prompt, which was you are an expert in

1099
00:40:02,449 --> 00:40:03,228
UK meteorology.

1100
00:40:03,688 --> 00:40:05,969
User prompt, analyze that 1 2nd video.

1101
00:40:07,079 --> 00:40:07,938
This is a

1102
00:40:08,719 --> 00:40:11,199
uh input video URI S3 bucket,

1103
00:40:11,559 --> 00:40:12,418
and that's it.

1104
00:40:13,199 --> 00:40:14,458
And it will give the output.

1105
00:40:15,320 --> 00:40:16,378
And this is the ground root.

1106
00:40:16,918 --> 00:40:18,648
So it is, it is very close to this one.

1107
00:40:19,969 --> 00:40:22,039
So, uh, here, uh, we,

1108
00:40:22,168 --> 00:40:24,070
we did a lot of experiments

1109
00:40:24,398 --> 00:40:26,269
around 2025 experiments we did,

1110
00:40:26,559 --> 00:40:28,648
and we, we did experiments with

1111
00:40:28,648 --> 00:40:30,889
Thalora as well as, uh, self fine tuning,

1112
00:40:30,929 --> 00:40:32,869
like fine tuning the entire model,

1113
00:40:33,289 --> 00:40:35,489
and that's where that experimentation

1114
00:40:35,489 --> 00:40:37,610
and the model of the evaluation

1115
00:40:37,610 --> 00:40:39,769
will be run by Edward, so I'll, I'll hand it over

1116
00:40:39,769 --> 00:40:41,289
to you. Uh, thank you so much.

1117
00:40:42,070 --> 00:40:42,579
Yeah, thank you.

1118
00:40:43,590 --> 00:40:44,188
Great.

1119
00:40:44,708 --> 00:40:45,550
Thanks Dinesh.

1120
00:40:47,019 --> 00:40:49,179
So having discussed the architecture, we'll

1121
00:40:49,179 --> 00:40:51,659
just talk briefly about some experiments

1122
00:40:51,659 --> 00:40:53,679
and evaluations that we conducted

1123
00:40:53,929 --> 00:40:56,219
er to further optimize our

1124
00:40:56,219 --> 00:40:58,309
particularly presentation of the information

1125
00:40:58,530 --> 00:41:00,599
er to the foundation model.

1126
00:41:01,769 --> 00:41:02,300
So

1127
00:41:02,570 --> 00:41:05,168
these experiments in Nova fine tuning

1128
00:41:05,168 --> 00:41:06,708
boiled down to four things.

1129
00:41:07,050 --> 00:41:09,349
Firstly, we were looking at combined versus

1130
00:41:09,929 --> 00:41:10,918
individual models.

1131
00:41:11,208 --> 00:41:13,239
We were looking at continuous versus

1132
00:41:13,239 --> 00:41:15,590
categorical data that's provided to these.

1133
00:41:15,929 --> 00:41:18,389
We were looking at overfitting versus simulation

1134
00:41:18,389 --> 00:41:19,349
of overstopping.

1135
00:41:19,610 --> 00:41:21,610
And finally, as Dinesh also

1136
00:41:21,610 --> 00:41:23,869
mentioned, uh, full rank fine tuning

1137
00:41:23,869 --> 00:41:26,269
versus uh versus Laura

1138
00:41:26,519 --> 00:41:27,090
approaches.

1139
00:41:27,829 --> 00:41:29,320
Again, just to recap,

1140
00:41:29,688 --> 00:41:31,320
er for our

1141
00:41:31,579 --> 00:41:33,780
um LLM experiments, this

1142
00:41:33,780 --> 00:41:34,398
was using

1143
00:41:34,820 --> 00:41:35,409
um

1144
00:41:35,898 --> 00:41:37,938
the Nova Pro model um

1145
00:41:38,378 --> 00:41:40,438
without any fine tuning for our

1146
00:41:40,579 --> 00:41:41,208
um

1147
00:41:41,739 --> 00:41:42,878
VLM approaches,

1148
00:41:43,139 --> 00:41:45,860
then this was using the Nova Light model er

1149
00:41:46,050 --> 00:41:48,139
with er with the fine tuning as

1150
00:41:48,139 --> 00:41:50,269
er Dinesh has just

1151
00:41:50,269 --> 00:41:53,110
described. So

1152
00:41:53,110 --> 00:41:54,969
into some of these experiments then.

1153
00:41:55,590 --> 00:41:57,949
First of all, we considered how we're

1154
00:41:57,949 --> 00:41:59,478
formatting these videos.

1155
00:41:59,789 --> 00:42:02,030
So it's attractive on the face of it, to

1156
00:42:02,030 --> 00:42:04,429
have just a single video that we're providing

1157
00:42:04,429 --> 00:42:06,579
with all of the information about all of the weather

1158
00:42:06,579 --> 00:42:07,449
attributes.

1159
00:42:07,708 --> 00:42:09,708
Um, so clubbing together each of

1160
00:42:09,708 --> 00:42:11,829
the parameters of interest, so for

1161
00:42:11,829 --> 00:42:14,369
example, wind speed, wind direction, visibility,

1162
00:42:14,579 --> 00:42:16,909
weather type, uh, sea state into

1163
00:42:16,909 --> 00:42:18,708
a single 5 2nd video.

1164
00:42:19,860 --> 00:42:21,878
But actually, what we found was that

1165
00:42:21,878 --> 00:42:24,280
if we're instead breaking

1166
00:42:24,280 --> 00:42:26,458
these down by individual attributes and training

1167
00:42:26,458 --> 00:42:28,579
models for each of these, then actually we

1168
00:42:28,579 --> 00:42:30,059
get a much better result and

1169
00:42:30,458 --> 00:42:32,559
the individual models were outperforming

1170
00:42:32,559 --> 00:42:34,820
a combined model by on average

1171
00:42:34,820 --> 00:42:36,445
about 2. 7%,

1172
00:42:36,715 --> 00:42:38,715
because we've got increased opportunity

1173
00:42:38,715 --> 00:42:40,875
in terms of uh in terms of

1174
00:42:40,875 --> 00:42:42,155
the prompts that we're providing,

1175
00:42:42,554 --> 00:42:44,675
increased control in terms of how

1176
00:42:44,675 --> 00:42:46,784
we're we're doing this, and uh

1177
00:42:46,784 --> 00:42:48,945
there's not so much context switching

1178
00:42:48,945 --> 00:42:50,793
uh taking place on that.

1179
00:42:52,860 --> 00:42:54,909
The next question was actually how

1180
00:42:54,909 --> 00:42:57,090
we're presenting this meteorological data

1181
00:42:57,510 --> 00:42:58,820
to the model itself.

1182
00:42:59,148 --> 00:43:01,628
So typically, uh we

1183
00:43:01,628 --> 00:43:03,829
consider, for example, uh fields

1184
00:43:03,829 --> 00:43:05,849
of sea state, so our wave height

1185
00:43:05,849 --> 00:43:08,030
is effectively a map of how high

1186
00:43:08,030 --> 00:43:10,250
the waves are, with that height being

1187
00:43:10,250 --> 00:43:11,989
a numeric representation.

1188
00:43:13,378 --> 00:43:15,500
But given the specific language of the shipping

1189
00:43:15,500 --> 00:43:16,260
forecast,

1190
00:43:16,619 --> 00:43:19,000
we actually have an opportunity to categorize

1191
00:43:19,000 --> 00:43:21,500
these into the particular bands that correspond

1192
00:43:21,500 --> 00:43:23,059
to this particular terminology.

1193
00:43:23,590 --> 00:43:24,128
And again,

1194
00:43:24,750 --> 00:43:26,750
changing the color scales from

1195
00:43:26,750 --> 00:43:29,030
just a continuous color scale uh for

1196
00:43:29,030 --> 00:43:31,510
the videos that were provided to these banded

1197
00:43:31,510 --> 00:43:33,550
categorical values that correspond to the

1198
00:43:33,550 --> 00:43:35,750
specific terminology of the shipping forecast,

1199
00:43:36,148 --> 00:43:38,469
again, enabled us to achieve

1200
00:43:38,469 --> 00:43:40,989
further improvements with our categorical

1201
00:43:40,989 --> 00:43:43,610
data models typically outperforming the continuous

1202
00:43:44,269 --> 00:43:46,590
models uh by on average

1203
00:43:46,590 --> 00:43:48,909
about 25.4%.

1204
00:43:49,000 --> 00:43:51,199
And if you drill a little bit deeper into

1205
00:43:51,199 --> 00:43:53,030
this, actually the biggest

1206
00:43:53,478 --> 00:43:55,800
improvements um are in areas

1207
00:43:55,800 --> 00:43:57,840
such as the weather type, where

1208
00:43:57,840 --> 00:43:59,918
previously just a numeric text

1209
00:43:59,918 --> 00:44:01,938
label has no real meaning,

1210
00:44:02,159 --> 00:44:04,599
uh, but actually when we're converting this and labeling

1211
00:44:04,599 --> 00:44:06,869
this on the y axis or our color scale

1212
00:44:06,869 --> 00:44:08,550
on the videos that we're providing,

1213
00:44:08,840 --> 00:44:09,739
we can provide,

1214
00:44:10,000 --> 00:44:11,458
uh, this additional

1215
00:44:12,760 --> 00:44:14,280
information to, to, to help.

1216
00:44:15,969 --> 00:44:17,030
What was quite an

1217
00:44:17,599 --> 00:44:19,719
unintuitive result in many ways,

1218
00:44:20,010 --> 00:44:22,750
was that in our experiments of overfitting

1219
00:44:22,750 --> 00:44:24,128
versus early stopping,

1220
00:44:24,449 --> 00:44:26,449
the overfitting outperformed the

1221
00:44:26,449 --> 00:44:27,269
early stopping,

1222
00:44:27,530 --> 00:44:30,050
despite these having a higher validation

1223
00:44:30,050 --> 00:44:32,289
loss. And if we trace this back, this

1224
00:44:32,289 --> 00:44:34,349
is due to a discrepancy between

1225
00:44:35,010 --> 00:44:37,228
the training objective that we were using

1226
00:44:37,929 --> 00:44:40,849
in training, compared to our evaluation

1227
00:44:40,849 --> 00:44:42,849
approach. And really, the overfitting

1228
00:44:42,849 --> 00:44:45,090
is enhancing the memorization of precise

1229
00:44:45,090 --> 00:44:47,704
word pattern. Which is specific in the shipping forecast,

1230
00:44:47,914 --> 00:44:50,014
and there and therefore produces

1231
00:44:50,204 --> 00:44:52,394
more confident output, so it increases

1232
00:44:52,394 --> 00:44:54,155
our word-based F1 scores,

1233
00:44:54,425 --> 00:44:56,885
giving more accurate and more complete forecasts.

1234
00:44:57,235 --> 00:45:00,014
But the early stopping, which optimizes

1235
00:45:00,474 --> 00:45:02,974
this embedded probability or perplexity,

1236
00:45:03,594 --> 00:45:04,894
fails to capture

1237
00:45:05,353 --> 00:45:06,753
these types of wants.

1238
00:45:10,489 --> 00:45:12,519
And the final experiment that we'll

1239
00:45:12,519 --> 00:45:14,769
talk about um relates to

1240
00:45:14,769 --> 00:45:16,789
this full rank fine tuning

1241
00:45:16,789 --> 00:45:18,889
versus uh the the low

1242
00:45:18,889 --> 00:45:19,849
rank adaption.

1243
00:45:20,289 --> 00:45:22,728
And what we saw was that typically our full rank

1244
00:45:22,969 --> 00:45:25,309
uh fine tuning is outperforming

1245
00:45:26,010 --> 00:45:28,070
the use of the adapter in these experiments

1246
00:45:28,250 --> 00:45:30,349
by about 6.2%

1247
00:45:30,570 --> 00:45:32,148
um across our

1248
00:45:32,668 --> 00:45:34,148
uh our models.

1249
00:45:34,570 --> 00:45:36,050
As Dinesh explained.

1250
00:45:36,449 --> 00:45:38,570
The law approach is basically applying

1251
00:45:38,570 --> 00:45:40,769
an adapter to the pre-trained weights

1252
00:45:41,039 --> 00:45:43,168
that then enable us er to

1253
00:45:43,168 --> 00:45:45,289
make er these adjustments. But

1254
00:45:45,289 --> 00:45:47,760
it's worth just drilling a little bit deeper

1255
00:45:47,760 --> 00:45:50,010
um into some of the characteristics of these

1256
00:45:50,010 --> 00:45:52,090
approaches and particularly some of

1257
00:45:52,090 --> 00:45:52,889
the er.

1258
00:45:53,659 --> 00:45:55,878
The the cautions in terms of using,

1259
00:45:55,978 --> 00:45:58,300
uh using some of these approaches,

1260
00:45:58,579 --> 00:46:00,739
particularly compared to a full weight fine tune.

1261
00:46:01,840 --> 00:46:03,958
So fundamentally, if the model

1262
00:46:03,958 --> 00:46:06,019
has to perform only a single narrow

1263
00:46:06,019 --> 00:46:06,780
task,

1264
00:46:07,199 --> 00:46:09,228
full model training can still have a

1265
00:46:09,228 --> 00:46:11,280
performance advantage. We're very closely

1266
00:46:11,280 --> 00:46:13,500
optimizing it um to learn

1267
00:46:13,500 --> 00:46:15,978
that very specific er behavior.

1268
00:46:16,398 --> 00:46:17,510
In contrast,

1269
00:46:17,800 --> 00:46:19,918
Laura will learn less, and

1270
00:46:19,918 --> 00:46:22,000
I'd refer you to the paper by er

1271
00:46:22,000 --> 00:46:24,239
Dan Bidderman et al. which describes

1272
00:46:24,239 --> 00:46:26,579
this in er in nice detail.

1273
00:46:27,820 --> 00:46:30,438
But Laura learns less, but also

1274
00:46:30,438 --> 00:46:31,780
it forgets less.

1275
00:46:32,059 --> 00:46:33,800
So with full model fine tuning,

1276
00:46:34,199 --> 00:46:36,438
you can risk catastrophically forgetting

1277
00:46:36,438 --> 00:46:38,500
the other capabilities that haven't

1278
00:46:38,500 --> 00:46:40,668
been represented in the, in the training data

1279
00:46:40,668 --> 00:46:43,139
set. So you can effectively just overwrite

1280
00:46:43,139 --> 00:46:45,260
this um with er with

1281
00:46:45,260 --> 00:46:47,378
er with catastrophic effect

1282
00:46:47,378 --> 00:46:48,378
if you're not careful.

1283
00:46:50,320 --> 00:46:52,079
There is, however, um.

1284
00:46:53,280 --> 00:46:55,559
Compromise in uh in some of this

1285
00:46:55,559 --> 00:46:57,719
space, and as Dinesh said, we were applying

1286
00:46:57,719 --> 00:46:59,719
this LA adapter just to the final

1287
00:46:59,719 --> 00:47:00,719
layer of the model.

1288
00:47:01,079 --> 00:47:03,199
However, there's some recent evidence to suggest

1289
00:47:03,199 --> 00:47:05,438
that instead of just applying it to that

1290
00:47:05,438 --> 00:47:07,478
final layer, applying a L

1291
00:47:07,478 --> 00:47:09,599
adapter to each layer of the

1292
00:47:09,599 --> 00:47:12,489
network uh could significantly

1293
00:47:12,489 --> 00:47:14,599
increase performance, albeit at the cost

1294
00:47:14,599 --> 00:47:15,918
of increased latency.

1295
00:47:16,289 --> 00:47:17,489
Um, however,

1296
00:47:17,780 --> 00:47:19,898
this would provide a more regret-free

1297
00:47:19,898 --> 00:47:22,250
approach, uh, to being able to,

1298
00:47:22,300 --> 00:47:24,570
uh, to be able to tune these approaches using,

1299
00:47:24,659 --> 00:47:26,659
using law-based, uh, methods,

1300
00:47:26,969 --> 00:47:29,099
and in contrast to the full fine

1301
00:47:29,099 --> 00:47:31,559
tuning, if you're also increasing the learning

1302
00:47:31,559 --> 00:47:33,958
rate as well, could potentially,

1303
00:47:34,139 --> 00:47:35,039
uh, achieve,

1304
00:47:35,340 --> 00:47:37,378
um, parity of er

1305
00:47:37,659 --> 00:47:40,039
of performance in in that space,

1306
00:47:40,219 --> 00:47:41,519
um, as well.

1307
00:47:41,869 --> 00:47:43,949
It's also worth mentioning just in the context of

1308
00:47:43,949 --> 00:47:46,030
some of the announcements this week er that

1309
00:47:46,030 --> 00:47:48,329
some of the capabilities of Nova Forge as well

1310
00:47:48,329 --> 00:47:50,708
might offer additional opportunities

1311
00:47:50,708 --> 00:47:52,949
for being able to er comprehensively

1312
00:47:52,949 --> 00:47:55,389
er tune these types of approaches er

1313
00:47:55,389 --> 00:47:56,148
in future.

1314
00:48:00,039 --> 00:48:00,579
So,

1315
00:48:00,918 --> 00:48:03,239
having talked a little bit about some of the experiments

1316
00:48:03,239 --> 00:48:05,559
and some of those opttimisations, I think it's also

1317
00:48:05,559 --> 00:48:07,679
worth touching on the evaluations. And I've

1318
00:48:07,679 --> 00:48:09,719
mentioned, for example, some of the F1 based

1319
00:48:09,719 --> 00:48:11,809
uh metrics that we'd used, um, and I'm

1320
00:48:11,809 --> 00:48:13,599
just going to dive a little bit more

1321
00:48:13,989 --> 00:48:15,099
deeply into these

1322
00:48:15,398 --> 00:48:16,139
before presenting

1323
00:48:16,750 --> 00:48:18,679
the final model comparisons.

1324
00:48:20,199 --> 00:48:22,519
So throughout our experiments, we used

1325
00:48:22,519 --> 00:48:24,159
word-based F1 scoring.

1326
00:48:24,510 --> 00:48:26,340
So as we heard right at the start,

1327
00:48:26,639 --> 00:48:28,760
the text of the shipping forecast is highly

1328
00:48:28,760 --> 00:48:30,869
specific and it's structured in its nature,

1329
00:48:30,918 --> 00:48:33,079
so it doesn't leave much room for creative

1330
00:48:33,079 --> 00:48:35,260
writing. We therefore wanted

1331
00:48:35,260 --> 00:48:37,360
to use a direct comparison metric,

1332
00:48:37,659 --> 00:48:38,239
so we're

1333
00:48:38,519 --> 00:48:40,728
counting the number of matching words,

1334
00:48:40,978 --> 00:48:43,340
the number of missing words, the number of extra

1335
00:48:43,340 --> 00:48:43,849
words

1336
00:48:44,208 --> 00:48:46,300
between the generated text and

1337
00:48:46,300 --> 00:48:47,500
the expected text.

1338
00:48:47,889 --> 00:48:49,929
And I'm showing an example of this just here. So

1339
00:48:49,929 --> 00:48:50,500
for example,

1340
00:48:50,809 --> 00:48:52,809
if our expected text relating to

1341
00:48:52,809 --> 00:48:55,110
the wind characteristics was northeast

1342
00:48:55,110 --> 00:48:57,269
veering southeast 3 to 5,

1343
00:48:57,530 --> 00:48:59,570
and our generated text was east

1344
00:48:59,570 --> 00:49:00,349
or northeast,

1345
00:49:00,719 --> 00:49:02,829
3 to 5, uh, then we've got

1346
00:49:02,829 --> 00:49:05,489
4 true positives, 2 false negatives,

1347
00:49:05,688 --> 00:49:07,378
2 false positives in this,

1348
00:49:07,648 --> 00:49:09,929
giving an overall F1 score

1349
00:49:10,128 --> 00:49:11,728
of 67%.

1350
00:49:13,409 --> 00:49:15,469
Now an alternative approach that other people might

1351
00:49:15,469 --> 00:49:17,489
have considered might have been using, for

1352
00:49:17,489 --> 00:49:18,909
example, a BERT score.

1353
00:49:19,250 --> 00:49:21,369
So this is an alternative to a word-based

1354
00:49:21,369 --> 00:49:23,489
precision recall, um, but it

1355
00:49:23,489 --> 00:49:25,489
provides a much softer match between the

1356
00:49:25,489 --> 00:49:26,000
words,

1357
00:49:26,369 --> 00:49:28,369
resulting in an overly lenient impression

1358
00:49:28,369 --> 00:49:30,449
of performance in applications such as

1359
00:49:30,449 --> 00:49:31,869
the Shipping Forecast where

1360
00:49:32,199 --> 00:49:34,208
um extreme precision is required.

1361
00:49:34,760 --> 00:49:36,500
So for example, using that same

1362
00:49:36,760 --> 00:49:39,000
uh case that I just showed for the Word-based

1363
00:49:39,000 --> 00:49:40,378
F1 score that we used,

1364
00:49:40,648 --> 00:49:43,119
uh, if this was a word, if this was a BERT-based

1365
00:49:43,119 --> 00:49:43,679
score,

1366
00:49:44,030 --> 00:49:46,360
then rather than having um.

1367
00:49:47,050 --> 00:49:49,289
Rather than having a result of 67%

1368
00:49:49,289 --> 00:49:51,550
accuracy as we, we were measuring here,

1369
00:49:51,809 --> 00:49:53,978
you would result in, um, in,

1370
00:49:54,010 --> 00:49:56,250
in a performance of about 86%,

1371
00:49:56,489 --> 00:49:57,889
which is a little bit misleading.

1372
00:49:58,168 --> 00:50:00,250
And really to drill down into why these types

1373
00:50:00,250 --> 00:50:02,360
of approaches are so misleading, um, is

1374
00:50:02,360 --> 00:50:04,829
if we consider, just for example, the word

1375
00:50:05,050 --> 00:50:06,530
uh north and south.

1376
00:50:06,849 --> 00:50:09,289
So obviously, geographical direction is fundamental

1377
00:50:09,289 --> 00:50:11,369
in communicating where weather systems

1378
00:50:11,369 --> 00:50:12,228
are coming from.

1379
00:50:12,610 --> 00:50:15,438
Um, but if we generated an expected

1380
00:50:15,438 --> 00:50:16,389
word uh north.

1381
00:50:16,918 --> 00:50:19,309
And the, sorry, if you had an expected

1382
00:50:19,309 --> 00:50:21,438
outcome of north and the generated word was

1383
00:50:21,438 --> 00:50:22,039
south,

1384
00:50:22,398 --> 00:50:22,978
then actually,

1385
00:50:23,478 --> 00:50:26,659
this would result in a BERT score of 82%,

1386
00:50:27,239 --> 00:50:28,300
despite this having

1387
00:50:28,708 --> 00:50:30,869
the complete opposite geographical meaning, and

1388
00:50:30,869 --> 00:50:33,079
this is because it's working on the embedding level rather

1389
00:50:33,079 --> 00:50:33,938
than on that word

1390
00:50:34,409 --> 00:50:35,398
word level in the score.

1391
00:50:35,719 --> 00:50:37,769
So. I thought I'd mention that just as a word of caution

1392
00:50:37,769 --> 00:50:39,769
in terms of evaluating these types of approaches,

1393
00:50:40,010 --> 00:50:42,570
but also to demonstrate the strictness and the robustness

1394
00:50:42,570 --> 00:50:43,708
that we applied to these

1395
00:50:44,128 --> 00:50:44,929
experiments,

1396
00:50:45,228 --> 00:50:47,329
because we're literally comparing to

1397
00:50:47,329 --> 00:50:47,969
what the

1398
00:50:48,688 --> 00:50:50,769
expert meteorologists issued in

1399
00:50:50,769 --> 00:50:51,829
their bulletins

1400
00:50:52,369 --> 00:50:54,449
and counting those words on that

1401
00:50:54,449 --> 00:50:55,010
basis.

1402
00:50:57,320 --> 00:50:59,478
Now, as Dinesh also mentioned, we

1403
00:50:59,478 --> 00:51:01,599
considered some comparisons between the Nova

1404
00:51:01,599 --> 00:51:04,079
Pro model uh and the Claude 3.7

1405
00:51:04,079 --> 00:51:06,159
Sonet model, um, for,

1406
00:51:06,398 --> 00:51:09,000
uh, the LLM uh comparisons.

1407
00:51:09,280 --> 00:51:11,469
And again, as Dinesh mentioned, there

1408
00:51:11,469 --> 00:51:13,949
was a difference in in performance, um,

1409
00:51:14,119 --> 00:51:16,199
with the Nova Pro scoring

1410
00:51:16,199 --> 00:51:18,719
an average uh Word F1 score of 62%

1411
00:51:18,719 --> 00:51:21,300
to Claude 3.7 Sonnet of 57%.

1412
00:51:21,610 --> 00:51:23,239
Um, and significantly,

1413
00:51:23,530 --> 00:51:25,559
from an operational perspective, um,

1414
00:51:25,570 --> 00:51:27,769
at a, at a, at a cheaper cost.

1415
00:51:28,168 --> 00:51:30,168
But I wouldn't drill too much into the

1416
00:51:30,168 --> 00:51:32,199
actual numbers here, because as we know,

1417
00:51:32,610 --> 00:51:33,309
within this field,

1418
00:51:33,728 --> 00:51:35,918
foundation models are being released all

1419
00:51:35,918 --> 00:51:38,389
the time, and this is, this is only,

1420
00:51:38,449 --> 00:51:39,559
uh, only growing.

1421
00:51:39,889 --> 00:51:42,199
So the main thing to take from this, and as it relates

1422
00:51:42,199 --> 00:51:44,208
to the architecture that Dinesh talked

1423
00:51:44,208 --> 00:51:45,208
about at the start.

1424
00:51:45,820 --> 00:51:48,099
Is the great thing about having architected

1425
00:51:48,099 --> 00:51:49,110
this in Bedrock,

1426
00:51:49,378 --> 00:51:51,500
means that actually, as new models are

1427
00:51:51,500 --> 00:51:53,668
released, so for example, uh Cloud

1428
00:51:53,668 --> 00:51:54,969
4.5 sonnet,

1429
00:51:55,260 --> 00:51:57,418
then actually it's just a one line change

1430
00:51:57,418 --> 00:51:59,500
within our uh within our code to

1431
00:51:59,500 --> 00:52:01,860
then be able to test this with a new model. And

1432
00:52:01,860 --> 00:52:03,898
depending on our application, we might find some

1433
00:52:03,898 --> 00:52:05,938
perform better or worse, but we have

1434
00:52:05,938 --> 00:52:08,019
this complete flexibility uh to

1435
00:52:08,019 --> 00:52:10,168
be able to take advantage of new models,

1436
00:52:10,179 --> 00:52:12,300
um, as they're as they're released on

1437
00:52:12,300 --> 00:52:13,539
this, uh, basis.

1438
00:52:15,070 --> 00:52:17,260
It's also worth saying something a bit about

1439
00:52:17,260 --> 00:52:19,590
the uh LLM versus

1440
00:52:19,590 --> 00:52:21,750
uh the VLM uh comparisons

1441
00:52:21,750 --> 00:52:22,289
here.

1442
00:52:22,829 --> 00:52:24,909
Now, with our LLM based approach,

1443
00:52:25,030 --> 00:52:27,030
we were providing uh information

1444
00:52:27,030 --> 00:52:29,309
to that through a through a text uh

1445
00:52:29,309 --> 00:52:30,208
intermediary,

1446
00:52:30,500 --> 00:52:33,168
um, and we were able to provide a little bit of extra

1447
00:52:33,168 --> 00:52:35,228
information uh due to the way we were able

1448
00:52:35,228 --> 00:52:37,750
to format some of these experiments, and it was on

1449
00:52:37,750 --> 00:52:39,869
uh the Nova Pro rather than uh the

1450
00:52:39,869 --> 00:52:40,750
Nova Lite model.

1451
00:52:41,409 --> 00:52:43,530
In comparison, our VLM approach

1452
00:52:43,530 --> 00:52:45,530
used, for example, a more simple

1453
00:52:45,530 --> 00:52:47,849
representation of some of our probabilistic

1454
00:52:47,849 --> 00:52:49,869
information, and this could be significantly improved,

1455
00:52:50,159 --> 00:52:52,030
um, but nonetheless, we're getting

1456
00:52:52,289 --> 00:52:54,519
scores compared to the human.

1457
00:52:55,250 --> 00:52:57,530
Meteorologists written bulletins of

1458
00:52:57,530 --> 00:53:00,340
between 52 and 62%,

1459
00:53:00,530 --> 00:53:02,728
and the opportunities for evolving this, particularly

1460
00:53:02,728 --> 00:53:04,070
in the VLM space,

1461
00:53:04,449 --> 00:53:07,079
is particularly significant, and we think that this is

1462
00:53:07,090 --> 00:53:09,909
going to be key going forwards in terms of

1463
00:53:10,050 --> 00:53:12,628
reducing some of the information bottlenecks

1464
00:53:12,769 --> 00:53:15,478
that would otherwise be associated with

1465
00:53:15,478 --> 00:53:15,989
providing

1466
00:53:16,840 --> 00:53:19,300
information to LLMs through a through a text intermediary

1467
00:53:19,300 --> 00:53:21,208
stage as well.

1468
00:53:23,329 --> 00:53:25,489
So just to summarize then, some

1469
00:53:25,489 --> 00:53:27,668
of the key learnings uh from this.

1470
00:53:28,208 --> 00:53:30,409
Fundamentally, we've evolved a pattern

1471
00:53:30,409 --> 00:53:32,648
for architecting scalable AI

1472
00:53:32,648 --> 00:53:35,168
solutions with considerations of alternatives,

1473
00:53:35,329 --> 00:53:37,329
advantages and disadvantages that

1474
00:53:37,329 --> 00:53:39,570
can be applied to er this

1475
00:53:39,570 --> 00:53:42,250
type of data to text conversion

1476
00:53:42,478 --> 00:53:44,530
um that applies to so many

1477
00:53:44,530 --> 00:53:46,668
of the types of products and services that we

1478
00:53:47,000 --> 00:53:48,728
er deliver at the Met Office.

1479
00:53:49,539 --> 00:53:50,070
Also,

1480
00:53:50,409 --> 00:53:52,489
uh, this provides a framework for rapid

1481
00:53:52,489 --> 00:53:55,050
experimentation, um, and enhancement.

1482
00:53:55,250 --> 00:53:56,449
As Dinesh mentioned,

1483
00:53:56,840 --> 00:53:59,090
all of this work was conducted within a four-week

1484
00:53:59,090 --> 00:54:01,148
period, from spinning up the environment,

1485
00:54:01,369 --> 00:54:03,909
uh, to conducting, uh, the evaluations,

1486
00:54:04,119 --> 00:54:06,289
um, and, and assessments. So

1487
00:54:06,489 --> 00:54:08,610
it just shows the types of velocity that you can

1488
00:54:08,610 --> 00:54:10,708
work at both, uh, within,

1489
00:54:10,750 --> 00:54:12,889
within these, but also with uh working

1490
00:54:12,889 --> 00:54:14,929
practices and, uh, support from er the

1491
00:54:14,929 --> 00:54:15,688
team at Amazon.

1492
00:54:17,030 --> 00:54:19,309
Fundamentally, we were looking at comparisons

1493
00:54:19,309 --> 00:54:21,449
of uh LLM based approaches

1494
00:54:21,579 --> 00:54:23,590
versus VLM based approaches.

1495
00:54:23,829 --> 00:54:25,949
And as I said, the LLM performs better

1496
00:54:25,949 --> 00:54:28,269
because we're able to apply some of the extra domain knowledge

1497
00:54:28,269 --> 00:54:30,090
that we're able to simplify some of its input.

1498
00:54:30,929 --> 00:54:33,010
But these textual data

1499
00:54:33,010 --> 00:54:35,059
descriptions do represent an

1500
00:54:35,059 --> 00:54:35,840
information bottleneck.

1501
00:54:36,179 --> 00:54:38,000
So in the future, we're anticipating

1502
00:54:38,429 --> 00:54:40,820
that the VLM will actually outperform the

1503
00:54:40,820 --> 00:54:42,079
LLM on this task,

1504
00:54:42,500 --> 00:54:44,079
um, and we've got some exciting steps

1505
00:54:44,699 --> 00:54:46,679
to implement to be able to move forward

1506
00:54:47,340 --> 00:54:48,019
on this.

1507
00:54:48,889 --> 00:54:51,030
Finally, we talked in uh

1508
00:54:51,289 --> 00:54:53,648
in length around some of the fine tuning

1509
00:54:53,648 --> 00:54:55,648
experiments that were conducted, and

1510
00:54:55,648 --> 00:54:57,769
particularly the use of Laura versus full

1511
00:54:57,769 --> 00:54:58,550
fine tuning.

1512
00:54:58,849 --> 00:55:00,128
So Laura learns less,

1513
00:55:01,000 --> 00:55:03,489
so for specialized applications, then actually

1514
00:55:03,688 --> 00:55:05,809
uh full model training could

1515
00:55:05,809 --> 00:55:07,030
have a performance advantage,

1516
00:55:07,329 --> 00:55:09,610
but it also forgets less, so you risk

1517
00:55:09,610 --> 00:55:11,969
the or reduce the risk of catastrophic

1518
00:55:11,969 --> 00:55:14,050
failure. And as I said, some of the really

1519
00:55:14,050 --> 00:55:15,300
emergent. Research at the moment

1520
00:55:15,760 --> 00:55:17,500
suggests that applying this to each layer in the

1521
00:55:18,628 --> 00:55:20,659
model could give a regret-free approach,

1522
00:55:20,878 --> 00:55:23,659
and there are further considerations and opportunities

1523
00:55:23,659 --> 00:55:25,800
associated with the Nova Forge framework which

1524
00:55:25,800 --> 00:55:26,378
might provide

1525
00:55:27,398 --> 00:55:29,559
an additional basis um for

1526
00:55:29,559 --> 00:55:33,260
this. I'll

1527
00:55:33,260 --> 00:55:35,458
leave this uh up here at the moment, so

1528
00:55:35,458 --> 00:55:37,539
really just bringing it all together. We've

1529
00:55:37,539 --> 00:55:39,739
got the full architecture diagram that we used

1530
00:55:39,739 --> 00:55:41,898
for the prototype on the left-hand side.

1531
00:55:42,139 --> 00:55:44,139
On the right hand side, we're just showing some of

1532
00:55:44,139 --> 00:55:46,219
the interface uh and it's recording

1533
00:55:46,219 --> 00:55:48,250
the interface for the er LLM

1534
00:55:48,250 --> 00:55:48,958
based approach

1535
00:55:49,599 --> 00:55:51,800
that provided us a means of being able to expose

1536
00:55:51,800 --> 00:55:53,300
this to some of our users.

1537
00:55:53,619 --> 00:55:55,659
And really excitingly, uh we're moving.

1538
00:55:55,750 --> 00:55:56,929
Forward with actually

1539
00:55:57,389 --> 00:56:00,550
being able to provide some of these outputs

1540
00:56:00,550 --> 00:56:03,019
for evaluation by our meteorologists,

1541
00:56:03,309 --> 00:56:05,469
not just applied to the shipping forecast, but

1542
00:56:05,469 --> 00:56:06,489
applied to similar

1543
00:56:06,820 --> 00:56:09,019
workflows as well. So

1544
00:56:09,019 --> 00:56:10,398
we're really looking forward to this.

1545
00:56:10,708 --> 00:56:12,789
Just incidentally, what we're seeing here on the left hand

1546
00:56:12,789 --> 00:56:14,610
side is we've loaded in

1547
00:56:15,708 --> 00:56:18,489
the shipping forecast bulletin, this is for evaluation.

1548
00:56:18,820 --> 00:56:20,898
Uh, we're running the LLM task here and

1549
00:56:20,898 --> 00:56:22,918
we can see the progress, uh,

1550
00:56:22,938 --> 00:56:25,219
that is associated with this,

1551
00:56:25,418 --> 00:56:27,500
um, as it's running, um, in

1552
00:56:27,500 --> 00:56:29,659
this case, they say it's the, the LLM

1553
00:56:29,659 --> 00:56:30,320
task.

1554
00:56:30,619 --> 00:56:32,719
Um, and then on the right hand side we'll start

1555
00:56:32,719 --> 00:56:35,019
to see the, uh, the, the

1556
00:56:35,019 --> 00:56:37,280
result returned, um, from,

1557
00:56:37,500 --> 00:56:38,820
uh, from the LLM,

1558
00:56:39,090 --> 00:56:41,219
uh, corresponding to the shipping forecast that

1559
00:56:41,219 --> 00:56:43,378
then gives us, uh, this

1560
00:56:43,378 --> 00:56:44,079
opportunity,

1561
00:56:44,340 --> 00:56:44,958
um,

1562
00:56:45,820 --> 00:56:47,418
opportunity for, uh.

1563
00:56:48,250 --> 00:56:49,659
For, for evaluation,

1564
00:56:49,969 --> 00:56:50,489
um,

1565
00:56:50,849 --> 00:56:52,168
on, on that.

1566
00:56:54,438 --> 00:56:56,500
So we can see this is now generated on the

1567
00:56:56,500 --> 00:56:57,590
right hand side.

1568
00:56:57,860 --> 00:56:59,978
We can compare this directly to

1569
00:56:59,978 --> 00:57:02,059
the bulletin that was written by the

1570
00:57:02,059 --> 00:57:03,199
meteorologists,

1571
00:57:03,820 --> 00:57:05,478
but we can then also apply

1572
00:57:05,898 --> 00:57:08,360
our scoring based on these comparisons

1573
00:57:08,978 --> 00:57:11,340
so we can actually evaluate the performance

1574
00:57:11,340 --> 00:57:13,375
of This, which will then feature

1575
00:57:13,375 --> 00:57:14,934
in the tasks at the bottom

1576
00:57:15,434 --> 00:57:18,114
down here. So within this same interface

1577
00:57:18,114 --> 00:57:20,195
we're just able to start to

1578
00:57:20,195 --> 00:57:20,735
engage

1579
00:57:21,195 --> 00:57:23,594
some of those types of interested

1580
00:57:23,594 --> 00:57:24,614
interested users

1581
00:57:25,784 --> 00:57:26,655
on this basis.

1582
00:57:27,034 --> 00:57:28,614
And then finally at the bottom

1583
00:57:29,195 --> 00:57:30,375
we're just generating

1584
00:57:31,313 --> 00:57:32,655
some of the

1585
00:57:34,929 --> 00:57:36,929
Well, we're generating opportunities

1586
00:57:36,929 --> 00:57:39,360
for sort of translation, obviously as a national meteorological

1587
00:57:39,360 --> 00:57:40,309
Service, then

1588
00:57:40,648 --> 00:57:43,128
then being able to provide data in different or input

1589
00:57:43,128 --> 00:57:45,128
in different languages is obviously off the shelf

1590
00:57:45,128 --> 00:57:47,260
capability of these types of models,

1591
00:57:47,409 --> 00:57:50,010
and we can see the output for human reference

1592
00:57:50,099 --> 00:57:51,188
that was generated

1593
00:57:51,849 --> 00:57:56,119
there. I'd

1594
00:57:56,119 --> 00:57:58,398
just like to conclude um by

1595
00:57:58,398 --> 00:58:00,639
thanking, thanking you for being here, as Dinesh

1596
00:58:00,639 --> 00:58:02,800
said, um, really appreciate it

1597
00:58:02,800 --> 00:58:04,300
after, after replay,

1598
00:58:04,590 --> 00:58:06,938
um, but also I'd particularly like to thank

1599
00:58:06,938 --> 00:58:09,039
uh the team from uh the specialist

1600
00:58:09,039 --> 00:58:10,739
prototyping team from AWS.

1601
00:58:11,119 --> 00:58:12,958
Uh, whose support on this has been

1602
00:58:13,228 --> 00:58:15,599
absolutely fantastic, um, and absolutely

1603
00:58:15,599 --> 00:58:17,610
fundamental to, to unlocking this,

1604
00:58:17,878 --> 00:58:20,000
uh, so thanking Dinesh, also

1605
00:58:20,000 --> 00:58:22,039
Emilio and Lewis and Emir who couldn't

1606
00:58:22,039 --> 00:58:24,398
be here today, um, and Kanda, um,

1607
00:58:24,559 --> 00:58:25,860
who's the team leader as well.

1608
00:58:26,708 --> 00:58:29,228
Finally, I'd just like to say please do

1609
00:58:29,228 --> 00:58:30,369
provide any feedback

1610
00:58:31,228 --> 00:58:33,269
on this via the survey in the

1611
00:58:33,269 --> 00:58:33,909
mobile app.

1612
00:58:34,349 --> 00:58:35,369
That would be much appreciated.

1613
00:58:36,159 --> 00:58:38,168
Um, and if anybody wants to chat further,

1614
00:58:38,179 --> 00:58:40,519
um, afterwards, we'll be at the back

1615
00:58:40,519 --> 00:58:42,208
and, and very happy to pick up questions.

1616
00:58:43,360 --> 00:58:44,739
Also point you to

1617
00:58:45,269 --> 00:58:47,320
on archive this week we released a

1618
00:58:47,320 --> 00:58:49,059
paper describing in more detail

1619
00:58:49,429 --> 00:58:51,728
some of these experiments that were conducted.

1620
00:58:52,219 --> 00:58:54,349
So if you're interested in that, please speak to Dinesh Ryan.

1621
00:58:54,360 --> 00:58:57,000
We'll be pleased to provide that

1622
00:58:57,000 --> 00:58:58,398
link. Many thanks.

1623
00:58:58,708 --> 00:58:59,398
Thank you so much.

