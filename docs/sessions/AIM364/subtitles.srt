1
00:00:00,000 --> 00:00:01,980
- Today we will be talking about

2
00:00:01,980 --> 00:00:06,480
how you can streamline AI
model development lifecycle,

3
00:00:06,480 --> 00:00:08,610
as well as monitor

4
00:00:08,610 --> 00:00:10,530
and do lifecycle management

5
00:00:10,530 --> 00:00:15,530
for your AI workflows
using Amazon SageMaker AI.

6
00:00:15,810 --> 00:00:17,430
My name is Khushboo Srivastava.

7
00:00:17,430 --> 00:00:20,670
I'm a senior product manager at Amazon.

8
00:00:20,670 --> 00:00:24,000
I manage the Amazon
SageMaker Studio product.

9
00:00:24,000 --> 00:00:27,630
I have been with AWS for
almost six plus years.

10
00:00:27,630 --> 00:00:32,490
Along with me are joining
Bruno, a senior specialist essay

11
00:00:32,490 --> 00:00:37,490
and Manikandan, one of our
esteemed customers from KOHO

12
00:00:37,500 --> 00:00:40,680
who will also talk
briefly about themselves.

13
00:00:40,680 --> 00:00:43,140
- Hello everyone, thank you
for joining this session today.

14
00:00:43,140 --> 00:00:45,810
My name is Bruno Pistone,
coming from Italy, Milan,

15
00:00:45,810 --> 00:00:47,790
probably from the accent you can hear.

16
00:00:47,790 --> 00:00:51,840
I'm a senior worldwide specialist
solutions architect in AWS

17
00:00:51,840 --> 00:00:54,780
almost from five years and a half,

18
00:00:54,780 --> 00:00:56,490
focusing on model training

19
00:00:56,490 --> 00:01:00,390
and now model customization
for large language models.

20
00:01:00,390 --> 00:01:01,710
Mani.
- Hi everyone.

21
00:01:01,710 --> 00:01:03,540
My name is Manikandan Paramasivan.

22
00:01:03,540 --> 00:01:07,233
I'm a senior staff architect
for data, ML and AI at KOHO.

23
00:01:08,430 --> 00:01:10,920
I manage and lead the architecture infra

24
00:01:10,920 --> 00:01:14,430
and operations for our
data and ML platforms.

25
00:01:14,430 --> 00:01:16,230
Excited to be here to share our story

26
00:01:16,230 --> 00:01:17,780
about how we SageMaker at KOHO.

27
00:01:19,485 --> 00:01:20,535
- Perfect
- Awesome.

28
00:01:23,010 --> 00:01:26,670
Innovations in generative
AI are quickly transforming

29
00:01:26,670 --> 00:01:28,530
the business landscape.

30
00:01:28,530 --> 00:01:32,220
IDC predicts global
spending on generative AI

31
00:01:32,220 --> 00:01:37,050
is expected to reach $202 billion by 2028,

32
00:01:38,160 --> 00:01:42,960
representing around 32%
of the overall AI spending

33
00:01:42,960 --> 00:01:47,960
with a compound annual growth rate of 29%.

34
00:01:48,210 --> 00:01:52,410
Citing significant
breakthroughs in generative AI,

35
00:01:52,410 --> 00:01:54,270
Goldman Sachs predicts

36
00:01:54,270 --> 00:01:57,750
that generative AI can increase global GDP

37
00:01:57,750 --> 00:02:02,610
by as much as 7% or almost $7 trillion

38
00:02:02,610 --> 00:02:07,530
and lift productivity growth
by 1.5 percentage basis points

39
00:02:07,530 --> 00:02:09,003
over the next 10 years.

40
00:02:12,330 --> 00:02:16,620
We are examining the growth
trajectory of generative AI.

41
00:02:16,620 --> 00:02:18,570
As you can see from our slide,

42
00:02:18,570 --> 00:02:20,950
we are witnessing unprecedented growth

43
00:02:21,840 --> 00:02:24,450
and adoption rates across multiple fronts.

44
00:02:24,450 --> 00:02:28,470
89% of enterprises are advancing

45
00:02:28,470 --> 00:02:32,460
in generative AI
initiatives as of right now,

46
00:02:32,460 --> 00:02:36,813
with 92% planning to
increase investments by 2027.

47
00:02:37,770 --> 00:02:41,670
78% of organizations now use AI

48
00:02:41,670 --> 00:02:43,990
in at least one of
their business functions

49
00:02:44,970 --> 00:02:49,260
and 77% of organizations choose AI models

50
00:02:49,260 --> 00:02:53,523
that are of size 13 billion
parameters or smaller,

51
00:02:54,360 --> 00:02:57,420
which suggest a preference
for customization

52
00:02:57,420 --> 00:03:01,983
and for customizable and
cost-effective models.

53
00:03:02,850 --> 00:03:06,570
Achieving these benefits
is not without challenges.

54
00:03:06,570 --> 00:03:08,790
Let's talk a little bit about challenges

55
00:03:08,790 --> 00:03:11,103
that our enterprise customers face.

56
00:03:13,800 --> 00:03:18,060
Our enterprise customers are
constantly sharing with us

57
00:03:18,060 --> 00:03:21,930
couple of major challenges
in ML development.

58
00:03:21,930 --> 00:03:23,880
First disparate

59
00:03:23,880 --> 00:03:25,710
and disconnected ML tools

60
00:03:25,710 --> 00:03:29,343
are significantly
increasing time to market,

61
00:03:30,240 --> 00:03:32,970
teams are spending more
time managing tools

62
00:03:32,970 --> 00:03:35,580
than developing solutions.

63
00:03:35,580 --> 00:03:39,030
Second, isolation between team members

64
00:03:39,030 --> 00:03:42,513
is a real killer in
productivity and collaboration.

65
00:03:43,560 --> 00:03:45,540
Data scientists, AI developers

66
00:03:45,540 --> 00:03:48,660
and business teams often work in silos,

67
00:03:48,660 --> 00:03:52,353
leading to duplicated efforts
and missed opportunities.

68
00:03:53,280 --> 00:03:55,350
Third, governing AI

69
00:03:55,350 --> 00:03:57,570
and ML projects efficiently

70
00:03:57,570 --> 00:04:02,070
becomes exponentially
more complex as you scale.

71
00:04:02,070 --> 00:04:04,050
Without the right framework, security

72
00:04:04,050 --> 00:04:07,440
and compliance can
become major bottlenecks.

73
00:04:07,440 --> 00:04:09,990
And finally, availability

74
00:04:09,990 --> 00:04:14,190
and management of infrastructure
is key to training

75
00:04:14,190 --> 00:04:18,273
and fine-tuning machine learning
and large language models.

76
00:04:22,890 --> 00:04:26,910
This slide talks a little
bit about the similarities

77
00:04:26,910 --> 00:04:30,390
between a classic traditional
machine learning workflow

78
00:04:30,390 --> 00:04:33,240
as well as a generative AI workflow.

79
00:04:33,240 --> 00:04:36,043
We always start from preparation of data,

80
00:04:36,043 --> 00:04:39,150
a GenAI project that means format the data

81
00:04:39,150 --> 00:04:44,150
into the prompt template or
structure of the identified LLM.

82
00:04:44,610 --> 00:04:48,390
Second, we select the
right foundational model

83
00:04:48,390 --> 00:04:51,180
that we wanna fine tune for our use case

84
00:04:51,180 --> 00:04:53,160
and with our own data.

85
00:04:53,160 --> 00:04:57,960
Third, we run the workload on
the compute cluster required

86
00:04:57,960 --> 00:04:59,730
and finally we evaluate

87
00:04:59,730 --> 00:05:01,113
and deploy the model.

88
00:05:04,320 --> 00:05:06,870
Amazon SageMaker Studio.

89
00:05:06,870 --> 00:05:08,580
To address these challenges

90
00:05:08,580 --> 00:05:13,580
and help our customers build
end-to-end ML workflows,

91
00:05:14,460 --> 00:05:17,970
we launched Amazon SageMaker
Studio couple of years ago.

92
00:05:17,970 --> 00:05:20,070
It provides a purpose-built,

93
00:05:20,070 --> 00:05:23,310
end-to-end ML development platform

94
00:05:23,310 --> 00:05:27,840
where data scientists can not only build,

95
00:05:27,840 --> 00:05:31,770
deploy their models or
fine tune their models,

96
00:05:31,770 --> 00:05:36,660
but also manage and
monitor their AI workflows.

97
00:05:36,660 --> 00:05:41,640
Data scientists can select
an ID of their preference.

98
00:05:41,640 --> 00:05:46,640
They can select from a choice
of our Studio, JupyterLab

99
00:05:46,800 --> 00:05:49,713
or Code Editor built
on open-source VS Code.

100
00:05:50,760 --> 00:05:54,000
They can use SageMaker Studio Notebooks

101
00:05:54,000 --> 00:05:55,980
for data preparation.

102
00:05:55,980 --> 00:06:00,720
They can write data
scripts for data generation

103
00:06:00,720 --> 00:06:02,610
and preparation,

104
00:06:02,610 --> 00:06:05,580
or they can use the
built-in EMR connections

105
00:06:05,580 --> 00:06:09,270
to run their large-scale Spark workflows

106
00:06:09,270 --> 00:06:13,143
for running their data
preparation jobs at scale.

107
00:06:14,670 --> 00:06:19,670
Finally, they can select from
a hub of foundation models.

108
00:06:20,250 --> 00:06:22,590
They can choose one of the pre-built

109
00:06:22,590 --> 00:06:24,723
foundation model already available,

110
00:06:25,560 --> 00:06:29,190
select a fine-tuning
technique of their choice,

111
00:06:29,190 --> 00:06:30,750
any fine-tuning techniques

112
00:06:30,750 --> 00:06:33,063
or reinforcement learning techniques,

113
00:06:34,350 --> 00:06:36,630
or you can also bring in your own model

114
00:06:36,630 --> 00:06:38,940
and train it from scratch.

115
00:06:38,940 --> 00:06:41,253
Finally, deploy it in production.

116
00:06:42,960 --> 00:06:46,350
SageMaker Studio provides
you a visual interface

117
00:06:46,350 --> 00:06:50,520
for deploying your endpoints
in production as well.

118
00:06:50,520 --> 00:06:52,020
And finally, you can manage

119
00:06:52,020 --> 00:06:55,320
and monitor all your
endpoints, all your models,

120
00:06:55,320 --> 00:06:57,543
everything under a single pane of glass.

121
00:06:59,400 --> 00:07:02,430
The key thing to note
is that SageMaker Studio

122
00:07:02,430 --> 00:07:06,510
provides multiple SageMaker
AI service offerings

123
00:07:06,510 --> 00:07:09,720
within a single platform that you can use

124
00:07:09,720 --> 00:07:13,980
for each step of your
AI development workflow.

125
00:07:13,980 --> 00:07:17,640
You can also run experiments using MLflow

126
00:07:17,640 --> 00:07:19,590
and you can monitor your experiments.

127
00:07:19,590 --> 00:07:22,140
You can also build your pipelines

128
00:07:22,140 --> 00:07:25,620
using SageMaker pipelines within Studio.

129
00:07:25,620 --> 00:07:29,580
It could be as simple as drag
and drop to build a pipeline

130
00:07:29,580 --> 00:07:31,770
or you could write your own code

131
00:07:31,770 --> 00:07:34,440
in the SageMaker Studio Notebooks.

132
00:07:34,440 --> 00:07:37,023
The flexibility is really your choice.

133
00:07:40,530 --> 00:07:43,950
Today there are tens of
thousands of customers

134
00:07:43,950 --> 00:07:47,430
using Amazon SageMaker AI, to name a few,

135
00:07:47,430 --> 00:07:51,897
we have 3M, Coinbase, Intuit, Dominoes,

136
00:07:53,610 --> 00:07:56,763
and many other that are
not listed on this slide.

137
00:07:59,250 --> 00:08:03,300
Before we jump into the
details of each offering

138
00:08:03,300 --> 00:08:07,800
involved in an end-to-end
generative AI project development

139
00:08:07,800 --> 00:08:09,030
for building, fine-tuning

140
00:08:09,030 --> 00:08:12,810
and deploying LLMs, we would
like to see in practice

141
00:08:12,810 --> 00:08:15,400
how SageMaker AI can help developers

142
00:08:16,320 --> 00:08:18,210
with these fundamental activities.

143
00:08:18,210 --> 00:08:21,303
For this I will hand over the
mic to my colleague, Bruno.

144
00:08:24,510 --> 00:08:26,400
- Thank you, Khushboo.

145
00:08:26,400 --> 00:08:30,480
So in this demo we would
like to actually give you

146
00:08:30,480 --> 00:08:34,680
a tangible example of what
are the main activities

147
00:08:34,680 --> 00:08:37,050
that are related to a machine learning,

148
00:08:37,050 --> 00:08:39,510
as well as a generative AI project.

149
00:08:39,510 --> 00:08:40,343
And in particular,

150
00:08:40,343 --> 00:08:42,450
we are going to start
from the data preparation

151
00:08:42,450 --> 00:08:44,940
to fine tune a large language model

152
00:08:44,940 --> 00:08:48,000
and then deploy this model
by using different type

153
00:08:48,000 --> 00:08:51,060
of SageMaker AI or services.

154
00:08:51,060 --> 00:08:54,780
In particular, I actually
prepared this architecture

155
00:08:54,780 --> 00:08:58,080
by impersonating two main personas.

156
00:08:58,080 --> 00:09:02,280
As a platform administrator,
I actually prepared

157
00:09:02,280 --> 00:09:05,070
the environment for my development

158
00:09:05,070 --> 00:09:06,570
and training and deployment,

159
00:09:06,570 --> 00:09:10,170
and in particular I created a
private networking environment

160
00:09:10,170 --> 00:09:11,820
as well as deploying the cluster

161
00:09:11,820 --> 00:09:16,300
where I'm going to submit
these jobs by using HyperPod

162
00:09:17,280 --> 00:09:20,490
with EKS orchestration as
well as using the same cluster

163
00:09:20,490 --> 00:09:22,200
for the deployment.

164
00:09:22,200 --> 00:09:26,670
For the development part,
I deployed SageMaker Studio

165
00:09:26,670 --> 00:09:29,580
where basically I can give the possibility

166
00:09:29,580 --> 00:09:32,820
to data scientist engineers
to connect, prepare data

167
00:09:32,820 --> 00:09:37,170
and link Studio with HyperPod
by using a shared file system

168
00:09:37,170 --> 00:09:40,080
with Amazon FSx for bplstone volume.

169
00:09:40,080 --> 00:09:43,230
So as an engineer, data
scientist or developers,

170
00:09:43,230 --> 00:09:44,850
I can connect to Studio,

171
00:09:44,850 --> 00:09:47,400
I can select the IDE of choice,

172
00:09:47,400 --> 00:09:50,730
that can be, for example,
JupyterLab or Code Editor.

173
00:09:50,730 --> 00:09:52,530
I can start prototyping my code,

174
00:09:52,530 --> 00:09:55,290
save the data in the shared FSx

175
00:09:55,290 --> 00:09:58,980
in order to have this accessible
from the cluster itself.

176
00:09:58,980 --> 00:10:03,600
And then once I'm ready I
can submit jobs for training

177
00:10:03,600 --> 00:10:06,630
and deployment by using the HyperPod CLI

178
00:10:06,630 --> 00:10:09,633
as well as the Kubernetes CLI kubectl.

179
00:10:11,490 --> 00:10:14,670
Once the job is running,
I can monitor everything

180
00:10:14,670 --> 00:10:17,820
in terms of what is happening
as well as training metrics

181
00:10:17,820 --> 00:10:20,730
or system metrics directly
in SageMaker Studio

182
00:10:20,730 --> 00:10:23,100
by using task governance capabilities

183
00:10:23,100 --> 00:10:25,083
as well as manage them workflow.

184
00:10:28,050 --> 00:10:31,380
Now we would like to actually
jump directly into the demo.

185
00:10:31,380 --> 00:10:35,100
This is the SageMaker
Studio user interface.

186
00:10:35,100 --> 00:10:36,963
I prepared the,

187
00:10:38,550 --> 00:10:43,140
I can select the JupyterLab
space that I already prepared.

188
00:10:43,140 --> 00:10:45,633
There are all the necessary
components installed.

189
00:10:46,920 --> 00:10:50,010
I can connect to the JupyterLab space.

190
00:10:50,010 --> 00:10:54,243
What is happening is that in
a few minutes it is loading.

191
00:10:55,650 --> 00:10:58,440
So here I have my user interface.

192
00:10:58,440 --> 00:11:02,370
I can click on or custom
file system, select the FSx.

193
00:11:02,370 --> 00:11:05,010
I already prepared the entire content.

194
00:11:05,010 --> 00:11:06,720
The first step is the data preparation,

195
00:11:06,720 --> 00:11:09,060
but initially we want to actually install

196
00:11:09,060 --> 00:11:10,710
all the necessary Python models

197
00:11:10,710 --> 00:11:12,600
that I need for preparing the data.

198
00:11:12,600 --> 00:11:15,120
So I can actually install
a requirements.txt

199
00:11:15,120 --> 00:11:17,040
that contains all my Python models

200
00:11:17,040 --> 00:11:19,170
and I can do interactively.

201
00:11:19,170 --> 00:11:21,450
Once I'm ready, I can restart the kernel

202
00:11:21,450 --> 00:11:24,210
and I can start the
data preparation piece.

203
00:11:24,210 --> 00:11:26,915
I selected a model, Qwen3-4B

204
00:11:26,915 --> 00:11:30,570
and I want to improve, as task,
the possibility to reason,

205
00:11:30,570 --> 00:11:35,220
and or invoke tools that is
pretty important for Agentic AI.

206
00:11:35,220 --> 00:11:37,260
Here I can define all the functions

207
00:11:37,260 --> 00:11:39,570
for preparing this data
directly in the Notebook,

208
00:11:39,570 --> 00:11:42,390
such as extracting the tool
content, the thinking content,

209
00:11:42,390 --> 00:11:44,490
and validate the messages.

210
00:11:44,490 --> 00:11:47,490
And after that I can
actually prepare the dataset

211
00:11:47,490 --> 00:11:49,140
with a proper function.

212
00:11:49,140 --> 00:11:50,520
What is going to happen is

213
00:11:50,520 --> 00:11:54,090
that this function is
formatting the entire dataset

214
00:11:54,090 --> 00:11:56,850
into the proper prompt
style accepted by the model.

215
00:11:56,850 --> 00:11:59,460
We will see here with
all the specific tags

216
00:11:59,460 --> 00:12:01,920
and highlighting what
are the main components

217
00:12:01,920 --> 00:12:04,800
such as for example, system
prompt, how to invoke tools,

218
00:12:04,800 --> 00:12:07,143
the reasoning and how
to generate the answer.

219
00:12:09,090 --> 00:12:13,890
Once I'm ready I can
directly upload on Amazon S3

220
00:12:13,890 --> 00:12:16,650
as well as save in the Share-ify system.

221
00:12:16,650 --> 00:12:19,920
So here for example, I can
even access this data directly

222
00:12:19,920 --> 00:12:21,423
from SageMaker Studio.

223
00:12:22,920 --> 00:12:26,880
Now it's time to start
the training activity.

224
00:12:26,880 --> 00:12:29,310
So the first thing is
to define the parameters

225
00:12:29,310 --> 00:12:30,800
for my training workload.

226
00:12:30,800 --> 00:12:34,080
So I specify where the
data are saved are stored

227
00:12:34,080 --> 00:12:36,240
on the FSx for bplstone volume

228
00:12:36,240 --> 00:12:39,390
and parameters such as
learning rate epochs.

229
00:12:39,390 --> 00:12:41,760
Since I'm operating
with HyperPod with EKS,

230
00:12:41,760 --> 00:12:43,860
I'm actually defining a manifest file

231
00:12:43,860 --> 00:12:47,760
which contains all the information
around the PyTorch job,

232
00:12:47,760 --> 00:12:50,550
such as the number of
distances, the GPU type

233
00:12:50,550 --> 00:12:52,893
and docker image that they want to use.

234
00:12:54,570 --> 00:12:56,970
Once I'm ready directly from JupyterLab,

235
00:12:56,970 --> 00:12:58,080
I can open the terminal

236
00:12:58,080 --> 00:13:02,250
and start interacting with HyperPod.

237
00:13:02,250 --> 00:13:07,250
In this case, I used the
kubectl SDK I applied,

238
00:13:07,680 --> 00:13:09,420
so I deployed the workload,

239
00:13:09,420 --> 00:13:11,760
and here I can see that
there are two new pods

240
00:13:11,760 --> 00:13:13,320
in my cluster.

241
00:13:13,320 --> 00:13:16,800
I can investigate the
logs of the first one

242
00:13:16,800 --> 00:13:18,510
and we will see that the first step for me

243
00:13:18,510 --> 00:13:20,820
is actually to align the same environment

244
00:13:20,820 --> 00:13:22,980
that is available on SageMaker Studio

245
00:13:22,980 --> 00:13:24,810
with the pod environment

246
00:13:24,810 --> 00:13:27,810
by installing the same
libraries to give continuity

247
00:13:27,810 --> 00:13:28,773
to my workload.

248
00:13:30,210 --> 00:13:32,430
I can clear the terminal,

249
00:13:32,430 --> 00:13:34,080
and now I can actually investigate

250
00:13:34,080 --> 00:13:38,250
what is the main node since
it is a distributed workload,

251
00:13:38,250 --> 00:13:41,370
by analyzing what is the main address

252
00:13:41,370 --> 00:13:42,900
that is acting in the cluster.

253
00:13:42,900 --> 00:13:45,240
So it is the same pod,

254
00:13:45,240 --> 00:13:47,760
and now I can actually
connect in order to see

255
00:13:47,760 --> 00:13:50,100
what is happening in terms of workloads.

256
00:13:50,100 --> 00:13:53,730
So here the model is actually downloading,

257
00:13:53,730 --> 00:13:55,770
then the dataset is prepared,

258
00:13:55,770 --> 00:13:59,640
and after a while it is actually
connecting also to MLflow

259
00:13:59,640 --> 00:14:02,880
because we are going to track
all the these metrics directly

260
00:14:02,880 --> 00:14:04,593
accessible from SageMaker Studio.

261
00:14:06,090 --> 00:14:08,550
Once the job start we will see a new log

262
00:14:08,550 --> 00:14:12,210
that is actually showing
the evolution of the epochs.

263
00:14:12,210 --> 00:14:15,750
So now we want to actually
move directly in the Studio UX

264
00:14:15,750 --> 00:14:19,590
and see directly in MLflow
what is going to happen.

265
00:14:19,590 --> 00:14:22,920
So I already created an
MLflow tracking server.

266
00:14:22,920 --> 00:14:24,212
I can open it.

267
00:14:24,212 --> 00:14:27,523
(audience murmuring)

268
00:14:27,523 --> 00:14:30,060
We will see that there
is a new experiment.

269
00:14:30,060 --> 00:14:32,820
I can analyze directly in the graph

270
00:14:32,820 --> 00:14:35,880
the metrics that I want to
collect for training metrics

271
00:14:35,880 --> 00:14:37,560
as well as system metrics.

272
00:14:37,560 --> 00:14:39,750
In this case, for example,
for the first node,

273
00:14:39,750 --> 00:14:43,890
I'm also analyzing the GPU utilization

274
00:14:43,890 --> 00:14:47,493
for this specific node, how it
is evolving during the time.

275
00:14:48,420 --> 00:14:50,760
But if you want to
actually have more details

276
00:14:50,760 --> 00:14:53,000
around is happening on
the cluster directly

277
00:14:53,000 --> 00:14:54,630
in SageMaker Studio,

278
00:14:54,630 --> 00:14:57,810
I can actually access
informations on the cluster,

279
00:14:57,810 --> 00:14:59,460
on the HyperPod cluster,

280
00:14:59,460 --> 00:15:03,060
and in particular,
under compute, HyperPod,

281
00:15:03,060 --> 00:15:06,060
I can select the cluster
that is available.

282
00:15:06,060 --> 00:15:10,320
I can see the task that are
ongoing such as for example,

283
00:15:10,320 --> 00:15:12,990
here there is a new task that is running

284
00:15:12,990 --> 00:15:17,670
and under metrics I can
actually see the evolution

285
00:15:17,670 --> 00:15:20,670
of the utilization for
example for the GPU, CPU,

286
00:15:20,670 --> 00:15:24,900
the total number of GPUs
available as well as the evolution

287
00:15:24,900 --> 00:15:26,673
and utilization over the time.

288
00:15:29,910 --> 00:15:34,860
Now I'm just waiting for
the completion of the job.

289
00:15:34,860 --> 00:15:38,970
Once it finish I can see that the pod

290
00:15:38,970 --> 00:15:43,950
are completed and the same
thing is actually available also

291
00:15:43,950 --> 00:15:46,140
on SageMaker Studio in the console.

292
00:15:46,140 --> 00:15:48,423
So the job, the task
is actually succeeded.

293
00:15:49,860 --> 00:15:52,050
The model is actually
available directly in Studio.

294
00:15:52,050 --> 00:15:54,840
So I can even use for offline evaluation,

295
00:15:54,840 --> 00:15:56,160
which is actually pretty important

296
00:15:56,160 --> 00:15:58,083
since there is this shared FSx.

297
00:15:59,610 --> 00:16:01,470
Now I want to deploy the model.

298
00:16:01,470 --> 00:16:05,940
So the first thing is to copy
this model into Amazon S3

299
00:16:05,940 --> 00:16:07,140
in order to make sure

300
00:16:07,140 --> 00:16:10,830
that this deployment is access,
this model is accessible

301
00:16:10,830 --> 00:16:13,473
for the deployment component on HyperPod.

302
00:16:15,600 --> 00:16:17,100
This is the manifest file

303
00:16:17,100 --> 00:16:19,740
which is describing the deployment.

304
00:16:19,740 --> 00:16:22,470
So it is saying where is the model located

305
00:16:22,470 --> 00:16:25,560
as well as what is the instance
type that I want to use

306
00:16:25,560 --> 00:16:26,760
and the container image,

307
00:16:26,760 --> 00:16:28,877
also in this case that I
want to use a prebuilt one

308
00:16:28,877 --> 00:16:31,983
that is the LMI container
offered by SageMaker.

309
00:16:34,620 --> 00:16:38,100
In a similar manner,
what I can actually do is

310
00:16:38,100 --> 00:16:42,420
to deploy this component
from SageMaker Studio.

311
00:16:42,420 --> 00:16:43,590
So I clean the space

312
00:16:43,590 --> 00:16:48,180
and I apply these new
deployment.yaml file.

313
00:16:48,180 --> 00:16:50,790
This deployment is actually
creating different components

314
00:16:50,790 --> 00:16:54,420
such as for example, the
inference endpoint configuration,

315
00:16:54,420 --> 00:16:55,950
which is actually describing

316
00:16:55,950 --> 00:16:58,590
what we saw in this manifest file,

317
00:16:58,590 --> 00:16:59,880
but it is really important

318
00:16:59,880 --> 00:17:01,800
because in case you have multiple endpoint

319
00:17:01,800 --> 00:17:03,750
and you want to understand
what is happening,

320
00:17:03,750 --> 00:17:06,120
this is actually the description of

321
00:17:06,120 --> 00:17:09,573
what is the configuration
for the specific endpoint.

322
00:17:12,540 --> 00:17:16,020
As we can see there are the
same kind of informations.

323
00:17:16,020 --> 00:17:19,530
Now I can understand the
pods that are available.

324
00:17:19,530 --> 00:17:20,460
So there is a new pod

325
00:17:20,460 --> 00:17:24,570
that is under deployment,
I can describe it.

326
00:17:24,570 --> 00:17:27,780
And what is happening is that
there are multiple images

327
00:17:27,780 --> 00:17:30,240
that are created in order
to have, for example,

328
00:17:30,240 --> 00:17:32,520
the web server up and running

329
00:17:32,520 --> 00:17:34,353
in order to expose this endpoint,

330
00:17:35,340 --> 00:17:39,120
I can go in Studio under
deployment endpoints,

331
00:17:39,120 --> 00:17:41,790
I will see that there is this new endpoint

332
00:17:41,790 --> 00:17:43,560
that is under creation.

333
00:17:43,560 --> 00:17:45,120
I can refresh the page,

334
00:17:45,120 --> 00:17:47,460
and once it is in service,

335
00:17:47,460 --> 00:17:49,890
I can start interrogating my model.

336
00:17:49,890 --> 00:17:51,660
So here I prepared a Notebook

337
00:17:51,660 --> 00:17:55,860
that is directly using
the SageMaker Python SDK,

338
00:17:55,860 --> 00:17:58,560
I defined the system prompt
in order to ask the model

339
00:17:58,560 --> 00:18:02,220
to use the reasoning part
within this tag's think,

340
00:18:02,220 --> 00:18:04,350
and just to repeat,

341
00:18:04,350 --> 00:18:06,480
this was not actually a reasoning model,

342
00:18:06,480 --> 00:18:08,070
this was an older version,

343
00:18:08,070 --> 00:18:09,510
and once I'm invoking

344
00:18:09,510 --> 00:18:12,540
and the prompt is, "Say
hello to re:Invent 2025,"

345
00:18:12,540 --> 00:18:15,150
we'll see that the model is now improved

346
00:18:15,150 --> 00:18:18,690
because it is actually now
following what I want to do.

347
00:18:18,690 --> 00:18:20,100
There are the think tag

348
00:18:20,100 --> 00:18:23,070
as well as we will see
the generated answer

349
00:18:23,070 --> 00:18:26,463
that is actually reflecting
what is my specific task.

350
00:18:28,140 --> 00:18:32,730
So as probably you understood
there are different phases

351
00:18:32,730 --> 00:18:35,850
that are really, really
important for a machine learning

352
00:18:35,850 --> 00:18:38,160
as well as for a generative AI project.

353
00:18:38,160 --> 00:18:41,040
The first step is the
data preparation piece.

354
00:18:41,040 --> 00:18:42,600
For the data preparation piece,

355
00:18:42,600 --> 00:18:45,150
there are multiple ways in
order to prepare our data

356
00:18:45,150 --> 00:18:46,050
for machine learning.

357
00:18:46,050 --> 00:18:47,430
We saw in the previous demo

358
00:18:47,430 --> 00:18:49,817
that we used an interactive
approach directly

359
00:18:49,817 --> 00:18:51,570
in the JupyterLab Notebook.

360
00:18:51,570 --> 00:18:54,330
So as a user I can connect
to the IDE of choice

361
00:18:54,330 --> 00:18:57,270
in SageMaker Studio,
JupyterLab or Code Editor,

362
00:18:57,270 --> 00:18:59,550
install whatever libraries I want

363
00:18:59,550 --> 00:19:02,760
and start the preparation
piece with also the possibility

364
00:19:02,760 --> 00:19:05,940
to select multiple instance
types in case for example,

365
00:19:05,940 --> 00:19:08,460
I need more computational power,

366
00:19:08,460 --> 00:19:10,260
but in case I have a lot of data

367
00:19:10,260 --> 00:19:13,170
and I need to probably
distribute this workload

368
00:19:13,170 --> 00:19:14,670
for data preparation,

369
00:19:14,670 --> 00:19:18,930
I can actually connect JupyterLab
Notebooks to EMR servers,

370
00:19:18,930 --> 00:19:21,150
EMR servers that can be self-managed,

371
00:19:21,150 --> 00:19:24,390
so basically you manage the EMR server

372
00:19:24,390 --> 00:19:26,820
as well as Amazon EMR serverless.

373
00:19:26,820 --> 00:19:28,287
So the possibility to actually use

374
00:19:28,287 --> 00:19:31,980
the serverless utilization of Amazon EMR,

375
00:19:31,980 --> 00:19:34,110
and prepare this data
in a distributed manner

376
00:19:34,110 --> 00:19:37,680
directly in the Notebook
by using Spark framework.

377
00:19:37,680 --> 00:19:39,450
But if you want to actually operate

378
00:19:39,450 --> 00:19:41,910
more in a programmatic approach
in an asynchronous manner,

379
00:19:41,910 --> 00:19:45,300
so have this step into a proper pipeline

380
00:19:45,300 --> 00:19:47,700
for machine learning
while also in this case

381
00:19:47,700 --> 00:19:50,190
we can use a service of SageMaker AI

382
00:19:50,190 --> 00:19:54,120
that is Amazon SageMaker
processing in order to create a job

383
00:19:54,120 --> 00:19:57,660
that can use whatever framework
you want, so custom images

384
00:19:57,660 --> 00:20:00,780
as well as prebuilt container
for example, for Spark,

385
00:20:00,780 --> 00:20:05,643
and make this step as
part of your pipeline.

386
00:20:09,750 --> 00:20:11,700
After the data preparation piece,

387
00:20:11,700 --> 00:20:14,850
there is the training
of a machine learning

388
00:20:14,850 --> 00:20:17,520
of a generative origin,
or a generative AI model,

389
00:20:17,520 --> 00:20:21,510
and in particular when in
specifically for generative AI,

390
00:20:21,510 --> 00:20:24,390
when we are talking with
leaders about, you know,

391
00:20:24,390 --> 00:20:27,000
generative AI projects,
there are new challenges

392
00:20:27,000 --> 00:20:30,090
and new questions that may come.

393
00:20:30,090 --> 00:20:32,280
For example, which model should I use?

394
00:20:32,280 --> 00:20:36,300
You probably ask your question
this question by yourself.

395
00:20:36,300 --> 00:20:39,420
Every day, the open-source
world is releasing a new model,

396
00:20:39,420 --> 00:20:41,520
so which is the right
one that I should use

397
00:20:41,520 --> 00:20:44,105
and how I can access this type of model.

398
00:20:44,105 --> 00:20:47,340
The question is how do
I customize my model?

399
00:20:47,340 --> 00:20:49,650
Well, there are different
techniques that you may want

400
00:20:49,650 --> 00:20:52,650
to apply that might be
related to your business goals

401
00:20:52,650 --> 00:20:54,510
that you want to achieve.

402
00:20:54,510 --> 00:20:58,620
And of course, how can I optimize
my training performances?

403
00:20:58,620 --> 00:20:59,850
Now in the next slides,

404
00:20:59,850 --> 00:21:02,673
we are trying to answer
all these questions.

405
00:21:03,900 --> 00:21:05,790
From a technique approach,

406
00:21:05,790 --> 00:21:10,020
there are different type of techniques

407
00:21:10,020 --> 00:21:13,083
that we will want to apply
based on your business goals.

408
00:21:14,070 --> 00:21:15,870
If I want to adapt a model

409
00:21:15,870 --> 00:21:17,910
to a specific industry,

410
00:21:17,910 --> 00:21:19,740
what does it mean, a specific industry?

411
00:21:19,740 --> 00:21:21,030
For example, have the model

412
00:21:21,030 --> 00:21:23,220
that knows specific terminology

413
00:21:23,220 --> 00:21:25,680
of the industry you are
operating such as for example,

414
00:21:25,680 --> 00:21:28,590
healthcare and life
science, financial services.

415
00:21:28,590 --> 00:21:31,800
We are using continued
pre-training activity.

416
00:21:31,800 --> 00:21:33,480
What does it mean in terms of data?

417
00:21:33,480 --> 00:21:38,310
We are taking a text that can
be, for example a PowerPoint

418
00:21:38,310 --> 00:21:40,500
or it can be a Word document.

419
00:21:40,500 --> 00:21:42,930
We translate into a
machine readable format.

420
00:21:42,930 --> 00:21:44,910
So for example, txt file

421
00:21:44,910 --> 00:21:48,450
and we feed the entire
document into the model itself.

422
00:21:48,450 --> 00:21:51,510
So the task for the model
is to predict the next token

423
00:21:51,510 --> 00:21:52,800
that is basically the word

424
00:21:52,800 --> 00:21:55,860
or the combination of the words
based on the entire context

425
00:21:55,860 --> 00:21:57,723
that is passed to the model itself.

426
00:21:58,860 --> 00:22:02,100
In case we want to teach
to the model a new task

427
00:22:02,100 --> 00:22:05,730
or we want to improve the
capability of an existing task,

428
00:22:05,730 --> 00:22:08,820
like for example, as I
did in the previous demo,

429
00:22:08,820 --> 00:22:10,650
well in this case we are actually using

430
00:22:10,650 --> 00:22:12,840
supervised fine-tuning techniques.

431
00:22:12,840 --> 00:22:14,460
We supervised fine-tuning techniques,

432
00:22:14,460 --> 00:22:15,903
we are actually using a dataset

433
00:22:15,903 --> 00:22:19,230
that is named, labeled dataset.

434
00:22:19,230 --> 00:22:21,030
Labeled dataset means that the,

435
00:22:21,030 --> 00:22:24,060
like in this example we
have an array of messages

436
00:22:24,060 --> 00:22:28,050
where there is an alternation
between user and assistant.

437
00:22:28,050 --> 00:22:30,360
So the user is basically the user promise

438
00:22:30,360 --> 00:22:32,850
that you want to feed,
so the user question,

439
00:22:32,850 --> 00:22:35,460
and the assistant is the expected answer

440
00:22:35,460 --> 00:22:37,500
that the model should generate.

441
00:22:37,500 --> 00:22:39,270
So basically we are instructing the model

442
00:22:39,270 --> 00:22:41,160
that given a user prompt,

443
00:22:41,160 --> 00:22:43,563
the expected answer should be that one.

444
00:22:44,610 --> 00:22:46,530
Then there are techniques

445
00:22:46,530 --> 00:22:48,510
that are named post-training techniques

446
00:22:48,510 --> 00:22:51,870
that are standing into the
preference alignment part,

447
00:22:51,870 --> 00:22:53,880
preference alignment
part can be also related

448
00:22:53,880 --> 00:22:55,500
to reinforcement learning.

449
00:22:55,500 --> 00:22:57,840
It means that we want to align a model

450
00:22:57,840 --> 00:23:00,030
in order to be more human-like

451
00:23:00,030 --> 00:23:02,340
in the sense that it is
actually we want the model

452
00:23:02,340 --> 00:23:05,340
that is generating answers
that are more similar

453
00:23:05,340 --> 00:23:07,350
to what a human can generate, you know?

454
00:23:07,350 --> 00:23:09,150
So basically what we are doing,

455
00:23:09,150 --> 00:23:10,740
we are applying different type

456
00:23:10,740 --> 00:23:12,840
of enforcement learning techniques.

457
00:23:12,840 --> 00:23:16,500
That is just an example of
a dataset where basically

458
00:23:16,500 --> 00:23:19,800
in that case we are providing
us input the user prompt

459
00:23:19,800 --> 00:23:21,780
and we are giving to the model an example

460
00:23:21,780 --> 00:23:23,490
of what is a good answer,

461
00:23:23,490 --> 00:23:25,350
but we are also giving
to the model an example

462
00:23:25,350 --> 00:23:26,730
of what is a bad answer.

463
00:23:26,730 --> 00:23:29,460
So in this way, the model is actually,

464
00:23:29,460 --> 00:23:32,493
we learn how to distinguish
between the two.

465
00:23:35,370 --> 00:23:37,860
How to do it on SageMaker AI?

466
00:23:37,860 --> 00:23:40,710
In the previous demo we actually used

467
00:23:40,710 --> 00:23:45,458
a self-managed cluster with
Amazon SageMaker AI report.

468
00:23:45,458 --> 00:23:46,800
With Amazon SageMaker HyperPod

469
00:23:46,800 --> 00:23:49,260
we are talking about a
purpose-built resilient

470
00:23:49,260 --> 00:23:51,120
and self orchestration infrastructure

471
00:23:51,120 --> 00:23:53,850
for maximized resource control.

472
00:23:53,850 --> 00:23:56,130
In this case, we have full
control of the cluster

473
00:23:56,130 --> 00:23:58,830
that can be orchestrated with Amazon EKS

474
00:23:58,830 --> 00:24:00,960
as well as for example with Slurm,

475
00:24:00,960 --> 00:24:03,990
and we have also advanced
capabilities for task governance

476
00:24:03,990 --> 00:24:06,960
and organizations in order
to organize, for example,

477
00:24:06,960 --> 00:24:08,340
the execution of the tasks

478
00:24:08,340 --> 00:24:11,340
based on the policies
that you want to define.

479
00:24:11,340 --> 00:24:13,350
But in case you want to focus more

480
00:24:13,350 --> 00:24:14,970
on the machine learning part,

481
00:24:14,970 --> 00:24:17,970
so you don't want to actually
manage a proper cluster.

482
00:24:17,970 --> 00:24:20,700
Well, in that case we can
use SageMaker Training Jobs.

483
00:24:20,700 --> 00:24:23,070
That is a fully managed
resilient infrastructure

484
00:24:23,070 --> 00:24:26,130
for large-scale and
cost-effective training.

485
00:24:26,130 --> 00:24:27,480
In this case, with Training Jobs,

486
00:24:27,480 --> 00:24:29,520
we actually prototype our code.

487
00:24:29,520 --> 00:24:31,560
Once we want to submit the job,

488
00:24:31,560 --> 00:24:33,390
we are actually invoking an API,

489
00:24:33,390 --> 00:24:36,270
an API by specifying what
is the instance type,

490
00:24:36,270 --> 00:24:37,710
the number of instances,

491
00:24:37,710 --> 00:24:40,140
and SageMaker AI is
taking care of everything.

492
00:24:40,140 --> 00:24:43,170
It's turning up the infrastructure,

493
00:24:43,170 --> 00:24:44,490
executes the job.

494
00:24:44,490 --> 00:24:46,290
Once the job is finished,

495
00:24:46,290 --> 00:24:48,330
the infrastructure is turning down.

496
00:24:48,330 --> 00:24:50,850
So you are paying, in
this on-demand approach,

497
00:24:50,850 --> 00:24:54,510
only for the amount of time
that the job is under execution.

498
00:24:54,510 --> 00:24:56,580
But of course if you have workloads

499
00:24:56,580 --> 00:24:59,220
that are more predictable
so you know when to use

500
00:24:59,220 --> 00:25:00,900
and how many instances to use,

501
00:25:00,900 --> 00:25:02,730
there are options such as for example,

502
00:25:02,730 --> 00:25:04,710
flexible training plan or spot instances

503
00:25:04,710 --> 00:25:07,353
where you can reserve
some capacity upfront.

504
00:25:09,270 --> 00:25:11,700
The observability part is
actually super important

505
00:25:11,700 --> 00:25:13,740
that we saw in the previous demo

506
00:25:13,740 --> 00:25:15,207
and in this case in SageMaker AI

507
00:25:15,207 --> 00:25:17,040
and in particular in SageMaker Studio

508
00:25:17,040 --> 00:25:19,890
we have actually a lot of
options in order to do that.

509
00:25:19,890 --> 00:25:22,380
For example, we saw that
with SageMaker HyperPod,

510
00:25:22,380 --> 00:25:26,070
we can use task governance
capabilities within Studio

511
00:25:26,070 --> 00:25:28,410
in order to analyze what are the tasks

512
00:25:28,410 --> 00:25:30,120
that are under execution.

513
00:25:30,120 --> 00:25:32,130
We can also define custom policies

514
00:25:32,130 --> 00:25:34,830
in order to prioritize specific customers,

515
00:25:34,830 --> 00:25:38,580
for example, specific task pair, I dunno,

516
00:25:38,580 --> 00:25:41,100
based on the team as well
as based on the priority

517
00:25:41,100 --> 00:25:43,170
that we want to give to the workload,

518
00:25:43,170 --> 00:25:45,360
in a similar manner we can
actually do the same thing

519
00:25:45,360 --> 00:25:46,200
with Training Jobs.

520
00:25:46,200 --> 00:25:49,470
We can actually analyze the
metrics that are related

521
00:25:49,470 --> 00:25:52,440
to the cluster that is used
for the training job itself

522
00:25:52,440 --> 00:25:55,680
and orchestrate the execution
of these jobs by using,

523
00:25:55,680 --> 00:25:58,620
for example, the connection
between Training Jobs

524
00:25:58,620 --> 00:26:00,123
and AWS Batch.

525
00:26:02,760 --> 00:26:05,160
Regarding which model should I use?

526
00:26:05,160 --> 00:26:08,970
Well last year during the re:Invent 2024,

527
00:26:08,970 --> 00:26:10,500
we released this capability

528
00:26:10,500 --> 00:26:13,950
that is named Amazon
SageMaker HyperPod recipes,

529
00:26:13,950 --> 00:26:17,340
which is a curated,
ready-to-use set of parameters,

530
00:26:17,340 --> 00:26:20,640
so set of recipes for open-source models

531
00:26:20,640 --> 00:26:24,150
such as for example, DeepSeek,
Meta, Llama, Mistral,

532
00:26:24,150 --> 00:26:26,160
as well as first-party model

533
00:26:26,160 --> 00:26:28,770
since starting from the New
York Summit of this year,

534
00:26:28,770 --> 00:26:31,920
we have also the possibility
to customize Amazon Nova models

535
00:26:31,920 --> 00:26:33,990
with SageMaker AI.

536
00:26:33,990 --> 00:26:35,460
But what is a recipe?

537
00:26:35,460 --> 00:26:37,260
It's a collection of parameters

538
00:26:37,260 --> 00:26:40,950
that are pre-configured and
related to the model itself.

539
00:26:40,950 --> 00:26:45,750
So we basically have to just
the recipe that we want to use

540
00:26:45,750 --> 00:26:48,930
without actually writing any kind of code

541
00:26:48,930 --> 00:26:51,870
and SageMaker AI takes
care of the execution

542
00:26:51,870 --> 00:26:53,970
of the workload itself.

543
00:26:53,970 --> 00:26:58,440
The recipe is available for
both SageMaker Training Jobs

544
00:26:58,440 --> 00:27:00,900
as well as for SageMaker HyperPod.

545
00:27:00,900 --> 00:27:02,310
In the training job part,

546
00:27:02,310 --> 00:27:04,710
we can use the SageMaker Python SDK

547
00:27:04,710 --> 00:27:08,040
just to specify what is the
recipe that we want to use

548
00:27:08,040 --> 00:27:09,810
and in a similar manner with HyperPod,

549
00:27:09,810 --> 00:27:14,223
we can use the HyperPod CLI
in order to submit those jobs.

550
00:27:17,190 --> 00:27:21,540
Regarding the deployment
part, also in this case,

551
00:27:21,540 --> 00:27:23,430
we have actually two options

552
00:27:23,430 --> 00:27:25,980
in order to deploy those models.

553
00:27:25,980 --> 00:27:29,280
The first one is to use
SageMaker Managed Inference.

554
00:27:29,280 --> 00:27:33,930
So if we want to stay in the
area of fully managed approach,

555
00:27:33,930 --> 00:27:35,790
what is SageMaker Managed Inference?

556
00:27:35,790 --> 00:27:37,050
It is offering the possibility

557
00:27:37,050 --> 00:27:39,720
to use fully managed real-time endpoints

558
00:27:39,720 --> 00:27:40,950
with automatic scaling.

559
00:27:40,950 --> 00:27:41,880
So what does it mean?

560
00:27:41,880 --> 00:27:44,610
We can actually define
auto scaling policies

561
00:27:44,610 --> 00:27:45,630
in order to scale up

562
00:27:45,630 --> 00:27:49,710
and scale down based
on the spikes that can,

563
00:27:49,710 --> 00:27:51,120
you can receive during the day

564
00:27:51,120 --> 00:27:53,790
based on the request of the model itself.

565
00:27:53,790 --> 00:27:56,670
But if we want to maximize
utilization of the cluster

566
00:27:56,670 --> 00:27:59,040
that we used also for
the training workload,

567
00:27:59,040 --> 00:28:01,290
while also in that case we can reuse

568
00:28:01,290 --> 00:28:04,260
the same HyperPod cluster
as we did in the demo

569
00:28:04,260 --> 00:28:08,133
and deploy those models in
the same HyperPod cluster.

570
00:28:09,000 --> 00:28:11,940
And in particular, in order
to accelerate the deployment

571
00:28:11,940 --> 00:28:16,290
of open models since New
York Summit of this year,

572
00:28:16,290 --> 00:28:17,820
there is also the possibility

573
00:28:17,820 --> 00:28:21,120
to actually deploy directly
from SageMaker Studio

574
00:28:21,120 --> 00:28:23,040
in literally few clicks,

575
00:28:23,040 --> 00:28:25,860
open-source models directly
from the Studio interface

576
00:28:25,860 --> 00:28:28,800
by selecting the HyperPod
cluster that might be up

577
00:28:28,800 --> 00:28:31,203
and running within your account.

578
00:28:33,210 --> 00:28:36,060
Now I am calling back Khushboo

579
00:28:36,060 --> 00:28:39,453
in order to discuss about the
recent launches that we did.

580
00:28:41,610 --> 00:28:42,560
- Thank you, Bruno.

581
00:28:43,440 --> 00:28:45,660
Alright, so let's talk about some

582
00:28:45,660 --> 00:28:49,113
of the recent launches in
Amazon SageMaker Studio.

583
00:28:51,720 --> 00:28:54,510
Before we start, I just
wanted to reiterate

584
00:28:54,510 --> 00:28:58,773
that Amazon SageMaker Studio
provides you a suite of IDEs.

585
00:28:59,610 --> 00:29:03,330
AI developers can pick
an IDE of their choice.

586
00:29:03,330 --> 00:29:04,930
These are all fully managed IDEs

587
00:29:05,910 --> 00:29:09,540
and the three key IDEs that
we provide are JupyterLab,

588
00:29:09,540 --> 00:29:13,440
it's a web-based IDE for
Notebooks, code and data.

589
00:29:13,440 --> 00:29:14,280
Its flexible

590
00:29:14,280 --> 00:29:16,860
and extensible interface allows you

591
00:29:16,860 --> 00:29:19,350
to easily configure ML workflows.

592
00:29:19,350 --> 00:29:20,940
- [Speaker] I think
you have a lot of time.

593
00:29:20,940 --> 00:29:24,813
- Code Editor based on
open-source code OSS,

594
00:29:25,710 --> 00:29:28,530
boost productivity with
its familiar shortcuts,

595
00:29:28,530 --> 00:29:32,010
terminal debugger and refactoring tools.

596
00:29:32,010 --> 00:29:33,653
And RStudio,

597
00:29:33,653 --> 00:29:38,040
RStudio is a fully managed
IDE for R with a console,

598
00:29:38,040 --> 00:29:40,020
a syntax highlighting editor

599
00:29:40,020 --> 00:29:43,140
that supports direct code execution

600
00:29:43,140 --> 00:29:45,990
and tools for plotting, history, debugging

601
00:29:45,990 --> 00:29:47,673
and workspace management.

602
00:29:49,740 --> 00:29:52,083
Remote access from local IDE.

603
00:29:53,430 --> 00:29:56,160
For AI developers who prefer to operate

604
00:29:56,160 --> 00:29:58,800
and develop code in their local IDE,

605
00:29:58,800 --> 00:30:02,043
such as the local Visual Studio Code IDE,

606
00:30:03,090 --> 00:30:05,280
but at the same time wanna benefit

607
00:30:05,280 --> 00:30:09,450
from the compute infrastructure
defined for cloud IDE

608
00:30:09,450 --> 00:30:11,730
on SageMaker Studio,

609
00:30:11,730 --> 00:30:15,090
we recently released the capability

610
00:30:15,090 --> 00:30:17,790
where you can connect your local IDE

611
00:30:17,790 --> 00:30:20,343
from your SageMaker Studio Spaces,

612
00:30:21,270 --> 00:30:23,070
with remote connection to IDE,

613
00:30:23,070 --> 00:30:27,280
we leverage SageMaker AI's
powerful compute resources

614
00:30:28,590 --> 00:30:33,480
and your data to analyze
process data, develop AI models,

615
00:30:33,480 --> 00:30:36,360
while maintaining all the
existing SageMaker Studio

616
00:30:36,360 --> 00:30:38,040
security controls

617
00:30:38,040 --> 00:30:41,010
and permissions without any additional

618
00:30:41,010 --> 00:30:42,693
configurations required.

619
00:30:43,560 --> 00:30:48,560
With AWS toolkit integration,
your Visual Studio Code IDE

620
00:30:48,600 --> 00:30:53,373
can show all your SageMaker
Studio Spaces in the left nav.

621
00:30:54,600 --> 00:30:57,360
As you see in the GIF here,

622
00:30:57,360 --> 00:31:00,210
all you need to do is
enable remote access,

623
00:31:00,210 --> 00:31:02,910
open it in Visual Studio,

624
00:31:02,910 --> 00:31:05,853
and there you go, you have
your spaces right there.

625
00:31:08,970 --> 00:31:11,490
Trusted identity propagation.

626
00:31:11,490 --> 00:31:13,890
Trusted identity propagation is a feature

627
00:31:13,890 --> 00:31:18,877
of AWS identity center where you can send

628
00:31:19,920 --> 00:31:21,990
the end user's identity,

629
00:31:21,990 --> 00:31:26,790
which is the human user identity
across all the workflows

630
00:31:26,790 --> 00:31:30,630
within Amazon AWS services.

631
00:31:30,630 --> 00:31:35,630
So let's say a user logs in
using AWS Identity Center

632
00:31:36,210 --> 00:31:38,590
into their Amazon SageMaker Studio

633
00:31:39,450 --> 00:31:42,960
and open SageMaker Studio
Notebooks to connect

634
00:31:42,960 --> 00:31:47,310
to downstream AWS services
such as Redshift, EMR,

635
00:31:47,310 --> 00:31:49,170
Lake Formation,

636
00:31:49,170 --> 00:31:52,499
or ML services such as SageMaker training,

637
00:31:52,499 --> 00:31:55,443
SageMaker processing
or SageMaker Inference.

638
00:31:57,060 --> 00:31:58,920
Their human user identity

639
00:31:58,920 --> 00:32:03,450
or the corporate identity defined
in IDC will get propagated

640
00:32:03,450 --> 00:32:05,880
through all these workflows

641
00:32:05,880 --> 00:32:09,843
and they will get logged
into CloudTrail logs.

642
00:32:10,950 --> 00:32:13,590
So as an admin you can now go ahead

643
00:32:13,590 --> 00:32:17,520
and audit who accessed which resources.

644
00:32:17,520 --> 00:32:18,540
Not only that,

645
00:32:18,540 --> 00:32:22,200
if you want to apply fine
grain access controls

646
00:32:22,200 --> 00:32:26,610
using S3 access grants Lake
Formation access grants,

647
00:32:26,610 --> 00:32:30,120
or Redshift data APIs, you can do so.

648
00:32:30,120 --> 00:32:33,780
You can say that Khushboo
as a user should have access

649
00:32:33,780 --> 00:32:36,990
to these S3 buckets only

650
00:32:36,990 --> 00:32:39,750
and SageMaker Studio will honor that

651
00:32:39,750 --> 00:32:41,523
thanks to the TIP integration.

652
00:32:44,640 --> 00:32:48,123
Amazon Nova customization
on SageMaker Studio.

653
00:32:49,230 --> 00:32:52,230
Early this year we released the capability

654
00:32:52,230 --> 00:32:56,810
where users can customize
Amazon Nova models

655
00:32:56,810 --> 00:32:58,413
on SageMaker AI.

656
00:32:59,280 --> 00:33:02,070
Everything from SageMaker Studio.

657
00:33:02,070 --> 00:33:06,000
Users can explore multiple
Amazon Nova models

658
00:33:06,000 --> 00:33:08,973
such as Nova Micro, Light and Pro.

659
00:33:09,840 --> 00:33:13,680
They can select their preferred
customization techniques

660
00:33:13,680 --> 00:33:17,880
for their use case such
as supervised fine-tuning

661
00:33:17,880 --> 00:33:20,343
or reinforcement learning techniques.

662
00:33:21,480 --> 00:33:24,510
They can open a pre-built Notebook

663
00:33:24,510 --> 00:33:27,340
to start customizing their workloads

664
00:33:28,440 --> 00:33:33,440
on SageMaker Training Jobs
or SageMaker HyperPod.

665
00:33:37,170 --> 00:33:40,110
As you see, this is an example

666
00:33:40,110 --> 00:33:42,160
where you can open a sample Notebook

667
00:33:43,020 --> 00:33:44,580
and it drops you into a Notebook

668
00:33:44,580 --> 00:33:46,473
where you can do the customization.

669
00:33:50,674 --> 00:33:54,810
IDEs and Notebooks on SageMaker HyperPod.

670
00:33:54,810 --> 00:33:56,910
You can now maximize your CPU

671
00:33:56,910 --> 00:34:01,910
and GPU investments across
each stage of ML lifecycle

672
00:34:02,430 --> 00:34:05,130
by running IDEs and Notebooks

673
00:34:05,130 --> 00:34:08,433
also on Amazon SageMaker
HyperPod clusters.

674
00:34:09,930 --> 00:34:13,080
Last week we launched a new capability

675
00:34:13,080 --> 00:34:16,440
which lets you install a new add-on

676
00:34:16,440 --> 00:34:19,140
on your HyperPod EKS clusters.

677
00:34:19,140 --> 00:34:23,910
The name of the add-on is
Amazon SageMaker Spaces.

678
00:34:23,910 --> 00:34:26,730
What is this add-on? What is a space?

679
00:34:26,730 --> 00:34:30,190
A space can be thought of
as a self-contained entity

680
00:34:31,470 --> 00:34:35,610
where you can define all the
configurations such as image,

681
00:34:35,610 --> 00:34:40,610
computer sources, local
storage, any persistent volumes,

682
00:34:40,770 --> 00:34:44,160
and even more configurations
that are required

683
00:34:44,160 --> 00:34:46,623
by AI developers to run their IDEs.

684
00:34:48,900 --> 00:34:51,750
For AI developers,

685
00:34:51,750 --> 00:34:54,783
this means accelerating
gen AI development.

686
00:34:55,680 --> 00:34:58,950
Now AI developers can
launch their JupyterLab

687
00:34:58,950 --> 00:35:03,210
as well as Code Editor
IDEs on web browsers

688
00:35:03,210 --> 00:35:07,320
or connect their local IDE
such as local VS Code IDE

689
00:35:07,320 --> 00:35:11,403
to run Notebooks on the
HyperPod compute cluster,

690
00:35:13,050 --> 00:35:15,000
which means they can now run their IDEs

691
00:35:15,000 --> 00:35:17,520
on the same persistent clusters

692
00:35:17,520 --> 00:35:18,840
where they have their training

693
00:35:18,840 --> 00:35:22,110
and inference workloads
using their familiar tools

694
00:35:22,110 --> 00:35:24,450
such as HyperPod CLI

695
00:35:24,450 --> 00:35:27,840
or sharing data across
interactive AI workloads

696
00:35:27,840 --> 00:35:31,860
and Training Jobs using the
already-mounted file system

697
00:35:31,860 --> 00:35:33,993
such as FSx or EFS,

698
00:35:35,070 --> 00:35:38,130
it provides faster ID startup latencies

699
00:35:38,130 --> 00:35:41,073
with image caching and
supports idle shutdown.

700
00:35:42,330 --> 00:35:44,220
Not only this,

701
00:35:44,220 --> 00:35:47,583
AI developers can now
maximize cluster utilization.

702
00:35:48,660 --> 00:35:51,903
We support MIG profiles for GPU sharing.

703
00:35:53,130 --> 00:35:57,510
So now AI developers can
bin pack multiple spaces

704
00:35:57,510 --> 00:36:00,240
onto a single instance.

705
00:36:00,240 --> 00:36:01,380
But not only that,

706
00:36:01,380 --> 00:36:03,720
they can also bin pack
their multiple spaces

707
00:36:03,720 --> 00:36:05,850
on a single GPU.

708
00:36:05,850 --> 00:36:09,063
Thus they can run IDEs on
fractional GPU as well.

709
00:36:10,890 --> 00:36:13,020
For the admin persona,

710
00:36:13,020 --> 00:36:16,923
this means unified
governance and observability.

711
00:36:18,060 --> 00:36:20,310
Administrators can now leverage

712
00:36:20,310 --> 00:36:24,000
SageMaker HyperPod's task governance

713
00:36:24,000 --> 00:36:26,700
to efficiently utilize GPU investments

714
00:36:26,700 --> 00:36:29,460
across diverse workloads

715
00:36:29,460 --> 00:36:32,333
and they can view memory, CPU,

716
00:36:32,333 --> 00:36:37,320
and GPU usage across diverse workloads

717
00:36:37,320 --> 00:36:42,320
and reprioritize using SageMaker
HyperPod task awareness

718
00:36:42,840 --> 00:36:45,693
as well as HyperPod observability.

719
00:36:48,240 --> 00:36:50,650
Now we are going to see a brief demo

720
00:36:51,930 --> 00:36:56,930
for the capability for Amazon
SageMaker Spaces add-on.

721
00:36:57,180 --> 00:36:59,070
But before we get into the demo,

722
00:36:59,070 --> 00:37:01,170
I wanted to quickly call out a few things

723
00:37:01,170 --> 00:37:02,790
that we would be seeing today.

724
00:37:02,790 --> 00:37:05,250
First we will see how can you go ahead

725
00:37:05,250 --> 00:37:08,550
and install Amazon SageMaker Spaces add-on

726
00:37:08,550 --> 00:37:11,280
on an existing HyperPod cluster.

727
00:37:11,280 --> 00:37:14,905
You can also install it
on a new HyperPod cluster.

728
00:37:14,905 --> 00:37:16,230
There are two ways to install it.

729
00:37:16,230 --> 00:37:20,370
Quick install comes
with automatic defaults

730
00:37:20,370 --> 00:37:22,680
and custom install is
where you can provide

731
00:37:22,680 --> 00:37:26,160
your own custom install also
comes with automatic defaults

732
00:37:26,160 --> 00:37:28,050
but you can override those defaults

733
00:37:28,050 --> 00:37:30,213
with your own configurations.

734
00:37:31,380 --> 00:37:33,360
We discussed what a space already is,

735
00:37:33,360 --> 00:37:35,460
it's a self-contained entity

736
00:37:35,460 --> 00:37:38,460
where you can define different specs

737
00:37:38,460 --> 00:37:40,510
and configurations that will be used

738
00:37:41,550 --> 00:37:43,263
to run your IDE on HyperPod.

739
00:37:44,970 --> 00:37:49,050
I also wanna briefly talk about templates.

740
00:37:49,050 --> 00:37:52,830
Templates are a mechanism
that you as an admin can use

741
00:37:52,830 --> 00:37:54,810
to create default templates

742
00:37:54,810 --> 00:37:57,480
for defining some default configurations

743
00:37:57,480 --> 00:38:00,360
for your Amazon SageMaker Spaces.

744
00:38:00,360 --> 00:38:03,390
So let's say if you want teams to use

745
00:38:03,390 --> 00:38:06,240
a specific set of custom images

746
00:38:06,240 --> 00:38:09,450
or if you want your teams to
bring in their own images.

747
00:38:09,450 --> 00:38:13,080
If you wanna define
already-mounted file systems,

748
00:38:13,080 --> 00:38:15,120
local storage, you wanna define the range

749
00:38:15,120 --> 00:38:18,120
of the local storage minimum to maximum,

750
00:38:18,120 --> 00:38:21,060
you wanna define compute resources,

751
00:38:21,060 --> 00:38:24,930
default resources required to run a space.

752
00:38:24,930 --> 00:38:27,450
Everything could be done using templates.

753
00:38:27,450 --> 00:38:30,420
We provide you two default
templates, one for JupyterLab

754
00:38:30,420 --> 00:38:31,920
and one for Code Editor.

755
00:38:31,920 --> 00:38:34,740
But you can go ahead
create your own template

756
00:38:34,740 --> 00:38:36,333
and then mark it as a default.

757
00:38:37,710 --> 00:38:40,420
And the last thing I wanted to call out is

758
00:38:41,400 --> 00:38:44,580
that your AI developers
can access their spaces

759
00:38:44,580 --> 00:38:47,010
in a visual interface as well.

760
00:38:47,010 --> 00:38:49,860
How can they do it? There are
three ways they can do it.

761
00:38:49,860 --> 00:38:53,010
One is using web browser.

762
00:38:53,010 --> 00:38:57,510
In this web browser, the
admins provide a custom DNS

763
00:38:57,510 --> 00:39:00,750
and all that the AI developers have to do

764
00:39:00,750 --> 00:39:02,850
is run a HyperPod CLI command

765
00:39:02,850 --> 00:39:04,863
to generate a web UI URL.

766
00:39:05,940 --> 00:39:10,940
If they click on it opens their
JupyterLab plain vanilla IDE

767
00:39:12,180 --> 00:39:15,480
on their web browsers on the custom DNS

768
00:39:15,480 --> 00:39:17,640
provided by the admins.

769
00:39:17,640 --> 00:39:19,590
Second way to access their spaces is

770
00:39:19,590 --> 00:39:21,873
through their local IDE VS Code.

771
00:39:23,940 --> 00:39:26,250
Again, as an admin you
can choose to define

772
00:39:26,250 --> 00:39:28,350
that you want to enable remote connection.

773
00:39:28,350 --> 00:39:31,680
If you do, then all that
your AI developers have

774
00:39:31,680 --> 00:39:34,710
to do is run a HyperPod CLI command

775
00:39:34,710 --> 00:39:39,710
to get the URL or a string
that you can then click

776
00:39:40,470 --> 00:39:43,540
and it directly prompts you
to open your VS Code IDE

777
00:39:45,120 --> 00:39:47,973
and opens your space
directly in the VS Code IDE.

778
00:39:48,900 --> 00:39:49,733
Let's say if you,

779
00:39:49,733 --> 00:39:52,260
if your AI developers
don't wanna use web browser

780
00:39:52,260 --> 00:39:57,180
using custom DNS or remote
connection to their local IDE,

781
00:39:57,180 --> 00:39:58,950
they can still access their spaces

782
00:39:58,950 --> 00:40:02,400
using Kubernetes local
port forwarding feature,

783
00:40:02,400 --> 00:40:07,350
which will open up your
plain JupyterLab IDE

784
00:40:07,350 --> 00:40:09,150
on your web browser.

785
00:40:09,150 --> 00:40:12,483
So let's get into the demo.

786
00:40:16,950 --> 00:40:20,100
So we click on the custom install.

787
00:40:20,100 --> 00:40:25,020
Here you see options to create
remote access configuration,

788
00:40:25,020 --> 00:40:27,030
which is connecting to local IDE

789
00:40:27,030 --> 00:40:31,590
as well as web browser
access configuration

790
00:40:31,590 --> 00:40:36,590
where you provide a custom
DNS and a valid certificate.

791
00:40:43,950 --> 00:40:47,327
You provide your KMS key and hit install.

792
00:40:54,450 --> 00:40:57,330
Awesome, so your add-on is now installed

793
00:40:57,330 --> 00:40:59,313
on the ML training cluster.

794
00:41:09,810 --> 00:41:12,570
Now you can see two default templates

795
00:41:12,570 --> 00:41:14,313
that we provide by default,

796
00:41:16,260 --> 00:41:18,450
we will now create a default,

797
00:41:18,450 --> 00:41:20,913
another template for your end users.

798
00:41:32,640 --> 00:41:35,100
You can the application type,

799
00:41:35,100 --> 00:41:38,670
if you have task awareness
enabled for this cluster,

800
00:41:38,670 --> 00:41:43,500
you can select a preexisting
task awareness priority label

801
00:41:43,500 --> 00:41:46,953
to assign to all these spaces
created using this template.

802
00:41:48,450 --> 00:41:50,703
Here is where you provide your images.

803
00:41:52,050 --> 00:41:53,490
Please remember that you can use

804
00:41:53,490 --> 00:41:57,300
SageMaker distribution images, SMD images,

805
00:41:57,300 --> 00:42:00,450
as well as provide ECR repos.

806
00:42:00,450 --> 00:42:03,183
Your data scientists can
bring in their own images.

807
00:42:04,560 --> 00:42:08,250
This is where we define the storage.

808
00:42:08,250 --> 00:42:12,393
The storage is we have built
storage using local EBS.

809
00:42:15,000 --> 00:42:18,360
You define your compute, GPU, CPU,

810
00:42:18,360 --> 00:42:20,073
as well as memory per space.

811
00:42:21,120 --> 00:42:26,010
And finally you define a
lifecycle configuration script

812
00:42:26,010 --> 00:42:29,640
that you want to use to run your spaces

813
00:42:29,640 --> 00:42:32,913
so you can install any custom
package that you need to,

814
00:42:37,620 --> 00:42:39,693
and you have your template ready.

815
00:42:41,760 --> 00:42:43,710
Now you can go ahead

816
00:42:43,710 --> 00:42:46,443
and make this new template
as your default template.

817
00:42:50,670 --> 00:42:54,510
Now let's see how you can
set up namespace as well.

818
00:42:54,510 --> 00:42:59,510
Here what we will see is how
to create a new namespace

819
00:42:59,520 --> 00:43:01,740
or use an existing namespace,

820
00:43:01,740 --> 00:43:03,170
for existing namespaces

821
00:43:03,170 --> 00:43:05,620
we will also show you task
governance namespaces.

822
00:43:17,400 --> 00:43:18,600
You can then go ahead

823
00:43:18,600 --> 00:43:22,080
and create EKS pod identity associations,

824
00:43:22,080 --> 00:43:23,910
which are the service accounts

825
00:43:23,910 --> 00:43:27,660
with which your space will run these.

826
00:43:27,660 --> 00:43:29,280
Once you create the service accounts,

827
00:43:29,280 --> 00:43:32,973
you can assign a runtime
role to these as well.

828
00:43:33,972 --> 00:43:37,639
(audience softly murmuring)

829
00:43:49,380 --> 00:43:52,560
Now that you have defined
some service accounts

830
00:43:52,560 --> 00:43:56,253
and their runtime rules, you
can go ahead and create users.

831
00:43:57,870 --> 00:44:00,210
You can create users and groups.

832
00:44:00,210 --> 00:44:04,050
When you do so, you can assign them

833
00:44:04,050 --> 00:44:06,180
one or more service accounts,

834
00:44:06,180 --> 00:44:10,440
which means when a user is running a space

835
00:44:10,440 --> 00:44:11,910
in a given name space,

836
00:44:11,910 --> 00:44:15,690
they will be able to use from
a set of service accounts

837
00:44:15,690 --> 00:44:19,053
to run their pods or
spaces in this scenario.

838
00:44:23,730 --> 00:44:24,720
Awesome!

839
00:44:24,720 --> 00:44:26,970
You can also see a list of spaces

840
00:44:26,970 --> 00:44:30,270
that the users are running
on the HyperPod cluster.

841
00:44:30,270 --> 00:44:35,010
As an admin you have the
controls to manage these spaces.

842
00:44:35,010 --> 00:44:38,880
Let's say you wanna stop
a space, restart a space,

843
00:44:38,880 --> 00:44:40,920
or if the user leaves the company

844
00:44:40,920 --> 00:44:43,020
and you want to delete a space

845
00:44:43,020 --> 00:44:45,420
from consuming any further resources,

846
00:44:45,420 --> 00:44:48,333
you can take those actions
directly from the console.

847
00:44:52,950 --> 00:44:53,850
Now we will go ahead

848
00:44:53,850 --> 00:44:56,310
and see how the data scientist persona

849
00:44:56,310 --> 00:44:58,500
will create spaces.

850
00:44:58,500 --> 00:45:01,990
What we see here is the
data scientist persona

851
00:45:03,870 --> 00:45:05,260
running the help command

852
00:45:06,390 --> 00:45:08,400
and finally creating a space

853
00:45:08,400 --> 00:45:10,300
with the name data science space

854
00:45:12,060 --> 00:45:14,400
and using default configurations.

855
00:45:14,400 --> 00:45:17,370
If you list, you see the space is running

856
00:45:17,370 --> 00:45:20,520
and is the status available is true.

857
00:45:20,520 --> 00:45:21,870
If you describe these space,

858
00:45:21,870 --> 00:45:24,903
you can see all the configurations
or specs of the space.

859
00:45:26,040 --> 00:45:27,700
Key thing to note is that

860
00:45:28,920 --> 00:45:31,320
data scientists can use
the default configurations,

861
00:45:31,320 --> 00:45:34,803
but they can also override or
customize these spaces back.

862
00:45:35,640 --> 00:45:37,180
If you stop the space

863
00:45:38,310 --> 00:45:40,200
and list the space, you will finally see

864
00:45:40,200 --> 00:45:43,743
that the status has changed
from available to false.

865
00:45:50,100 --> 00:45:52,353
All right, let's start the space again,

866
00:45:58,800 --> 00:46:00,850
and we see these status changing to true.

867
00:46:07,620 --> 00:46:08,910
As we said, talked about,

868
00:46:08,910 --> 00:46:11,520
data scientists can update the space too.

869
00:46:11,520 --> 00:46:14,520
In this scenario, we are
updating the memory computer

870
00:46:14,520 --> 00:46:16,623
sources as well as the display name.

871
00:46:17,520 --> 00:46:19,080
So once the update is done

872
00:46:19,080 --> 00:46:22,650
and you run a describe command,
you'll see that these spec

873
00:46:22,650 --> 00:46:24,693
of the space have been updated.

874
00:46:28,890 --> 00:46:31,350
You can get logs for your space as well

875
00:46:31,350 --> 00:46:33,960
if you want to deep dive
into a specific scenario

876
00:46:33,960 --> 00:46:35,043
or troubleshoot.

877
00:46:45,930 --> 00:46:48,090
Finally, you can access this space

878
00:46:48,090 --> 00:46:51,120
as we talked about using a web UI URL.

879
00:46:51,120 --> 00:46:55,950
If you click on this URL, it
directly opens up your space

880
00:46:55,950 --> 00:46:59,580
as a open, as a plain vanilla IDE

881
00:46:59,580 --> 00:47:01,143
on your local web browser.

882
00:47:02,190 --> 00:47:04,413
You can run delete command as well.

883
00:47:05,640 --> 00:47:07,830
And then if you describe,
sorry, if you list it,

884
00:47:07,830 --> 00:47:10,083
you will see that the space doesn't exist.

885
00:47:14,880 --> 00:47:18,030
Alright, with this we conclude the demo.

886
00:47:18,030 --> 00:47:21,660
One thing that we haven't
covered here in the demo is

887
00:47:21,660 --> 00:47:26,540
how you can generate the web UI URL

888
00:47:26,540 --> 00:47:31,540
or the VS Code connection
URL that can directly open.

889
00:47:32,730 --> 00:47:34,350
So if you click on those URLs,

890
00:47:34,350 --> 00:47:38,100
it'll prompt you to open your VS Code IDE

891
00:47:38,100 --> 00:47:43,100
or open up your IDE space
in the local web browser.

892
00:47:45,810 --> 00:47:46,650
Awesome!

893
00:47:46,650 --> 00:47:49,800
With that we will talk
about how SageMaker AI

894
00:47:49,800 --> 00:47:53,520
has helped KOHO accelerate
their ML journey.

895
00:47:53,520 --> 00:47:57,486
And I'll give the mic
to Mani then, thank you.

896
00:47:57,486 --> 00:48:01,236
(audience softly chattering)

897
00:48:04,440 --> 00:48:07,653
- Hi everyone, can you hear me okay? Cool.

898
00:48:08,610 --> 00:48:09,893
Thank you, Khushboo, and thank you, Bruno.

899
00:48:11,760 --> 00:48:13,925
Quick intro about me, Manikandan,

900
00:48:13,925 --> 00:48:17,730
I'm a senior staff architect at KOHO.

901
00:48:17,730 --> 00:48:21,540
I lead the architecture, infra engineering

902
00:48:21,540 --> 00:48:26,540
and operations for our data
and ML platform, all that.

903
00:48:26,550 --> 00:48:29,133
Plus I'm also a busy dad
to a 6-year-old daughter.

904
00:48:31,560 --> 00:48:35,073
I'm excited to share how we
use SageMaker Studio in KOHO.

905
00:48:36,090 --> 00:48:37,800
First a quick context about KOHO.

906
00:48:38,850 --> 00:48:42,360
KOHO is a Canadian FinTech
founded in 2014 with the mission

907
00:48:42,360 --> 00:48:46,620
to provide better financial
solutions for all Canadians.

908
00:48:46,620 --> 00:48:50,400
Today we have over 2 million
customers using our products

909
00:48:50,400 --> 00:48:53,073
across banking, credit and lifestyle.

910
00:48:54,180 --> 00:48:56,520
We have products ranging from checking

911
00:48:56,520 --> 00:48:58,740
and savings account, line of credit,

912
00:48:58,740 --> 00:49:01,680
buy now pay later, insurance,

913
00:49:01,680 --> 00:49:04,263
we even have eSIM for
international travelers.

914
00:49:05,730 --> 00:49:08,850
We are about 230 employees in total.

915
00:49:08,850 --> 00:49:12,540
That's a startup size
team who we are facing

916
00:49:12,540 --> 00:49:14,730
and growing at enterprise scale

917
00:49:14,730 --> 00:49:18,423
and facing and building
enterprise scale solutions.

918
00:49:19,320 --> 00:49:21,210
So that's exactly where SageMaker Studio

919
00:49:21,210 --> 00:49:26,210
became critical for us, that
friction of startup resources

920
00:49:26,310 --> 00:49:27,813
with enterprise demands.

921
00:49:30,300 --> 00:49:34,173
We start with the challenges
we faced, first, vendor cost.

922
00:49:36,510 --> 00:49:39,840
Like many startups, we
started with different vendors

923
00:49:39,840 --> 00:49:42,570
for different machine learning use cases.

924
00:49:42,570 --> 00:49:44,560
We used to spend about $1.5 million

925
00:49:45,450 --> 00:49:48,100
just for our real-time fraud
deduction use case alone

926
00:49:49,290 --> 00:49:52,050
because typically the
vendors-based solutions

927
00:49:52,050 --> 00:49:56,100
charge based on the API
based as as we grow,

928
00:49:56,100 --> 00:49:58,710
as our customer base increase,

929
00:49:58,710 --> 00:50:01,923
as our transactions volume
increase, our cost exploded.

930
00:50:03,930 --> 00:50:06,810
Next, performance requirements.

931
00:50:06,810 --> 00:50:09,420
In real-time use cases
like fraud detection,

932
00:50:09,420 --> 00:50:11,790
we cannot compromise on speed.

933
00:50:11,790 --> 00:50:13,650
We need to make the fraud decision

934
00:50:13,650 --> 00:50:16,830
and return the response
within 50 milliseconds

935
00:50:16,830 --> 00:50:19,170
as our customers are
waiting at the checkout

936
00:50:19,170 --> 00:50:20,163
using their card.

937
00:50:21,660 --> 00:50:23,763
Next, security and data access.

938
00:50:24,630 --> 00:50:27,270
We are a regulated financial institution.

939
00:50:27,270 --> 00:50:30,210
We wanted to make sure our
customer's data is safe.

940
00:50:30,210 --> 00:50:33,093
We wanted to make sure the
data stays within our VPC.

941
00:50:34,620 --> 00:50:37,320
At the same time we wanted
to give secured access

942
00:50:37,320 --> 00:50:39,030
to all these data in our warehouse

943
00:50:39,030 --> 00:50:42,030
and lake to all of our data scientists

944
00:50:42,030 --> 00:50:44,193
for experimenting and building models.

945
00:50:45,450 --> 00:50:49,500
And finally we needed a
end-to-end ML platform.

946
00:50:49,500 --> 00:50:52,920
We are a lean data scientist
who cannot afford our time

947
00:50:52,920 --> 00:50:56,280
stitching together different
tools to do our job.

948
00:50:56,280 --> 00:51:00,030
So we needed a single IDE to access data,

949
00:51:00,030 --> 00:51:02,910
to engineer features, to train models,

950
00:51:02,910 --> 00:51:04,760
and deploy and operate in production.

951
00:51:05,610 --> 00:51:06,510
So those are the problems

952
00:51:06,510 --> 00:51:08,510
and those are the situations we were in.

953
00:51:09,450 --> 00:51:11,820
I'll be show you how
we use SageMaker Studio

954
00:51:11,820 --> 00:51:13,890
in our development environment.

955
00:51:13,890 --> 00:51:17,400
So with that vendor cost
as a primary problem,

956
00:51:17,400 --> 00:51:19,920
we started our in-house
model development journey

957
00:51:19,920 --> 00:51:24,363
back in 2023 with SageMaker
Studio as a core foundation.

958
00:51:25,290 --> 00:51:27,990
Today we have over 20 data scientists

959
00:51:27,990 --> 00:51:31,560
and ML engineers using our domains

960
00:51:31,560 --> 00:51:34,890
across four different teams, fraud risk,

961
00:51:34,890 --> 00:51:37,893
credit risk, marketing,
and our own platform team.

962
00:51:38,940 --> 00:51:42,570
JupyterLab is the primary IDE
most of our engineers use,

963
00:51:42,570 --> 00:51:46,293
some prefer Code Editor as
well, Studio gives both.

964
00:51:47,820 --> 00:51:49,830
So when our data scientists login

965
00:51:49,830 --> 00:51:52,320
to our internal SSO portal,

966
00:51:52,320 --> 00:51:55,330
they get redirected to the
team-specific Studio domain

967
00:51:56,280 --> 00:51:59,880
in which their own individual
JupyterLab environment

968
00:51:59,880 --> 00:52:03,753
is provisioned and
everything is ready for them,

969
00:52:04,680 --> 00:52:06,660
everything is already connected.

970
00:52:06,660 --> 00:52:09,060
They get to access the data in the lake

971
00:52:09,060 --> 00:52:13,323
and warehouse securely and
they get to access glue and EMR

972
00:52:13,323 --> 00:52:15,810
and SageMaker processing jobs

973
00:52:15,810 --> 00:52:18,753
for data processing and model development.

974
00:52:19,800 --> 00:52:22,650
And every team have a dedicated S3 buckets

975
00:52:22,650 --> 00:52:24,177
for storing intermediate data,

976
00:52:24,177 --> 00:52:26,527
and access for all this
is managed through IAM.

977
00:52:27,870 --> 00:52:30,960
From a platform perspective, we manage,

978
00:52:30,960 --> 00:52:33,840
provision and manage all
these through infrastructure

979
00:52:33,840 --> 00:52:34,923
as code terraform.

980
00:52:35,940 --> 00:52:38,820
So when we want to onboard
a new data scientist,

981
00:52:38,820 --> 00:52:41,670
all we had to do is just a single PR

982
00:52:41,670 --> 00:52:44,120
to create a user profile
and they're ready to go.

983
00:52:46,530 --> 00:52:48,990
So that's how we set up our
development environment.

984
00:52:48,990 --> 00:52:52,263
Now let me show you how we
deploy models in production.

985
00:52:54,000 --> 00:52:56,883
This is our complete end-to-end
ML platform architecture.

986
00:52:58,890 --> 00:53:00,990
I know there is a lot here,
but I just wanted to show

987
00:53:00,990 --> 00:53:03,810
how this all connects end to end.

988
00:53:03,810 --> 00:53:06,450
So it all starts with the SageMaker Studio

989
00:53:06,450 --> 00:53:08,730
as a model development environment.

990
00:53:08,730 --> 00:53:10,860
This is where our data scientist

991
00:53:10,860 --> 00:53:13,233
explore the data engineer features,

992
00:53:14,400 --> 00:53:16,710
experiment with different models,

993
00:53:16,710 --> 00:53:20,130
and finally, when they're ready
to move this to production,

994
00:53:20,130 --> 00:53:22,050
we use AWS-managed Airflow

995
00:53:22,050 --> 00:53:24,603
as our MLOps pipelines
orchestration engine.

996
00:53:25,800 --> 00:53:29,730
So these pipelines trigger
the SageMaker processing job

997
00:53:29,730 --> 00:53:31,330
for feature engineering at scale

998
00:53:32,430 --> 00:53:34,880
and they trigger Training
Jobs for model training

999
00:53:36,090 --> 00:53:37,890
and everything gets versioned

1000
00:53:37,890 --> 00:53:39,740
and registered in the model registry.

1001
00:53:40,680 --> 00:53:44,280
It's the same pipelines that
we build for model evaluation,

1002
00:53:44,280 --> 00:53:47,520
model monitoring, our scoring pipelines,

1003
00:53:47,520 --> 00:53:49,670
our rollout pipelines
for different models.

1004
00:53:50,940 --> 00:53:54,270
When we want to deploy these
models for real-time serving,

1005
00:53:54,270 --> 00:53:57,213
we use SageMaker endpoints.

1006
00:53:58,440 --> 00:54:01,980
That's a key component in
meeting our latency requirement

1007
00:54:01,980 --> 00:54:03,093
of 50 milliseconds,

1008
00:54:04,529 --> 00:54:06,840
and the entire ML platform is built on top

1009
00:54:06,840 --> 00:54:10,950
of our core data platform, which
is S3 for raw data storage,

1010
00:54:10,950 --> 00:54:13,050
Redshift for warehousing.

1011
00:54:13,050 --> 00:54:15,720
And we use SageMaker
Feature Store for storing

1012
00:54:15,720 --> 00:54:18,990
and serving ML features
both in real, off, online

1013
00:54:18,990 --> 00:54:19,890
and offline board.

1014
00:54:21,960 --> 00:54:25,710
So SageMaker Studio is
not only giving us the IDE

1015
00:54:25,710 --> 00:54:27,240
for model development,

1016
00:54:27,240 --> 00:54:29,280
but it's also giving us a visibility

1017
00:54:29,280 --> 00:54:31,430
of everything you see
here in this diagram.

1018
00:54:32,310 --> 00:54:33,990
So from the same IDE,

1019
00:54:33,990 --> 00:54:37,530
our engineers can explore
all the existing features

1020
00:54:37,530 --> 00:54:39,120
in the Feature Store.

1021
00:54:39,120 --> 00:54:40,830
They can look at all the Training Jobs

1022
00:54:40,830 --> 00:54:43,653
and the history, their
experiments, results,

1023
00:54:45,390 --> 00:54:47,280
all the models that are registered

1024
00:54:47,280 --> 00:54:49,560
and all the endpoints that are deployed,

1025
00:54:49,560 --> 00:54:51,033
all that from one single IDE,

1026
00:54:52,050 --> 00:54:55,080
so they don't have to
jump different consoles

1027
00:54:55,080 --> 00:54:57,303
to gain visibility into
their ML lifecycle.

1028
00:54:58,680 --> 00:55:01,800
And everything is deployed within our VPC

1029
00:55:01,800 --> 00:55:03,750
and the data never leaves our boundary.

1030
00:55:05,010 --> 00:55:09,360
So that's our complete architecture
from how we build model,

1031
00:55:09,360 --> 00:55:12,453
how we deploy in prod, how
we monitor and we operate.

1032
00:55:15,810 --> 00:55:17,043
Now let's talk business.

1033
00:55:19,230 --> 00:55:21,753
Cost, that's our primary problem.

1034
00:55:22,800 --> 00:55:27,690
We went from spending $1.5
million per year with our vendor

1035
00:55:27,690 --> 00:55:31,197
to just $26,000 per year with SageMaker.

1036
00:55:32,400 --> 00:55:34,590
That's a 98% age cost reduction.

1037
00:55:34,590 --> 00:55:36,033
That's right, 98,

1038
00:55:37,530 --> 00:55:40,743
with over $1.47 million in annual savings.

1039
00:55:41,940 --> 00:55:45,183
That's huge for a startup and
any company for that matter.

1040
00:55:47,790 --> 00:55:51,510
And we are processing over 1
million transactions per day

1041
00:55:51,510 --> 00:55:54,840
with just 15 milliseconds average latency,

1042
00:55:54,840 --> 00:55:57,033
way below our budget of 50 milliseconds.

1043
00:55:58,710 --> 00:56:03,180
Because we built the models
on top of our own data

1044
00:56:03,180 --> 00:56:07,413
from our data platform, from
our own custom ML platform,

1045
00:56:08,490 --> 00:56:11,820
we get to improve the model's accuracy,

1046
00:56:11,820 --> 00:56:14,520
reduce false positive in our
fraud detection use case.

1047
00:56:15,450 --> 00:56:18,933
That means less dispute
claims, more happy customers.

1048
00:56:20,790 --> 00:56:23,640
Would you stop in the real-time
fraud deduction use case.

1049
00:56:24,630 --> 00:56:29,043
We scaled the same solution
across many different use cases.

1050
00:56:30,240 --> 00:56:33,160
We are building models for
underwriting our loan products

1051
00:56:34,800 --> 00:56:36,580
for predicting the user churn

1052
00:56:37,560 --> 00:56:40,503
and marketing use cases like LTV and more.

1053
00:56:42,297 --> 00:56:44,543
And now we are moving to GenAI

1054
00:56:44,543 --> 00:56:47,253
and LLM-based applications
and solutions as well.

1055
00:56:48,270 --> 00:56:50,740
The way SageMaker Studio helps us is

1056
00:56:52,380 --> 00:56:55,350
the same Studio environment
our data scientists

1057
00:56:55,350 --> 00:56:58,350
and ML engineers gets to
access to the foundation models

1058
00:56:58,350 --> 00:57:00,180
through Jumpstart.

1059
00:57:00,180 --> 00:57:03,150
So that's where we access
these foundation models

1060
00:57:03,150 --> 00:57:06,750
for prototyping, test our prompts,

1061
00:57:06,750 --> 00:57:10,170
compare our prompt results,
compare different models,

1062
00:57:10,170 --> 00:57:11,550
model distillation,

1063
00:57:11,550 --> 00:57:14,553
fine tune as needed for
that particular prototype.

1064
00:57:15,450 --> 00:57:17,920
Once those prototyping is complete

1065
00:57:18,810 --> 00:57:22,410
and we prove the value,
though we take those solutions

1066
00:57:22,410 --> 00:57:25,260
and deploy them in purpose-build tools

1067
00:57:25,260 --> 00:57:30,120
and infra like EKS for example
or Bedrock directly APA

1068
00:57:30,120 --> 00:57:32,403
calls to Bedrock in some cases,

1069
00:57:33,510 --> 00:57:36,183
so with that SageMaker
Studio first approach,

1070
00:57:37,140 --> 00:57:39,290
we have many use cases
in production today.

1071
00:57:40,140 --> 00:57:40,973
Our compliance team

1072
00:57:40,973 --> 00:57:44,823
is generating anti-money
laundering reports faster.

1073
00:57:46,080 --> 00:57:48,580
We have an enhanced
merchant categorization

1074
00:57:49,500 --> 00:57:51,090
that drives our accuracy

1075
00:57:51,090 --> 00:57:53,643
of our customer's cashback percentage.

1076
00:57:55,080 --> 00:57:57,663
We are analyzing our
customer's feedback faster.

1077
00:57:58,980 --> 00:58:01,080
We are prioritizing our product requests

1078
00:58:01,080 --> 00:58:03,873
and roadmap based on those insights.

1079
00:58:05,790 --> 00:58:10,740
The key takeaway is SageMaker
Studio is a single IDE

1080
00:58:10,740 --> 00:58:13,890
that can help with both
building, deploying

1081
00:58:13,890 --> 00:58:17,070
and operating traditional
machine learning models

1082
00:58:17,070 --> 00:58:20,250
and also experimenting, prototyping,

1083
00:58:20,250 --> 00:58:23,523
and building GenAI and LLM applications.

1084
00:58:26,010 --> 00:58:29,700
To conclude, SageMaker
Studio gave KOHO the power

1085
00:58:29,700 --> 00:58:31,650
to build enterprise scale

1086
00:58:31,650 --> 00:58:35,703
and cost-effective solutions
with our startup agility.

1087
00:58:36,900 --> 00:58:38,070
Thank you everyone.

1088
00:58:38,070 --> 00:58:40,469
I hope you find this
useful in your ML journey.

1089
00:58:40,469 --> 00:58:43,636
(audience applauding)

1090
00:58:45,270 --> 00:58:47,130
- Thank you, Mani.

1091
00:58:47,130 --> 00:58:48,960
Before we conclude the session,

1092
00:58:48,960 --> 00:58:51,450
we would like to share some material

1093
00:58:51,450 --> 00:58:54,870
related to the official
Amazon SageMaker AI website,

1094
00:58:54,870 --> 00:58:56,550
as well as customer quotes

1095
00:58:56,550 --> 00:58:58,600
and references on how SageMaker AI helped

1096
00:58:59,550 --> 00:59:02,070
across multiple type of use cases,

1097
00:59:02,070 --> 00:59:05,190
as well as some links related to workshops

1098
00:59:05,190 --> 00:59:07,740
that you can start with for understanding

1099
00:59:07,740 --> 00:59:08,970
how SageMaker AI works

1100
00:59:08,970 --> 00:59:11,763
and test all the different capabilities.

1101
00:59:13,230 --> 00:59:14,490
Before we complete the session

1102
00:59:14,490 --> 00:59:16,830
I kindly ask you to rate
the session on the app,

1103
00:59:16,830 --> 00:59:18,570
this is really, really important for us

1104
00:59:18,570 --> 00:59:21,510
to improve the content
of the future session.

1105
00:59:21,510 --> 00:59:25,140
And I would like to thank you
again for attending this one

1106
00:59:25,140 --> 00:59:27,450
and hope you can enjoy the rest of the day

1107
00:59:27,450 --> 00:59:28,530
and the rest re:Invent.

1108
00:59:28,530 --> 00:59:29,571
Thank you.

1109
00:59:29,571 --> 00:59:32,738
(audience applauding)

