1
00:00:01,084 --> 00:00:02,460
- [Speaker] All right.

2
00:00:02,460 --> 00:00:03,660
Looks like a crowd.

3
00:00:03,660 --> 00:00:04,493
Wow.

4
00:00:05,580 --> 00:00:06,413
Yeah.

5
00:00:06,413 --> 00:00:07,709
Nice.

6
00:00:07,709 --> 00:00:08,730
Love the energy from the three people

7
00:00:08,730 --> 00:00:10,650
that are probably done with their day

8
00:00:10,650 --> 00:00:11,550
and have to deal

9
00:00:11,550 --> 00:00:14,130
with one more person talking about AI.

10
00:00:14,130 --> 00:00:14,970
Of course.

11
00:00:14,970 --> 00:00:16,252
All right.

12
00:00:16,252 --> 00:00:17,790
So I'll try to make it
fun for the three people

13
00:00:17,790 --> 00:00:19,573
that are looking at me.

14
00:00:19,573 --> 00:00:21,270
I will not be the first

15
00:00:21,270 --> 00:00:22,890
person you've heard this from.

16
00:00:22,890 --> 00:00:23,940
I will most definitely

17
00:00:23,940 --> 00:00:26,370
not be the last person
you've heard this from.

18
00:00:26,370 --> 00:00:29,130
But 2025 was the year of agents,

19
00:00:29,130 --> 00:00:32,700
and I would say probably the next decade

20
00:00:32,700 --> 00:00:34,140
will be the year of agents.

21
00:00:34,140 --> 00:00:37,980
So at Fireworks AI, we've helped a ton

22
00:00:37,980 --> 00:00:39,870
of companies, right across all different

23
00:00:39,870 --> 00:00:42,468
use cases to build agents in production.

24
00:00:42,468 --> 00:00:45,120
That includes coding agents with a lot

25
00:00:45,120 --> 00:00:47,070
of the companies when
you walk around here,

26
00:00:47,070 --> 00:00:49,129
they're using us, document agents

27
00:00:49,129 --> 00:00:52,593
that process banking information,

28
00:00:55,470 --> 00:00:58,260
legal information, all of
these different things,

29
00:00:58,260 --> 00:00:59,790
sales and marketing agents

30
00:00:59,790 --> 00:01:00,780
that are doing outbound,

31
00:01:00,780 --> 00:01:02,850
that are doing inbound, hiring agents

32
00:01:02,850 --> 00:01:05,730
that are doing recruiting automatically,

33
00:01:05,730 --> 00:01:07,320
customer service agents

34
00:01:07,320 --> 00:01:09,540
that are increasing csat,

35
00:01:09,540 --> 00:01:11,340
reducing average handling time,

36
00:01:11,340 --> 00:01:12,450
you name it, right?

37
00:01:12,450 --> 00:01:13,560
Across all of the industries

38
00:01:13,560 --> 00:01:15,720
that you can imagine, retail, insurance,

39
00:01:15,720 --> 00:01:19,200
finance, education, life science,

40
00:01:19,200 --> 00:01:21,240
security, et cetera, right?

41
00:01:21,240 --> 00:01:23,760
And you're saying, "Okay,
well everybody's saying

42
00:01:23,760 --> 00:01:24,900
they build agents."

43
00:01:24,900 --> 00:01:28,050
But the reality is a
lot of people don't talk

44
00:01:28,050 --> 00:01:31,290
about how hard it is
to build agents, right?

45
00:01:31,290 --> 00:01:32,430
So it sounds really sexy.

46
00:01:32,430 --> 00:01:33,630
Agents are really cool.

47
00:01:33,630 --> 00:01:35,610
Everybody loves talking about agents,

48
00:01:35,610 --> 00:01:37,020
what people don't really talk about is

49
00:01:37,020 --> 00:01:40,290
how difficult it can be to build agents

50
00:01:40,290 --> 00:01:43,380
for specific business use cases, right?

51
00:01:43,380 --> 00:01:46,590
So you have all of these possible errors

52
00:01:46,590 --> 00:01:47,423
that can happen.

53
00:01:47,423 --> 00:01:48,930
All of these possible failure modes

54
00:01:48,930 --> 00:01:50,820
that can happen across multiple layers

55
00:01:50,820 --> 00:01:51,840
of the stack, right?

56
00:01:51,840 --> 00:01:54,000
So just starting here, like one is,

57
00:01:54,000 --> 00:01:55,500
are you gonna use a closed-source model

58
00:01:55,500 --> 00:01:56,418
or an open-source model?

59
00:01:56,418 --> 00:01:58,560
If you're using an open-source model,

60
00:01:58,560 --> 00:02:01,473
are you using a small model, maybe a 8B,

61
00:02:01,473 --> 00:02:03,900
a 5B model, or are you using a large model

62
00:02:03,900 --> 00:02:04,980
like Kimi or DeepSeek

63
00:02:04,980 --> 00:02:07,140
that are in the trillion
of parameters, right?

64
00:02:07,140 --> 00:02:08,640
Once you pick your selection

65
00:02:08,640 --> 00:02:09,810
and you get your model for,

66
00:02:09,810 --> 00:02:11,970
let's say it's a search use case,

67
00:02:11,970 --> 00:02:13,260
what about the latency, right?

68
00:02:13,260 --> 00:02:15,990
You try to deploy it, you
want your search to be

69
00:02:15,990 --> 00:02:18,390
below a second, you need your LLM to be

70
00:02:18,390 --> 00:02:21,390
below 300 milliseconds, right?

71
00:02:21,390 --> 00:02:22,560
And putting that at scale

72
00:02:22,560 --> 00:02:24,300
when you have millions of users,

73
00:02:24,300 --> 00:02:27,602
that's really frigging hard, right?

74
00:02:27,602 --> 00:02:30,780
And then of course,
quality is king, right?

75
00:02:30,780 --> 00:02:33,000
You need to ensure that the accuracy

76
00:02:33,000 --> 00:02:35,940
and the quality is the highest it can be.

77
00:02:35,940 --> 00:02:38,190
Once you have all of that set out,

78
00:02:38,190 --> 00:02:40,410
let's say you use the
closed-source provider,

79
00:02:40,410 --> 00:02:42,990
you start to launch and
then you see, "Holy cow,

80
00:02:42,990 --> 00:02:44,970
my costs are absolutely
ballooning, right?"

81
00:02:44,970 --> 00:02:46,680
So you kind of got across the first three,

82
00:02:46,680 --> 00:02:47,970
but now you can't really support it

83
00:02:47,970 --> 00:02:49,650
because it's too expensive, right?

84
00:02:49,650 --> 00:02:50,790
And then of course,

85
00:02:50,790 --> 00:02:52,222
if you want to do open-source models,

86
00:02:52,222 --> 00:02:53,160
then there's a lot

87
00:02:53,160 --> 00:02:54,690
of infra complexity involved, right?

88
00:02:54,690 --> 00:02:57,450
Are you running an EKS
cluster, ECS cluster?

89
00:02:57,450 --> 00:03:01,830
How are you deploying
a across multiple nodes

90
00:03:01,830 --> 00:03:04,920
with tens of GPUs, dozens of GPUs, right?

91
00:03:04,920 --> 00:03:06,510
Of course there's data privacy,

92
00:03:06,510 --> 00:03:09,840
compliance, security, availability.

93
00:03:09,840 --> 00:03:10,800
Do you have the ML expertise?

94
00:03:10,800 --> 00:03:11,820
I can go on and on, right?

95
00:03:11,820 --> 00:03:12,810
And you're probably getting bored

96
00:03:12,810 --> 00:03:13,643
because there's just like

97
00:03:13,643 --> 00:03:14,970
so many of these errors.

98
00:03:14,970 --> 00:03:17,280
Now, what I want to
convince you about today is

99
00:03:17,280 --> 00:03:18,660
that at Fireworks,

100
00:03:18,660 --> 00:03:20,610
we are basically the one stop shop,

101
00:03:20,610 --> 00:03:21,930
the AI platform

102
00:03:21,930 --> 00:03:24,060
inference and customization
engine that kind

103
00:03:24,060 --> 00:03:26,970
of lets you forget about all of these

104
00:03:26,970 --> 00:03:28,590
and really build what we call,

105
00:03:28,590 --> 00:03:30,000
I know it's a little bit corny,

106
00:03:30,000 --> 00:03:32,400
but magical AI experiences.

107
00:03:32,400 --> 00:03:35,460
So what is a Fireworks platform, right?

108
00:03:35,460 --> 00:03:38,370
As I said, we are an open-source inference

109
00:03:38,370 --> 00:03:41,640
and customization engine
for open-source models.

110
00:03:41,640 --> 00:03:44,010
At the very top layer, we make it really,

111
00:03:44,010 --> 00:03:46,200
really easy for developers to use

112
00:03:46,200 --> 00:03:48,540
any of the top open-source models

113
00:03:48,540 --> 00:03:50,340
with day one access, right?

114
00:03:50,340 --> 00:03:52,380
So that includes DeepSeek models,

115
00:03:52,380 --> 00:03:54,660
actually DeepSeek
released the model today,

116
00:03:54,660 --> 00:03:57,585
and it's gonna be available
on Fireworks today.

117
00:03:57,585 --> 00:04:01,530
Kimi models, Llama, Minstrel, Qwen,

118
00:04:01,530 --> 00:04:02,400
you name it, right?

119
00:04:02,400 --> 00:04:05,610
Not only LLMs, also
vision language models,

120
00:04:05,610 --> 00:04:08,280
also voice and ASR models, right?

121
00:04:08,280 --> 00:04:10,140
It's incredibly easy to get started.

122
00:04:10,140 --> 00:04:12,120
So we are compatible

123
00:04:12,120 --> 00:04:14,370
with the OpenAI SDK Python library.

124
00:04:14,370 --> 00:04:16,350
So if you're using OpenAI,

125
00:04:16,350 --> 00:04:18,300
it's literally two lines of code

126
00:04:18,300 --> 00:04:19,440
that you have to switch.

127
00:04:19,440 --> 00:04:21,240
One is the model selection,

128
00:04:21,240 --> 00:04:23,010
and then two, you just need an API key

129
00:04:23,010 --> 00:04:23,979
for Fireworks, right?

130
00:04:23,979 --> 00:04:28,620
Once developers basically
find the proper model

131
00:04:28,620 --> 00:04:29,640
that they wanna run,

132
00:04:29,640 --> 00:04:31,650
let's say they're doing
a search use case, right,

133
00:04:31,650 --> 00:04:34,200
so they're running a really fast 8B

134
00:04:34,200 --> 00:04:36,480
or smaller model, then what happens

135
00:04:36,480 --> 00:04:38,760
very likely is that once they want

136
00:04:38,760 --> 00:04:40,860
to deploy that, then they want the quality

137
00:04:40,860 --> 00:04:44,040
to not only match closed-source providers,

138
00:04:44,040 --> 00:04:46,320
but beat closed-source providers, right?

139
00:04:46,320 --> 00:04:48,270
And they want to have
that latency of let's say,

140
00:04:48,270 --> 00:04:51,570
300 milliseconds to be
stable as they scale

141
00:04:51,570 --> 00:04:53,760
to millions and millions of users.

142
00:04:53,760 --> 00:04:55,538
That's where our customization engine

143
00:04:55,538 --> 00:04:57,975
FireOptimizer comes in, right?

144
00:04:57,975 --> 00:05:01,410
So we do, and I'll talk about it in detail

145
00:05:01,410 --> 00:05:02,310
in the next slide,

146
00:05:02,310 --> 00:05:04,950
but we do two types of customization.

147
00:05:04,950 --> 00:05:06,930
One is workload optimization.

148
00:05:06,930 --> 00:05:09,332
That means if it's a search use case,

149
00:05:09,332 --> 00:05:11,250
very different deployment options

150
00:05:11,250 --> 00:05:12,083
than if you were

151
00:05:12,083 --> 00:05:13,890
to do an agentic AI use case, right,

152
00:05:13,890 --> 00:05:14,940
like some of our partners

153
00:05:14,940 --> 00:05:16,890
that are around here in the area.

154
00:05:16,890 --> 00:05:19,020
And then the second one is you want

155
00:05:19,020 --> 00:05:20,610
to fine tune models, right?

156
00:05:20,610 --> 00:05:22,225
So I have this saying that like,

157
00:05:22,225 --> 00:05:24,330
generic closed-source models

158
00:05:24,330 --> 00:05:25,861
are built for everyone,

159
00:05:25,861 --> 00:05:27,930
but optimized for no one, right?

160
00:05:27,930 --> 00:05:30,960
They're great, they're
inch deep, miles wide,

161
00:05:30,960 --> 00:05:32,580
they can do a ton of different things,

162
00:05:32,580 --> 00:05:33,810
but if you really push them

163
00:05:33,810 --> 00:05:35,193
on a very specific use case,

164
00:05:35,193 --> 00:05:37,050
they might have a lot of failure.

165
00:05:37,050 --> 00:05:39,450
And I have a saying that at Fireworks,

166
00:05:39,450 --> 00:05:42,570
we basically have our clients build models

167
00:05:42,570 --> 00:05:45,210
with their data and then we optimize

168
00:05:45,210 --> 00:05:46,504
for them, right?

169
00:05:46,504 --> 00:05:50,550
As I say, like one of our co-founders

170
00:05:50,550 --> 00:05:51,750
used the phrase that I like a lot,

171
00:05:51,750 --> 00:05:54,630
which is, "You don't need a bazooka

172
00:05:54,630 --> 00:05:56,370
to kill a mosquito," right?

173
00:05:56,370 --> 00:05:58,320
And that's exactly what
Fireworks provides.

174
00:05:58,320 --> 00:05:59,940
Instead of bazooka, you need something

175
00:05:59,940 --> 00:06:01,530
very specific to kill that mosquito

176
00:06:01,530 --> 00:06:02,761
to actually hit it, right.

177
00:06:02,761 --> 00:06:04,470
That is around the fine-tuning

178
00:06:04,470 --> 00:06:06,690
and the RFT reinforcement
learning fine-tuning

179
00:06:06,690 --> 00:06:08,430
to really push that quality

180
00:06:08,430 --> 00:06:11,010
above the closed-source providers.

181
00:06:11,010 --> 00:06:12,420
And then finally, of course,

182
00:06:12,420 --> 00:06:13,740
how do you scale, right?

183
00:06:13,740 --> 00:06:16,290
When you're scaling to millions of users,

184
00:06:16,290 --> 00:06:18,750
tens of millions of users,
you want the platform

185
00:06:18,750 --> 00:06:21,030
to be super, super reliable.

186
00:06:21,030 --> 00:06:23,160
You want the SLAs to be topnotch

187
00:06:23,160 --> 00:06:25,860
and you want to have the highest

188
00:06:25,860 --> 00:06:27,030
reliability there is.

189
00:06:27,030 --> 00:06:27,870
We're able to do this

190
00:06:27,870 --> 00:06:29,490
because of our phenomenal partners,

191
00:06:29,490 --> 00:06:32,820
of course, of AWS as
well as Nvidia and AMD.

192
00:06:32,820 --> 00:06:35,040
We have a virtual cloud infrastructure

193
00:06:35,040 --> 00:06:37,470
with dozens of regions across the world

194
00:06:37,470 --> 00:06:38,880
where all of the companies

195
00:06:38,880 --> 00:06:42,720
basically run their open-source models on.

196
00:06:42,720 --> 00:06:45,810
So that is a high level,
right, of our platform.

197
00:06:45,810 --> 00:06:49,050
Now, for the few people
that are listening to me

198
00:06:49,050 --> 00:06:50,880
and that are actually interested

199
00:06:50,880 --> 00:06:53,370
in the nitty gritty,
I'm gonna jump into it.

200
00:06:53,370 --> 00:06:54,915
So as I talked about, right,

201
00:06:54,915 --> 00:06:57,300
I talked about customization
and how we customize

202
00:06:57,300 --> 00:06:59,040
across two different levers, right?

203
00:06:59,040 --> 00:07:01,470
One is the workload optimization.

204
00:07:01,470 --> 00:07:03,300
So I don't know how many of,

205
00:07:03,300 --> 00:07:04,830
there's literally five or six people,

206
00:07:04,830 --> 00:07:08,220
but a show of hands who took like ML 101?

207
00:07:08,220 --> 00:07:09,090
No, no one?

208
00:07:09,090 --> 00:07:09,923
Okay, a couple.

209
00:07:09,923 --> 00:07:11,370
You remember grid search, right,

210
00:07:11,370 --> 00:07:14,020
hyperparameter tuning and
all that good stuff, right?

211
00:07:15,810 --> 00:07:17,880
We computed essentially the space

212
00:07:17,880 --> 00:07:19,140
that you have in order

213
00:07:19,140 --> 00:07:21,390
to do a deployment across quality, speed,

214
00:07:21,390 --> 00:07:22,770
and cost dimensions,

215
00:07:22,770 --> 00:07:24,450
which means like spec decoder models,

216
00:07:24,450 --> 00:07:26,190
the hardware that you're using,

217
00:07:26,190 --> 00:07:28,500
the different execution modes,

218
00:07:28,500 --> 00:07:30,000
the kernel options, et cetera.

219
00:07:30,000 --> 00:07:33,090
And it's around 84,000 parameters, right?

220
00:07:33,090 --> 00:07:34,830
That is a very, very large space

221
00:07:34,830 --> 00:07:36,540
that no one wants to run through.

222
00:07:36,540 --> 00:07:37,830
So what we've developed

223
00:07:37,830 --> 00:07:39,600
is our proprietary technology,

224
00:07:39,600 --> 00:07:41,280
which is called FireOptimizer,

225
00:07:41,280 --> 00:07:43,710
where essentially clients work with us

226
00:07:43,710 --> 00:07:45,150
and they give us their,

227
00:07:45,150 --> 00:07:46,830
what we call workload profile.

228
00:07:46,830 --> 00:07:47,663
So they'll tell us,

229
00:07:47,663 --> 00:07:49,170
"Hey, I have a search use case.

230
00:07:49,170 --> 00:07:50,454
I want the latency to be

231
00:07:50,454 --> 00:07:52,884
below 200 milliseconds

232
00:07:52,884 --> 00:07:55,080
and I'm gonna scale to a throughput

233
00:07:55,080 --> 00:07:58,680
of let's say 200 queries
per second, right?"

234
00:07:58,680 --> 00:08:00,960
From that, we basically
run it through our stack

235
00:08:00,960 --> 00:08:03,540
and we identify what the optimal setup

236
00:08:03,540 --> 00:08:04,650
is for them to run,

237
00:08:04,650 --> 00:08:07,800
and we create a personalized
configuration deployment

238
00:08:07,800 --> 00:08:09,450
with that open-source models.

239
00:08:09,450 --> 00:08:13,020
Now, why we've been able to support all

240
00:08:13,020 --> 00:08:14,340
of these different use cases

241
00:08:14,340 --> 00:08:15,780
from the very, very low latency

242
00:08:15,780 --> 00:08:18,330
to the super complex agentic AI workloads

243
00:08:18,330 --> 00:08:19,860
with huge parameters is

244
00:08:19,860 --> 00:08:22,050
that we've built our
stack from the ground up.

245
00:08:22,050 --> 00:08:24,570
So we've optimized for every single layer

246
00:08:24,570 --> 00:08:25,403
in our stack.

247
00:08:25,403 --> 00:08:27,750
That starts at the lowest layer,

248
00:08:27,750 --> 00:08:30,450
which is the actual CUDA kernels running

249
00:08:30,450 --> 00:08:31,680
in the GPUs, right?

250
00:08:31,680 --> 00:08:33,240
So we have a custom CUDA kernels

251
00:08:33,240 --> 00:08:34,590
called FireAttention.

252
00:08:34,590 --> 00:08:36,328
And then as you move up on the stack,

253
00:08:36,328 --> 00:08:39,630
how you deploy across
multiple nodes, right,

254
00:08:39,630 --> 00:08:42,270
you have to deploy, let's say a Kimi,

255
00:08:42,270 --> 00:08:44,070
which is a trillion parameter model.

256
00:08:44,070 --> 00:08:47,010
You need maybe 16, maybe 24 GPUs.

257
00:08:47,010 --> 00:08:50,250
How you deploy that is
not trivial at all, right?

258
00:08:50,250 --> 00:08:51,950
So we have disaggregated inference,

259
00:08:51,950 --> 00:08:53,910
which means you're separating the prefill

260
00:08:53,910 --> 00:08:55,740
from the generation, and we make it

261
00:08:55,740 --> 00:08:57,600
incredibly easy for
clients to kind of forget

262
00:08:57,600 --> 00:08:59,190
about all this and we'll just manage it

263
00:08:59,190 --> 00:09:00,240
all for 'em, right?

264
00:09:00,240 --> 00:09:01,890
And then at the very, very high level,

265
00:09:01,890 --> 00:09:03,510
which is the model layer, right?

266
00:09:03,510 --> 00:09:04,650
So all of these methods,

267
00:09:04,650 --> 00:09:06,150
they're used across the industry,

268
00:09:06,150 --> 00:09:07,470
I'd say what we do is we just

269
00:09:07,470 --> 00:09:09,060
use them way better.

270
00:09:09,060 --> 00:09:10,620
And that is some methods

271
00:09:10,620 --> 00:09:12,330
like speculative decoding, right?

272
00:09:12,330 --> 00:09:14,040
So for you that don't know

273
00:09:14,040 --> 00:09:15,150
what speculative decoding is,

274
00:09:15,150 --> 00:09:17,280
essentially you have a very large model,

275
00:09:17,280 --> 00:09:18,983
you pair it with a very small model

276
00:09:18,983 --> 00:09:21,330
that's actually generating the tokens,

277
00:09:21,330 --> 00:09:23,070
and then the large model would accept

278
00:09:23,070 --> 00:09:24,600
or reject the tokens, right?

279
00:09:24,600 --> 00:09:26,310
If the acceptance rate is high,

280
00:09:26,310 --> 00:09:28,500
then your latency can come much lower.

281
00:09:28,500 --> 00:09:30,960
If the acceptance rate is low,

282
00:09:30,960 --> 00:09:32,490
then it's actually counterproductive

283
00:09:32,490 --> 00:09:33,900
and the latency is worse, right?

284
00:09:33,900 --> 00:09:36,090
So we train our own draft

285
00:09:36,090 --> 00:09:37,830
or spec decoder models in order

286
00:09:37,830 --> 00:09:40,950
to push the acceptance rate way above 70%,

287
00:09:40,950 --> 00:09:42,825
which makes it so that
you can get latencies

288
00:09:42,825 --> 00:09:44,970
below a hundred milliseconds

289
00:09:44,970 --> 00:09:46,440
for certain use cases.

290
00:09:46,440 --> 00:09:49,255
So that is the workload
optimization part, right?

291
00:09:49,255 --> 00:09:52,503
All about latency, cost, scalability.

292
00:09:53,550 --> 00:09:54,660
The other part is really

293
00:09:54,660 --> 00:09:55,800
around fine-tuning, right?

294
00:09:55,800 --> 00:09:58,681
I use the maybe very
shitty but useful metaphor

295
00:09:58,681 --> 00:10:01,980
of you don't need a
bazooka to kill a mosquito.

296
00:10:01,980 --> 00:10:03,656
And at Fireworks, we really think that

297
00:10:03,656 --> 00:10:07,740
company's moat is in owning

298
00:10:07,740 --> 00:10:08,850
their own AI, right?

299
00:10:08,850 --> 00:10:10,710
Like the model is their IP,

300
00:10:10,710 --> 00:10:11,880
the model is their product,

301
00:10:11,880 --> 00:10:13,530
and the way they do it is

302
00:10:13,530 --> 00:10:15,442
by using their subject matter expertise,

303
00:10:15,442 --> 00:10:18,240
their data to fine-tune models.

304
00:10:18,240 --> 00:10:20,370
So we have a fine-tuning platform

305
00:10:20,370 --> 00:10:21,480
that makes it really, really easy

306
00:10:21,480 --> 00:10:24,420
for developers to use either
supervised fine-tuning

307
00:10:24,420 --> 00:10:27,480
for, let's say intent classification,

308
00:10:27,480 --> 00:10:30,720
sentiment analysis,
product catalog cleansing,

309
00:10:30,720 --> 00:10:32,859
which is essentially you have images

310
00:10:32,859 --> 00:10:34,860
and then you're tagging them,

311
00:10:34,860 --> 00:10:37,140
whether it's gender, whether it's shoes,

312
00:10:37,140 --> 00:10:39,420
whether it's the different categories,

313
00:10:39,420 --> 00:10:41,010
jeans, basically the only thing

314
00:10:41,010 --> 00:10:41,850
that's coming to my mind

315
00:10:41,850 --> 00:10:43,950
is what I'm wearing, jackets, shirts,

316
00:10:43,950 --> 00:10:45,420
et cetera, right?

317
00:10:45,420 --> 00:10:46,980
So that's supervised fine-tuning.

318
00:10:46,980 --> 00:10:48,540
And then we just recently released

319
00:10:48,540 --> 00:10:50,760
as well reinforcement fine-tuning.

320
00:10:50,760 --> 00:10:52,170
Essentially there, instead of actually

321
00:10:52,170 --> 00:10:54,510
having the data set and putting the prompt

322
00:10:54,510 --> 00:10:55,343
and then the response

323
00:10:55,343 --> 00:10:56,970
that you want the model
for, you're writing

324
00:10:56,970 --> 00:10:58,890
what is called an evaluator function

325
00:10:58,890 --> 00:11:01,680
that scores the model between 0 and 1.

326
00:11:01,680 --> 00:11:04,217
So the model will become
better as it learns

327
00:11:04,217 --> 00:11:06,270
through its environment, right?

328
00:11:06,270 --> 00:11:07,890
We've seen incredible success

329
00:11:07,890 --> 00:11:09,510
with reinforcement fine-tuning

330
00:11:09,510 --> 00:11:11,220
both on coding agents

331
00:11:11,220 --> 00:11:14,190
as well as a very recent
client success story

332
00:11:14,190 --> 00:11:15,510
with our partners from Genspark,

333
00:11:15,510 --> 00:11:18,240
where they use RFT to move

334
00:11:18,240 --> 00:11:20,490
from one of the cloud models

335
00:11:20,490 --> 00:11:22,710
to an open-source Fireworks model.

336
00:11:22,710 --> 00:11:25,710
And they had around like a 20% increase

337
00:11:25,710 --> 00:11:27,962
in quality while reducing
latency and costs.

338
00:11:27,962 --> 00:11:30,870
So what we do essentially is we wanna make

339
00:11:30,870 --> 00:11:32,445
it really simple for companies

340
00:11:32,445 --> 00:11:35,440
to use their expertise
through this data flywheel

341
00:11:35,440 --> 00:11:37,890
so that they can fine-tune really quickly,

342
00:11:37,890 --> 00:11:39,300
deploy really quickly,

343
00:11:39,300 --> 00:11:41,370
and whenever there's a new model drop,

344
00:11:41,370 --> 00:11:43,080
they can just switch the new model,

345
00:11:43,080 --> 00:11:45,090
fine-tune it again and
deploy again, right?

346
00:11:45,090 --> 00:11:47,670
As well for everybody
that took their ML 101,

347
00:11:47,670 --> 00:11:49,560
there's covariate drift and data drift

348
00:11:49,560 --> 00:11:50,790
and things change, right?

349
00:11:50,790 --> 00:11:53,100
So we make it really fast for companies

350
00:11:53,100 --> 00:11:56,422
just iterate through the data flywheel.

351
00:11:56,422 --> 00:11:58,290
We have a couple of examples here, right?

352
00:11:58,290 --> 00:11:59,490
VLM fine-tuning

353
00:11:59,490 --> 00:12:02,053
and how we use an open-source model,

354
00:12:02,053 --> 00:12:03,729
a Qwen model, fine tune it

355
00:12:03,729 --> 00:12:05,190
on the specific data set.

356
00:12:05,190 --> 00:12:06,270
And you see that we don't only

357
00:12:06,270 --> 00:12:08,040
match the closed-source provider,

358
00:12:08,040 --> 00:12:09,240
but we beat it, right?

359
00:12:09,240 --> 00:12:10,380
And then the same with RFT,

360
00:12:10,380 --> 00:12:12,210
with reinforcer link fine-tuning.

361
00:12:12,210 --> 00:12:14,250
This is for a text to SQL use case.

362
00:12:14,250 --> 00:12:15,083
Same, right?

363
00:12:15,083 --> 00:12:16,050
We not only match,

364
00:12:16,050 --> 00:12:17,850
but we beat the closed-source provider.

365
00:12:17,850 --> 00:12:21,030
So again, really it's
all about owning your AI.

366
00:12:21,030 --> 00:12:21,990
The model is your product.

367
00:12:21,990 --> 00:12:23,040
The model is your a IP

368
00:12:23,040 --> 00:12:25,530
and a company's real
moat is in their own data

369
00:12:25,530 --> 00:12:27,900
and in their own expertise
and how they put that

370
00:12:27,900 --> 00:12:31,083
into their AI models for
their specific use cases.

371
00:12:32,310 --> 00:12:34,410
Now, why am I talking about this

372
00:12:34,410 --> 00:12:36,930
on AWS re:Invent?

373
00:12:36,930 --> 00:12:39,300
Well, because we built our entire stack

374
00:12:39,300 --> 00:12:43,530
on top of AWS re:Invent,
or sorry, on top of AWS,

375
00:12:43,530 --> 00:12:45,180
not on top of AWS re:Invent.

376
00:12:45,180 --> 00:12:49,620
So essentially we use EC2, ECS and EKS

377
00:12:49,620 --> 00:12:51,060
and we're just an inference

378
00:12:51,060 --> 00:12:53,226
and optimization layer that runs right

379
00:12:53,226 --> 00:12:54,750
on top of it, right?

380
00:12:54,750 --> 00:12:56,010
So the FireOptimizer,

381
00:12:56,010 --> 00:12:57,690
the fine-tuning that I described,

382
00:12:57,690 --> 00:13:00,480
as well as the incredibly
fast inference engine,

383
00:13:00,480 --> 00:13:02,880
is just this software layer

384
00:13:02,880 --> 00:13:05,340
that runs on top of AWS.

385
00:13:05,340 --> 00:13:07,090
Now we work with companies

386
00:13:08,062 --> 00:13:11,430
from gen AI startups
that move incredibly fast

387
00:13:11,430 --> 00:13:13,511
and they use our SaaS platform all the way

388
00:13:13,511 --> 00:13:15,600
to legacy companies that have

389
00:13:15,600 --> 00:13:18,900
to be incredibly compliant,
incredibly secure

390
00:13:18,900 --> 00:13:20,340
in terms of data privacy.

391
00:13:20,340 --> 00:13:21,540
And that's why we've developed

392
00:13:21,540 --> 00:13:23,490
these different deployment options

393
00:13:23,490 --> 00:13:25,290
on top of AWS, right?

394
00:13:25,290 --> 00:13:27,937
So from our SaaS platform
where quote unquote,

395
00:13:27,937 --> 00:13:29,550
"it's the least private,"

396
00:13:29,550 --> 00:13:31,230
even though we do have
zero data retention,

397
00:13:31,230 --> 00:13:33,420
et cetera, but it's all
in our environment, right,

398
00:13:33,420 --> 00:13:35,370
and then all the way to fully air gapped.

399
00:13:35,370 --> 00:13:37,050
So if you want to deploy Fireworks

400
00:13:37,050 --> 00:13:38,400
in a fully air gapped environment,

401
00:13:38,400 --> 00:13:42,180
meaning absolutely
nothing leaves your VPC,

402
00:13:42,180 --> 00:13:44,430
then we can do that through deploying

403
00:13:44,430 --> 00:13:46,050
through a Kubernetes cluster

404
00:13:46,050 --> 00:13:47,580
or as well as SageMaker.

405
00:13:47,580 --> 00:13:48,840
And then everything in between, right?

406
00:13:48,840 --> 00:13:50,730
So we can deploy with AWS private link

407
00:13:50,730 --> 00:13:51,911
where the network is secure

408
00:13:51,911 --> 00:13:55,350
or as well through non-air gapped BYOC,

409
00:13:55,350 --> 00:13:56,670
where we basically just have

410
00:13:56,670 --> 00:13:59,010
what we call a control plane, right?

411
00:13:59,010 --> 00:13:59,843
Okay.

412
00:13:59,843 --> 00:14:02,040
Now I'm gonna talk in
the last five minutes

413
00:14:02,040 --> 00:14:04,590
that I have about some
client success stories

414
00:14:04,590 --> 00:14:06,057
so that you guys kind of believe me

415
00:14:06,057 --> 00:14:08,790
in all the BS that I'm saying now.

416
00:14:08,790 --> 00:14:10,890
One example, and of course I'm gonna start

417
00:14:10,890 --> 00:14:13,500
with one that I can't use the logo,

418
00:14:13,500 --> 00:14:16,590
but a very large grocery
delivery platform,

419
00:14:16,590 --> 00:14:21,480
they basically use us to
fine-tune a very small

420
00:14:21,480 --> 00:14:24,185
Llama 10B 8B model

421
00:14:24,185 --> 00:14:26,910
in a search use case, right?

422
00:14:26,910 --> 00:14:28,650
So these are the types of use cases

423
00:14:28,650 --> 00:14:30,720
that are requiring incredibly low latency.

424
00:14:30,720 --> 00:14:32,580
They're handling around two to 3 million

425
00:14:32,580 --> 00:14:33,413
daily queries of

426
00:14:33,413 --> 00:14:37,440
what they call ambiguous queries, right?

427
00:14:37,440 --> 00:14:39,632
So let's say that you're using Uber Eats

428
00:14:39,632 --> 00:14:41,520
or Instacart or DoorDash
or whatever it is,

429
00:14:41,520 --> 00:14:43,380
like sometimes users will put something

430
00:14:43,380 --> 00:14:45,000
that is very vague, right,

431
00:14:45,000 --> 00:14:46,950
and you want to make it so

432
00:14:46,950 --> 00:14:48,156
that an LLM, a very fast LLM

433
00:14:48,156 --> 00:14:50,160
can do things like query expansion,

434
00:14:50,160 --> 00:14:52,316
it'll rewrite it, maybe
it'll tag it, et cetera.

435
00:14:52,316 --> 00:14:56,254
They use Fireworks to
fine tune that model,

436
00:14:56,254 --> 00:14:59,490
reduce the search the
search support tickets

437
00:14:59,490 --> 00:15:01,690
by 50% and run everything

438
00:15:01,690 --> 00:15:04,110
at 300 milliseconds or less.

439
00:15:04,110 --> 00:15:07,050
And that's not only technical targets,

440
00:15:07,050 --> 00:15:09,900
but also they actually ran an A and B test

441
00:15:09,900 --> 00:15:12,120
and they saw that they increased uplift

442
00:15:12,120 --> 00:15:14,970
once they moved to Fireworks.

443
00:15:14,970 --> 00:15:16,170
Another example is Notion.

444
00:15:16,170 --> 00:15:18,480
They're a great great partner of us.

445
00:15:18,480 --> 00:15:19,860
If you guys are a Notion user,

446
00:15:19,860 --> 00:15:21,180
I'm a huge Notion user.

447
00:15:21,180 --> 00:15:22,013
A lot of Notion...

448
00:15:22,013 --> 00:15:23,100
Yeah, good stuff, okay?

449
00:15:23,100 --> 00:15:24,210
If you love Notion AI,

450
00:15:24,210 --> 00:15:25,650
then you'll love Fireworks AI.

451
00:15:25,650 --> 00:15:28,770
A lot of Notion AI runs on Fireworks AI.

452
00:15:28,770 --> 00:15:30,737
They've also made use of fine-tuning

453
00:15:30,737 --> 00:15:34,710
and very small models
for very fast inference.

454
00:15:34,710 --> 00:15:35,970
So they actually moved

455
00:15:35,970 --> 00:15:37,950
from a closed-source provider to us.

456
00:15:37,950 --> 00:15:40,260
They saw around a 4x lower latency

457
00:15:40,260 --> 00:15:42,090
to 500 milliseconds and below,

458
00:15:42,090 --> 00:15:43,410
and they've scaled

459
00:15:43,410 --> 00:15:45,570
to a hundred million users plus, right?

460
00:15:45,570 --> 00:15:47,550
So again, incredibly low latency

461
00:15:47,550 --> 00:15:50,519
at scale while increasing quality

462
00:15:50,519 --> 00:15:52,170
through fine-tuning.

463
00:15:52,170 --> 00:15:53,580
And then the last one I'll talk

464
00:15:53,580 --> 00:15:55,257
about is DoorDash, right?

465
00:15:55,257 --> 00:15:57,960
So I talked a little bit about VLMs,

466
00:15:57,960 --> 00:15:59,313
vision language models.

467
00:16:00,587 --> 00:16:02,277
We don't only have text
models, we also have VLMs

468
00:16:02,277 --> 00:16:03,630
and we have a lot of,

469
00:16:03,630 --> 00:16:05,880
I'd say it's a growing
field, a lot of interest

470
00:16:05,880 --> 00:16:07,620
from clients in these use cases.

471
00:16:07,620 --> 00:16:08,790
One very interesting one

472
00:16:08,790 --> 00:16:10,740
is this product catalog cleansing, right?

473
00:16:10,740 --> 00:16:12,540
So these companies have millions

474
00:16:12,540 --> 00:16:13,710
and millions of images.

475
00:16:13,710 --> 00:16:15,420
They want to basically label them,

476
00:16:15,420 --> 00:16:16,253
as I said, right,

477
00:16:16,253 --> 00:16:20,370
Jeans, shorts, pants,
et cetera, automatically

478
00:16:20,370 --> 00:16:23,130
and to a high degree of
fidelity and accuracy.

479
00:16:23,130 --> 00:16:24,990
What they've done is,
for example, DoorDash,

480
00:16:24,990 --> 00:16:26,840
they fine-tune an open-source model

481
00:16:26,840 --> 00:16:28,710
that is running 3x faster

482
00:16:28,710 --> 00:16:29,700
than the closed-source model

483
00:16:29,700 --> 00:16:31,050
that they were using before,

484
00:16:31,050 --> 00:16:33,021
while increasing quality with fine-tuning

485
00:16:33,021 --> 00:16:37,170
and reducing the cost by 10%.

486
00:16:37,170 --> 00:16:39,150
So all in all right,

487
00:16:39,150 --> 00:16:41,700
we provide basically a one stop shop

488
00:16:41,700 --> 00:16:44,040
for anything that you need to build.

489
00:16:44,040 --> 00:16:48,810
We call magical AI
applications to really match

490
00:16:48,810 --> 00:16:51,180
and beat closed-source provider quality

491
00:16:51,180 --> 00:16:53,700
while keeping latency incredibly low

492
00:16:53,700 --> 00:16:55,980
and costs very controlled.

493
00:16:55,980 --> 00:16:57,030
Thank you very much.

494
00:16:57,030 --> 00:16:58,470
If you have any questions,

495
00:16:58,470 --> 00:16:59,670
we're somewhere around there.

496
00:16:59,670 --> 00:17:00,520
Don't tell me

497
00:17:01,530 --> 00:17:05,310
what the number of our area is

498
00:17:05,310 --> 00:17:06,930
because I have terrible working memory,

499
00:17:06,930 --> 00:17:08,430
but you will find us
somewhere around there

500
00:17:08,430 --> 00:17:09,540
and if not, I'll walk around here

501
00:17:09,540 --> 00:17:10,800
and I can tell you.

502
00:17:10,800 --> 00:17:13,290
Yeah, happy to answer any questions

503
00:17:13,290 --> 00:17:15,540
and hopefully I'll meet the 10 people

504
00:17:15,540 --> 00:17:16,470
that listened to me

505
00:17:16,470 --> 00:17:17,790
and hopefully I wasn't super boring.

506
00:17:17,790 --> 00:17:19,323
So anyways, thank you so much.

