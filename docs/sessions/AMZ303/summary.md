# AWS re:Invent 2025 会议总结：Amazon Ads 大规模 LLM 推理解决方案

## 会议概述

本次会议介绍了 Amazon Ads 团队如何利用大语言模型（LLM）技术在 AWS 上构建大规模推理服务，以更深入地理解购物者行为并为广告主提供更好的投放效果。Amazon Ads 作为 AWS 的客户和合作伙伴，在 Prime Day 等大型活动期间需要处理海量数据（会议展示的数据仅为 4 天 Prime Day 活动的统计）。团队面临的核心挑战是如何在确定性流程中使用 LLM，并在 Amazon 的超大规模场景下应对复杂性。

该解决方案完全构建在 AWS 平台上，使用了超过 180 种 AWS 服务。团队开发了一套专用的 LLM 推理服务，运行在 Amazon EKS 上，管理着超过 10,000 个 GPU。通过采用分离式推理（disaggregated inference）、KV 缓存感知路由等优化技术，以及与 NVIDIA Dynamo 的集成，系统能够支持每日数十亿次请求，同时满足从亚秒级到几百毫秒的不同延迟要求。解决方案涵盖了三种主要场景：离线批处理推理（高吞吐量）、近实时推理（几秒延迟）和实时推理（几百毫秒延迟）。

## 详细时间线

### 开场与背景介绍

[00:00 - 02:30] 会议开场，介绍 Amazon on AWS 专题
- 本专题展示 Amazon 内部团队构建的解决方案
- Amazon 由 100 多个不同实体组成（Prime Video、Ads、Ring 等）
- 展示 Prime Day 活动 4 天期间的惊人数据规模

[02:30 - 04:00] 会议主题与挑战说明
- 在确定性流程中使用 LLM 具有挑战性
- Amazon 规模增加了复杂性层级
- 介绍演讲嘉宾：Varun Kamla Karn（AWS）、Shanga Bao（Amazon Ads 总监）、Bole Chen（高级经理）

[04:00 - 06:00] 会议议程概览
- AWS 与 Amazon Ads 的关系
- 使用场景介绍
- 为何选择 Gen AI 解决方案
- 构建过程、架构选择和经验教训

### Amazon Ads 与 AWS 的合作关系

[06:00 - 08:30] AWS 与 Amazon Ads 的双重关系
- Amazon Ads 既是 AWS 客户，也是合作伙伴
- 诞生于云端，十多年前推出首个广告产品
- 早期投资 AI 技术
- 最近推出 Creative Agent 和 Ads Agent（基于 AWS 的代理解决方案）

[08:30 - 10:00] 使用的 AWS 服务演进
- 早期使用 EC2、S3 等基础服务
- 发展到使用 Step Functions、EMR 等编排服务
- 早期采用 SageMaker AI
- 最近采用 Bedrock
- 总共使用超过 180 种 AWS 服务

### Amazon Ads 工作原理与 Gen AI 机会

[10:00 - 13:00] Amazon Ads 基本介绍（Shanga Bao）
- 20 年前的核心问题：如何结合购物体验与品牌发现
- 数亿购物者通过不同渠道与 Amazon 互动
- 提供全漏斗广告解决方案

[13:00 - 16:00] 万圣节服装查询示例
- 展示品牌推荐和特色产品以建立品牌认知
- 在搜索结果中展示相关产品供考虑
- 在产品页面展示互补产品或替代品

[16:00 - 18:30] 广告系统后台工作流程
- 从数亿产品中检索最相关子集（通常数万个）
- 使用机器学习模型评分并缩减到几百个
- 最终选择少数几个展示给购物者
- 每日处理数十亿次请求
- 数百个模型实时运行

[18:30 - 21:00] 传统模型的工作方式
- 输入包括查询、搜索上下文、产品特征、购物者信号
- 使用神经网络架构（注意力机制、专家混合设计）
- 产生点击或购买概率分数
- 模型拥有数十亿参数
- 推理时执行数百亿次操作，延迟预算 40 毫秒

[21:00 - 24:00] Gen AI 的优势
- 传统模型擅长预测可能性，但不擅长解释原因
- Gen AI 可以理解产品和购物者意图的细微差别
- 示例：识别寻找"吸引眼球造型"vs"优雅幻想场景"的购物者
- 能够用人类可理解的语言解释匹配原因

[24:00 - 26:00] Gen AI 模型的能力
- 理解机器学习和人类语言
- 拥有世界知识，能识别产品属性的细微差异
- 理解购物者兴趣的演变
- 能够推理产品与购物者的匹配度

[26:00 - 28:00] 面临的挑战
- Gen AI 模型可达数千亿参数
- 许多用例需要亚秒级响应
- 需要响应购物者兴趣和活动变化的演变
- 每日数十亿次请求规模
- 工作负载可能是常见消费级语言模型的 10 倍

### LLM 推理解决方案构建过程

[28:00 - 30:30] LLM 推理性能影响因素（Bole Chen）
- 模型大小：更大的模型意味着更多计算和更高延迟
- Token 长度：更长的输入和输出 token 增加每个请求的工作量
- 延迟 SLA 和流量规模影响所需容量
- 通用规则：延迟和吞吐量相互权衡

[30:30 - 34:00] 产品理解用例
- 目标：识别哪些购物者群体对特定产品感兴趣
- 输入：产品图片、标题、描述、客户评论等长文本内容
- Token 数量可达 100,000
- 信息相对静态，不需要超低延迟
- 重点是高吞吐量：每日或每周数亿次请求

[34:00 - 36:00] 产品理解架构
- 使用 AWS Step Functions 和 EventBridge 编排大规模离线批处理
- 数据从 S3 流入吞吐量优化的 LLM 端点
- 支持高并发
- 数据同步到 ElastiCache 和 S3 存储层
- 其他广告生态系统作为额外信号消费

[36:00 - 39:00] 购物者理解用例
- 目标：根据近期活动推断购物者寻找什么
- 输入：点击、浏览但未点击、购买、购物车中的产品
- 组装会话上下文，输入提示范围从几百到几千 token
- 延迟要求：在购物者下次互动前返回（通常几秒）
- 平衡成本和效率

[39:00 - 41:00] 购物者理解架构
- 使用 Amazon MSK（Kafka）和 Flink 扩展流处理管道
- 支持每秒数十万次查询（QPS）
- 异步调用 LLM 端点
- 输出写入存储层

[41:00 - 43:00] 实时推理用例
- 场景：购物者-产品匹配、产品排名
- 延迟最关键：需要在几百毫秒内返回响应
- 支持每秒数十万次查询规模
- 广告服务器直接同步调用延迟优化的 LLM 端点

[43:00 - 45:00] 系统需求总结
- 灵活性：支持多种模型选择、工作负载、优化目标
- 高吞吐量：成本效率，特别是离线批处理和近实时推理
- 超低延迟：某些场景下 LLM 推理不能增加额外延迟

### 架构与优化技术

[45:00 - 48:00] 专用 LLM 推理服务架构
- 运行在 Amazon EKS 上
- 混合使用不同 EC2 实例类型
- 不仅为成本效率，也为应对 GPU 限制
- 许多工作负载可在更易获得的实例类型（如 G6E）上运行
- 避免与 P5、P6（Nvidia Blackwell GPU）竞争

[48:00 - 50:00] 编排层
- 开发模型路由器和作业调度器
- 根据优先级和当前容量状态路由流量和调度作业
- 保持技术选择灵活性
- 支持多种模型服务器：vLLM、TRT-LLM、SGLang
- 对不同配置进行基准测试，为每个用例选择最佳方案

[50:00 - 52:00] 系统优化
- 集成 NVIDIA Dynamo 采用多项系统级优化
- 提供轻量级 Java 客户端供广告服务器直接同步调用
- 支持流处理管道异步调用
- 提供批处理接口供不同团队提交和调度作业

[52:00 - 54:00] 数据流模式
- 实时推理：广告服务器直接调用 LLM 端点
- 离线批处理和近实时推理：将 LLM 输出写入存储层
- 存储层由 ElastiCache 和 S3 管理
- 广告服务器将其作为额外信号消费

[54:00 - 58:00] Prefill 和 Decode 阶段说明
- Prefill：处理整个输入 token，生成第一个输出 token
- Decode：逐个 token 自回归生成
- 连续批处理：将不同大小的请求打包在一起
- 在 token 级别混合 prefill 和 decode 阶段
- 延迟总是受最慢阶段限制（通常是 prefill）

[58:00 - 61:00] 分离式推理优化
- 将 prefill 和 decode 阶段分离到不同容器和服务
- 允许独立运行和扩展
- 提高资源利用率，改善延迟和吞吐量
- 示例配置：12 个 prefill worker（每个 1 GPU）+ 1 个 decode worker（4 GPU）
- 针对 Qwen 2.5 70B 模型测试，在相同延迟 SLA 下吞吐量提升 50%

[61:00 - 65:00] KV 缓存感知路由
- KV 缓存：存储先前处理 token 的中间结果和计算
- 避免冗余计算
- KV 感知路由：将请求定向到已持有最相关缓存数据的 GPU worker
- 示例：同一用户的请求共享相同输入提示（除最后的查询部分）
- 最大化 KV 缓存命中率

[65:00 - 67:00] KV 路由性能提升
- 针对 32B 模型在 NVIDIA Dynamo 上的基准测试
- 端到端延迟在不同百分位降低 20-40%
- 仅关注 prefill 阶段（首 token 时间）时差异更明显

[67:00 - 70:00] 关键技术支持
- 数据平面层：Amazon EKS 编排和管理超过 10,000 个不同实例类型的 GPU 集群
- 高性能网络：AWS EFA 加速节点间通信，更好利用网络带宽
- 对运行分布式推理和采用优化至关重要
- 集成 NVIDIA Dynamo：汇集多项系统级优化（KV 路由、分离式推理、低延迟数据传输）

[70:00 - 73:00] 动态容量分配
- GPU 昂贵且可能面临供应链限制
- 根据流量模式动态分配容量
- Amazon 购物网站实时推理有可预测的每日流量模式
- 高峰时段：更多 GPU 分配给实时推理以满足延迟 SLA
- 非高峰时段：相同 GPU 转移支持离线批处理推理
- 避免过度配置容量

[73:00 - 75:30] GPU 故障处理
- GPU 寿命通常 3-7 年
- 随时间推移会出现故障（高延迟、异常错误代码）
- 超过 10,000 GPU 的集群中，问题每天都可能发生
- 使用 AWS CloudWatch 持续监控信号
- 识别到性能下降时标记 GPU 实例进行替换
- 无需工程师人工干预即可保持系统可用性

### 经验教训与总结

[75:30 - 78:00] 主要经验教训
1. 不从头构建一切，大量依赖 AWS 服务和产品
2. 这是实现可扩展性和可靠性的关键
3. 使团队专注于应用层、业务逻辑和优化

[78:00 - 79:30] 与行业合作
- 与行业合作伙伴和社区保持紧密联系
- 能够在 GA 之前采用优化和新解决方案
- 保持系统灵活和开放
- 采用新方法时无需重新设计整个技术栈

[79:30 - 81:00] 实用建议
- 并非每个新兴优化都适用于所有用例
- 与应用合作伙伴密切合作了解用例
- 包括流量模式、输入提示设计、优化目标
- 这些信息帮助做出正确的解决方案和优化决策

[81:00 - 82:30] 致谢
- 感谢 AWS 团队和 NVIDIA 团队（包括 NVIDIA AWS 和 NVIDIA Dynamo 团队）
- 支持范围从优化基准测试到帮助在 EKS 上运行 Dynamo
- 对将技术解决方案大规模投入生产至关重要

[82:30 - 85:00] 会议结束
- 解决方案方法相对简单，基于基础构建块
- 观众可以在自己的用例中使用相同方法和技术
- 邀请参观 Caesars Forum 的 Amazon 演示区（One Amazon Lane）
- 包括 Rivian 送货车、Prime Video 直播体育、Zoox 机器人出租车等演示
- 鼓励提供反馈和未来会议建议