1
00:00:05,460 --> 00:00:06,446
- Okay.

2
00:00:06,446 --> 00:00:09,420
(audience applauding)

3
00:00:09,420 --> 00:00:10,260
Thank you.

4
00:00:10,260 --> 00:00:11,760
Thank you, and welcome.

5
00:00:11,760 --> 00:00:13,830
We're gonna go ahead and start today

6
00:00:13,830 --> 00:00:16,350
with a few interesting facts.

7
00:00:16,350 --> 00:00:19,140
The first being, did you know

8
00:00:19,140 --> 00:00:23,430
that for every breakthrough
drug that makes it to market,

9
00:00:23,430 --> 00:00:25,560
pharmaceutical companies analyze

10
00:00:25,560 --> 00:00:29,820
over 10 million patient data points?

11
00:00:29,820 --> 00:00:34,820
And yet 80% of healthcare
data remains unstructured

12
00:00:36,300 --> 00:00:38,013
and difficult to access.

13
00:00:39,180 --> 00:00:43,170
In today's rapidly evolving
healthcare landscape

14
00:00:43,170 --> 00:00:46,527
real-world data and real-world evidence

15
00:00:46,527 --> 00:00:51,527
are a game changer in how we
understand patient outcomes

16
00:00:51,540 --> 00:00:53,343
and develop new treatments.

17
00:00:54,480 --> 00:00:59,070
In fact, studies show that
leveraging real-world evidence

18
00:00:59,070 --> 00:01:03,330
in clinical trials can
reduce clinical trial times

19
00:01:03,330 --> 00:01:08,330
by up to 40% and cut
the costs by millions.

20
00:01:10,260 --> 00:01:11,940
I'm Anne Evans,

21
00:01:11,940 --> 00:01:15,570
and I serve as the Strategic
Solution Development Program

22
00:01:15,570 --> 00:01:20,520
Leader in AWS Industries'
Healthcare and Life Sciences.

23
00:01:20,520 --> 00:01:24,540
Today I'm joined by Greg
Cunningham from Eli Lilly,

24
00:01:24,540 --> 00:01:28,620
who will share how Lilly is
revolutionizing their approach

25
00:01:28,620 --> 00:01:30,660
to real-world data.

26
00:01:30,660 --> 00:01:35,460
And Eric Brooks, our AWS
Principal Solution Architect,

27
00:01:35,460 --> 00:01:38,520
who will dive into agentic workflows

28
00:01:38,520 --> 00:01:42,333
and how we are transforming
data accessibility.

29
00:01:43,590 --> 00:01:46,230
So whether you're a
pharmaceutical researcher,

30
00:01:46,230 --> 00:01:49,830
a healthcare provider,
or a data scientist,

31
00:01:49,830 --> 00:01:51,630
we're genuinely excited

32
00:01:51,630 --> 00:01:54,720
to share how we've been
collaborating with our partners

33
00:01:54,720 --> 00:01:59,720
and customers to accelerate
and simplify access

34
00:02:00,030 --> 00:02:01,473
to real-world data.

35
00:02:02,400 --> 00:02:06,420
So let's dive into how
data is used in healthcare

36
00:02:06,420 --> 00:02:09,180
and life sciences and how AWS

37
00:02:09,180 --> 00:02:11,733
is revolutionizing data access.

38
00:02:15,303 --> 00:02:19,350
In the last decade,
global data generation,

39
00:02:19,350 --> 00:02:24,350
all data being generated,
has increased 11 fold

40
00:02:24,840 --> 00:02:29,840
from 16 zetabytes in 2015
to 181 zetabytes of data

41
00:02:32,190 --> 00:02:35,193
floating around at the end of 2025.

42
00:02:36,420 --> 00:02:38,910
The same is true in healthcare data.

43
00:02:38,910 --> 00:02:41,160
Between the imaging data, the lab data,

44
00:02:41,160 --> 00:02:44,670
all the data that is becoming
available in healthcare,

45
00:02:44,670 --> 00:02:48,780
we are projecting a 10
zetabytes of healthcare data

46
00:02:48,780 --> 00:02:51,423
being available at the end of 2025.

47
00:02:53,850 --> 00:02:57,480
This proliferation of data availability

48
00:02:57,480 --> 00:03:00,960
has driven transformative growth,

49
00:03:00,960 --> 00:03:03,540
evolving the use of real-world data

50
00:03:03,540 --> 00:03:07,290
and real-world evidence
from an emerging field

51
00:03:07,290 --> 00:03:11,400
to a mainstream component in
pharmaceutical development

52
00:03:11,400 --> 00:03:13,173
and healthcare decision making.

53
00:03:14,640 --> 00:03:17,010
But while this exponential growth

54
00:03:17,010 --> 00:03:20,010
creates unprecedented opportunities,

55
00:03:20,010 --> 00:03:22,920
it also creates critical challenges

56
00:03:22,920 --> 00:03:27,810
such as how do you organize,
access and make value

57
00:03:27,810 --> 00:03:30,933
from such massive volumes of information?

58
00:03:33,900 --> 00:03:37,020
So before we dive into
more specifics about this,

59
00:03:37,020 --> 00:03:40,530
with a quick show of hands,
how many of you are familiar

60
00:03:40,530 --> 00:03:43,380
with real-world data and how it is used

61
00:03:43,380 --> 00:03:45,243
in healthcare and life sciences?

62
00:03:47,670 --> 00:03:48,783
Excellent. Thank you.

63
00:03:49,650 --> 00:03:52,470
So for those newer to this space,

64
00:03:52,470 --> 00:03:55,500
real-world data is healthcare data

65
00:03:55,500 --> 00:03:58,620
collected during routine patient care,

66
00:03:58,620 --> 00:04:01,440
which means it's everything
from electronic health records

67
00:04:01,440 --> 00:04:05,613
to wearable devices to the
insurance claims we all make.

68
00:04:06,900 --> 00:04:10,140
Real-world evidence is what we learn

69
00:04:10,140 --> 00:04:12,273
from analyzing this data.

70
00:04:13,470 --> 00:04:16,320
This evidence is used to provide insights

71
00:04:16,320 --> 00:04:21,320
into the possible benefits
and risks of medical products.

72
00:04:21,690 --> 00:04:25,920
It's also used to study clinical
conditions and biomarkers

73
00:04:25,920 --> 00:04:27,993
when developing new drugs.

74
00:04:29,460 --> 00:04:32,100
Driven by an increase in demand

75
00:04:32,100 --> 00:04:34,710
for data-backed decision making,

76
00:04:34,710 --> 00:04:37,650
for accelerated drug development,

77
00:04:37,650 --> 00:04:40,080
for value-based care models,

78
00:04:40,080 --> 00:04:43,710
and of course the continual
regulatory expectations

79
00:04:43,710 --> 00:04:48,600
across pharmaceutical,
biotech and med devices,

80
00:04:48,600 --> 00:04:52,530
this increasing demand
for evidence and insight

81
00:04:52,530 --> 00:04:55,650
is driving a surge in the demand

82
00:04:55,650 --> 00:04:58,233
for real-world data-backed studies.

83
00:04:59,430 --> 00:05:01,980
The evidence that is being generated

84
00:05:01,980 --> 00:05:06,780
is used to help boost preventative efforts

85
00:05:06,780 --> 00:05:11,040
to identify patients that
are at risk of illness

86
00:05:11,040 --> 00:05:13,500
or eligible for clinical trials.

87
00:05:13,500 --> 00:05:16,260
It's used to shape clinical trials.

88
00:05:16,260 --> 00:05:20,160
It's also used to create public policy

89
00:05:20,160 --> 00:05:23,310
and expand drug safety testing.

90
00:05:23,310 --> 00:05:25,530
It's used to determine the value

91
00:05:25,530 --> 00:05:30,390
of medical-based intervention
and to establish reimbursement

92
00:05:30,390 --> 00:05:35,250
strategies while supporting
regulatory expectations.

93
00:05:35,250 --> 00:05:38,430
We create this flywheel
because the more data

94
00:05:38,430 --> 00:05:41,670
that is available, the more
evidence is being used,

95
00:05:41,670 --> 00:05:44,733
creating that demand for yet more data.

96
00:05:47,010 --> 00:05:51,510
This is why establishing a strong data

97
00:05:51,510 --> 00:05:54,483
foundation layer is essential.

98
00:05:55,320 --> 00:05:58,890
To effectively use the
data and allow agents

99
00:05:58,890 --> 00:06:03,450
to use the data to generate
these insights and evidence,

100
00:06:03,450 --> 00:06:07,773
we must start with a strong
data foundation layer.

101
00:06:09,090 --> 00:06:12,630
In life sciences, this
includes historical data

102
00:06:12,630 --> 00:06:14,550
that is sitting in your warehouses

103
00:06:14,550 --> 00:06:18,060
or on individual scientists' workstations.

104
00:06:18,060 --> 00:06:22,500
It's experimental data being
generated in your labs,

105
00:06:22,500 --> 00:06:26,250
computational data being
run in simulations.

106
00:06:26,250 --> 00:06:29,040
And then when you take all of your data

107
00:06:29,040 --> 00:06:31,830
and you mix it with third-party data

108
00:06:31,830 --> 00:06:33,990
that will enrich your data,

109
00:06:33,990 --> 00:06:37,140
increasing the breadth and
the depth of the questions

110
00:06:37,140 --> 00:06:41,850
that you can answer with
information previously unavailable,

111
00:06:41,850 --> 00:06:46,850
the ability to make that data
work for you is limitless.

112
00:06:47,880 --> 00:06:52,290
But we have to remember it all starts

113
00:06:52,290 --> 00:06:54,690
with the data foundation layer

114
00:06:54,690 --> 00:06:58,410
because the most
sophisticated AI in the world

115
00:06:58,410 --> 00:07:02,370
will only ever be as smart and effective

116
00:07:02,370 --> 00:07:04,983
as the data that it can access.

117
00:07:08,460 --> 00:07:12,690
Despite this tremendous growth
that we've talked about,

118
00:07:12,690 --> 00:07:16,440
finding the right data,
the data that you need,

119
00:07:16,440 --> 00:07:19,530
continues to be messy and challenging.

120
00:07:19,530 --> 00:07:23,100
It's unpredictable, which
in this case is fitting

121
00:07:23,100 --> 00:07:25,503
for data that comes from the real world.

122
00:07:26,370 --> 00:07:31,370
Today, 40% of data
purchases are incomplete,

123
00:07:33,150 --> 00:07:35,253
which really means ineffective.

124
00:07:36,660 --> 00:07:40,350
And given that real-world
data is collected all along

125
00:07:40,350 --> 00:07:44,610
that patient care journey,
there's no one source,

126
00:07:44,610 --> 00:07:48,210
no one provider that
will have all of the data

127
00:07:48,210 --> 00:07:49,563
that you need.

128
00:07:50,970 --> 00:07:55,970
And yet, 70% of studies
require very specific,

129
00:07:56,670 --> 00:07:59,760
fit for purpose data that is linked

130
00:07:59,760 --> 00:08:02,013
across multiple datasets.

131
00:08:02,970 --> 00:08:06,570
And with HIPAA requirements
that all of our data

132
00:08:06,570 --> 00:08:11,010
remains de-identified,
connecting patient data

133
00:08:11,010 --> 00:08:15,303
across multiple datasets
is extremely challenging.

134
00:08:16,410 --> 00:08:21,390
When you're looking for very
specific fit for purpose data,

135
00:08:21,390 --> 00:08:25,953
this fragmentation creates major hurdles.

136
00:08:27,540 --> 00:08:32,040
For example, it typically
takes four to 12 weeks

137
00:08:32,040 --> 00:08:37,040
just to find and obtain what
looks to be the necessary

138
00:08:37,140 --> 00:08:38,313
third-party data.

139
00:08:39,750 --> 00:08:42,690
This is further complicated
by data producers'

140
00:08:42,690 --> 00:08:46,383
reluctance to share data
outside of their control.

141
00:08:47,580 --> 00:08:49,540
And once you've obtained the data

142
00:08:51,240 --> 00:08:55,830
because each producer will
use their own unique format,

143
00:08:55,830 --> 00:08:58,140
you still have to harmonize that data,

144
00:08:58,140 --> 00:09:01,023
which is gonna add another
four weeks to your process.

145
00:09:02,610 --> 00:09:05,070
And then once you've done all of that

146
00:09:05,070 --> 00:09:08,490
and you have your data,
your different team members

147
00:09:08,490 --> 00:09:10,500
from the computational scientist

148
00:09:10,500 --> 00:09:12,540
to the experimental scientist,

149
00:09:12,540 --> 00:09:14,820
they all need their own tools

150
00:09:14,820 --> 00:09:18,363
to make that data
effectively work for them.

151
00:09:19,620 --> 00:09:24,620
All of these challenges create
timelines exceeding 10 years

152
00:09:26,100 --> 00:09:28,290
and cost in the millions.

153
00:09:28,290 --> 00:09:30,660
Depending on the therapeutic area,

154
00:09:30,660 --> 00:09:34,410
even in the two to $3 billion range

155
00:09:34,410 --> 00:09:37,293
just to put that new drug out to market.

156
00:09:40,950 --> 00:09:45,150
At AWS, we're approaching
this complex challenge

157
00:09:45,150 --> 00:09:48,213
with a comprehensive multi-part solution.

158
00:09:49,170 --> 00:09:51,720
In the first part, shown here,

159
00:09:51,720 --> 00:09:56,310
we are streamlining data
discovery and access.

160
00:09:56,310 --> 00:10:01,050
We've been collaborating
with our AWS partner Datavant

161
00:10:01,050 --> 00:10:05,040
to build a solution on AWS Clean Rooms

162
00:10:05,040 --> 00:10:09,420
that uses Datavant's privacy
preserving technology

163
00:10:09,420 --> 00:10:13,830
to help researchers quickly find, analyze,

164
00:10:13,830 --> 00:10:18,333
and obtain linked patient
level data sources.

165
00:10:20,190 --> 00:10:25,190
In this solution data
producers host their tokenized,

166
00:10:25,740 --> 00:10:30,003
personally identifiable
information in their AWS account.

167
00:10:31,230 --> 00:10:35,640
Data consumers can then
discover and evaluate

168
00:10:35,640 --> 00:10:40,140
multimodal fit for purpose
data across Datavant's

169
00:10:40,140 --> 00:10:42,723
broad network of data producers.

170
00:10:44,190 --> 00:10:47,313
Consumers will then negotiate
directly with the producer,

171
00:10:48,300 --> 00:10:51,450
subscribe to the data
in a private offering

172
00:10:51,450 --> 00:10:55,143
with billing directly
through AWS Data Exchange.

173
00:10:58,950 --> 00:11:03,060
Datavant's proprietary
software takes an individual's

174
00:11:03,060 --> 00:11:06,960
personally identifiable
information such as name,

175
00:11:06,960 --> 00:11:08,883
gender, and date of birth,

176
00:11:11,220 --> 00:11:15,090
and converts it into a
unique string of characters

177
00:11:15,090 --> 00:11:20,013
known as a token that
de-identifies the information.

178
00:11:21,150 --> 00:11:24,570
Now this is a token that is
different than an LLM token.

179
00:11:24,570 --> 00:11:27,720
This token is specific to Datavant,

180
00:11:27,720 --> 00:11:30,633
that is within their proprietary software.

181
00:11:33,420 --> 00:11:36,900
Each token in each dataset

182
00:11:36,900 --> 00:11:39,543
is unique to an individual patient.

183
00:11:40,530 --> 00:11:44,100
And yet, with Datavant's
proprietary software,

184
00:11:44,100 --> 00:11:48,330
consumers are able to
connect patient level data

185
00:11:48,330 --> 00:11:52,530
with this token all without revealing

186
00:11:52,530 --> 00:11:56,433
any underlying personally
identifiable information.

187
00:11:59,520 --> 00:12:03,720
Because that same patient
will always generate

188
00:12:03,720 --> 00:12:08,720
that same encrypted,
nonreversible deterministic token,

189
00:12:10,440 --> 00:12:14,760
that same patient unique token can be used

190
00:12:14,760 --> 00:12:19,590
to link datasets with that
token across all of these

191
00:12:19,590 --> 00:12:22,740
multiple datasets at the patient level,

192
00:12:22,740 --> 00:12:26,133
which will provide that
longitudinal patient journey.

193
00:12:27,720 --> 00:12:32,160
And this tokenization happens
in the data producer's

194
00:12:32,160 --> 00:12:36,300
container behind that
data producer's firewall,

195
00:12:36,300 --> 00:12:39,780
ensuring that no personal
identifiable information

196
00:12:39,780 --> 00:12:44,133
is actually leaving the data
producer's secure system.

197
00:12:47,640 --> 00:12:50,340
Through our collaboration with Datavant,

198
00:12:50,340 --> 00:12:53,190
we have been working with
healthcare organizations

199
00:12:53,190 --> 00:12:57,450
and life sciences companies
to bring this tokenized data

200
00:12:57,450 --> 00:13:02,450
into AWS Clean Rooms,
transforming how we access

201
00:13:02,880 --> 00:13:06,153
and analyze these valuable datasets.

202
00:13:07,650 --> 00:13:12,270
Datavant Connect, powered
by AWS Clean Rooms,

203
00:13:12,270 --> 00:13:15,360
gives data producers the ability

204
00:13:15,360 --> 00:13:20,310
to discover and evaluate fit
for purpose multimodal data

205
00:13:20,310 --> 00:13:23,913
in a HIPAA compliant
privacy preserving manner.

206
00:13:25,410 --> 00:13:30,410
For data producers, this new
discovery and evaluation method

207
00:13:30,600 --> 00:13:34,680
allows them to share their
data with data consumers

208
00:13:34,680 --> 00:13:38,190
while maintaining complete control.

209
00:13:38,190 --> 00:13:41,370
Because no data has actually moved

210
00:13:41,370 --> 00:13:43,710
in between these environments,

211
00:13:43,710 --> 00:13:46,110
data consumers retain the ability

212
00:13:46,110 --> 00:13:50,250
to share as much or as little of the data

213
00:13:50,250 --> 00:13:52,833
as that data producer deems necessary.

214
00:13:53,850 --> 00:13:58,170
For data consumers,
multiple personas are able

215
00:13:58,170 --> 00:14:02,310
to discover and evaluate
fit for purpose data

216
00:14:02,310 --> 00:14:05,490
with an easy to use front end agent

217
00:14:05,490 --> 00:14:09,030
that acts as their own virtual assistant,

218
00:14:09,030 --> 00:14:12,453
helping to ensure that
they find the right data.

219
00:14:15,900 --> 00:14:17,853
So how does this all work?

220
00:14:19,800 --> 00:14:23,730
Data sources host their tokenized,

221
00:14:23,730 --> 00:14:28,423
personally identifiable
information in the AWS account.

222
00:14:30,990 --> 00:14:34,263
They register directly with
the Datavant Connect platform.

223
00:14:35,220 --> 00:14:37,950
They do this by creating a Glue database

224
00:14:37,950 --> 00:14:41,400
and a Glue table that has pointers

225
00:14:41,400 --> 00:14:44,643
to their S3 bucket and table schema.

226
00:14:45,810 --> 00:14:49,590
Consumers then evaluating
different datasets

227
00:14:49,590 --> 00:14:53,340
will use AWS Clean Rooms as a secure,

228
00:14:53,340 --> 00:14:56,280
neutral evaluation space.

229
00:14:56,280 --> 00:14:58,440
Again, meaning that no data

230
00:14:58,440 --> 00:15:01,203
is moving between these environments.

231
00:15:02,430 --> 00:15:04,710
While researchers are able to evaluate

232
00:15:04,710 --> 00:15:07,950
multiple datasets simultaneously,

233
00:15:07,950 --> 00:15:11,253
drastically reducing evaluation time,

234
00:15:12,270 --> 00:15:16,200
data producers don't
have to manually update

235
00:15:16,200 --> 00:15:19,770
their information in the
Datavant Connect platform

236
00:15:19,770 --> 00:15:23,520
because that live connection
with AWS Clean Rooms

237
00:15:23,520 --> 00:15:27,213
ensures that their latest
data is always available.

238
00:15:28,680 --> 00:15:31,918
Then once the right data is found,

239
00:15:31,918 --> 00:15:35,400
Datavant will certify the
data and it can be seamlessly

240
00:15:35,400 --> 00:15:38,403
procured through AWS Data Exchange.

241
00:15:41,610 --> 00:15:45,180
Pilot customers have been
testing this solution

242
00:15:45,180 --> 00:15:48,330
and they've been sharing how
they are unlocking access

243
00:15:48,330 --> 00:15:53,330
to data that once available
is accelerating timelines

244
00:15:54,060 --> 00:15:55,773
like never before.

245
00:15:59,670 --> 00:16:03,183
For the second part of this
solution, shown here in blue,

246
00:16:04,320 --> 00:16:07,320
we have been working with
partners such as Aetion

247
00:16:07,320 --> 00:16:09,570
and their Activate platform,

248
00:16:09,570 --> 00:16:11,790
Manifold and their data layer,

249
00:16:11,790 --> 00:16:16,790
and Panalgo with the IHD Cloud,
to enable faster insights

250
00:16:17,610 --> 00:16:22,610
through modern, scalable
data harmonization platforms,

251
00:16:22,800 --> 00:16:26,310
which allows us to support both technical

252
00:16:26,310 --> 00:16:27,993
and non-technical users.

253
00:16:28,950 --> 00:16:33,720
Technical users like
biostatisticians and data scientists

254
00:16:33,720 --> 00:16:38,190
will use open source
tools like R and Python,

255
00:16:38,190 --> 00:16:41,340
but non-technical users can access

256
00:16:41,340 --> 00:16:45,090
these agentic capabilities
to build cohorts

257
00:16:45,090 --> 00:16:48,390
applying that inclusion exclusion criteria

258
00:16:48,390 --> 00:16:51,423
when analyzing that
longitudinal patient journey.

259
00:16:54,840 --> 00:16:57,720
Through our collaboration with Eli Lilly,

260
00:16:57,720 --> 00:17:02,720
we have developed this agent
that allows non-technical users

261
00:17:04,560 --> 00:17:08,280
to analyze complex healthcare datasets

262
00:17:08,280 --> 00:17:10,053
using natural language.

263
00:17:10,950 --> 00:17:14,820
Built on AWS's AI stack,

264
00:17:14,820 --> 00:17:19,820
our agent seamlessly
connects to data sources

265
00:17:20,910 --> 00:17:25,293
across Redshift, S3,
Athena and Databricks.

266
00:17:28,860 --> 00:17:33,660
Now with data delivered
directly to the S3 bucket,

267
00:17:33,660 --> 00:17:36,840
with automated harmonization,

268
00:17:36,840 --> 00:17:40,500
technical users continue
using R and Python

269
00:17:40,500 --> 00:17:43,620
and non-technical users
can access the agent

270
00:17:43,620 --> 00:17:47,580
to act as a persona-driven
virtual assistant

271
00:17:47,580 --> 00:17:49,140
making sense of the data

272
00:17:49,140 --> 00:17:52,473
and providing insights instantaneously,

273
00:17:53,370 --> 00:17:57,483
all within a responsible,
auditable framework.

274
00:17:58,920 --> 00:18:01,440
This streamlined approach eliminates

275
00:18:01,440 --> 00:18:06,120
the traditional bottlenecks
while maintaining data security

276
00:18:06,120 --> 00:18:08,190
and data privacy.

277
00:18:08,190 --> 00:18:10,950
And what used to take four months

278
00:18:10,950 --> 00:18:14,853
now can be done in under four weeks.

279
00:18:18,780 --> 00:18:22,710
As we continue to navigate
the evolving field

280
00:18:22,710 --> 00:18:25,716
of real-world data and evidence,

281
00:18:25,716 --> 00:18:30,510
AWS continues to actively
collaborate with multiple partners

282
00:18:30,510 --> 00:18:34,710
and customers to tackle
the complex challenges

283
00:18:34,710 --> 00:18:39,210
of real-world data discovery and analysis.

284
00:18:39,210 --> 00:18:44,210
Companies such as Manifold,
Deloitte, Panalgo, Aetion,

285
00:18:45,120 --> 00:18:50,120
EPAM and Atropos are all
leveraging AWS infrastructure

286
00:18:51,150 --> 00:18:54,510
and incorporating AI agents

287
00:18:54,510 --> 00:18:58,143
to transform real-world data analysis.

288
00:18:59,370 --> 00:19:03,000
It's important to remember these solutions

289
00:19:03,000 --> 00:19:06,690
are not just about data processing.

290
00:19:06,690 --> 00:19:10,350
These solutions are about intelligent,

291
00:19:10,350 --> 00:19:15,000
automated decision making
that is identifying patterns,

292
00:19:15,000 --> 00:19:18,690
generating evidence and
insights that is accelerating

293
00:19:18,690 --> 00:19:23,163
that path from data through
evidence to discovery.

294
00:19:26,430 --> 00:19:28,800
But as we stated earlier,

295
00:19:28,800 --> 00:19:32,193
it all starts with the
data foundation layer.

296
00:19:33,270 --> 00:19:37,230
Because even the most
sophisticated AI in the world

297
00:19:37,230 --> 00:19:41,250
will only ever be as smart and effective

298
00:19:41,250 --> 00:19:43,593
as the data that it can access.

299
00:19:48,270 --> 00:19:49,950
So with that,

300
00:19:49,950 --> 00:19:52,590
I am really excited to welcome

301
00:19:52,590 --> 00:19:55,050
our next speaker to the stage,

302
00:19:55,050 --> 00:19:56,940
who will share firsthand experience

303
00:19:56,940 --> 00:19:58,863
of putting this technology to work.

304
00:20:01,560 --> 00:20:04,680
It is my pleasure to
introduce Greg Cunningham,

305
00:20:04,680 --> 00:20:08,073
Senior Director of Real-World
Data from Eli Lilly.

306
00:20:09,120 --> 00:20:11,880
Greg brings an impressive 12 years

307
00:20:11,880 --> 00:20:16,263
of real-world data across
Lilly's pharmaceutical portfolio.

308
00:20:17,760 --> 00:20:20,160
Greg's insights will be invaluable

309
00:20:20,160 --> 00:20:22,800
as he shares their real-life experience

310
00:20:22,800 --> 00:20:26,940
and concrete examples of
using this technology.

311
00:20:26,940 --> 00:20:31,112
So please join me in giving
a warm welcome to Greg.

312
00:20:31,112 --> 00:20:34,279
(audience applauding)

313
00:20:36,810 --> 00:20:38,280
- Thank you, Anne.

314
00:20:38,280 --> 00:20:39,204
It's great to be here.

315
00:20:39,204 --> 00:20:41,703
This is the first time
I've been at re:Invent.

316
00:20:42,690 --> 00:20:45,780
Like Anne said, I work
at Eli Lilly & Company.

317
00:20:45,780 --> 00:20:47,520
We're a pharmaceutical company

318
00:20:47,520 --> 00:20:49,323
based in Indianapolis, Indiana.

319
00:20:51,090 --> 00:20:55,890
Unlike my four sons, I've been
at one company for 39 years.

320
00:20:55,890 --> 00:20:58,980
I've been the last 12
in real-world evidence

321
00:20:58,980 --> 00:21:02,100
responsible for our licensing and data

322
00:21:02,100 --> 00:21:06,810
and also the data platforms
that we have the data on.

323
00:21:06,810 --> 00:21:10,383
Prior to that, I worked in
our clinical trial area,

324
00:21:11,280 --> 00:21:13,773
working on new drugs that
made it to the market.

325
00:21:16,290 --> 00:21:19,800
And today I want to talk, like Anne said,

326
00:21:19,800 --> 00:21:21,780
about kind of a couple of things,

327
00:21:21,780 --> 00:21:24,720
the business need for
a scientific company,

328
00:21:24,720 --> 00:21:28,830
but I think this goes beyond
just a scientific company.

329
00:21:28,830 --> 00:21:31,680
I think any company can follow the pattern

330
00:21:31,680 --> 00:21:36,243
that we have done with AWS,
so we'll dive into that.

331
00:21:37,080 --> 00:21:40,770
In pharma companies, the
gold standard for data,

332
00:21:40,770 --> 00:21:44,010
that data foundation,
is clinical trial data.

333
00:21:44,010 --> 00:21:48,540
This data is very clean, it
follows very rigid standards.

334
00:21:48,540 --> 00:21:52,503
It's really looked after to
make sure it's high quality.

335
00:21:54,180 --> 00:21:55,620
And when I first started with Lilly,

336
00:21:55,620 --> 00:21:58,290
we would do a lot of phase four studies

337
00:21:58,290 --> 00:22:00,030
after we got approved.

338
00:22:00,030 --> 00:22:02,850
These studies helped us understand

339
00:22:02,850 --> 00:22:04,500
what was going on with the patient

340
00:22:04,500 --> 00:22:06,330
after the drug was approved

341
00:22:06,330 --> 00:22:08,130
'cause there was really
no other way to know

342
00:22:08,130 --> 00:22:10,620
what was going on at that time.

343
00:22:10,620 --> 00:22:14,190
But then healthcare
changed with the, you know,

344
00:22:14,190 --> 00:22:17,880
the healthcare records going
from paper to electronic.

345
00:22:17,880 --> 00:22:20,250
If you go to a doctor's office today,

346
00:22:20,250 --> 00:22:22,230
they aren't writing a lot down on paper,

347
00:22:22,230 --> 00:22:25,410
they're doing most of it in tablet

348
00:22:25,410 --> 00:22:29,040
or computer to enter directly.

349
00:22:29,040 --> 00:22:33,480
So we found a new source
of healthcare research data

350
00:22:33,480 --> 00:22:36,690
that allowed us to stop doing costly

351
00:22:36,690 --> 00:22:39,030
and slow clinical trials,

352
00:22:39,030 --> 00:22:44,030
to do more real-time work
of ingesting data like this

353
00:22:44,070 --> 00:22:46,380
that I'll talk about in a minute.

354
00:22:46,380 --> 00:22:49,350
But, first, what is real-world evidence?

355
00:22:49,350 --> 00:22:53,460
I have a definition that the FDA uses

356
00:22:53,460 --> 00:22:56,610
and, you know, it's clinical evidence

357
00:22:56,610 --> 00:22:59,460
about the use and the potential benefits

358
00:22:59,460 --> 00:23:02,490
and risk of a product derived solely

359
00:23:02,490 --> 00:23:07,490
from these electronic
healthcare sources in the system

360
00:23:07,920 --> 00:23:10,470
that we all use in our
U.S. healthcare system

361
00:23:10,470 --> 00:23:12,393
and in other countries too now.

362
00:23:13,410 --> 00:23:17,910
And at Lilly, we kind of have
formulated our own formula.

363
00:23:17,910 --> 00:23:21,270
To get this evidence you see to the right,

364
00:23:21,270 --> 00:23:23,610
there's three key ingredients.

365
00:23:23,610 --> 00:23:25,860
First, you have to have
a scientific question

366
00:23:25,860 --> 00:23:29,820
or a hypothesis you want to test.

367
00:23:29,820 --> 00:23:32,490
The foundational thing I
think Anne covered really well

368
00:23:32,490 --> 00:23:33,780
is the data.

369
00:23:33,780 --> 00:23:38,640
Getting the right data, our
group spends time helping people

370
00:23:38,640 --> 00:23:41,940
get the data that has the best
quality for their question,

371
00:23:41,940 --> 00:23:46,260
but also looking at
what variables they need

372
00:23:46,260 --> 00:23:48,390
and ensuring those variables are present

373
00:23:48,390 --> 00:23:51,120
in the datasets they use.

374
00:23:51,120 --> 00:23:54,810
And then, finally, having
a statistical analysis plan

375
00:23:54,810 --> 00:23:57,690
and then the execution of the analytics.

376
00:23:57,690 --> 00:24:01,140
This RWE is increasingly more vital

377
00:24:01,140 --> 00:24:03,540
and important to life science companies

378
00:24:03,540 --> 00:24:08,070
because we wanna have
actionable real-time insights

379
00:24:08,070 --> 00:24:09,480
to what's happening to our drug

380
00:24:09,480 --> 00:24:11,760
now that its left the clinical trial area

381
00:24:11,760 --> 00:24:14,340
and it's being used out in the real world,

382
00:24:14,340 --> 00:24:16,260
and the results are not always the same

383
00:24:16,260 --> 00:24:18,000
and there's sometimes surprises

384
00:24:18,000 --> 00:24:20,313
on what patients you're
getting in the market.

385
00:24:22,920 --> 00:24:25,080
Real-world evidence is used

386
00:24:25,080 --> 00:24:29,073
across the whole product
lifecycle that we have.

387
00:24:30,270 --> 00:24:33,180
You know, when I look at the uses,

388
00:24:33,180 --> 00:24:35,490
we use it in our early phase discovery

389
00:24:35,490 --> 00:24:38,580
as we're trying to discover new molecules,

390
00:24:38,580 --> 00:24:40,920
to we use it in our clinical trials

391
00:24:40,920 --> 00:24:43,803
and we use it post-launch,
like I mentioned earlier.

392
00:24:44,670 --> 00:24:48,270
The clinical trial examples I can give,

393
00:24:48,270 --> 00:24:52,530
we have protocols that we
are always trying to improve

394
00:24:52,530 --> 00:24:56,820
to make them easier for the
physician to do the trial.

395
00:24:56,820 --> 00:24:58,950
And in doing that we
use real-world evidence

396
00:24:58,950 --> 00:25:03,420
to identify what's the best, you know,

397
00:25:03,420 --> 00:25:06,180
inclusion and exclusion
criterias we can have?

398
00:25:06,180 --> 00:25:09,570
It's best to use real data
to then make those decisions

399
00:25:09,570 --> 00:25:12,903
at what cut points you get
patients with the disease.

400
00:25:14,250 --> 00:25:18,393
And the health outcomes
function that I am part of,

401
00:25:19,636 --> 00:25:21,510
we do observational studies.

402
00:25:21,510 --> 00:25:23,220
So these studies,

403
00:25:23,220 --> 00:25:25,890
we're looking to demonstrate
what's the economic value

404
00:25:25,890 --> 00:25:30,270
of the product and what's the
comparative effectiveness?

405
00:25:30,270 --> 00:25:33,060
We'll look at one of our
drugs versus the competitor's.

406
00:25:33,060 --> 00:25:34,830
We know the clinical trial results.

407
00:25:34,830 --> 00:25:37,890
We also will publish papers to show

408
00:25:37,890 --> 00:25:41,100
is this drug performing the same or higher

409
00:25:41,100 --> 00:25:43,170
or lower in the real world?

410
00:25:43,170 --> 00:25:45,750
And our end goal with that exercise

411
00:25:45,750 --> 00:25:48,450
is to publish in scientific journals.

412
00:25:48,450 --> 00:25:51,480
And this is one of the
high-priority things

413
00:25:51,480 --> 00:25:53,970
we do in our area.

414
00:25:53,970 --> 00:25:56,250
But after you launch in the market,

415
00:25:56,250 --> 00:25:59,130
we also use it to look at
patient characteristics.

416
00:25:59,130 --> 00:26:00,870
Who did we think we were gonna get

417
00:26:00,870 --> 00:26:03,510
in our marketing plans and who actually

418
00:26:03,510 --> 00:26:06,273
is taking your drug and
how's it performing?

419
00:26:09,000 --> 00:26:11,370
There are a variety of RWD types.

420
00:26:11,370 --> 00:26:14,760
Anne talked about it at
kind of a macro level.

421
00:26:14,760 --> 00:26:18,600
I'll give you some
perspectives of how we see data

422
00:26:18,600 --> 00:26:20,073
and using it today.

423
00:26:21,180 --> 00:26:25,470
The last 10 to 15 years we've
used that foundational data

424
00:26:25,470 --> 00:26:28,020
you see on the screen.

425
00:26:28,020 --> 00:26:32,730
It's been the tried and true
data for real-world data.

426
00:26:32,730 --> 00:26:36,060
Electronic health records,
as it became electronic.

427
00:26:36,060 --> 00:26:38,160
It's got great clinical variables

428
00:26:38,160 --> 00:26:40,230
for people to use to understand.

429
00:26:40,230 --> 00:26:42,213
Lab data is usually in there.

430
00:26:43,050 --> 00:26:46,320
The the one we use the
most in health outcomes

431
00:26:46,320 --> 00:26:48,420
is claims and billing data from insurance

432
00:26:48,420 --> 00:26:51,450
because it gives you every
healthcare transaction

433
00:26:51,450 --> 00:26:54,930
someone had over a long period of time.

434
00:26:54,930 --> 00:26:58,740
'Cause a lot of us are with
the same insurer over time,

435
00:26:58,740 --> 00:27:01,740
it's very nice to give you that long view.

436
00:27:01,740 --> 00:27:04,320
I would say that the foundational data

437
00:27:04,320 --> 00:27:07,020
is typically broad, it's all diseases

438
00:27:07,020 --> 00:27:09,240
because they're big databases

439
00:27:09,240 --> 00:27:12,120
and that's what they're
used for predominantly.

440
00:27:12,120 --> 00:27:14,850
The most exciting thing
in the last few years

441
00:27:14,850 --> 00:27:17,250
is this emerging data.

442
00:27:17,250 --> 00:27:19,710
Our holy grail is to have data

443
00:27:19,710 --> 00:27:21,870
that really is disease specific,

444
00:27:21,870 --> 00:27:24,120
that tells you is the
disease getting worse

445
00:27:24,120 --> 00:27:25,950
or is it getting better?

446
00:27:25,950 --> 00:27:28,950
So wearables, sensors and health apps

447
00:27:28,950 --> 00:27:31,050
are really, really critical.

448
00:27:31,050 --> 00:27:35,307
An example of how pharma
companies use, you know,

449
00:27:36,240 --> 00:27:39,660
an app, I've seen migraine studies

450
00:27:39,660 --> 00:27:44,460
where they've had people using the app

451
00:27:44,460 --> 00:27:46,110
who are migraine patients.

452
00:27:46,110 --> 00:27:48,000
They enter every time they have a migraine

453
00:27:48,000 --> 00:27:49,710
and they enter the severity.

454
00:27:49,710 --> 00:27:51,240
Well then that gives us the ability

455
00:27:51,240 --> 00:27:53,550
to see what drug they were on

456
00:27:53,550 --> 00:27:57,450
and then which drug is
actually reducing the severity

457
00:27:57,450 --> 00:28:00,420
and or the amount of migraines.

458
00:28:00,420 --> 00:28:05,070
So this merging of broad
data and this deep,

459
00:28:05,070 --> 00:28:07,773
rich data is extremely helpful.

460
00:28:08,760 --> 00:28:11,520
As the human genome is
continuing to evolve

461
00:28:11,520 --> 00:28:14,580
biomarkers and genomic data are critical.

462
00:28:14,580 --> 00:28:18,930
Our oncology team uses a lot of this data

463
00:28:18,930 --> 00:28:21,630
with our real-world data to match up

464
00:28:21,630 --> 00:28:26,250
and see which biomarkers
are really making an impact

465
00:28:26,250 --> 00:28:30,213
with our drugs and are modifying
the disease or treating it.

466
00:28:31,350 --> 00:28:33,600
The last thing Anne
talked about the token,

467
00:28:33,600 --> 00:28:36,540
so I won't kind of talk about the process,

468
00:28:36,540 --> 00:28:39,900
but the token allowed
us to take this broad,

469
00:28:39,900 --> 00:28:42,510
longitudinal data and match it up

470
00:28:42,510 --> 00:28:46,620
with deep and rich data from the disease

471
00:28:46,620 --> 00:28:50,640
and we're able to do
much, much richer analysis

472
00:28:50,640 --> 00:28:53,400
that is helping us understand
the patient journey

473
00:28:53,400 --> 00:28:57,483
that people are on and how do
we help impact their lives.

474
00:29:01,170 --> 00:29:04,830
The acquiring of real-world
data and ingesting it

475
00:29:04,830 --> 00:29:08,040
is a process that I think you're starting

476
00:29:08,040 --> 00:29:11,070
to see Datavant and AWS really crack

477
00:29:11,070 --> 00:29:13,293
and change where this is going.

478
00:29:14,340 --> 00:29:16,950
Our process isn't much
different that you see here

479
00:29:16,950 --> 00:29:19,320
from other pharma companies.

480
00:29:19,320 --> 00:29:22,590
We spend time, okay, what's
the scientific question?

481
00:29:22,590 --> 00:29:25,140
What's that need that someone has?

482
00:29:25,140 --> 00:29:27,690
And then our team, the
real-world data team,

483
00:29:27,690 --> 00:29:31,380
looks at how do you evaluate what data

484
00:29:31,380 --> 00:29:32,610
is gonna best meet that need?

485
00:29:32,610 --> 00:29:35,880
We have data already
purchased and licensed inside

486
00:29:35,880 --> 00:29:37,470
versus the external market,

487
00:29:37,470 --> 00:29:40,590
which is changing every week.

488
00:29:40,590 --> 00:29:42,750
So we spend, on those first two boxes,

489
00:29:42,750 --> 00:29:46,050
I've seen it take, I think,
Anne, your numbers was months.

490
00:29:46,050 --> 00:29:49,500
I've actually seen it take
up to a year at times.

491
00:29:49,500 --> 00:29:50,730
So we're trying to figure out

492
00:29:50,730 --> 00:29:53,220
how can you do this better and faster?

493
00:29:53,220 --> 00:29:56,040
Once you've made the
decision, the transfer,

494
00:29:56,040 --> 00:29:58,350
ingestion and moving the data,

495
00:29:58,350 --> 00:30:02,310
we use Redshift database for our analysis.

496
00:30:02,310 --> 00:30:05,861
Those things we're real
excited 'cause the Datavant,

497
00:30:05,861 --> 00:30:10,140
AWS solution they're
looking to put in place

498
00:30:10,140 --> 00:30:13,860
will take that from, I've seen
this take a couple of months

499
00:30:13,860 --> 00:30:16,560
and, you know, if this can
get down to a month or less,

500
00:30:16,560 --> 00:30:19,290
it allows us to use more current data,

501
00:30:19,290 --> 00:30:21,060
real-time data, faster.

502
00:30:21,060 --> 00:30:24,420
And so we're real excited
with where the technology

503
00:30:24,420 --> 00:30:25,953
is going in this space.

504
00:30:29,520 --> 00:30:31,650
You know, I've talked
about real-world evidence.

505
00:30:31,650 --> 00:30:33,750
I've talked about real-world data.

506
00:30:33,750 --> 00:30:36,840
To set the context of the challenges

507
00:30:36,840 --> 00:30:41,730
we're facing in generating
evidence from real-world data.

508
00:30:41,730 --> 00:30:44,310
Having been in this
space more than a decade,

509
00:30:44,310 --> 00:30:48,240
I've seen us try to hire analysts,

510
00:30:48,240 --> 00:30:53,240
statisticians, data scientists,
and we have made huge leaps,

511
00:30:54,210 --> 00:30:57,390
but we're still dependent on analytic pros

512
00:30:57,390 --> 00:31:00,600
who are those data
scientists and statisticians.

513
00:31:00,600 --> 00:31:03,840
These end users, our scientists,

514
00:31:03,840 --> 00:31:08,310
use and count on these folks
to do those publications,

515
00:31:08,310 --> 00:31:10,560
to do regulated work.

516
00:31:10,560 --> 00:31:12,930
So for us, it's the core,

517
00:31:12,930 --> 00:31:15,240
it's the most important work we do.

518
00:31:15,240 --> 00:31:18,090
We have trouble just staffing that

519
00:31:18,090 --> 00:31:19,980
with all the improvements we've done

520
00:31:19,980 --> 00:31:22,680
with all the changes we've done,

521
00:31:22,680 --> 00:31:25,260
that is really keeping us busy.

522
00:31:25,260 --> 00:31:27,000
And we've added more and more people

523
00:31:27,000 --> 00:31:28,680
and it's not solving the problem.

524
00:31:28,680 --> 00:31:31,350
So we took on...

525
00:31:31,350 --> 00:31:33,600
I'll talk in a minute on the next slide

526
00:31:33,600 --> 00:31:36,003
about what we did to try to address this.

527
00:31:36,990 --> 00:31:38,160
The second thing we've had,

528
00:31:38,160 --> 00:31:39,750
it's a significant learning curve.

529
00:31:39,750 --> 00:31:44,700
This data is not collected
for like a clinical trial.

530
00:31:44,700 --> 00:31:46,290
This data is collected for insurance,

531
00:31:46,290 --> 00:31:49,710
it's collected for your
doctor's notes, for billing.

532
00:31:49,710 --> 00:31:51,030
It is not meant for this.

533
00:31:51,030 --> 00:31:53,730
So it is not as clean and accurate.

534
00:31:53,730 --> 00:31:57,390
And with that and then
the completeness issues,

535
00:31:57,390 --> 00:31:59,220
this data is much more difficult.

536
00:31:59,220 --> 00:32:01,140
And when new analysts
or new statisticians,

537
00:32:01,140 --> 00:32:03,030
it takes up to six months for them

538
00:32:03,030 --> 00:32:05,043
to be really productive in this space.

539
00:32:05,970 --> 00:32:08,370
The third issue is real-world data

540
00:32:08,370 --> 00:32:10,260
does not have clear standards.

541
00:32:10,260 --> 00:32:12,153
There are several floating around.

542
00:32:13,080 --> 00:32:16,560
Not one of 'em has just not
taken over and made it easy.

543
00:32:16,560 --> 00:32:19,680
And this hinders the ability to make tools

544
00:32:19,680 --> 00:32:23,313
or write standard code that
can be used and leveraged.

545
00:32:26,370 --> 00:32:28,530
So what we did, to expand on,

546
00:32:28,530 --> 00:32:31,830
we had the first two columns you see.

547
00:32:31,830 --> 00:32:35,340
Our original users were
doing complex coding

548
00:32:35,340 --> 00:32:38,130
using AWS Cloud platform.

549
00:32:38,130 --> 00:32:41,223
We've done a lot of
great work in this space.

550
00:32:42,780 --> 00:32:45,270
We then said, okay, we've
gotta do something different

551
00:32:45,270 --> 00:32:46,920
because we can't get enough work.

552
00:32:46,920 --> 00:32:48,360
We took some of our scientists

553
00:32:48,360 --> 00:32:51,540
who actually did coding in college,

554
00:32:51,540 --> 00:32:53,670
they did their own analysis.

555
00:32:53,670 --> 00:32:57,690
We'd let them use an analytic
tool that's on the market.

556
00:32:57,690 --> 00:32:59,220
It's a low-code environment.

557
00:32:59,220 --> 00:33:03,720
They're able to enter parameters
and get output themselves,

558
00:33:03,720 --> 00:33:05,490
answer questions themselves.

559
00:33:05,490 --> 00:33:10,490
So to the left, we do those publications,

560
00:33:10,950 --> 00:33:13,800
we do those regulated activities.

561
00:33:13,800 --> 00:33:17,040
This middle is to let them
generate their own answers,

562
00:33:17,040 --> 00:33:18,810
but we found that not all the scientists

563
00:33:18,810 --> 00:33:20,250
have the same background,

564
00:33:20,250 --> 00:33:22,953
it's not easy for them all to use a tool.

565
00:33:23,940 --> 00:33:27,960
So we worked with Amazon to
say, how can we change the game?

566
00:33:27,960 --> 00:33:31,110
How can we have a no-code environment

567
00:33:31,110 --> 00:33:33,990
where people can enter a text question

568
00:33:33,990 --> 00:33:37,830
to get results running across our data?

569
00:33:37,830 --> 00:33:41,190
So we've been working
on this for over a year

570
00:33:41,190 --> 00:33:46,190
and it has come to fruition,
we're going live next week.

571
00:33:46,590 --> 00:33:48,540
And those scientist users are able

572
00:33:48,540 --> 00:33:51,360
to get questions answered themselves.

573
00:33:51,360 --> 00:33:53,310
They can even do it in a meeting

574
00:33:53,310 --> 00:33:56,167
where someone on the clinical team asks,

575
00:33:56,167 --> 00:33:59,880
"Hey, what's the percent of
patients that have disease X?"

576
00:33:59,880 --> 00:34:02,580
Or, "What's the most
common drug in this space?"

577
00:34:02,580 --> 00:34:05,070
These are questions that
we don't have time to give

578
00:34:05,070 --> 00:34:07,710
to a statistician and it
may take a week or longer

579
00:34:07,710 --> 00:34:10,080
for them do it, if they
can get to it at all.

580
00:34:10,080 --> 00:34:13,413
So this is a way to let
self-service take over.

581
00:34:14,610 --> 00:34:16,980
I would say the biggest
things we've learned

582
00:34:16,980 --> 00:34:20,850
through doing this is the agent,

583
00:34:20,850 --> 00:34:25,050
the LLMs and all those
things, they do great work,

584
00:34:25,050 --> 00:34:28,200
but they need context,
they need business context

585
00:34:28,200 --> 00:34:31,560
that's in my head, it's in our analyst,

586
00:34:31,560 --> 00:34:33,030
our statisticians head.

587
00:34:33,030 --> 00:34:35,310
So, you know, really putting those things

588
00:34:35,310 --> 00:34:37,650
that run before the agents

589
00:34:37,650 --> 00:34:39,780
so that we can put that context in

590
00:34:39,780 --> 00:34:42,303
and allow the agent to write better code.

591
00:34:43,590 --> 00:34:45,600
The second thing we learned in the testing

592
00:34:45,600 --> 00:34:50,190
was your questions, the more general

593
00:34:50,190 --> 00:34:53,370
the more assumptions you're
gonna have from your agent

594
00:34:53,370 --> 00:34:55,770
because it's trying to figure
out what you're saying.

595
00:34:55,770 --> 00:34:58,470
So we've told our users be very specific.

596
00:34:58,470 --> 00:35:01,560
If you want data from
the prescription claims,

597
00:35:01,560 --> 00:35:03,150
say that in your question.

598
00:35:03,150 --> 00:35:07,020
Don't leave it to be hanging out there.

599
00:35:07,020 --> 00:35:10,290
And this has produced a greater accuracy.

600
00:35:10,290 --> 00:35:15,120
The last thing I would say is
as we had people do questions,

601
00:35:15,120 --> 00:35:17,550
I thought you'd just put
in a big block of questions

602
00:35:17,550 --> 00:35:19,980
and it would spit out
the answers and enter it,

603
00:35:19,980 --> 00:35:21,840
get your answers, move on.

604
00:35:21,840 --> 00:35:23,280
It's a dialogue.

605
00:35:23,280 --> 00:35:26,220
You have to ask question,

606
00:35:26,220 --> 00:35:28,410
then ask another question
and another question.

607
00:35:28,410 --> 00:35:31,480
And the answers and the
accuracy of those answers

608
00:35:32,460 --> 00:35:35,220
have gotten better because of that.

609
00:35:35,220 --> 00:35:38,610
Scientists by nature are very skeptical.

610
00:35:38,610 --> 00:35:41,130
So we're worried about releasing a tool

611
00:35:41,130 --> 00:35:44,100
that people who are
very skeptical will say,

612
00:35:44,100 --> 00:35:45,720
well, is this answer right?

613
00:35:45,720 --> 00:35:46,830
And I've had that question

614
00:35:46,830 --> 00:35:49,290
four times already from our scientists.

615
00:35:49,290 --> 00:35:52,350
And so we're gonna do some
tests of real questions

616
00:35:52,350 --> 00:35:54,780
that they ask as we go live,

617
00:35:54,780 --> 00:35:57,810
to just give them a little more sense of,

618
00:35:57,810 --> 00:35:59,370
yes, this is on the right path.

619
00:35:59,370 --> 00:36:02,790
The code being written,
the SQL code being written,

620
00:36:02,790 --> 00:36:05,010
has actually been very
good and it's improved

621
00:36:05,010 --> 00:36:07,350
every step of the way so far.

622
00:36:07,350 --> 00:36:12,270
So we're very, very excited to
go live with this next week.

623
00:36:12,270 --> 00:36:15,393
We did a pilot group,
and with the pilot group,

624
00:36:16,440 --> 00:36:19,500
our intent was to take people
who are more tech savvy,

625
00:36:19,500 --> 00:36:21,660
take people who were into this

626
00:36:21,660 --> 00:36:25,350
and they're gonna be the
soft launch that we go with

627
00:36:25,350 --> 00:36:27,870
and then we'll go with a
hard launch with the people

628
00:36:27,870 --> 00:36:30,990
maybe who are less techy and those people

629
00:36:30,990 --> 00:36:33,360
who sit with them can work with them more.

630
00:36:33,360 --> 00:36:34,950
So I think that's been
some of the learnings

631
00:36:34,950 --> 00:36:36,483
we've seen so far.

632
00:36:39,870 --> 00:36:41,310
So I will finish here,

633
00:36:41,310 --> 00:36:44,730
and I'm very excited to have Eric Brooks

634
00:36:44,730 --> 00:36:47,310
join us and give his firsthand experience

635
00:36:47,310 --> 00:36:50,220
of implementing what we've just discussed.

636
00:36:50,220 --> 00:36:53,100
Eric is a Principal Solution
Architect from Amazon,

637
00:36:53,100 --> 00:36:54,030
and he has worked with us

638
00:36:54,030 --> 00:36:57,660
over the last year and a
half to bring this to life.

639
00:36:57,660 --> 00:36:59,090
Please give Eric a warm welcome.

640
00:36:59,090 --> 00:37:02,257
(audience applauding)

641
00:37:04,098 --> 00:37:05,550
- Hey everyone.

642
00:37:05,550 --> 00:37:07,050
Thanks, Greg. I appreciate it.

643
00:37:07,890 --> 00:37:11,100
You know, what we've seen
so far is this journey

644
00:37:11,100 --> 00:37:14,310
from really data acquisition.

645
00:37:14,310 --> 00:37:16,620
You know, we're buying the
data from various providers,

646
00:37:16,620 --> 00:37:19,410
we're loading it into
Redshift or S3, you know,

647
00:37:19,410 --> 00:37:22,110
there we're governing
access with permissions

648
00:37:22,110 --> 00:37:25,950
through Redshift access
through SageMaker lakehouse.

649
00:37:25,950 --> 00:37:28,950
And we've gotten to the point
now we've made data available,

650
00:37:29,820 --> 00:37:32,070
but we have to tackle this complex,

651
00:37:32,070 --> 00:37:36,180
multimodal, you know, disparate
dataset kind of landscape

652
00:37:36,180 --> 00:37:38,490
because, you know, as Greg described,

653
00:37:38,490 --> 00:37:39,930
what we're not talking about here

654
00:37:39,930 --> 00:37:42,210
is a simple sales dataset.

655
00:37:42,210 --> 00:37:45,150
This is not simple data
that can simply be queried

656
00:37:45,150 --> 00:37:46,530
by a text to SQL agent.

657
00:37:46,530 --> 00:37:48,480
And what we're also not talking about

658
00:37:48,480 --> 00:37:50,400
is a simple text to SQL use case.

659
00:37:50,400 --> 00:37:52,620
You know, one of the most
common questions we get is,

660
00:37:52,620 --> 00:37:55,230
is how does this type of a solution

661
00:37:55,230 --> 00:37:57,720
differ from a common text to SQL?

662
00:37:57,720 --> 00:37:59,700
We've seen this a million times already.

663
00:37:59,700 --> 00:38:01,200
You know, nothing new.

664
00:38:01,200 --> 00:38:05,100
And so as we explored this use case

665
00:38:05,100 --> 00:38:06,420
with Greg and his team,

666
00:38:06,420 --> 00:38:09,180
what we found is that
this is very much well fit

667
00:38:09,180 --> 00:38:10,863
for an agentic use case.

668
00:38:11,940 --> 00:38:14,850
But as we tried to move
through this process

669
00:38:14,850 --> 00:38:18,090
of taking this mountain of data,

670
00:38:18,090 --> 00:38:19,740
one data provider I think provides

671
00:38:19,740 --> 00:38:23,820
something like 15 terabytes on
a monthly basis of new data,

672
00:38:23,820 --> 00:38:25,500
how do we go ahead and tackle

673
00:38:25,500 --> 00:38:28,590
that kind of an evolving landscape

674
00:38:28,590 --> 00:38:31,320
where data might be siloed
across different services,

675
00:38:31,320 --> 00:38:33,780
maybe different data modalities,

676
00:38:33,780 --> 00:38:35,940
perhaps even different data schemas

677
00:38:35,940 --> 00:38:37,983
or oftentimes different data schemas?

678
00:38:39,000 --> 00:38:40,743
You know, with a research team

679
00:38:40,743 --> 00:38:44,310
and ultimately a consumer
team, which aren't SaaS coders,

680
00:38:44,310 --> 00:38:46,620
they're not our coders,
they're not Python developers,

681
00:38:46,620 --> 00:38:50,430
they're not deeply knowledgeable
about this use case.

682
00:38:50,430 --> 00:38:52,050
They don't know that things

683
00:38:52,050 --> 00:38:53,700
like continuous enrollment matter

684
00:38:53,700 --> 00:38:56,910
when it comes to claims data
from a quality perspective.

685
00:38:56,910 --> 00:38:58,740
And so what we're trying to tackle

686
00:38:58,740 --> 00:39:00,600
is this how do we build a system

687
00:39:00,600 --> 00:39:05,160
that uses the most recent
frontier models from Anthropic

688
00:39:05,160 --> 00:39:09,090
and Amazon to make sure that
we can democratize the access

689
00:39:09,090 --> 00:39:11,490
to this data and make
sure, as Greg mentioned,

690
00:39:11,490 --> 00:39:15,120
that it's the most accurate
and the highest quality outputs

691
00:39:15,120 --> 00:39:17,310
because these are critical things

692
00:39:17,310 --> 00:39:19,383
that people are interested in at Lilly?

693
00:39:20,670 --> 00:39:22,770
And so, you know, and
then we have to decide

694
00:39:22,770 --> 00:39:24,180
how are we gonna architect that system?

695
00:39:24,180 --> 00:39:25,013
And then, of course,

696
00:39:25,013 --> 00:39:27,660
that's what I'm here
for to talk about today.

697
00:39:27,660 --> 00:39:29,340
But I always think it's good to start

698
00:39:29,340 --> 00:39:31,830
with a really solid mental model.

699
00:39:31,830 --> 00:39:33,240
So as we think about agents,

700
00:39:33,240 --> 00:39:35,280
there's a lot of talk
about agents in the market.

701
00:39:35,280 --> 00:39:37,860
We heard a lot of updates
from Matt this morning

702
00:39:37,860 --> 00:39:40,890
about AgentCore and about
all the capabilities

703
00:39:40,890 --> 00:39:43,620
within AWS to build agents.

704
00:39:43,620 --> 00:39:45,690
But I always like to start
with a good mental model

705
00:39:45,690 --> 00:39:48,903
so that I can understand the
system I'm trying to build.

706
00:39:50,280 --> 00:39:53,220
And so the thing I always
start with is, what's the goal?

707
00:39:53,220 --> 00:39:55,470
You know, what's the user
experience I'm trying to provide?

708
00:39:55,470 --> 00:39:57,630
Am I just trying to provide SQL queries

709
00:39:57,630 --> 00:40:00,060
or am I trying to build
a system which outputs

710
00:40:00,060 --> 00:40:03,330
real analysis that has rich insights,

711
00:40:03,330 --> 00:40:05,880
that has, you know, solid visualizations,

712
00:40:05,880 --> 00:40:08,400
that has a really, you
know, high speed experience,

713
00:40:08,400 --> 00:40:11,103
sort of high quality, high
performance experience?

714
00:40:12,300 --> 00:40:15,120
So what am I working backwards from?

715
00:40:15,120 --> 00:40:17,160
At Amazon, we like to talk
about working backwards

716
00:40:17,160 --> 00:40:18,750
and that's a very
important thing to define,

717
00:40:18,750 --> 00:40:21,690
is what's the goal and
what's the goal of the agent?

718
00:40:21,690 --> 00:40:23,070
And then I start to think about,

719
00:40:23,070 --> 00:40:24,870
okay, well if I understand the goal,

720
00:40:24,870 --> 00:40:26,760
then I know maybe what
tools I'm gonna use.

721
00:40:26,760 --> 00:40:30,630
As a human, I have lots of
tools that are available to me.

722
00:40:30,630 --> 00:40:33,840
I have a Python notebook
or I have a SaaS console,

723
00:40:33,840 --> 00:40:35,550
or I have my email client, you know,

724
00:40:35,550 --> 00:40:39,240
anything that I use that lives
within the technology realm

725
00:40:39,240 --> 00:40:40,380
is a tool of some kind.

726
00:40:40,380 --> 00:40:42,060
I have a tool in my hand right here.

727
00:40:42,060 --> 00:40:42,893
It doesn't do that much,

728
00:40:42,893 --> 00:40:46,110
but it's pretty important
to the job that I'm doing.

729
00:40:46,110 --> 00:40:48,270
So what are the tools that I need?

730
00:40:48,270 --> 00:40:50,370
If let's say I was a
human doing this process

731
00:40:50,370 --> 00:40:51,420
or doing this job,

732
00:40:51,420 --> 00:40:53,310
what are the tools I would
need to get that job done?

733
00:40:53,310 --> 00:40:55,260
And probably more importantly,

734
00:40:55,260 --> 00:40:56,970
what are the tools that I need to be able

735
00:40:56,970 --> 00:41:00,660
to develop the knowledge in the moment

736
00:41:00,660 --> 00:41:02,550
where I've been asked that question

737
00:41:02,550 --> 00:41:04,170
to be able to answer that question?

738
00:41:04,170 --> 00:41:05,910
That's a really important thing to know.

739
00:41:05,910 --> 00:41:08,010
Then I have to start
to define the protocol.

740
00:41:08,010 --> 00:41:10,650
What are the steps I have
to take to get to that thing

741
00:41:10,650 --> 00:41:12,660
that I'm trying to go towards?

742
00:41:12,660 --> 00:41:14,430
And the goal and the protocol

743
00:41:14,430 --> 00:41:15,810
are really what's gonna help you start

744
00:41:15,810 --> 00:41:17,820
to work towards what's the prompt

745
00:41:17,820 --> 00:41:19,440
that I'm gonna hand to the agent,

746
00:41:19,440 --> 00:41:21,600
and we'll talk about this in a few slides,

747
00:41:21,600 --> 00:41:24,420
in order for it to be able to do its job?

748
00:41:24,420 --> 00:41:25,253
And then, finally,

749
00:41:25,253 --> 00:41:27,603
if I'm thinking about
a multi-agent system,

750
00:41:27,603 --> 00:41:29,790
well then I also have
to start thinking about

751
00:41:29,790 --> 00:41:30,720
what are the handoffs?

752
00:41:30,720 --> 00:41:33,900
Because what I don't want
to do is say I've got six

753
00:41:33,900 --> 00:41:35,940
or seven or eight agents in a system

754
00:41:35,940 --> 00:41:38,760
and I'm gonna hand every
agent all the context

755
00:41:38,760 --> 00:41:40,080
of all of the conversations,

756
00:41:40,080 --> 00:41:41,790
of all the things that have come before

757
00:41:41,790 --> 00:41:45,000
because that's not a successful
architecture approach.

758
00:41:45,000 --> 00:41:46,320
So what are the handoffs?

759
00:41:46,320 --> 00:41:50,070
If I have a team of people
working on a problem,

760
00:41:50,070 --> 00:41:51,810
I'm not just gonna do a data dump

761
00:41:51,810 --> 00:41:53,550
of all the things that have come before.

762
00:41:53,550 --> 00:41:55,470
I'm gonna say, here's the things you need,

763
00:41:55,470 --> 00:41:57,180
here's what I'm asking of you,

764
00:41:57,180 --> 00:42:00,450
and here's maybe some guidance
on perhaps how to do the job.

765
00:42:00,450 --> 00:42:02,340
Please, you know, use your
tools, come back to me.

766
00:42:02,340 --> 00:42:04,350
And that's very much the mental model

767
00:42:04,350 --> 00:42:09,350
for how to start to think about
and align multi-agent teams

768
00:42:09,780 --> 00:42:11,403
to be able to accomplish a task.

769
00:42:12,810 --> 00:42:14,010
And so there are critical decisions

770
00:42:14,010 --> 00:42:15,000
we have to make along the way.

771
00:42:15,000 --> 00:42:17,220
The first one is, what type of system

772
00:42:17,220 --> 00:42:20,670
is gonna fit my use case
the most effectively?

773
00:42:20,670 --> 00:42:22,200
Can I build it with a single agent?

774
00:42:22,200 --> 00:42:24,240
Can I simply just have a text to SQL agent

775
00:42:24,240 --> 00:42:26,340
which uses the myriad of tools

776
00:42:26,340 --> 00:42:29,130
and the capabilities that
I need that has a prompt,

777
00:42:29,130 --> 00:42:32,460
that has additional context
that's going to be able

778
00:42:32,460 --> 00:42:35,910
to answer the questions,
that total set of questions

779
00:42:35,910 --> 00:42:37,890
that I think I'm gonna have to answer?

780
00:42:37,890 --> 00:42:40,080
Or do I want to think about
this more in the sense

781
00:42:40,080 --> 00:42:44,370
of specialized skills and
capabilities and knowledge sets,

782
00:42:44,370 --> 00:42:47,520
context, that's gonna be
able to break a problem down

783
00:42:47,520 --> 00:42:49,260
and be able to answer it effectively?

784
00:42:49,260 --> 00:42:51,270
One of the biggest learnings
for me in working with Greg

785
00:42:51,270 --> 00:42:54,330
and his team early on
was that in the context

786
00:42:54,330 --> 00:42:57,090
of real-world data analysis, you know,

787
00:42:57,090 --> 00:42:59,010
getting to that real-world
evidence outcome

788
00:42:59,010 --> 00:43:02,160
is there are lots of different personas

789
00:43:02,160 --> 00:43:04,800
or skills that are
involved in this process.

790
00:43:04,800 --> 00:43:06,450
It's not just a text to SQL agent

791
00:43:06,450 --> 00:43:08,130
that looks at the data and says, you know,

792
00:43:08,130 --> 00:43:10,470
here's your SQL statement and you move on.

793
00:43:10,470 --> 00:43:12,990
That's not the problem we're
trying to solve here at all.

794
00:43:12,990 --> 00:43:15,570
And I think, as Greg
mentioned, in a lot of cases,

795
00:43:15,570 --> 00:43:17,550
even if you're not looking
at real-world evidence

796
00:43:17,550 --> 00:43:18,720
or real-world data,

797
00:43:18,720 --> 00:43:20,940
you may find that some of these similar

798
00:43:20,940 --> 00:43:25,080
kinds of aspects are also
a part of your use case.

799
00:43:25,080 --> 00:43:27,390
It's not just about the dataset.

800
00:43:27,390 --> 00:43:30,000
There's adjacent data,
there's context that you need

801
00:43:30,000 --> 00:43:32,070
to be able to answer complex questions

802
00:43:32,070 --> 00:43:34,560
to really build a system that scales

803
00:43:34,560 --> 00:43:37,533
and that generalizes to
a larger problem set.

804
00:43:38,390 --> 00:43:40,260
And so as we look at the single agent

805
00:43:40,260 --> 00:43:43,200
versus multi-agent comparison,
we have this trade-off,

806
00:43:43,200 --> 00:43:45,000
it's the classic architecture trade-off.

807
00:43:45,000 --> 00:43:47,670
It's complexity for capability.

808
00:43:47,670 --> 00:43:50,403
And so in the multi-agent
system, as we did in this case,

809
00:43:50,403 --> 00:43:53,430
what we ended up with is
this more complex system,

810
00:43:53,430 --> 00:43:56,130
which was orchestrated by
an underlying framework,

811
00:43:56,130 --> 00:43:58,710
which enabled us to define agents

812
00:43:58,710 --> 00:44:01,950
which encapsulated the key capabilities,

813
00:44:01,950 --> 00:44:04,410
those functionalities, that were necessary

814
00:44:04,410 --> 00:44:07,593
to be able to scale to meet
the needs of the team at Lilly.

815
00:44:09,510 --> 00:44:12,060
And so as we look at a multi-agent system,

816
00:44:12,060 --> 00:44:14,040
then we start to realize
that maybe we need

817
00:44:14,040 --> 00:44:15,720
a manager of sorts.

818
00:44:15,720 --> 00:44:17,130
You know, if we have a larger team,

819
00:44:17,130 --> 00:44:19,770
we like to talk about two
pizza teams at Amazon.

820
00:44:19,770 --> 00:44:22,260
And usually that team has a manager,

821
00:44:22,260 --> 00:44:24,600
and that manager is sort of the decider

822
00:44:24,600 --> 00:44:25,830
of what the business wants,

823
00:44:25,830 --> 00:44:27,540
say if you have a developer team.

824
00:44:27,540 --> 00:44:31,650
And so that manager, in
this case, the supervisor,

825
00:44:31,650 --> 00:44:33,990
is an agent that works with the user

826
00:44:33,990 --> 00:44:35,820
that's kind of that front door.

827
00:44:35,820 --> 00:44:39,030
So the agent is going
to be able to identify

828
00:44:39,030 --> 00:44:41,850
what the user is looking for,
do some entity recognition,

829
00:44:41,850 --> 00:44:45,090
some sort of, you know,
some summarization,

830
00:44:45,090 --> 00:44:47,370
and then it's gonna
work with its partners.

831
00:44:47,370 --> 00:44:50,130
Say a SQL expert, a medical coder,

832
00:44:50,130 --> 00:44:52,830
these are some of the
aspects of this use case

833
00:44:52,830 --> 00:44:54,090
that are really important.

834
00:44:54,090 --> 00:44:56,940
Even a research planner,
an agent who's familiar

835
00:44:56,940 --> 00:44:58,530
with a general methodology

836
00:44:58,530 --> 00:45:00,870
of how to answer
questions in this context,

837
00:45:00,870 --> 00:45:02,190
as well as the data navigator.

838
00:45:02,190 --> 00:45:04,050
One of the things that
we found very quickly too

839
00:45:04,050 --> 00:45:06,510
is because of what Greg
mentioned around the fact

840
00:45:06,510 --> 00:45:09,000
that there's not a consistent standard

841
00:45:09,000 --> 00:45:11,430
for most of these datasets and the fact

842
00:45:11,430 --> 00:45:13,320
that we don't have the
convenience of saying,

843
00:45:13,320 --> 00:45:15,873
hey, let's ETL this data
all into the same format,

844
00:45:16,830 --> 00:45:18,150
into the same schema,

845
00:45:18,150 --> 00:45:20,490
that we need to be able to
meet the data where it is.

846
00:45:20,490 --> 00:45:23,190
And so we need to be able to
navigate multiple datasets

847
00:45:23,190 --> 00:45:24,840
to understand very quickly

848
00:45:24,840 --> 00:45:27,630
on where to find the
answer to the question

849
00:45:27,630 --> 00:45:31,260
in a data set which has tens of tables.

850
00:45:31,260 --> 00:45:32,820
And then, finally, we need to be able

851
00:45:32,820 --> 00:45:34,230
to actually interact with that dataset.

852
00:45:34,230 --> 00:45:36,240
So how do we interact with the landscape?

853
00:45:36,240 --> 00:45:38,070
And it's not just the underlying data.

854
00:45:38,070 --> 00:45:41,010
One of the biggest learnings
for me was that there are many,

855
00:45:41,010 --> 00:45:42,840
many code sets, for example,

856
00:45:42,840 --> 00:45:44,640
as reference data that's critical

857
00:45:44,640 --> 00:45:47,160
to translating a human question,

858
00:45:47,160 --> 00:45:49,350
which we'll see here
in a couple of slides,

859
00:45:49,350 --> 00:45:51,870
into what we actually need to know.

860
00:45:51,870 --> 00:45:53,730
So it's not necessarily in the data set

861
00:45:53,730 --> 00:45:54,900
the thing that we're trying to find,

862
00:45:54,900 --> 00:45:57,300
but I need to take a
couple stops along the way

863
00:45:57,300 --> 00:45:59,100
to be able to get the right context.

864
00:46:00,900 --> 00:46:02,730
And so as we started to think more

865
00:46:02,730 --> 00:46:04,200
and more about this problem,

866
00:46:04,200 --> 00:46:06,150
this is where the module three

867
00:46:06,150 --> 00:46:08,490
from what Anne was talking
about earlier really came out.

868
00:46:08,490 --> 00:46:12,480
And so what we landed on was
seven specialized agents.

869
00:46:12,480 --> 00:46:13,950
The first, of course, is the assistant,

870
00:46:13,950 --> 00:46:15,150
or that's the supervisor,

871
00:46:15,150 --> 00:46:17,610
that's the thing that chats with the user.

872
00:46:17,610 --> 00:46:18,930
The next one is the SQL creator.

873
00:46:18,930 --> 00:46:20,730
Of course, we can't do
this without interacting

874
00:46:20,730 --> 00:46:22,140
with a structured dataset so, of course,

875
00:46:22,140 --> 00:46:24,780
we need to write some SQL, which is great.

876
00:46:24,780 --> 00:46:26,220
Then we need a medical coder.

877
00:46:26,220 --> 00:46:28,980
So if I ask a question
about say a diagnosis

878
00:46:28,980 --> 00:46:33,270
or some medication or some
lab procedure or other things,

879
00:46:33,270 --> 00:46:35,010
or maybe I ask about all three,

880
00:46:35,010 --> 00:46:37,650
I need an expert in medical coding

881
00:46:37,650 --> 00:46:39,150
to make sure that I can translate

882
00:46:39,150 --> 00:46:42,330
that human question if I
don't know all those codes,

883
00:46:42,330 --> 00:46:44,640
and most people don't, neither
do the LLMs by the way,

884
00:46:44,640 --> 00:46:48,840
without the right context,
into the the real question.

885
00:46:48,840 --> 00:46:50,670
Then we need to create visualizations,

886
00:46:50,670 --> 00:46:52,770
we need to make a
research plan potentially

887
00:46:52,770 --> 00:46:53,970
from our complex questions,

888
00:46:53,970 --> 00:46:56,430
we may even need to
consult some publications.

889
00:46:56,430 --> 00:46:58,500
And then, finally, we need
to be able to navigate

890
00:46:58,500 --> 00:47:01,410
that data and do so in
a way which integrates

891
00:47:01,410 --> 00:47:04,290
with that complex, harmonized dataset

892
00:47:04,290 --> 00:47:05,703
there you see in module two.

893
00:47:07,530 --> 00:47:10,350
And so it's always good to
zoom in on one of these agents

894
00:47:10,350 --> 00:47:12,630
to really see what it's doing.

895
00:47:12,630 --> 00:47:15,180
And so the neat thing
about an agent is it works

896
00:47:15,180 --> 00:47:18,480
very much like my brain does,
like your brain probably does

897
00:47:18,480 --> 00:47:19,590
where you say, I've got a problem,

898
00:47:19,590 --> 00:47:22,410
I wanna solve it, but I
have to iterate over it.

899
00:47:22,410 --> 00:47:23,970
I have to think about the problem

900
00:47:23,970 --> 00:47:26,460
and understand the problem
before I can go and solve that.

901
00:47:26,460 --> 00:47:27,960
And that's exactly what an agent does.

902
00:47:27,960 --> 00:47:30,060
It has this iterative execution loop,

903
00:47:30,060 --> 00:47:34,320
it's a cyclical node on
a graph, in this case,

904
00:47:34,320 --> 00:47:36,390
that takes a number of inputs.

905
00:47:36,390 --> 00:47:40,920
So it starts with the prompt
and the reasoning loop.

906
00:47:40,920 --> 00:47:44,190
This notion of of iterating
over a problem until it's solved

907
00:47:44,190 --> 00:47:46,290
and really until the
agent decides it's solved,

908
00:47:46,290 --> 00:47:47,850
which is even more interesting.

909
00:47:47,850 --> 00:47:51,240
And so it's gotta perform some
kind of structured reasoning.

910
00:47:51,240 --> 00:47:53,390
This is something that's built into LLMs.

911
00:47:53,390 --> 00:47:54,870
So you hear about tool-calling,

912
00:47:54,870 --> 00:47:57,060
function-calling, instruction following.

913
00:47:57,060 --> 00:48:00,270
These are all aspects of what makes LLMs

914
00:48:00,270 --> 00:48:03,308
the reasoning engine behind this process.

915
00:48:03,308 --> 00:48:05,550
Then I need to be able to
do things like in this case,

916
00:48:05,550 --> 00:48:10,230
since my task is to plan
and execute SQL queries

917
00:48:10,230 --> 00:48:12,570
and provide analysis, I
need to be able to plan

918
00:48:12,570 --> 00:48:16,053
the query structure and
potentially have examples.

919
00:48:17,040 --> 00:48:18,090
And I've got the graph state,

920
00:48:18,090 --> 00:48:19,530
this is all the things
that happened before.

921
00:48:19,530 --> 00:48:22,320
Have there been other
actions, other agents,

922
00:48:22,320 --> 00:48:24,450
has the medical coding
agent already deciphered

923
00:48:24,450 --> 00:48:27,570
all that complex human
language into a set of codes

924
00:48:27,570 --> 00:48:29,400
or have there been queries run before?

925
00:48:29,400 --> 00:48:30,387
Has somebody asked a question

926
00:48:30,387 --> 00:48:32,370
and then asked a follow up question

927
00:48:32,370 --> 00:48:34,740
as the process that that Greg described?

928
00:48:34,740 --> 00:48:37,020
But there's something missing here

929
00:48:37,020 --> 00:48:39,540
because this isn't enough to
be able to accomplish this.

930
00:48:39,540 --> 00:48:44,160
We need a set of tools and we
need a set of curated tools

931
00:48:44,160 --> 00:48:45,510
to be able to make this agent

932
00:48:45,510 --> 00:48:47,010
able to do the things it's doing.

933
00:48:47,010 --> 00:48:48,480
Just like as a human,

934
00:48:48,480 --> 00:48:51,840
I would use say a query
reference database,

935
00:48:51,840 --> 00:48:54,330
or I would use a dataset metadata.

936
00:48:54,330 --> 00:48:56,820
So, for example, I might
use a data dictionary

937
00:48:56,820 --> 00:48:58,260
or I might go into the database

938
00:48:58,260 --> 00:49:00,660
and look at the actual table DDL.

939
00:49:00,660 --> 00:49:03,720
So I'm gonna be looking
through table and column names,

940
00:49:03,720 --> 00:49:07,260
foreign key relationships,
data types and constraints

941
00:49:07,260 --> 00:49:08,550
as I'm constructing a query.

942
00:49:08,550 --> 00:49:10,650
And that's exactly what
an agent has to do.

943
00:49:10,650 --> 00:49:13,230
It has to have the context to be able

944
00:49:13,230 --> 00:49:14,610
to solve the problem in front of it.

945
00:49:14,610 --> 00:49:15,600
And, by the way,

946
00:49:15,600 --> 00:49:17,730
this is something that
applies to any agent.

947
00:49:17,730 --> 00:49:19,980
Remember back to our mental model,

948
00:49:19,980 --> 00:49:21,540
this is about defining the tools

949
00:49:21,540 --> 00:49:24,003
that enable the agent
to accomplish the task.

950
00:49:25,230 --> 00:49:26,280
But then there's more to it.

951
00:49:26,280 --> 00:49:27,840
There's few-shot prompts.

952
00:49:27,840 --> 00:49:31,440
So we've got that in as
a part of the tool set.

953
00:49:31,440 --> 00:49:34,320
So an interesting part about
this is this is a good way

954
00:49:34,320 --> 00:49:35,760
to integrate that human in the loop.

955
00:49:35,760 --> 00:49:38,250
So what we found here was that a great set

956
00:49:38,250 --> 00:49:40,830
of baseline ground truth queries

957
00:49:40,830 --> 00:49:42,600
that answer related questions

958
00:49:42,600 --> 00:49:45,630
are a really, really good
way to encourage an LLM

959
00:49:45,630 --> 00:49:46,770
to make the right decisions

960
00:49:46,770 --> 00:49:48,690
when it comes to query generation.

961
00:49:48,690 --> 00:49:52,860
And so taking those golden
queries as we've stated here,

962
00:49:52,860 --> 00:49:55,560
and making them available
through a semantic search,

963
00:49:55,560 --> 00:49:59,850
so a similarity search or
a similar meaning search

964
00:49:59,850 --> 00:50:01,920
to the agent on demand through a tool

965
00:50:01,920 --> 00:50:04,893
is a great way to encourage
and improve outputs.

966
00:50:05,760 --> 00:50:07,320
Also having SQL validation

967
00:50:07,320 --> 00:50:10,410
so we have a very quick way
to validate those queries.

968
00:50:10,410 --> 00:50:12,450
And then the ability to actually execute

969
00:50:12,450 --> 00:50:15,960
and manage the outputs of those
queries is really important.

970
00:50:15,960 --> 00:50:18,660
And then, finally, what that
leads us to is this idea

971
00:50:18,660 --> 00:50:21,840
of being able to not only
generate multiple queries,

972
00:50:21,840 --> 00:50:24,060
execute them, troubleshoot them,

973
00:50:24,060 --> 00:50:26,940
gather information and
context around them,

974
00:50:26,940 --> 00:50:30,390
to then take that and actually
turn that into analysis.

975
00:50:30,390 --> 00:50:33,150
And so that's all part
of that define the role,

976
00:50:33,150 --> 00:50:35,400
define the tools, define the protocol,

977
00:50:35,400 --> 00:50:38,010
and then define the handoff that results

978
00:50:38,010 --> 00:50:39,943
in a set of agents which can answer

979
00:50:39,943 --> 00:50:41,970
a complex set of questions

980
00:50:41,970 --> 00:50:45,750
through this iterative,
cyclical execution loop

981
00:50:45,750 --> 00:50:48,633
with context gathering and
then eventual analysis.

982
00:50:49,830 --> 00:50:51,690
So let's talk about how
this actually works.

983
00:50:51,690 --> 00:50:52,920
So we've got a question here.

984
00:50:52,920 --> 00:50:55,050
What is the age distribution
of patients diagnosed

985
00:50:55,050 --> 00:50:57,450
with hypertension who
are prescribed drug X

986
00:50:57,450 --> 00:50:58,590
in the last 12 months?

987
00:50:58,590 --> 00:51:01,050
So there are about four or
five different attributes

988
00:51:01,050 --> 00:51:02,550
depending on how you
look at that question.

989
00:51:02,550 --> 00:51:04,200
So how do we break that down?

990
00:51:04,200 --> 00:51:06,360
Well, it starts with the
supervisor, of course,

991
00:51:06,360 --> 00:51:08,190
that's the entry point to this graph.

992
00:51:08,190 --> 00:51:10,140
And so the supervisor is
gonna look at that question

993
00:51:10,140 --> 00:51:11,520
and say, well, I gotta
come up with a plan.

994
00:51:11,520 --> 00:51:13,980
I see that there's
demographic information,

995
00:51:13,980 --> 00:51:17,160
there's a diagnosis in there,
there's a drug in there,

996
00:51:17,160 --> 00:51:18,570
there's a timeframe in there.

997
00:51:18,570 --> 00:51:20,430
So I've got a bunch of
things I gotta figure out.

998
00:51:20,430 --> 00:51:21,630
So the thing I'm gonna do first

999
00:51:21,630 --> 00:51:24,660
is let's take a stop at the
medical coding specialist.

1000
00:51:24,660 --> 00:51:26,880
I gotta figure out what's
the coding for hypertension?

1001
00:51:26,880 --> 00:51:28,800
Is it one, is it many?

1002
00:51:28,800 --> 00:51:32,160
What's the coding for the
drug I'm looking for, right?

1003
00:51:32,160 --> 00:51:33,840
And then what's additional information

1004
00:51:33,840 --> 00:51:35,130
that might be of interest?

1005
00:51:35,130 --> 00:51:37,620
Are there lab codes that
might represent that drug?

1006
00:51:37,620 --> 00:51:40,230
It's one we we ran into
that was unexpected.

1007
00:51:40,230 --> 00:51:41,063
So once we've stopped

1008
00:51:41,063 --> 00:51:42,570
at the medical coding agent, that's great.

1009
00:51:42,570 --> 00:51:45,510
Now we gotta figure out
where this is in the dataset.

1010
00:51:45,510 --> 00:51:47,937
So we stop at the data navigator agent

1011
00:51:47,937 --> 00:51:49,860
and each one of these
stops is going through

1012
00:51:49,860 --> 00:51:53,010
this iterative loop of
understanding the task

1013
00:51:53,010 --> 00:51:56,250
that's given to it and
providing a high quality output

1014
00:51:56,250 --> 00:51:58,140
for the rest of the system to use.

1015
00:51:58,140 --> 00:52:00,420
So then we stop, of course,
at the SQL expert agent,

1016
00:52:00,420 --> 00:52:01,650
which we just talked about.

1017
00:52:01,650 --> 00:52:04,110
And then, finally, we land
on the visualization creator

1018
00:52:04,110 --> 00:52:06,750
because the supervisor
decided that this would be

1019
00:52:06,750 --> 00:52:10,350
a great use case to generate
some visualizations for.

1020
00:52:10,350 --> 00:52:13,110
So not only do we get
a high quality output

1021
00:52:13,110 --> 00:52:16,350
that's iteratively followed
almost what feels like

1022
00:52:16,350 --> 00:52:19,680
a very human logic kind of a process.

1023
00:52:19,680 --> 00:52:21,270
Then we've also been able to give

1024
00:52:21,270 --> 00:52:23,220
some high quality visualizations

1025
00:52:23,220 --> 00:52:26,433
based on the context of the
question that was asked.

1026
00:52:28,320 --> 00:52:29,760
And so as we built this system,

1027
00:52:29,760 --> 00:52:31,500
we learned some pretty valuable lessons.

1028
00:52:31,500 --> 00:52:34,140
The first one was, a great
thing about multi-agent systems

1029
00:52:34,140 --> 00:52:38,070
is multi-agents can be
evaluated individually.

1030
00:52:38,070 --> 00:52:40,290
And so it's really important
as you start to think

1031
00:52:40,290 --> 00:52:42,300
through the problem set
that you're addressing

1032
00:52:42,300 --> 00:52:44,130
and you start to build individual agents

1033
00:52:44,130 --> 00:52:47,190
that do these critically encapsulated,

1034
00:52:47,190 --> 00:52:50,520
almost atomic kind of skills things,

1035
00:52:50,520 --> 00:52:53,820
that you make sure that you
validate them individually.

1036
00:52:53,820 --> 00:52:56,130
Do they do the thing that you
think that they're supposed

1037
00:52:56,130 --> 00:52:58,560
to do given the problem
set that you've defined

1038
00:52:58,560 --> 00:53:02,010
when you thought about
the original problem set?

1039
00:53:02,010 --> 00:53:04,260
You gotta deep dive on
guardrails is the second one.

1040
00:53:04,260 --> 00:53:06,030
So one of the things that
we found very quickly

1041
00:53:06,030 --> 00:53:08,610
was that the complexity and the nuance

1042
00:53:08,610 --> 00:53:11,100
in language of the questions
that people would ask

1043
00:53:11,100 --> 00:53:13,830
in this space is really, really important

1044
00:53:13,830 --> 00:53:15,990
to get exactly right
because the criticality

1045
00:53:15,990 --> 00:53:18,240
of answering say a safety-related question

1046
00:53:18,240 --> 00:53:21,360
or not is paramount.

1047
00:53:21,360 --> 00:53:24,450
And so being able to
understand some of the nuances

1048
00:53:24,450 --> 00:53:27,030
and deep diving with the
subject matter experts

1049
00:53:27,030 --> 00:53:29,070
on what exactly are some of the nuances

1050
00:53:29,070 --> 00:53:30,810
that they do or do not want to allow

1051
00:53:30,810 --> 00:53:32,520
from an inputs and outputs perspective,

1052
00:53:32,520 --> 00:53:33,870
what's in the underlying data

1053
00:53:33,870 --> 00:53:37,230
or their say de-identification
kinds of concerns

1054
00:53:37,230 --> 00:53:39,210
or data privacy issues,

1055
00:53:39,210 --> 00:53:42,183
deep dive on guardrails is
really, really important.

1056
00:53:43,230 --> 00:53:44,520
Sorry.

1057
00:53:44,520 --> 00:53:47,340
And so the next one then is
perform system level validation.

1058
00:53:47,340 --> 00:53:50,640
So just because you've
validated the individual agents

1059
00:53:50,640 --> 00:53:52,770
doesn't mean that when
you put them all together

1060
00:53:52,770 --> 00:53:55,080
that the system is gonna
do what you expect.

1061
00:53:55,080 --> 00:53:57,390
So make sure that you
come up with a good set,

1062
00:53:57,390 --> 00:53:59,220
a good test set of questions

1063
00:53:59,220 --> 00:54:01,260
that you can run through
on a regular basis

1064
00:54:01,260 --> 00:54:03,780
as not only you move towards production,

1065
00:54:03,780 --> 00:54:05,980
but then also as you
change language models.

1066
00:54:07,140 --> 00:54:09,240
Sonnet 4.5 was just
released, I don't know,

1067
00:54:09,240 --> 00:54:11,490
a month ago or so, maybe not even.

1068
00:54:11,490 --> 00:54:16,490
And the pace of model change
is gonna continue to increase.

1069
00:54:17,070 --> 00:54:18,870
And so as new models come up,

1070
00:54:18,870 --> 00:54:21,090
it's very important to
be able to switch quickly

1071
00:54:21,090 --> 00:54:23,220
to new models, which means validation,

1072
00:54:23,220 --> 00:54:25,140
which means testing,
which means automation

1073
00:54:25,140 --> 00:54:27,120
and these are all really key things

1074
00:54:27,120 --> 00:54:29,790
to build into your original system.

1075
00:54:29,790 --> 00:54:32,430
Including human in the loop
is also very important.

1076
00:54:32,430 --> 00:54:34,890
Improving the quality of the outputs

1077
00:54:34,890 --> 00:54:36,480
based on the quality of the inputs.

1078
00:54:36,480 --> 00:54:39,142
I think Anne mentioned this early on.

1079
00:54:39,142 --> 00:54:41,040
This is almost a garbage
in, garbage out situation.

1080
00:54:41,040 --> 00:54:43,140
So including a human in
the loop in this case,

1081
00:54:43,140 --> 00:54:47,640
in a very passive way to provide
approval of golden queries

1082
00:54:47,640 --> 00:54:50,730
and human feedback on the quality

1083
00:54:50,730 --> 00:54:52,770
of that ground truth
reference for the agents

1084
00:54:52,770 --> 00:54:54,330
is really, really important.

1085
00:54:54,330 --> 00:54:56,673
And then, finally,
develop and track metrics.

1086
00:54:57,690 --> 00:55:00,510
Human feedback from experience,

1087
00:55:00,510 --> 00:55:05,280
quality of outputs, even just
latency of query execution.

1088
00:55:05,280 --> 00:55:07,890
These are all things as you
start to instrument your system

1089
00:55:07,890 --> 00:55:11,730
with observability capabilities,
like say as in AgentCore,

1090
00:55:11,730 --> 00:55:14,670
you really have the
opportunity to continually

1091
00:55:14,670 --> 00:55:18,180
and iteratively improve the
solution that you're building

1092
00:55:18,180 --> 00:55:22,050
by developing those and
tracking those metrics early on.

1093
00:55:22,050 --> 00:55:24,690
And so, finally, from
an overall standpoint,

1094
00:55:24,690 --> 00:55:26,440
as you think about building agents,

1095
00:55:27,360 --> 00:55:30,960
the key aspect here is
taking that raw data

1096
00:55:30,960 --> 00:55:32,280
into actionable insights.

1097
00:55:32,280 --> 00:55:34,590
And as you see, that's
a multi-step process.

1098
00:55:34,590 --> 00:55:37,380
A person can't do this in an instant

1099
00:55:37,380 --> 00:55:38,850
and neither can an agent system,

1100
00:55:38,850 --> 00:55:43,230
but the key is to build
yourself that research team

1101
00:55:43,230 --> 00:55:46,440
to be able to accomplish the
task that you're going for.

1102
00:55:46,440 --> 00:55:49,350
The second one is really key,
and Greg touched on this,

1103
00:55:49,350 --> 00:55:50,850
is this context aspect.

1104
00:55:50,850 --> 00:55:54,570
So it's about architecting
a set of tools, you know,

1105
00:55:54,570 --> 00:55:56,940
an overall kind of system architecture,

1106
00:55:56,940 --> 00:56:01,680
which results in the ability
to capture the right tools

1107
00:56:01,680 --> 00:56:04,730
that are gonna curate
that context to each agent

1108
00:56:04,730 --> 00:56:06,930
in the right place at the right time.

1109
00:56:06,930 --> 00:56:08,040
And actually one thing I'll mention

1110
00:56:08,040 --> 00:56:09,600
on that past one is model choice.

1111
00:56:09,600 --> 00:56:12,810
A lot of attention is paid
to model choice these days

1112
00:56:12,810 --> 00:56:14,610
and it's very, very important.

1113
00:56:14,610 --> 00:56:18,540
But increasingly so as the
models continue to evolve,

1114
00:56:18,540 --> 00:56:22,680
context becomes a critical
aspect of high quality,

1115
00:56:22,680 --> 00:56:24,570
high performance agent execution.

1116
00:56:24,570 --> 00:56:26,517
So the next one, of course,
is testing and observability.

1117
00:56:26,517 --> 00:56:28,260
And a lot of this has to do with rigor

1118
00:56:28,260 --> 00:56:30,600
around application or
solution development.

1119
00:56:30,600 --> 00:56:32,190
And then also picking the right framework

1120
00:56:32,190 --> 00:56:34,560
like a strands agent running on AgentCore

1121
00:56:34,560 --> 00:56:36,900
to be able to quickly and
easily instrument your system

1122
00:56:36,900 --> 00:56:39,420
with the observability
capabilities that you need

1123
00:56:39,420 --> 00:56:41,733
to move from dev all
the way to production.

1124
00:56:43,290 --> 00:56:45,840
And then, you know, building
an extensive ecosystem,

1125
00:56:45,840 --> 00:56:47,610
one of the great things
about multi-agent systems

1126
00:56:47,610 --> 00:56:49,590
is you can extend them
and add more capabilities.

1127
00:56:49,590 --> 00:56:51,240
As your problem set evolves,

1128
00:56:51,240 --> 00:56:53,217
you can build reusable
agents for different things

1129
00:56:53,217 --> 00:56:56,100
and you can imagine that
the medical coding agent

1130
00:56:56,100 --> 00:56:57,960
can be used for a lot of things.

1131
00:56:57,960 --> 00:57:00,510
It's not just exclusive to this use case.

1132
00:57:00,510 --> 00:57:03,420
And so being able to build individual,

1133
00:57:03,420 --> 00:57:06,480
capable agents and then
continuing to add more

1134
00:57:06,480 --> 00:57:08,910
and more agents, you know,
agent to agent communication now

1135
00:57:08,910 --> 00:57:10,890
as a protocol is becoming very popular

1136
00:57:10,890 --> 00:57:12,300
for this exact reason.

1137
00:57:12,300 --> 00:57:14,520
And so that's a really important
thing to be thinking about

1138
00:57:14,520 --> 00:57:16,890
is building that ecosystem of agents

1139
00:57:16,890 --> 00:57:19,473
so that they can plug
together is critical.

1140
00:57:20,910 --> 00:57:22,980
Safer and auditable workflows.

1141
00:57:22,980 --> 00:57:24,360
So, you know, one of the things

1142
00:57:24,360 --> 00:57:25,980
that like in this use case

1143
00:57:25,980 --> 00:57:28,410
embedding compliance into
the reasoning process,

1144
00:57:28,410 --> 00:57:31,200
being able to manage things like citations

1145
00:57:31,200 --> 00:57:33,540
as a part of the output is
a really important thing

1146
00:57:33,540 --> 00:57:37,350
to build trust in
consumers as they interact

1147
00:57:37,350 --> 00:57:38,880
with an AI system because they don't see

1148
00:57:38,880 --> 00:57:40,350
all of the reasoning.

1149
00:57:40,350 --> 00:57:45,180
So they need to know how the
agent arrived at the output.

1150
00:57:45,180 --> 00:57:47,850
And then, finally, that human
plus agent collaboration

1151
00:57:47,850 --> 00:57:49,050
is really, really important.

1152
00:57:49,050 --> 00:57:52,110
Greg touched on it with
regard to that, you know,

1153
00:57:52,110 --> 00:57:55,650
kind of interaction process
of question and answer,

1154
00:57:55,650 --> 00:57:57,300
question and answer dialogue.

1155
00:57:57,300 --> 00:57:59,820
But it's also really important,
as I mentioned earlier,

1156
00:57:59,820 --> 00:58:03,900
for a human to be involved
in a less direct capacity

1157
00:58:03,900 --> 00:58:05,400
to provide that ground truth.

1158
00:58:05,400 --> 00:58:08,040
That subject matter expertise is critical

1159
00:58:08,040 --> 00:58:11,220
for both this and probably
many of the use cases

1160
00:58:11,220 --> 00:58:13,650
that you might be thinking about today.

1161
00:58:13,650 --> 00:58:14,940
And then, finally, of course,

1162
00:58:14,940 --> 00:58:17,130
we always want you to build on AWS.

1163
00:58:17,130 --> 00:58:21,150
The agent services continue
to evolve on the AWS platform

1164
00:58:21,150 --> 00:58:24,630
to an incredible set of
capabilities that enable you

1165
00:58:24,630 --> 00:58:29,010
to move forward with your
development to production process.

1166
00:58:29,010 --> 00:58:30,150
And with that in mind,

1167
00:58:30,150 --> 00:58:33,930
we will be at Sushisamba
tomorrow, 12:45 to 2:45.

1168
00:58:33,930 --> 00:58:35,550
So please come and see us.

1169
00:58:35,550 --> 00:58:38,100
We'll be doing a demo
of this actual solution

1170
00:58:38,100 --> 00:58:40,080
on a real-world dataset.

1171
00:58:40,080 --> 00:58:42,570
So please come join us and hopefully

1172
00:58:42,570 --> 00:58:44,880
we'll see you at Sushisamba tomorrow.

1173
00:58:44,880 --> 00:58:48,570
With that in mind, the
Life Sciences team at AWS

1174
00:58:48,570 --> 00:58:52,020
has also built an incredible
set of accelerators

1175
00:58:52,020 --> 00:58:54,990
in addition to the RWD agent.

1176
00:58:54,990 --> 00:58:57,300
This is things like
competitive intelligence,

1177
00:58:57,300 --> 00:59:00,600
what we call Clinical
Supply Chain Control Tower.

1178
00:59:00,600 --> 00:59:02,610
And the intent of these solutions

1179
00:59:02,610 --> 00:59:04,980
is to say how do you get from idea

1180
00:59:04,980 --> 00:59:07,080
to production in the fastest way?

1181
00:59:07,080 --> 00:59:08,820
You can take that native build process,

1182
00:59:08,820 --> 00:59:10,830
which I know as builders we like to do,

1183
00:59:10,830 --> 00:59:14,370
but then also you can
use that AWS OS toolkit

1184
00:59:14,370 --> 00:59:16,890
for Life Sciences as a next step.

1185
00:59:16,890 --> 00:59:17,940
And then, finally, of course,

1186
00:59:17,940 --> 00:59:20,520
with the accelerators that we're
building and iterating over

1187
00:59:20,520 --> 00:59:22,410
with customers like yourselves,

1188
00:59:22,410 --> 00:59:24,240
we're able to get there even faster

1189
00:59:24,240 --> 00:59:27,510
by doing say 40 or 50 or
60% of the work already.

1190
00:59:27,510 --> 00:59:29,820
So we can hand you something
and partner with you

1191
00:59:29,820 --> 00:59:31,410
to move forward to production.

1192
00:59:31,410 --> 00:59:33,750
So really excited to
hopefully see a lot of you

1193
00:59:33,750 --> 00:59:36,570
at Sushisamba tomorrow to
talk about real-world data,

1194
00:59:36,570 --> 00:59:39,063
as well as the other
accelerators that we have.

1195
00:59:41,133 --> 00:59:43,530
If you're interested in more information,

1196
00:59:43,530 --> 00:59:45,540
grab the QR code here,

1197
00:59:45,540 --> 00:59:48,480
get some more information
on the agentic accelerators.

1198
00:59:48,480 --> 00:59:51,810
Of course, also in the
the massive expo hall,

1199
00:59:51,810 --> 00:59:54,270
please come and see us at
the Industries Pavilion,

1200
00:59:54,270 --> 00:59:56,580
the Life Sciences team from AWS.

1201
00:59:56,580 --> 00:59:59,640
We'll be there today and
and for the rest of the week

1202
00:59:59,640 --> 01:00:02,070
and we'll be excited to see
any one of you as come out.

1203
01:00:02,070 --> 01:00:05,880
And, you know, please, again,
thank you for your time today.

1204
01:00:05,880 --> 01:00:07,770
Hopefully you learned
something from this session.

1205
01:00:07,770 --> 01:00:11,880
Please provide feedback
on the session content

1206
01:00:11,880 --> 01:00:14,610
with the survey and hope everybody

1207
01:00:14,610 --> 01:00:15,960
has a great rest of their day.

1208
01:00:15,960 --> 01:00:16,835
Thank you.

1209
01:00:16,835 --> 01:00:19,886
(audience applauding)

