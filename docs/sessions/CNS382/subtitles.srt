1
00:00:00,489 --> 00:00:02,829
Um, good afternoon, and this is probably

2
00:00:02,829 --> 00:00:05,360
one of the final sessions before, um,

3
00:00:05,448 --> 00:00:07,368
before Reinvent comes in the close, and

4
00:00:07,730 --> 00:00:10,089
thanks for joining us. I know we've got some lots of competition

5
00:00:10,089 --> 00:00:11,000
today with, uh,

6
00:00:11,429 --> 00:00:13,569
Werner Vogel's keynote, but, uh, I appreciate

7
00:00:13,569 --> 00:00:15,470
your attendance today. Thank you very much.

8
00:00:16,010 --> 00:00:18,089
So my name's Stephen Leedig, I'm a principal solutions

9
00:00:18,089 --> 00:00:20,440
architect, uh, with the surplus team out of

10
00:00:20,440 --> 00:00:21,170
Australia and New Zealand.

11
00:00:21,568 --> 00:00:24,109
Um, and I'm joined today by Archana Shikantha,

12
00:00:24,500 --> 00:00:26,978
who, um, is a principal engineer with the AWS

13
00:00:26,978 --> 00:00:27,568
Lambda team.

14
00:00:28,068 --> 00:00:30,149
And today we want to introduce you to a

15
00:00:30,149 --> 00:00:32,539
new feature that we launched on earlier

16
00:00:32,539 --> 00:00:33,289
this week,

17
00:00:33,618 --> 00:00:35,459
um, called Lambda Managed Instances.

18
00:00:37,520 --> 00:00:38,179
So

19
00:00:38,840 --> 00:00:40,048
AWS has been,

20
00:00:40,679 --> 00:00:42,908
has not just pioneered Servius, um,

21
00:00:42,918 --> 00:00:44,548
but as you can see here,

22
00:00:44,880 --> 00:00:47,779
we've um continually um and rigorously,

23
00:00:47,880 --> 00:00:48,439
um,

24
00:00:49,340 --> 00:00:51,500
Innovated in this space. From the early

25
00:00:51,500 --> 00:00:53,918
beginnings of introducing runtime

26
00:00:53,918 --> 00:00:55,899
like you know Python, um,

27
00:00:56,270 --> 00:00:57,348
types, uh, uh,

28
00:00:58,130 --> 00:00:59,770
Node JS and Java,

29
00:01:00,289 --> 00:01:02,389
we've now and and and for now and

30
00:01:02,399 --> 00:01:04,569
and bringing this to today where

31
00:01:04,569 --> 00:01:06,168
we're virtually um

32
00:01:06,650 --> 00:01:08,088
supporting any type of run time.

33
00:01:09,319 --> 00:01:11,808
To introducing VPC integration,

34
00:01:12,189 --> 00:01:14,370
optimizing network integration through

35
00:01:14,370 --> 00:01:16,269
um hyperplane interfaces,

36
00:01:16,709 --> 00:01:18,778
and also introducing things like layers and

37
00:01:18,778 --> 00:01:21,638
concurrency, and continually optimizing

38
00:01:21,638 --> 00:01:22,579
cold starts

39
00:01:22,870 --> 00:01:24,900
to, you know, help you build the

40
00:01:24,900 --> 00:01:25,989
applications you're building today.

41
00:01:27,069 --> 00:01:29,230
And we've also along the way introduced

42
00:01:29,230 --> 00:01:31,230
some new services, step functions and

43
00:01:31,230 --> 00:01:32,409
Eventbridge to help you

44
00:01:32,750 --> 00:01:35,028
orchestrate millions of workflows and

45
00:01:35,028 --> 00:01:36,629
build event-driven architectures.

46
00:01:37,879 --> 00:01:39,500
And over the last 10 years,

47
00:01:39,790 --> 00:01:41,790
customers have been, you know, really enjoying the

48
00:01:41,790 --> 00:01:43,540
benefits of the, this

49
00:01:43,959 --> 00:01:46,388
really wide portfolio of serverless services,

50
00:01:46,400 --> 00:01:47,659
to help them run and build

51
00:01:48,000 --> 00:01:49,900
modern applications in the cloud today.

52
00:01:50,668 --> 00:01:52,730
And doing that with, you know,

53
00:01:53,230 --> 00:01:56,189
with without having to write or um or

54
00:01:56,189 --> 00:01:58,469
write a lot of less code and focusing

55
00:01:58,469 --> 00:01:59,769
on business logic,

56
00:02:00,069 --> 00:02:02,579
and not getting bogged down by scaling

57
00:02:02,579 --> 00:02:04,870
and security updates and other

58
00:02:04,870 --> 00:02:06,028
maintenance issues.

59
00:02:06,588 --> 00:02:08,719
And only paying for the services

60
00:02:08,719 --> 00:02:10,788
that, um, you know, are driving value for

61
00:02:10,788 --> 00:02:13,149
your organization, um while minimizing

62
00:02:13,149 --> 00:02:15,649
waste. And also being able to take advantage

63
00:02:16,028 --> 00:02:18,110
of the best practices um that

64
00:02:18,110 --> 00:02:20,110
we have for distributed architectures and

65
00:02:20,110 --> 00:02:23,189
security. And

66
00:02:23,189 --> 00:02:25,449
as you can see here, we've got a huge

67
00:02:25,449 --> 00:02:27,729
um diversity in um in

68
00:02:27,830 --> 00:02:29,929
um use cases for errs today.

69
00:02:30,719 --> 00:02:33,520
Um, we've got financial institutions migrating

70
00:02:33,520 --> 00:02:35,069
core banking applications,

71
00:02:35,399 --> 00:02:37,399
um, to AWS leveraging LAMDA

72
00:02:37,399 --> 00:02:39,939
and other AWS services to cut

73
00:02:39,939 --> 00:02:42,080
operational costs and to accelerate future

74
00:02:42,080 --> 00:02:42,618
development.

75
00:02:43,599 --> 00:02:45,639
We're looking at healthcare, um, providers

76
00:02:45,639 --> 00:02:48,169
who've been using LAMDA to automate,

77
00:02:48,360 --> 00:02:50,508
um, appointment bookings,

78
00:02:50,719 --> 00:02:52,758
um, insurance claims processing, and

79
00:02:52,758 --> 00:02:54,740
secure patient portal access.

80
00:02:55,740 --> 00:02:57,770
Retail companies are adopting event-driven

81
00:02:57,770 --> 00:02:59,889
architectures as a way of being

82
00:02:59,889 --> 00:03:02,149
able to innovate quickly and handle

83
00:03:02,149 --> 00:03:04,250
um peak shopping, um, peak shopping

84
00:03:04,250 --> 00:03:05,000
spikes.

85
00:03:05,449 --> 00:03:07,719
And start-up organizations are using Servius

86
00:03:08,169 --> 00:03:10,250
to, you know, keep their costs to net

87
00:03:10,250 --> 00:03:10,830
zero,

88
00:03:11,490 --> 00:03:13,710
especially in the early parts of their growth period,

89
00:03:13,969 --> 00:03:16,250
and then, um being able to iterate on features

90
00:03:16,250 --> 00:03:18,778
quickly. And government agencies

91
00:03:18,778 --> 00:03:20,949
as well, um, are, are leveraging

92
00:03:21,129 --> 00:03:22,399
uh servalus, um,

93
00:03:23,008 --> 00:03:25,508
agencies like the Australian Bureau of Statistics,

94
00:03:25,808 --> 00:03:27,830
who built their last Australian census

95
00:03:28,210 --> 00:03:29,349
entirely on servius.

96
00:03:31,219 --> 00:03:33,219
And yet, despite all of these

97
00:03:33,219 --> 00:03:35,778
successes and all of these, all of this diversity,

98
00:03:36,008 --> 00:03:38,460
um, you know, customers are still telling us we

99
00:03:38,460 --> 00:03:40,500
like the programming model, but there are still some

100
00:03:40,500 --> 00:03:41,360
use cases

101
00:03:41,618 --> 00:03:43,879
for which we need to look at other options.

102
00:03:44,219 --> 00:03:46,179
And that's led them to, um, you know,

103
00:03:46,520 --> 00:03:48,229
significant architectural shifts,

104
00:03:48,500 --> 00:03:50,899
um, and taking on much greater responsibility

105
00:03:50,899 --> 00:03:52,379
from an operations perspective.

106
00:03:53,149 --> 00:03:55,439
And they're asking for more control around

107
00:03:55,439 --> 00:03:57,679
where their um functions are running, and

108
00:03:57,679 --> 00:03:58,639
what they're running on.

109
00:03:59,080 --> 00:04:00,939
And also being able to apply

110
00:04:01,199 --> 00:04:03,758
some of the, you know, um, early commitments

111
00:04:03,758 --> 00:04:05,838
that they and usage discounts that they're making

112
00:04:05,838 --> 00:04:08,110
on other um on other um

113
00:04:08,110 --> 00:04:09,368
compute services.

114
00:04:09,800 --> 00:04:12,058
And looking at multi-concurrency as a way of

115
00:04:12,058 --> 00:04:14,300
um optimizing price and performance

116
00:04:14,300 --> 00:04:15,099
um as well.

117
00:04:16,668 --> 00:04:18,908
So land and managed instances is a solution

118
00:04:18,908 --> 00:04:19,468
to this.

119
00:04:19,858 --> 00:04:22,028
Um, we've got, uh, lender managed

120
00:04:22,028 --> 00:04:24,127
instances allows you to keep the

121
00:04:24,139 --> 00:04:26,148
the same programming model that you're familiar today

122
00:04:26,148 --> 00:04:28,088
and build architectures in the same,

123
00:04:28,427 --> 00:04:30,648
uh, surplus way as well, while maintaining,

124
00:04:30,988 --> 00:04:33,307
uh, a, a consistent and familiar

125
00:04:33,307 --> 00:04:34,389
development experience.

126
00:04:35,170 --> 00:04:37,488
And we're giving you more control over specialized

127
00:04:37,488 --> 00:04:38,149
compute

128
00:04:38,410 --> 00:04:40,428
and extensive choices around

129
00:04:40,428 --> 00:04:42,470
um the the compute that um you

130
00:04:42,608 --> 00:04:44,358
that that your functions are running on.

131
00:04:44,809 --> 00:04:47,069
And also taking advantage of no cold starts.

132
00:04:47,790 --> 00:04:50,189
And in addition to that, we're driving efficiencies

133
00:04:50,509 --> 00:04:51,579
through the um through and

134
00:04:51,939 --> 00:04:54,428
and predictability through ECP

135
00:04:54,428 --> 00:04:56,350
EC2 pricing mechanisms,

136
00:04:56,750 --> 00:04:57,869
while at the same time

137
00:04:58,149 --> 00:05:00,670
giving you an option around multi-concurrency invokes.

138
00:05:02,750 --> 00:05:05,250
So what is lambda managed instances? Fundamentally,

139
00:05:05,389 --> 00:05:07,699
it's the ability for you to run AWS

140
00:05:07,699 --> 00:05:08,790
lambda functions

141
00:05:09,059 --> 00:05:11,149
on EC2 instances of your

142
00:05:11,149 --> 00:05:12,889
choice in your account.

143
00:05:13,470 --> 00:05:15,629
You've got access to over 400

144
00:05:15,629 --> 00:05:16,910
different instance types

145
00:05:17,449 --> 00:05:19,709
across um general purpose compute

146
00:05:19,709 --> 00:05:20,428
optimized,

147
00:05:20,750 --> 00:05:22,899
memory optimized instance family to best

148
00:05:22,899 --> 00:05:24,949
suit your particular workload

149
00:05:24,949 --> 00:05:26,959
needs. And AWS

150
00:05:26,959 --> 00:05:29,088
is still handling all of the operational

151
00:05:29,088 --> 00:05:31,319
elements. We're dealing with the life cycle

152
00:05:31,319 --> 00:05:33,369
uh the appli the the the life cycle of

153
00:05:33,369 --> 00:05:34,009
the instance.

154
00:05:34,329 --> 00:05:35,869
We're managing the operating system,

155
00:05:36,290 --> 00:05:38,449
um, and the runtime patching that is that's built

156
00:05:38,449 --> 00:05:39,639
into those instances.

157
00:05:40,009 --> 00:05:42,350
We're dealing with all the routing and auto scaling

158
00:05:42,608 --> 00:05:45,009
and doing that according to your configurations.

159
00:05:45,869 --> 00:05:48,028
And at the same time, you're benefiting

160
00:05:48,028 --> 00:05:50,509
from the ability to um apply

161
00:05:50,509 --> 00:05:52,629
easy to pricing um constructs like

162
00:05:52,629 --> 00:05:54,338
easy to um savings plans,

163
00:05:54,709 --> 00:05:56,819
compute savings plans, reserved instances,

164
00:05:56,829 --> 00:05:58,829
and any other special agreements that you have with

165
00:05:58,829 --> 00:06:01,278
us. So

166
00:06:01,278 --> 00:06:03,838
the question is now, when do you use

167
00:06:03,838 --> 00:06:04,879
managed instances?

168
00:06:05,949 --> 00:06:07,988
Um So managed

169
00:06:07,988 --> 00:06:10,048
instances is not a um

170
00:06:10,048 --> 00:06:12,040
in place replacement for lander today.

171
00:06:12,500 --> 00:06:14,670
Specifically, you would be looking at using

172
00:06:14,670 --> 00:06:15,709
managed instances

173
00:06:15,988 --> 00:06:18,230
for things like high traffic, steady

174
00:06:18,230 --> 00:06:20,509
state workloads that have um

175
00:06:20,509 --> 00:06:22,829
you know, smooth and predictable traffic patterns.

176
00:06:23,569 --> 00:06:24,629
You'd be looking at

177
00:06:25,699 --> 00:06:27,230
using managed instances for

178
00:06:27,889 --> 00:06:29,519
applications that um have

179
00:06:29,798 --> 00:06:32,088
very specific computational needs

180
00:06:32,410 --> 00:06:34,439
or memory requirements or network

181
00:06:34,439 --> 00:06:35,730
uh throughput requirements.

182
00:06:37,048 --> 00:06:39,329
And for um and for everything else, you

183
00:06:39,329 --> 00:06:41,009
would continue to lose lambda.

184
00:06:41,449 --> 00:06:43,689
Lambda today, or lambda default as we're calling

185
00:06:43,689 --> 00:06:45,928
it, um, is really ideal

186
00:06:45,928 --> 00:06:47,928
for um you know, workloads

187
00:06:47,928 --> 00:06:49,480
with unpredictable traffic.

188
00:06:50,009 --> 00:06:52,389
And where you have short duration

189
00:06:52,689 --> 00:06:53,959
um and um

190
00:06:54,369 --> 00:06:56,428
and and infrequent invocations.

191
00:06:58,588 --> 00:07:00,629
So what we're gonna do today, um, I'm I'm

192
00:07:00,629 --> 00:07:01,540
just gonna walk you through,

193
00:07:02,069 --> 00:07:04,470
um, an experience around how to build out

194
00:07:04,639 --> 00:07:06,980
your lambda managed instances, um,

195
00:07:06,988 --> 00:07:09,069
environment. Um, and we'll talk about some

196
00:07:09,069 --> 00:07:10,059
key differences

197
00:07:10,439 --> 00:07:12,459
between what lambda default provides,

198
00:07:12,600 --> 00:07:14,879
and also what lambda, how lambda managed

199
00:07:14,879 --> 00:07:15,879
instances works.

200
00:07:16,309 --> 00:07:18,798
And then I'll round it off with some part integrations

201
00:07:18,798 --> 00:07:20,879
and information about the developer tooling

202
00:07:20,879 --> 00:07:23,199
that we're supporting, as well as some pricing

203
00:07:23,199 --> 00:07:24,858
information. Over to your channel.

204
00:07:25,939 --> 00:07:28,059
Thank you all right.

205
00:07:28,869 --> 00:07:30,988
Uh, hi everyone. So as Steven mentioned, I'm Arch.

206
00:07:31,000 --> 00:07:33,040
I'm a principal engineer with LAMDA, and

207
00:07:33,040 --> 00:07:35,079
most recently I've been the tech lead on this

208
00:07:35,079 --> 00:07:37,540
project which we're all very excited to bring to you.

209
00:07:38,040 --> 00:07:38,619
Um,

210
00:07:39,160 --> 00:07:41,160
so Steven gave us a really great introduction into

211
00:07:41,160 --> 00:07:43,358
why we built LAMDA managed instances or

212
00:07:43,358 --> 00:07:45,559
LMI as I'm gonna call it for the rest of this talk,

213
00:07:46,040 --> 00:07:48,439
um, and I'm gonna take you on a kind of technical

214
00:07:48,439 --> 00:07:50,579
deep dive into the experience of actually

215
00:07:50,920 --> 00:07:52,769
creating a function on LMI.

216
00:07:53,588 --> 00:07:55,750
And um as we walk through that

217
00:07:55,750 --> 00:07:57,949
experience um I want us to just pay

218
00:07:57,949 --> 00:07:59,689
attention to the ways in which

219
00:08:00,028 --> 00:08:02,410
the LMI experience is actually very

220
00:08:02,410 --> 00:08:04,470
similar and very familiar with the default

221
00:08:04,470 --> 00:08:06,548
lambda experience that you all kind of know and

222
00:08:06,548 --> 00:08:07,088
love today

223
00:08:07,750 --> 00:08:09,829
and then in a subsequent section we'll talk about

224
00:08:09,829 --> 00:08:11,379
the ways in which LMI is different from

225
00:08:12,108 --> 00:08:14,230
from lambda default and how that should inform

226
00:08:14,230 --> 00:08:16,488
your choice of when to use which platform.

227
00:08:18,649 --> 00:08:20,798
All right, so the eventual setup

228
00:08:20,798 --> 00:08:23,019
that we're going for here, like Steven mentioned

229
00:08:23,019 --> 00:08:25,000
is in your customer account

230
00:08:25,298 --> 00:08:26,619
in your VPC

231
00:08:26,939 --> 00:08:29,119
Lambda will launch EC2 instances

232
00:08:29,220 --> 00:08:31,500
and then deploy your function on

233
00:08:31,500 --> 00:08:33,000
those EC2 instances.

234
00:08:33,538 --> 00:08:35,538
Now these EC2 instances are what we

235
00:08:35,538 --> 00:08:37,585
call lambda. Managed instances

236
00:08:37,864 --> 00:08:39,904
and what that means is it's mostly

237
00:08:39,904 --> 00:08:42,125
just a regular EC2 instance except

238
00:08:42,125 --> 00:08:44,183
that it's fully managed by lambda.

239
00:08:44,504 --> 00:08:46,543
We handle the launching of this instance.

240
00:08:46,864 --> 00:08:49,264
We handle the OS patching of these instances,

241
00:08:49,585 --> 00:08:51,705
uh, and the entire life cycle of the instance right

242
00:08:51,705 --> 00:08:53,625
up to the termination of the instance.

243
00:08:54,830 --> 00:08:56,908
What you can do with the instance is you can see

244
00:08:56,908 --> 00:08:59,009
it in your console you can describe

245
00:08:59,009 --> 00:09:01,259
the instance, but you can't touch it in any way,

246
00:09:01,269 --> 00:09:02,889
even if you wanted to, so you can't

247
00:09:03,229 --> 00:09:05,229
update the instance you can't edit the instance you

248
00:09:05,229 --> 00:09:06,788
can't SSH into the instance,

249
00:09:07,070 --> 00:09:08,950
nor can you actually even terminate the instance.

250
00:09:09,308 --> 00:09:11,590
So the entire management of these instances are

251
00:09:11,590 --> 00:09:13,000
completely delegated to lambda,

252
00:09:13,389 --> 00:09:13,928
um,

253
00:09:14,469 --> 00:09:15,788
as the service managing it.

254
00:09:16,460 --> 00:09:18,590
And in terms of billing like Steven mentioned, you know,

255
00:09:18,869 --> 00:09:20,950
regular EC-2 billing applies to these

256
00:09:20,950 --> 00:09:23,330
instances along with any kind of pricing

257
00:09:23,330 --> 00:09:25,210
instruments that you have with EC-2.

258
00:09:26,820 --> 00:09:28,899
Now in terms of these functions that are deployed

259
00:09:28,899 --> 00:09:31,038
on your on your instances it's

260
00:09:31,038 --> 00:09:33,038
what we call um execution

261
00:09:33,038 --> 00:09:35,058
environments and for those of you of you

262
00:09:35,058 --> 00:09:36,119
who are not familiar,

263
00:09:36,379 --> 00:09:38,379
a lambda function execution environment is

264
00:09:38,379 --> 00:09:40,460
basically a live running copy

265
00:09:40,460 --> 00:09:42,779
of your application so it's your function code,

266
00:09:43,019 --> 00:09:45,168
the language run time underneath it, your

267
00:09:45,168 --> 00:09:47,239
layers, your extensions, all of that

268
00:09:47,250 --> 00:09:49,558
that's kind of bootstrapped and up and running,

269
00:09:49,739 --> 00:09:51,779
ready to handle and invoke so that's what

270
00:09:51,779 --> 00:09:53,899
we call a function execution environment.

271
00:09:55,639 --> 00:09:57,969
Alright, so how do you make all this magic happen

272
00:09:57,969 --> 00:09:58,928
in your account?

273
00:09:59,408 --> 00:10:01,908
The experience here involves 3 steps.

274
00:10:02,599 --> 00:10:04,558
And we'll look at each of these steps in detail,

275
00:10:04,879 --> 00:10:06,918
but the first step here is to create a

276
00:10:06,918 --> 00:10:08,279
capacity provider.

277
00:10:08,599 --> 00:10:10,599
This is where you give us all your

278
00:10:10,599 --> 00:10:12,599
instance level configuration and

279
00:10:12,599 --> 00:10:13,219
settings.

280
00:10:14,298 --> 00:10:15,918
And then you create a function

281
00:10:16,178 --> 00:10:18,178
and then you associate it with that capacity

282
00:10:18,178 --> 00:10:19,599
provider that you just created

283
00:10:20,099 --> 00:10:22,500
and then finally you publish a function

284
00:10:22,500 --> 00:10:24,619
version and this is the step that makes

285
00:10:24,619 --> 00:10:26,820
all the magic happen so the the launching of the

286
00:10:26,820 --> 00:10:29,139
instances and the deployment of your function on those

287
00:10:29,139 --> 00:10:29,719
instances.

288
00:10:30,879 --> 00:10:32,879
So let's take a look at each of these in a little

289
00:10:32,879 --> 00:10:35,399
bit deeper detail, starting with the capacity

290
00:10:35,399 --> 00:10:37,928
provider. The capacity

291
00:10:37,928 --> 00:10:40,168
provider is a brand new construct. It's a lambda

292
00:10:40,168 --> 00:10:42,288
construct that we've introduced just for

293
00:10:42,288 --> 00:10:44,369
LMI and like I said, it's

294
00:10:44,369 --> 00:10:46,408
basically all things instances all of

295
00:10:46,408 --> 00:10:48,529
your overrides and settings that you wanna

296
00:10:48,529 --> 00:10:50,529
provide to us in terms of your

297
00:10:50,529 --> 00:10:52,649
instance configuration goes in this capacity

298
00:10:52,649 --> 00:10:53,408
provider object.

299
00:10:55,879 --> 00:10:57,298
And here are some of the settings here.

300
00:10:57,798 --> 00:11:00,399
So the first one is your instance VPC config.

301
00:11:00,759 --> 00:11:02,798
The second one is the actual instance

302
00:11:02,798 --> 00:11:04,580
types that you want us to use,

303
00:11:04,840 --> 00:11:06,168
um, for your functions.

304
00:11:06,440 --> 00:11:07,538
And finally we'll look at,

305
00:11:07,798 --> 00:11:10,038
um, some guard rails that you can put

306
00:11:10,200 --> 00:11:12,210
in terms of how we scale your instances

307
00:11:12,210 --> 00:11:14,259
when the load on your functions goes up.

308
00:11:14,918 --> 00:11:16,639
So let's start with the VPC config.

309
00:11:17,979 --> 00:11:20,500
So here we have our new create capacity

310
00:11:20,500 --> 00:11:21,759
provider API.

311
00:11:22,379 --> 00:11:24,538
The first thing in there is the capacity

312
00:11:24,538 --> 00:11:26,580
provider name. You give it a name. You can put tags

313
00:11:26,580 --> 00:11:27,158
on it,

314
00:11:27,700 --> 00:11:29,779
and then, and then you have to give us a role. So

315
00:11:29,779 --> 00:11:31,808
this is the capacity provider operator role.

316
00:11:31,859 --> 00:11:33,918
So this is just a standard IAM role

317
00:11:34,119 --> 00:11:36,379
where you're giving lambda permissions to actually

318
00:11:36,379 --> 00:11:38,450
launch and manage those EC2 instances

319
00:11:38,450 --> 00:11:39,219
in your account.

320
00:11:40,788 --> 00:11:43,070
And then we come to the VPC config. This

321
00:11:43,070 --> 00:11:45,308
is standard VPC config subnets and

322
00:11:45,308 --> 00:11:46,109
security groups,

323
00:11:46,548 --> 00:11:48,668
and this is the VPC that will launch

324
00:11:48,668 --> 00:11:49,889
your instances into.

325
00:11:51,279 --> 00:11:53,460
Now in terms of required

326
00:11:54,000 --> 00:11:54,779
parameters

327
00:11:55,070 --> 00:11:57,158
this is all you really need to create

328
00:11:57,158 --> 00:11:58,450
a capacity provider.

329
00:11:58,879 --> 00:12:00,879
Now there's a whole bunch of other settings which

330
00:12:00,879 --> 00:12:03,000
we're gonna talk about, but just know

331
00:12:03,000 --> 00:12:04,178
that those are all

332
00:12:04,519 --> 00:12:06,840
kind of they all have defaults they're advanced

333
00:12:06,840 --> 00:12:08,918
settings for the power users who really

334
00:12:08,918 --> 00:12:11,239
wanna kind of fine tune the capacity that's

335
00:12:11,239 --> 00:12:12,759
that's underneath your functions,

336
00:12:13,080 --> 00:12:15,428
but in terms of required parameters,

337
00:12:15,440 --> 00:12:17,558
that's all you need. You need a role and you need a VPC

338
00:12:17,558 --> 00:12:18,940
and you need to give that to us.

339
00:12:20,739 --> 00:12:22,820
In terms of the subnets that you give

340
00:12:22,820 --> 00:12:24,340
us in your VPC config,

341
00:12:24,678 --> 00:12:26,239
um, you know, if it is a prod application,

342
00:12:26,798 --> 00:12:29,219
the standard kind of guidance, AWS

343
00:12:29,219 --> 00:12:30,099
guidance applies.

344
00:12:30,460 --> 00:12:32,889
Give us subnets in 3 availability zones

345
00:12:33,298 --> 00:12:35,308
because when you do that we will actually

346
00:12:35,308 --> 00:12:37,500
spread the instances that we launch and

347
00:12:37,500 --> 00:12:39,678
thus the execution environments across

348
00:12:39,859 --> 00:12:41,739
those availability zones evenly.

349
00:12:43,000 --> 00:12:44,250
In terms of networking.

350
00:12:45,058 --> 00:12:47,298
These are just regular EC2 instances,

351
00:12:47,308 --> 00:12:49,580
so they have a primary network interface

352
00:12:49,580 --> 00:12:51,908
in your VPC, which means they get an IP

353
00:12:51,908 --> 00:12:53,269
address from your VPC

354
00:12:53,629 --> 00:12:55,989
and all of the outbound traffic

355
00:12:55,989 --> 00:12:58,298
from your function execution environments

356
00:12:58,308 --> 00:13:00,210
actually transits through this

357
00:13:00,700 --> 00:13:02,330
network interface of the instance.

358
00:13:03,029 --> 00:13:05,149
So if your function wants to talk to any

359
00:13:05,149 --> 00:13:07,190
downstream dependencies, make sure that

360
00:13:07,190 --> 00:13:09,048
you have a path through your,

361
00:13:09,308 --> 00:13:11,369
uh, capacity provider VPC

362
00:13:11,369 --> 00:13:13,798
to those endpoints of those dependencies.

363
00:13:15,479 --> 00:13:17,918
Also, the, the application logs

364
00:13:17,918 --> 00:13:20,558
that we ship uh to CloudWatch,

365
00:13:20,960 --> 00:13:22,820
those logs also transit through

366
00:13:23,239 --> 00:13:25,178
this network interface of the instance.

367
00:13:25,599 --> 00:13:27,798
So another thing you wanna remember and

368
00:13:27,798 --> 00:13:29,798
make sure is that you actually have a path to the

369
00:13:29,798 --> 00:13:31,000
cloud watch endpoint

370
00:13:31,279 --> 00:13:32,678
through your your VPC.

371
00:13:32,960 --> 00:13:34,139
You can do this by either

372
00:13:34,599 --> 00:13:37,099
allowing Internet access to hit the public endpoint

373
00:13:37,320 --> 00:13:39,399
or you can use a private link cloud watch

374
00:13:39,399 --> 00:13:40,859
endpoint within your VPC,

375
00:13:41,119 --> 00:13:43,190
but just remember that all your logs are also

376
00:13:43,190 --> 00:13:43,979
transiting through

377
00:13:44,399 --> 00:13:45,519
this instance VPC.

378
00:13:47,808 --> 00:13:49,849
And then in terms of ingress traffic

379
00:13:49,849 --> 00:13:51,908
this is um the same as is true

380
00:13:51,908 --> 00:13:53,989
of lambda default functions today

381
00:13:54,320 --> 00:13:56,479
uh there is no ingress traffic that's

382
00:13:56,479 --> 00:13:57,469
coming in through

383
00:13:57,969 --> 00:14:00,009
that network interface to your instance

384
00:14:00,009 --> 00:14:02,048
or your execution environment so you can go

385
00:14:02,048 --> 00:14:04,168
ahead and kind of close all those inbound

386
00:14:04,168 --> 00:14:06,538
rules on the security groups that that you give us,

387
00:14:06,808 --> 00:14:09,288
right? All

388
00:14:09,288 --> 00:14:11,259
right, the other thing about VPC configuration is

389
00:14:12,369 --> 00:14:14,570
because all of the egress traffic from your functions

390
00:14:14,570 --> 00:14:16,548
is going through the instances network,

391
00:14:17,009 --> 00:14:19,048
um, we actually do not allow

392
00:14:19,048 --> 00:14:21,210
you to specify a VPC config

393
00:14:21,210 --> 00:14:23,609
at the function level. So in the create function API,

394
00:14:23,849 --> 00:14:25,808
if you're creating an LMI function,

395
00:14:26,080 --> 00:14:28,090
um, you cannot provide a VPC config.

396
00:14:28,168 --> 00:14:30,369
We'll always just use the VPC config that you

397
00:14:30,369 --> 00:14:31,989
provide in your capacity provider.

398
00:14:34,288 --> 00:14:36,570
All right, let's talk instance types.

399
00:14:38,519 --> 00:14:40,729
So the full set of instance

400
00:14:40,729 --> 00:14:42,820
types that is supported on LMI

401
00:14:43,210 --> 00:14:45,389
is basically these latest generation

402
00:14:45,389 --> 00:14:47,450
C, M, and R instance

403
00:14:47,450 --> 00:14:49,889
families. So C is the compute optimized

404
00:14:49,889 --> 00:14:51,109
DC2 instance family,

405
00:14:51,570 --> 00:14:53,570
uh, M is general purpose, and R is

406
00:14:53,570 --> 00:14:54,629
memory optimized.

407
00:14:55,009 --> 00:14:57,168
And in terms of sizes we support the

408
00:14:57,168 --> 00:14:59,250
dot large instance sizes and, and

409
00:14:59,250 --> 00:15:00,710
bigger within these families.

410
00:15:01,558 --> 00:15:03,639
And um in terms of architectures

411
00:15:03,639 --> 00:15:05,700
we support both Intel and AMD

412
00:15:05,700 --> 00:15:07,840
for X86 and then we also

413
00:15:07,840 --> 00:15:10,259
support the ARM graviton instance types.

414
00:15:11,580 --> 00:15:13,658
Now within this kind of large set of

415
00:15:13,658 --> 00:15:14,879
instance types that we support

416
00:15:15,440 --> 00:15:16,359
by default

417
00:15:16,649 --> 00:15:18,940
lambda will select the instance types

418
00:15:18,940 --> 00:15:20,119
for your function

419
00:15:20,408 --> 00:15:22,500
based on your functions, memory size

420
00:15:22,500 --> 00:15:24,580
and configuration, and we'll talk a little bit more

421
00:15:24,580 --> 00:15:26,340
about that when we get to the function section.

422
00:15:27,288 --> 00:15:27,879
But

423
00:15:28,460 --> 00:15:30,308
you can always override

424
00:15:30,619 --> 00:15:32,658
if you want to constrain the set of

425
00:15:32,658 --> 00:15:34,779
instance types that we use some more like

426
00:15:34,779 --> 00:15:36,308
not use this entire set

427
00:15:36,580 --> 00:15:38,940
you can do that via an override

428
00:15:39,019 --> 00:15:41,139
and that's where we come to this instance requirements

429
00:15:41,139 --> 00:15:43,158
section within the capacity provider

430
00:15:43,700 --> 00:15:44,960
here you can specify

431
00:15:46,058 --> 00:15:48,099
uh allowed instance types which is only use

432
00:15:48,099 --> 00:15:50,259
these instance types or you can specify

433
00:15:50,259 --> 00:15:52,500
excluded instance types which is saying you

434
00:15:52,500 --> 00:15:53,399
know use

435
00:15:53,979 --> 00:15:55,099
everything else but these.

436
00:15:55,899 --> 00:15:57,940
And then a few other settings in here, um,

437
00:15:58,038 --> 00:16:01,210
architecture by default we assume it's an X86

438
00:16:01,210 --> 00:16:03,200
application, uh, you have to overwrite it for ARM.

439
00:16:03,779 --> 00:16:05,779
The other thing is to remember that your architecture

440
00:16:05,779 --> 00:16:07,859
of your function matches the architecture of your

441
00:16:07,859 --> 00:16:08,798
capacity provider.

442
00:16:11,058 --> 00:16:13,058
And finally for the

443
00:16:13,058 --> 00:16:13,649
um

444
00:16:14,019 --> 00:16:16,219
EBS volumes that are attached

445
00:16:16,219 --> 00:16:17,298
to your instances

446
00:16:17,619 --> 00:16:19,940
by default they're encrypted with a service

447
00:16:19,940 --> 00:16:20,779
managed key

448
00:16:21,099 --> 00:16:23,178
uh here you can provide your own KMS

449
00:16:23,178 --> 00:16:25,408
key that we we can use to to encrypt the EBS

450
00:16:25,408 --> 00:16:26,038
volumes.

451
00:16:28,038 --> 00:16:30,090
All right, um, scaling, so we

452
00:16:30,090 --> 00:16:31,548
have a, we have a,

453
00:16:31,849 --> 00:16:34,090
a pretty deep dive into scaling, um,

454
00:16:34,168 --> 00:16:35,590
a few sections later,

455
00:16:35,879 --> 00:16:38,200
uh, and we'll talk about some of the more advanced scaling

456
00:16:38,200 --> 00:16:39,379
configurations there,

457
00:16:39,798 --> 00:16:41,969
but here I just wanted to introduce that

458
00:16:41,969 --> 00:16:44,219
there is a capacity provider scaling config

459
00:16:44,219 --> 00:16:46,678
section, so this is all instance level scaling

460
00:16:46,678 --> 00:16:47,928
here that we're talking about,

461
00:16:48,489 --> 00:16:51,168
um, and the first setting there is a max VCPU

462
00:16:51,168 --> 00:16:53,629
count. Uh, this is basically

463
00:16:53,808 --> 00:16:54,940
a limit on the.

464
00:16:55,279 --> 00:16:57,359
The maximum instance capacity that we

465
00:16:57,359 --> 00:16:59,719
can scale out to uh as the load

466
00:16:59,719 --> 00:17:01,609
increases within your capacity provider,

467
00:17:02,000 --> 00:17:04,150
um, it's mostly useful as a cost control

468
00:17:04,150 --> 00:17:06,160
knob so you can put kind of a hard

469
00:17:06,160 --> 00:17:08,439
limit on the kind of instance billing

470
00:17:08,439 --> 00:17:10,680
that can occur from a given capacity

471
00:17:10,680 --> 00:17:11,259
provider

472
00:17:11,880 --> 00:17:14,318
again optional setting, uh, we have defaults

473
00:17:14,318 --> 00:17:16,358
you can override it only if you have a need to.

474
00:17:18,548 --> 00:17:20,708
Yeah, and like I said, there's a, there's more settings in

475
00:17:20,708 --> 00:17:22,827
here which we'll talk about in the context of the

476
00:17:22,827 --> 00:17:23,868
scaling section.

477
00:17:26,059 --> 00:17:28,009
All right, so now you have a capacity provider.

478
00:17:28,348 --> 00:17:30,078
Your next step is to create a function,

479
00:17:30,390 --> 00:17:32,549
and this is where kind of the, the

480
00:17:32,549 --> 00:17:34,630
familiar lambda experience kicks in.

481
00:17:34,910 --> 00:17:37,068
The, the process to create a function is

482
00:17:37,068 --> 00:17:39,189
almost exactly the same thing as you would

483
00:17:39,189 --> 00:17:40,150
go through today.

484
00:17:41,108 --> 00:17:43,229
With the only little change is that when you're creating

485
00:17:43,229 --> 00:17:45,309
a function you have to associate it with this

486
00:17:45,309 --> 00:17:47,689
capacity provider that you just created,

487
00:17:47,910 --> 00:17:50,189
which is what lets us know that this is an LMI

488
00:17:50,189 --> 00:17:52,549
function and it needs to be deployed on your

489
00:17:52,549 --> 00:17:54,779
instances as opposed to a default lambda

490
00:17:54,779 --> 00:17:56,430
function that goes on our infrastructure.

491
00:17:57,769 --> 00:18:00,078
Um, and then we'll just talk about some of the function

492
00:18:00,078 --> 00:18:02,118
features that are supported with LMI,

493
00:18:02,930 --> 00:18:05,068
um, and finally we'll,

494
00:18:05,199 --> 00:18:07,368
uh, talk about the function

495
00:18:07,368 --> 00:18:08,979
memory and CPU settings

496
00:18:09,289 --> 00:18:11,449
and how that influences the, the instance

497
00:18:11,449 --> 00:18:13,660
type that we select underneath

498
00:18:13,660 --> 00:18:14,509
those functions.

499
00:18:15,420 --> 00:18:16,000
All right.

500
00:18:17,328 --> 00:18:18,150
So this is our

501
00:18:18,608 --> 00:18:21,170
good old familiar create function API

502
00:18:21,608 --> 00:18:24,289
and in here we have a new section

503
00:18:24,289 --> 00:18:26,769
for the capacity provider config

504
00:18:26,930 --> 00:18:29,410
and that's where you provide the capacity

505
00:18:29,410 --> 00:18:31,568
provider R and it's just as simple as

506
00:18:31,568 --> 00:18:33,410
that to make it an LMI function.

507
00:18:34,380 --> 00:18:36,739
And uh you can associate

508
00:18:36,739 --> 00:18:38,818
multiple functions with the same

509
00:18:38,818 --> 00:18:40,078
capacity provider,

510
00:18:40,338 --> 00:18:42,420
in which case all of those functions will

511
00:18:42,420 --> 00:18:44,439
share the same instance capacity

512
00:18:44,689 --> 00:18:45,279
um

513
00:18:45,618 --> 00:18:46,979
within that capacity provider.

514
00:18:49,598 --> 00:18:51,880
All right, moving on to function features

515
00:18:51,880 --> 00:18:52,979
that are supported,

516
00:18:53,439 --> 00:18:53,959
um.

517
00:18:54,949 --> 00:18:56,979
Packaging formats, we support both

518
00:18:56,979 --> 00:18:59,239
OCI containers and zip

519
00:18:59,239 --> 00:18:59,910
format,

520
00:19:00,509 --> 00:19:01,588
language runtime,

521
00:19:02,019 --> 00:19:04,150
latest versions of Java, Python,

522
00:19:04,390 --> 00:19:05,949
Node, and .NET.

523
00:19:06,750 --> 00:19:08,858
So in addition to OS patching that's

524
00:19:08,858 --> 00:19:10,910
happening at the instance level, you continue to

525
00:19:10,910 --> 00:19:13,108
get the benefit of like the actual language run

526
00:19:13,108 --> 00:19:15,430
time is being managed and patched by lambda

527
00:19:15,430 --> 00:19:17,729
as well so that's the same familiar experience.

528
00:19:18,348 --> 00:19:20,390
Uh, other features that we support in terms

529
00:19:20,390 --> 00:19:21,088
of kind of

530
00:19:21,390 --> 00:19:23,509
the observability space layers and

531
00:19:23,509 --> 00:19:25,358
extensions are supported with LMI

532
00:19:26,029 --> 00:19:28,150
in terms of invoke kind of dynamics

533
00:19:28,150 --> 00:19:31,068
function URLs you can use with LMI response streaming

534
00:19:31,068 --> 00:19:31,989
works with LMI.

535
00:19:32,729 --> 00:19:33,318
Um

536
00:19:34,088 --> 00:19:36,130
The invoked timeout is 15 minutes, which

537
00:19:36,130 --> 00:19:38,289
is the same as, uh, default

538
00:19:38,289 --> 00:19:39,170
functions today.

539
00:19:39,880 --> 00:19:40,699
Um,

540
00:19:41,239 --> 00:19:43,799
but finally we did announce, uh, another

541
00:19:43,799 --> 00:19:46,368
big launch from Lambda was durable functions

542
00:19:46,368 --> 00:19:48,019
which allows you to run

543
00:19:48,358 --> 00:19:49,979
longer running functions,

544
00:19:50,279 --> 00:19:50,910
um,

545
00:19:51,239 --> 00:19:53,318
that can tolerate interruptions and kind

546
00:19:53,318 --> 00:19:55,358
of multi-step applications and

547
00:19:55,358 --> 00:19:57,150
that also works with LMI.

548
00:19:59,920 --> 00:20:01,630
In terms of some things that are

549
00:20:01,890 --> 00:20:04,009
not supported or not applicable I should

550
00:20:04,009 --> 00:20:04,858
say to LMI,

551
00:20:05,529 --> 00:20:07,809
uh, Snapstart is one that is

552
00:20:07,809 --> 00:20:09,420
not applicable to LMI

553
00:20:09,930 --> 00:20:12,348
because there are actually no cold starts in

554
00:20:12,449 --> 00:20:14,489
LMI and we'll talk a little bit more about

555
00:20:14,489 --> 00:20:16,608
that in the scaling section, but because we don't

556
00:20:16,608 --> 00:20:17,130
have cold

557
00:20:17,459 --> 00:20:18,130
cold starts,

558
00:20:18,410 --> 00:20:20,449
uh, snapstart is not meaningful in LMI.

559
00:20:21,289 --> 00:20:24,279
And, uh, provision and reserved concurrency,

560
00:20:25,059 --> 00:20:27,059
um, these are, are not supported

561
00:20:27,059 --> 00:20:29,338
with LMI because basically we have,

562
00:20:29,739 --> 00:20:31,979
um, equally valent concepts in LMI

563
00:20:31,979 --> 00:20:34,299
by means of min and max execution environments

564
00:20:34,299 --> 00:20:36,358
again this is something we'll see when we talk about

565
00:20:36,358 --> 00:20:37,160
scaling.

566
00:20:40,279 --> 00:20:42,459
All right, so let's talk about function

567
00:20:42,459 --> 00:20:43,459
settings now.

568
00:20:44,039 --> 00:20:46,529
Um, as most of you are familiar,

569
00:20:46,828 --> 00:20:49,039
uh, the create function has a memory

570
00:20:49,039 --> 00:20:51,170
size setting where you tell us how much memory your

571
00:20:51,170 --> 00:20:53,199
functions execution environments should get.

572
00:20:53,680 --> 00:20:55,920
Um, in terms of the ranges, the range

573
00:20:55,920 --> 00:20:57,880
has gone up higher for LMI,

574
00:20:58,358 --> 00:21:00,358
uh, so we do support up to 32

575
00:21:00,358 --> 00:21:02,719
gigs of memory size for your LMI

576
00:21:02,719 --> 00:21:03,380
functions.

577
00:21:05,189 --> 00:21:07,588
And then another new setting that we've

578
00:21:07,588 --> 00:21:10,108
introduced for LMI is this execution

579
00:21:10,108 --> 00:21:12,229
environment memory per VCPU. So

580
00:21:12,229 --> 00:21:14,348
this is basically a ratio of memory

581
00:21:14,348 --> 00:21:16,608
to VCPU for your functions execution

582
00:21:16,608 --> 00:21:17,209
environments.

583
00:21:18,459 --> 00:21:20,818
So based on your memory and this ratio

584
00:21:20,818 --> 00:21:23,439
we will extrapolate how much CPU needs to be allocated.

585
00:21:23,900 --> 00:21:26,088
Um, the default is 2 to 1,

586
00:21:26,380 --> 00:21:28,598
so, uh, 2 gigs

587
00:21:28,779 --> 00:21:29,969
per per VCPU,

588
00:21:30,618 --> 00:21:32,618
uh, and it can go up to 4 to 1

589
00:21:32,618 --> 00:21:34,650
or 8 to 1 are the allowed values

590
00:21:34,650 --> 00:21:37,559
there. Um,

591
00:21:37,640 --> 00:21:39,880
so applying those ratios and the memory settings,

592
00:21:39,920 --> 00:21:42,000
this is kind of the table of all combinations

593
00:21:42,000 --> 00:21:42,900
that we allow.

594
00:21:44,430 --> 00:21:45,848
One thing to note here

595
00:21:46,400 --> 00:21:49,029
is that we don't allow fractional VCPUs,

596
00:21:49,189 --> 00:21:51,189
so, uh, depending on the ratio

597
00:21:51,189 --> 00:21:53,269
that you're using, your memory will kind of jump

598
00:21:53,269 --> 00:21:55,469
in multiples of the ratios if you're using 8

599
00:21:55,469 --> 00:21:57,989
to 1, you can only have memory that's in multiples

600
00:21:57,989 --> 00:22:00,199
of 8. If you're using 4 to 1, you can only have

601
00:22:00,199 --> 00:22:01,769
memory that's multiples of 4.

602
00:22:03,380 --> 00:22:05,699
And then in terms of instance type selection,

603
00:22:05,900 --> 00:22:08,380
these ratios that we have, they basically

604
00:22:08,380 --> 00:22:10,459
match the family, the

605
00:22:10,459 --> 00:22:12,299
instance family ratios that we have up there.

606
00:22:12,618 --> 00:22:14,618
So if, if you're doing a 2 to 1 kind

607
00:22:14,618 --> 00:22:15,279
of ratio,

608
00:22:15,618 --> 00:22:16,358
uh, function,

609
00:22:16,809 --> 00:22:19,000
then that's when we'll select the, the compute,

610
00:22:19,219 --> 00:22:21,449
uh, optimized instance types. If you're

611
00:22:21,449 --> 00:22:23,630
on the other end of the spectrum and you're doing an 8

612
00:22:23,630 --> 00:22:24,400
to 1 ratio,

613
00:22:24,699 --> 00:22:26,739
then we'll select the memory optimized instance

614
00:22:26,739 --> 00:22:27,719
types underneath your

615
00:22:28,059 --> 00:22:30,199
function, and 4 to 1 is the general purpose,

616
00:22:30,670 --> 00:22:31,880
um, instance types.

617
00:22:34,479 --> 00:22:36,608
All right, so now we have a capacity provider

618
00:22:36,608 --> 00:22:38,608
we have a function that's designated

619
00:22:38,608 --> 00:22:39,848
as an LMI function,

620
00:22:40,209 --> 00:22:42,519
but there's still no instances

621
00:22:42,519 --> 00:22:44,689
or execution environments in your in your

622
00:22:44,689 --> 00:22:45,509
account yet.

623
00:22:46,049 --> 00:22:48,930
All of that magic happens when you actually

624
00:22:48,939 --> 00:22:50,509
publish a function version.

625
00:22:51,880 --> 00:22:54,000
Which is what actually triggers the deployment in

626
00:22:54,000 --> 00:22:54,699
this case.

627
00:22:55,670 --> 00:22:57,910
So publishing a function version is actually

628
00:22:57,910 --> 00:23:00,289
a functionality again that exists today.

629
00:23:00,630 --> 00:23:02,729
Uh, it's not required

630
00:23:02,729 --> 00:23:04,910
for lambda default functions so the only

631
00:23:04,910 --> 00:23:07,019
change here is with LMI functions you have to

632
00:23:07,019 --> 00:23:08,568
publish a version to actually deploy it.

633
00:23:09,269 --> 00:23:11,309
Um, but there's, there's two ways

634
00:23:11,309 --> 00:23:13,469
you can do that. You can either use the existing published

635
00:23:13,469 --> 00:23:14,328
version API

636
00:23:14,910 --> 00:23:16,930
or we actually have a little bit of

637
00:23:16,930 --> 00:23:18,989
syn syntactic sugar in the create and

638
00:23:18,989 --> 00:23:21,029
update function API where you can just

639
00:23:21,029 --> 00:23:23,630
set a flag to be true, the publish

640
00:23:23,630 --> 00:23:24,630
flag to be true,

641
00:23:24,989 --> 00:23:27,269
and then every create and update will automatically

642
00:23:27,269 --> 00:23:29,049
publish a version for you behind the scenes.

643
00:23:31,598 --> 00:23:33,650
So it's when you publish this version that's when all

644
00:23:33,650 --> 00:23:35,489
the action starts happening in your account.

645
00:23:36,189 --> 00:23:38,229
And we'll take a look at what that deployment looks

646
00:23:38,229 --> 00:23:40,549
like and then we'll also take a look at the function

647
00:23:40,549 --> 00:23:42,209
level scaling configuration.

648
00:23:44,848 --> 00:23:46,029
All right, so

649
00:23:46,449 --> 00:23:48,529
when you call published version, the first thing

650
00:23:48,529 --> 00:23:49,709
we'll do is we'll actually

651
00:23:50,130 --> 00:23:52,289
map your function to the instance type that's

652
00:23:52,289 --> 00:23:54,489
appropriate for it. We'll launch those EC2

653
00:23:54,489 --> 00:23:55,630
instances in your account,

654
00:23:55,969 --> 00:23:58,068
and on those EC2 instances

655
00:23:58,250 --> 00:24:00,469
we'll actually go ahead and initialize,

656
00:24:00,920 --> 00:24:01,449
um,

657
00:24:01,769 --> 00:24:04,209
by default 3 execution environments

658
00:24:04,209 --> 00:24:06,489
on those instances. We'll launch 3 instances

659
00:24:06,489 --> 00:24:08,489
in 3 availability zones if you've

660
00:24:08,489 --> 00:24:09,309
given us 3

661
00:24:09,608 --> 00:24:11,799
and and initialize 3 execution

662
00:24:11,799 --> 00:24:12,750
environments on them.

663
00:24:13,229 --> 00:24:15,348
And until until the

664
00:24:15,348 --> 00:24:17,400
initialization is complete, your function is

665
00:24:17,400 --> 00:24:19,549
actually in a pending state and only

666
00:24:19,549 --> 00:24:21,630
after it goes active can you

667
00:24:21,630 --> 00:24:22,809
actually start invoking

668
00:24:23,108 --> 00:24:23,838
your function,

669
00:24:24,108 --> 00:24:24,650
all right?

670
00:24:25,979 --> 00:24:28,180
Now these, these 3 execution environments

671
00:24:28,180 --> 00:24:30,680
that come up as part of your function activation,

672
00:24:31,000 --> 00:24:33,279
uh, we call them min execution environments

673
00:24:33,618 --> 00:24:35,739
it's 3 by default, but of

674
00:24:35,739 --> 00:24:38,140
course, um, you can always overwrite

675
00:24:38,140 --> 00:24:40,299
them which comes to our function

676
00:24:40,299 --> 00:24:41,578
scaling config section.

677
00:24:42,289 --> 00:24:43,189
So this is

678
00:24:43,608 --> 00:24:45,848
yet another new API for you

679
00:24:45,848 --> 00:24:48,049
to override some function level scaling

680
00:24:48,049 --> 00:24:48,729
parameters.

681
00:24:49,130 --> 00:24:51,368
This is the put function scaling configuration

682
00:24:51,368 --> 00:24:52,068
API.

683
00:24:53,019 --> 00:24:55,539
Here you have min and max execution

684
00:24:55,539 --> 00:24:57,019
environments which is bounding

685
00:24:57,299 --> 00:24:59,539
at the function level how many execution

686
00:24:59,539 --> 00:25:01,199
environments we can scale in and out.

687
00:25:02,750 --> 00:25:04,959
In terms of min, like we said, the default

688
00:25:04,959 --> 00:25:05,660
is 3,

689
00:25:06,519 --> 00:25:08,680
some, some, some kind of reasons why

690
00:25:08,680 --> 00:25:11,239
you might wanna override these some min, if you

691
00:25:11,239 --> 00:25:12,118
wanna get,

692
00:25:12,559 --> 00:25:14,680
uh, if you wanna pre-provision capacity

693
00:25:14,680 --> 00:25:16,890
for like a known peak or a known

694
00:25:16,890 --> 00:25:19,150
known incoming demand or a buffer,

695
00:25:19,400 --> 00:25:21,880
you can set your min to be higher and we'll always

696
00:25:21,880 --> 00:25:23,910
keep those execution environments warm and up

697
00:25:23,910 --> 00:25:25,969
and running. Um, on the other

698
00:25:25,969 --> 00:25:28,250
hand, you, you can override it lower if

699
00:25:28,250 --> 00:25:30,150
you don't care for having 3,

700
00:25:31,130 --> 00:25:33,209
execution environments up and running and for that kind

701
00:25:33,209 --> 00:25:35,529
of high availability stance if you're using

702
00:25:35,529 --> 00:25:37,568
dev or test workloads you can override that

703
00:25:37,568 --> 00:25:38,219
to be lower.

704
00:25:39,239 --> 00:25:41,640
In terms of max execution environments,

705
00:25:41,719 --> 00:25:43,769
uh, by default there is no max, so

706
00:25:43,769 --> 00:25:45,890
we're basically allowed to scale as much as we

707
00:25:45,890 --> 00:25:46,509
need to,

708
00:25:47,009 --> 00:25:47,598
um,

709
00:25:47,969 --> 00:25:50,088
you can override it for

710
00:25:50,098 --> 00:25:52,289
fair sharing between multiple functions

711
00:25:52,289 --> 00:25:54,410
that are mapped to the same capacity provider.

712
00:25:54,489 --> 00:25:56,848
So like I said, if you have multiple functions

713
00:25:56,848 --> 00:25:58,969
in the same capacity provider, they share the

714
00:25:58,969 --> 00:26:01,309
instance capacity within that capacity provider

715
00:26:01,608 --> 00:26:03,689
so you can cap how much each

716
00:26:03,689 --> 00:26:04,750
can scale to,

717
00:26:05,009 --> 00:26:07,529
um, to kind of prevent noisy neighbor

718
00:26:07,529 --> 00:26:08,250
disturbance.

719
00:26:08,838 --> 00:26:10,920
So this is what I was saying. So these are kind

720
00:26:10,920 --> 00:26:13,078
of equivalent to the provision

721
00:26:13,078 --> 00:26:15,519
capacity reserve uh provision concurrency

722
00:26:15,519 --> 00:26:17,799
reserve concurrency model that we have with on

723
00:26:17,799 --> 00:26:19,959
demand. We're just doing it slightly differently here

724
00:26:19,959 --> 00:26:20,509
for LMI.

725
00:26:22,199 --> 00:26:22,709
And,

726
00:26:23,009 --> 00:26:24,979
finally, uh, another small trick here,

727
00:26:25,519 --> 00:26:27,680
um, you can set your

728
00:26:27,680 --> 00:26:29,769
min and max to 0,

729
00:26:30,078 --> 00:26:32,199
and what that will do is that will basically

730
00:26:32,199 --> 00:26:34,459
cause your function to completely descale

731
00:26:34,559 --> 00:26:36,779
and scale down within your capacity provider.

732
00:26:37,160 --> 00:26:39,180
It is in a deactivated state

733
00:26:39,269 --> 00:26:41,500
at that point, so invokes won't go

734
00:26:41,500 --> 00:26:42,939
through, um,

735
00:26:43,640 --> 00:26:45,719
so you'll have to come back and set min and max to

736
00:26:45,719 --> 00:26:47,759
something greater than 0 for us to scale it

737
00:26:47,759 --> 00:26:49,838
back up and allow those invokes to go

738
00:26:49,838 --> 00:26:51,400
through. So it's just a way for you to.

739
00:26:51,789 --> 00:26:53,910
Basically deactivate the function. So if you're going home

740
00:26:53,910 --> 00:26:56,029
at night, um, you know, for your dev

741
00:26:56,029 --> 00:26:58,479
test workloads, you don't have to actually delete the function.

742
00:26:58,509 --> 00:27:00,588
You can scale it down and then when you come back in the

743
00:27:00,588 --> 00:27:02,568
morning you can scale it back up, right?

744
00:27:04,890 --> 00:27:06,309
All right, so

745
00:27:06,930 --> 00:27:09,009
you know you created a capacity provider, you created

746
00:27:09,009 --> 00:27:09,588
a function,

747
00:27:09,930 --> 00:27:12,219
you published the version we did the deployment

748
00:27:12,219 --> 00:27:14,588
on your instances and now you're ready to invoke

749
00:27:14,969 --> 00:27:17,348
and the the beauty of this feature

750
00:27:17,348 --> 00:27:19,559
is that your invoke experience is

751
00:27:19,559 --> 00:27:21,769
exactly the same as what it is today.

752
00:27:23,529 --> 00:27:25,608
Um, so when your invoke

753
00:27:25,608 --> 00:27:27,769
comes to us we will check if your function is

754
00:27:27,769 --> 00:27:30,009
an LMI function, if it has that capacity

755
00:27:30,009 --> 00:27:31,229
provider associated with it,

756
00:27:31,529 --> 00:27:33,568
and if it does, then we will just route

757
00:27:33,568 --> 00:27:35,598
all of your invokes to these, these

758
00:27:35,598 --> 00:27:37,650
function and, uh, execution environments that we've

759
00:27:37,650 --> 00:27:39,150
deployed on your instances.

760
00:27:40,828 --> 00:27:43,029
And because the invoke experience is

761
00:27:43,029 --> 00:27:44,059
exactly the same,

762
00:27:44,430 --> 00:27:46,430
you actually get all of the event

763
00:27:46,430 --> 00:27:48,650
source integrations that are supported with Lambda

764
00:27:48,650 --> 00:27:50,670
today just work out of the box

765
00:27:50,670 --> 00:27:52,910
just the same with your LMI functions,

766
00:27:53,390 --> 00:27:55,699
um, including kind of the, uh,

767
00:27:55,709 --> 00:27:57,769
the, the different invoke types that we have.

768
00:27:57,818 --> 00:28:00,029
We have the direct invoke or synchronous

769
00:28:00,029 --> 00:28:02,189
invoke as we call it. We also support the event

770
00:28:02,189 --> 00:28:04,309
invocation type and all of

771
00:28:04,309 --> 00:28:06,348
this whole slew of event

772
00:28:06,348 --> 00:28:08,650
integrations work just as is.

773
00:28:13,059 --> 00:28:15,078
So you know we saw how the the

774
00:28:15,088 --> 00:28:17,779
the create function and the invoke experience

775
00:28:17,779 --> 00:28:18,779
basically we

776
00:28:19,039 --> 00:28:21,160
we've retained that as close

777
00:28:21,459 --> 00:28:23,739
closely as possible with the the lambda

778
00:28:23,739 --> 00:28:24,939
default experience

779
00:28:25,500 --> 00:28:27,578
but with just a few extra clicks and a few

780
00:28:27,578 --> 00:28:29,630
extra steps with the capacity provider

781
00:28:29,858 --> 00:28:32,000
we've effectively completely

782
00:28:32,338 --> 00:28:34,779
changed the infrastructure underneath

783
00:28:34,779 --> 00:28:37,500
your functions from service owned infrastructure

784
00:28:37,500 --> 00:28:40,140
to just regular good old EC2

785
00:28:40,140 --> 00:28:41,279
instances in your account.

786
00:28:42,029 --> 00:28:44,029
And because of that kind of big

787
00:28:44,029 --> 00:28:46,630
capacity shift underneath your functions

788
00:28:46,630 --> 00:28:48,250
there are some differences

789
00:28:48,719 --> 00:28:50,789
between lambda default and LMI in terms of

790
00:28:50,789 --> 00:28:52,789
the the management of the capacity

791
00:28:52,789 --> 00:28:53,529
underneath

792
00:28:53,868 --> 00:28:54,449
um

793
00:28:54,868 --> 00:28:56,328
that you should be aware of.

794
00:28:57,309 --> 00:28:58,549
When you're using LMI,

795
00:28:59,180 --> 00:29:00,729
so let's take a look at those.

796
00:29:02,598 --> 00:29:04,390
The first one here is concurrency.

797
00:29:04,969 --> 00:29:06,989
I'm sure for for those of you who've used

798
00:29:06,989 --> 00:29:09,328
lambda before, you're probably intimately

799
00:29:09,328 --> 00:29:11,449
familiar with the concurrency

800
00:29:11,449 --> 00:29:12,259
of lambda

801
00:29:12,519 --> 00:29:13,650
default functions.

802
00:29:14,098 --> 00:29:16,209
Uh, concurrency has a slightly different

803
00:29:16,209 --> 00:29:18,500
meaning in the context of LMI,

804
00:29:18,809 --> 00:29:21,068
and Steven briefly mentioned that we do, um,

805
00:29:21,368 --> 00:29:23,410
support multi-concurrent functions in

806
00:29:23,410 --> 00:29:25,549
LMI, so we'll take a look at that here.

807
00:29:26,130 --> 00:29:28,009
The second one is scaling.

808
00:29:28,670 --> 00:29:30,709
Uh, this is another thing that Steven mentioned is that

809
00:29:30,709 --> 00:29:32,250
we don't have any cold starts

810
00:29:32,539 --> 00:29:33,269
in LMI,

811
00:29:33,588 --> 00:29:35,670
so we do scaling in a slightly different way

812
00:29:35,670 --> 00:29:37,709
in LMI, and we'll, we'll take a deeper look at

813
00:29:37,709 --> 00:29:40,170
that and understand how scaling happens in LMI.

814
00:29:40,779 --> 00:29:43,068
And finally, um, the

815
00:29:43,068 --> 00:29:44,068
security boundary.

816
00:29:44,469 --> 00:29:46,549
With, uh, with lambda default

817
00:29:46,549 --> 00:29:48,910
you're running in in our service

818
00:29:48,910 --> 00:29:50,880
account it's a fully multi-tenant set up

819
00:29:51,229 --> 00:29:53,328
here with, uh, LMI

820
00:29:53,328 --> 00:29:55,390
you're gonna be running in your account which is

821
00:29:55,390 --> 00:29:57,549
a fully kind of single tenant set up so

822
00:29:57,549 --> 00:29:59,630
the boundaries are a little bit different there in

823
00:29:59,630 --> 00:30:01,608
terms of security and we'll, we'll talk about that.

824
00:30:02,699 --> 00:30:04,368
All right, let's start with concurrency.

825
00:30:05,900 --> 00:30:08,108
Before we dive into LMI, let's recap a little

826
00:30:08,108 --> 00:30:10,380
bit about what concurrency means for

827
00:30:10,380 --> 00:30:12,150
um lambda default.

828
00:30:13,078 --> 00:30:15,239
So in lambda default when you have a function.

829
00:30:15,949 --> 00:30:17,449
You see an invoke come in

830
00:30:17,750 --> 00:30:19,868
um if we don't have any execution environments

831
00:30:19,868 --> 00:30:20,939
for your function yet,

832
00:30:21,269 --> 00:30:23,269
what we will do is in the path of the

833
00:30:23,269 --> 00:30:25,549
invoke we will say ah there's no execution

834
00:30:25,549 --> 00:30:27,709
environment we'll initialize a new execution

835
00:30:27,709 --> 00:30:29,750
environment and then execute your

836
00:30:29,750 --> 00:30:31,709
invoke within that execution environment.

837
00:30:32,890 --> 00:30:35,039
Now while that invoke is ongoing,

838
00:30:35,410 --> 00:30:37,608
if we get a 2nd invoke, we'll say, oh

839
00:30:37,608 --> 00:30:39,739
that execution environment one is busy,

840
00:30:40,000 --> 00:30:42,049
and what that will do is it'll initialize a

841
00:30:42,049 --> 00:30:44,289
second execution environment within which your 2nd

842
00:30:44,289 --> 00:30:45,410
invoke will execute.

843
00:30:45,930 --> 00:30:47,650
Now these invokes that.

844
00:30:48,390 --> 00:30:50,469
Cause the initialization of

845
00:30:50,469 --> 00:30:52,390
new execution environments,

846
00:30:52,650 --> 00:30:54,949
um, we call them cold starts and they incur

847
00:30:54,949 --> 00:30:57,368
slightly higher latency because we have to actually

848
00:30:57,588 --> 00:30:59,529
uh run your initialization logic

849
00:30:59,789 --> 00:31:01,868
uh as part of kind of the the synchronous

850
00:31:01,868 --> 00:31:03,529
invoke request path, right?

851
00:31:05,150 --> 00:31:06,750
Now this invoke number 3,

852
00:31:07,150 --> 00:31:09,299
it comes in after invoke

853
00:31:09,299 --> 00:31:10,430
1 has completed.

854
00:31:10,789 --> 00:31:12,630
So what happens with invoke 3 is

855
00:31:12,949 --> 00:31:14,969
we actually keep execution environment

856
00:31:14,969 --> 00:31:17,068
1 around in the hope that you'll

857
00:31:17,068 --> 00:31:18,180
send us more invokes.

858
00:31:18,469 --> 00:31:20,469
So with invoke 3 we can just route it to

859
00:31:20,469 --> 00:31:22,588
a pre-existing execution environment and

860
00:31:22,588 --> 00:31:25,289
we don't have to pay the cost of that initialization

861
00:31:25,709 --> 00:31:27,529
and this is what we call warm invokes

862
00:31:28,068 --> 00:31:30,259
and everyone loves warm invokes because they're

863
00:31:30,259 --> 00:31:32,549
super fast and they, they don't incur that latency

864
00:31:32,549 --> 00:31:33,430
cost of the in it.

865
00:31:34,989 --> 00:31:36,699
So the thing to notice here

866
00:31:37,118 --> 00:31:39,118
is that in lambda default

867
00:31:39,118 --> 00:31:41,199
these execution environments are all what we call

868
00:31:41,199 --> 00:31:42,479
singly concurrent,

869
00:31:42,838 --> 00:31:44,170
which means that

870
00:31:44,449 --> 00:31:47,160
um there's only ever one invoke

871
00:31:47,160 --> 00:31:49,358
being executed out of a given execution

872
00:31:49,358 --> 00:31:50,509
environment at a time.

873
00:31:50,828 --> 00:31:53,078
It may get reused for multiple invokes

874
00:31:53,078 --> 00:31:55,239
later in time, but at a given time only

875
00:31:55,239 --> 00:31:57,559
one invoke is active per execution environment.

876
00:32:00,250 --> 00:32:02,328
So when we say concurrency

877
00:32:02,328 --> 00:32:04,449
in lambda default, what that really means it's

878
00:32:04,449 --> 00:32:07,068
just the number of active in-flight invokes

879
00:32:07,568 --> 00:32:10,068
or the number of of active execution

880
00:32:10,068 --> 00:32:12,449
environments that are serving in those those in-flight

881
00:32:12,449 --> 00:32:14,689
invokes that's what concurrency means in lambda

882
00:32:14,689 --> 00:32:15,250
on demand.

883
00:32:16,900 --> 00:32:18,068
In LMI

884
00:32:18,459 --> 00:32:20,680
things are a little bit different, so here we have

885
00:32:20,979 --> 00:32:23,019
3 execution environments that we talked about that

886
00:32:23,019 --> 00:32:25,160
were pre-initialized when you published your version.

887
00:32:26,250 --> 00:32:28,309
And then when your invokes start to come in

888
00:32:28,729 --> 00:32:30,930
an LMI will actually send we can send

889
00:32:30,930 --> 00:32:32,930
multiple concurrent invokes to a

890
00:32:32,930 --> 00:32:34,430
single execution environment

891
00:32:34,809 --> 00:32:37,068
and this is what we call multi-concurrency

892
00:32:37,410 --> 00:32:39,449
single execution environment can be

893
00:32:39,449 --> 00:32:42,390
handling multiple invokes simultaneously.

894
00:32:45,229 --> 00:32:46,380
And um

895
00:32:47,130 --> 00:32:49,250
And this is why we, we don't have cold

896
00:32:49,250 --> 00:32:51,729
starts because we've pre-initialized these execution

897
00:32:51,729 --> 00:32:54,219
environments as part of your function activation.

898
00:32:54,410 --> 00:32:56,689
So when the invokes come in they will always

899
00:32:56,689 --> 00:32:58,969
get routed to one of these execution environments

900
00:32:58,969 --> 00:33:00,549
that we've pre-initialized.

901
00:33:01,618 --> 00:33:03,108
So no cold starts in LMI.

902
00:33:04,229 --> 00:33:06,250
So the first question here is

903
00:33:06,588 --> 00:33:08,009
how do you know how much

904
00:33:08,348 --> 00:33:10,469
concurrency or how much multi concurrency

905
00:33:10,469 --> 00:33:12,588
can you send to each execution

906
00:33:12,588 --> 00:33:13,269
environment?

907
00:33:16,118 --> 00:33:18,328
Now we came up with some defaults

908
00:33:18,328 --> 00:33:20,439
for the maximum concurrency that an

909
00:33:20,439 --> 00:33:21,858
execution environment can take

910
00:33:22,118 --> 00:33:23,539
based on the language

911
00:33:23,838 --> 00:33:25,910
just after researching all the

912
00:33:25,910 --> 00:33:28,039
applications that you all have built on lambda and

913
00:33:28,039 --> 00:33:29,380
other kind of serverless

914
00:33:29,640 --> 00:33:30,279
um

915
00:33:31,000 --> 00:33:32,479
products in our our suite,

916
00:33:33,000 --> 00:33:35,000
uh, we looked at all of the workloads and these are

917
00:33:35,000 --> 00:33:37,219
kind of the defaults that we came up with.

918
00:33:39,328 --> 00:33:41,420
But you can always override it and

919
00:33:41,420 --> 00:33:43,509
this is in our create function

920
00:33:43,509 --> 00:33:45,858
API in that new capacity

921
00:33:45,858 --> 00:33:47,219
provider config section

922
00:33:47,500 --> 00:33:49,858
you can also specify a per execution

923
00:33:49,858 --> 00:33:52,199
environment environment max concurrency

924
00:33:52,380 --> 00:33:54,380
so this is if you have certain bottlenecks that

925
00:33:54,380 --> 00:33:56,439
we're not aware of in terms of your dependencies

926
00:33:56,439 --> 00:33:58,680
or anything like that, you can come in here

927
00:33:58,858 --> 00:34:01,140
and override what's the max concurrency

928
00:34:01,140 --> 00:34:02,539
and execution environment can take.

929
00:34:04,630 --> 00:34:06,868
Now we, we also

930
00:34:06,868 --> 00:34:07,959
realized that it's not

931
00:34:08,269 --> 00:34:10,708
easy to come up with this magic

932
00:34:10,708 --> 00:34:12,829
max concurrency number and there is

933
00:34:12,829 --> 00:34:14,010
no one size fits all,

934
00:34:14,750 --> 00:34:16,869
uh, so we have some protections

935
00:34:16,869 --> 00:34:19,030
here in place in case you set that

936
00:34:19,030 --> 00:34:21,489
max concurrency number to be too high.

937
00:34:22,128 --> 00:34:23,789
So let's take a look at how that works.

938
00:34:24,269 --> 00:34:26,750
So here you have your managed DC2

939
00:34:26,750 --> 00:34:29,030
instances with a few execution environments deployed

940
00:34:29,030 --> 00:34:31,239
on it. And

941
00:34:31,239 --> 00:34:33,398
um also on these instances sitting

942
00:34:33,398 --> 00:34:35,398
in front of your execution environments is an

943
00:34:35,398 --> 00:34:36,759
agent that we deploy

944
00:34:37,039 --> 00:34:39,157
we're calling it the LMI agent here

945
00:34:39,478 --> 00:34:41,858
and this agent also acts as a proxy

946
00:34:42,079 --> 00:34:44,438
to the to your execution environments

947
00:34:44,438 --> 00:34:45,518
when the invokes come in.

948
00:34:46,039 --> 00:34:48,248
So when the invokes come into our service,

949
00:34:48,588 --> 00:34:50,599
we will try to kind of balance the load

950
00:34:50,599 --> 00:34:52,668
evenly across all the execution environments

951
00:34:52,668 --> 00:34:54,867
that you have. We'll pick an execution

952
00:34:54,867 --> 00:34:57,197
environment and we'll route it to this LMI agent

953
00:34:57,197 --> 00:34:58,239
on that instance.

954
00:34:59,369 --> 00:35:01,449
Now if that this execution environment

955
00:35:01,449 --> 00:35:03,809
has either reached the max con concurrency

956
00:35:03,809 --> 00:35:04,909
that you've configured

957
00:35:05,409 --> 00:35:07,409
or if it's running even

958
00:35:07,409 --> 00:35:09,610
before it reaches the max concurrency, if

959
00:35:09,610 --> 00:35:12,329
it starts running into high mem uh

960
00:35:12,329 --> 00:35:12,989
resource pressure

961
00:35:13,530 --> 00:35:15,559
either memory pressure or CPU

962
00:35:15,559 --> 00:35:16,070
pressure,

963
00:35:16,409 --> 00:35:18,489
then our LMI agent will say

964
00:35:18,489 --> 00:35:20,878
hey back off this thing is in is is

965
00:35:20,878 --> 00:35:22,789
running hot go try somewhere else

966
00:35:23,090 --> 00:35:25,139
and then we will reroute your invoke

967
00:35:25,570 --> 00:35:28,030
to a different execution environment.

968
00:35:28,918 --> 00:35:30,989
Now if all if all

969
00:35:30,989 --> 00:35:33,099
of these execution environments kind of

970
00:35:33,099 --> 00:35:35,239
start to heat up and everybody's is telling

971
00:35:35,239 --> 00:35:36,329
us to back off,

972
00:35:36,599 --> 00:35:39,219
then we'll actually just come back to you and throttle

973
00:35:39,429 --> 00:35:41,438
your invoke uh back

974
00:35:41,438 --> 00:35:43,539
and we do this like I said for

975
00:35:43,550 --> 00:35:45,260
for what I call good put protection.

976
00:35:45,550 --> 00:35:47,668
So this is we don't wanna just blindly

977
00:35:47,668 --> 00:35:49,309
send invoke traffic

978
00:35:49,628 --> 00:35:51,708
all the way through to your execution

979
00:35:51,708 --> 00:35:52,329
environments

980
00:35:52,750 --> 00:35:54,829
until you hit this kind of max magic

981
00:35:54,829 --> 00:35:56,829
max concurrency number and actually brown

982
00:35:56,829 --> 00:35:59,188
out or crash all of your execution environments

983
00:35:59,269 --> 00:36:01,478
and you have a full kind of outage of your app.

984
00:36:01,789 --> 00:36:04,349
So we're measuring and seeing how your execution

985
00:36:04,349 --> 00:36:06,389
environments are doing and if it starts to

986
00:36:06,389 --> 00:36:08,449
get to a point where we think it's gonna brown out,

987
00:36:08,590 --> 00:36:10,820
we'll start rejecting some traffic so

988
00:36:10,820 --> 00:36:12,949
that at least you can make forward progress with

989
00:36:12,949 --> 00:36:14,969
the with the capacity that you have, right?

990
00:36:17,489 --> 00:36:19,889
In terms of metrics, if this starts happening,

991
00:36:20,030 --> 00:36:22,280
um, we have new cloud watch

992
00:36:22,280 --> 00:36:22,909
metrics,

993
00:36:23,208 --> 00:36:25,250
uh, for the exact reason that you got

994
00:36:25,250 --> 00:36:27,188
throttled in the bottlenecked resource,

995
00:36:27,449 --> 00:36:29,530
um, so you can get throttled because of your max

996
00:36:29,530 --> 00:36:31,769
concurrency, that's concurrency throttles, CPU

997
00:36:31,769 --> 00:36:34,489
throttles, memory disk, what was the bottleneck

998
00:36:34,489 --> 00:36:36,469
resource that caused those throttles.

999
00:36:38,648 --> 00:36:41,010
And another thing about multi-concurrency

1000
00:36:41,010 --> 00:36:42,530
before we move to the next section

1001
00:36:42,898 --> 00:36:45,398
is because your execution environments

1002
00:36:45,398 --> 00:36:47,449
are handling multiple invokes

1003
00:36:47,449 --> 00:36:48,550
simultaneously.

1004
00:36:49,030 --> 00:36:51,110
Um, it is important for you

1005
00:36:51,110 --> 00:36:53,188
to realize that you need to have these

1006
00:36:53,188 --> 00:36:55,010
threat safety best practices,

1007
00:36:55,309 --> 00:36:56,929
um, in mind as you

1008
00:36:57,510 --> 00:36:59,708
code and develop your application. So this is

1009
00:36:59,708 --> 00:37:01,869
again something that's different from lambda,

1010
00:37:02,110 --> 00:37:04,510
uh, default where you have singly concurrent,

1011
00:37:04,628 --> 00:37:06,250
um, execution environments

1012
00:37:06,668 --> 00:37:09,059
here thread safety best practices,

1013
00:37:09,389 --> 00:37:10,530
you know, avoid

1014
00:37:10,820 --> 00:37:12,989
mutating any shared objects or

1015
00:37:12,989 --> 00:37:15,030
global objects when you have multiple

1016
00:37:15,030 --> 00:37:16,708
invokes running at the same time.

1017
00:37:17,030 --> 00:37:19,030
Uh, use thread local storage and

1018
00:37:19,030 --> 00:37:21,369
thread-safe storage for your,

1019
00:37:21,378 --> 00:37:22,570
your data structures,

1020
00:37:22,949 --> 00:37:23,510
um.

1021
00:37:24,300 --> 00:37:26,398
Any shared clients or connections

1022
00:37:26,398 --> 00:37:27,699
that you're initializing,

1023
00:37:27,978 --> 00:37:29,978
uh, make sure that the configuration on those

1024
00:37:29,978 --> 00:37:31,000
are immutable,

1025
00:37:31,409 --> 00:37:33,699
uh, in the invoke itself and

1026
00:37:33,699 --> 00:37:35,079
also writing to disk,

1027
00:37:35,458 --> 00:37:37,820
um, you need to remember if multiple invokes

1028
00:37:37,820 --> 00:37:39,820
are writing to disk at the same time they can

1029
00:37:39,820 --> 00:37:41,438
clobber on top of each other

1030
00:37:41,820 --> 00:37:44,059
so use request specific file

1031
00:37:44,059 --> 00:37:46,300
names for any, any rights you're doing to slash

1032
00:37:46,300 --> 00:37:48,949
10. All

1033
00:37:48,949 --> 00:37:50,750
right, so that was concurrency.

1034
00:37:51,188 --> 00:37:53,070
So let's talk about scaling.

1035
00:37:55,239 --> 00:37:57,309
So again let's recap what scaling

1036
00:37:57,309 --> 00:37:58,849
means in lambda default.

1037
00:37:59,269 --> 00:38:01,590
Um, in lambda default, the scaling

1038
00:38:01,590 --> 00:38:03,550
is just kind of organic. It's

1039
00:38:03,869 --> 00:38:05,949
these cold starts this is

1040
00:38:05,949 --> 00:38:08,188
that is when we initialize new execution

1041
00:38:08,188 --> 00:38:10,349
environments and that's really just how we scale

1042
00:38:10,349 --> 00:38:12,349
when you send us your big kind of load of

1043
00:38:12,349 --> 00:38:14,708
invokes if, if you're getting a lot of concurrent

1044
00:38:14,708 --> 00:38:16,708
invokes, we just naturally scale out

1045
00:38:16,708 --> 00:38:18,708
the execution environments uh as

1046
00:38:18,708 --> 00:38:20,438
cold starts in the invoke path.

1047
00:38:22,159 --> 00:38:22,800
But in LMI,

1048
00:38:23,469 --> 00:38:25,510
um, we said we don't have cold starts,

1049
00:38:25,989 --> 00:38:27,269
so the question becomes

1050
00:38:27,530 --> 00:38:29,628
if all of these existing execution

1051
00:38:29,628 --> 00:38:32,570
environments start to hit that point of saturation,

1052
00:38:33,110 --> 00:38:35,269
um, how and when do we actually scale up new

1053
00:38:35,269 --> 00:38:36,610
execution environments?

1054
00:38:38,139 --> 00:38:38,849
And

1055
00:38:39,550 --> 00:38:41,668
the scaling in LMI

1056
00:38:41,668 --> 00:38:43,728
is actually asynchronous

1057
00:38:43,949 --> 00:38:46,199
and resource based scaling. So what this means

1058
00:38:46,199 --> 00:38:46,889
is that

1059
00:38:47,159 --> 00:38:49,449
we're constantly monitoring again the

1060
00:38:49,449 --> 00:38:51,688
resource heat on your execution environments

1061
00:38:51,869 --> 00:38:53,909
and when we see that it's starting to get

1062
00:38:53,909 --> 00:38:55,250
close to a certain threshold,

1063
00:38:55,628 --> 00:38:58,070
that's when we'll, uh, asynchronously

1064
00:38:58,070 --> 00:39:00,110
scale up new execution

1065
00:39:00,110 --> 00:39:02,369
environments within your capacity provider.

1066
00:39:03,500 --> 00:39:05,550
And if that needs new instances we'll do that as

1067
00:39:05,550 --> 00:39:07,139
well, so we'll we'll take a look at that here.

1068
00:39:08,800 --> 00:39:11,168
So here you have, um, you know,

1069
00:39:11,478 --> 00:39:13,489
3 instances with 3 execution

1070
00:39:13,489 --> 00:39:14,050
environments.

1071
00:39:14,610 --> 00:39:16,909
You have a friendly LMI agent there

1072
00:39:17,128 --> 00:39:19,648
and the LMI agent is constantly kind of gathering

1073
00:39:19,648 --> 00:39:21,840
CPU usage stats from your execution

1074
00:39:21,840 --> 00:39:24,188
environments and your, your instances and,

1075
00:39:24,208 --> 00:39:26,469
uh, kind of sending that data over to us.

1076
00:39:27,918 --> 00:39:29,958
And we are monitoring the

1077
00:39:29,958 --> 00:39:32,860
kind of average um CPU utilization

1078
00:39:33,159 --> 00:39:35,239
uh at the capacity provider level but also at

1079
00:39:35,239 --> 00:39:37,320
your function level and we're trying to

1080
00:39:37,320 --> 00:39:39,398
maintain this target kind of CPU

1081
00:39:39,398 --> 00:39:40,820
utilization threshold.

1082
00:39:41,668 --> 00:39:43,780
And if you're kind of within

1083
00:39:43,780 --> 00:39:45,840
this plus or minus 10% of

1084
00:39:45,840 --> 00:39:47,128
that target threshold,

1085
00:39:47,398 --> 00:39:49,438
then that's when we call you as you know you're

1086
00:39:49,438 --> 00:39:51,679
in steady state, you're happy, there's no scaling

1087
00:39:51,679 --> 00:39:52,599
action happening.

1088
00:39:53,260 --> 00:39:55,469
If you start to heat up your CPU

1089
00:39:55,469 --> 00:39:57,510
and you start to get into that range above

1090
00:39:57,510 --> 00:39:59,329
the the the +10%,

1091
00:39:59,628 --> 00:40:02,349
that's when we'll start to scale up new execution

1092
00:40:02,349 --> 00:40:02,969
environments.

1093
00:40:03,309 --> 00:40:05,628
Um, first we'll fill up existing EC2

1094
00:40:05,628 --> 00:40:07,969
instances and then if you need more,

1095
00:40:08,070 --> 00:40:10,070
we'll launch new EC2

1096
00:40:10,070 --> 00:40:12,429
instances and, and deploy more execution

1097
00:40:12,429 --> 00:40:13,449
environments on those.

1098
00:40:15,110 --> 00:40:15,648
Now

1099
00:40:16,269 --> 00:40:18,340
if you have a burst of

1100
00:40:18,340 --> 00:40:19,090
traffic

1101
00:40:19,389 --> 00:40:19,969
that's

1102
00:40:20,309 --> 00:40:22,500
um very spiky and it bursts up

1103
00:40:22,500 --> 00:40:24,628
faster than we can react and add these

1104
00:40:24,628 --> 00:40:26,648
instances and execution environments

1105
00:40:26,938 --> 00:40:28,289
to your capacity provider,

1106
00:40:28,550 --> 00:40:29,168
that's when

1107
00:40:29,500 --> 00:40:31,708
you can push your CPU utilization into

1108
00:40:31,708 --> 00:40:34,128
that kind of good put protection mode

1109
00:40:34,349 --> 00:40:36,628
uh where we have to throttle you until we can actually

1110
00:40:36,628 --> 00:40:38,708
bring up more instances and more execution

1111
00:40:38,708 --> 00:40:40,909
environments uh to meet that that

1112
00:40:40,909 --> 00:40:41,809
load, right?

1113
00:40:43,429 --> 00:40:45,429
In terms of scale down, it's the opposite.

1114
00:40:45,628 --> 00:40:47,628
If you then start to cool down and come below that

1115
00:40:47,628 --> 00:40:49,090
-10% mark,

1116
00:40:49,418 --> 00:40:51,418
uh, we'll first take, take, uh, scale

1117
00:40:51,418 --> 00:40:53,429
down your execution environments and then

1118
00:40:53,429 --> 00:40:55,708
we'll decompress and then actually scale, scale

1119
00:40:55,708 --> 00:40:57,750
down instances if we can, uh, kind of

1120
00:40:57,750 --> 00:40:59,030
pack it back in, right.

1121
00:41:02,378 --> 00:41:04,869
And uh in terms of scale down you're always

1122
00:41:04,869 --> 00:41:07,188
protected by your min execution environments

1123
00:41:07,188 --> 00:41:09,458
that you configure which is 3 by default but

1124
00:41:09,458 --> 00:41:10,688
we'll never scale you down

1125
00:41:11,139 --> 00:41:13,148
uh below that number even if if

1126
00:41:13,148 --> 00:41:15,188
you're seeing no invokes to your

1127
00:41:15,188 --> 00:41:18,260
function. Alright,

1128
00:41:18,378 --> 00:41:20,418
so the, the question that all of you are probably having is

1129
00:41:20,418 --> 00:41:23,090
what is this magic target CPU utilization

1130
00:41:23,090 --> 00:41:24,000
threshold?

1131
00:41:25,340 --> 00:41:25,918
Um

1132
00:41:26,639 --> 00:41:28,719
So this is this is where we come back to that create

1133
00:41:28,719 --> 00:41:31,000
capacity provider scaling config and the more

1134
00:41:31,000 --> 00:41:33,418
advanced scaling options that I talked about here.

1135
00:41:34,079 --> 00:41:36,119
We have a scaling mode that can

1136
00:41:36,119 --> 00:41:37,938
be automatic or manual.

1137
00:41:38,199 --> 00:41:40,398
If it's automatic, um, that's

1138
00:41:40,398 --> 00:41:42,139
basically we are

1139
00:41:42,478 --> 00:41:44,639
looking at your load patterns, your scaling patterns,

1140
00:41:44,659 --> 00:41:47,019
and automatically tuning that threshold,

1141
00:41:47,320 --> 00:41:49,570
um. To be at a good place

1142
00:41:49,570 --> 00:41:50,769
for your application

1143
00:41:51,289 --> 00:41:53,409
in manual mode you can take

1144
00:41:53,409 --> 00:41:55,449
things into your hands into

1145
00:41:55,449 --> 00:41:56,429
your control,

1146
00:41:56,769 --> 00:41:59,329
and you can manually basically configure

1147
00:41:59,329 --> 00:42:00,789
a target CPU

1148
00:42:01,050 --> 00:42:02,070
utilization percentage.

1149
00:42:06,599 --> 00:42:08,599
In terms of metrics to monitor all

1150
00:42:08,599 --> 00:42:10,849
of this fun stuff again these are all new metrics

1151
00:42:10,849 --> 00:42:12,039
that we've added for LMI

1152
00:42:12,679 --> 00:42:14,719
at the function level we give you

1153
00:42:14,719 --> 00:42:17,188
CPU utilization, memory utilization,

1154
00:42:17,199 --> 00:42:19,320
so this is kind of aggregated across all

1155
00:42:19,320 --> 00:42:21,418
the execution environments for that function.

1156
00:42:21,918 --> 00:42:24,159
You can also see the concurrent executions

1157
00:42:24,159 --> 00:42:26,199
like where your max limit is versus what

1158
00:42:26,199 --> 00:42:27,398
you're actually using

1159
00:42:27,918 --> 00:42:30,110
and finally, um, the chart

1160
00:42:30,110 --> 00:42:32,760
at the end there is the actual number of execution

1161
00:42:32,760 --> 00:42:35,039
environments that we've scaled up to for your function.

1162
00:42:37,309 --> 00:42:39,668
And we also have similar metrics out

1163
00:42:39,668 --> 00:42:41,648
at the capacity provider level,

1164
00:42:41,989 --> 00:42:42,570
um,

1165
00:42:42,869 --> 00:42:45,429
so this is if you have multiple functions within the capacity

1166
00:42:45,429 --> 00:42:47,469
provider at the instance level

1167
00:42:47,469 --> 00:42:48,429
you can look at

1168
00:42:48,750 --> 00:42:50,829
memory utilization, CPU utilization, and

1169
00:42:50,829 --> 00:42:51,489
then the actual

1170
00:42:51,869 --> 00:42:53,989
count of instances that we've launched

1171
00:42:53,989 --> 00:42:55,250
within your capacity provider.

1172
00:42:57,889 --> 00:43:00,239
All right, so the scaling takeaway

1173
00:43:00,239 --> 00:43:02,418
here that I wanted to leave you with, and this is

1174
00:43:02,418 --> 00:43:04,119
again something that Steven brought up

1175
00:43:04,579 --> 00:43:06,800
is with lambda default,

1176
00:43:06,969 --> 00:43:08,059
um, we,

1177
00:43:08,429 --> 00:43:10,760
you know, when your functions run on our infrastructure

1178
00:43:11,030 --> 00:43:13,179
we are running a very, very

1179
00:43:13,179 --> 00:43:15,260
specialized stack that's built on

1180
00:43:15,260 --> 00:43:17,340
Firecracker and that is hyper

1181
00:43:17,340 --> 00:43:19,340
optimized to handle these kind of.

1182
00:43:19,478 --> 00:43:21,909
Spiky sparse bursty workloads,

1183
00:43:22,199 --> 00:43:24,478
it's hyper optimized for cold starts. It's

1184
00:43:24,478 --> 00:43:27,070
hyper optimized to keep those cold start latencies

1185
00:43:27,070 --> 00:43:29,119
down so that we can actually like bring up

1186
00:43:29,119 --> 00:43:31,719
a new, uh, kind of lightweight firecracker

1187
00:43:31,719 --> 00:43:33,719
VM, um, in the path of your

1188
00:43:33,719 --> 00:43:35,849
invoke. Lambda

1189
00:43:35,849 --> 00:43:38,110
managed instances on the other hand is actually

1190
00:43:38,449 --> 00:43:40,929
built and designed for a different profile of workloads

1191
00:43:40,929 --> 00:43:43,168
like Steven was saying this is built

1192
00:43:43,168 --> 00:43:43,708
more

1193
00:43:44,090 --> 00:43:46,289
for kind of their stable workloads

1194
00:43:46,289 --> 00:43:48,179
that have a good baseline of traffic,

1195
00:43:48,489 --> 00:43:50,820
uh, and more smoother and predictable workloads,

1196
00:43:51,250 --> 00:43:53,449
you know, the whole, the purpose of this feature

1197
00:43:53,449 --> 00:43:55,728
is to move away from specialized stacks

1198
00:43:55,728 --> 00:43:57,728
that we are using behind the scenes and

1199
00:43:57,728 --> 00:43:59,728
to move to more general purpose kind of

1200
00:43:59,728 --> 00:44:01,570
easy to instances in your account.

1201
00:44:02,000 --> 00:44:02,559
Um,

1202
00:44:02,918 --> 00:44:05,030
so the machinery underneath is very

1203
00:44:05,030 --> 00:44:07,039
different and that's why the profile of workloads

1204
00:44:07,039 --> 00:44:09,070
that are that LMI is suited for is,

1205
00:44:09,079 --> 00:44:09,958
is different,

1206
00:44:10,228 --> 00:44:12,438
um, from the profile that's suited

1207
00:44:12,438 --> 00:44:13,349
for for default.

1208
00:44:13,639 --> 00:44:15,378
So the two are really complementary,

1209
00:44:15,760 --> 00:44:17,878
um, features of lambda

1210
00:44:18,070 --> 00:44:20,079
that you can use together to kind of cover

1211
00:44:20,079 --> 00:44:21,418
more of your use cases.

1212
00:44:23,289 --> 00:44:24,269
All right, um,

1213
00:44:24,570 --> 00:44:26,849
the final section here is the security

1214
00:44:26,849 --> 00:44:27,590
boundary.

1215
00:44:27,929 --> 00:44:30,539
Like I said, with, uh, Lambda

1216
00:44:30,539 --> 00:44:32,789
default, all of your functions are running

1217
00:44:33,208 --> 00:44:35,648
in our service account. It's a big multi-tenant

1218
00:44:35,648 --> 00:44:36,909
account, um.

1219
00:44:37,590 --> 00:44:39,918
And uh in lambda default every every

1220
00:44:39,918 --> 00:44:42,119
functions execution environment runs in its own

1221
00:44:42,119 --> 00:44:43,760
VM because it's a multi-tenant setup

1222
00:44:44,398 --> 00:44:45,648
with LMI,

1223
00:44:46,320 --> 00:44:48,199
uh, everything's running in your account,

1224
00:44:48,599 --> 00:44:50,019
nothing, no, nobody else's,

1225
00:44:50,320 --> 00:44:50,869
um,

1226
00:44:51,239 --> 00:44:53,599
code is running in your account it's a single tenant

1227
00:44:53,599 --> 00:44:54,780
set up in that sense.

1228
00:44:55,280 --> 00:44:57,648
And the security boundary here for LMI

1229
00:44:57,648 --> 00:44:59,909
is really the capacity provider,

1230
00:45:00,289 --> 00:45:02,449
um, so if you want

1231
00:45:02,449 --> 00:45:04,449
VM level isolation

1232
00:45:04,449 --> 00:45:05,648
between your

1233
00:45:05,949 --> 00:45:08,090
um functions, the way you

1234
00:45:08,090 --> 00:45:10,128
would do that is to have separate capacity

1235
00:45:10,128 --> 00:45:11,128
providers because

1236
00:45:11,530 --> 00:45:13,188
by definition, um.

1237
00:45:14,000 --> 00:45:16,510
The instances within your two separate capacity

1238
00:45:16,510 --> 00:45:18,550
providers are going to be different. So if you map two

1239
00:45:18,550 --> 00:45:20,559
functions to separate capacity providers, they will

1240
00:45:20,559 --> 00:45:22,860
be on different VMs in different instances.

1241
00:45:23,519 --> 00:45:25,519
Uh, within the EC2 instance,

1242
00:45:25,599 --> 00:45:26,139
um,

1243
00:45:26,679 --> 00:45:29,289
the function execution environments that we deploy,

1244
00:45:29,559 --> 00:45:31,559
uh, they're, they're basically containers, so they're

1245
00:45:31,559 --> 00:45:33,179
separated by a container boundary.

1246
00:45:34,398 --> 00:45:36,559
Also, if you map multiple

1247
00:45:36,559 --> 00:45:38,699
functions to the capacity provider

1248
00:45:39,000 --> 00:45:41,478
like I mentioned, um, then

1249
00:45:41,478 --> 00:45:43,840
the execution environments from those different functions

1250
00:45:43,840 --> 00:45:45,918
can also share the same instances

1251
00:45:45,918 --> 00:45:48,039
and again so it's, it's a container boundary

1252
00:45:48,039 --> 00:45:50,199
there so the takeaway here really

1253
00:45:50,199 --> 00:45:50,820
is if you

1254
00:45:51,099 --> 00:45:53,159
want VM level isolation between your

1255
00:45:53,159 --> 00:45:55,280
functions, um, map them to

1256
00:45:55,280 --> 00:45:57,438
separate capacity providers because that is

1257
00:45:57,438 --> 00:46:00,059
the kind of VM level security boundary

1258
00:46:00,059 --> 00:46:00,599
in LMI.

1259
00:46:02,958 --> 00:46:05,110
All right, with that, um,

1260
00:46:05,179 --> 00:46:07,179
I will hand back to Steven

1261
00:46:07,179 --> 00:46:09,360
to talk about partner integration

1262
00:46:09,360 --> 00:46:10,780
and tooling and pricing.

1263
00:46:12,639 --> 00:46:14,719
OK, so that was a really good

1264
00:46:14,719 --> 00:46:16,500
introduction into um

1265
00:46:16,878 --> 00:46:18,878
into LMI um and I'm just

1266
00:46:18,878 --> 00:46:20,918
gonna talk through some of the new partner

1267
00:46:20,918 --> 00:46:23,079
integrations that we have um and

1268
00:46:23,079 --> 00:46:25,119
introduce you to our launch partners DataDog and

1269
00:46:25,119 --> 00:46:27,458
SEI. So Data Dog provides

1270
00:46:27,458 --> 00:46:29,469
full observability for lambda managed

1271
00:46:29,469 --> 00:46:30,179
instances.

1272
00:46:30,500 --> 00:46:32,599
Um, customers can monitor

1273
00:46:32,599 --> 00:46:34,769
um key metrics to understand the health

1274
00:46:34,769 --> 00:46:37,250
and the utilization of the lambda managed instances,

1275
00:46:37,579 --> 00:46:39,579
and customers can also alert on those

1276
00:46:39,579 --> 00:46:40,219
metrics.

1277
00:46:40,510 --> 00:46:42,918
Um, and any errors and anomalous,

1278
00:46:42,929 --> 00:46:45,019
um, behavior, um, that might need

1279
00:46:45,019 --> 00:46:47,019
attention. Um, using

1280
00:46:47,019 --> 00:46:49,199
the automatically correlated metrics,

1281
00:46:49,280 --> 00:46:50,458
logs, and tracers,

1282
00:46:50,800 --> 00:46:53,039
um, that span upstream and downstream

1283
00:46:53,039 --> 00:46:55,159
services, um, customers

1284
00:46:55,159 --> 00:46:57,429
can also investigate and resolve any, um,

1285
00:46:57,438 --> 00:46:58,300
any issues.

1286
00:46:58,719 --> 00:47:01,219
And for any new instances that get launched,

1287
00:47:01,280 --> 00:47:01,809
um,

1288
00:47:02,199 --> 00:47:04,199
there, there are, um, trace

1289
00:47:04,199 --> 00:47:05,938
support and auto instrumentation,

1290
00:47:06,239 --> 00:47:08,280
um, so customers can get automatic trace

1291
00:47:08,280 --> 00:47:10,668
propagation simply by adding the

1292
00:47:10,668 --> 00:47:12,679
installing the Data Dog, um, extension.

1293
00:47:13,628 --> 00:47:15,708
Now SI also supports lambda managed

1294
00:47:15,708 --> 00:47:16,289
instances

1295
00:47:16,789 --> 00:47:19,168
um through its autonomous

1296
00:47:19,168 --> 00:47:21,349
um optimization platform um

1297
00:47:21,349 --> 00:47:23,478
that helps improve performance, cost

1298
00:47:23,789 --> 00:47:25,820
and reliability of uh the lambda,

1299
00:47:25,989 --> 00:47:28,110
EC2 instances and the rest

1300
00:47:28,110 --> 00:47:28,929
of the environment.

1301
00:47:29,599 --> 00:47:31,599
Um, this gives, um, engineering and

1302
00:47:31,599 --> 00:47:33,639
platform teams as well as, uh, fin

1303
00:47:33,639 --> 00:47:35,800
ops teams, a single workflow,

1304
00:47:36,070 --> 00:47:38,199
um, um, for making

1305
00:47:38,199 --> 00:47:40,329
safe, uh, data optimized,

1306
00:47:40,360 --> 00:47:41,398
um, decisions.

1307
00:47:41,878 --> 00:47:43,918
Um, and their platform automatically scores

1308
00:47:43,918 --> 00:47:46,099
lambda managed, uh, lambda functions,

1309
00:47:46,349 --> 00:47:48,599
um, to show you which are strong candidates

1310
00:47:48,599 --> 00:47:50,860
for moving to lambda managed instances

1311
00:47:51,079 --> 00:47:52,719
so you don't have to do the guesswork.

1312
00:47:53,228 --> 00:47:54,510
And once you're ready to move,

1313
00:47:54,829 --> 00:47:56,909
ZI's co-pilot also lets you

1314
00:47:56,909 --> 00:47:57,809
um migrate

1315
00:47:58,188 --> 00:48:00,269
uh and configure functions um with a

1316
00:48:00,269 --> 00:48:01,050
single click

1317
00:48:01,389 --> 00:48:03,590
and it's a simple low friction way of

1318
00:48:03,590 --> 00:48:05,909
moving and migrating your functions to lambda

1319
00:48:05,909 --> 00:48:06,829
managed instances.

1320
00:48:08,208 --> 00:48:10,688
Now AWS app config um

1321
00:48:10,688 --> 00:48:12,989
also featured flag um capability

1322
00:48:12,989 --> 00:48:15,329
um and and the other dynamic

1323
00:48:15,329 --> 00:48:17,398
configuration that comes with that service is

1324
00:48:17,398 --> 00:48:19,579
also fully managed, supported by lambda managed

1325
00:48:19,579 --> 00:48:20,239
instances.

1326
00:48:20,610 --> 00:48:22,929
So by using AWS app config agent

1327
00:48:22,929 --> 00:48:24,188
um lambda extension.

1328
00:48:24,628 --> 00:48:26,829
As a, as your, um, as your lambda

1329
00:48:26,829 --> 00:48:28,989
functions, you can make calling those

1330
00:48:28,989 --> 00:48:30,760
feature flags, uh, simpler,

1331
00:48:31,030 --> 00:48:32,889
and also the extension itself

1332
00:48:33,148 --> 00:48:35,469
includes best practices that simplify

1333
00:48:35,469 --> 00:48:37,510
using um AWS app config

1334
00:48:37,750 --> 00:48:39,909
um by by reducing costs. And

1335
00:48:39,909 --> 00:48:40,969
that cost reduction

1336
00:48:41,300 --> 00:48:43,668
uh results from fewer API calls

1337
00:48:43,668 --> 00:48:46,070
uh to the AWS app config service

1338
00:48:46,070 --> 00:48:48,188
and shorter and also um results in

1339
00:48:48,188 --> 00:48:49,889
shorter lambda processing times.

1340
00:48:51,478 --> 00:48:53,519
Now, Amazon Cloudwatch Lambda

1341
00:48:53,519 --> 00:48:55,628
Insights um also

1342
00:48:55,628 --> 00:48:57,878
has uh provides uh 11

1343
00:48:57,878 --> 00:49:00,159
click deployment for uh from the lambda

1344
00:49:00,159 --> 00:49:00,750
console.

1345
00:49:01,199 --> 00:49:03,800
So this allows you to filter uh capacity

1346
00:49:03,800 --> 00:49:05,878
providers. It gives you, um, it allows you

1347
00:49:05,878 --> 00:49:07,978
to drill down into instance types and

1348
00:49:07,978 --> 00:49:10,090
functions, uh. As well as providing

1349
00:49:10,090 --> 00:49:12,250
you with 12 key metrics, um,

1350
00:49:12,289 --> 00:49:14,070
um, with 1 minute granularity,

1351
00:49:14,329 --> 00:49:16,369
that, um, uh, for things like the

1352
00:49:16,369 --> 00:49:18,958
maximum and average CPU utilization,

1353
00:49:18,969 --> 00:49:21,378
as well as memory utilization statistics.

1354
00:49:21,688 --> 00:49:24,280
And this is providing you with a fully integrated

1355
00:49:24,280 --> 00:49:26,510
experience to monitor your lambda managed

1356
00:49:26,510 --> 00:49:27,050
instances.

1357
00:49:30,090 --> 00:49:32,128
Now if you're using power tools for AWS today, uh,

1358
00:49:32,148 --> 00:49:34,539
Power tools is um is really a

1359
00:49:34,539 --> 00:49:37,179
suite of utilities that help you, um, um,

1360
00:49:37,188 --> 00:49:39,329
standardize, um, application development,

1361
00:49:39,539 --> 00:49:41,769
um, and support across a number of different,

1362
00:49:41,789 --> 00:49:43,550
um, uh, a number of different, um,

1363
00:49:43,869 --> 00:49:46,148
uh, use cases, such as observability,

1364
00:49:46,378 --> 00:49:48,389
batch processing, um, helps you

1365
00:49:48,389 --> 00:49:50,389
with item potency implementations, as well

1366
00:49:50,389 --> 00:49:52,628
as, um, dealing with things like feature

1367
00:49:52,628 --> 00:49:53,949
flags and data extraction.

1368
00:49:54,728 --> 00:49:56,800
And the the power tool suite is

1369
00:49:56,800 --> 00:49:59,050
also fully compatible and thread safety,

1370
00:49:59,239 --> 00:50:01,329
uh a thread safe um uh and

1371
00:50:01,329 --> 00:50:03,369
ready to run on your lab and managed instances.

1372
00:50:04,739 --> 00:50:07,119
And of course we've got um full infrastructure

1373
00:50:07,119 --> 00:50:09,378
as code support uh through AWS

1374
00:50:09,378 --> 00:50:11,688
cloud formation, the surus

1375
00:50:11,688 --> 00:50:13,769
application model, and the um AW

1376
00:50:13,769 --> 00:50:16,099
the AWS cloud development kit,

1377
00:50:16,300 --> 00:50:18,199
as well as our partners Terraform.

1378
00:50:18,579 --> 00:50:20,699
And all of those things uh uh all of the, all of that,

1379
00:50:20,780 --> 00:50:23,019
all of the APIs that Channa was talking

1380
00:50:23,019 --> 00:50:25,119
about are fully supported um within

1381
00:50:25,119 --> 00:50:26,820
those um within those frameworks.

1382
00:50:28,010 --> 00:50:29,469
Now from a pricing perspective,

1383
00:50:30,090 --> 00:50:32,128
um, AWS um sorry, Lambda

1384
00:50:32,128 --> 00:50:34,530
Managed instances uses EC EC2

1385
00:50:34,530 --> 00:50:35,820
basing uh pricing,

1386
00:50:36,090 --> 00:50:38,728
um, with the addition of a 15%

1387
00:50:38,728 --> 00:50:41,168
management fee on top of the EC2

1388
00:50:41,168 --> 00:50:41,958
instance costs.

1389
00:50:42,449 --> 00:50:44,750
Now the price of the instance

1390
00:50:44,750 --> 00:50:47,030
itself will largely depend on

1391
00:50:47,030 --> 00:50:49,489
um the amount, the discounts that are applied

1392
00:50:49,489 --> 00:50:52,168
to those instances. However, the EC2

1393
00:50:52,168 --> 00:50:54,429
um management, so the um the management fee

1394
00:50:54,429 --> 00:50:56,849
is still based on the default price for the EC2

1395
00:50:56,849 --> 00:50:59,099
instance. Now on top of that,

1396
00:50:59,269 --> 00:51:01,269
um, you would still be

1397
00:51:01,269 --> 00:51:03,648
charged for the um the same

1398
00:51:03,648 --> 00:51:05,949
uh cost for per invocation

1399
00:51:06,110 --> 00:51:08,070
um that lambda default has today,

1400
00:51:08,389 --> 00:51:10,628
uh, with the exception that you're no longer

1401
00:51:10,628 --> 00:51:12,869
paying for the uh function duration

1402
00:51:12,869 --> 00:51:14,909
costs because everything is running on your

1403
00:51:14,909 --> 00:51:18,530
machine. So

1404
00:51:18,530 --> 00:51:21,148
to wrap things up, let's have a look at some key takeaways.

1405
00:51:22,148 --> 00:51:23,280
As I mentioned earlier,

1406
00:51:23,590 --> 00:51:25,719
lambda management instances isn't

1407
00:51:25,719 --> 00:51:27,918
designed to be a a replacement for

1408
00:51:27,918 --> 00:51:30,000
the lambda that you're uh lambda functions

1409
00:51:30,000 --> 00:51:31,659
that you're running on lambda default today.

1410
00:51:32,239 --> 00:51:34,239
It's really designed for those specific use

1411
00:51:34,239 --> 00:51:34,800
cases

1412
00:51:35,079 --> 00:51:37,159
to help you with high traffic and

1413
00:51:37,159 --> 00:51:39,199
steady state workloads or where you need

1414
00:51:39,199 --> 00:51:41,199
specialized compute options to

1415
00:51:41,199 --> 00:51:43,398
run functions um uh for

1416
00:51:43,398 --> 00:51:45,000
uh for specific use cases.

1417
00:51:45,739 --> 00:51:47,659
For everything else, lambda, um,

1418
00:51:48,110 --> 00:51:50,309
lambda uh default, um, provides

1419
00:51:50,309 --> 00:51:52,478
you, um, you know, uh, the, the ability

1420
00:51:52,478 --> 00:51:54,590
to continue working, uh, running new

1421
00:51:54,590 --> 00:51:56,938
applications that have unpredictable traffic,

1422
00:51:57,309 --> 00:51:59,820
short duration, or infrequent invocations.

1423
00:52:00,228 --> 00:52:02,148
So that's a really key takeaway here.

1424
00:52:03,219 --> 00:52:05,228
And also when you've seen what we've,

1425
00:52:05,340 --> 00:52:07,349
what we've, when we, when we, when you understand what

1426
00:52:07,349 --> 00:52:09,708
we've just done for you is basically we've

1427
00:52:09,708 --> 00:52:12,219
allowed you to define your own execution environments.

1428
00:52:12,550 --> 00:52:13,059
This is

1429
00:52:13,349 --> 00:52:15,628
you running lambda as you do today,

1430
00:52:15,909 --> 00:52:18,030
the same experience, you get to maintain

1431
00:52:18,030 --> 00:52:20,188
the same programming model, you you

1432
00:52:20,188 --> 00:52:22,208
get to maintain the same architecture,

1433
00:52:22,438 --> 00:52:24,688
you get to use the same development tools,

1434
00:52:24,829 --> 00:52:26,898
but you've got more control over where

1435
00:52:26,898 --> 00:52:28,590
and how your functions are running.

1436
00:52:29,110 --> 00:52:31,139
And you're also able to apply

1437
00:52:31,280 --> 00:52:34,019
all of the cost benefits that you're getting with EC-2

1438
00:52:34,019 --> 00:52:34,898
on top of that.

1439
00:52:37,119 --> 00:52:39,349
So that's all we have for you today. Um,

1440
00:52:39,438 --> 00:52:41,519
we are really interested to see what

1441
00:52:41,519 --> 00:52:43,878
you build with land and managed instances um

1442
00:52:43,878 --> 00:52:45,719
and look forward to um

1443
00:52:46,208 --> 00:52:48,418
to to seeing what you do with that. Thank you very much.

