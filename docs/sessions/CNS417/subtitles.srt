1
00:00:00,060 --> 00:00:01,310
- Good morning, everyone.

2
00:00:02,370 --> 00:00:04,470
Can everybody hear me well?

3
00:00:04,470 --> 00:00:05,430
Give me a thumbs up.

4
00:00:05,430 --> 00:00:06,821
Okay.

5
00:00:06,821 --> 00:00:08,190
So remember this is a silent room.

6
00:00:08,190 --> 00:00:10,140
Don't forget to put your headphones on.

7
00:00:11,880 --> 00:00:13,710
So welcome to Las Vegas.

8
00:00:13,710 --> 00:00:15,600
Welcome to re:Invent 2025.

9
00:00:15,600 --> 00:00:16,433
We make it.

10
00:00:17,790 --> 00:00:20,430
I can believe

11
00:00:20,430 --> 00:00:21,603
it's already December,

12
00:00:22,440 --> 00:00:26,490
and today, this is the session CNS 417

13
00:00:26,490 --> 00:00:29,370
for Network Observability and Strategies.

14
00:00:29,370 --> 00:00:32,700
And on this session we'll be
guiding you through a journey

15
00:00:32,700 --> 00:00:35,340
to achieve observability

16
00:00:35,340 --> 00:00:37,770
and improve your overall security posture

17
00:00:37,770 --> 00:00:42,420
on Kubernetes environment by
simplifying overall operations

18
00:00:42,420 --> 00:00:44,610
and application lifecycle, right?

19
00:00:44,610 --> 00:00:49,590
So we want to simplify our
daily tasks so we can be more,

20
00:00:49,590 --> 00:00:52,413
achieve more velocity and
faster time to market.

21
00:00:54,030 --> 00:00:58,630
One thing to keep in mind is we want

22
00:00:59,880 --> 00:01:01,620
to make sure that all the features

23
00:01:01,620 --> 00:01:04,590
that are coming out is, have that goal

24
00:01:04,590 --> 00:01:06,930
to simplify your overall operations.

25
00:01:06,930 --> 00:01:09,300
And today we are now
seeing a very nice feature

26
00:01:09,300 --> 00:01:12,960
that I wish was there when
I was a platform engineering

27
00:01:12,960 --> 00:01:15,030
managing a lot of Kubernetes environments.

28
00:01:15,030 --> 00:01:19,350
So how many of you are using
Kubernetes in production today?

29
00:01:19,350 --> 00:01:20,700
Give me a show of hands.

30
00:01:20,700 --> 00:01:21,533
So nice.

31
00:01:21,533 --> 00:01:23,190
Basically everybody.

32
00:01:23,190 --> 00:01:25,563
How many of you are
aware of EKS Auto Mode?

33
00:01:26,820 --> 00:01:27,653
Nice.

34
00:01:27,653 --> 00:01:29,550
So we'll be covering some features

35
00:01:29,550 --> 00:01:32,490
that cover overall EKS overall Kubernetes,

36
00:01:32,490 --> 00:01:36,840
but also how EKS Auto Mode
can help you simplifying

37
00:01:36,840 --> 00:01:38,910
those daily operations.

38
00:01:38,910 --> 00:01:39,780
My name is Rodrigo Bersa.

39
00:01:39,780 --> 00:01:42,900
I'm a Container Specialist
Solutions Architect at AWS,

40
00:01:42,900 --> 00:01:47,370
and my goal here is to help
customers overcome challenges

41
00:01:47,370 --> 00:01:48,330
and build solutions

42
00:01:48,330 --> 00:01:50,430
to simplify their daily activities.

43
00:01:50,430 --> 00:01:53,940
And I have the pleasure to
have here with me Master Luke.

44
00:01:53,940 --> 00:01:55,260
He's not Luke's Sky Walker,

45
00:01:55,260 --> 00:01:58,230
but he's also a Master Jedi in Kubernetes.

46
00:01:58,230 --> 00:01:59,063
- Thanks, Rodrigo.

47
00:01:59,063 --> 00:02:00,405
Hey, everyone.

48
00:02:00,405 --> 00:02:01,950
I'm Lukonde Mwila, you
can also call me Luke,

49
00:02:01,950 --> 00:02:04,833
and I'm a Product Manager
on the Amazon EKS team.

50
00:02:08,340 --> 00:02:11,490
So I wanna start by
asking a quick question.

51
00:02:11,490 --> 00:02:14,820
By show of hands, does
this architecture diagram

52
00:02:14,820 --> 00:02:18,210
resemble your environment to some degree,

53
00:02:18,210 --> 00:02:20,580
at least at the fundamental layer?

54
00:02:20,580 --> 00:02:21,870
You can see a couple of hands going up.

55
00:02:21,870 --> 00:02:23,130
Okay, more going up.

56
00:02:23,130 --> 00:02:24,240
That's great.

57
00:02:24,240 --> 00:02:26,370
So this is something that
we put in front of a lot

58
00:02:26,370 --> 00:02:28,980
of customers, and many of
them were able to resonate

59
00:02:28,980 --> 00:02:33,090
with this, and again, emphasizing
on the fundamental layer.

60
00:02:33,090 --> 00:02:34,530
And what we have represented here

61
00:02:34,530 --> 00:02:39,510
is a application environment
that has Amazon EKS

62
00:02:39,510 --> 00:02:41,130
as the foundation for its platform.

63
00:02:41,130 --> 00:02:43,410
And this is what a lot
of customers are doing.

64
00:02:43,410 --> 00:02:44,820
Now, it's not the entire picture,

65
00:02:44,820 --> 00:02:45,720
and that's something that we're going

66
00:02:45,720 --> 00:02:48,030
to be taking you on a journey about.

67
00:02:48,030 --> 00:02:50,040
You have a lot of Kubernetes workloads,

68
00:02:50,040 --> 00:02:52,680
or to be more specific,
microservices that are running

69
00:02:52,680 --> 00:02:55,650
on EKS, and a lot of
these are communicating

70
00:02:55,650 --> 00:02:58,350
with each other in order to
drive business functionality.

71
00:02:58,350 --> 00:02:59,550
These microservices are speaking

72
00:02:59,550 --> 00:03:01,080
to each other within the
scope of the cluster.

73
00:03:01,080 --> 00:03:03,720
So there's a lot of east to west traffic.

74
00:03:03,720 --> 00:03:05,100
Now, in addition to that, a lot

75
00:03:05,100 --> 00:03:09,300
of these microservices also
rely on AWS infrastructure.

76
00:03:09,300 --> 00:03:10,710
For example, you could
be running some kind

77
00:03:10,710 --> 00:03:13,250
of storage application that relies on S3s

78
00:03:13,250 --> 00:03:15,210
to actually storage objects there.

79
00:03:15,210 --> 00:03:18,240
And then in order to have
some metadata associated

80
00:03:18,240 --> 00:03:21,363
with those objects, you could
be storing it in DynamoDB,

81
00:03:22,380 --> 00:03:25,380
or perhaps you're using a
traditional three tier application

82
00:03:25,380 --> 00:03:29,160
and you're using Elastic Cache
as the caching layer, RDS

83
00:03:29,160 --> 00:03:32,370
or DocumentDB for persistent storage.

84
00:03:32,370 --> 00:03:34,800
We also have a lot of
customers that are training

85
00:03:34,800 --> 00:03:39,273
their ML models and using
S3 four checkpointing,

86
00:03:40,470 --> 00:03:42,393
but again, it doesn't stop there.

87
00:03:43,470 --> 00:03:47,460
A lot of customers are also
integrating these environments

88
00:03:47,460 --> 00:03:48,480
with additional services

89
00:03:48,480 --> 00:03:51,750
and systems that are running
on-prem or on the internet.

90
00:03:51,750 --> 00:03:53,970
Does this pattern sound familiar?

91
00:03:53,970 --> 00:03:55,620
Again, any show of hands on that?

92
00:03:57,000 --> 00:03:59,370
So what we have here in the bigger picture

93
00:03:59,370 --> 00:04:02,610
is this widely distributed
environment with a lot of threads

94
00:04:02,610 --> 00:04:05,250
that essentially tie
back to EKS as the hub

95
00:04:05,250 --> 00:04:07,143
or one of the primary hubs.

96
00:04:08,040 --> 00:04:09,810
And the core fabric that ties all

97
00:04:09,810 --> 00:04:11,643
of this together is the network.

98
00:04:12,960 --> 00:04:15,780
And the more that companies

99
00:04:15,780 --> 00:04:19,200
or organizations continue to innovate

100
00:04:19,200 --> 00:04:21,750
and introduce additional enhancements,

101
00:04:21,750 --> 00:04:24,570
that will typically translate
to new microservices

102
00:04:24,570 --> 00:04:27,420
that land either in your EKS
cluster or perhaps on-prem

103
00:04:27,420 --> 00:04:29,040
or some other service on the internet,

104
00:04:29,040 --> 00:04:31,863
but still ties back to
this foundational layer.

105
00:04:33,210 --> 00:04:35,190
And so in such environments,

106
00:04:35,190 --> 00:04:37,770
it is becoming increasingly
critical to understand

107
00:04:37,770 --> 00:04:39,600
the landscape of the network,

108
00:04:39,600 --> 00:04:42,270
to understand its security, to understand

109
00:04:42,270 --> 00:04:44,640
its performance and its behavior

110
00:04:44,640 --> 00:04:47,760
primarily so you can
measure how well that aligns

111
00:04:47,760 --> 00:04:49,980
with your internal standards,

112
00:04:49,980 --> 00:04:52,770
because the goal at the end
of the day is to optimize it

113
00:04:52,770 --> 00:04:54,810
so that it's serving the
ultimate purpose of driving

114
00:04:54,810 --> 00:04:56,850
the kind of business value that you want.

115
00:04:56,850 --> 00:05:00,480
This should ultimately be
serving that bigger purpose,

116
00:05:00,480 --> 00:05:02,820
but that becomes especially difficult

117
00:05:02,820 --> 00:05:05,550
if you have limited visibility.

118
00:05:05,550 --> 00:05:09,543
You can't accurately
optimize what you can't see.

119
00:05:12,210 --> 00:05:14,190
Some additional challenges to be aware of,

120
00:05:14,190 --> 00:05:17,070
especially in the context
of an EKS environment.

121
00:05:17,070 --> 00:05:21,240
When you're running a Kubernetes
cluster in AWS, you have

122
00:05:21,240 --> 00:05:22,620
to be mindful of the fact that you have

123
00:05:22,620 --> 00:05:25,560
a broader network plane
and that is the VPC.

124
00:05:25,560 --> 00:05:28,020
But in addition to that,
Kubernetes also has

125
00:05:28,020 --> 00:05:29,730
its own network plane.

126
00:05:29,730 --> 00:05:31,470
So you essentially have two landscapes

127
00:05:31,470 --> 00:05:33,000
or two network planes,

128
00:05:33,000 --> 00:05:33,833
which introduces

129
00:05:33,833 --> 00:05:36,840
additional operational
complexity on your part.

130
00:05:36,840 --> 00:05:38,700
And if you're trying to
achieve that objective

131
00:05:38,700 --> 00:05:40,800
that I pointed out earlier of optimizing

132
00:05:40,800 --> 00:05:42,630
for security, performance

133
00:05:42,630 --> 00:05:46,020
and ensuring that application's
traffic behavior aligns

134
00:05:46,020 --> 00:05:47,100
with your internal standards

135
00:05:47,100 --> 00:05:49,320
or designs, you need to be able

136
00:05:49,320 --> 00:05:52,080
to consolidate these two planes

137
00:05:52,080 --> 00:05:53,550
and easily understand the data points

138
00:05:53,550 --> 00:05:54,750
between the two of them.

139
00:05:55,770 --> 00:05:58,770
And as I've already spoken about,

140
00:05:58,770 --> 00:06:01,920
network observability is multidimensional.

141
00:06:01,920 --> 00:06:04,320
You're concerned about network
observability with respect

142
00:06:04,320 --> 00:06:07,780
to security, performance,
cost optimization

143
00:06:08,940 --> 00:06:10,470
and ensuring that behavior aligns

144
00:06:10,470 --> 00:06:13,500
with applications that you have designed.

145
00:06:13,500 --> 00:06:14,730
Are the right applications talking

146
00:06:14,730 --> 00:06:16,620
to each other as expected?

147
00:06:16,620 --> 00:06:17,763
These are key things.

148
00:06:19,410 --> 00:06:20,400
And so the main thing

149
00:06:20,400 --> 00:06:22,830
that we do wanna draw out in
terms of the problem definition

150
00:06:22,830 --> 00:06:25,530
to begin with is that when
you have a lack of visibility

151
00:06:25,530 --> 00:06:28,773
or limited visibility, you
end up with imprecise data.

152
00:06:29,640 --> 00:06:31,380
And the challenge with
having imprecise data

153
00:06:31,380 --> 00:06:34,983
is you have inaccurate conclusions
and slow decision-making.

154
00:06:36,510 --> 00:06:39,330
And finally, that leads
to suboptimal actions.

155
00:06:39,330 --> 00:06:41,680
Any platform engineers
that can relate to this?

156
00:06:42,990 --> 00:06:46,080
And you get, you get
stuck in a loop of trying

157
00:06:46,080 --> 00:06:48,483
to figure out exactly
what a root cause is,

158
00:06:49,560 --> 00:06:52,890
and fundamentally it's because
of a lack of visibility,

159
00:06:52,890 --> 00:06:54,390
and that also eventually ties

160
00:06:54,390 --> 00:06:56,943
to a negative impact on the business.

161
00:06:59,010 --> 00:07:01,650
There are many scenarios
in which the business

162
00:07:01,650 --> 00:07:03,690
is ultimately disrupted
because of some kind

163
00:07:03,690 --> 00:07:05,100
of network issue.

164
00:07:05,100 --> 00:07:07,290
It could be congestion at the pod layer,

165
00:07:07,290 --> 00:07:09,540
specifically an ENI issue.

166
00:07:09,540 --> 00:07:10,860
But without sufficient visibility

167
00:07:10,860 --> 00:07:14,280
and data, it's hard to actually
detect that in a quick way.

168
00:07:14,280 --> 00:07:16,500
What you're interested
in is reducing the amount

169
00:07:16,500 --> 00:07:18,690
of time it takes to
quickly detect an issue

170
00:07:18,690 --> 00:07:21,663
so that you can fast-track
resolving it as well.

171
00:07:22,590 --> 00:07:25,320
Or perhaps maybe it's a
bandwidth issue related

172
00:07:25,320 --> 00:07:27,060
to the specific worker node.

173
00:07:27,060 --> 00:07:28,740
But again, that requires having

174
00:07:28,740 --> 00:07:30,990
sufficient network visibility
at both a pod level

175
00:07:30,990 --> 00:07:32,040
and a worker node level

176
00:07:32,040 --> 00:07:36,120
and having enough context so
that when you detect an issue,

177
00:07:36,120 --> 00:07:38,790
you're also quickly able to tie it

178
00:07:38,790 --> 00:07:40,710
to the relevant metadata.

179
00:07:40,710 --> 00:07:42,390
What cluster is this coming from?

180
00:07:42,390 --> 00:07:44,310
What namespace is this coming from?

181
00:07:44,310 --> 00:07:46,260
What is the workload associated with this?

182
00:07:46,260 --> 00:07:48,420
And exactly what pod is this tied to?

183
00:07:48,420 --> 00:07:51,033
What are the ENIs
associated with this pod?

184
00:07:52,710 --> 00:07:55,560
There's also the challenge
of increasing costs

185
00:07:55,560 --> 00:07:58,230
when you have insufficient visibility.

186
00:07:58,230 --> 00:08:00,720
A lot of customers try to
adopt different strategies

187
00:08:00,720 --> 00:08:03,513
and techniques to manage
the flow of traffic.

188
00:08:05,160 --> 00:08:06,710
Anyone can relate to that here?

189
00:08:07,650 --> 00:08:08,483
Yep.

190
00:08:08,483 --> 00:08:09,330
One of the key things when it comes

191
00:08:09,330 --> 00:08:11,130
to cost optimization is trying to ensure

192
00:08:11,130 --> 00:08:13,650
that when traffic is flowing
from a particular workload

193
00:08:13,650 --> 00:08:15,720
to a certain destination,
you try to ensure

194
00:08:15,720 --> 00:08:17,490
that that traffic ends up

195
00:08:17,490 --> 00:08:21,240
in the same AZ in which
it originated from.

196
00:08:21,240 --> 00:08:22,470
And there are different
ways of doing this.

197
00:08:22,470 --> 00:08:25,470
You can use native functionalities

198
00:08:25,470 --> 00:08:28,200
to Kubernetes like topology aware routing.

199
00:08:28,200 --> 00:08:31,140
We also know that a lot of
customers use Istio for this,

200
00:08:31,140 --> 00:08:33,423
for locality-based routing,

201
00:08:34,380 --> 00:08:36,810
or you can even use Envoy for this.

202
00:08:36,810 --> 00:08:38,430
But again, how can you quickly verify

203
00:08:38,430 --> 00:08:40,710
that those optimizations
that you've put in place

204
00:08:40,710 --> 00:08:43,830
actually are working as expected?

205
00:08:43,830 --> 00:08:46,500
How can you review the topology setup

206
00:08:46,500 --> 00:08:49,890
in your Kubernetes environment
and verify that the flows

207
00:08:49,890 --> 00:08:51,630
are working as expected

208
00:08:51,630 --> 00:08:54,353
based on certain techniques
that you would've put in place?

209
00:08:55,680 --> 00:08:58,830
And then finally, there's the
impact of security as well,

210
00:08:58,830 --> 00:09:00,360
which is fundamental,

211
00:09:00,360 --> 00:09:02,880
and Rodrigo's gonna be diving
into this even further,

212
00:09:02,880 --> 00:09:05,970
which also impacts the time to market.

213
00:09:05,970 --> 00:09:07,860
Again, as I mentioned earlier, the point

214
00:09:07,860 --> 00:09:11,490
is to ultimately deliver
business value faster,

215
00:09:11,490 --> 00:09:14,610
and it's hard to confidently do that

216
00:09:14,610 --> 00:09:17,400
if security is unsure

217
00:09:17,400 --> 00:09:20,790
about how well the
network has been optimized

218
00:09:20,790 --> 00:09:22,620
to meet certain internal standards,

219
00:09:22,620 --> 00:09:25,230
that it complies as expected.

220
00:09:25,230 --> 00:09:28,290
Our traffic patterns
aligning with the designs

221
00:09:28,290 --> 00:09:30,540
that were set up for
specific applications.

222
00:09:30,540 --> 00:09:34,260
Is workload A speaking to
workload B is expected?

223
00:09:34,260 --> 00:09:35,910
And based on the designs, ensuring

224
00:09:35,910 --> 00:09:37,890
that workload C should not be receiving

225
00:09:37,890 --> 00:09:39,930
any traffic from workload C.

226
00:09:39,930 --> 00:09:41,370
How quickly can you verify that

227
00:09:41,370 --> 00:09:43,650
so that you have sufficient
confidence to move from dev

228
00:09:43,650 --> 00:09:44,883
to stage to prod?

229
00:09:47,400 --> 00:09:49,950
Rodrigo, where do we go from here?

230
00:09:49,950 --> 00:09:51,250
- Yes, you have one slide.

231
00:09:54,420 --> 00:09:55,693
- Right.

232
00:09:55,693 --> 00:09:56,526
(Rodrigo laughs)

233
00:09:56,526 --> 00:09:57,480
Almost missed this one.

234
00:09:57,480 --> 00:10:00,450
Another reason why this is
so important is because this

235
00:10:00,450 --> 00:10:01,653
is an increasing trend.

236
00:10:03,960 --> 00:10:05,400
We have a lot of customers

237
00:10:05,400 --> 00:10:08,070
that are using microservice architectures,

238
00:10:08,070 --> 00:10:10,770
and of course it introduces
its own challenges,

239
00:10:10,770 --> 00:10:12,840
but we don't want to
take a step away from it

240
00:10:12,840 --> 00:10:14,727
because of those challenges.

241
00:10:14,727 --> 00:10:16,500
And so we have to be
mindful of these hurdles,

242
00:10:16,500 --> 00:10:18,990
but at the same time,
the key thing on our part

243
00:10:18,990 --> 00:10:21,630
is how do we simplify
operations on your part

244
00:10:21,630 --> 00:10:24,060
so that as you adopt a usage pattern

245
00:10:24,060 --> 00:10:25,050
that certainly has a number

246
00:10:25,050 --> 00:10:28,320
of advantages, in this case
microservice architectures,

247
00:10:28,320 --> 00:10:30,180
how do we ensure that the kind of way,

248
00:10:30,180 --> 00:10:32,550
the ways in which we are
simplifying the operations

249
00:10:32,550 --> 00:10:36,270
in your life helps you
to continue the trend

250
00:10:36,270 --> 00:10:39,090
or leaning into specific usage
patterns that you're trying

251
00:10:39,090 --> 00:10:41,610
to adopt in your EKS environments?

252
00:10:41,610 --> 00:10:42,570
And that is gonna be a key thing

253
00:10:42,570 --> 00:10:43,560
that we'll speak about here,

254
00:10:43,560 --> 00:10:45,510
and Rodrigo already alluded to earlier.

255
00:10:48,990 --> 00:10:51,090
- So where do we go from here?

256
00:10:51,090 --> 00:10:54,480
So how do we translate those kinds

257
00:10:54,480 --> 00:10:58,230
of business challenges
to technical solutions?

258
00:10:58,230 --> 00:10:59,310
Right?

259
00:10:59,310 --> 00:11:00,630
Because if we think about it,

260
00:11:00,630 --> 00:11:02,310
they're not really business challenges.

261
00:11:02,310 --> 00:11:05,310
They are technical
challenges that are leading

262
00:11:05,310 --> 00:11:06,930
to business challenges.

263
00:11:06,930 --> 00:11:09,030
So let's bring back that architecture

264
00:11:09,030 --> 00:11:10,650
that Luke just showed us,

265
00:11:10,650 --> 00:11:13,650
but let's dive deep on the core center

266
00:11:13,650 --> 00:11:17,010
or our Amazon EKS cluster,
our Kubernetes cluster.

267
00:11:17,010 --> 00:11:19,560
Because if you think about
it, the communication

268
00:11:19,560 --> 00:11:22,770
through storage or to a database,

269
00:11:22,770 --> 00:11:23,910
it was always, always there?

270
00:11:23,910 --> 00:11:25,320
Like it doesn't matter what kind

271
00:11:25,320 --> 00:11:28,320
of application you have,
it's running on EC2

272
00:11:28,320 --> 00:11:30,330
or if it's containerized,

273
00:11:30,330 --> 00:11:34,377
but the growing trend of those
containerized applications

274
00:11:34,377 --> 00:11:38,910
and multi-tenant applications
is what's causing

275
00:11:38,910 --> 00:11:41,643
this complexity to increase over and over.

276
00:11:42,870 --> 00:11:44,730
So how did we get

277
00:11:44,730 --> 00:11:48,990
to like this very complex
microservices scenario?

278
00:11:48,990 --> 00:11:50,370
Let's start from the beginning.

279
00:11:50,370 --> 00:11:54,000
So the multi-tenancy evolution
started with monolith.

280
00:11:54,000 --> 00:11:55,560
So we had one team

281
00:11:55,560 --> 00:11:58,650
or two teams managing
one specific application.

282
00:11:58,650 --> 00:12:00,840
There's nothing much to do.

283
00:12:00,840 --> 00:12:03,540
Like there's, it's just
one piece, a black box.

284
00:12:03,540 --> 00:12:05,160
They don't have like too much interaction,

285
00:12:05,160 --> 00:12:07,910
they don't have too much
connection like microservices.

286
00:12:09,090 --> 00:12:11,460
With the growing trend of Kubernetes,

287
00:12:11,460 --> 00:12:15,025
we started containerizing those monoliths,

288
00:12:15,025 --> 00:12:18,630
not really in the right way, we just make

289
00:12:18,630 --> 00:12:22,200
those big applications containers

290
00:12:22,200 --> 00:12:25,020
and we put in a cluster,
like a single tenant cluster.

291
00:12:25,020 --> 00:12:27,030
So this is what we call
like hard multi-tenancy

292
00:12:27,030 --> 00:12:28,950
where you have like
one application running

293
00:12:28,950 --> 00:12:30,330
in a specific cluster.

294
00:12:30,330 --> 00:12:32,023
And this is not wrong.

295
00:12:32,023 --> 00:12:33,630
Like this is maybe required if you have

296
00:12:33,630 --> 00:12:36,630
like a very PCI compliant environment

297
00:12:36,630 --> 00:12:40,863
or if you have like specific
security restrictions.

298
00:12:41,760 --> 00:12:44,760
But then we started learning
that Kubernetes very,

299
00:12:44,760 --> 00:12:47,820
very effective on managing
multiple workloads.

300
00:12:47,820 --> 00:12:50,430
So we transitioned
basically to two patterns.

301
00:12:50,430 --> 00:12:54,240
We have one cluster with
multiple applications segregated

302
00:12:54,240 --> 00:12:57,600
by this logical pieces called namespace,

303
00:12:57,600 --> 00:13:00,900
as we call it soft multi-tenancy.

304
00:13:00,900 --> 00:13:02,820
And the other pattern is when you have

305
00:13:02,820 --> 00:13:06,510
like several applications
running in the same namespace,

306
00:13:06,510 --> 00:13:08,847
sharing the same namespace.

307
00:13:08,847 --> 00:13:10,830
But how can we ensure

308
00:13:10,830 --> 00:13:13,710
that we have security among all of them?

309
00:13:13,710 --> 00:13:15,690
We usually start seeing

310
00:13:15,690 --> 00:13:18,780
that this is like an overly
permissive environment

311
00:13:18,780 --> 00:13:21,030
because we're not controlling this.

312
00:13:21,030 --> 00:13:23,250
Are we sure that the application A

313
00:13:23,250 --> 00:13:25,620
can communicate to application B?

314
00:13:25,620 --> 00:13:26,760
We're not.

315
00:13:26,760 --> 00:13:29,940
And then these are concerns
about like the development

316
00:13:29,940 --> 00:13:30,960
or application teams,

317
00:13:30,960 --> 00:13:33,030
and they're not really the,

318
00:13:33,030 --> 00:13:36,090
to keep security is not
really their main focus

319
00:13:36,090 --> 00:13:38,100
or to keep like this access control.

320
00:13:38,100 --> 00:13:40,650
So there's one team
missing on this picture,

321
00:13:40,650 --> 00:13:43,590
which is the platform
team or the security team.

322
00:13:43,590 --> 00:13:45,120
The security team and the platform team,

323
00:13:45,120 --> 00:13:48,510
they are responsible for
all of those environments

324
00:13:48,510 --> 00:13:50,700
to like help control access

325
00:13:50,700 --> 00:13:54,480
to help like make sure it's
secure to avoid outages,

326
00:13:54,480 --> 00:13:58,650
whereas our application
teams want to develop fast,

327
00:13:58,650 --> 00:14:01,620
they want to experiment, they
want faster time to market,

328
00:14:01,620 --> 00:14:04,380
they want to speed up
application lifecycle.

329
00:14:04,380 --> 00:14:05,880
And as Luke mentioned,

330
00:14:05,880 --> 00:14:09,000
if the security team doesn't
have like clear visibility,

331
00:14:09,000 --> 00:14:11,850
it's really hard for them to create a,

332
00:14:11,850 --> 00:14:15,550
for the platform team to
have a secure environment,

333
00:14:15,550 --> 00:14:19,620
and then it creates like an
overly permissive environment,

334
00:14:19,620 --> 00:14:22,680
like I said, because if
the security teams put

335
00:14:22,680 --> 00:14:26,190
like too much restriction, they
block the application teams

336
00:14:26,190 --> 00:14:27,963
and it's not good for anyone.

337
00:14:29,190 --> 00:14:32,490
So how do we strike this
balance to have control

338
00:14:32,490 --> 00:14:34,680
and security that's required

339
00:14:34,680 --> 00:14:38,910
for like high regulated environments

340
00:14:38,910 --> 00:14:43,230
and have team autonomy so the teams,

341
00:14:43,230 --> 00:14:45,630
the development teams can be efficient?

342
00:14:45,630 --> 00:14:47,940
Usually, like I said, we fall back

343
00:14:47,940 --> 00:14:51,210
into an overly permissive environment

344
00:14:51,210 --> 00:14:53,250
because there's no like visibility

345
00:14:53,250 --> 00:14:56,880
across networking
communications and we don't want

346
00:14:56,880 --> 00:14:59,130
to block the development.

347
00:14:59,130 --> 00:15:02,400
Does this resonates to like
anyone's environment here?

348
00:15:02,400 --> 00:15:06,090
Anyone's like application
lifecycle process?

349
00:15:06,090 --> 00:15:10,230
Okay, but where do we
really want to go is here.

350
00:15:10,230 --> 00:15:14,250
So we want to have assertive
controls so we are secure

351
00:15:14,250 --> 00:15:16,353
and we are efficient at the same time.

352
00:15:19,140 --> 00:15:21,090
And how can AWS help?

353
00:15:21,090 --> 00:15:24,930
Like I said, our goal is to
simplify overall operations

354
00:15:24,930 --> 00:15:27,270
and overall application life cycle.

355
00:15:27,270 --> 00:15:30,150
So we started tackling a layered approach

356
00:15:30,150 --> 00:15:31,980
to cover all of that.

357
00:15:31,980 --> 00:15:34,110
So starting from the bottom, we want

358
00:15:34,110 --> 00:15:37,290
to simplify overall
infrastructure management,

359
00:15:37,290 --> 00:15:41,250
so platform engineers can really
focus on what's important.

360
00:15:41,250 --> 00:15:43,350
We want to give you full clarity

361
00:15:43,350 --> 00:15:45,990
and full visibility of
all the network paths

362
00:15:45,990 --> 00:15:49,140
and everything that's
happening in your environment

363
00:15:49,140 --> 00:15:53,550
so you can achieve an improved
overall security posture,

364
00:15:53,550 --> 00:15:56,010
which is like one of
the most critical pieces

365
00:15:56,010 --> 00:15:58,083
of everyone's environment.

366
00:15:59,610 --> 00:16:02,030
So starting from the bottom again,

367
00:16:02,030 --> 00:16:06,090
last year at re:Invent we
introduced it EKS Auto Mode.

368
00:16:06,090 --> 00:16:08,100
Remember I talked about Auto Mode.

369
00:16:08,100 --> 00:16:11,310
I know some of you already using it.

370
00:16:11,310 --> 00:16:12,660
And what Auto Mode gives you.

371
00:16:12,660 --> 00:16:15,720
So it's basically one
click create cluster,

372
00:16:15,720 --> 00:16:18,810
and all of the production-ready
best practices,

373
00:16:18,810 --> 00:16:20,910
capabilities are out of the box.

374
00:16:20,910 --> 00:16:23,820
So you don't need to be word
about deploying any of those.

375
00:16:23,820 --> 00:16:26,730
It provisions all the
cluster infrastructure

376
00:16:26,730 --> 00:16:30,930
and automatically does compute scalability

377
00:16:30,930 --> 00:16:35,220
through a key piece company
that's called Karpenter.

378
00:16:35,220 --> 00:16:38,163
Everybody here knows what Karpenter is?

379
00:16:39,750 --> 00:16:41,004
Nice.

380
00:16:41,004 --> 00:16:42,860
So just a quick quick recap
for those who don't know.

381
00:16:42,860 --> 00:16:44,790
Karpenter is a node out scaler

382
00:16:44,790 --> 00:16:48,510
that scales out compute research
based on research requests.

383
00:16:48,510 --> 00:16:51,600
So you have like pending pods,
Karpenter will read those

384
00:16:51,600 --> 00:16:55,290
and in a few seconds it
you spin up any compute

385
00:16:55,290 --> 00:16:57,240
that's required for those workloads

386
00:16:57,240 --> 00:17:00,140
to run based on cost efficiency
and performance efficiency.

387
00:17:00,140 --> 00:17:03,420
And it uses like basically two components

388
00:17:03,420 --> 00:17:06,780
that are completely customizable,

389
00:17:06,780 --> 00:17:09,270
and it's managed entirely
inside Kubernetes.

390
00:17:09,270 --> 00:17:11,550
You don't need to be worried
about managed note groups,

391
00:17:11,550 --> 00:17:13,000
out-scaling groups, whatever.

392
00:17:14,430 --> 00:17:16,590
So together with that like Karpenter

393
00:17:16,590 --> 00:17:18,900
also continuously optimized for cost.

394
00:17:18,900 --> 00:17:21,270
So it just doesn't scale resources,

395
00:17:21,270 --> 00:17:23,433
it optimizes your overall environment.

396
00:17:24,360 --> 00:17:26,550
And then the part that's most important

397
00:17:26,550 --> 00:17:30,420
for this session, Auto Mode
comes with core components

398
00:17:30,420 --> 00:17:32,640
for networking, and we'll be talking

399
00:17:32,640 --> 00:17:35,223
about those three components in a bit,

400
00:17:36,180 --> 00:17:39,600
but also it keeps like all
of those components updated

401
00:17:39,600 --> 00:17:42,690
to patch and security
fixes on the AWS side.

402
00:17:42,690 --> 00:17:47,070
So this way is like
where we started creating

403
00:17:47,070 --> 00:17:49,833
a simplified infrastructure
baseline for you.

404
00:17:50,880 --> 00:17:52,620
So let's talk about like
those three components,

405
00:17:52,620 --> 00:17:54,120
start from the edges.

406
00:17:54,120 --> 00:17:56,310
So CoreDNS and kube-proxy

407
00:17:56,310 --> 00:17:59,790
are very well known components
from the Kubernetes world.

408
00:17:59,790 --> 00:18:03,540
So CoreDNS basically, name
resolution inside your cluster

409
00:18:03,540 --> 00:18:06,990
so you don't need to like be memorizing

410
00:18:06,990 --> 00:18:08,670
IP addresses of course.

411
00:18:08,670 --> 00:18:12,390
And kube-proxy is the component

412
00:18:12,390 --> 00:18:16,140
that understands the egress
and ingress communication

413
00:18:16,140 --> 00:18:20,120
and know hows to direct
the right communication

414
00:18:20,120 --> 00:18:22,260
to the right pods because
you have like a couple

415
00:18:22,260 --> 00:18:23,760
of VNIs probably in your nodes

416
00:18:23,760 --> 00:18:25,410
but you have like a 100 pods.

417
00:18:25,410 --> 00:18:28,230
So kube-proxy we will
do like this transition

418
00:18:28,230 --> 00:18:31,563
and direct the the the
traffic to the right pod.

419
00:18:32,460 --> 00:18:34,507
And in the middle we have like the CNI.

420
00:18:34,507 --> 00:18:36,960
Okay, CNI is one of the
most important parts

421
00:18:36,960 --> 00:18:38,283
for our session today.

422
00:18:39,330 --> 00:18:42,000
So CNI, there are a lot

423
00:18:42,000 --> 00:18:45,180
of CNIs out there on
the open source world,

424
00:18:45,180 --> 00:18:46,933
but last year,

425
00:18:46,933 --> 00:18:49,890
in fact in 2023, we released
it like native support

426
00:18:49,890 --> 00:18:53,180
for network policies on CNI
where in the past you need

427
00:18:53,180 --> 00:18:55,920
to use like a third
party CNI to have access

428
00:18:55,920 --> 00:18:59,343
to all the network capabilities
in your Kubernetes cluster.

429
00:19:00,480 --> 00:19:04,353
How many of you are using
actually using network policies?

430
00:19:06,120 --> 00:19:07,200
Just a few.

431
00:19:07,200 --> 00:19:09,510
So that's, that's, that's really common

432
00:19:09,510 --> 00:19:13,200
because doing network
policies is not easy.

433
00:19:13,200 --> 00:19:14,435
Why?

434
00:19:14,435 --> 00:19:15,270
Because you don't have that kind

435
00:19:15,270 --> 00:19:18,240
of visibility like ease
to west communication part

436
00:19:18,240 --> 00:19:20,463
to part namespace who communicates to who?

437
00:19:22,560 --> 00:19:25,410
All of these components
run in inside the node

438
00:19:25,410 --> 00:19:27,610
and you don't need to
to manage all of them.

439
00:19:28,680 --> 00:19:31,050
So let's talk a little bit
more about network policies

440
00:19:31,050 --> 00:19:34,440
which will improve the security
posture of the cluster.

441
00:19:34,440 --> 00:19:36,720
So in the past before Auto Mode,

442
00:19:36,720 --> 00:19:38,910
there's a custom setup required

443
00:19:38,910 --> 00:19:40,800
for you to enable those network policies.

444
00:19:40,800 --> 00:19:43,800
You need to be worried about
roles and how to enable that.

445
00:19:43,800 --> 00:19:47,070
And this could cause like a
cluster level blast radius

446
00:19:47,070 --> 00:19:51,843
where in EKS Auto Mode it's
like much more streamlined.

447
00:19:53,310 --> 00:19:55,230
Network policies are enabled by default.

448
00:19:55,230 --> 00:19:57,030
You just need to tell them to use it.

449
00:19:57,030 --> 00:19:59,580
So basically you, you need
to create a config map,

450
00:19:59,580 --> 00:20:01,830
and say hey, I want to
use network policies.

451
00:20:01,830 --> 00:20:03,720
Boom, it'll be there.

452
00:20:03,720 --> 00:20:06,000
Then one of the components for Karpenter

453
00:20:06,000 --> 00:20:09,630
that helps you manage the
nodes is the node class, right?

454
00:20:09,630 --> 00:20:11,850
So you have the node
class, it's pretty simple,

455
00:20:11,850 --> 00:20:13,410
it's baseline infrastructure

456
00:20:13,410 --> 00:20:16,620
and then you can define
the default behavior

457
00:20:16,620 --> 00:20:18,600
of your network policies.

458
00:20:18,600 --> 00:20:21,450
By default it comes with allow policies,

459
00:20:21,450 --> 00:20:22,980
but you can change that

460
00:20:22,980 --> 00:20:25,620
to default deny if you
want to, like I said,

461
00:20:25,620 --> 00:20:28,710
if you're running high
regulated environments

462
00:20:28,710 --> 00:20:31,980
and also you can separate
specific node classes.

463
00:20:31,980 --> 00:20:33,570
So I want to have one application

464
00:20:33,570 --> 00:20:35,997
that's fully isolated
from the rest of them,

465
00:20:35,997 --> 00:20:38,190
the the applications that
are running in my cluster.

466
00:20:38,190 --> 00:20:39,630
You don't need to create another cluster.

467
00:20:39,630 --> 00:20:42,120
You can do that like in the compute level,

468
00:20:42,120 --> 00:20:44,547
in the same cluster using EKS Auto Mode

469
00:20:44,547 --> 00:20:45,933
and network policies.

470
00:20:48,210 --> 00:20:49,980
So don't take my word.

471
00:20:49,980 --> 00:20:54,980
Let's just create a really
quick one EKS Auto Cluster here

472
00:20:55,170 --> 00:20:56,760
for the sake of the presentation.

473
00:20:56,760 --> 00:20:59,730
Like I said, we are going
into a very quick journey.

474
00:20:59,730 --> 00:21:01,500
So let's create like a one cluster.

475
00:21:01,500 --> 00:21:03,210
I just put the name,

476
00:21:03,210 --> 00:21:07,800
I see we don't have the role
specific for that cluster.

477
00:21:07,800 --> 00:21:09,330
You can just click a button,

478
00:21:09,330 --> 00:21:11,970
it'll come like with
all the best practices,

479
00:21:11,970 --> 00:21:14,640
granular list privilege for the cluster,

480
00:21:14,640 --> 00:21:17,520
and you just few clicks you
have the recommended role

481
00:21:17,520 --> 00:21:19,520
to create a cluster in your environment.

482
00:21:20,430 --> 00:21:22,740
Secondarily, we have the node role.

483
00:21:22,740 --> 00:21:27,330
So node role in cluster roll,
they have different personas,

484
00:21:27,330 --> 00:21:30,000
so they need to have like
different level of access.

485
00:21:30,000 --> 00:21:32,220
So we can create the same way

486
00:21:32,220 --> 00:21:33,630
that we did for the cluster.

487
00:21:33,630 --> 00:21:37,607
So few clicks, create roll,
we have the node roll running,

488
00:21:37,607 --> 00:21:38,514
(audience clapping)

489
00:21:38,514 --> 00:21:41,250
and then let's go to the network part.

490
00:21:41,250 --> 00:21:45,030
So because of the way that Auto Mode runs,

491
00:21:45,030 --> 00:21:46,770
you can create a very,

492
00:21:46,770 --> 00:21:50,040
a specific network for your cluster.

493
00:21:50,040 --> 00:21:51,930
Like all the best practices will be there,

494
00:21:51,930 --> 00:21:54,000
just click create VPC.

495
00:21:54,000 --> 00:21:55,620
The only things that
I'll be changing, I want

496
00:21:55,620 --> 00:21:57,960
to run in three AZs, not just two.

497
00:21:57,960 --> 00:22:00,990
And I want to use like this
new feature for a regional net.

498
00:22:00,990 --> 00:22:04,890
So with that like I'll be like
in a few minutes I can create

499
00:22:04,890 --> 00:22:08,730
like a cluster rolls and VPC all together

500
00:22:08,730 --> 00:22:09,720
for my environment.

501
00:22:09,720 --> 00:22:12,440
As you see, like all
of that will be created

502
00:22:12,440 --> 00:22:14,280
in a few minutes.

503
00:22:14,280 --> 00:22:16,050
The only thing that takes a little bit

504
00:22:16,050 --> 00:22:19,620
is that gateway to be
there and propagated.

505
00:22:19,620 --> 00:22:23,970
But then we have like
now IM rolls networks

506
00:22:23,970 --> 00:22:26,250
with subnets, everything ready,

507
00:22:26,250 --> 00:22:29,820
and also Auto Mode knows that we want

508
00:22:29,820 --> 00:22:32,040
to run the pods just on
the private networks.

509
00:22:32,040 --> 00:22:34,920
We don't want to have pods
on public-facing networks

510
00:22:34,920 --> 00:22:36,450
with public IP addresses.

511
00:22:36,450 --> 00:22:40,140
So we are protecting
our pods like by design.

512
00:22:40,140 --> 00:22:41,490
And if you want to dive deep there,

513
00:22:41,490 --> 00:22:44,490
like there's some specific
configurations that you can go,

514
00:22:44,490 --> 00:22:47,880
like all the pods and service
networking are already there,

515
00:22:47,880 --> 00:22:50,010
and there's some stuff
that you can opt out

516
00:22:50,010 --> 00:22:51,690
after you create your cluster.

517
00:22:51,690 --> 00:22:54,780
So pretty, pretty easy,
pretty straightforward.

518
00:22:54,780 --> 00:22:56,623
Now we have a cluster running.

519
00:22:56,623 --> 00:22:58,110
In a few minutes,

520
00:22:58,110 --> 00:23:01,680
like we'll have like the cluster
activating than creating.

521
00:23:01,680 --> 00:23:05,130
One thing that I want to
mention is the only add-on

522
00:23:05,130 --> 00:23:08,280
that will be created
is the metrics server.

523
00:23:08,280 --> 00:23:11,280
So all the other add-ons like
for core network capabilities,

524
00:23:11,280 --> 00:23:13,110
Karpenter, port identity,

525
00:23:13,110 --> 00:23:15,120
everything is embedded in the cluster

526
00:23:15,120 --> 00:23:17,430
to simplify your daily operations.

527
00:23:17,430 --> 00:23:19,530
The only thing that you see
there is the metric server

528
00:23:19,530 --> 00:23:20,760
that's not embedded,

529
00:23:20,760 --> 00:23:22,920
but metric server is required

530
00:23:22,920 --> 00:23:24,450
to have like that scalability

531
00:23:24,450 --> 00:23:27,090
and the cost optimization
that we talked about.

532
00:23:27,090 --> 00:23:30,480
So we see clusters there already active.

533
00:23:30,480 --> 00:23:33,540
Now let's enable the network
policy on this cluster.

534
00:23:33,540 --> 00:23:34,860
So the cluster comes

535
00:23:34,860 --> 00:23:37,350
with the network policy already there

536
00:23:37,350 --> 00:23:40,860
on as a capability from the EKS CNI.

537
00:23:40,860 --> 00:23:43,638
But we need to tell
them on the, like I said

538
00:23:43,638 --> 00:23:47,220
on that specific config map that we want

539
00:23:47,220 --> 00:23:49,290
to use network policies.

540
00:23:49,290 --> 00:23:51,450
So we just have like those two pods.

541
00:23:51,450 --> 00:23:53,580
Everything is away,

542
00:23:53,580 --> 00:23:55,770
because the goal of EKS
Auto Mode is for you

543
00:23:55,770 --> 00:23:57,600
to just deploy your applications.

544
00:23:57,600 --> 00:23:59,970
So like you said, like like you saw,

545
00:23:59,970 --> 00:24:03,060
like there's nothing there
other than the metrics server

546
00:24:03,060 --> 00:24:06,330
and the nodes where the
metrics server is running.

547
00:24:06,330 --> 00:24:09,090
So let's see the node class where I show

548
00:24:09,090 --> 00:24:14,090
the, that configuration
for the default behavior

549
00:24:14,730 --> 00:24:16,410
of the network policy.

550
00:24:16,410 --> 00:24:19,290
So let's take a look on
that configuration file.

551
00:24:19,290 --> 00:24:21,690
I'll just filter the
output so it's easier.

552
00:24:21,690 --> 00:24:23,670
We don't have like a big one.

553
00:24:23,670 --> 00:24:25,890
So we have network policy as fallout

554
00:24:25,890 --> 00:24:28,080
and event logs are disabled by default.

555
00:24:28,080 --> 00:24:30,480
Event logs is just to
everything that happens,

556
00:24:30,480 --> 00:24:33,570
everything that matches a
network policy will be logged

557
00:24:33,570 --> 00:24:35,760
on the Kubernetes Events log.

558
00:24:35,760 --> 00:24:39,540
So I have like this manifest already done

559
00:24:39,540 --> 00:24:42,150
and I'll just create it.

560
00:24:42,150 --> 00:24:44,220
Just double checking is not there yet.

561
00:24:44,220 --> 00:24:45,280
I'll just be enabling

562
00:24:46,500 --> 00:24:50,010
that specific config map so we can have

563
00:24:50,010 --> 00:24:53,253
network policy support fully
enabled in the cluster.

564
00:24:54,930 --> 00:24:57,000
So we cover like this, this bottom part

565
00:24:57,000 --> 00:24:58,560
for simplifying overall infrastructure

566
00:24:58,560 --> 00:24:59,820
with those core components,

567
00:24:59,820 --> 00:25:03,660
and being able to enable network policies.

568
00:25:03,660 --> 00:25:06,840
Let's talk a little bit
more about network policies.

569
00:25:06,840 --> 00:25:11,400
Like I said, this was introduced
in 2023 as native feature

570
00:25:11,400 --> 00:25:14,550
for EKS CNI, and it works
a little bit different

571
00:25:14,550 --> 00:25:18,630
from the standard network
policy that you can get

572
00:25:18,630 --> 00:25:19,920
from the community.

573
00:25:19,920 --> 00:25:22,920
So it's a single plugin, fully
integrated, you don't need

574
00:25:22,920 --> 00:25:24,060
to deploy anything else,

575
00:25:24,060 --> 00:25:29,060
and uses EBPF to be
performant as it is today.

576
00:25:29,970 --> 00:25:31,620
Quick recap on what's EBPF.

577
00:25:31,620 --> 00:25:35,910
So EBPF means like Extended
Berkeley Package Filter,

578
00:25:35,910 --> 00:25:37,680
but it's basically a kernel engine

579
00:25:37,680 --> 00:25:40,380
or a kernel module that's enabled

580
00:25:40,380 --> 00:25:43,650
and filter all the SIS calls
that's happening from the user.

581
00:25:43,650 --> 00:25:45,840
So you have like an application

582
00:25:45,840 --> 00:25:50,840
or anything that does a
SIS call, EBPF will inspect

583
00:25:50,850 --> 00:25:53,250
that package and then go back

584
00:25:53,250 --> 00:25:55,680
to the Linux kernel after inspected.

585
00:25:55,680 --> 00:25:57,900
This is like very common use case

586
00:25:57,900 --> 00:26:00,930
for load balancing security and tracing.

587
00:26:00,930 --> 00:26:04,200
But our goal here for
network policy is security,

588
00:26:04,200 --> 00:26:05,610
but we'll be talking,

589
00:26:05,610 --> 00:26:08,220
but we'll be using EBPF
for tracing as well,

590
00:26:08,220 --> 00:26:09,780
but I don't want to spoil the surprise

591
00:26:09,780 --> 00:26:11,230
that Luke will be presenting.

592
00:26:12,270 --> 00:26:13,800
And how does it work under the hood?

593
00:26:13,800 --> 00:26:16,680
So a user can create a network policy.

594
00:26:16,680 --> 00:26:19,110
The network policy controller will read it

595
00:26:19,110 --> 00:26:24,000
and trigger an endpoint CRD
that's exclusive for EKS CNI,

596
00:26:24,000 --> 00:26:27,780
and then EKS CNI, using EBPF
we'll inspect that package

597
00:26:27,780 --> 00:26:30,193
and decide, hey, should I apply

598
00:26:30,193 --> 00:26:33,573
a network policy restriction
or isolation here are not?

599
00:26:36,480 --> 00:26:38,700
When we do like this, this kind of changes

600
00:26:38,700 --> 00:26:40,170
and we apply network policies

601
00:26:40,170 --> 00:26:45,170
and change the default
behavior for that node class,

602
00:26:45,720 --> 00:26:47,970
a specific rollout happens.

603
00:26:47,970 --> 00:26:51,240
So we cover the EC2 node class

604
00:26:51,240 --> 00:26:53,160
in the previous slides,

605
00:26:53,160 --> 00:26:56,490
and the other component from
Karpenter is the node pool.

606
00:26:56,490 --> 00:26:58,530
So the node pool defines the behavior

607
00:26:58,530 --> 00:27:00,780
whereas the node class defines

608
00:27:00,780 --> 00:27:03,300
the baseline infrastructure for the nodes.

609
00:27:03,300 --> 00:27:06,180
So we're just getting
like this one isolated.

610
00:27:06,180 --> 00:27:08,250
The disruption that I'm highlighting

611
00:27:08,250 --> 00:27:11,190
on the slide is just the way

612
00:27:11,190 --> 00:27:14,520
that the rollout will be, will happen.

613
00:27:14,520 --> 00:27:17,130
So for example, I want to disrupt just 10%

614
00:27:17,130 --> 00:27:18,300
of my nodes at a time.

615
00:27:18,300 --> 00:27:21,390
Let's say I have like a huge
environment, it'll not disrupt

616
00:27:21,390 --> 00:27:23,910
and roll out and apply network policies

617
00:27:23,910 --> 00:27:27,870
or any other behavior for
more than 10% at a time.

618
00:27:27,870 --> 00:27:29,700
So this is just an example.

619
00:27:29,700 --> 00:27:32,160
You can do like 1%, 50%,

620
00:27:32,160 --> 00:27:36,750
and a node expires from
time to time to renew

621
00:27:36,750 --> 00:27:38,970
the, the nodes and apply new patches

622
00:27:38,970 --> 00:27:40,720
and new configurations like I said.

623
00:27:41,730 --> 00:27:43,263
So let's see this in action.

624
00:27:44,880 --> 00:27:46,770
This is the cluster that we just created.

625
00:27:46,770 --> 00:27:50,553
Now, what I'll do, I will
deploy an application here.

626
00:27:51,390 --> 00:27:53,460
So this application consists

627
00:27:53,460 --> 00:27:56,490
like a few name spaces, few deployments.

628
00:27:56,490 --> 00:27:59,850
I'll just use like customization
for the sake of time.

629
00:27:59,850 --> 00:28:02,280
Of course we should be
using like GitHub's approach

630
00:28:02,280 --> 00:28:05,670
for this, but this is a talk
for for next year maybe.

631
00:28:05,670 --> 00:28:08,460
So I created all of the
resource components.

632
00:28:08,460 --> 00:28:10,200
We have our application running.

633
00:28:10,200 --> 00:28:12,540
And because the policies that fall out,

634
00:28:12,540 --> 00:28:14,430
everything communicates to everything.

635
00:28:14,430 --> 00:28:17,490
Remember, overly permissive environment.

636
00:28:17,490 --> 00:28:20,010
So we can just get like all the pods.

637
00:28:20,010 --> 00:28:24,000
Now, let's get like the, the URL for us

638
00:28:24,000 --> 00:28:25,200
to access this application,

639
00:28:25,200 --> 00:28:28,680
see if it's really
running and if it's there

640
00:28:28,680 --> 00:28:30,753
and behaving as expected.

641
00:28:31,740 --> 00:28:34,440
So let's get this, let's
paste on our browser.

642
00:28:34,440 --> 00:28:37,620
We have just a specific secret shop

643
00:28:37,620 --> 00:28:39,630
we can buy like anything here.

644
00:28:39,630 --> 00:28:41,880
Let's just do explore.

645
00:28:41,880 --> 00:28:44,881
So we have on this
application, we have this UI,

646
00:28:44,881 --> 00:28:46,860
we have a catalog for all the
products, we have the cards

647
00:28:46,860 --> 00:28:48,210
and we have the orders.

648
00:28:48,210 --> 00:28:49,620
So all of those pieces need

649
00:28:49,620 --> 00:28:51,510
to be communicating together

650
00:28:51,510 --> 00:28:53,823
in order for this shop to really work.

651
00:28:54,960 --> 00:28:57,300
So now what we're going to do, we're going

652
00:28:57,300 --> 00:29:02,010
to apply a default NI
policy for this environment,

653
00:29:02,010 --> 00:29:04,530
and let's see what happens.

654
00:29:04,530 --> 00:29:06,540
So remember today's the fallout.

655
00:29:06,540 --> 00:29:09,930
Let's change this to a default NI policy

656
00:29:09,930 --> 00:29:13,050
and wait for Karpenter to do the rollout

657
00:29:13,050 --> 00:29:14,400
of all the nodes.

658
00:29:14,400 --> 00:29:15,233
For the sake of time,

659
00:29:15,233 --> 00:29:18,346
like i, I cut like the
part of the the the rollout

660
00:29:18,346 --> 00:29:19,613
because it takes like a few minutes,

661
00:29:20,520 --> 00:29:23,010
but you see like the, it, it
won't take more than than three

662
00:29:23,010 --> 00:29:27,390
to five minutes on the the pod history.

663
00:29:27,390 --> 00:29:30,390
So let's change the
network policy for the NI

664
00:29:30,390 --> 00:29:32,430
and enable the event logs

665
00:29:32,430 --> 00:29:34,860
so we can see what's actually happening

666
00:29:34,860 --> 00:29:36,393
on the event logs if we need.

667
00:29:38,220 --> 00:29:39,780
So done.

668
00:29:39,780 --> 00:29:42,930
Let's watch for the nodes behavior.

669
00:29:42,930 --> 00:29:45,930
We didn't trigger like the nose
have like around 20 minutes.

670
00:29:47,910 --> 00:29:49,880
Now let's watch.

671
00:29:49,880 --> 00:29:52,440
In a few seconds you
everything will be triggered

672
00:29:52,440 --> 00:29:54,720
and you see like new nodes running

673
00:29:54,720 --> 00:29:56,070
with the new configuration.

674
00:29:56,070 --> 00:29:58,500
So this is what the rollout is about.

675
00:29:58,500 --> 00:30:00,900
So see we have like a new
node, like this is the time

676
00:30:00,900 --> 00:30:03,120
that Karpenter takes
to spin up a new node.

677
00:30:03,120 --> 00:30:04,443
It's really, really fast.

678
00:30:06,480 --> 00:30:09,153
Now let's go to our application and check.

679
00:30:10,500 --> 00:30:12,603
It doesn't seem to be running.

680
00:30:13,890 --> 00:30:17,820
So on this time, you
receive a call from your CEO

681
00:30:17,820 --> 00:30:21,630
or from a financial
person that will say, hey,

682
00:30:21,630 --> 00:30:23,490
our application is offline.

683
00:30:23,490 --> 00:30:25,983
Whatever you have done, just roll it back.

684
00:30:27,000 --> 00:30:28,500
We have like the new nodes there

685
00:30:28,500 --> 00:30:30,960
and the applications are not
like really communicating.

686
00:30:30,960 --> 00:30:34,620
So what we are going to do, we see,

687
00:30:34,620 --> 00:30:38,070
you see like the applications
weren't even able to spin up

688
00:30:38,070 --> 00:30:40,290
because there's no
communication, live-ness probe,

689
00:30:40,290 --> 00:30:43,170
redness probe, everything just failed.

690
00:30:43,170 --> 00:30:45,510
And how do we investigate
this if we don't have

691
00:30:45,510 --> 00:30:47,880
like proper visibility on the network?

692
00:30:47,880 --> 00:30:51,570
Even if we have access to
the logs, is not traceable.

693
00:30:51,570 --> 00:30:53,550
We don't have like all clear visibility

694
00:30:53,550 --> 00:30:56,100
on who's communicating to who

695
00:30:56,100 --> 00:30:57,750
to approve like this, this configuration.

696
00:30:57,750 --> 00:31:00,690
So let's do what like our other team ask

697
00:31:00,690 --> 00:31:04,350
and then go back to an overly
permissive environment,

698
00:31:04,350 --> 00:31:06,810
and this will sit there forever.

699
00:31:06,810 --> 00:31:09,870
Like the application will go back running,

700
00:31:09,870 --> 00:31:13,380
but then how can we fix those things

701
00:31:13,380 --> 00:31:17,280
to be back to a controlled environment

702
00:31:17,280 --> 00:31:20,403
with our application
running in an efficient way?

703
00:31:21,510 --> 00:31:22,620
Right?

704
00:31:22,620 --> 00:31:25,260
So how do we control access?

705
00:31:25,260 --> 00:31:27,360
Let's say we go back to the default NI

706
00:31:27,360 --> 00:31:29,910
and then we need to apply a network policy

707
00:31:29,910 --> 00:31:34,233
to open the communication to the orders,

708
00:31:35,520 --> 00:31:37,943
namespace from the catalog,
from the UI namespace, right?

709
00:31:39,030 --> 00:31:42,533
So you can see like this is
a, this is a quick example

710
00:31:42,533 --> 00:31:43,366
of a network policy.

711
00:31:43,366 --> 00:31:46,950
So on the spec we have
like the pod selector.

712
00:31:46,950 --> 00:31:51,840
So all the orders, pods on the
order name space is allowed

713
00:31:51,840 --> 00:31:53,550
to receive ingress communication

714
00:31:53,550 --> 00:31:56,100
from the UI as you see in the bottom.

715
00:31:56,100 --> 00:31:59,160
But then you need to open
for the checkout as well,

716
00:31:59,160 --> 00:32:00,840
and then you need to go to the UI

717
00:32:00,840 --> 00:32:03,720
and say hey UI, you need, you can go

718
00:32:03,720 --> 00:32:07,890
to these two name spaces and
then you need another policy

719
00:32:07,890 --> 00:32:10,260
and another policy.

720
00:32:10,260 --> 00:32:12,900
And then what about DNS resolution?

721
00:32:12,900 --> 00:32:15,420
Do I need to open
communication to the CoreDNS

722
00:32:15,420 --> 00:32:16,923
so I can have name resolution?

723
00:32:17,790 --> 00:32:20,520
Do the spots need to access internet?

724
00:32:20,520 --> 00:32:22,170
Who talks to who?

725
00:32:22,170 --> 00:32:25,650
And all of these questions
falls into the platform team

726
00:32:25,650 --> 00:32:27,540
because they don't have visibility.

727
00:32:27,540 --> 00:32:29,422
Even if the application teams knows

728
00:32:29,422 --> 00:32:32,130
where their application is going,

729
00:32:32,130 --> 00:32:34,710
they usually don't know
who's accessing them.

730
00:32:34,710 --> 00:32:36,930
So how do you overcome that

731
00:32:36,930 --> 00:32:39,480
if we don't have proper visibility?

732
00:32:39,480 --> 00:32:41,910
So we cover like the top layer

733
00:32:41,910 --> 00:32:45,270
to improve overall security, right?

734
00:32:45,270 --> 00:32:46,290
But if we go

735
00:32:46,290 --> 00:32:50,760
to an overly restricted
environment, things break.

736
00:32:50,760 --> 00:32:52,800
Something's still missing.

737
00:32:52,800 --> 00:32:55,620
Can you help me out with this one, Luke?

738
00:32:55,620 --> 00:32:56,453
- Absolutely.

739
00:32:57,630 --> 00:33:00,798
So as Rodrigo pointed out, it's very hard

740
00:33:00,798 --> 00:33:04,200
to accurately optimize
without sufficient visibility.

741
00:33:04,200 --> 00:33:06,065
You can have the capabilities

742
00:33:06,065 --> 00:33:09,090
but normally what ends
up happening is a lot

743
00:33:09,090 --> 00:33:11,910
of customers end up bouncing
between two extremes,

744
00:33:11,910 --> 00:33:13,980
overly permissive and overly restrictive,

745
00:33:13,980 --> 00:33:15,990
and that's what impacts
their environments.

746
00:33:15,990 --> 00:33:17,910
So there's a critical
layer that is needed,

747
00:33:17,910 --> 00:33:19,710
and that's the missing
piece from the diagram

748
00:33:19,710 --> 00:33:21,360
that Rodrigo just showed,

749
00:33:21,360 --> 00:33:23,260
and that is essentially observability.

750
00:33:24,600 --> 00:33:26,190
And we're really excited
for this new feature

751
00:33:26,190 --> 00:33:27,630
that we recently launched.

752
00:33:27,630 --> 00:33:30,333
Container network observability in EKS.

753
00:33:31,650 --> 00:33:33,500
And this is a key

754
00:33:33,500 --> 00:33:36,690
or fundamental aspect to
achieving the right kind

755
00:33:36,690 --> 00:33:38,850
of optimization you want for security,

756
00:33:38,850 --> 00:33:42,390
as Rodrigo's been emphasizing
on, but also for performance

757
00:33:42,390 --> 00:33:44,640
and also for cost,

758
00:33:44,640 --> 00:33:47,160
and ensuring that traffic
behavior in the cluster

759
00:33:47,160 --> 00:33:49,410
aligns with design.

760
00:33:49,410 --> 00:33:51,270
This allows you to essentially have

761
00:33:51,270 --> 00:33:54,510
your ES network environment
a lot more observable,

762
00:33:54,510 --> 00:33:56,580
which is very important when it comes

763
00:33:56,580 --> 00:34:00,210
to successfully scaling in
widely distributed environments.

764
00:34:00,210 --> 00:34:02,160
You need to be able to track east

765
00:34:02,160 --> 00:34:04,770
to west traffic within
the scope of the cluster

766
00:34:04,770 --> 00:34:08,220
but also you wanna be able
to track traffic between pods

767
00:34:08,220 --> 00:34:09,933
and certain AWS services.

768
00:34:11,370 --> 00:34:14,430
In addition to that, as I
saw some hands raised, a lot

769
00:34:14,430 --> 00:34:16,680
of you are also running
applications in the cluster

770
00:34:16,680 --> 00:34:18,570
that are speaking to
services outside the scope

771
00:34:18,570 --> 00:34:19,803
of AWS as a whole.

772
00:34:22,170 --> 00:34:24,960
Now, there's some key ways

773
00:34:24,960 --> 00:34:27,660
that this new feature can
essentially enable you

774
00:34:27,660 --> 00:34:29,710
or empower you in this particular regard.

775
00:34:30,690 --> 00:34:32,040
I wanna focus on the use cases

776
00:34:32,040 --> 00:34:33,870
before we get into the
specific feature sets

777
00:34:33,870 --> 00:34:36,030
with container network observability.

778
00:34:36,030 --> 00:34:37,110
So a first one

779
00:34:37,110 --> 00:34:39,840
that I wanna touch on is
measuring network performance.

780
00:34:39,840 --> 00:34:42,180
So this is, this is especially important

781
00:34:42,180 --> 00:34:45,000
because this is something that
a lot of customers run into,

782
00:34:45,000 --> 00:34:47,790
to be more specific
platform teams, being able

783
00:34:47,790 --> 00:34:52,170
to quickly pinpoint where
an issue originated from.

784
00:34:52,170 --> 00:34:53,940
A lot of teams have already standardized

785
00:34:53,940 --> 00:34:55,560
on their observability stack

786
00:34:55,560 --> 00:34:58,770
and a lot of customers will
essentially be sending data

787
00:34:58,770 --> 00:35:01,740
from their Kubernetes
cluster or their EKS cluster

788
00:35:01,740 --> 00:35:03,810
to some kind of observability stack,

789
00:35:03,810 --> 00:35:07,860
and that may be managed
solutions within AWS.

790
00:35:07,860 --> 00:35:09,630
You could be using CloudWatch,

791
00:35:09,630 --> 00:35:12,477
you could be using
Amazon managed Prometheus

792
00:35:12,477 --> 00:35:13,310
and Amazon managed Grafana,

793
00:35:13,310 --> 00:35:15,540
or you could be using
open source solutions

794
00:35:15,540 --> 00:35:18,477
and managing them yourself
with Prometheus and Grafana.

795
00:35:18,477 --> 00:35:20,940
And so this was especially
important for us to ensure

796
00:35:20,940 --> 00:35:23,280
that if we're going to
enable our customers

797
00:35:23,280 --> 00:35:25,230
in the right kind of way,
then we need to align

798
00:35:25,230 --> 00:35:26,763
with your usage patterns.

799
00:35:27,630 --> 00:35:30,780
And so we're providing
more key metrics now

800
00:35:30,780 --> 00:35:33,630
that are a lot more
granular at the pod level

801
00:35:33,630 --> 00:35:36,300
and the worker node level that
are related to networking,

802
00:35:36,300 --> 00:35:38,400
allowing you to quickly pinpoint issues

803
00:35:38,400 --> 00:35:42,180
like network congestion at
a pod level, seeing the enis

804
00:35:42,180 --> 00:35:44,070
that are associated with that, being able

805
00:35:44,070 --> 00:35:45,766
to quickly determine

806
00:35:45,766 --> 00:35:46,650
if there was a bandwidth exhaustion issue

807
00:35:46,650 --> 00:35:49,410
in a specific worker node,
whether it was related

808
00:35:49,410 --> 00:35:53,040
to ingress or egress, being
able to track the flow count

809
00:35:53,040 --> 00:35:56,460
for pods, and being able

810
00:35:56,460 --> 00:36:01,410
to see in a very quick way
what the particular issue is.

811
00:36:01,410 --> 00:36:03,900
And that reduces the mean
time to detection as well

812
00:36:03,900 --> 00:36:06,660
as mean time to resolution,
which is extremely important

813
00:36:06,660 --> 00:36:08,490
when incidents arise.

814
00:36:08,490 --> 00:36:10,140
Measuring your net,
the network performance

815
00:36:10,140 --> 00:36:12,120
to detect anomalies.

816
00:36:12,120 --> 00:36:14,460
And in many cases customers
essentially have thresholds

817
00:36:14,460 --> 00:36:15,870
that they'll set,

818
00:36:15,870 --> 00:36:17,850
and once that threshold gets breached,

819
00:36:17,850 --> 00:36:19,230
they can proactively respond

820
00:36:19,230 --> 00:36:21,873
to these issues before they get worse.

821
00:36:23,850 --> 00:36:25,890
Now, in addition to that, what we've seen

822
00:36:25,890 --> 00:36:27,930
is that is normally the starting point.

823
00:36:27,930 --> 00:36:29,910
You'll have some kind
of observability stack

824
00:36:29,910 --> 00:36:33,243
where some alert or alarm will notify you,

825
00:36:34,110 --> 00:36:37,233
but after that, there's a
tail end of the investigation.

826
00:36:38,220 --> 00:36:40,500
You wanna wait to scope
down the particular area

827
00:36:40,500 --> 00:36:42,180
where the issue originated from,

828
00:36:42,180 --> 00:36:44,520
you wanna double-click on it essentially

829
00:36:44,520 --> 00:36:47,250
and then carry your
chair of investigations.

830
00:36:47,250 --> 00:36:49,560
Now, sometimes this can
be especially painful

831
00:36:49,560 --> 00:36:52,920
if you're solely relying
on TCP dump in the cluster.

832
00:36:52,920 --> 00:36:54,220
Can anyone relate to that?

833
00:36:55,230 --> 00:36:57,180
That can be a lengthy and painful

834
00:36:57,180 --> 00:36:58,980
and costly process as well.

835
00:36:58,980 --> 00:37:00,780
And so we thought it
was especially important

836
00:37:00,780 --> 00:37:03,570
to provide additional
capabilities for the tail end

837
00:37:03,570 --> 00:37:04,980
of the investigation as well,

838
00:37:04,980 --> 00:37:07,860
something that compliments
proactive anomaly detection

839
00:37:07,860 --> 00:37:10,170
with whatever observability
stack you're already using.

840
00:37:10,170 --> 00:37:13,230
And so there are now native
visualizations in the cluster

841
00:37:13,230 --> 00:37:17,100
as well that you can make use
of, being able to track east

842
00:37:17,100 --> 00:37:19,470
to west traffic within the
scope of the cluster, traffic

843
00:37:19,470 --> 00:37:21,780
between pods and AWS services.

844
00:37:21,780 --> 00:37:23,850
At launch we support traffic

845
00:37:23,850 --> 00:37:26,283
between pods and S3 as well as DynamoDB,

846
00:37:27,404 --> 00:37:29,190
but then also being able to
see pods that are speaking

847
00:37:29,190 --> 00:37:32,070
to a cluster external destinations

848
00:37:32,070 --> 00:37:33,930
and more specific
destinations outside the scope

849
00:37:33,930 --> 00:37:35,700
of AWS as a whole.

850
00:37:35,700 --> 00:37:38,850
You can also leverage these
native console capabilities

851
00:37:38,850 --> 00:37:40,500
to track the top talkers.

852
00:37:40,500 --> 00:37:41,790
That's something that a lot of customers

853
00:37:41,790 --> 00:37:43,170
are particularly interested in.

854
00:37:43,170 --> 00:37:45,420
What are the workloads
responsible for the most traffic

855
00:37:45,420 --> 00:37:46,770
that is being sent within the scope

856
00:37:46,770 --> 00:37:49,620
of the cluster or outside the cluster?

857
00:37:49,620 --> 00:37:51,900
Being able to see traffic
that is traversing

858
00:37:51,900 --> 00:37:54,120
different availability zones.

859
00:37:54,120 --> 00:37:56,010
How do you verify accurately

860
00:37:56,010 --> 00:37:58,260
that the cost optimization techniques

861
00:37:58,260 --> 00:38:00,923
that you're adopting are
actually working as expected?

862
00:38:05,010 --> 00:38:06,660
And so a lot of what
I've been speaking about

863
00:38:06,660 --> 00:38:09,630
is essentially captured
in this diagram over here.

864
00:38:09,630 --> 00:38:12,570
We essentially took a closer
look at the usage pattern

865
00:38:12,570 --> 00:38:16,350
among our customers, and we
didn't want want platform teams

866
00:38:16,350 --> 00:38:18,690
to have to deviate to fit into a new norm,

867
00:38:18,690 --> 00:38:20,970
but rather how can we make
the EKS network environment

868
00:38:20,970 --> 00:38:22,530
more observable aligning

869
00:38:22,530 --> 00:38:24,660
with what customers
are already practicing?

870
00:38:24,660 --> 00:38:29,040
And so what you'll see is teams
will essentially be relying

871
00:38:29,040 --> 00:38:33,750
on data that they can leverage
to measure performance.

872
00:38:33,750 --> 00:38:35,670
That is a key one for a lot of customers.

873
00:38:35,670 --> 00:38:37,860
How do we accurately measure
the network performance

874
00:38:37,860 --> 00:38:39,120
in our environment?

875
00:38:39,120 --> 00:38:41,370
And so you've already got
some observability stack.

876
00:38:41,370 --> 00:38:43,777
And so the new metrics
that we're providing

877
00:38:43,777 --> 00:38:46,950
are available in open metrics format,

878
00:38:46,950 --> 00:38:51,950
and EKS is using Amazon
CloudWatch Network Monitoring.

879
00:38:52,200 --> 00:38:54,843
So this is a feature that
was launched last year,

880
00:38:55,710 --> 00:38:59,010
and so we partnered
together to further align

881
00:38:59,010 --> 00:39:01,290
with EKS use cases.

882
00:39:01,290 --> 00:39:04,110
And so CloudWatch Network
Monitoring has an EBPF agent

883
00:39:04,110 --> 00:39:05,730
that runs on the worker nodes

884
00:39:05,730 --> 00:39:08,370
and it captures the network
flows, to be more specific,

885
00:39:08,370 --> 00:39:11,230
the top 500 network flows
on every worker node

886
00:39:12,150 --> 00:39:15,300
as well as the flow metrics associated

887
00:39:15,300 --> 00:39:17,013
with each of those flows.

888
00:39:17,013 --> 00:39:18,630
And so we have two sets of metrics.

889
00:39:18,630 --> 00:39:21,330
We have system metrics
as well as flow metrics.

890
00:39:21,330 --> 00:39:23,400
The system metrics can be scraped directly

891
00:39:23,400 --> 00:39:24,660
from the worker node and sent

892
00:39:24,660 --> 00:39:26,970
to your preferred monitoring solution.

893
00:39:26,970 --> 00:39:29,610
So if you are using
Prometheus, for example,

894
00:39:29,610 --> 00:39:34,290
then you can directly scrape
these from the NFM agent.

895
00:39:34,290 --> 00:39:36,915
Alternatively, you may be
using Amazon-managed Prometheus

896
00:39:36,915 --> 00:39:39,370
and you could use a scrape
list mechanism as well

897
00:39:40,770 --> 00:39:42,543
to essentially ingest those.

898
00:39:46,470 --> 00:39:48,870
Now, the tail end of it, as
I pointed out, you'll see

899
00:39:48,870 --> 00:39:50,490
that there's an arrow
from the platform team

900
00:39:50,490 --> 00:39:52,590
that's also going to the EKS console,

901
00:39:52,590 --> 00:39:54,450
and that is where you'll essentially find

902
00:39:54,450 --> 00:39:57,060
the additional capabilities
that we are now providing

903
00:39:57,060 --> 00:39:59,100
to help you in accelerating

904
00:39:59,100 --> 00:40:01,323
and having more precise troubleshooting.

905
00:40:03,660 --> 00:40:07,110
Now, I think what you're
looking forward to seeing

906
00:40:07,110 --> 00:40:09,693
is what this actually
looks like in action.

907
00:40:12,570 --> 00:40:15,630
And so if you're using the
console to enable this feature,

908
00:40:15,630 --> 00:40:18,330
you would essentially just
click this one button over here

909
00:40:18,330 --> 00:40:21,330
or this checkbox rather to
enable network monitoring

910
00:40:21,330 --> 00:40:23,310
and it will automatically create

911
00:40:23,310 --> 00:40:24,660
the underlying dependencies

912
00:40:24,660 --> 00:40:26,730
that are in CloudWatch
network flow monitoring.

913
00:40:26,730 --> 00:40:28,440
And we've documented these
clearly for you to see

914
00:40:28,440 --> 00:40:30,150
exactly what will be provided

915
00:40:30,150 --> 00:40:31,590
and we'll also guide
you through the process

916
00:40:31,590 --> 00:40:34,110
of installing the relevant NFM add-on.

917
00:40:34,110 --> 00:40:36,977
Now, in this case over
here it's already enabled,

918
00:40:36,977 --> 00:40:38,670
and so you can also see the
capabilities over there.

919
00:40:38,670 --> 00:40:40,230
There's a service map and a flow table

920
00:40:40,230 --> 00:40:42,090
and those are provided in the console.

921
00:40:42,090 --> 00:40:43,890
The performance metrics
endpoint is something

922
00:40:43,890 --> 00:40:46,240
that you can leverage
and directly scrape from.

923
00:40:48,270 --> 00:40:50,620
We can see the status
over here of the monitor.

924
00:40:52,890 --> 00:40:54,540
So from what we gather,

925
00:40:54,540 --> 00:40:57,423
everything looks like it's
already set up and enabled,

926
00:40:58,466 --> 00:41:00,090
and so we're gonna follow the
normal workflow that a lot

927
00:41:00,090 --> 00:41:01,200
of customers already use.

928
00:41:01,200 --> 00:41:03,150
And so the starting point
is typically you've got

929
00:41:03,150 --> 00:41:05,490
your own observability
stack as I pointed out.

930
00:41:05,490 --> 00:41:08,460
So this is a Grafana dashboard
over here, which is something

931
00:41:08,460 --> 00:41:10,260
that a lot of customers rely on already.

932
00:41:10,260 --> 00:41:11,310
They're already standardized

933
00:41:11,310 --> 00:41:13,470
on using Grafana and so
we fit right into that.

934
00:41:13,470 --> 00:41:15,300
And so over here this is a dashboard

935
00:41:15,300 --> 00:41:17,280
that actually visualizes
all the key metrics

936
00:41:17,280 --> 00:41:20,130
that we're providing now, and
you can see the top 10 pods

937
00:41:20,130 --> 00:41:21,990
by ingress bandwidth,

938
00:41:21,990 --> 00:41:24,570
and this allows you to
actually track these things.

939
00:41:24,570 --> 00:41:26,310
We've had a lot of customers
speak to us to say,

940
00:41:26,310 --> 00:41:28,980
how can I easily understand the amount

941
00:41:28,980 --> 00:41:31,230
of bandwidth being leveraged
by CoreDNS, for example?

942
00:41:31,230 --> 00:41:33,373
'Cause that's a critical
component that lots

943
00:41:33,373 --> 00:41:35,880
pods are essentially
communicating with in order

944
00:41:35,880 --> 00:41:38,790
to resolve their
communication with other pods.

945
00:41:38,790 --> 00:41:42,420
We can do the same for
egress bandwidth as well.

946
00:41:42,420 --> 00:41:45,310
We can look at the ingress
bandwidth allowance exceeded

947
00:41:47,730 --> 00:41:49,857
and detect any anomalies
associated with the worker nodes,

948
00:41:49,857 --> 00:41:51,780
and we've got the metadata
that you can capture

949
00:41:51,780 --> 00:41:52,890
that is captured here as well.

950
00:41:52,890 --> 00:41:56,130
And you can see that in
your Grafana dashboard now.

951
00:41:56,130 --> 00:41:57,630
Everything consolidated in one place

952
00:41:57,630 --> 00:42:00,423
fitting right into your
existing usage pattern.

953
00:42:02,280 --> 00:42:04,410
We can also see the
packets per second allow

954
00:42:04,410 --> 00:42:07,500
and succeeded, connection
tracking allow and succeeded,

955
00:42:07,500 --> 00:42:08,973
and you can see the all the worker nodes

956
00:42:08,973 --> 00:42:10,713
that this is associated with.

957
00:42:11,610 --> 00:42:15,030
And the key thing here again
is to further enable you

958
00:42:15,030 --> 00:42:17,100
to easily detect an issue

959
00:42:17,100 --> 00:42:18,840
and trace it back to the specific point

960
00:42:18,840 --> 00:42:22,470
of origin without having
a long stretch in getting

961
00:42:22,470 --> 00:42:24,960
to what the issue actually is,

962
00:42:24,960 --> 00:42:27,960
detecting it as well as resolving it.

963
00:42:27,960 --> 00:42:30,900
You also have the ability now
to track the egress bandwidth

964
00:42:30,900 --> 00:42:34,080
for your pods as well as the
ingress bandwidth, ensuring

965
00:42:34,080 --> 00:42:35,940
that this aligns with what you expect,

966
00:42:35,940 --> 00:42:38,340
or if you need to be
modifying your environment,

967
00:42:40,500 --> 00:42:43,773
to align with usage by
your customers as well.

968
00:42:45,990 --> 00:42:48,060
We can see as well for the TCP flow count

969
00:42:48,060 --> 00:42:49,263
for the different pods.

970
00:42:50,370 --> 00:42:53,220
We can see the packet rate
from ingress rate as well

971
00:42:53,220 --> 00:42:54,933
as egress as well for the pods.

972
00:42:56,550 --> 00:42:59,490
Drilling in more and more and
giving you more granularity

973
00:42:59,490 --> 00:43:01,710
so that you have sufficient information.

974
00:43:01,710 --> 00:43:03,450
And as Rodrigo pointed out,

975
00:43:03,450 --> 00:43:06,480
before we get to the point
of optimizing things, we need

976
00:43:06,480 --> 00:43:09,030
to have this detailed layer of visibility

977
00:43:09,030 --> 00:43:10,950
that can give us informed decision-making

978
00:43:10,950 --> 00:43:13,200
and that empowers it,
empowers our customers

979
00:43:13,200 --> 00:43:15,900
or enables them even better
to be able to respond quickly

980
00:43:15,900 --> 00:43:17,733
to issues when they arise.

981
00:43:22,260 --> 00:43:23,550
Now, the next thing that follows

982
00:43:23,550 --> 00:43:26,160
after this is you may
get some kind of page

983
00:43:26,160 --> 00:43:28,500
or alert that would happen,

984
00:43:28,500 --> 00:43:30,550
and you would have sufficient information

985
00:43:31,470 --> 00:43:34,320
to then zone in on a specific cluster.

986
00:43:34,320 --> 00:43:36,107
Now, in this case we have

987
00:43:36,107 --> 00:43:37,020
a very basic e-commerce application

988
00:43:37,020 --> 00:43:39,767
that uses the GraphQL API

989
00:43:39,767 --> 00:43:41,910
and this is just showing again
that this is already enabled.

990
00:43:41,910 --> 00:43:43,170
So I'm gonna switch back and we're looking

991
00:43:43,170 --> 00:43:44,373
at the service map now,

992
00:43:45,900 --> 00:43:48,360
and this is specific to
the e-commerce namespace.

993
00:43:48,360 --> 00:43:50,010
We've got three microservices here.

994
00:43:50,010 --> 00:43:53,610
We have a GraphQL API that
speaks to an orders application

995
00:43:53,610 --> 00:43:54,960
and a products application.

996
00:43:56,190 --> 00:43:58,110
You can change time if
you need to scope it

997
00:43:58,110 --> 00:43:59,910
to a one-hour session

998
00:43:59,910 --> 00:44:00,969
or if you want to reduce that

999
00:44:00,969 --> 00:44:02,040
to five minutes to see what's happening.

1000
00:44:02,040 --> 00:44:04,380
We can click on any one of these flows.

1001
00:44:04,380 --> 00:44:06,030
And then what we see
here is forward direction

1002
00:44:06,030 --> 00:44:07,560
as well as reverse direction in the case

1003
00:44:07,560 --> 00:44:10,050
that there is bidirectional traffic.

1004
00:44:10,050 --> 00:44:11,280
We can zone in even further

1005
00:44:11,280 --> 00:44:12,540
'cause this is the kind of granularity

1006
00:44:12,540 --> 00:44:15,930
that becomes extremely useful,
being able to see the flow

1007
00:44:15,930 --> 00:44:17,880
between pods that are actually talking

1008
00:44:17,880 --> 00:44:19,980
to each other at a specific replica level.

1009
00:44:21,840 --> 00:44:23,640
And because this can be
a bit noisy, for example,

1010
00:44:23,640 --> 00:44:25,290
for the last one hour
or the last 30 minutes,

1011
00:44:25,290 --> 00:44:26,820
you can also change this
if you want it to be

1012
00:44:26,820 --> 00:44:27,990
for the last five minutes.

1013
00:44:27,990 --> 00:44:29,820
And again, you can zone in further

1014
00:44:29,820 --> 00:44:30,690
and see the amount of data

1015
00:44:30,690 --> 00:44:33,330
that was transferred on
these specific flows.

1016
00:44:33,330 --> 00:44:34,860
And this is where the
flow metrics come in.

1017
00:44:34,860 --> 00:44:36,240
You can also see retransmissions

1018
00:44:36,240 --> 00:44:37,530
and retransmission timeouts.

1019
00:44:37,530 --> 00:44:39,810
If you're trying to detect
latency for example,

1020
00:44:39,810 --> 00:44:42,120
the higher your retransmission timeouts,

1021
00:44:42,120 --> 00:44:44,100
the more you see a
significant impact of latency

1022
00:44:44,100 --> 00:44:45,720
between specific pods,

1023
00:44:45,720 --> 00:44:46,950
and you'll be able to use

1024
00:44:46,950 --> 00:44:48,840
these kinds of capabilities for that.

1025
00:44:48,840 --> 00:44:50,070
Let's drill in even further.

1026
00:44:50,070 --> 00:44:51,150
We can go to view details

1027
00:44:51,150 --> 00:44:53,520
and what we have over here
is actually a flow table

1028
00:44:53,520 --> 00:44:56,520
that allows us to see at a
more granular level the flows

1029
00:44:56,520 --> 00:44:57,720
for the different pods that are talking

1030
00:44:57,720 --> 00:45:00,420
to each other in this specific namespace.

1031
00:45:00,420 --> 00:45:03,330
You can filter for more
granularity and precision.

1032
00:45:03,330 --> 00:45:05,130
Say we wanna look at flows

1033
00:45:05,130 --> 00:45:08,133
for where the local pod
is is a GraphQL pod,

1034
00:45:09,037 --> 00:45:10,020
and we can scroll to the side there again,

1035
00:45:10,020 --> 00:45:12,000
we can still see each of
those flow level metrics.

1036
00:45:12,000 --> 00:45:15,510
Retransmissions, retransmission
timeouts for remote pod.

1037
00:45:15,510 --> 00:45:17,510
In this case I'm going to choose orders.

1038
00:45:19,320 --> 00:45:21,720
And there we see the
amount of data transferred.

1039
00:45:24,750 --> 00:45:25,800
And you can see right at the top

1040
00:45:25,800 --> 00:45:27,593
there we've also got events.

1041
00:45:27,593 --> 00:45:29,520
So this links to the
resources for you to be able

1042
00:45:29,520 --> 00:45:31,260
to see any pod events in the event

1043
00:45:31,260 --> 00:45:34,010
that there were particular
issues that you saw as well.

1044
00:45:35,430 --> 00:45:37,560
Just sort of consolidating
things even further for you

1045
00:45:37,560 --> 00:45:41,970
to have this additional information

1046
00:45:41,970 --> 00:45:43,380
when you're carrying
out your troubleshooting

1047
00:45:43,380 --> 00:45:44,613
or debugging of issues.

1048
00:45:49,710 --> 00:45:51,180
And next up, we're gonna look

1049
00:45:51,180 --> 00:45:53,070
at a photo gallery application.

1050
00:45:53,070 --> 00:45:55,530
So over here we currently
see that there is nothing.

1051
00:45:55,530 --> 00:45:57,930
Might take a little bit of
time, we switch it to 15 minutes

1052
00:45:57,930 --> 00:45:59,010
and there's nothing going on here.

1053
00:45:59,010 --> 00:46:00,915
So we're gonna switch
to the flow table view.

1054
00:46:00,915 --> 00:46:03,480
And what this photo gallery
application is actually doing

1055
00:46:03,480 --> 00:46:06,900
is it's essentially just
uploading images to S3

1056
00:46:06,900 --> 00:46:09,600
and then storing metadata in DynamoDB.

1057
00:46:09,600 --> 00:46:10,980
So if we go to the flow table you'll see

1058
00:46:10,980 --> 00:46:13,020
that there are three
perspectives over here.

1059
00:46:13,020 --> 00:46:14,970
There's the AWS Service view, which is pod

1060
00:46:14,970 --> 00:46:16,560
to an AWS Service.

1061
00:46:16,560 --> 00:46:18,210
There's the cluster view which is east

1062
00:46:18,210 --> 00:46:19,470
to west traffic within the cluster

1063
00:46:19,470 --> 00:46:22,260
and then there's external
view pod to a destination

1064
00:46:22,260 --> 00:46:24,360
that is outside of AWS.

1065
00:46:24,360 --> 00:46:25,560
Now, in this case we can see here

1066
00:46:25,560 --> 00:46:28,170
for our photo gallery
application, we can see the pods

1067
00:46:28,170 --> 00:46:30,240
is actually, the pods
that are speaking to S3

1068
00:46:30,240 --> 00:46:32,100
as well as DynamoDB.

1069
00:46:32,100 --> 00:46:33,990
We can also see the volume
of data transferred.

1070
00:46:33,990 --> 00:46:36,753
We can see the local pod
IP and the remote pod IP.

1071
00:46:37,710 --> 00:46:40,470
If you wanna modify the
columns that you're seeing,

1072
00:46:40,470 --> 00:46:43,110
you can also do that and
customize it for what's specific

1073
00:46:43,110 --> 00:46:44,340
and more purposeful

1074
00:46:44,340 --> 00:46:45,450
for the way you carry out

1075
00:46:45,450 --> 00:46:47,300
your investigations within your team.

1076
00:46:48,840 --> 00:46:51,060
Data transfer is one that is
typically really important

1077
00:46:51,060 --> 00:46:52,660
for customers to be able to see.

1078
00:46:57,300 --> 00:46:59,100
I'm gonna switch to the cluster
view so you can see this.

1079
00:46:59,100 --> 00:47:00,960
And for the cluster view, I'll switch back

1080
00:47:00,960 --> 00:47:02,793
to the e-commerce namespace.

1081
00:47:04,050 --> 00:47:05,670
At the moment here we'll be seeing things

1082
00:47:05,670 --> 00:47:07,200
for the photo gallery application,

1083
00:47:07,200 --> 00:47:09,000
but we switch back to the e-commerce

1084
00:47:09,000 --> 00:47:11,130
and you'll see that now
we're seeing about 300 flows

1085
00:47:11,130 --> 00:47:12,390
that are represented over here.

1086
00:47:12,390 --> 00:47:14,430
And again, you can change the time range,

1087
00:47:14,430 --> 00:47:16,260
and of course the greater the
time range, the more flows

1088
00:47:16,260 --> 00:47:18,146
that you're going to have.

1089
00:47:18,146 --> 00:47:19,055
For this particular demo,

1090
00:47:19,055 --> 00:47:20,453
you'll see it'll probably only go up

1091
00:47:20,453 --> 00:47:21,286
by two additional flows.

1092
00:47:21,286 --> 00:47:23,250
So we'll see about 302 flows represented.

1093
00:47:23,250 --> 00:47:24,600
There you go.

1094
00:47:24,600 --> 00:47:27,633
If we switch back to 15
minutes, it's about 300.

1095
00:47:31,080 --> 00:47:33,030
And similar to what we
saw when we were going

1096
00:47:33,030 --> 00:47:35,040
through the service map,
even here in this flow table,

1097
00:47:35,040 --> 00:47:35,940
you can filter further

1098
00:47:35,940 --> 00:47:37,350
if you're looking for something specific.

1099
00:47:37,350 --> 00:47:40,170
Again, this fits within that
workflow that we pointed out

1100
00:47:40,170 --> 00:47:41,700
that I pointed out earlier

1101
00:47:41,700 --> 00:47:44,190
where you would've started
from your observability stack,

1102
00:47:44,190 --> 00:47:45,540
you would already have some kind of alert

1103
00:47:45,540 --> 00:47:47,250
that's giving you sufficient
information to know

1104
00:47:47,250 --> 00:47:49,530
what you should be looking
for so that when you come

1105
00:47:49,530 --> 00:47:50,970
to the service map or the flow table,

1106
00:47:50,970 --> 00:47:52,290
you have enough context

1107
00:47:52,290 --> 00:47:54,640
to just accelerate the
troubleshooting process.

1108
00:47:57,540 --> 00:47:58,650
As I mentioned earlier as well,

1109
00:47:58,650 --> 00:47:59,970
this is something that can also inform

1110
00:47:59,970 --> 00:48:01,493
the cost optimization techniques.

1111
00:48:01,493 --> 00:48:03,870
Customers are typically
interested in seeing

1112
00:48:03,870 --> 00:48:07,983
what traffic is going
from AZ to another AZ.

1113
00:48:09,720 --> 00:48:12,293
So that's something you can
also make, leverage here.

1114
00:48:15,450 --> 00:48:17,970
And something that's also
especially important in this case

1115
00:48:17,970 --> 00:48:20,580
and very relevant for this
particular session is the fact

1116
00:48:20,580 --> 00:48:22,710
that you're able to actually track

1117
00:48:22,710 --> 00:48:25,530
the traffic behavior within your cluster,

1118
00:48:25,530 --> 00:48:27,240
and you can now see, is this aligning

1119
00:48:27,240 --> 00:48:30,630
with the network security
policies that I've put in place?

1120
00:48:30,630 --> 00:48:32,850
This can now be the key thing that informs

1121
00:48:32,850 --> 00:48:35,310
how you define your network policies,

1122
00:48:35,310 --> 00:48:37,620
and is especially important
as you're shifting left

1123
00:48:37,620 --> 00:48:39,840
and you're doing this essentially
in the dev environment

1124
00:48:39,840 --> 00:48:41,348
instead of having to find out

1125
00:48:41,348 --> 00:48:44,433
in a production environment
when things are going wrong.

1126
00:48:46,110 --> 00:48:48,210
And what we have over here
is the historical explorer,

1127
00:48:48,210 --> 00:48:50,010
which essentially launchpads you

1128
00:48:50,010 --> 00:48:51,960
to the cloud wash network
monitoring dashboard

1129
00:48:51,960 --> 00:48:54,330
where you can see this in the
broader spectrum of things,

1130
00:48:54,330 --> 00:48:56,400
and it is especially
important to kind of see

1131
00:48:56,400 --> 00:48:58,680
that you now have that
consolidated transparency

1132
00:48:58,680 --> 00:49:00,480
between the Kubernetes network plane

1133
00:49:00,480 --> 00:49:03,870
as well as the VPC Network
plane as I pointed out earlier.

1134
00:49:03,870 --> 00:49:06,090
So you can see over there in
corresponding network path,

1135
00:49:06,090 --> 00:49:08,160
you can be able to
correlate a specific flow

1136
00:49:08,160 --> 00:49:10,860
that is taking place
within your EKS environment

1137
00:49:10,860 --> 00:49:12,540
or your Kubernetes network context

1138
00:49:12,540 --> 00:49:14,281
and see how that relates

1139
00:49:14,281 --> 00:49:15,960
to the underlying VPC constructs as well,

1140
00:49:15,960 --> 00:49:18,330
being able to see the
specific EC2 instance

1141
00:49:18,330 --> 00:49:21,303
and the ENI associated with
that flows particular path.

1142
00:49:23,160 --> 00:49:24,870
And in here and zoning in particularly

1143
00:49:24,870 --> 00:49:26,463
on the Istio ingress gateway.

1144
00:49:27,390 --> 00:49:29,130
And we can see the
correspondent network path,

1145
00:49:29,130 --> 00:49:30,540
and each one of those flow level metrics

1146
00:49:30,540 --> 00:49:31,830
are also visible here.

1147
00:49:31,830 --> 00:49:33,660
You'll be able to see the retransmissions,

1148
00:49:33,660 --> 00:49:36,570
the retransmission timeouts
as well as the volume

1149
00:49:36,570 --> 00:49:37,953
of data that was sent.

1150
00:49:40,440 --> 00:49:42,180
If I switch back and now
we can see the photo,

1151
00:49:42,180 --> 00:49:44,640
the photo gallery is now
generated in the service map.

1152
00:49:44,640 --> 00:49:46,200
And similar to before as I showed

1153
00:49:46,200 --> 00:49:47,640
with the e-commerce application as well,

1154
00:49:47,640 --> 00:49:49,260
we can see the API gateway is talking

1155
00:49:49,260 --> 00:49:52,410
to both the upload service as
well as the gallery service.

1156
00:49:52,410 --> 00:49:54,180
And similarly if we want
the more detailed view

1157
00:49:54,180 --> 00:49:57,600
where we're seeing flows between
the specific pod replicas,

1158
00:49:57,600 --> 00:50:00,783
we can have that represented
as well in the service map.

1159
00:50:02,850 --> 00:50:05,190
So all of this is in the tail
end of your investigation

1160
00:50:05,190 --> 00:50:07,240
but meant to help in accelerating things.

1161
00:50:12,420 --> 00:50:15,510
Now, a key thing that we also see a lot

1162
00:50:15,510 --> 00:50:17,430
of our customers doing is making use

1163
00:50:17,430 --> 00:50:19,080
of a service mesh implementation.

1164
00:50:20,460 --> 00:50:22,650
And the reason why we
wanted to highlight this

1165
00:50:22,650 --> 00:50:24,350
as well in this session
is we wanted to know,

1166
00:50:24,350 --> 00:50:25,890
we wanted our customers to know

1167
00:50:25,890 --> 00:50:28,560
that what we're introducing
container network observability

1168
00:50:28,560 --> 00:50:30,510
is complimentary as well,

1169
00:50:30,510 --> 00:50:31,980
and in many ways allows you

1170
00:50:31,980 --> 00:50:32,910
to be less reliant

1171
00:50:32,910 --> 00:50:35,160
on a service mesh implementation
for observability.

1172
00:50:35,160 --> 00:50:36,630
We've heard a lot of our customers say

1173
00:50:36,630 --> 00:50:38,610
that they would adopt a service mesh

1174
00:50:38,610 --> 00:50:41,010
primarily because of observability,

1175
00:50:41,010 --> 00:50:44,040
and now you no longer
need to use it primarily

1176
00:50:44,040 --> 00:50:46,080
for that, but rather you can leverage it

1177
00:50:46,080 --> 00:50:47,790
for other features as well.

1178
00:50:47,790 --> 00:50:49,200
And in addition to that, we found

1179
00:50:49,200 --> 00:50:52,410
that there would be
sometimes an inconsistency

1180
00:50:52,410 --> 00:50:54,750
'cause not every workload
may be running in the mesh.

1181
00:50:54,750 --> 00:50:56,040
And so you may have a certain level

1182
00:50:56,040 --> 00:50:58,770
of observability at the network
layer for certain workloads

1183
00:50:58,770 --> 00:51:00,960
that are in the mesh, but
any workload that is outside

1184
00:51:00,960 --> 00:51:03,960
of the mesh doesn't have the
same level of observability.

1185
00:51:03,960 --> 00:51:05,190
And now that can change

1186
00:51:05,190 --> 00:51:07,140
because you're using container
network observability

1187
00:51:07,140 --> 00:51:10,230
that will surface the sufficient
data whether an application

1188
00:51:10,230 --> 00:51:11,553
is in the mesh or not.

1189
00:51:12,990 --> 00:51:15,840
You can still use your service
mesh for advanced networking

1190
00:51:15,840 --> 00:51:17,280
for fine grain traffic control

1191
00:51:17,280 --> 00:51:19,260
and an advanced load balancing mechanism.

1192
00:51:19,260 --> 00:51:22,170
You can still use it for end
to end encryption like MTLS,

1193
00:51:22,170 --> 00:51:23,490
which is a common use case.

1194
00:51:23,490 --> 00:51:26,280
And as I already pointed out
for detailed observability,

1195
00:51:26,280 --> 00:51:27,780
there's some key metrics that you can get

1196
00:51:27,780 --> 00:51:30,180
from service mesh
implementations like Istio.

1197
00:51:30,180 --> 00:51:32,283
Envoy emits a lot of great statistics.

1198
00:51:34,350 --> 00:51:36,000
But it is also worth noting

1199
00:51:36,000 --> 00:51:37,860
that adopting a service
mesh implementation

1200
00:51:37,860 --> 00:51:41,640
does typically introduce some
operational overhead as well.

1201
00:51:41,640 --> 00:51:44,160
Managing a service mesh
at scale can be something

1202
00:51:44,160 --> 00:51:45,810
that's particularly challenging.

1203
00:51:45,810 --> 00:51:48,930
And so by introducing container
network observability,

1204
00:51:48,930 --> 00:51:51,150
how this helps our customers
is they no longer need

1205
00:51:51,150 --> 00:51:53,790
to adopt an entire service
mesh just for a single piece

1206
00:51:53,790 --> 00:51:56,460
of it, but rather they can
make better informed decisions

1207
00:51:56,460 --> 00:51:58,650
as well and seeing what
capabilities they can actually get

1208
00:51:58,650 --> 00:52:00,720
out of the mesh knowing
that they also have

1209
00:52:00,720 --> 00:52:03,540
additional native
capabilities provided by EKS

1210
00:52:03,540 --> 00:52:05,340
that can help them in their journey.

1211
00:52:09,480 --> 00:52:10,740
So we've seen the different layers

1212
00:52:10,740 --> 00:52:12,390
that we've been taking over here.

1213
00:52:13,260 --> 00:52:17,610
We started off by highlighting
how we were making it easier

1214
00:52:17,610 --> 00:52:20,490
for our customers at an operational
layer with EKS Auto Mode

1215
00:52:20,490 --> 00:52:22,470
and taking on that responsibility

1216
00:52:22,470 --> 00:52:24,660
of managing those core
networking components.

1217
00:52:24,660 --> 00:52:26,820
And then Rodrigo also highlighted

1218
00:52:26,820 --> 00:52:29,010
the network security
capabilities that can enhance

1219
00:52:29,010 --> 00:52:30,570
the security posture

1220
00:52:30,570 --> 00:52:32,620
of your environment at the network layer.

1221
00:52:33,600 --> 00:52:35,370
But we've also been trying
to drive home the point

1222
00:52:35,370 --> 00:52:37,830
of how this is especially difficult

1223
00:52:37,830 --> 00:52:40,050
without sufficient visibility.

1224
00:52:40,050 --> 00:52:42,420
And now with container
network observability,

1225
00:52:42,420 --> 00:52:43,710
you can make more informed decisions

1226
00:52:43,710 --> 00:52:45,903
and more accurate optimizations as well.

1227
00:52:48,750 --> 00:52:49,920
- How cool is that?

1228
00:52:49,920 --> 00:52:50,820
Right?

1229
00:52:50,820 --> 00:52:53,370
So now that we know what's going on

1230
00:52:53,370 --> 00:52:56,010
in our underlying network,

1231
00:52:56,010 --> 00:52:57,750
we can shift left security.

1232
00:52:57,750 --> 00:53:00,982
So using that service
map we can understand

1233
00:53:00,982 --> 00:53:03,480
what are the pod to pod communication?

1234
00:53:03,480 --> 00:53:05,490
What are the east, west,

1235
00:53:05,490 --> 00:53:07,860
or even north south com communication?

1236
00:53:07,860 --> 00:53:11,400
And we can start narrowing
down the access control.

1237
00:53:11,400 --> 00:53:14,340
So let's go back to that
cluster that was broken

1238
00:53:14,340 --> 00:53:16,410
with the applications not running,

1239
00:53:16,410 --> 00:53:19,620
and let's enable that default deny again.

1240
00:53:19,620 --> 00:53:22,680
But at this time we will create

1241
00:53:22,680 --> 00:53:23,760
the network policies

1242
00:53:23,760 --> 00:53:26,820
based on the rules that we observed

1243
00:53:26,820 --> 00:53:30,300
using the network
observability capabilities.

1244
00:53:30,300 --> 00:53:33,870
But then you can tell me,
Hey, Rodrigo, I can use GenAI

1245
00:53:33,870 --> 00:53:36,770
to create the network
policies for me, right?

1246
00:53:36,770 --> 00:53:39,930
I tried that and I have to use

1247
00:53:39,930 --> 00:53:41,850
the network observability feature

1248
00:53:41,850 --> 00:53:43,800
to really understand what was going on

1249
00:53:43,800 --> 00:53:47,970
because the network policies
that the model generated

1250
00:53:47,970 --> 00:53:49,770
to me didn't work.

1251
00:53:49,770 --> 00:53:52,140
The application was still broken.

1252
00:53:52,140 --> 00:53:54,330
So I, I was able to create
the network policies.

1253
00:53:54,330 --> 00:53:58,890
Now everything is is there,
you have the cluster running,

1254
00:53:58,890 --> 00:54:00,030
the application still running,

1255
00:54:00,030 --> 00:54:03,510
but we still have default allow policy.

1256
00:54:03,510 --> 00:54:05,220
Now we're going to change that.

1257
00:54:05,220 --> 00:54:07,320
So this is what we have done.

1258
00:54:07,320 --> 00:54:09,300
So you observed you can use that,

1259
00:54:09,300 --> 00:54:11,910
in a development
environment, you can track

1260
00:54:11,910 --> 00:54:14,100
and understand your networking map,

1261
00:54:14,100 --> 00:54:15,960
you can create your network policies

1262
00:54:15,960 --> 00:54:18,270
and then change to a default NI.

1263
00:54:18,270 --> 00:54:21,750
And these applications will
be ready to go to production

1264
00:54:21,750 --> 00:54:25,980
already, with network policies
already embedded on that.

1265
00:54:25,980 --> 00:54:28,230
So that's huge shift left security,

1266
00:54:28,230 --> 00:54:30,000
and is a shared responsibility

1267
00:54:30,000 --> 00:54:34,950
between both platform security
and application teams, right?

1268
00:54:34,950 --> 00:54:37,920
So now we have like the
default NI policy set

1269
00:54:37,920 --> 00:54:40,470
that we can have, we can see

1270
00:54:40,470 --> 00:54:44,760
that I have several network
policies running on the cluster,

1271
00:54:44,760 --> 00:54:47,940
each one with specific things.

1272
00:54:47,940 --> 00:54:50,250
And I have the application still running.

1273
00:54:50,250 --> 00:54:53,340
I can explore the catalog,
I can put things on my cart

1274
00:54:53,340 --> 00:54:55,953
and I can finish buying.

1275
00:54:58,140 --> 00:55:02,700
So this study from CNCF
demonstrates exactly that.

1276
00:55:02,700 --> 00:55:06,600
If we shift lab security,
the cost to fix a niche,

1277
00:55:06,600 --> 00:55:09,900
a security issue in a
development environment

1278
00:55:09,900 --> 00:55:14,370
versus a production
environment is 640 times more.

1279
00:55:14,370 --> 00:55:18,663
So that's why we should shift
lab security on this case.

1280
00:55:20,970 --> 00:55:23,070
So we covered all of those layers,

1281
00:55:23,070 --> 00:55:25,980
but then I was talking about
like application development

1282
00:55:25,980 --> 00:55:27,330
and shifting lab security

1283
00:55:27,330 --> 00:55:29,910
and putting everything
together with the application.

1284
00:55:29,910 --> 00:55:31,500
So there's more.

1285
00:55:31,500 --> 00:55:36,500
Yesterday we announced it the
managed capabilities for EKS.

1286
00:55:36,570 --> 00:55:38,190
So this is a brand new feature

1287
00:55:38,190 --> 00:55:41,640
that was just announced it
really yesterday, 4:00 p.m.

1288
00:55:41,640 --> 00:55:45,390
So this new capabilities
are managed capabilities

1289
00:55:45,390 --> 00:55:50,095
leveraging open source
tooling like Argo CD, KRO,

1290
00:55:50,095 --> 00:55:53,640
and ACK, where you can use EKS

1291
00:55:53,640 --> 00:55:56,820
to manage your entire platform

1292
00:55:56,820 --> 00:55:59,070
and application lifecycle stack.

1293
00:55:59,070 --> 00:56:02,580
So this will like be overall
increasing all the velocity

1294
00:56:02,580 --> 00:56:05,160
and time to market using all

1295
00:56:05,160 --> 00:56:09,720
of these managed features from
the baseline with DKS Auto

1296
00:56:09,720 --> 00:56:12,510
to network observability,
network policies,

1297
00:56:12,510 --> 00:56:14,043
and now manage capabilities.

1298
00:56:15,090 --> 00:56:17,540
So what are the key
takeaways for us today, Luke?

1299
00:56:19,080 --> 00:56:19,980
- Thanks, Rodrigo.

1300
00:56:21,150 --> 00:56:22,320
So as we pointed out

1301
00:56:22,320 --> 00:56:25,980
and have been emphasizing
throughout the session

1302
00:56:25,980 --> 00:56:28,080
is what we're seeing is our customers

1303
00:56:28,080 --> 00:56:31,410
are creating complex
environments that are increasing

1304
00:56:31,410 --> 00:56:32,970
in their distribution,

1305
00:56:32,970 --> 00:56:35,250
they're growing in their overall scope,

1306
00:56:35,250 --> 00:56:36,930
in their heterogeneity,

1307
00:56:36,930 --> 00:56:40,020
and the network is a core
fabric or a critical layer.

1308
00:56:40,020 --> 00:56:41,790
And in order to sufficiently optimize

1309
00:56:41,790 --> 00:56:43,530
according to whatever your standards are,

1310
00:56:43,530 --> 00:56:45,430
you need the right kind of visibility.

1311
00:56:46,530 --> 00:56:49,176
In addition to that, we've pointed out

1312
00:56:49,176 --> 00:56:50,009
how having the right level

1313
00:56:50,009 --> 00:56:53,259
of security in your network
environment is actually going

1314
00:56:53,259 --> 00:56:54,810
to enable you to move even faster.

1315
00:56:54,810 --> 00:56:55,643
So we hope you see this tire,

1316
00:56:55,643 --> 00:56:57,360
or this connection between them.

1317
00:56:57,360 --> 00:56:59,910
Informed decisions that
can help empower you

1318
00:56:59,910 --> 00:57:01,020
in making the right kind

1319
00:57:01,020 --> 00:57:04,740
of tactics for the security
policies that you're going

1320
00:57:04,740 --> 00:57:06,930
to be rolling out with
a lot more confidence,

1321
00:57:06,930 --> 00:57:09,530
knowing what the expected
behavior will actually be.

1322
00:57:11,580 --> 00:57:14,730
And as we've highlighted just
in the previous session with,

1323
00:57:14,730 --> 00:57:18,060
in the previous slide, sorry,
that Rodrigo just pointed out

1324
00:57:18,060 --> 00:57:20,580
with EKS capabilities, our commitment

1325
00:57:20,580 --> 00:57:25,230
is to increasingly simplify
operations for our customers.

1326
00:57:25,230 --> 00:57:27,480
And so taking that layered
approach, we can see now

1327
00:57:27,480 --> 00:57:31,050
how EKS capabilities
fits into that as well.

1328
00:57:31,050 --> 00:57:34,290
So we're very excited for you
to try out these new features,

1329
00:57:34,290 --> 00:57:36,240
container network observability and EKS,

1330
00:57:36,240 --> 00:57:37,073
and of course

1331
00:57:37,073 --> 00:57:40,080
what was just launched
yesterday, EKS capabilities.

1332
00:57:40,080 --> 00:57:42,660
Rodrigo and myself will hang
out a little bit after this.

1333
00:57:42,660 --> 00:57:43,860
We'd love to chat to any of you

1334
00:57:43,860 --> 00:57:45,750
that have particular questions,

1335
00:57:45,750 --> 00:57:46,920
and look forward to hearing

1336
00:57:46,920 --> 00:57:49,170
about how you use these
features in your environment.

1337
00:57:49,170 --> 00:57:50,856
Thanks a lot for coming for the session.

1338
00:57:50,856 --> 00:57:53,856
(audience clapping)

