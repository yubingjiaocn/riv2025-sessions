1
00:00:01,350 --> 00:00:03,660
- Right. Good afternoon folks.

2
00:00:03,660 --> 00:00:05,550
You're here hopefully to learn about

3
00:00:05,550 --> 00:00:09,240
how AMD Epic CPUs can help
you across a broad set

4
00:00:09,240 --> 00:00:12,513
of general compute workloads
as well as AI workloads.

5
00:00:13,860 --> 00:00:14,693
Quick bookkeeping.

6
00:00:14,693 --> 00:00:16,920
I'll talk really loudly
until you wear your headset.

7
00:00:16,920 --> 00:00:17,753
You're gonna have...

8
00:00:17,753 --> 00:00:19,650
You're gonna need your headset to hear me.

9
00:00:21,150 --> 00:00:23,700
Quick intro. I'm Madhu Rangarajan,

10
00:00:23,700 --> 00:00:26,490
I run the server product team at AMD,

11
00:00:26,490 --> 00:00:29,010
so all of the epic CPUs, product planning,

12
00:00:29,010 --> 00:00:30,900
product management, and product marketing.

13
00:00:30,900 --> 00:00:33,390
And I'll let Mike and Kyle
introduce themselves as well.

14
00:00:33,390 --> 00:00:34,980
- Hey everyone. Mike Thompson here.

15
00:00:34,980 --> 00:00:37,170
I am responsible for AMD's

16
00:00:37,170 --> 00:00:39,360
public cloud products out in the world

17
00:00:39,360 --> 00:00:41,970
across all the hyperscalers
based in North America

18
00:00:41,970 --> 00:00:43,533
and soon to be Neo clouds.

19
00:00:44,820 --> 00:00:46,380
- Good afternoon everyone.
I'm Kyle McLaughlin.

20
00:00:46,380 --> 00:00:48,600
I lead our FinOps practice at CVS Health

21
00:00:48,600 --> 00:00:50,550
and talk more later.

22
00:00:50,550 --> 00:00:52,470
- Okay, cool. Thanks everyone for coming.

23
00:00:52,470 --> 00:00:53,760
I'll see you in a sec. All right.

24
00:00:53,760 --> 00:00:56,340
- So I'm gonna cover the
first part and then Mike

25
00:00:56,340 --> 00:00:59,040
and Kyle are gonna get into
progressively more detail.

26
00:01:00,750 --> 00:01:02,460
If you look at AMD as a company

27
00:01:02,460 --> 00:01:04,350
and how we've evolved, right?

28
00:01:04,350 --> 00:01:07,590
We are looking at everything
as end-to-end solutions

29
00:01:07,590 --> 00:01:09,480
because we understand our customers

30
00:01:09,480 --> 00:01:12,030
aren't just looking
for a piece of silicon.

31
00:01:12,030 --> 00:01:15,810
We've got CPUs, we got
GPUs, we got networking,

32
00:01:15,810 --> 00:01:19,020
but more importantly, there
needs to be a software layer

33
00:01:19,020 --> 00:01:20,670
that ties it all together.

34
00:01:20,670 --> 00:01:22,740
And then there are solutions

35
00:01:22,740 --> 00:01:25,770
and cluster level designs that
now we have the ability to do

36
00:01:25,770 --> 00:01:27,990
through our acquisition of ZT systems.

37
00:01:27,990 --> 00:01:30,270
So we are really looking at at ourselves

38
00:01:30,270 --> 00:01:32,940
as an end-to-end solutions provider.

39
00:01:32,940 --> 00:01:36,483
I'll be diving a little more
into the CPU today. Yeah.

40
00:01:37,680 --> 00:01:41,910
Now jumping into the CPU,
if you look at the spectrum

41
00:01:41,910 --> 00:01:43,740
of workloads, right?

42
00:01:43,740 --> 00:01:48,180
You got general compute
and small AI workloads

43
00:01:48,180 --> 00:01:51,870
and AI pipeline workloads and
classical machine learning

44
00:01:51,870 --> 00:01:52,980
and things like that

45
00:01:52,980 --> 00:01:55,260
that have been traditionally
done in the CPU

46
00:01:55,260 --> 00:01:57,063
and continue to be done on the CPU.

47
00:01:57,900 --> 00:01:59,520
If you go all the way to the right,

48
00:01:59,520 --> 00:02:02,220
you got the AMD instinct GPUs.

49
00:02:02,220 --> 00:02:06,210
So if you're doing any AI
at scale, LLMs at scale,

50
00:02:06,210 --> 00:02:08,460
you're looking for real time inference,

51
00:02:08,460 --> 00:02:11,220
really large generative
AI models and so on.

52
00:02:11,220 --> 00:02:14,163
The instinct GPUs work
really well for that.

53
00:02:15,300 --> 00:02:17,670
And of course the CPU continues

54
00:02:17,670 --> 00:02:19,170
to be relevant throughout this

55
00:02:19,170 --> 00:02:21,180
because AI doesn't live in a vacuum.

56
00:02:21,180 --> 00:02:25,110
AI is a pipeline that
includes data input, cleaning,

57
00:02:25,110 --> 00:02:27,810
pre-processing model
training, and then deployment.

58
00:02:27,810 --> 00:02:31,110
So there's always this
system level interplay

59
00:02:31,110 --> 00:02:34,200
between the CPU and the
GPU and the the networking

60
00:02:34,200 --> 00:02:35,850
and clusters of all of these.

61
00:02:35,850 --> 00:02:36,683
Okay.

62
00:02:38,190 --> 00:02:40,800
Now jumping into inference a bit,

63
00:02:40,800 --> 00:02:42,570
if you look at what's been happening

64
00:02:42,570 --> 00:02:46,140
just over the last two
years, in 18 months,

65
00:02:46,140 --> 00:02:51,140
the cost of inference
has gone down 280x, okay?

66
00:02:51,170 --> 00:02:54,150
It was $20 per million tokens in 2022,

67
00:02:54,150 --> 00:02:56,670
and it's $0.07 in 2024.

68
00:02:56,670 --> 00:02:59,640
So this is like a new
version of Moore's Law

69
00:02:59,640 --> 00:03:03,330
where the cost per token is
going down exponentially.

70
00:03:03,330 --> 00:03:06,960
And what that means is if,
for those of you familiar

71
00:03:06,960 --> 00:03:10,080
with Jevons Paradox, Jevons Paradox says,

72
00:03:10,080 --> 00:03:15,080
when any resource becomes
low enough intern cost,

73
00:03:15,720 --> 00:03:17,310
there is a significantly higher

74
00:03:17,310 --> 00:03:19,020
consumption of that resource.

75
00:03:19,020 --> 00:03:21,090
And that's exactly what
we are saying with all

76
00:03:21,090 --> 00:03:23,550
of the insatiable desire for AI

77
00:03:23,550 --> 00:03:25,900
and all of the silicon
growth that's happening.

78
00:03:27,450 --> 00:03:28,800
And what does that mean?

79
00:03:28,800 --> 00:03:30,630
The growth of inference.

80
00:03:30,630 --> 00:03:32,820
As most of the big labs

81
00:03:32,820 --> 00:03:35,520
and all of the companies
were doing training,

82
00:03:35,520 --> 00:03:38,307
it was a little less "complicated,"

83
00:03:39,322 --> 00:03:42,060
where the CPO did some data preparation

84
00:03:42,060 --> 00:03:45,390
and then you had clusters of
GPUs doing all of the training

85
00:03:45,390 --> 00:03:47,730
and there wasn't as much interplay,

86
00:03:47,730 --> 00:03:49,740
although the CP was involved in it.

87
00:03:49,740 --> 00:03:52,020
But as you transition to a world

88
00:03:52,020 --> 00:03:55,533
where there's a lot more
inference happening than training,

89
00:03:56,700 --> 00:03:58,530
it's a much more complex workload.

90
00:03:58,530 --> 00:04:00,840
Now you've got a lot of pre-processing,

91
00:04:00,840 --> 00:04:03,420
you've got things like rag pipelines,

92
00:04:03,420 --> 00:04:05,430
and then you do the inference itself

93
00:04:05,430 --> 00:04:07,600
and then you have to do post-processing

94
00:04:08,730 --> 00:04:09,870
and there's an interplay

95
00:04:09,870 --> 00:04:13,455
where there can sometimes be
iterative loops going back

96
00:04:13,455 --> 00:04:15,360
and forth between the CPU and the GPU,

97
00:04:15,360 --> 00:04:18,900
between general purpose
tasks and these more AI

98
00:04:18,900 --> 00:04:21,180
or inference focused tasks.

99
00:04:21,180 --> 00:04:24,570
And agent AI takes that

100
00:04:24,570 --> 00:04:27,480
to an entirely different level, right?

101
00:04:27,480 --> 00:04:30,930
Previously you just had a bunch
of users accessing a bunch

102
00:04:30,930 --> 00:04:33,060
of compute of different kinds,

103
00:04:33,060 --> 00:04:35,760
whether it's GPUs or GPUs or networking.

104
00:04:35,760 --> 00:04:37,890
As agentic AI grows,

105
00:04:37,890 --> 00:04:40,530
what you have is a bunch of users

106
00:04:40,530 --> 00:04:42,670
deploying a whole bunch of agents

107
00:04:43,770 --> 00:04:45,090
and these agents never sleep.

108
00:04:45,090 --> 00:04:48,630
These agents are accessing
compute resources thousands

109
00:04:48,630 --> 00:04:50,760
of times more than human beings.

110
00:04:50,760 --> 00:04:54,510
And they're accessing all of
these web servers, databases,

111
00:04:54,510 --> 00:04:58,530
media, CDNs, then they're
running all of the...

112
00:04:58,530 --> 00:05:00,750
They're accessing AI accelerators.

113
00:05:00,750 --> 00:05:02,700
So this is what's driving

114
00:05:02,700 --> 00:05:05,550
that insatiable desire for compute.

115
00:05:05,550 --> 00:05:09,300
You're looking at all of these
giant data center buildouts,

116
00:05:09,300 --> 00:05:11,760
you're hearing about all of
the constraints on power.

117
00:05:11,760 --> 00:05:14,190
You're seeing all of these
huge data center buildouts

118
00:05:14,190 --> 00:05:15,870
that are happening.

119
00:05:15,870 --> 00:05:18,660
It's happening exactly because of this.

120
00:05:18,660 --> 00:05:21,510
And we don't see any signs of this abating

121
00:05:21,510 --> 00:05:24,570
because as these agents
get more and more useful,

122
00:05:24,570 --> 00:05:27,070
you're gonna see more and
more people deploy this.

123
00:05:29,250 --> 00:05:33,570
Now getting further onto Epic
CPU side of things in general,

124
00:05:33,570 --> 00:05:34,890
if you look at the Epic CPUs

125
00:05:34,890 --> 00:05:36,750
and what we've been
delivering consistently

126
00:05:36,750 --> 00:05:38,970
for a few generations now,

127
00:05:38,970 --> 00:05:42,723
extremely high performance
across a broad set of workloads,

128
00:05:44,010 --> 00:05:45,120
very power efficient,

129
00:05:45,120 --> 00:05:48,270
one of the most efficient CPUs out there,

130
00:05:48,270 --> 00:05:50,850
and TCO, total cost of ownership,

131
00:05:50,850 --> 00:05:53,880
which we deliver through a
combination of performance

132
00:05:53,880 --> 00:05:56,313
and core count as well as efficiency.

133
00:05:58,650 --> 00:06:00,780
And if you look at the evolution

134
00:06:00,780 --> 00:06:03,093
of the AMD Epic CPU roadmap,

135
00:06:04,080 --> 00:06:07,920
we started with the Zen
one servers in 2017.

136
00:06:07,920 --> 00:06:11,400
And when the Zen one
servers came out in 2017,

137
00:06:11,400 --> 00:06:13,740
we probably had low
single digit percentage

138
00:06:13,740 --> 00:06:15,750
in terms of market share.

139
00:06:15,750 --> 00:06:18,993
Now going all the way to
the Zen five servers on SP5,

140
00:06:19,980 --> 00:06:24,720
the product we call Turin, we
are now at 41% market share

141
00:06:24,720 --> 00:06:28,740
and Zen six or Venice is
coming very soon next year.

142
00:06:28,740 --> 00:06:31,650
And we expect the trajectory of growth

143
00:06:31,650 --> 00:06:34,200
and market share growth at that AMD has

144
00:06:34,200 --> 00:06:36,063
to continue for a while.

145
00:06:39,870 --> 00:06:41,400
Now, this is a quick snapshot

146
00:06:41,400 --> 00:06:43,680
of our fifth generation Epic or Turin,

147
00:06:43,680 --> 00:06:45,423
which we launched late last year.

148
00:06:46,710 --> 00:06:48,180
At the time of its launch,

149
00:06:48,180 --> 00:06:50,850
we again delivered significant performance

150
00:06:50,850 --> 00:06:52,290
over competition, right?

151
00:06:52,290 --> 00:06:53,700
If you look at all of these metrics,

152
00:06:53,700 --> 00:06:56,430
whether it's just
benchmarks like spec CPU,

153
00:06:56,430 --> 00:06:59,310
or you're looking at a broad
set of enterprise workloads

154
00:06:59,310 --> 00:07:01,920
or HPC workloads like two, three,

155
00:07:01,920 --> 00:07:04,560
and 4x more performance than
anything competition has

156
00:07:04,560 --> 00:07:08,043
to offer even on CPU based AI workloads,

157
00:07:08,880 --> 00:07:10,830
just a core count and the memory bandwidth

158
00:07:10,830 --> 00:07:13,890
and everything else we had
to offer gives you up to 3.8x

159
00:07:13,890 --> 00:07:16,530
on various AI workloads.

160
00:07:16,530 --> 00:07:19,770
And now, even if you are using a GPU,

161
00:07:19,770 --> 00:07:23,100
it is actually extremely
important to pick the right CPU.

162
00:07:23,100 --> 00:07:26,320
So we created a part
called the Epic 9575F,

163
00:07:26,320 --> 00:07:29,220
which a high frequency processor,

164
00:07:29,220 --> 00:07:32,580
and the main job of the processor
is to get out of the way

165
00:07:32,580 --> 00:07:35,130
of the GPU so that the
GPU can process things

166
00:07:35,130 --> 00:07:36,930
and not bottleneck the GPU.

167
00:07:36,930 --> 00:07:41,520
And what that translates
into is on a GPU server,

168
00:07:41,520 --> 00:07:44,040
20% more performance out of the GPU just

169
00:07:44,040 --> 00:07:46,380
by minimizing the time spent waiting

170
00:07:46,380 --> 00:07:48,270
for the CPU to do stuff.

171
00:07:48,270 --> 00:07:51,024
If you look at that in the
context of, I dunno, 300,

172
00:07:51,024 --> 00:07:55,650
$400,000 GPU server, that's
tens of thousands of dollars

173
00:07:55,650 --> 00:07:58,140
of performance per dollar
that it translates into.

174
00:07:58,140 --> 00:08:00,840
So whether you're running
your workloads on a CPU

175
00:08:00,840 --> 00:08:02,340
or a GPU,

176
00:08:02,340 --> 00:08:05,190
the CPU is kind of ubiquitous there

177
00:08:05,190 --> 00:08:06,900
and it's extremely important to make sure

178
00:08:06,900 --> 00:08:09,450
that you pick the right
CPU to run those workloads.

179
00:08:12,030 --> 00:08:14,760
And again, if you're running
AI inference itself on the CPU,

180
00:08:14,760 --> 00:08:18,720
and Mike will get into maybe
more details on this broad set

181
00:08:18,720 --> 00:08:22,230
of AI inference workloads
run really well on Epic.

182
00:08:22,230 --> 00:08:24,390
For example, if you look
at machine learning,

183
00:08:24,390 --> 00:08:28,380
decision tree, sub flow support
vector machines and so on,

184
00:08:28,380 --> 00:08:32,250
those workloads run use high
precision vector engines.

185
00:08:32,250 --> 00:08:34,527
They do really well on the CPU.

186
00:08:34,527 --> 00:08:36,510
When you have LLMs and generative AI,

187
00:08:36,510 --> 00:08:37,950
especially if you have a small

188
00:08:37,950 --> 00:08:39,810
and smaller language models,

189
00:08:39,810 --> 00:08:42,270
small scale deployments
where you have, for example,

190
00:08:42,270 --> 00:08:44,760
a chat bot that's not busy all the time.

191
00:08:44,760 --> 00:08:46,290
The CPU actually does really well

192
00:08:46,290 --> 00:08:49,230
in the large memory capacity
that a CPU has to offer,

193
00:08:49,230 --> 00:08:50,790
is very helpful there.

194
00:08:50,790 --> 00:08:54,420
And then just recommendation
systems as well as AI apps

195
00:08:54,420 --> 00:08:57,060
where you have a mix
of like general compute

196
00:08:57,060 --> 00:09:00,270
with AI embedded in that overall pipeline.

197
00:09:00,270 --> 00:09:03,123
CPUs tend to be really
good at those workloads.

198
00:09:05,100 --> 00:09:07,710
And of course that's the hardware piece

199
00:09:07,710 --> 00:09:10,620
and it's extremely important
for our customers to be able

200
00:09:10,620 --> 00:09:14,100
to consume this easily
in terms of software.

201
00:09:14,100 --> 00:09:17,400
So what we are, AMD is
extremely focused on making sure

202
00:09:17,400 --> 00:09:19,920
the open source ecosystem is enabled.

203
00:09:19,920 --> 00:09:23,010
So if you look at how we are
gonna make sure you are able

204
00:09:23,010 --> 00:09:25,710
to consume the performance easily,

205
00:09:25,710 --> 00:09:28,470
we are enabling the standard
frameworks, PyTorch,

206
00:09:28,470 --> 00:09:31,140
TensorFlow and Onyx.

207
00:09:31,140 --> 00:09:34,740
Now, all of these will
work right out of the box,

208
00:09:34,740 --> 00:09:37,710
and in general, they
actually do pretty well.

209
00:09:37,710 --> 00:09:39,870
Now, if you wanna kick your
performance up a notch,

210
00:09:39,870 --> 00:09:42,180
we got this plugin called ZenDNN,

211
00:09:42,180 --> 00:09:45,210
they plug into these
frameworks pretty seamlessly

212
00:09:45,210 --> 00:09:47,643
and you can get more
performance out of it.

213
00:09:48,840 --> 00:09:50,340
It's not required,

214
00:09:50,340 --> 00:09:53,220
but it can help tune
your performance further.

215
00:09:53,220 --> 00:09:56,130
And we are actually working
through upstreaming a lot

216
00:09:56,130 --> 00:09:58,620
of those optimization directly
into those frameworks.

217
00:09:58,620 --> 00:10:01,470
So you at some point you don't
even have to use a plugin.

218
00:10:04,260 --> 00:10:06,030
And again, just to sum it up

219
00:10:06,030 --> 00:10:07,503
before I hand over to Mike,

220
00:10:08,490 --> 00:10:11,370
why use Epic CPUs for AI workloads?

221
00:10:11,370 --> 00:10:13,950
The first is accessibility.
CPUs are omnipresent.

222
00:10:13,950 --> 00:10:16,620
You can find it across
regions, across data center,

223
00:10:16,620 --> 00:10:18,450
across clouds, whether you're on-prem

224
00:10:18,450 --> 00:10:20,220
or in the cloud, it's ubiquitous.

225
00:10:20,220 --> 00:10:23,220
And you got this standard
X86 software stack

226
00:10:23,220 --> 00:10:24,750
that you can leverage across

227
00:10:24,750 --> 00:10:27,510
and without any porting you can get.

228
00:10:27,510 --> 00:10:29,490
You can run your workload.

229
00:10:29,490 --> 00:10:31,200
Utilization opportunities.

230
00:10:31,200 --> 00:10:34,380
A lot of customers have servers in either

231
00:10:34,380 --> 00:10:36,390
in their data centers where
they're sitting at 20,

232
00:10:36,390 --> 00:10:38,250
30% utilization

233
00:10:38,250 --> 00:10:41,070
or they've bought a whole
bunch of reserved instances.

234
00:10:41,070 --> 00:10:44,670
And those reserved instances
are sitting idle a lot.

235
00:10:44,670 --> 00:10:47,430
So you can actually do batch processing

236
00:10:47,430 --> 00:10:50,490
of AI workloads on the downtimes

237
00:10:50,490 --> 00:10:52,170
and increase your utilization

238
00:10:52,170 --> 00:10:53,520
and make more effective use

239
00:10:53,520 --> 00:10:55,520
of the hardware that you've already got.

240
00:10:56,430 --> 00:10:57,960
Third one is scalability,

241
00:10:57,960 --> 00:11:00,150
especially when you're in the cloud.

242
00:11:00,150 --> 00:11:02,100
You can have a baseline amount of capacity

243
00:11:02,100 --> 00:11:04,300
and you can scale your
capacity up and down.

244
00:11:05,384 --> 00:11:06,510
That's the value prop of cloud.

245
00:11:06,510 --> 00:11:08,010
That's why all of you're here.

246
00:11:08,010 --> 00:11:11,310
And the last one of course is

247
00:11:11,310 --> 00:11:13,890
if you want operational simplicity

248
00:11:13,890 --> 00:11:16,860
where your AI workloads are
a little more small scale

249
00:11:16,860 --> 00:11:19,350
and you want to have a
relatively uniform fleet

250
00:11:19,350 --> 00:11:21,960
without having to manage
multiple types of infrastructure

251
00:11:21,960 --> 00:11:23,730
and figure out how many of each type

252
00:11:23,730 --> 00:11:26,430
of infrastructure you
wanna deploy and so on.

253
00:11:26,430 --> 00:11:28,320
With the CPU, you get a unified platform,

254
00:11:28,320 --> 00:11:30,390
you can get a single
type of cloud instance

255
00:11:30,390 --> 00:11:32,793
to support a broad set of workloads.

256
00:11:34,440 --> 00:11:36,420
And with that, I'm
gonna hand over to Mike,

257
00:11:36,420 --> 00:11:38,970
who's gonna kind of get you go
get you through more details.

258
00:11:38,970 --> 00:11:42,360
- Awesome, thank you Madhu. Hey everyone.

259
00:11:42,360 --> 00:11:47,360
Mike here again, I manage
AMD's global cloud product,

260
00:11:47,400 --> 00:11:49,230
and I'm here to talk
to you today about some

261
00:11:49,230 --> 00:11:51,750
of the cost efficiencies
that you can extract

262
00:11:51,750 --> 00:11:54,180
by just making the simple choice

263
00:11:54,180 --> 00:11:58,560
of selecting your processor,
your compute platform wisely.

264
00:11:58,560 --> 00:12:02,310
And I'll also go into some
details about when does it

265
00:12:02,310 --> 00:12:05,850
actually make sense to
do inference on CPU.

266
00:12:05,850 --> 00:12:07,827
A lot of people think AI is GPU,

267
00:12:07,827 --> 00:12:09,510
and to some extent that is true,

268
00:12:09,510 --> 00:12:12,990
but there's, we've been
doing AI on CPU since 1950,

269
00:12:12,990 --> 00:12:14,790
so 75 years.

270
00:12:14,790 --> 00:12:18,120
And especially as you know,
we finish up this huge era

271
00:12:18,120 --> 00:12:20,550
of training, or at least
we've done enough training

272
00:12:20,550 --> 00:12:23,040
to be able to start
actualizing those models,

273
00:12:23,040 --> 00:12:25,530
making them available to the broad public.

274
00:12:25,530 --> 00:12:28,470
You should really consider
carefully where you deploy those.

275
00:12:28,470 --> 00:12:30,090
There's some pretty significant benefits

276
00:12:30,090 --> 00:12:33,180
that you can get from an
availability and cost perspective.

277
00:12:33,180 --> 00:12:35,160
Now, to take a step back,

278
00:12:35,160 --> 00:12:36,840
choosing your compute platform

279
00:12:36,840 --> 00:12:38,700
really does matter.

280
00:12:38,700 --> 00:12:41,190
Historically, a lot of
folks have thought about

281
00:12:41,190 --> 00:12:42,360
their compute platforms,

282
00:12:42,360 --> 00:12:46,320
the CPUs that they're renting
in the cloud as a commodity

283
00:12:46,320 --> 00:12:50,640
and may overlook some of the benefits

284
00:12:50,640 --> 00:12:52,740
that you can get from performance.

285
00:12:52,740 --> 00:12:55,890
A little bit later today we'll
have Kyle from CVS on stage

286
00:12:55,890 --> 00:12:57,180
and he'll talk a little bit about

287
00:12:57,180 --> 00:12:59,850
how they were able to get an OPEX return

288
00:12:59,850 --> 00:13:02,430
by choosing the compute platforms wisely.

289
00:13:02,430 --> 00:13:04,350
This is an example,

290
00:13:04,350 --> 00:13:09,350
AMD Epic CPUs are have a really,
really good architecture.

291
00:13:09,420 --> 00:13:12,060
They have really good
implementations in AWS

292
00:13:12,060 --> 00:13:15,840
that enable them to have
incredible performance up

293
00:13:15,840 --> 00:13:17,070
to 2X performance.

294
00:13:17,070 --> 00:13:19,560
And that comparison is comparing Ice Lake

295
00:13:19,560 --> 00:13:23,370
or a 6I to a Genoa or a 6A.

296
00:13:23,370 --> 00:13:25,020
So one generation difference,

297
00:13:25,020 --> 00:13:27,510
you can get a 2X performance uplift.

298
00:13:27,510 --> 00:13:29,730
If you're just choosing
your compute platforms

299
00:13:29,730 --> 00:13:32,010
by whatever's ever been done before,

300
00:13:32,010 --> 00:13:34,550
you could be leaving an
awful lot of your OPEX

301
00:13:34,550 --> 00:13:36,210
or your TCO on the table.

302
00:13:36,210 --> 00:13:39,330
So with that 2X performance,

303
00:13:39,330 --> 00:13:42,900
you can drive almost 50% lower cost

304
00:13:42,900 --> 00:13:44,670
for the jobs that you're running.

305
00:13:44,670 --> 00:13:46,260
The jobs that are most suited for that

306
00:13:46,260 --> 00:13:48,840
are ephemeral jobs or containerized jobs

307
00:13:48,840 --> 00:13:50,160
or batch jobs

308
00:13:50,160 --> 00:13:53,640
where you spin up the servers
when they're ready to be used

309
00:13:53,640 --> 00:13:56,460
and then you spin them down
when they're done through.

310
00:13:56,460 --> 00:13:57,390
That mechanism,

311
00:13:57,390 --> 00:14:00,810
what we call performance
driven cost optimization.

312
00:14:00,810 --> 00:14:04,440
Literally with four clicks of
your mouse in the AWS console,

313
00:14:04,440 --> 00:14:07,740
stop your instance, hit the
instance select or drop down,

314
00:14:07,740 --> 00:14:10,290
select an instance with
an A in it for AMD,

315
00:14:10,290 --> 00:14:12,960
you'll see the little lowercase A for EC2

316
00:14:12,960 --> 00:14:14,730
and start your instance again.

317
00:14:14,730 --> 00:14:19,710
You can drive your your
burn rate, 45 down 45%.

318
00:14:19,710 --> 00:14:21,060
And so that's the impact

319
00:14:21,060 --> 00:14:23,640
of performance driven optimizations.

320
00:14:23,640 --> 00:14:26,640
And then do we have anyone
here from Europe today?

321
00:14:26,640 --> 00:14:29,670
Anyone from AMEA? Couple?

322
00:14:29,670 --> 00:14:34,110
Okay, so what I've observed is the concern

323
00:14:34,110 --> 00:14:38,310
with energy consumption in
Europe is pretty significant.

324
00:14:38,310 --> 00:14:40,437
There's some existential challenges there.

325
00:14:40,437 --> 00:14:42,240
And so I see a lot

326
00:14:42,240 --> 00:14:46,500
of the decisions about deploying
infrastructure in Europe.

327
00:14:46,500 --> 00:14:50,040
One of the primary concerns is
actually energy consumption.

328
00:14:50,040 --> 00:14:52,920
And so by the same token,

329
00:14:52,920 --> 00:14:55,590
by choosing a MD powered instances,

330
00:14:55,590 --> 00:14:57,150
whether you're doing them on-prem

331
00:14:57,150 --> 00:14:58,170
or in the cloud,

332
00:14:58,170 --> 00:15:01,440
you get up to 2.2 x better
performance per watt.

333
00:15:01,440 --> 00:15:02,940
So every watt you burn,

334
00:15:02,940 --> 00:15:05,943
you get more than double
the work out of it.

335
00:15:07,230 --> 00:15:09,450
Sometimes those numbers
can be a little bit hard

336
00:15:09,450 --> 00:15:12,720
to tease out of the hyperscalers,

337
00:15:12,720 --> 00:15:14,130
but you get both of those benefits,

338
00:15:14,130 --> 00:15:16,530
you get performance
and hence a lower cost,

339
00:15:16,530 --> 00:15:19,980
but you also consume a lot
less energy while you're at it.

340
00:15:19,980 --> 00:15:21,480
If anyone is interested in diving

341
00:15:21,480 --> 00:15:24,450
into more details about that,
please stop by afterwards.

342
00:15:24,450 --> 00:15:26,550
We'll be out here to
chat for a little while

343
00:15:26,550 --> 00:15:30,153
or swing by the AMD
booth on the show floor.

344
00:15:31,950 --> 00:15:36,420
All right, so A MD CPUs can really help

345
00:15:36,420 --> 00:15:40,710
address a couple of the key AI challenges.

346
00:15:40,710 --> 00:15:44,070
One, when most customers

347
00:15:44,070 --> 00:15:45,540
or most organizations,

348
00:15:45,540 --> 00:15:47,700
they set their budgets at
the beginning of the year,

349
00:15:47,700 --> 00:15:49,980
whether it's calendar year or fiscal year.

350
00:15:49,980 --> 00:15:51,240
You get your budget,

351
00:15:51,240 --> 00:15:54,630
you allocate it to whatever
applications or business units

352
00:15:54,630 --> 00:15:56,130
and then you go and execute.

353
00:15:56,130 --> 00:15:59,040
Well, there's so much innovation
in AI happening right now

354
00:15:59,040 --> 00:16:01,110
that you can't anticipate

355
00:16:01,110 --> 00:16:02,640
what you're gonna need in August

356
00:16:02,640 --> 00:16:05,010
when you set your budget in January.

357
00:16:05,010 --> 00:16:06,840
And so one of the ways that

358
00:16:06,840 --> 00:16:08,880
we've seen customers take advantage

359
00:16:08,880 --> 00:16:11,730
of these performance driven
cost optimizations is

360
00:16:11,730 --> 00:16:15,570
to go into their maybe just
their general IT infrastructure,

361
00:16:15,570 --> 00:16:19,050
do that four click
transformation to drive roughly,

362
00:16:19,050 --> 00:16:21,900
we see 27 to 45% cost savings

363
00:16:21,900 --> 00:16:23,820
that can come from just simply switching

364
00:16:23,820 --> 00:16:25,680
to AMD powered instances.

365
00:16:25,680 --> 00:16:27,567
They free up that budget
and they can take that

366
00:16:27,567 --> 00:16:29,250
and either invest it in.

367
00:16:29,250 --> 00:16:31,620
A lot of times it's innovation.

368
00:16:31,620 --> 00:16:34,830
So AI applications or advanced analytics,

369
00:16:34,830 --> 00:16:36,420
but also for business growth.

370
00:16:36,420 --> 00:16:39,330
You know, if customers
are having, you know,

371
00:16:39,330 --> 00:16:40,800
significant amount of business growth,

372
00:16:40,800 --> 00:16:42,870
they need more capital to
deploy, more infrastructure,

373
00:16:42,870 --> 00:16:44,880
to drive more revenue as
you're growing, right?

374
00:16:44,880 --> 00:16:46,890
And so that's kind of a secret way

375
00:16:46,890 --> 00:16:48,690
that you can unlock
budget that you might not

376
00:16:48,690 --> 00:16:50,610
otherwise think you have.

377
00:16:50,610 --> 00:16:52,560
Those efficiencies in the infrastructure

378
00:16:52,560 --> 00:16:54,120
they already have deployed.

379
00:16:54,120 --> 00:16:59,120
So higher performance
means faster runtime,

380
00:16:59,580 --> 00:17:03,630
smaller footprint, or
smaller instance size.

381
00:17:03,630 --> 00:17:05,730
All of those can drive lower OPEX,

382
00:17:05,730 --> 00:17:07,830
especially for instance size.

383
00:17:07,830 --> 00:17:10,650
Like for instance, if
you get 2X performance

384
00:17:10,650 --> 00:17:13,860
and you're running SQL server,
which is licensed per core,

385
00:17:13,860 --> 00:17:17,010
and those software, those
license software applications,

386
00:17:17,010 --> 00:17:19,740
that license cost tends
to dominate your TCO,

387
00:17:19,740 --> 00:17:22,470
it's usually 80 to 90, often 80 to 90%

388
00:17:22,470 --> 00:17:25,500
of your TCO is wrapped up
in those license costs.

389
00:17:25,500 --> 00:17:28,620
And so if you take advantage
of that 2X performance,

390
00:17:28,620 --> 00:17:30,930
that means you can cut
your instant size in half,

391
00:17:30,930 --> 00:17:34,620
that means your number of cores
or VCPUs goes down by half

392
00:17:34,620 --> 00:17:37,140
and that's what your software
is often licensed on.

393
00:17:37,140 --> 00:17:40,050
So you can dramatically
drive down your TCO

394
00:17:40,050 --> 00:17:41,880
including the licensing cost.

395
00:17:41,880 --> 00:17:43,800
And in that case, the infrastructure

396
00:17:43,800 --> 00:17:45,270
is a small portion of it.

397
00:17:45,270 --> 00:17:48,810
You roughly can expect
about a 45% TCO return

398
00:17:48,810 --> 00:17:52,470
when you're doing that
based on SQL server packaged

399
00:17:52,470 --> 00:17:56,670
with the instance from AWS 45% cost return

400
00:17:56,670 --> 00:17:59,550
for those software applications.

401
00:17:59,550 --> 00:18:01,530
And then finally, since
we're mostly here today

402
00:18:01,530 --> 00:18:04,920
to talk about inference or AI,

403
00:18:04,920 --> 00:18:07,290
Inference on CPU is very cost effective.

404
00:18:07,290 --> 00:18:10,353
I'll show you some ways
that that is enabled today.

405
00:18:11,670 --> 00:18:13,740
CPU instances are highly available.

406
00:18:13,740 --> 00:18:16,800
They tend to be more broadly
geographically available

407
00:18:16,800 --> 00:18:18,840
than GPU instances.

408
00:18:18,840 --> 00:18:20,610
There tends to be more pool depth,

409
00:18:20,610 --> 00:18:22,050
they're highly portable as well.

410
00:18:22,050 --> 00:18:24,420
Let's say your AI workload shuts down,

411
00:18:24,420 --> 00:18:27,030
you can put any general
purpose workload on a CPU,

412
00:18:27,030 --> 00:18:29,310
can't always do that with
GPUs, don't get me wrong.

413
00:18:29,310 --> 00:18:32,430
GPUs are great, also very
important, we need those,

414
00:18:32,430 --> 00:18:34,720
but CPUs tend to be more portable

415
00:18:36,270 --> 00:18:38,700
and you can also repurpose those instances

416
00:18:38,700 --> 00:18:41,073
for other applications.

417
00:18:42,900 --> 00:18:45,180
You know, one of the
things, talking with dozens

418
00:18:45,180 --> 00:18:47,490
and dozens of customers,
one of the challenges

419
00:18:47,490 --> 00:18:50,340
that customers face in terms
of getting the most out

420
00:18:50,340 --> 00:18:52,050
of their investment in infrastructure,

421
00:18:52,050 --> 00:18:54,690
and this is honestly on-prem and cloud,

422
00:18:54,690 --> 00:18:56,640
but here what we're looking at is

423
00:18:56,640 --> 00:18:59,340
what is the typical CPU utilization

424
00:18:59,340 --> 00:19:01,470
over the course of 24 hours?

425
00:19:01,470 --> 00:19:04,380
You'll notice from midnight
till early in the morning,

426
00:19:04,380 --> 00:19:06,330
a lot of the compute
isn't getting used at all.

427
00:19:06,330 --> 00:19:08,520
And then it peaks when
everyone comes to work,

428
00:19:08,520 --> 00:19:10,380
then it drops off around dinner time,

429
00:19:10,380 --> 00:19:12,900
then after dinner time you
can see another little peak

430
00:19:12,900 --> 00:19:14,490
and then it drops off again.

431
00:19:14,490 --> 00:19:17,340
So that's kind of
dramatically underutilized

432
00:19:17,340 --> 00:19:20,340
compute resource,
especially if you've paid

433
00:19:20,340 --> 00:19:24,450
for like a long term
fixed price contracts,

434
00:19:24,450 --> 00:19:26,970
you're literally burning
money doing nothing.

435
00:19:26,970 --> 00:19:30,270
And so one of the things that
you can do for AI applications

436
00:19:30,270 --> 00:19:31,740
that aren't necessarily real time,

437
00:19:31,740 --> 00:19:34,200
there's a lot of analytics
associated with it.

438
00:19:34,200 --> 00:19:36,690
Maybe like let's say you
run a streaming service

439
00:19:36,690 --> 00:19:39,960
and you want to come up with
new movie titles to recommend

440
00:19:39,960 --> 00:19:41,820
to your users tomorrow.

441
00:19:41,820 --> 00:19:44,670
Well, you can insert those AI workloads

442
00:19:44,670 --> 00:19:47,400
during these non-peak hours.

443
00:19:47,400 --> 00:19:50,670
If you're doing inference
that is not latency sensitive,

444
00:19:50,670 --> 00:19:52,437
that's really, really well suited to CPU

445
00:19:52,437 --> 00:19:55,230
and it enables you to get
more out of your investments.

446
00:19:55,230 --> 00:19:57,570
So you can deliver
insight during off hours.

447
00:19:57,570 --> 00:19:59,670
You can maximize the value

448
00:19:59,670 --> 00:20:03,990
of your reserved instance
fixed price contracts.

449
00:20:03,990 --> 00:20:06,720
And I'll show you a little
bit more in a moment.

450
00:20:06,720 --> 00:20:08,880
You can leverage the really,
really good performance

451
00:20:08,880 --> 00:20:10,200
of AMD Epic CPUs.

452
00:20:10,200 --> 00:20:11,220
Again, look for the A

453
00:20:11,220 --> 00:20:16,220
in the instance name to run
these inference applications.

454
00:20:17,040 --> 00:20:20,220
Now, here are some examples
of why you might want

455
00:20:20,220 --> 00:20:22,470
to do inference on CPU.

456
00:20:22,470 --> 00:20:25,065
First, I'm showing one

457
00:20:25,065 --> 00:20:26,850
of the leading GPU instances

458
00:20:26,850 --> 00:20:30,690
for doing inference
versus an M7A instances.

459
00:20:30,690 --> 00:20:33,300
This is from our Genoa Epic processor,

460
00:20:33,300 --> 00:20:36,960
7A, M7A versus G6E.

461
00:20:36,960 --> 00:20:41,940
You can see that the CPU instance in blue

462
00:20:41,940 --> 00:20:45,090
is about one third the
cost of the GPU instance.

463
00:20:45,090 --> 00:20:47,970
Consider with that utilization
curve we just looked at,

464
00:20:47,970 --> 00:20:50,640
why would you wanna work
burn three times the rate

465
00:20:50,640 --> 00:20:52,800
when you're only using about half of it?

466
00:20:52,800 --> 00:20:55,170
So there's many applications for inference

467
00:20:55,170 --> 00:20:57,960
where using CPUs makes a lot of sense.

468
00:20:57,960 --> 00:20:59,460
If you're not latency sensitive,

469
00:20:59,460 --> 00:21:02,100
it makes a lot of sense if you
have small and medium models,

470
00:21:02,100 --> 00:21:03,547
it makes a lot of sense.

471
00:21:03,547 --> 00:21:06,990
Has anybody in the audience had like

472
00:21:06,990 --> 00:21:11,250
a meeting summary agent join
any of your calls afterwards?

473
00:21:11,250 --> 00:21:12,870
Five minutes after your meeting ends,

474
00:21:12,870 --> 00:21:14,310
it sends you a summary.

475
00:21:14,310 --> 00:21:16,830
What about, has anyone had,
what do they call that,

476
00:21:16,830 --> 00:21:19,290
the transcript where it just
does the voice recognition

477
00:21:19,290 --> 00:21:20,970
and sends you the transcript?

478
00:21:20,970 --> 00:21:25,320
Does anybody read the
transcripts in full? Nobody does.

479
00:21:25,320 --> 00:21:28,500
Does anybody read the summary? I do.

480
00:21:28,500 --> 00:21:30,300
I read the transcripts once about a year

481
00:21:30,300 --> 00:21:31,470
and a half ago I think,

482
00:21:31,470 --> 00:21:33,270
and then when the summary
agents came around.

483
00:21:33,270 --> 00:21:36,960
So now those summary agents,
do you really care whether

484
00:21:36,960 --> 00:21:39,570
that summary turns up in
your inbox immediately

485
00:21:39,570 --> 00:21:40,860
as you exit the meeting?

486
00:21:40,860 --> 00:21:43,890
Or if it shows up five
minutes later, is that fine?

487
00:21:43,890 --> 00:21:45,180
It's fine, right?

488
00:21:45,180 --> 00:21:47,640
And so that's an example of a case

489
00:21:47,640 --> 00:21:49,830
where CPU inference makes a lot of sense.

490
00:21:49,830 --> 00:21:53,220
You do it at a 66% discount,

491
00:21:53,220 --> 00:21:55,290
it's much more highly available

492
00:21:55,290 --> 00:21:57,360
and more geographically distributed.

493
00:21:57,360 --> 00:21:59,580
So that's one of the
reasons you might consider

494
00:21:59,580 --> 00:22:01,503
inference on CPU.

495
00:22:02,340 --> 00:22:04,410
And so now I'm gonna
start diving into some

496
00:22:04,410 --> 00:22:05,490
of the technical details.

497
00:22:05,490 --> 00:22:07,680
We've run loads and loads of benchmarks

498
00:22:07,680 --> 00:22:09,390
for inference workloads.

499
00:22:09,390 --> 00:22:11,070
I'm just selecting one of them here.

500
00:22:11,070 --> 00:22:12,900
Here we're looking at a model

501
00:22:12,900 --> 00:22:17,040
that does natural language
processing, like something that,

502
00:22:17,040 --> 00:22:20,100
a model that can deliver
customer service agent,

503
00:22:20,100 --> 00:22:21,753
customer service agents,

504
00:22:22,710 --> 00:22:27,060
or interact with you to
understand, you know,

505
00:22:27,060 --> 00:22:28,530
understand the language
that you're asking.

506
00:22:28,530 --> 00:22:31,653
It's a way to interact with
humans to process language.

507
00:22:32,610 --> 00:22:36,000
Just making a simple choice
in your compute platform can

508
00:22:36,000 --> 00:22:38,220
have a dramatic impact on your cost.

509
00:22:38,220 --> 00:22:42,300
And here we're looking
at an M7I versus an M7A

510
00:22:42,300 --> 00:22:44,190
and because of the performance uplift,

511
00:22:44,190 --> 00:22:46,680
if you look at the instant
sizes under the chart here,

512
00:22:46,680 --> 00:22:48,120
you'll see the gray.

513
00:22:48,120 --> 00:22:50,100
When you move from gray to blue,

514
00:22:50,100 --> 00:22:51,600
you cut the instant size in half.

515
00:22:51,600 --> 00:22:55,560
We either go from 4XL
to 2XL or 8XL to 4XL.

516
00:22:55,560 --> 00:22:58,743
If you speak instant
sizes in terms of EC2.

517
00:23:01,519 --> 00:23:03,990
That higher performance on the M7A

518
00:23:03,990 --> 00:23:06,780
AMD epic powered instances enables you

519
00:23:06,780 --> 00:23:11,780
to achieve similar performance
at roughly 40% lower cost.

520
00:23:12,330 --> 00:23:14,040
You know, right now we're going

521
00:23:14,040 --> 00:23:16,470
through a fairly decent economic cycle,

522
00:23:16,470 --> 00:23:20,580
but the next time an
economic downturn comes,

523
00:23:20,580 --> 00:23:23,190
that 40% difference in
your burn rate can make

524
00:23:23,190 --> 00:23:25,110
a tremendous difference in frankly,

525
00:23:25,110 --> 00:23:28,860
the number of people that
you can keep employed, right?

526
00:23:28,860 --> 00:23:32,940
So it also can free up a lot of capital

527
00:23:32,940 --> 00:23:36,060
to invest in other
applications, new applications,

528
00:23:36,060 --> 00:23:38,250
business growth and so forth.

529
00:23:38,250 --> 00:23:39,390
So that's the impact

530
00:23:39,390 --> 00:23:43,110
that performance can
have on your applications

531
00:23:43,110 --> 00:23:46,440
with a very just simple
conscious selection

532
00:23:46,440 --> 00:23:49,203
of an optimal platform
for your applications.

533
00:23:50,070 --> 00:23:53,730
All right, and so now looking
at when does it make sense

534
00:23:53,730 --> 00:23:55,680
to do inference on CPU?

535
00:23:55,680 --> 00:23:58,920
So on the left here, we're listing a range

536
00:23:58,920 --> 00:24:01,620
of applications from
classical NML, which you know,

537
00:24:01,620 --> 00:24:03,480
technically don't really use models

538
00:24:03,480 --> 00:24:04,890
to recommendation systems.

539
00:24:04,890 --> 00:24:08,550
That's what your Netflix movie
recommendations come from.

540
00:24:08,550 --> 00:24:10,620
Natural language processing, we probably,

541
00:24:10,620 --> 00:24:12,603
we interact with that all the time.

542
00:24:13,440 --> 00:24:17,610
Gen AI for large language models
as well as mixed workloads.

543
00:24:17,610 --> 00:24:20,670
A lot of applications now have
a mix of traditional software

544
00:24:20,670 --> 00:24:22,680
and AI driven software.

545
00:24:22,680 --> 00:24:26,640
So for the model size,
generally you're gonna see

546
00:24:26,640 --> 00:24:28,620
CPU inference works well for small

547
00:24:28,620 --> 00:24:32,535
and medium sized models except
for classical ML and MLP

548
00:24:32,535 --> 00:24:35,370
and NLP, Natural Language Processing.

549
00:24:35,370 --> 00:24:38,140
CPU can handle all the model sizes

550
00:24:39,450 --> 00:24:42,780
and then offline versus
real time offline is a way

551
00:24:42,780 --> 00:24:46,530
to express not latency
sensitive or not real time.

552
00:24:46,530 --> 00:24:49,290
So generally, if you're
not latency sensitive

553
00:24:49,290 --> 00:24:50,580
or not running real time,

554
00:24:50,580 --> 00:24:53,490
CPU makes sense in all of these cases.

555
00:24:53,490 --> 00:24:55,671
In a few of the cases you want GPUs

556
00:24:55,671 --> 00:24:58,590
and AMD has great GPUs as
well though today we're here

557
00:24:58,590 --> 00:25:01,860
to mostly talk about
CPUs, but for classical ML

558
00:25:01,860 --> 00:25:05,760
and recommendation systems,
you can even run those on CPU.

559
00:25:05,760 --> 00:25:08,250
There are some large
streaming services today

560
00:25:08,250 --> 00:25:09,720
that are doing just that,

561
00:25:09,720 --> 00:25:12,810
that I would imagine almost
everyone in the audience here

562
00:25:12,810 --> 00:25:15,090
experiences every time they go

563
00:25:15,090 --> 00:25:17,253
to use their favorite streaming service.

564
00:25:19,020 --> 00:25:23,910
And so Net-Net, why use
AMD CPU powered instances

565
00:25:23,910 --> 00:25:25,380
for inference?

566
00:25:25,380 --> 00:25:27,900
It really boils down to performance,

567
00:25:27,900 --> 00:25:29,730
which drives cost savings for you

568
00:25:29,730 --> 00:25:32,430
and can drive much better user experience

569
00:25:32,430 --> 00:25:33,360
for your end users.

570
00:25:33,360 --> 00:25:35,730
You get over double the price performance,

571
00:25:35,730 --> 00:25:37,680
meaning performance per dollar

572
00:25:37,680 --> 00:25:40,590
or whatever currency you
happen to be operating in

573
00:25:40,590 --> 00:25:45,453
just by choosing instances with
the lowercase a in the name.

574
00:25:46,560 --> 00:25:48,900
And that is what enables you

575
00:25:48,900 --> 00:25:52,410
to drive on average across a wide range

576
00:25:52,410 --> 00:25:56,850
of inference workloads, 44% lower OPEX.

577
00:25:56,850 --> 00:26:01,290
And so the the fundamental
concept there is consider

578
00:26:01,290 --> 00:26:04,020
the platform that you're
deploying your applications on.

579
00:26:04,020 --> 00:26:06,120
If you just deploy it
wherever you have before

580
00:26:06,120 --> 00:26:09,030
you're probably spending
more than you need to.

581
00:26:09,030 --> 00:26:11,310
This is one of the main interest

582
00:26:11,310 --> 00:26:14,640
that we see broadly from
our customers today.

583
00:26:14,640 --> 00:26:19,170
And up next we're gonna have
Kyle McLaughlin from CVS Health

584
00:26:19,170 --> 00:26:20,730
telling you a little bit about

585
00:26:20,730 --> 00:26:24,330
how in CVS's FinOps practice,
they've taken advantage

586
00:26:24,330 --> 00:26:29,010
of these kinds of performance
driven optimizations.

587
00:26:29,010 --> 00:26:30,063
So Kyle.

588
00:26:31,650 --> 00:26:32,483
- Thanks Mike.

589
00:26:34,530 --> 00:26:36,180
Good afternoon everyone.

590
00:26:36,180 --> 00:26:37,290
See if I can figure out, remember.

591
00:26:37,290 --> 00:26:39,210
There we go. All right, cool.

592
00:26:39,210 --> 00:26:41,010
So quickly, just a
little bit about myself.

593
00:26:41,010 --> 00:26:42,300
I'm Kyle McLaughlin.

594
00:26:42,300 --> 00:26:45,180
I lead the FinOps practice at CVS Health.

595
00:26:45,180 --> 00:26:47,100
I've been there for about 18 months,

596
00:26:47,100 --> 00:26:50,790
but been in the FinOps space
for about seven years now.

597
00:26:50,790 --> 00:26:52,980
Previously worked at
the FinOps Foundation,

598
00:26:52,980 --> 00:26:56,310
which sets the standards and
best practices for FinOps.

599
00:26:56,310 --> 00:26:58,380
It's a project of the Linux Foundation.

600
00:26:58,380 --> 00:27:01,620
Spent several years as
well at a number of FinOps.

601
00:27:01,620 --> 00:27:04,260
So what was really exciting
for me about the opportunity

602
00:27:04,260 --> 00:27:08,130
to come to CVS is I could take
a startup experience working

603
00:27:08,130 --> 00:27:10,530
with different organizations
all over the world.

604
00:27:11,400 --> 00:27:12,870
The industry best practices,

605
00:27:12,870 --> 00:27:15,570
and go back to where I started my career,

606
00:27:15,570 --> 00:27:16,710
get back to a big company and see

607
00:27:16,710 --> 00:27:19,260
about how we do FinOps at scale.

608
00:27:19,260 --> 00:27:21,963
Enough about me though.
Let's talk about CVS.

609
00:27:23,250 --> 00:27:26,550
Most people know CVS
for the retail presence.

610
00:27:26,550 --> 00:27:28,470
You see the stores on every corner.

611
00:27:28,470 --> 00:27:29,850
What a lot of folks don't know

612
00:27:29,850 --> 00:27:32,730
is CVS also owns Aetna Insurance.

613
00:27:32,730 --> 00:27:34,590
We have a giant pharmacy division

614
00:27:34,590 --> 00:27:37,200
and we also have a
healthcare delivery division.

615
00:27:37,200 --> 00:27:39,030
Combine all that together in one package.

616
00:27:39,030 --> 00:27:41,220
You have a US-based healthcare company

617
00:27:41,220 --> 00:27:43,350
that has a full end-to-end solution about

618
00:27:43,350 --> 00:27:45,480
how we can treat our members at every step

619
00:27:45,480 --> 00:27:47,220
of their healthcare journey.

620
00:27:47,220 --> 00:27:48,810
We're a Fortune five organization

621
00:27:48,810 --> 00:27:49,747
and we're on pace for about

622
00:27:49,747 --> 00:27:51,993
$400 billion in revenue this year.

623
00:27:53,190 --> 00:27:55,320
Combine all that with us going

624
00:27:55,320 --> 00:27:57,630
through a major tech transformation,

625
00:27:57,630 --> 00:27:59,400
we are trying to modernize

626
00:27:59,400 --> 00:28:02,820
and enhance our tech
stack while still serving

627
00:28:02,820 --> 00:28:05,280
100 million customers a day.

628
00:28:05,280 --> 00:28:08,940
So safe to say we have a
fairly significant cloud spend,

629
00:28:08,940 --> 00:28:10,830
but at the end of the day,
for us to support our members,

630
00:28:10,830 --> 00:28:13,380
stability is our overarching priority.

631
00:28:13,380 --> 00:28:15,810
So where does FinOps
come into play for CVS?

632
00:28:15,810 --> 00:28:17,640
Well, to manage all that spend,

633
00:28:17,640 --> 00:28:20,280
make sure we're adding value
for our member experience.

634
00:28:20,280 --> 00:28:22,710
It's really important that
we have good visibility

635
00:28:22,710 --> 00:28:24,930
into tracking the application spend,

636
00:28:24,930 --> 00:28:26,760
understand who those owners are.

637
00:28:26,760 --> 00:28:28,860
And it's also not just about spend,

638
00:28:28,860 --> 00:28:30,090
it's about what's the business value

639
00:28:30,090 --> 00:28:32,590
that we're getting out of
that incremental dollar.

640
00:28:34,740 --> 00:28:36,273
So we'll go back to Slide.

641
00:28:37,590 --> 00:28:40,170
Who here was not familiar with FinOps

642
00:28:40,170 --> 00:28:41,760
before they showed up at this talk

643
00:28:41,760 --> 00:28:43,950
or has never heard of FinOps?

644
00:28:43,950 --> 00:28:44,797
Love it my friend.

645
00:28:44,797 --> 00:28:47,313
You're going to get baptized by fire here.

646
00:28:49,200 --> 00:28:51,240
This is the FinOps framework.

647
00:28:51,240 --> 00:28:54,450
I no longer work there,
so if you don't like it,

648
00:28:54,450 --> 00:28:55,890
I'll tell you who you can take it up with.

649
00:28:55,890 --> 00:28:58,503
But a lot of content
here work with me folks.

650
00:29:00,210 --> 00:29:01,530
Being in the space for seven years,

651
00:29:01,530 --> 00:29:04,260
I have found everything boils down to,

652
00:29:04,260 --> 00:29:06,390
I'm gonna pick the smallest
graphic on the screen,

653
00:29:06,390 --> 00:29:08,790
which is great for a presentation, inform,

654
00:29:08,790 --> 00:29:10,230
optimize, operate.

655
00:29:10,230 --> 00:29:12,180
We call this the phases of FinOps.

656
00:29:12,180 --> 00:29:14,160
And really everything
that we're doing ties back

657
00:29:14,160 --> 00:29:15,690
to these three phases.

658
00:29:15,690 --> 00:29:16,770
It's impossible to go save

659
00:29:16,770 --> 00:29:18,120
that money if we don't know

660
00:29:18,120 --> 00:29:19,860
who's spending it in the first place.

661
00:29:19,860 --> 00:29:23,340
So everything always boils
back to how's our visibility?

662
00:29:23,340 --> 00:29:25,380
How are we doing in the informed phase?

663
00:29:25,380 --> 00:29:27,810
From there we think about optimizing

664
00:29:27,810 --> 00:29:28,890
and then once we've optimized,

665
00:29:28,890 --> 00:29:30,810
there's frankly too much work to do

666
00:29:30,810 --> 00:29:32,880
that we can't just keep sitting
there doing the same things.

667
00:29:32,880 --> 00:29:35,670
We need to find ways to
operationalize that work,

668
00:29:35,670 --> 00:29:37,380
automate it when possible,

669
00:29:37,380 --> 00:29:39,957
and then we enter right
back into the wheel again.

670
00:29:39,957 --> 00:29:43,470
And we talk about a lot of
work to do in the FinOps space.

671
00:29:43,470 --> 00:29:45,660
We touch a lot of different personas.

672
00:29:45,660 --> 00:29:48,705
Let's see if I make sure I got
the laser right. There we go.

673
00:29:48,705 --> 00:29:52,530
A FinOps practice.
Executives care about it.

674
00:29:52,530 --> 00:29:53,850
Our friends in cloud engineering,

675
00:29:53,850 --> 00:29:55,770
we are heavily dependent
on them in a partnership

676
00:29:55,770 --> 00:29:57,570
in order to have success.

677
00:29:57,570 --> 00:29:58,950
Procurement's extremely interested,

678
00:29:58,950 --> 00:30:00,510
finance is extremely interested.

679
00:30:00,510 --> 00:30:02,400
You have to work with all
these different personas

680
00:30:02,400 --> 00:30:04,320
across the organization.

681
00:30:04,320 --> 00:30:05,153
And then the framework,

682
00:30:05,153 --> 00:30:07,770
we have these ideas of
domains and capabilities.

683
00:30:07,770 --> 00:30:11,580
A domain is really what you
should be doing in this case.

684
00:30:11,580 --> 00:30:14,070
How do we quantify business value?

685
00:30:14,070 --> 00:30:17,160
The capabilities are how
we go about doing that.

686
00:30:17,160 --> 00:30:18,450
How we measure the business value

687
00:30:18,450 --> 00:30:20,400
is concepts like unit economics.

688
00:30:20,400 --> 00:30:22,650
It's fine if your cloud
spend goes up by 10%

689
00:30:22,650 --> 00:30:25,650
as long as you're serving
20, 30% more customers.

690
00:30:25,650 --> 00:30:27,300
And that's really how we
think about cloud spend

691
00:30:27,300 --> 00:30:29,483
and the value that we're
generating out of it.

692
00:30:32,040 --> 00:30:34,800
So I'll go back again,

693
00:30:34,800 --> 00:30:37,080
there's a lot of content on this slide.

694
00:30:37,080 --> 00:30:39,540
I really wanna talk through
what we've been doing at CVS,

695
00:30:39,540 --> 00:30:41,070
what I think are our biggest priorities

696
00:30:41,070 --> 00:30:41,970
and we're leading to some of

697
00:30:41,970 --> 00:30:44,220
the successful results
that we've had so far.

698
00:30:45,630 --> 00:30:46,980
So let's start with inform.

699
00:30:48,330 --> 00:30:49,740
Again, everything comes back

700
00:30:49,740 --> 00:30:52,980
to knowing who's the resource
owner who spun this thing up,

701
00:30:52,980 --> 00:30:55,080
and then what are we gonna do about it?

702
00:30:55,080 --> 00:30:56,460
We are able to map

703
00:30:56,460 --> 00:30:58,950
all of our resource
owners through our CMDB.

704
00:30:58,950 --> 00:31:02,490
This contains the technical
owner information, app owners,

705
00:31:02,490 --> 00:31:04,530
all that good detail.

706
00:31:04,530 --> 00:31:07,020
Not only do we need this
for security purposes,

707
00:31:07,020 --> 00:31:09,660
but we're also able to charge
back all of our cloud costs

708
00:31:09,660 --> 00:31:11,070
to those application owners.

709
00:31:11,070 --> 00:31:13,200
So we put all this emphasis on our CMDB

710
00:31:13,200 --> 00:31:15,120
to make sure we have the
proper visibility of who

711
00:31:15,120 --> 00:31:18,090
that owner is and then
also hold 'em accountable

712
00:31:18,090 --> 00:31:20,043
for the spend that they're generating.

713
00:31:21,150 --> 00:31:22,410
We also believe in operating

714
00:31:22,410 --> 00:31:25,950
with complete transparency
on your cloud spend.

715
00:31:25,950 --> 00:31:28,380
I have engineering teams that
are interested in knowing

716
00:31:28,380 --> 00:31:29,970
what they're spending in cloud.

717
00:31:29,970 --> 00:31:31,800
We make sure they have
the appropriate visibility

718
00:31:31,800 --> 00:31:34,013
through different tools
that we have internally.

719
00:31:35,760 --> 00:31:37,743
Optimize, the fun stuff.

720
00:31:39,180 --> 00:31:41,760
One of the biggest wins
we've had is really trying

721
00:31:41,760 --> 00:31:43,680
to focus on standardizing our SKUs

722
00:31:43,680 --> 00:31:46,410
that we have engineering
spinning up infrastructure.

723
00:31:46,410 --> 00:31:48,330
By standardizing the SKUs,

724
00:31:48,330 --> 00:31:50,520
that really allows us to
take advantage of the price

725
00:31:50,520 --> 00:31:51,600
and performance reasons

726
00:31:51,600 --> 00:31:54,390
and implement those Enos
best practices at time

727
00:31:54,390 --> 00:31:55,800
of resource creation.

728
00:31:55,800 --> 00:31:58,710
Then we can focus more of our
time on how do we chase down,

729
00:31:58,710 --> 00:32:00,300
you know, resources that are created

730
00:32:00,300 --> 00:32:02,000
before automation went into place.

731
00:32:03,540 --> 00:32:06,060
Consolidate on SKUs gets back
to what Madhu talked about.

732
00:32:06,060 --> 00:32:08,580
It allows us to really optimize

733
00:32:08,580 --> 00:32:09,960
and max out the savings

734
00:32:09,960 --> 00:32:11,580
that we're getting from reserved instances

735
00:32:11,580 --> 00:32:12,753
and savings plans.

736
00:32:14,070 --> 00:32:15,420
And we're able to manage all this

737
00:32:15,420 --> 00:32:17,220
with a combination of tools.

738
00:32:17,220 --> 00:32:19,620
some of the folks that you
might see out on the expo floor.

739
00:32:19,620 --> 00:32:21,240
And we're fortunate enough to
have some engineering teams

740
00:32:21,240 --> 00:32:23,580
that build custom solutions for us.

741
00:32:23,580 --> 00:32:25,620
You know, when we can't
quite meet our needs

742
00:32:25,620 --> 00:32:26,453
and we again,

743
00:32:26,453 --> 00:32:28,920
give that total visibility
to teams internally.

744
00:32:28,920 --> 00:32:31,200
So if people want to
go and see their costs,

745
00:32:31,200 --> 00:32:32,460
they want to optimize,

746
00:32:32,460 --> 00:32:34,960
they have tools available
to 'em to get that done.

747
00:32:37,050 --> 00:32:39,870
But again, going back to the first slide,

748
00:32:39,870 --> 00:32:42,120
we can't just keep doing the
same thing over and over again.

749
00:32:42,120 --> 00:32:44,040
We're a relatively small team.

750
00:32:44,040 --> 00:32:46,230
How do we operationalize this?

751
00:32:46,230 --> 00:32:49,650
A couple places where we
found a lot of great results,

752
00:32:49,650 --> 00:32:53,730
specifically partnering with
our largest application teams.

753
00:32:53,730 --> 00:32:56,490
Generally it's just
like any 80/20 analysis,

754
00:32:56,490 --> 00:32:58,680
80% of your spend is
probably gonna come from

755
00:32:58,680 --> 00:33:01,380
about 20% of your total applications.

756
00:33:01,380 --> 00:33:03,540
And what we found is by partnering closely

757
00:33:03,540 --> 00:33:05,640
with these large application teams,

758
00:33:05,640 --> 00:33:07,860
building the relationship,
investing the time,

759
00:33:07,860 --> 00:33:10,200
getting to know what
they're trying to solve,

760
00:33:10,200 --> 00:33:12,990
typically leads to great
results down the road.

761
00:33:12,990 --> 00:33:15,000
We had one application team in particular

762
00:33:15,000 --> 00:33:17,910
where we just approached
them about a fairly simple,

763
00:33:17,910 --> 00:33:20,340
fairly straightforward SKU change.

764
00:33:20,340 --> 00:33:22,710
Fast forward about nine
months, we now have

765
00:33:22,710 --> 00:33:26,130
that engineering team refactoring
applications completely

766
00:33:26,130 --> 00:33:28,080
re-architecting how they deploy

767
00:33:28,080 --> 00:33:31,680
and even finding really nuanced
optimization opportunities

768
00:33:31,680 --> 00:33:34,800
that are far outside the
skill set of our FinOps team.

769
00:33:34,800 --> 00:33:36,780
But because we built that
relationship with them,

770
00:33:36,780 --> 00:33:39,000
frankly got 'em a little
hooked on the savings,

771
00:33:39,000 --> 00:33:40,170
they've really leaned into it

772
00:33:40,170 --> 00:33:41,460
and we've have a great partnership

773
00:33:41,460 --> 00:33:42,480
and we've been able to replicate

774
00:33:42,480 --> 00:33:44,853
that across other application teams.

775
00:33:46,470 --> 00:33:50,400
We were talking earlier, CVSs,
we have 300,000 employees.

776
00:33:50,400 --> 00:33:51,870
It's a large organization

777
00:33:51,870 --> 00:33:54,370
and we're spread across
four major business units.

778
00:33:55,380 --> 00:33:56,520
So we've also been working

779
00:33:56,520 --> 00:33:58,200
to establish working relationships

780
00:33:58,200 --> 00:34:00,390
with all the different bus and make sure

781
00:34:00,390 --> 00:34:01,680
that we're evangelizing

782
00:34:01,680 --> 00:34:03,060
and spreading these best practices

783
00:34:03,060 --> 00:34:04,950
throughout the organization.

784
00:34:04,950 --> 00:34:07,470
And then where possible automate.

785
00:34:07,470 --> 00:34:09,450
So we have automated
communication channels,

786
00:34:09,450 --> 00:34:11,040
making sure we're getting
the right information

787
00:34:11,040 --> 00:34:13,230
to the right people at the right time.

788
00:34:13,230 --> 00:34:16,800
We're constantly exploring new
ways to get that information

789
00:34:16,800 --> 00:34:18,180
to our app owners and the people

790
00:34:18,180 --> 00:34:19,620
who are passionate about managing

791
00:34:19,620 --> 00:34:22,230
their cloud resources effectively.

792
00:34:22,230 --> 00:34:23,280
And that's really been a key for

793
00:34:23,280 --> 00:34:27,063
how we scale FinOps
program at the size of CVS.

794
00:34:29,280 --> 00:34:30,880
So where are we going from here?

795
00:34:32,370 --> 00:34:35,040
Containers continue to
be a growing presence

796
00:34:35,040 --> 00:34:36,720
in our overall cloud portfolio.

797
00:34:36,720 --> 00:34:39,150
It's gonna be a major focus
for us for the next few years.

798
00:34:39,150 --> 00:34:42,180
Is same thing going through
the same three phases

799
00:34:42,180 --> 00:34:44,070
of the FinOps lifecycle

800
00:34:44,070 --> 00:34:45,660
and form, optimize, operate,

801
00:34:45,660 --> 00:34:47,520
make sure we're getting
best in class visibility

802
00:34:47,520 --> 00:34:49,560
and granularity of what's
going on within each

803
00:34:49,560 --> 00:34:51,870
of those namespace and pods.

804
00:34:51,870 --> 00:34:54,060
And then exploring different
optimization opportunities

805
00:34:54,060 --> 00:34:56,820
about how we more
efficiently pack resources

806
00:34:56,820 --> 00:34:59,133
and use the right VM for that workload.

807
00:35:00,720 --> 00:35:04,500
We've done a good job so far
of implementing standards,

808
00:35:04,500 --> 00:35:06,660
but we really need to start
transitioning to a place

809
00:35:06,660 --> 00:35:08,460
of policy and governance

810
00:35:08,460 --> 00:35:10,230
and for an organization of our scale,

811
00:35:10,230 --> 00:35:13,290
it's also how do you
do real world mediation

812
00:35:13,290 --> 00:35:15,360
with real exception management.

813
00:35:15,360 --> 00:35:16,920
You know, it's easy for anyone to go

814
00:35:16,920 --> 00:35:19,380
and say, we need to go do this one thing,

815
00:35:19,380 --> 00:35:21,720
but a company as as large

816
00:35:21,720 --> 00:35:23,190
and as complex as ours,

817
00:35:23,190 --> 00:35:26,040
we need to be able to handle
different use cases as well.

818
00:35:27,240 --> 00:35:28,073
And you're welcome.

819
00:35:28,073 --> 00:35:30,000
I'm saving my last bullet point at the end

820
00:35:30,000 --> 00:35:31,293
to say the word AI.

821
00:35:32,850 --> 00:35:34,200
We are also looking at ways

822
00:35:34,200 --> 00:35:37,350
to leverage the latest
technology for FinOps best.

823
00:35:37,350 --> 00:35:38,520
And it's really two lens.

824
00:35:38,520 --> 00:35:40,710
There's FinOps for AI,

825
00:35:40,710 --> 00:35:42,720
everyone knows the spends growing there,

826
00:35:42,720 --> 00:35:45,690
the tooling's still
relatively new in the market.

827
00:35:45,690 --> 00:35:49,020
We need to get more granular
visibility into what's going on

828
00:35:49,020 --> 00:35:50,430
with these AI workloads.

829
00:35:50,430 --> 00:35:52,770
And then we can have the fun optimization

830
00:35:52,770 --> 00:35:56,010
and operational conversation
at the same time.

831
00:35:56,010 --> 00:35:59,370
AI can also be used to help
you generate your insights,

832
00:35:59,370 --> 00:36:01,290
the introduction of chatbots and agents.

833
00:36:01,290 --> 00:36:03,150
We can now be leveraging AI

834
00:36:03,150 --> 00:36:05,730
to help us generate our
insights at the end of the month

835
00:36:05,730 --> 00:36:07,860
as opposed to fully relying on dashboards

836
00:36:07,860 --> 00:36:09,560
and analysts diving into the data.

837
00:36:10,410 --> 00:36:13,080
So I'll be here after the talk
if anyone wants to talk more,

838
00:36:13,080 --> 00:36:14,340
happy to answer any questions

839
00:36:14,340 --> 00:36:17,220
and I'll pass it back over to Madhu.

840
00:36:17,220 --> 00:36:18,053
Cool.

841
00:36:20,550 --> 00:36:22,250
- Right. Thank you very much Kyle.

842
00:36:23,280 --> 00:36:25,143
So just to wrap it all up,

843
00:36:27,240 --> 00:36:29,460
tying it back to the
overall team of this, right?

844
00:36:29,460 --> 00:36:33,030
How do you build an excellent platform

845
00:36:33,030 --> 00:36:34,620
that can span a broad set

846
00:36:34,620 --> 00:36:37,350
of workloads including AI inference,

847
00:36:37,350 --> 00:36:40,320
epic CPUs provide that platform.

848
00:36:40,320 --> 00:36:42,770
I think Mike walked you
through various examples.

849
00:36:44,400 --> 00:36:48,660
And then again, the CPUs are
just the means to an end.

850
00:36:48,660 --> 00:36:50,880
The instance is how you're consuming it.

851
00:36:50,880 --> 00:36:54,540
And Mike of course walked
you through the EC2 instances

852
00:36:54,540 --> 00:36:58,500
that can help you realize this TCO value.

853
00:36:58,500 --> 00:37:02,910
And in terms of the ease
of use getting started

854
00:37:02,910 --> 00:37:06,780
with an AMD epic powered
EC2 instance is really easy

855
00:37:06,780 --> 00:37:08,910
regardless of what your starting point is,

856
00:37:08,910 --> 00:37:13,530
your starting from an old Intel instance,

857
00:37:13,530 --> 00:37:14,543
it just works, right?

858
00:37:14,543 --> 00:37:17,190
I think for those of
you with long memories,

859
00:37:17,190 --> 00:37:20,070
you might remember that
back in the day there was

860
00:37:20,070 --> 00:37:24,930
this whole IA-64 versus
X86-64 instructions.

861
00:37:24,930 --> 00:37:26,640
That debate

862
00:37:26,640 --> 00:37:28,380
and AMD went down the path

863
00:37:28,380 --> 00:37:32,610
of X86-64 while Intel
went down the IA-64 path

864
00:37:32,610 --> 00:37:34,500
and the market spoke.

865
00:37:34,500 --> 00:37:37,200
And X86-64 is what's arrived.

866
00:37:37,200 --> 00:37:39,660
And that is why all the applications

867
00:37:39,660 --> 00:37:41,490
just work in most cases.

868
00:37:41,490 --> 00:37:45,570
And once you move over to an AMD instance,

869
00:37:45,570 --> 00:37:48,300
most cases you're gonna
get more performance.

870
00:37:48,300 --> 00:37:49,830
Most cases it just works

871
00:37:49,830 --> 00:37:51,690
and then you can get even more performance

872
00:37:51,690 --> 00:37:53,640
by doing further tuning.

873
00:37:53,640 --> 00:37:57,300
So really I said, so the Epic
kind of gives you the ability

874
00:37:57,300 --> 00:38:01,860
to essentially just lift
and shift and then optimize.

875
00:38:01,860 --> 00:38:04,833
You don't have to do a whole
lot of porting work together.

876
00:38:05,700 --> 00:38:08,430
With that, I wanna thank Kyle and Mike

877
00:38:08,430 --> 00:38:10,410
and all of you for attending.

878
00:38:10,410 --> 00:38:12,540
We will be available to answer questions.

879
00:38:12,540 --> 00:38:15,810
We have about 20 minutes
and so we can hang around

880
00:38:15,810 --> 00:38:17,040
and answer any questions you have.

881
00:38:17,040 --> 00:38:18,040
Thank you very much.

