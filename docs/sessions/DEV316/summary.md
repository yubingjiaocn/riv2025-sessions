# AWS re:Invent 2025 技术分享会总结

## 会议概述

本次技术分享会由Soracom的IoT和AI布道师Max（Cohe）主讲，他同时也是AWS Hero。会议主题聚焦于在本地设备上运行生成式AI的实践方案。Max通过机器人手臂的实际演示，深入探讨了物理AI（Physical AI）的概念、本地推理与云端推理的权衡，以及如何使用AWS IoT Greengrass实现边缘设备上AI模型的持续更新和部署。

演讲者以日本公司Link With的机器人视觉解决方案为案例，展示了视觉-语言-行动模型（VLA）如何突破传统示教机器人的局限性。传统机器人只能重复固定动作，而集成3D扫描仪和AI的新方案能够识别物体的形状和角度，自主调整机械臂动作。整个会议通过理论讲解、现场演示和视频展示相结合的方式，为观众呈现了边缘AI在实际工业场景中的应用价值和技术实现路径。

会议的核心观点是：物理AI时代最重要的不是选择某个特定模型，而是构建可持续更新的系统架构。通过AWS IoT Greengrass和Docker容器技术，开发者可以像管理云端服务一样管理边缘设备上的AI模型，实现快速迭代和远程部署。

## 详细时间线

00:00 - 开场介绍
- 演讲者自我介绍：Max（Cohe），Soracom的IoT和AI布道师，AWS Hero
- 会议主题：在本地设备上运行生成式AI
- 互动环节：询问观众是否尝试过在设备上运行生成式AI模型

02:30 - 会议议程说明
- 第一部分：解释物理AI（Physical AI）的概念
- 第二部分：通过现场演示对比本地推理和云端推理的延迟差异
- 第三部分：展示如何使用AWS IoT Greengrass更新边缘设备上的基础模型
- 第四部分：总结

04:00 - 物理AI概念讲解
- 物理AI定义：机器能够通过摄像头和传感器感知真实世界，做出决策并采取行动改变环境
- AI从数字世界向物理世界的转变
- 本地AI vs 云端AI的区别：运行位置不同，而非技术本质不同

06:30 - 物理AI的三大核心需求
- 响应性（Responsiveness）：AI能够快速响应所见所感
- 自主性（Autonomy）：AI不仅执行命令，还能独立思考和行动
- 协作性（Collaboration）：AI能理解人类意图，与人类自然协作

08:00 - 客户案例：Link With公司
- 日本公司Link With为机器人手臂添加3D扫描仪实现视觉能力
- 传统机器人手臂使用示教方式，只能重复固定动作
- 如果物体位置或角度改变，传统机器人无法拾取

10:00 - 传统示教方法的局限性
- 示教（Teaching）：通过手动移动机器人手臂记录动作轨迹
- 基于规则的方法在重复性任务中表现良好
- 但无法适应物体位置变化，需要人工干预

12:30 - VLA模型介绍
- VLA（Vision-Language-Action）模型：视觉-语言-行动模型
- 通过摄像头和传感器理解场景并生成正确的动作
- 将生成式AI从网络空间扩展到物理世界

14:00 - VLM vs VLA对比
- VLM（Vision-Language Model）：理解文本和图像
- VLA：接收传感器和摄像头的真实世界数据，输出动作计划

15:30 - VLA架构说明
- 输入：来自传感器和摄像头的真实世界数据
- 处理：进行推理并生成动作计划
- 输出：向执行器（如机器人手臂）发送信号

17:00 - 黑色物体识别演示
- 使用8个摄像头捕捉桌面场景
- VLA模型识别黑色物体并将其放回原位
- 即使物体位置改变，模型仍能正常工作
- 系统运行在Raspberry Pi和简单网络摄像头上

20:00 - LLM与VLA对比
- LLM：输入文本和图像，输出文本和图像
- VLA：输入来自真实世界的传感器数据，输出动作指令
- 示例模型：RT2、Small VLA

22:00 - 本地推理vs云端推理架构对比
- 云端推理需要将数据发送到云端并等待返回
- 往返延迟（Round Trip）导致更高的延迟

23:30 - 现场延迟对比演示
- 演示设备：两个机器人手臂通过USB直连，延迟极低
- 通过LTE网络和美国东部1区服务器的云端方案
- 使用socat命令建立TCP端口转发

28:00 - 延迟演示结果
- 本地USB连接：几乎无延迟，实时同步
- 云端连接（LTE + US East 1）：明显延迟，约500-600毫秒
- 演示成功展示了延迟对实时性能的影响

32:00 - 延迟阈值讨论
- 100毫秒是人类感知延迟的重要阈值
- 以自动售货机按钮响应时间为参考
- 如果系统可接受该延迟，云端推理仍然可行

34:00 - 边缘AI的三大应用场景
- 摄像头+机器人组合：生产线上需要快速推理和快速动作
- 安全场景：危险情况下不能等待长延迟，需要离线工作
- 隐私场景：敏感数据（如摄像头数据）必须本地处理

36:30 - 模型选择的重要性
- 引用CEO主题演讲：模型选择至关重要
- 云端可以轻松选择和切换模型（如Amazon Bedrock）
- 边缘设备受存储和内存限制，只能安装一两个模型

38:00 - 可更新性是关键
- 今天最新的AI模型明天可能就过时
- 必须构建保持AI模型可更新的机制
- 不要将系统锁定在单一模型上

40:00 - AWS IoT Greengrass介绍
- 将云端风格的更新能力带到边缘设备
- 最新版本V2基于Java，运行在Linux等操作系统上
- 应用以组件（Component）形式部署
- V2支持Docker容器作为组件

42:00 - 模型更新架构说明
- VLA模型存储在Hugging Face
- 通过Docker构建将模型打包到容器镜像
- 推送容器镜像到Amazon ECR
- 通过AWS IoT Greengrass发送部署命令
- 设备运行Docker并在本地启动容器

44:00 - Dockerfile演示
- 展示简单的Dockerfile（仅8行）
- 从Hugging Face下载VLA模型并打包到容器
- 标签BV3表示黑色物体检测模型

46:00 - 黑色物体检测模型演示
- 模型已部署到设备，部署状态为成功
- 视频演示：模型识别黑色物体并放回原位
- 即使物体位置改变，模型仍能正常工作

49:00 - 模型更新演示开始
- 目标：从黑色物体检测模型切换到白色物体检测模型
- 修改Dockerfile，将模型从黑色改为白色
- 执行Docker构建和推送到Amazon ECR

52:00 - IoT Greengrass部署更新
- 创建新的IoT Greengrass配方（Recipe）
- 更新组件版本号
- 通过IoT Greengrass部署新版本

54:00 - 实时更新过程
- Raspberry Pi上显示Docker拉取新容器镜像
- 停止黑色物体检测模型
- 启动白色物体检测模型
- 更新成功完成

56:00 - 白色物体检测演示
- 视频演示：新模型检测白色物体并放回原位
- 不同位置测试均成功
- 新模型只检测白色物体，忽略其他颜色物体

59:00 - Docker在IoT中的优势
- 可以导出设备文件（如摄像头、传感器）到容器内使用
- 强调可更新性的重要性：无法更新的AI应用会很快过时

61:00 - IoT设备开发的新范式
- 传统：只处理程序和数据
- 现在：还需要处理AI模型文件
- 使用IoT Greengrass比自建系统更稳定可靠

62:30 - 网络环境考虑
- 不需要持续连接，但部署和更新AI模型需要快速大数据传输
- 室内可使用Wi-Fi，室外可使用LTE（如Soracom提供的IoT移动连接）
- 也可使用宽带卫星网络（如Amazon Kuiper）

64:00 - 文件大小挑战
- AI模型文件非常大，小型模型约1GB
- 开源VLA模型超过15GB
- IoT Greengrass部署大文件的三种方法

65:30 - 三种大文件部署方法
- 方法1：IoT Greengrass工件文件（限制2GB）
- 方法2：将模型打包到Docker容器（本次演示使用的方法，限制取决于Amazon ECR）
- 方法3：在IoT Greengrass配方中使用脚本下载文件（避免大多数限制，但需自行管理）

67:00 - 推荐方案
- 建议将AI模型打包到容器中
- 容器使开发和测试更容易
- IoT Greengrass能够很好地管理容器
- 有助于避免文件大小限制

68:30 - 会议总结
- 物理AI不仅仅是本地AI，云端AI同样有用
- 关键是测量延迟并确定需求（记住100毫秒阈值）
- 物理AI最重要的理念是保持可更新性，而不仅仅是选择模型
- AWS IoT Greengrass使设备可更新且可靠
- AI将继续发展，让我们在真实世界中发挥其力量

70:00 - 结束致谢
- 感谢观众参加本次会议