# AWS re:Invent 2025 技术会议总结

## 会议概述

本次AWS re:Invent 2025技术会议由AWS公用计算高级副总裁Peter DeSantis主讲，重点探讨了AI转型对云基础设施的影响。会议强调AWS在过去20年中一直专注的核心属性——安全性、可用性、弹性、敏捷性和成本优化——在AI时代变得更加重要。Peter详细介绍了AWS如何通过深度投资自研芯片（Nitro、Graviton、Trainium）来应对AI工作负载带来的前所未有的需求增长。

会议展示了AWS在多个技术领域的重大创新：Graviton5处理器提供192核心和5倍于前代的L3缓存；Lambda托管实例将无服务器的简便性与EC2的性能控制相结合；Project Mantle重新设计了推理引擎架构；S3 Vectors将向量搜索能力引入海量数据存储；Trainium3芯片通过创新的微架构优化和NKI编程接口，实现了比前代高达5倍的能效提升。会议还邀请了Apple、TwelveLabs和Decart等客户分享了他们在AWS平台上的成功实践，展示了这些技术创新如何在实际生产环境中创造价值。

整场会议传递的核心信息是：AI不仅仅是新的工作负载类型，更是对云计算基础设施的全面重塑。AWS通过持续的技术创新和深度的客户合作，正在构建一个能够支撑AI时代应用开发的强大平台，让开发者能够专注于创新而非基础设施管理。

## 详细时间线

### 开场与核心理念 (00:00 - 15:00)

00:00 - 会议开场
- Peter DeSantis代替Werner Vogels主持早间主题演讲
- 介绍本次会议将探讨AI转型对云计算的意义

02:30 - AWS云计算的核心属性
- 强调六大核心属性：安全性、可用性、弹性、成本、敏捷性
- 这些属性在AI时代变得更加重要而非过时

05:00 - Nitro系统的起源故事
- 回顾2010年AWS面临的虚拟化性能抖动问题
- 介绍Nitro系统如何通过自研芯片消除虚拟化税收
- 宣布Nitro系统被收录进计算机科学经典教材第七版

### Graviton处理器创新 (15:00 - 35:00)

15:00 - Dave Brown登台介绍Graviton
- 解释为什么AWS要为云工作负载专门设计处理器
- 展示客户成果：Adobe减排37%、Epic Games支持大规模游戏、Pinterest降低成本47%

18:00 - 直接硅冷却技术
- 介绍传统CPU冷却方案的多层结构
- 展示Graviton如何通过去除保护盖实现直接硅冷却
- 风扇功耗降低33%

22:00 - 缓存优化策略
- 详细解释L1、L2、L3缓存层级结构
- Graviton4将L2缓存从1MB增加到2MB
- 分析缓存未命中对性能的影响

28:00 - Graviton5发布
- 单封装提供192核心
- L3缓存是前代的5倍以上
- 每核心获得2.6倍的L3缓存

30:00 - M9g实例预览
- 性能比M8g提升25%
- 提供EC2中最佳性价比
- 客户案例：Airbnb性能提升25%、SAP OLTP查询性能提升60%

### Apple客户案例 (35:00 - 45:00)

35:00 - Apple副总裁Payam Mirrashidi登台
- 介绍Apple在AWS上运行的大规模服务
- App Store、Apple Music、Apple TV等服务

38:00 - Swift语言在服务器端的应用
- 从Java和C++迁移到Swift
- 客户端和服务器端共享代码库的优势

40:00 - Swift on Graviton的性能提升
- 性能提升40%
- 成本降低30%
- 从x86无缝迁移到ARM架构

42:00 - iOS垃圾短信检测案例
- 使用Swift on Graviton处理数亿用户的计算密集型操作
- 采用同态加密保护隐私
- 宣布Amazon Linux首个官方Swift工具链包

### Lambda托管实例 (45:00 - 58:00)

45:00 - Lambda的起源故事
- 2013年S3团队处理图片缩略图的挑战
- Lambda如何从"无服务器"概念演变而来

48:00 - Lambda的演进
- 2014年发布改变了应用开发模式
- 持续增加功能：容器支持、VPC网络、SnapStart等

52:00 - 重新定义无服务器
- EC2和Lambda团队首次合并
- 提出新问题：能否在保持无服务器简便性的同时提供EC2级别的性能控制

55:00 - Lambda托管实例发布
- Lambda函数运行在用户账户内的EC2实例上
- 用户选择实例类型，Lambda管理配置、补丁、可用性和扩展
- 现有Lambda函数无需修改即可使用
- 适用于视频处理、ML预处理、高吞吐量分析等工作负载

### 推理引擎创新 (58:00 - 1:15:00)

58:00 - 推理工作负载的独特挑战
- 解释推理请求的四阶段流程：分词、预填充、解码、去分词
- 每个阶段对系统资源的需求完全不同

1:02:00 - Project Mantle推理引擎
- 重新设计Bedrock的推理架构
- 引入服务层级：优先级、标准、灵活

1:06:00 - 公平性和队列隔离
- 每个客户拥有独立队列
- 自适应容量共享
- 学习客户使用模式并预留容量

1:09:00 - Journal持久化系统
- 借鉴DynamoDB和S3的事务日志技术
- 支持长时间运行请求的容错恢复
- 实现高级调度策略

1:11:00 - Bedrock微调功能
- 微调作为长时间运行任务
- 实时流量高峰时暂停，流量下降后恢复
- 无需管理独立训练集群

1:13:00 - 机密计算集成
- 保护模型权重和客户数据
- 加密数据处理
- 提供加密验证保证

### 向量搜索能力 (1:15:00 - 1:30:00)

1:15:00 - 向量搜索介绍
- 使用法国斗牛犬Smokey的照片解释向量概念
- 向量如何在数学空间中表示数据关系

1:18:00 - 嵌入模型的工作原理
- 真实向量空间可达3000维
- 嵌入模型通过AI学习识别模式

1:20:00 - 企业数据挑战
- 非结构化数据分散在PDF、视频、图片中
- 传统嵌入模型只能处理单一数据类型

1:22:00 - Nova多模态嵌入模型
- 支持文本、文档、图片、视频和音频
- 统一向量空间
- 行业领先的精度

1:24:00 - 向量能力集成到AWS服务
- OpenSearch混合搜索：关键词+语义搜索
- 多个数据库和分析服务集成向量搜索

1:26:00 - S3 Vectors发布
- 将向量存储带到S3规模
- 像创建S3存储桶一样简单
- 本周宣布正式可用(GA)

1:28:00 - S3 Vectors技术实现
- 预计算向量邻域
- 近似最近邻算法
- 20亿向量数据库实现亚100毫秒查询
- 4个月内创建25万个向量索引，摄入40亿向量，执行10亿次搜索

### TwelveLabs客户案例 (1:30:00 - 1:42:00)

1:30:00 - TwelveLabs CEO Jae Lee登台
- 使用自家模型分析Peter过去8年的主题演讲
- 展示视频理解能力

1:32:00 - 视频数据的挑战
- 90%的数据是非结构化的，大部分来自视频
- 100万小时视频相当于连续观看114年

1:34:00 - Foundation模型介绍
- Marengo：多模态嵌入模型，支持精确视频搜索
- Pegasus：视频语言模型，生成摘要、字幕和元数据

1:36:00 - AWS基础设施的作用
- TwelveLabs诞生于AWS
- 数据基础设施建立在Amazon S3上
- 处理PB级视频无需移动数据

1:38:00 - S3 Vectors集成
- 数十亿嵌入向量直接存储在S3
- 自然语言查询转换为向量搜索
- 跨数十亿嵌入执行近似最近邻搜索

1:40:00 - ArcXP客户案例
- 华盛顿邮报的媒体管理平台
- 为全球新闻机构提供服务
- 使用TwelveLabs模型分析和丰富存档视频内容

### Trainium3芯片 (1:42:00 - 2:10:00)

1:42:00 - Trainium概述
- 支持训练和推理
- 支持所有模型架构：密集Transformer、专家混合、状态空间模型
- 支持所有模态：文本、图像、视频

1:45:00 - Trainium3 UltraServer
- 144个Trainium3芯片跨两个机架
- Neuron交换机提供全双向带宽和超低延迟
- 360 PetaFLOPS 8位浮点计算
- 20TB高带宽内存，700TB/秒内存带宽
- 比上一代计算性能高4.4倍，带宽高3.9倍

1:48:00 - Trainium3服务器设计
- 36个Trainium sled组成UltraServer
- 集成三种AWS自研芯片：Trainium3、Graviton、Nitro
- 顶部可维护设计，支持机器人组装
- 双Nitro卡提供高速网络
- EFA支持数千台服务器直接共享内存

1:52:00 - 芯片微架构优化
- 微缩放：使用低精度浮点数同时保持精度
- 更快的softmax、张量解引用、后台转置
- 流量整形、内存添加写入、哈希分散
- 提高内存和网络效率

1:55:00 - NKI (Neuron Kernel Interface)
- 结合矩阵操作简便性和指令级硬件访问
- Python编程环境
- Decart AI使用NKI优化实时视频生成模型
- Q1正式可用，完全开源

1:58:00 - Neuron Profiler和Explorer
- 芯片内置专用性能分析硬件
- 不影响性能的情况下分析生产代码
- Neuron Explorer自动检测瓶颈并建议优化

2:01:00 - 性能基准测试
- GPT-OSS 120亿参数模型
- 每兆瓦输出token数提升5倍
- 保持相同用户可见延迟
- 相关代码和NKI内核将开源

2:04:00 - PyTorch原生支持
- 只需将代码从"cuda"改为"neuron"
- 无需学习新软件栈
- 预计明年初发布

### Decart客户演示 (2:10:00 - 2:20:00)

2:10:00 - Decart CEO Dean Leitersdorf登台
- 展示实时视觉智能新类别
- 现场演示：将Peter实时转换为Werner的卡通形象
- 所有像素在舞台上实时生成

2:12:00 - 实时视觉智能技术
- 结合LLM和视频扩散模型
- 零延迟实时运行
- 基础模型用于视觉智能

2:14:00 - Trainium3性能优化
- 使用NKI语言优化
- 相比最先进GPU实现4倍帧率提升
- 80%张量核心利用率

2:16:00 - Megakernel架构
- 三个组件：LLM推理、视频模型、编码器
- 在同一芯片上零延迟同时运行
- 最大化HBM内存和张量引擎利用率

2:17:00 - 应用场景
- Twitch直播创造新体验
- 在线购物产品可视化
- 实时体育赛事卡通化
- 游戏自生成
- 机器人仿真训练

### 总结 (2:20:00 - 2:25:00)

2:20:00 - 会议总结
- AI时代基础设施的核心属性更加重要
- 回顾主要发布：Graviton5、Lambda托管实例、向量搜索、Trainium3

2:22:00 - 展望未来
- AI仍处于第一天
- AWS将继续移除约束、提供构建模块
- 帮助客户应对未知挑战

2:24:00 - 闭幕
- 提醒观众参加Werner的闭幕主题演讲
- 感谢参会者