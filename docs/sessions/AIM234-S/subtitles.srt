1
00:00:01,410 --> 00:00:02,243
- Hello.

2
00:00:02,243 --> 00:00:03,180
Hello, everybody!

3
00:00:03,180 --> 00:00:06,247
Good afternoon and welcome to

4
00:00:06,247 --> 00:00:09,060
"Autonomous Agents You Can Trust."

5
00:00:09,060 --> 00:00:12,240
My name is Neil LeBlanc and I am with IBM

6
00:00:12,240 --> 00:00:15,090
and I lead our go-to-market
for watsonx.governance.

7
00:00:15,090 --> 00:00:17,697
- And my name is Diana
Griffin and I am with KPMG,

8
00:00:17,697 --> 00:00:20,790
and I am focused AI governance

9
00:00:20,790 --> 00:00:24,180
and I am part of the alliance with IBM.

10
00:00:24,180 --> 00:00:26,977
- So hopefully you all didn't
get too stuffed at lunch,

11
00:00:26,977 --> 00:00:29,430
but, if you did, then we're
here to keep you awake

12
00:00:29,430 --> 00:00:32,010
and burn off some calories.

13
00:00:32,010 --> 00:00:35,310
Look, you know, I think as
we look back over the course

14
00:00:35,310 --> 00:00:36,690
of the last two or three years,

15
00:00:36,690 --> 00:00:38,580
I don't think too many
people would've predicted

16
00:00:38,580 --> 00:00:41,490
just how powerful or where
we would've gotten, right?

17
00:00:41,490 --> 00:00:43,320
How AI would've evolved.

18
00:00:43,320 --> 00:00:45,330
And especially as we kinda look at agents.

19
00:00:45,330 --> 00:00:47,580
And you look around the expo floor,

20
00:00:47,580 --> 00:00:49,800
you look through the thousands of sessions

21
00:00:49,800 --> 00:00:53,520
that were in the catalog,
there's a lot about agents.

22
00:00:53,520 --> 00:00:55,680
And as we kind of look
at how they've evolved

23
00:00:55,680 --> 00:00:57,750
over the course of the last year or so,

24
00:00:57,750 --> 00:00:59,700
they have certainly become more powerful

25
00:00:59,700 --> 00:01:01,410
and it's expected that
they're gonna become

26
00:01:01,410 --> 00:01:03,510
even more powerful and
they're gonna become

27
00:01:03,510 --> 00:01:05,100
even more sophisticated.

28
00:01:05,100 --> 00:01:06,810
But it comes with some concerns, right?

29
00:01:06,810 --> 00:01:08,340
It comes with some concerns around

30
00:01:08,340 --> 00:01:10,440
what their potential impact is gonna be,

31
00:01:10,440 --> 00:01:12,840
what some of the ethical
ramifications are gonna be,

32
00:01:12,840 --> 00:01:14,160
and even what some of the impacts

33
00:01:14,160 --> 00:01:16,899
on society are potentially gonna be.

34
00:01:16,899 --> 00:01:21,480
And so as we kinda look at
the promise of agentic AI

35
00:01:21,480 --> 00:01:25,170
and agents, you know, there's
been a number of talks

36
00:01:25,170 --> 00:01:26,550
that I've listened to,

37
00:01:26,550 --> 00:01:28,920
but they could be used in
a number of different ways.

38
00:01:28,920 --> 00:01:32,910
Certainly they could be used
to effectively help humans

39
00:01:32,910 --> 00:01:36,540
perform tasks and achieve
business outcomes.

40
00:01:36,540 --> 00:01:40,320
And they could be used to augment
human intelligence, right?

41
00:01:40,320 --> 00:01:44,523
We think we're smart, they're
smarter, but they can help us,

42
00:01:44,523 --> 00:01:47,093
if they're used in the right way.

43
00:01:47,093 --> 00:01:50,400
They can help automate a
lot of some of the mundane

44
00:01:50,400 --> 00:01:54,690
and the time consuming tasks
that we typically have done

45
00:01:54,690 --> 00:01:58,140
over the course of the last little while.

46
00:01:58,140 --> 00:02:00,873
They can help improve
efficiency and productivity,

47
00:02:01,800 --> 00:02:06,000
and they can also help us
improve our decision making

48
00:02:06,000 --> 00:02:08,850
as well as the quality of the
responses that we're given.

49
00:02:09,750 --> 00:02:13,350
You know, there are studies
that come out all the time

50
00:02:13,350 --> 00:02:15,300
and, you know, this is
an example of three,

51
00:02:15,300 --> 00:02:17,460
and I think if we probably
wait a week or two weeks

52
00:02:17,460 --> 00:02:19,500
or three weeks, we're
gonna get more studies.

53
00:02:19,500 --> 00:02:22,230
But you kinda look and the
first one was something

54
00:02:22,230 --> 00:02:25,770
that McKinsey had come out
with that said, you know,

55
00:02:25,770 --> 00:02:28,560
agentic AI and agents
are gonna help contribute

56
00:02:28,560 --> 00:02:33,560
to an added value of about $4.4
trillion of profit annually.

57
00:02:36,960 --> 00:02:40,650
And if you look at something
that the BCG had come out with,

58
00:02:40,650 --> 00:02:42,840
they recognize that
agents are here to stay

59
00:02:42,840 --> 00:02:44,760
and that they're gonna
lead to an expansion

60
00:02:44,760 --> 00:02:49,760
of about 45% growth over the
course of the next five years.

61
00:02:51,540 --> 00:02:53,250
And Gartner has said that, basically,

62
00:02:53,250 --> 00:02:58,250
by 2028 one-third of
all of our interactions

63
00:02:58,560 --> 00:03:02,280
with generative AI, with Gen
AI, is gonna be through the use

64
00:03:02,280 --> 00:03:05,613
of action models or
through autonomous agents.

65
00:03:07,080 --> 00:03:07,950
Agents are everywhere!

66
00:03:07,950 --> 00:03:10,680
Again, walk around the show floor

67
00:03:10,680 --> 00:03:12,330
and everyone's talking about agents.

68
00:03:12,330 --> 00:03:14,250
It's not that they're only everywhere,

69
00:03:14,250 --> 00:03:16,290
it's that they come from everywhere.

70
00:03:16,290 --> 00:03:17,310
And they don't know where.

71
00:03:17,310 --> 00:03:19,770
You know, organizations
are faced with a dilemma

72
00:03:19,770 --> 00:03:23,460
where they're struggling
to get a handle on

73
00:03:23,460 --> 00:03:24,660
where did they originate?

74
00:03:24,660 --> 00:03:25,560
Who built them?

75
00:03:25,560 --> 00:03:26,610
How were they tested?

76
00:03:26,610 --> 00:03:28,650
What data were they tested on?

77
00:03:28,650 --> 00:03:30,210
And that's a concern,

78
00:03:30,210 --> 00:03:32,324
because they don't know what is being used

79
00:03:32,324 --> 00:03:34,233
within an organization.

80
00:03:35,402 --> 00:03:37,590
And so, when we look at...

81
00:03:37,590 --> 00:03:38,880
When we talk to our customers,

82
00:03:38,880 --> 00:03:41,820
and Diana and I will talk
about this in a little bit,

83
00:03:41,820 --> 00:03:43,860
but as we look at our
customers and talk to them

84
00:03:43,860 --> 00:03:46,170
about their AI strategy,

85
00:03:46,170 --> 00:03:47,820
obviously, they want
to get as much a return

86
00:03:47,820 --> 00:03:49,710
out of their AI as possible,

87
00:03:49,710 --> 00:03:52,096
but that's only gonna be possible

88
00:03:52,096 --> 00:03:54,000
if there's good governance,

89
00:03:54,000 --> 00:03:56,970
if there's good AI
governance also put in place.

90
00:03:56,970 --> 00:03:59,700
And so when we look at
some of the reasons why

91
00:03:59,700 --> 00:04:03,300
governance is critical, one
of the things that, you know,

92
00:04:03,300 --> 00:04:05,820
if we look probably
two or three years ago,

93
00:04:05,820 --> 00:04:06,900
it was probably a lot easier,

94
00:04:06,900 --> 00:04:08,580
maybe three to five years,

95
00:04:08,580 --> 00:04:11,343
it was probably a lot
easier to understand,

96
00:04:11,343 --> 00:04:13,980
when we were looking at
traditional and predictive ML,

97
00:04:13,980 --> 00:04:15,150
how many models were being used,

98
00:04:15,150 --> 00:04:17,850
what were the use cases that
they were being used for?

99
00:04:17,850 --> 00:04:22,050
Then we saw Gen AI and the RAD use cases,

100
00:04:22,050 --> 00:04:23,340
and we started to lose track.

101
00:04:23,340 --> 00:04:25,830
We started to figure out,
okay, well, we know potentially

102
00:04:25,830 --> 00:04:28,410
what LLMs, what foundation models we have,

103
00:04:28,410 --> 00:04:29,670
but we don't know what the use cases are.

104
00:04:29,670 --> 00:04:31,560
We don't know what all
the prompt templates are.

105
00:04:31,560 --> 00:04:33,150
And so we started to lose sight.

106
00:04:33,150 --> 00:04:36,540
Now with agentic, it's getting
even more apparent, right?

107
00:04:36,540 --> 00:04:40,591
We're losing the transparency
into who's building them,

108
00:04:40,591 --> 00:04:44,190
how they're being built,
where they're being built,

109
00:04:44,190 --> 00:04:45,750
and how they're being trained.

110
00:04:45,750 --> 00:04:48,600
And that's cause for some concern as well.

111
00:04:48,600 --> 00:04:52,440
The inability to explain behavior.

112
00:04:52,440 --> 00:04:56,040
Again, when it was
traditional predictive ML,

113
00:04:56,040 --> 00:04:58,080
you kind of knew that the
models were gonna run,

114
00:04:58,080 --> 00:05:01,800
they were gonna spit some
outcome or output out.

115
00:05:01,800 --> 00:05:02,730
Even with Gen AI,

116
00:05:02,730 --> 00:05:04,350
there was kind of some
human in the loop, right?

117
00:05:04,350 --> 00:05:05,183
There was.

118
00:05:05,183 --> 00:05:07,950
Somebody would put in a prompt,
would get a response back,

119
00:05:07,950 --> 00:05:09,540
and go, no, that doesn't look right,

120
00:05:09,540 --> 00:05:11,460
and was able to make a decision.

121
00:05:11,460 --> 00:05:16,460
With agents, especially
with multi agentic systems,

122
00:05:16,470 --> 00:05:18,060
as they start to talk to each other,

123
00:05:18,060 --> 00:05:19,680
and as they become autonomous,

124
00:05:19,680 --> 00:05:23,250
the inability to understand
the decisions that were made,

125
00:05:23,250 --> 00:05:26,163
the lack of explainability
is a major concern.

126
00:05:27,000 --> 00:05:28,020
And there's still some challenges

127
00:05:28,020 --> 00:05:31,023
around manual error-prone testing.

128
00:05:32,220 --> 00:05:37,220
There's even more concern
around bias, around drift,

129
00:05:37,230 --> 00:05:41,310
just around the general health
and the quality of agents.

130
00:05:41,310 --> 00:05:43,020
And then there's a number
of disparate tools.

131
00:05:43,020 --> 00:05:46,200
And, of course, not to
mention sort of the lack

132
00:05:46,200 --> 00:05:47,970
of those guardrails, right?

133
00:05:47,970 --> 00:05:50,670
Those safe safety guardrails
that are put in place

134
00:05:50,670 --> 00:05:52,440
to ensure that they're behaving the way

135
00:05:52,440 --> 00:05:55,200
that they're supposed to behave
and that they're being used

136
00:05:55,200 --> 00:05:57,750
the way that they're supposed to behave.

137
00:05:57,750 --> 00:06:00,510
So, as we kind of look
at watsonx.governance,

138
00:06:00,510 --> 00:06:01,440
we kind of...

139
00:06:01,440 --> 00:06:04,380
Not kind of, but we look at how to address

140
00:06:04,380 --> 00:06:08,340
some of these concerns
and some of these risks.

141
00:06:08,340 --> 00:06:10,980
So, the first thing that
we kinda look at is,

142
00:06:10,980 --> 00:06:13,770
how do we start the whole observability?

143
00:06:13,770 --> 00:06:16,770
How do we monitor the agents
for their performance?

144
00:06:16,770 --> 00:06:19,470
How do we know that they are performing

145
00:06:19,470 --> 00:06:21,900
the way that they're supposed to perform?

146
00:06:21,900 --> 00:06:25,830
How do we know that they're producing

147
00:06:25,830 --> 00:06:28,080
the output that they're
supposed to produce?

148
00:06:28,080 --> 00:06:30,150
And this is a continuous monitor,

149
00:06:30,150 --> 00:06:32,910
continuously monitoring and
raising those alarm bells

150
00:06:32,910 --> 00:06:34,950
if something should happen.

151
00:06:34,950 --> 00:06:36,840
There's the evaluation
capabilities, right?

152
00:06:36,840 --> 00:06:40,650
Evaluating at build time, at testing time,

153
00:06:40,650 --> 00:06:44,850
and continuously evaluating
even when they're in production,

154
00:06:44,850 --> 00:06:48,000
and ensuring that the
same quality is adhered to

155
00:06:48,000 --> 00:06:51,540
while in production that
was met when prior to them

156
00:06:51,540 --> 00:06:53,070
being deployed into production.

157
00:06:53,070 --> 00:06:54,060
And then the last is,

158
00:06:54,060 --> 00:06:57,930
it's not just good enough
to understand the behavior.

159
00:06:57,930 --> 00:07:00,420
It's not good enough to
just understand the score

160
00:07:00,420 --> 00:07:01,950
and the results that are coming out,

161
00:07:01,950 --> 00:07:03,600
but it's about the optimization.

162
00:07:03,600 --> 00:07:07,140
How do you know what is the
right agent to do for the job?

163
00:07:07,140 --> 00:07:10,260
How do you know if something
is starting to fail?

164
00:07:10,260 --> 00:07:12,900
If the quality, if it's
starting to hallucinate,

165
00:07:12,900 --> 00:07:14,670
how do I go back and fix it?

166
00:07:14,670 --> 00:07:16,530
What are the steps that I need to take?

167
00:07:16,530 --> 00:07:19,170
So, these are some
capabilities that we look at

168
00:07:19,170 --> 00:07:20,910
with watsonx.governance.

169
00:07:20,910 --> 00:07:24,243
And I'm just gonna walk through
kind of a quick demo here.

170
00:07:24,243 --> 00:07:26,250
It's only about a minute and a half.

171
00:07:26,250 --> 00:07:27,083
Maybe I shouldn't say quick.

172
00:07:27,083 --> 00:07:27,930
Maybe I should say lightning

173
00:07:27,930 --> 00:07:29,880
since this is a lightning round.

174
00:07:29,880 --> 00:07:31,884
But let me just set up
the demo real quick.

175
00:07:31,884 --> 00:07:35,038
We could remove that placeholder.

176
00:07:35,038 --> 00:07:38,880
Mark did confirm with me,
so it is the latest one.

177
00:07:38,880 --> 00:07:42,859
But what we're about to
see is a scenario where

178
00:07:42,859 --> 00:07:46,380
somebody wants to onboard a new use case

179
00:07:46,380 --> 00:07:49,159
for a banking tooling.

180
00:07:49,159 --> 00:07:50,550
And as part of that use case,

181
00:07:50,550 --> 00:07:53,340
they wanna onboard a new agent.

182
00:07:53,340 --> 00:07:54,300
And so what we're gonna go through

183
00:07:54,300 --> 00:07:55,347
and we're gonna identify the use case

184
00:07:55,347 --> 00:07:56,550
and identify some risks.

185
00:07:56,550 --> 00:07:58,500
So, again, just setting this up,

186
00:07:58,500 --> 00:07:59,970
it is gonna go a little bit quick,

187
00:07:59,970 --> 00:08:01,650
but, basically, what
we're looking at here,

188
00:08:01,650 --> 00:08:04,040
this is watsonx.governance and this is

189
00:08:04,040 --> 00:08:05,790
what we call the Governance Console.

190
00:08:05,790 --> 00:08:07,950
This is kind of the landing page

191
00:08:07,950 --> 00:08:11,010
and this gives you sort
of that landscape of AI

192
00:08:11,010 --> 00:08:14,340
and how it's being deployed
across the organization.

193
00:08:14,340 --> 00:08:15,990
So, this is gonna go quick,

194
00:08:15,990 --> 00:08:18,390
but, basically from this area,

195
00:08:18,390 --> 00:08:19,920
we're able to start drilling down

196
00:08:19,920 --> 00:08:21,600
and getting into the details.

197
00:08:21,600 --> 00:08:24,390
Now we can start seeing
the inventory of the agents

198
00:08:24,390 --> 00:08:27,280
that have been onboarded
within my organization,

199
00:08:27,280 --> 00:08:31,320
within my department,
within my enterprise.

200
00:08:31,320 --> 00:08:32,730
We can drill down even further

201
00:08:32,730 --> 00:08:34,740
to start looking at some of the metadata.

202
00:08:34,740 --> 00:08:37,740
We can see who built
it, when was it built,

203
00:08:37,740 --> 00:08:40,950
what is the purpose, so on and so forth.

204
00:08:40,950 --> 00:08:45,120
The other area is around
being somewhat risk averse.

205
00:08:45,120 --> 00:08:48,150
So, within watsonx.governance,
we have the ability

206
00:08:48,150 --> 00:08:50,160
through what we call a questionnaire

207
00:08:50,160 --> 00:08:51,930
to ask a series of questions.

208
00:08:51,930 --> 00:08:53,190
Now, this is a bit of a cooking show,

209
00:08:53,190 --> 00:08:55,710
so we're not gonna go through
and answer all 60 questions,

210
00:08:55,710 --> 00:08:57,540
but we're gonna get to
the outcome real quick.

211
00:08:57,540 --> 00:09:00,780
So, based off of the
responses to the questions,

212
00:09:00,780 --> 00:09:03,630
it'll automatically
identify what are the risks

213
00:09:03,630 --> 00:09:05,250
that we need to be concerned with,

214
00:09:05,250 --> 00:09:06,810
how do we go and mitigate those risks,

215
00:09:06,810 --> 00:09:08,970
what are the controls that
we need to put in place,

216
00:09:08,970 --> 00:09:11,070
and how do we have an audit that we know

217
00:09:11,070 --> 00:09:13,560
that we've actually
taken the necessary steps

218
00:09:13,560 --> 00:09:16,650
and the appropriate actions to
mitigate some of those risks

219
00:09:16,650 --> 00:09:17,760
and document that,

220
00:09:17,760 --> 00:09:19,365
so, when auditors come in, they'll know

221
00:09:19,365 --> 00:09:21,300
what steps have been taken.

222
00:09:21,300 --> 00:09:23,400
So, all of this could happen before

223
00:09:23,400 --> 00:09:25,320
we either onboard a new agent

224
00:09:25,320 --> 00:09:28,290
or do it at the time of the use case

225
00:09:28,290 --> 00:09:31,020
to understand here's the
use case, here's the desire,

226
00:09:31,020 --> 00:09:32,508
here's what we actually want to do.

227
00:09:32,508 --> 00:09:34,650
Here are the agents or the assets,

228
00:09:34,650 --> 00:09:36,900
it doesn't necessarily
need to be a single agent,

229
00:09:36,900 --> 00:09:37,950
it could be multiple,

230
00:09:37,950 --> 00:09:39,360
here are the agents that we want to use,

231
00:09:39,360 --> 00:09:41,250
the assets we want to use.

232
00:09:41,250 --> 00:09:43,050
And then we combine the two,

233
00:09:43,050 --> 00:09:46,080
so we understand what the use case is,

234
00:09:46,080 --> 00:09:48,600
what are the agents, and it
might be one or multiple,

235
00:09:48,600 --> 00:09:52,050
and that same agent might be
used on multiple use cases.

236
00:09:52,050 --> 00:09:55,560
So, understanding the
relationship between them.

237
00:09:55,560 --> 00:09:57,660
And then last, as we're going through this

238
00:09:57,660 --> 00:10:01,380
and understanding sort
of the risk posture,

239
00:10:01,380 --> 00:10:04,080
and then as we get into the
approval, it's now saying,

240
00:10:04,080 --> 00:10:05,400
yes, I approve this,

241
00:10:05,400 --> 00:10:07,260
let's go ahead and approve this use case

242
00:10:07,260 --> 00:10:09,900
and onboard the the agents.

243
00:10:09,900 --> 00:10:12,150
So, a bit of a shameless plug.

244
00:10:12,150 --> 00:10:14,640
You know, analysts agree
that watsonx.governance

245
00:10:14,640 --> 00:10:16,900
has been recognized as one of the leaders

246
00:10:18,120 --> 00:10:20,100
as part of AI governance.

247
00:10:20,100 --> 00:10:21,480
And so, with that, I think, you know,

248
00:10:21,480 --> 00:10:23,880
let's have a conversation
about what we just saw,

249
00:10:23,880 --> 00:10:26,100
what I was just talking about.

250
00:10:26,100 --> 00:10:28,830
So, quickly, Diana, I think
one of the things that.

251
00:10:28,830 --> 00:10:31,890
We've heard that governance kind of has

252
00:10:31,890 --> 00:10:36,000
this sort of notion that
it's more of a hindrance,

253
00:10:36,000 --> 00:10:37,290
more of a hurdle.

254
00:10:37,290 --> 00:10:41,153
I'm curious to get what
is your take on that?

255
00:10:41,153 --> 00:10:44,490
- So, I think that we
should look at AI governance

256
00:10:44,490 --> 00:10:47,910
as guardrails and not as a blocker.

257
00:10:47,910 --> 00:10:50,610
Traditionally governance
is seen as a blocker,

258
00:10:50,610 --> 00:10:53,370
but when we think of
it as how do we adopt?

259
00:10:53,370 --> 00:10:56,130
A lot of organizations
don't know how to get going

260
00:10:56,130 --> 00:11:00,090
and they're using the lack of a plan

261
00:11:00,090 --> 00:11:02,790
or the lack of guardrails as an inhibitor.

262
00:11:02,790 --> 00:11:05,340
And so when you actually
put governance in place

263
00:11:05,340 --> 00:11:07,290
and you put thoughtful governance in place

264
00:11:07,290 --> 00:11:09,300
on what you are going to accept,

265
00:11:09,300 --> 00:11:12,150
it allows your innovation to accelerate.

266
00:11:12,150 --> 00:11:15,450
Organizations and your innovative people

267
00:11:15,450 --> 00:11:17,610
now know what they need to do

268
00:11:17,610 --> 00:11:20,400
in order to get those use cases passed.

269
00:11:20,400 --> 00:11:25,400
A tool such as WatsonX
allows you to build it in,

270
00:11:25,650 --> 00:11:28,500
have use cases already approved,

271
00:11:28,500 --> 00:11:31,059
so a developer can go in and say,

272
00:11:31,059 --> 00:11:33,300
I can start from this use case,

273
00:11:33,300 --> 00:11:36,877
I can use this type of code,
I know that will get approved,

274
00:11:36,877 --> 00:11:41,070
and go through the workflow
and get it approved quicker.

275
00:11:41,070 --> 00:11:44,820
I have clients who have really great ideas

276
00:11:44,820 --> 00:11:46,350
and they're really excited for them,

277
00:11:46,350 --> 00:11:50,190
and they take nine months
to be even able to get

278
00:11:50,190 --> 00:11:54,120
the AI agent idea approved

279
00:11:54,120 --> 00:11:56,610
before they can even start to build,

280
00:11:56,610 --> 00:11:59,610
because they don't know
what the governance is.

281
00:11:59,610 --> 00:12:03,990
By having strong and clear
governance guardrails,

282
00:12:03,990 --> 00:12:06,060
it accelerates innovation.

283
00:12:06,060 --> 00:12:08,340
So we need to change our paradigm

284
00:12:08,340 --> 00:12:10,770
on how we look at governance.

285
00:12:10,770 --> 00:12:12,060
And by using a tooling,

286
00:12:12,060 --> 00:12:15,420
it allows us to share that information

287
00:12:15,420 --> 00:12:19,350
and create a streamlined
approach to getting it approved

288
00:12:19,350 --> 00:12:20,490
and monitoring it.

289
00:12:20,490 --> 00:12:22,710
- Yeah, I think that you're
a hundred percent right

290
00:12:22,710 --> 00:12:24,360
that so many people were nervous

291
00:12:24,360 --> 00:12:27,420
and saw governance as
being sort of that blocker,

292
00:12:27,420 --> 00:12:29,131
but, really, it can help accelerate

293
00:12:29,131 --> 00:12:31,830
and help generate the
return on investment.

294
00:12:31,830 --> 00:12:33,000
- Yeah!
- On the AI investment

295
00:12:33,000 --> 00:12:35,400
that customers have have made.

296
00:12:35,400 --> 00:12:36,930
- When you don't know the governance

297
00:12:36,930 --> 00:12:38,369
and you don't know how to adopt it,

298
00:12:38,369 --> 00:12:42,390
it actually prohibits
people from innovating

299
00:12:42,390 --> 00:12:44,610
and also encourages them to build it

300
00:12:44,610 --> 00:12:46,140
on the side of their desk.

301
00:12:46,140 --> 00:12:48,540
You don't know what's going
on in your environment

302
00:12:48,540 --> 00:12:50,820
and you're increasing your risk profile.

303
00:12:50,820 --> 00:12:51,660
So, you really need...

304
00:12:51,660 --> 00:12:54,300
You don't want shadow AI.

305
00:12:54,300 --> 00:12:56,460
You want people to create this.

306
00:12:56,460 --> 00:12:58,830
You don't want to actually stop them.

307
00:12:58,830 --> 00:13:01,650
People think, oh, if I
don't tell them how to do it

308
00:13:01,650 --> 00:13:03,178
or if I don't give them governance,

309
00:13:03,178 --> 00:13:05,250
then they're not going to do it.

310
00:13:05,250 --> 00:13:06,810
They're doing it anyway.

311
00:13:06,810 --> 00:13:09,030
So you want to provide those guardrails

312
00:13:09,030 --> 00:13:12,660
and provide people with an
instruction manual of sorts

313
00:13:12,660 --> 00:13:16,191
in order to know how to
accelerate their innovation.

314
00:13:16,191 --> 00:13:19,121
- What are you seeing as
some of the challenges

315
00:13:19,121 --> 00:13:23,070
that customers are seeing in
terms of being able to adopt

316
00:13:23,070 --> 00:13:25,440
or implement governance,
implement those processes,

317
00:13:25,440 --> 00:13:28,020
implement the practices and so on.

318
00:13:28,020 --> 00:13:30,330
And, by the way, we only have
seven and a half minutes, so.

319
00:13:30,330 --> 00:13:31,163
- Okay. (laughing)

320
00:13:31,163 --> 00:13:34,028
So, some of the things
that I personally see

321
00:13:34,028 --> 00:13:36,630
is they don't know...

322
00:13:36,630 --> 00:13:38,520
They don't even know
what they're looking at.

323
00:13:38,520 --> 00:13:40,440
This is all happening fast.

324
00:13:40,440 --> 00:13:43,500
No one necessarily is
an expert in this space.

325
00:13:43,500 --> 00:13:46,440
People just have maybe a few
more at bats than others.

326
00:13:46,440 --> 00:13:49,530
So, they don't know what
to do, so they don't do it.

327
00:13:49,530 --> 00:13:51,060
They hide their head in the sand.

328
00:13:51,060 --> 00:13:52,200
That's one thing.

329
00:13:52,200 --> 00:13:54,810
We don't know who is the
AI governance committee.

330
00:13:54,810 --> 00:13:57,240
We don't know who the right people are,

331
00:13:57,240 --> 00:14:01,320
so they can't make a decision
because there's no consensus.

332
00:14:01,320 --> 00:14:04,530
The old ways of working no longer work.

333
00:14:04,530 --> 00:14:06,840
And so they don't wanna trust it,

334
00:14:06,840 --> 00:14:11,220
they think 'cause they don't
have the risks assessment,

335
00:14:11,220 --> 00:14:12,750
and they don't have all these things,

336
00:14:12,750 --> 00:14:14,340
they're just gonna be able to stop it.

337
00:14:14,340 --> 00:14:15,810
You're not stopping it!

338
00:14:15,810 --> 00:14:17,430
So, you're not governing it,

339
00:14:17,430 --> 00:14:19,560
you don't know what the words are,

340
00:14:19,560 --> 00:14:21,300
and you don't know how to get going,

341
00:14:21,300 --> 00:14:23,460
so you're just not doing it.

342
00:14:23,460 --> 00:14:26,490
But that's exposing your
organization to so much risk.

343
00:14:26,490 --> 00:14:28,110
- Yeah, I think, you know, if you...

344
00:14:28,110 --> 00:14:30,690
There was a funny story
where you're in a boardroom

345
00:14:30,690 --> 00:14:31,950
and you ask a group of people,

346
00:14:31,950 --> 00:14:35,910
whose responsibility is
it for AI governance?

347
00:14:35,910 --> 00:14:37,500
Everybody would look around and go,

348
00:14:37,500 --> 00:14:40,140
well, maybe that person, but,
at the end, it's everybody's.

349
00:14:40,140 --> 00:14:40,973
- Everybody, yeah!

350
00:14:40,973 --> 00:14:43,830
- And, you're right that they
don't know how to get started.

351
00:14:43,830 --> 00:14:44,663
Absolutely!

352
00:14:46,020 --> 00:14:48,780
What are you seeing, how do
you see sort of the agentic

353
00:14:48,780 --> 00:14:53,780
and agents sort of increasing
the need for agentic?

354
00:14:54,570 --> 00:14:56,670
- I think that with the increase

355
00:14:56,670 --> 00:15:00,030
and the promise of agentic and agents,

356
00:15:00,030 --> 00:15:03,270
it makes AI governance
even more important,

357
00:15:03,270 --> 00:15:06,120
because you need to know that that agent

358
00:15:06,120 --> 00:15:08,100
is now autonomous, potentially,

359
00:15:08,100 --> 00:15:11,160
is actually doing what it
says it's supposed to do.

360
00:15:11,160 --> 00:15:13,440
And through a tool like WatsonX,

361
00:15:13,440 --> 00:15:15,540
you can monitor it for drift,

362
00:15:15,540 --> 00:15:16,920
you can monitor it for bias,

363
00:15:16,920 --> 00:15:20,610
you can actually set these
thresholds and understand,

364
00:15:20,610 --> 00:15:22,950
is my agent performing?

365
00:15:22,950 --> 00:15:25,260
Is it doing what I said
it's supposed to do?

366
00:15:25,260 --> 00:15:28,500
Is it acting in the use case
that I've designed it to?

367
00:15:28,500 --> 00:15:30,660
And is it continuing to perform?

368
00:15:30,660 --> 00:15:33,510
It allows you to get out
of pencils and spreadsheets

369
00:15:33,510 --> 00:15:35,970
and allows you to have
a better understanding

370
00:15:35,970 --> 00:15:39,020
that it is actually doing
what it says it's going to do.

371
00:15:39,020 --> 00:15:41,250
I think that's so
important with the invent,

372
00:15:41,250 --> 00:15:43,320
with agents and the adoption of agents,

373
00:15:43,320 --> 00:15:46,320
especially agents doing
agents on upon agents.

374
00:15:46,320 --> 00:15:48,450
If you don't have a governance tooling,

375
00:15:48,450 --> 00:15:50,520
how do you know what's happening?

376
00:15:50,520 --> 00:15:53,910
And with the dashboard
you can see if it pops up

377
00:15:53,910 --> 00:15:55,290
if there's a problem.

378
00:15:55,290 --> 00:15:56,730
- Yeah, I just think that the whole

379
00:15:56,730 --> 00:16:00,240
autonomous nature of things
and not knowing, as you said,

380
00:16:00,240 --> 00:16:01,950
sort of the agents on top of agents.

381
00:16:01,950 --> 00:16:04,677
And, you know, like I said before,

382
00:16:04,677 --> 00:16:07,560
if you put a prompt in,
if you ask a question,

383
00:16:07,560 --> 00:16:09,630
sort of the Gen AI, the RAD use case,

384
00:16:09,630 --> 00:16:11,310
you get a response back, as a human,

385
00:16:11,310 --> 00:16:12,450
you can make a decision-
- Yes.

386
00:16:12,450 --> 00:16:14,880
- About whether or not
it's the right response.

387
00:16:14,880 --> 00:16:18,060
With agents, if you make a small mistake

388
00:16:18,060 --> 00:16:20,317
on that initial agent that's
calling the second one-

389
00:16:20,317 --> 00:16:21,990
- Yes!
- It's just gonna continue

390
00:16:21,990 --> 00:16:22,980
to get amplified.

391
00:16:22,980 --> 00:16:24,465
Absolutely!
- Absolutely!

392
00:16:24,465 --> 00:16:25,298
Absolutely!

393
00:16:25,298 --> 00:16:29,610
- Is there anything in specific that you,

394
00:16:29,610 --> 00:16:34,050
or could guide or advise the audience on

395
00:16:34,050 --> 00:16:36,240
when assessing agents

396
00:16:36,240 --> 00:16:41,010
and looking for different
kinds of agents to be used?

397
00:16:41,010 --> 00:16:43,350
- So, when I look across the landscape,

398
00:16:43,350 --> 00:16:45,150
I start with a process

399
00:16:45,150 --> 00:16:49,350
and I identify where the pain
points are in your process

400
00:16:49,350 --> 00:16:51,390
and I focus on outcomes.

401
00:16:51,390 --> 00:16:53,880
I know, probably, a lot
of people talk about this,

402
00:16:53,880 --> 00:16:56,400
but you should not automate your tasks

403
00:16:56,400 --> 00:16:59,190
and the specific things you're doing now.

404
00:16:59,190 --> 00:17:01,560
You wanna look across your
landscape of your process

405
00:17:01,560 --> 00:17:04,080
and identify where your opportunities are

406
00:17:04,080 --> 00:17:06,990
and then automate that outcome level.

407
00:17:06,990 --> 00:17:09,600
And then you can have
agents automating on top,

408
00:17:09,600 --> 00:17:13,410
monitoring that to make sure
it's performing as planned,

409
00:17:13,410 --> 00:17:17,370
but you don't want to just
automate your bad process.

410
00:17:17,370 --> 00:17:19,560
Like, when I used to do
software development,

411
00:17:19,560 --> 00:17:22,170
people would say, I really
want this red button.

412
00:17:22,170 --> 00:17:24,120
They didn't really want the red button.

413
00:17:24,120 --> 00:17:25,340
That was their pain point.

414
00:17:25,340 --> 00:17:28,290
Don't automate that red button.

415
00:17:28,290 --> 00:17:30,180
Look at what your outcome needs to be

416
00:17:30,180 --> 00:17:32,370
and that's how you can
design and build your agents.

417
00:17:32,370 --> 00:17:33,660
- Okay.

418
00:17:33,660 --> 00:17:37,936
Any sort of last thoughts
that you wanted to share

419
00:17:37,936 --> 00:17:40,560
that I didn't ask you about, or?

420
00:17:40,560 --> 00:17:43,860
- I think, to me, one of the
things I'm most excited about

421
00:17:43,860 --> 00:17:46,500
as adopting and looking across this is

422
00:17:46,500 --> 00:17:50,310
how you can get so much more with this.

423
00:17:50,310 --> 00:17:54,810
This is no longer prompt
engineering or machine learning.

424
00:17:54,810 --> 00:17:55,710
Those were ways...

425
00:17:55,710 --> 00:17:57,300
You could have done those things

426
00:17:57,300 --> 00:17:59,370
using your traditional means.

427
00:17:59,370 --> 00:18:02,580
With the evolution of where we sit today,

428
00:18:02,580 --> 00:18:05,370
you have to think about
an AI governance tool.

429
00:18:05,370 --> 00:18:08,790
If you don't have an AI
governance tool in place,

430
00:18:08,790 --> 00:18:11,910
it's going to make it really
difficult for you to know

431
00:18:11,910 --> 00:18:14,850
that the agents you're
installing in your building

432
00:18:14,850 --> 00:18:18,120
are actually performing, you're
getting the value from them,

433
00:18:18,120 --> 00:18:21,180
and you don't have a bunch of people

434
00:18:21,180 --> 00:18:25,680
building things on their desk
that are now in your system

435
00:18:25,680 --> 00:18:30,390
that expose you to penetration,
expose you to bias,

436
00:18:30,390 --> 00:18:32,610
or other things that could cause

437
00:18:32,610 --> 00:18:35,070
a lot of risk into your environment.

438
00:18:35,070 --> 00:18:38,520
And I think that a tooling
helps you understand

439
00:18:38,520 --> 00:18:41,223
what you're building,
what you're deploying.

440
00:18:42,587 --> 00:18:46,890
IBM WatsonX is hybrid for purpose, open,

441
00:18:46,890 --> 00:18:50,640
so it's not telling you
how to build your agents,

442
00:18:50,640 --> 00:18:52,140
it's not telling you
what tooling it can do.

443
00:18:52,140 --> 00:18:54,660
It can monitor your entire environment.

444
00:18:54,660 --> 00:18:57,180
I think it's a really strong platform

445
00:18:57,180 --> 00:19:00,720
that really opens you up to
accelerate your innovation.

446
00:19:00,720 --> 00:19:02,193
- Great, perfect, thank you.

447
00:19:03,300 --> 00:19:05,460
So, I think, with that,
there's a couple of things,

448
00:19:05,460 --> 00:19:08,790
so, obviously, we're working
on watsonx.governance,

449
00:19:08,790 --> 00:19:13,533
on delivering out a development
kit around AgentOps.

450
00:19:14,430 --> 00:19:18,000
And we have a booth back
there with a big IBM sign,

451
00:19:18,000 --> 00:19:19,590
and there's a big bumblebee outside.

452
00:19:19,590 --> 00:19:21,810
So, with that, thank you
very much for your time today

453
00:19:21,810 --> 00:19:23,700
and enjoy the rest of the conference.

454
00:19:23,700 --> 00:19:27,023
- [Diana] Thank you.
(audience applaud)

