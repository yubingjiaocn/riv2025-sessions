# AWS re:Invent 2025 - OPN 414: VLM 从测试到生产的完整旅程

## 会议概述

本次会议由 AWS 高级 GenAI 专家 F Nuan 和高级开源机器学习工程师 Amry Shiv 共同主讲，深入探讨了大语言模型(LLM)的部署之旅。会议分为两个主要部分：首先介绍了从原型到生产环境部署开源模型的四阶段优化旅程，重点讲解了如何通过 VLM 推理引擎降低成本、提升性能并获得更好的控制能力。其次详细介绍了 AI on EKS 开源项目，展示了在 Kubernetes 环境中部署和扩展 LLM 的实用模式。

演讲者强调，虽然早期许多团队使用 OpenAI 等商业服务进行原型开发，但在转向生产环境时往往会因高昂的运营成本而感到意外。好消息是开源模型已经赶上了前沿模型的性能，能够提供更好的成本效益、更多的模型选择和更强的控制能力。通过系统化的优化策略，组织可以将 GPU 利用率从 40-50% 提升到 80% 以上，实现 5 倍的吞吐量提升和 80% 的单 token 成本节省。

会议还展示了 Amazon Rufus 购物助手如何在 Prime Day 期间使用 VLM 扩展到超过 80,000 个 Inferentia 和 Trainium 芯片的实际案例，证明了这些技术在大规模生产环境中的可行性。

## 详细时间线与关键要点

### **00:00 - 会议开场与背景介绍**
- 会议主题：OPN 414 - VLM 从测试到生产的完整旅程
- 演讲者介绍：F Nuan（高级 GenAI 专家）和 Amry Shiv（高级开源 ML 工程师）
- 内容分工：F Nuan 负责 LLM 旅程和客户发现，Amry 负责 AI on EKS 项目和实用部署模式

### **01:30 - LLM 时代的典型挑战**
- 早期常见做法：使用 OpenAI 构建原型和问答聊天机器人
- 主要问题：从原型转向生产时，实际运营成本远超预期
- 解决方案：开源模型已赶上前沿模型，提供更好的成本控制和模型选择

### **02:45 - 第一阶段：基础设施（Foundation）**
- 问题识别：团队各自使用信用卡订阅模型服务（OpenAI、Claude、Bedrock）导致成本不透明、缺乏统一策略和合规性
- 解决方案：部署 AI 网关统一管理所有请求
- AI 网关功能：
  - 按用户、团队或项目追踪成本
  - 执行内容过滤和 PII 数据保护策略
  - 审计所有请求并控制访问权限
  - 存储和管理不同的 API 密钥
- 推荐框架：Envoy AI Gateway（Kubernetes 用户）、LiteLLM（Python 开发者）、OpenRouter（托管网关）

### **05:20 - 第二阶段：优化（Optimization）**
- 问题：直接使用 Hugging Face Transformer 库和 FastAPI/Flask 部署模型，GPU 利用率仅 40-50%
- 后果：基础设施利用不足、成本高昂、用户体验差

### **06:15 - Transformer 架构深度解析**
- 三个关键阶段：
  1. **Tokenization（分词）**：将文本分解为 token，CPU 密集型，通常不是瓶颈
  2. **Prefill（预填充）**：处理提示词生成 KV cache，GPU/加速器密集型
  3. **Decode（解码）**：使用 KV cache 预测下一个 token，内存密集型，GPU 利用率低
- 上下文窗口影响：序列长度翻倍需要内存资源翻倍来存储 KV cache

### **08:30 - VLM 推理引擎介绍**
- VLM 特点：
  - 开源项目，属于 Linux AI & Data Foundation 和 PyTorch Foundation
  - 超过 60,000 GitHub stars，每月 800+ PR，1,700+ 贡献者
  - 支持 100+ 模型架构
  - 跨加速器支持：Nvidia GPU、AWS Trainium/Inferentia、AMD 等

### **09:45 - VLM 核心优化技术**
- **Page Attention**：类似 GPU 虚拟内存，分页加载模型权重而非全部加载，释放更多资源处理批次
- **Continuous Batching（连续批处理）**：动态调整批次大小，平衡吞吐量和延迟
- **Quantization（量化）**：将 FP32 精度降低到 FP16 或 FP8，减少内存占用
- 性能提升：GPU 利用率提升至 80%，吞吐量提升 5 倍，单 token 成本节省 80%

### **12:00 - 其他推理引擎选项**
- 提及的开源推理引擎：SGLang、Nvidia Triton Inference Server
- 强调所有引擎都是开源的，用户可根据需求选择

### **12:30 - 第三阶段：内存管理（Memory Management）**
- 问题场景：
  - RAG 应用和工具调用需要重复处理系统提示词
  - 不是所有 token 都同等重要，某些可以缓存复用
- Token 比例反转趋势：
  - 早期：输入密集型（摘要、翻译）
  - 中期：平衡型（多轮对话、代码生成）
  - 现在：输出密集型（推理模型生成大量中间 token）

### **15:00 - KV Cache 卸载技术**
- **Nvidia NeMo**：跨 GPU、节点、存储层传输 KV cache
- **存储选项**：NVMe、FSx、EFS 支持直接 GPU 数据加载，绕过 CPU 和操作系统
- **AI 路由器**：感知 KV cache 位置和容量，将请求路由到已处理过 KV cache 的工作节点，优化缓存命中率

### **16:30 - 缓存策略与性能基准**
- 综合策略：前缀缓存 + KV cache 卸载 + 语义缓存 + AI 路由器
- Nvidia Dynamo 基准测试（100,000 请求，Llama 70B）：
  - 首 token 时间提升 3 倍
  - 整体延迟提升 2 倍
- 多轮对话基准测试（20 轮对话，15 用户，Qwen 3 8B）：
  - 首 token 时间提升 2-12 倍

### **18:00 - KV Cache 框架推荐**
- 开源框架：LM Cache、MoonCake、Nvidia Dynamo

### **18:30 - 第四阶段：分布式推理（Scale/Distributed Inference）**
- 适用场景：
  - 高流量（每秒/分钟数千请求）
  - 单节点无法容纳的大型模型
- 建议：如果可以保持单节点部署，尽量保持简单

### **19:15 - 并行化的三个维度**
- **Data Parallelism（数据并行）**：跨节点复制模型，分片数据
- **Tensor Parallelism（张量并行）**：分片模型权重，数据通过不同权重
- **Pipeline Parallelism（流水线并行）**：分片模型层，数据跨层流水线处理

### **20:30 - Expert Parallelism（专家并行）**
- 背景：最新开源模型（Qwen、DeepSeek、Gemini）使用混合专家（MoE）架构
- DeepSeek 示例：256 个专家，推理时仅激活 8 个，大幅提升效率
- 通信模式：
  - All-to-all token 交换或点对点通信
  - 通信量是传统方法的 3-6 倍
  - 非对称通信，难以预测节点间通信模式
- 对比：数据/张量/流水线并行的通信模式对称且固定

### **22:00 - 解耦架构（Disaggregated Architecture）**
- 问题：Prefill（GPU 密集）和 Decode（内存密集）在同一节点时，Decode 阶段 GPU 闲置
- 解决方案：将 Prefill 和 Decode 分离到不同集群
  - Prefill 集群：更多 GPU/算力
  - Decode 集群：更多内存
- 使用 Nvidia NeMo 在集群间传输 KV cache
- 优势：独立扩展两个阶段，优化资源分配

### **23:30 - 分布式推理性能基准**
- Nvidia 基准测试（解耦架构）：
  - 单节点：1.3 倍吞吐量提升
  - 双节点：2 倍吞吐量提升
  - 节点越多，吞吐量提升越明显

### **24:15 - 分布式推理框架**
- 推荐框架：Nvidia Dynamo、ByteDance AIBrix、Red Hat LLMD
- 所有框架均为开源

### **25:00 - 四阶段旅程总结**
1. Foundation：部署 AI 网关实现可见性和控制
2. Optimization：使用推理引擎提升 GPU 利用率和吞吐量
3. Memory Management：实施 KV cache 策略降低延迟
4. Scale：使用解耦架构和并行策略实现分布式推理

### **26:00 - 实际案例：Amazon Rufus**
- Amazon Rufus 购物助手在 Prime Day 期间使用 VLM 扩展到 80,000+ Inferentia 和 Trainium 芯片
- 提供博客文章链接供深入了解

### **26:45 - AI on EKS 项目介绍（Amry Shiv）**
- 开源仓库，提供三层支持：
  1. **基础设施层**：针对训练、推理、MLOps、Agent 的专用架构
  2. **部署层**：蓝图、Helm charts 实现各种模式
  3. **指导层**：最佳实践和使用指南

### **28:00 - AI on EKS 基础设施特点**
- 模块化设计：可轻松替换组件（如向量数据库）
- 硬件优化：支持 x86-64、AWS Neuron/Inferentia、Graviton
- 成本效益：精简基础设施 + Karpenter 自动扩展
- 灵活性：支持多种推理引擎和生产化栈

### **29:30 - AI on EKS 部署层**
- 提供多种场景蓝图：推理、MLOps、训练、Agent
- 灵活支持：VLM、AIBrix、LLMD 等不同工具
- 可扩展且成本优化

### **30:15 - AI on EKS 指导层**
- 性能和成本优化建议
- 工具选择的实用指导和替代方案
- 支持不同阶段用户：从零基础到全球生产环境
- 覆盖多种主题

### **31:00 - Inference Ready Cluster 架构详解**
- 使用 4 个可用区（US West 2）提高 GPU 实例获取成功率
- 拓扑感知部署：考虑跨可用区延迟和成本
- 预装所有必需控制器：支持 LLM 部署、基准测试等
- 部署简单：3 行命令（克隆仓库、进入目录、运行安装脚本）

### **32:30 - 基础设施定制化**
- 配置文件支持模块化启用/禁用组件
- 可调整可用区数量、区域、观测栈等
- 默认所有组件关闭，按需启用

### **33:15 - 部署 Charts 介绍**
- 支持多种模型类型：LLM、文本转图像（Diffusion）
- 推理服务器选项：VLM、Diffusers、Llama.cpp（Graviton/ARM）、Triton 等
- 提供基准测试工具和其他控制器

### **34:00 - Helm Chart 使用示例**
- 配置简单：修改模型名称即可切换模型（直接使用 Hugging Face 模型名）
- 部署命令：添加仓库、更新、安装
- 提供多个模板供不同场景使用

### **35:00 - LLM 部署旅程：模型测试阶段**
- 目标用户：从未部署过 LLM 的团队
- 目的：验证模型是否工作、输出是否合理、学习使用方法
- 示例模型：Qwen 3
- 特点：
  - 保持基础设施精简
  - 一致的配置和端点
  - 可定制的模型参数

### **36:30 - 模型测试架构演示**
- 初始状态：推理集群无 GPU 节点（成本优化）
- 部署流程：
  1. 安装 Helm chart 创建待处理的 VLM pod
  2. Karpenter 检测到未调度 pod，自动创建节点
  3. 拉取容器镜像
  4. 下载模型权重
  5. 模型服务器就绪，可接收请求
- 访问方式：集群内 pod、端口转发或 Ingress

### **38:00 - 多推理引擎支持**
- 演示 VLM 和 Triton 切换：仅需修改镜像配置
- 配置文件其他部分保持一致
- 端点保持一致性（除版本差异）

### **39:00 - 基准测试阶段（Benchmarking）**
- 目的：
  - 验证模型输出质量
  - 测试模型服务器性能
  - 优化参数以满足 SLO
  - **关键建议：使用自己的数据进行测试**
- 原因：Token 分布影响性能，合成数据可能与生产环境不符

### **40:30 - 基准测试工具**
- **Inference Perf**：详细指南可通过 QR 码访问
- **Guide LLM**：VLM 生态系统工具，提供补充功能
  - 运行测试并输出结果
  - 提供主观质量评估
  - 示例：使用 Databricks 真实数据集测试

### **42:00 - Guide LLM 输出示例**
- 展示指令（prompts）和预期响应对比
- 帮助主观判断输出是否符合预期
- 可根据结果调整提示词或更换模型
- 下一步：深入评估（evals）

### **43:00 - 模型扩展阶段（Model Scaling）**
- 问题：单实例优化后，如何应对生产环境流量变化
- 挑战：
  - 始终运行 200 个实例？浪费资源和成本
  - 从低副本数开始？可能导致超时和延迟问题
- 解决方案：使用 Ray 实现自动扩展（VLM 生态系统）

### **44:00 - 自动扩展策略**
- 根据流量增加而扩展（非"以防万一"）
- 流量下降时缩减（如夜间低流量时段）
- 成本效益优化
- 可配置的自动扩展参数

### **44:45 - Ray VLM Helm Chart**
- 配置：从 VLM 切换到 Ray VLM
- 暴露自动扩展专用选项

### **45:30 - 自动扩展实际演示**
- 场景：消费者发送大量请求，响应速度跟不上
- Ray head pod 检测到请求队列堆积
- 自动创建新副本：
  1. 扩展实例（非预先运行）
  2. 拉取容器镜像
  3. 下载模型权重
  4. 新副本就绪，处理请求
- 流量下降时自动缩减

### **47:00 - 冷启动问题**
- 挑战：容器镜像和模型权重都是数十 GB
- 传统流程：创建节点 → 拉取容器 → 拉取权重（耗时长）
- 用户影响：等待时间过长可能导致用户流失

### **48:00 - 冷启动优化策略**
- **容器缓存**：使用 EBS 快照，节点启动时容器镜像已存在
- **模型缓存**：将模型打包到容器或存储在易访问位置
- 问题：需要维护缓存，镜像和模型更新时需同步

### **49:00 - AI on EKS 优化方案**
- **Soci（Seekable OCI）**：
  - Inference Ready Cluster 默认启用
  - 并行拉取所有容器层，加速容器启动
- **Model Streaming**：
  - VLM 支持的功能
  - 使用 Runai Model Streamer
  - 更快地拉取模型权重到容器

### **50:00 - 模型流式传输演示**
- 提供简单模板：输入模型名称即可自动配置流式传输
- 显著减少模型加载时间

### **50:30 - 会议结束**
- 总结了从基础设施到生产环境的完整 LLM 部署旅程
- 强调 AI on EKS 开源项目的实用性和易用性
- 鼓励参会者访问仓库并尝试部署

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


关键资源：
- AI on EKS GitHub 仓库
- Guide LLM 工具（VLM 生态系统）
- Inference Perf 基准测试指南
- Amazon Rufus 案例研究博客