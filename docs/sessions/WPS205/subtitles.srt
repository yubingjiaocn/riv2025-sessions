1
00:00:01,050 --> 00:00:04,023
- Good afternoon and welcome to re:Invent.

2
00:00:07,530 --> 00:00:09,250
Imagine tracking

3
00:00:10,320 --> 00:00:13,653
a developing hurricane
in the Atlantic Ocean.

4
00:00:15,690 --> 00:00:17,073
Many lives at stake.

5
00:00:18,300 --> 00:00:22,353
As coastal communities
prepare for the impact,

6
00:00:24,000 --> 00:00:28,953
behind every forecast, behind
every evacuation order,

7
00:00:29,820 --> 00:00:33,480
behind every decision that saves lives,

8
00:00:33,480 --> 00:00:37,230
there is massive amount
of computational power

9
00:00:37,230 --> 00:00:38,283
that is at work.

10
00:00:40,080 --> 00:00:43,170
Today, along with my esteemed
colleague David Michaud

11
00:00:43,170 --> 00:00:47,640
from National Oceanic and
Atmospheric Administration

12
00:00:47,640 --> 00:00:50,373
and Rayette Toles-Abdullah from AWS,

13
00:00:51,510 --> 00:00:55,870
we will explore how
high-performance computing

14
00:00:57,090 --> 00:01:01,290
is evolving from
traditional supercomputers

15
00:01:01,290 --> 00:01:03,213
to AI-led forecasting,

16
00:01:04,290 --> 00:01:08,790
transforming the way in
which we predict, track,

17
00:01:08,790 --> 00:01:13,533
forecast dangerous weather
events that impact our planet.

18
00:01:15,870 --> 00:01:20,100
The convergence of
high-performance computing

19
00:01:20,100 --> 00:01:21,610
and artificial intelligence

20
00:01:23,460 --> 00:01:25,383
is an evolutionary technology.

21
00:01:26,520 --> 00:01:29,680
However, it is revolutionizing how quickly

22
00:01:30,660 --> 00:01:35,660
and how accurately we can
respond to very dangerous events

23
00:01:36,570 --> 00:01:40,350
and therefore save more lives.

24
00:01:40,350 --> 00:01:42,300
We're excited to take you

25
00:01:42,300 --> 00:01:44,050
through this journey with us today.

26
00:01:45,870 --> 00:01:49,110
Our presentation is gonna
be organized as follows.

27
00:01:49,110 --> 00:01:51,990
First, let's talk about

28
00:01:51,990 --> 00:01:56,580
what the market is doing in
terms of this convergence of AI

29
00:01:56,580 --> 00:01:58,143
and high-performance computing.

30
00:01:59,280 --> 00:02:02,673
According to Hyperion
Research, back in 2023,

31
00:02:03,720 --> 00:02:06,400
the HPC market was about $37 billion

32
00:02:07,980 --> 00:02:12,980
and it grew by 24% in 2024.

33
00:02:14,010 --> 00:02:17,283
We expect the numbers to
be even higher in 2025.

34
00:02:19,830 --> 00:02:24,830
By 2028, one-third of the HPC market

35
00:02:25,050 --> 00:02:27,033
is expected to be on the cloud.

36
00:02:28,380 --> 00:02:33,380
And by 2029, the converged
AI and HPC market

37
00:02:33,990 --> 00:02:36,573
is expected to be $49 billion.

38
00:02:38,100 --> 00:02:41,610
That is huge validation of the market

39
00:02:41,610 --> 00:02:43,960
by some of the early
results that we're seeing,

40
00:02:44,940 --> 00:02:46,620
and also,

41
00:02:46,620 --> 00:02:49,470
it's kind of following the innovations

42
00:02:49,470 --> 00:02:54,270
like agencies like NOAA
are actually coming up with

43
00:02:54,270 --> 00:02:57,243
and executing with HPC at scale.

44
00:02:58,380 --> 00:03:01,170
So then I will invite
David Michaud at that time

45
00:03:01,170 --> 00:03:02,910
to come up to the stage,

46
00:03:02,910 --> 00:03:07,620
and he will do a deep dive
into a NOAA case study.

47
00:03:07,620 --> 00:03:10,200
He'll talk about NOAA's mission.

48
00:03:10,200 --> 00:03:13,320
He'll talk about the
important work that they do,

49
00:03:13,320 --> 00:03:15,270
how it impacts lives,

50
00:03:15,270 --> 00:03:18,450
and how massive amount
of computing at scale

51
00:03:18,450 --> 00:03:22,533
is used at NOAA to do
various mission outcomes.

52
00:03:23,430 --> 00:03:26,730
Then Rayette Toles-Abdullah
will come to the stage.

53
00:03:26,730 --> 00:03:30,660
As a Principal Architect
at AWS, she's been leading

54
00:03:30,660 --> 00:03:33,540
the capability development
and service development

55
00:03:33,540 --> 00:03:38,490
for supercomputing and other
cloud opportunities at NOAA.

56
00:03:38,490 --> 00:03:39,840
She will do a deep dive

57
00:03:39,840 --> 00:03:41,790
into these architectural capabilities

58
00:03:41,790 --> 00:03:43,803
that power NOAA's missions.

59
00:03:44,640 --> 00:03:47,430
And then finally, I will
come back to the stage

60
00:03:47,430 --> 00:03:50,190
and I will discuss some
of the new investments

61
00:03:50,190 --> 00:03:53,190
that AWS has committed in this space,

62
00:03:53,190 --> 00:03:56,310
as well as give you a couple
of additional examples

63
00:03:56,310 --> 00:03:58,410
of innovation that we're seeing

64
00:03:58,410 --> 00:04:02,250
in the converged HPC and
AI space in the cloud

65
00:04:02,250 --> 00:04:04,140
by some of our other partners.

66
00:04:04,140 --> 00:04:05,640
So with that,

67
00:04:05,640 --> 00:04:07,410
I would be honored to have David Michaud

68
00:04:07,410 --> 00:04:08,733
join us on the stage.

69
00:04:09,604 --> 00:04:12,643
(audience applauding)

70
00:04:12,643 --> 00:04:15,636
- Thank you.

71
00:04:15,636 --> 00:04:17,520
All right. Good afternoon, everyone.

72
00:04:17,520 --> 00:04:19,050
My name's Dave Michaud

73
00:04:19,050 --> 00:04:23,460
and I work for the
National Weather Service.

74
00:04:23,460 --> 00:04:25,320
My role at the National Weather Service

75
00:04:25,320 --> 00:04:30,320
is essentially responsible
for managing the operations

76
00:04:31,290 --> 00:04:35,670
of the networking that
we have, enterprise-wide,

77
00:04:35,670 --> 00:04:37,320
all the supercomputing,

78
00:04:37,320 --> 00:04:39,750
all the data dissemination systems,

79
00:04:39,750 --> 00:04:44,750
and also, support locally
for all the national centers

80
00:04:45,660 --> 00:04:48,963
that we have in College Park, Maryland.

81
00:04:50,100 --> 00:04:52,290
I've been with NOAA for about 30 years.

82
00:04:52,290 --> 00:04:55,140
It's my life and my
passion, so I'm very excited

83
00:04:55,140 --> 00:04:58,983
to be here today to talk
to you about what we do.

84
00:05:02,160 --> 00:05:04,200
So first, a little bit about the mission

85
00:05:04,200 --> 00:05:05,520
of the National Weather Service.

86
00:05:05,520 --> 00:05:08,460
So oftentimes when I talk to people

87
00:05:08,460 --> 00:05:11,250
about the National Weather Service,

88
00:05:11,250 --> 00:05:15,120
they don't really realize
the diversity of what we do.

89
00:05:15,120 --> 00:05:16,920
A lot of people think about,

90
00:05:16,920 --> 00:05:20,040
okay, what do I need to
wear the following day?

91
00:05:20,040 --> 00:05:21,690
Do I need to bring an umbrella?

92
00:05:21,690 --> 00:05:24,690
We're much more than that in
the National Weather Service.

93
00:05:24,690 --> 00:05:26,373
We have a very diverse mission.

94
00:05:27,450 --> 00:05:30,480
Our domain space covers anywhere

95
00:05:30,480 --> 00:05:35,220
from the entire Atlantic
Basin to the mainland US

96
00:05:35,220 --> 00:05:38,550
to the entire Pacific Basin
in the Northern Hemisphere

97
00:05:38,550 --> 00:05:41,703
in terms of the domain and
scale of what we're forecasting.

98
00:05:42,780 --> 00:05:46,620
And the breadth is astounding.

99
00:05:46,620 --> 00:05:49,410
So we do everything from
the surface of the Sun

100
00:05:49,410 --> 00:05:50,640
to the bottom of the sea.

101
00:05:50,640 --> 00:05:55,170
So we'll monitor the activity
on the surface of the Sun

102
00:05:55,170 --> 00:06:00,170
and forecast electromagnetic
interruptions or storms

103
00:06:00,540 --> 00:06:03,450
that are produced through
coronal mass ejections

104
00:06:03,450 --> 00:06:06,333
and monitor those as
they approach the Earth.

105
00:06:07,230 --> 00:06:09,300
Again, we have the
traditional forecasting,

106
00:06:09,300 --> 00:06:12,570
but then we also have,
like Mickey mentioned,

107
00:06:12,570 --> 00:06:16,290
the tropical forecasting with
the National Hurricane Center

108
00:06:16,290 --> 00:06:19,080
and watching those storms.

109
00:06:19,080 --> 00:06:20,610
Aviation weather.

110
00:06:20,610 --> 00:06:24,480
Again, the entire Atlantic
and Pacific Basin.

111
00:06:24,480 --> 00:06:25,923
When you think of aviation,

112
00:06:28,920 --> 00:06:31,200
we're producing data that's helpful

113
00:06:31,200 --> 00:06:34,450
towards having the airlines calculate

114
00:06:35,370 --> 00:06:38,520
flight efficiency and how
much fuel to put on planes.

115
00:06:38,520 --> 00:06:41,700
Every pilot needs to have
a certified weather brief

116
00:06:41,700 --> 00:06:43,140
before they take off,

117
00:06:43,140 --> 00:06:45,900
and all of our data and forecasts

118
00:06:45,900 --> 00:06:48,510
feed those sorts of mechanisms.

119
00:06:48,510 --> 00:06:52,650
We have tsunami warnings and
we monitor seismic activity

120
00:06:52,650 --> 00:06:57,650
and those impacts on
the surface of the ocean

121
00:06:58,740 --> 00:07:02,970
and what impacts those would
have as they approach land.

122
00:07:02,970 --> 00:07:04,410
And fire weather,

123
00:07:04,410 --> 00:07:07,810
we have actual meteorologists that reside

124
00:07:08,820 --> 00:07:11,040
and are embedded with the fire chiefs

125
00:07:11,040 --> 00:07:15,720
at the site of forest fires and wildfires.

126
00:07:15,720 --> 00:07:19,710
So there's many more pieces
that I didn't hit on.

127
00:07:19,710 --> 00:07:21,840
And one last piece, the
Ocean Prediction Center.

128
00:07:21,840 --> 00:07:26,130
So when you think of the
Ocean Prediction Center,

129
00:07:26,130 --> 00:07:30,240
think of all the economic impact

130
00:07:30,240 --> 00:07:33,600
of all the vessel traffic
traveling across the ocean,

131
00:07:33,600 --> 00:07:36,750
and these massive storms that come

132
00:07:36,750 --> 00:07:39,270
creates the need for ship avoidance

133
00:07:39,270 --> 00:07:43,500
to circumvent or move around the storm.

134
00:07:43,500 --> 00:07:47,250
And so we're really focused on life safety

135
00:07:47,250 --> 00:07:49,083
and the economy there.

136
00:07:52,050 --> 00:07:55,380
So we operate across the United States,

137
00:07:55,380 --> 00:07:56,880
anywhere from America Samoa,

138
00:07:56,880 --> 00:08:00,840
to Hawaii, to Guam, to Alaska,

139
00:08:00,840 --> 00:08:03,183
Puerto Rico, the mainland US.

140
00:08:04,470 --> 00:08:07,590
We're really embedded in a
large number of communities

141
00:08:07,590 --> 00:08:09,570
across the United States.

142
00:08:09,570 --> 00:08:11,790
We have 122 forecast offices.

143
00:08:11,790 --> 00:08:15,510
Those are probably what you
would traditionally see as

144
00:08:15,510 --> 00:08:19,290
or recognize as like a
weather forecast office.

145
00:08:19,290 --> 00:08:21,750
But we also are embedded with the FAA

146
00:08:21,750 --> 00:08:23,580
in air traffic control centers,

147
00:08:23,580 --> 00:08:25,980
providing some decision support.

148
00:08:25,980 --> 00:08:29,070
We have forecast river crest

149
00:08:29,070 --> 00:08:32,340
and the river rise and falls and flooding.

150
00:08:32,340 --> 00:08:34,080
We have specialty national centers

151
00:08:34,080 --> 00:08:37,500
that are looking at unique
kind of niche type of weather.

152
00:08:37,500 --> 00:08:42,500
And then a water center and
looking at the hydrology.

153
00:08:42,750 --> 00:08:47,610
So we're really geared to
be out in the community

154
00:08:47,610 --> 00:08:51,690
with the people who need
to make those decisions.

155
00:08:51,690 --> 00:08:54,270
So today, we're gonna talk
about high-performance computing

156
00:08:54,270 --> 00:08:55,950
and the associated workloads

157
00:08:55,950 --> 00:09:00,570
and how we generate a large
amount of forecast information

158
00:09:00,570 --> 00:09:02,880
that helps with forecasters

159
00:09:02,880 --> 00:09:05,520
producing the forecast
and the forecast process.

160
00:09:05,520 --> 00:09:07,110
But I don't want you to lose sight

161
00:09:07,110 --> 00:09:08,460
from a mission perspective

162
00:09:08,460 --> 00:09:10,590
that that's the only thing that we do.

163
00:09:10,590 --> 00:09:14,850
Forecasts are one piece
of what we produce.

164
00:09:14,850 --> 00:09:18,690
But we've really been focused
over the past decade or so

165
00:09:18,690 --> 00:09:21,030
on making sure that we're working

166
00:09:21,030 --> 00:09:24,120
with decision-makers in
the community to understand

167
00:09:24,120 --> 00:09:26,490
what the impact of the
weather is going to be.

168
00:09:26,490 --> 00:09:29,010
So what do I mean by that?

169
00:09:29,010 --> 00:09:30,750
So give you an example, right?

170
00:09:30,750 --> 00:09:33,870
I'm from Washington, DC.

171
00:09:33,870 --> 00:09:36,630
If we have the slightest bit of snow,

172
00:09:36,630 --> 00:09:38,160
that wreaks havoc on traffic

173
00:09:38,160 --> 00:09:39,910
and that's a massive impact, right?

174
00:09:40,830 --> 00:09:42,870
Where I grew up in New Hampshire,

175
00:09:42,870 --> 00:09:44,400
we could have probably a foot of snow

176
00:09:44,400 --> 00:09:46,650
and people would still
be out moving around

177
00:09:46,650 --> 00:09:48,480
with ease, right?

178
00:09:48,480 --> 00:09:50,700
So impact wise, different places

179
00:09:50,700 --> 00:09:53,130
have different tolerances
for different weather

180
00:09:53,130 --> 00:09:55,380
and different weather types of events.

181
00:09:55,380 --> 00:09:58,920
So we're really focused
on delivering the forecast

182
00:09:58,920 --> 00:10:02,010
and making sure that we're
providing a risk assessment

183
00:10:02,010 --> 00:10:06,390
to those decision-makers in the
community that are providing

184
00:10:06,390 --> 00:10:08,850
those life-saving sort of
direction to the community.

185
00:10:08,850 --> 00:10:11,190
So like evacuating for a hurricane

186
00:10:11,190 --> 00:10:13,740
or closing highways
and so on and so forth.

187
00:10:13,740 --> 00:10:15,933
So we're really based.

188
00:10:18,240 --> 00:10:20,190
What we're really looking to do

189
00:10:20,190 --> 00:10:22,170
is create that trust relationship

190
00:10:22,170 --> 00:10:24,630
with the community decision-makers

191
00:10:24,630 --> 00:10:26,283
so that when things happen,

192
00:10:27,750 --> 00:10:30,090
we don't have to learn
who we're working with.

193
00:10:30,090 --> 00:10:31,590
We have that trust relationship

194
00:10:31,590 --> 00:10:33,390
so that we can just move on the dime

195
00:10:34,740 --> 00:10:36,363
and get those warnings out.

196
00:10:38,880 --> 00:10:40,950
Right, so high-performance computing.

197
00:10:40,950 --> 00:10:43,470
So let's talk a little bit about that.

198
00:10:43,470 --> 00:10:47,130
So in my career, when I look at this,

199
00:10:47,130 --> 00:10:50,190
what gets me excited about
high-performance computing is,

200
00:10:50,190 --> 00:10:52,500
it's one of those areas where

201
00:10:52,500 --> 00:10:55,560
any sort of improvement in
high-performance computing

202
00:10:55,560 --> 00:10:58,200
or technology with
high-performance computing

203
00:10:58,200 --> 00:11:00,720
directly correlates to improvement

204
00:11:00,720 --> 00:11:03,000
in the mission performance.

205
00:11:03,000 --> 00:11:04,650
So we see improvements

206
00:11:04,650 --> 00:11:06,870
in high-performance computing capacity

207
00:11:06,870 --> 00:11:08,247
or the way our software works

208
00:11:08,247 --> 00:11:10,650
and the efficiency on
high-performance computing.

209
00:11:10,650 --> 00:11:14,460
That results in better advancements,

210
00:11:14,460 --> 00:11:18,270
more accurate forecast,
higher integrity science

211
00:11:18,270 --> 00:11:20,850
that's going out to our
forecasters to allow them

212
00:11:20,850 --> 00:11:22,890
to perform the mission better.

213
00:11:22,890 --> 00:11:25,950
So really that direct correlation

214
00:11:25,950 --> 00:11:30,060
in improving HPC with
improving the forecast,

215
00:11:30,060 --> 00:11:32,703
which then improves those
trust relationships.

216
00:11:36,090 --> 00:11:37,020
So let's talk a little bit,

217
00:11:37,020 --> 00:11:38,340
we talked about the mission, right,

218
00:11:38,340 --> 00:11:40,260
and the diversity in the mission.

219
00:11:40,260 --> 00:11:43,290
The way in which we
operate within the mission

220
00:11:43,290 --> 00:11:45,240
provides certain constraints

221
00:11:45,240 --> 00:11:49,110
for the requirements and
how we operate the HPC

222
00:11:49,110 --> 00:11:50,970
and how we think about our workloads

223
00:11:50,970 --> 00:11:53,760
and the HPC environment.

224
00:11:53,760 --> 00:11:58,410
One of these metrics that we
chase is product timeliness.

225
00:11:58,410 --> 00:12:03,410
So on a daily basis, we have
a cycle of models that run

226
00:12:03,570 --> 00:12:07,983
and they run everywhere from
every hour to every six hours.

227
00:12:09,540 --> 00:12:11,400
I'll show you in a bit what the diversity

228
00:12:11,400 --> 00:12:12,963
of that workload looks like.

229
00:12:14,340 --> 00:12:17,730
But it's very time-driven
and cyclical in nature.

230
00:12:17,730 --> 00:12:21,690
So you can imagine for
a landfalling hurricane,

231
00:12:21,690 --> 00:12:24,510
there's a certain cadence
where our forecasters

232
00:12:24,510 --> 00:12:26,220
have to interact with decision-makers

233
00:12:26,220 --> 00:12:28,860
and those decision-makers
are looking for updates

234
00:12:28,860 --> 00:12:32,520
and people in the public
are looking for updates

235
00:12:32,520 --> 00:12:35,130
and there's a certain
timeliness and rhythm to that

236
00:12:35,130 --> 00:12:37,590
that creates a certain level of trust.

237
00:12:37,590 --> 00:12:40,440
So from a product timeliness perspective,

238
00:12:40,440 --> 00:12:42,540
we create 14 million products a day

239
00:12:42,540 --> 00:12:44,250
out of our supercomputer.

240
00:12:44,250 --> 00:12:45,960
And we look at each one of those

241
00:12:45,960 --> 00:12:48,210
and then we judge ourselves to say, okay,

242
00:12:48,210 --> 00:12:50,410
was it produced within 15 minutes

243
00:12:51,870 --> 00:12:55,290
on average on any previous day?

244
00:12:55,290 --> 00:12:56,850
If it is, we're good.

245
00:12:56,850 --> 00:13:00,300
If it's not, we kind of check
a mark against ourselves

246
00:13:00,300 --> 00:13:03,120
and we reduce our timeliness
numbers and stats.

247
00:13:03,120 --> 00:13:08,120
So we try to achieve
99.9% on time delivery.

248
00:13:08,940 --> 00:13:12,180
I think we have a metric of 99%,

249
00:13:12,180 --> 00:13:16,890
but we really achieve on a
regular basis about 99.9, 7,

250
00:13:16,890 --> 00:13:21,890
99.98% on time delivery on a daily basis.

251
00:13:27,390 --> 00:13:29,430
We'll kind of move on and
just keep that in mind

252
00:13:29,430 --> 00:13:31,410
in terms of the timeliness.

253
00:13:31,410 --> 00:13:36,000
The way in which we generate
and architect our solution,

254
00:13:36,000 --> 00:13:38,910
we have two high-performance
supercomputers

255
00:13:38,910 --> 00:13:41,640
that are about 14 petaFLOPS each

256
00:13:41,640 --> 00:13:46,110
or about 40,000 cores per system

257
00:13:46,110 --> 00:13:48,990
that run in different power grids,

258
00:13:48,990 --> 00:13:51,420
one on the East Coast,
one on the West Coast.

259
00:13:51,420 --> 00:13:53,310
We're able to switch operations

260
00:13:53,310 --> 00:13:55,920
between those within a 10-minute period.

261
00:13:55,920 --> 00:13:58,750
So I can move my entire operation

262
00:14:00,432 --> 00:14:02,490
of forecast operation in 10 minutes

263
00:14:02,490 --> 00:14:04,770
from one system to the next

264
00:14:04,770 --> 00:14:08,790
and with no real issue.

265
00:14:08,790 --> 00:14:12,540
So that's the type of
availability and data movement

266
00:14:12,540 --> 00:14:15,993
that we're looking at on a
regular basis with our systems.

267
00:14:17,490 --> 00:14:20,130
Let's talk a little bit about workflow.

268
00:14:20,130 --> 00:14:20,963
So,

269
00:14:23,040 --> 00:14:24,480
a modeling system,

270
00:14:24,480 --> 00:14:29,130
generally, we have data
that moves into the system,

271
00:14:29,130 --> 00:14:32,550
and so we'll stream on a consistent basis

272
00:14:32,550 --> 00:14:35,910
billions of observations a
day into our supercomputers.

273
00:14:35,910 --> 00:14:37,860
So we'll stream it,

274
00:14:37,860 --> 00:14:41,430
one feed into our operational
primary supercomputer.

275
00:14:41,430 --> 00:14:42,960
We'll have a separate parallel feed

276
00:14:42,960 --> 00:14:45,300
into our backup supercomputer.

277
00:14:45,300 --> 00:14:49,563
And then when the time cut-off
comes to generate a forecast,

278
00:14:50,400 --> 00:14:53,460
we'll dump that data out,

279
00:14:53,460 --> 00:14:55,950
we'll quality control that data,

280
00:14:55,950 --> 00:15:00,270
and then we'll run what
we call an analysis state.

281
00:15:00,270 --> 00:15:04,080
So we essentially use
a little bit of magic

282
00:15:04,080 --> 00:15:06,060
through the physical-based forecast models

283
00:15:06,060 --> 00:15:10,080
to look backwards at
the observational data,

284
00:15:10,080 --> 00:15:12,180
run it ahead three hours in a forecast,

285
00:15:12,180 --> 00:15:13,800
recheck it with some data,

286
00:15:13,800 --> 00:15:15,960
run it ahead three hours,
recheck it to its data,

287
00:15:15,960 --> 00:15:17,610
run it ahead three hours,

288
00:15:17,610 --> 00:15:20,460
and then recheck it and
do a final analysis state.

289
00:15:20,460 --> 00:15:25,460
And that gets us to an
atmosphere that's very continuous

290
00:15:26,190 --> 00:15:28,950
that the model recognizes, right?

291
00:15:28,950 --> 00:15:30,030
So you can imagine

292
00:15:30,030 --> 00:15:34,860
in the real world with
all the data observations,

293
00:15:34,860 --> 00:15:36,420
it's not always continuous

294
00:15:36,420 --> 00:15:39,210
and even where we have
an observation point

295
00:15:39,210 --> 00:15:43,020
at exactly every data
point that our model has,

296
00:15:43,020 --> 00:15:44,850
so we have to kind of reconcile that

297
00:15:44,850 --> 00:15:48,330
through a data-analysis process.

298
00:15:48,330 --> 00:15:50,400
And then we run a model forecast,

299
00:15:50,400 --> 00:15:52,920
which will go out into the future.

300
00:15:52,920 --> 00:15:54,120
So we have some forecast

301
00:15:54,120 --> 00:15:56,220
that'll run a few days into the future.

302
00:15:56,220 --> 00:15:59,490
Our global models will run
16 days out into the future.

303
00:15:59,490 --> 00:16:02,520
And then we have other models
that will run two weeks.

304
00:16:02,520 --> 00:16:07,440
And then a coupled climate analysis system

305
00:16:07,440 --> 00:16:09,093
that'll run nine months out.

306
00:16:11,040 --> 00:16:12,720
And then as the model's running,

307
00:16:12,720 --> 00:16:14,910
we need to output and prepare,

308
00:16:14,910 --> 00:16:19,140
dump that model memory out

309
00:16:19,140 --> 00:16:23,160
in each time step out to
kind of post that information

310
00:16:23,160 --> 00:16:26,220
out to a file format
that people can download

311
00:16:26,220 --> 00:16:29,580
and look at and our
forecasters can look at.

312
00:16:29,580 --> 00:16:31,800
And then we have a certain
amount of post-processing,

313
00:16:31,800 --> 00:16:34,680
whether it be statistical post-processing

314
00:16:34,680 --> 00:16:39,680
or just adjustments to the
model or adding derived fields.

315
00:16:40,200 --> 00:16:41,400
We have post-processing.

316
00:16:41,400 --> 00:16:43,290
So the interesting part of this

317
00:16:43,290 --> 00:16:45,870
from an overall workload perspective is,

318
00:16:45,870 --> 00:16:50,320
as you can see, we have certain
parts of the forecast model

319
00:16:51,210 --> 00:16:53,520
that require a large amount of cores

320
00:16:53,520 --> 00:16:55,080
that are massively parallel, right?

321
00:16:55,080 --> 00:16:57,060
So our global forecast model

322
00:16:57,060 --> 00:16:59,490
will require about 25,000 cores

323
00:16:59,490 --> 00:17:01,260
over an hour and a half period

324
00:17:01,260 --> 00:17:03,720
to operate a physical-based model.

325
00:17:03,720 --> 00:17:06,300
Our model analysis will be quite similar.

326
00:17:06,300 --> 00:17:09,210
But then we have a lot
of serial post-processing

327
00:17:09,210 --> 00:17:11,910
that runs around these jobs.

328
00:17:11,910 --> 00:17:16,170
And the way that we currently
construct our architectures,

329
00:17:16,170 --> 00:17:19,590
we have 40,000 of the
same kind of HPC cores.

330
00:17:19,590 --> 00:17:21,810
Some will have higher memory than others.

331
00:17:21,810 --> 00:17:24,000
But we have a lot of processing on there

332
00:17:24,000 --> 00:17:28,830
that's really not HPC
per-se type of processing,

333
00:17:28,830 --> 00:17:32,040
but it runs on our HPC because
it needs to be in proximity

334
00:17:32,040 --> 00:17:34,830
to the data where we run it.

335
00:17:34,830 --> 00:17:37,440
So this kind of presents
an interesting use case

336
00:17:37,440 --> 00:17:39,027
when you start to look at cloud

337
00:17:39,027 --> 00:17:40,980
and the flexibility that you can get

338
00:17:40,980 --> 00:17:43,680
with the various types of instances

339
00:17:43,680 --> 00:17:47,040
versus trying to forecast ahead

340
00:17:47,040 --> 00:17:49,140
when you're buying an on-prem system

341
00:17:49,140 --> 00:17:51,300
about exactly what the
architecture should be

342
00:17:51,300 --> 00:17:53,100
and what exact balance should be

343
00:17:53,100 --> 00:17:57,690
with the multiple-core workloads

344
00:17:57,690 --> 00:18:01,503
versus the serial workloads.

345
00:18:04,020 --> 00:18:06,180
And this is what our
production suite looks like.

346
00:18:06,180 --> 00:18:07,713
It looks like a mess, right?

347
00:18:08,760 --> 00:18:11,130
There's 80 different models.

348
00:18:11,130 --> 00:18:12,330
We have it all scheduled

349
00:18:12,330 --> 00:18:15,630
and it's a bit of a Tetris
puzzle to fit everything in.

350
00:18:15,630 --> 00:18:17,640
But you see the peaks and valleys

351
00:18:17,640 --> 00:18:19,860
generally four times a day.

352
00:18:19,860 --> 00:18:21,510
So if you look at the bottom of the scale,

353
00:18:21,510 --> 00:18:23,170
that's a 24-hour period

354
00:18:24,150 --> 00:18:28,713
from zero Greenwich Mean Time
to zero Greenwich Mean Time.

355
00:18:29,700 --> 00:18:32,580
And each one of those layers
represents a different type

356
00:18:32,580 --> 00:18:34,860
of model workload that we're running

357
00:18:34,860 --> 00:18:37,200
that's layered on top of each other.

358
00:18:37,200 --> 00:18:40,740
So where this gets kind of interesting is,

359
00:18:40,740 --> 00:18:43,170
okay, we have a timeliness metric.

360
00:18:43,170 --> 00:18:48,030
In general, on a daily basis,
things run pretty smooth.

361
00:18:48,030 --> 00:18:49,863
Everything works out pretty well.

362
00:18:51,060 --> 00:18:53,610
Certain days, you might
have a glitch in the system

363
00:18:54,690 --> 00:18:57,480
where your workload might get backed up.

364
00:18:57,480 --> 00:19:00,360
So on an on-prem system

365
00:19:00,360 --> 00:19:02,613
when your workload gets backed up,

366
00:19:03,570 --> 00:19:05,550
we have to do a bit of traffic control

367
00:19:05,550 --> 00:19:07,890
because you can only peak so far

368
00:19:07,890 --> 00:19:10,563
on your on-prem system to catch up.

369
00:19:11,640 --> 00:19:13,080
So oftentimes to catch up,

370
00:19:13,080 --> 00:19:15,960
you may have to load-shed certain models

371
00:19:15,960 --> 00:19:18,240
or certain model cycles

372
00:19:18,240 --> 00:19:19,953
to make that workload work.

373
00:19:20,940 --> 00:19:24,030
Whereas if we were to
contemplate a cloud environment,

374
00:19:24,030 --> 00:19:26,940
that could mean a quicker catch-up cycle

375
00:19:26,940 --> 00:19:29,610
because we could burst differently.

376
00:19:29,610 --> 00:19:32,190
So these are the types of
things that we're looking at

377
00:19:32,190 --> 00:19:35,820
when we're trying to weigh
the benefits of working

378
00:19:35,820 --> 00:19:39,513
with an on-prem versus
a cloud sort of setup.

379
00:19:40,740 --> 00:19:43,830
And just as a side note,
from an AI perspective,

380
00:19:43,830 --> 00:19:47,130
there's a lot of talk about
these inference models,

381
00:19:47,130 --> 00:19:50,763
which we'll get to in a
bit, a bit later here.

382
00:19:52,230 --> 00:19:53,100
You know, it's interesting

383
00:19:53,100 --> 00:19:55,560
when you look at the inference models,

384
00:19:55,560 --> 00:19:57,240
it's really easy to generate

385
00:19:57,240 --> 00:20:02,040
these niche kind of bespoke AI
models that have a specialty.

386
00:20:02,040 --> 00:20:05,130
One might be your standard
global forecast model.

387
00:20:05,130 --> 00:20:07,353
One might be geared towards
tropical forecasting.

388
00:20:07,353 --> 00:20:11,370
Another might be geared
towards a certain domain space,

389
00:20:11,370 --> 00:20:13,020
or you name it, right?

390
00:20:13,020 --> 00:20:15,540
You could get into hydrology
or different things.

391
00:20:15,540 --> 00:20:17,910
And so in a production environment

392
00:20:17,910 --> 00:20:20,250
when you're trying to run
things on a routine basis

393
00:20:20,250 --> 00:20:24,150
and also manage the implementations

394
00:20:24,150 --> 00:20:27,333
that go into operating
a system this reliably,

395
00:20:28,260 --> 00:20:32,130
you could really easily get
yourself into a situation

396
00:20:32,130 --> 00:20:34,530
where you're running so
many different models

397
00:20:34,530 --> 00:20:36,000
that it becomes a bit of a mess

398
00:20:36,000 --> 00:20:37,620
from a change-management perspective.

399
00:20:37,620 --> 00:20:41,730
So as we look at AI, it's exciting,

400
00:20:41,730 --> 00:20:44,490
but I think from a
change-management perspective,

401
00:20:44,490 --> 00:20:46,920
it's an interesting thing
to think about as well

402
00:20:46,920 --> 00:20:47,793
in terms of,

403
00:20:49,260 --> 00:20:52,110
okay, we're gonna have
like 500 models to manage.

404
00:20:52,110 --> 00:20:55,646
Or are we gonna focus
more in gaining integrity

405
00:20:55,646 --> 00:20:57,843
in a lower number of models?

406
00:21:01,427 --> 00:21:04,590
All right, so what I showed
you in the last slide

407
00:21:04,590 --> 00:21:08,460
was the general layout
of the modeling suite,

408
00:21:08,460 --> 00:21:10,170
and that's the bulk

409
00:21:10,170 --> 00:21:13,410
kind of regular, routine,
scheduled work that we have.

410
00:21:13,410 --> 00:21:16,680
So you have kind of 80 different models.

411
00:21:16,680 --> 00:21:18,780
They're all dumping data.

412
00:21:18,780 --> 00:21:21,210
They're all doing their initial
state to the atmosphere.

413
00:21:21,210 --> 00:21:23,790
They're all forecasting into the future.

414
00:21:23,790 --> 00:21:25,200
They're all posting data

415
00:21:25,200 --> 00:21:27,450
and they're all kind of
stacked on each other.

416
00:21:27,450 --> 00:21:30,130
But we also have some
other different workloads

417
00:21:31,923 --> 00:21:32,850
that lend well

418
00:21:32,850 --> 00:21:37,740
to a more kind of on-demand
elastic environment.

419
00:21:37,740 --> 00:21:42,420
One of those is tropical storm prediction

420
00:21:42,420 --> 00:21:45,513
or hurricane storm prediction.

421
00:21:46,980 --> 00:21:49,803
So we have in our
production suite right now,

422
00:21:50,850 --> 00:21:54,510
reserved slots in our
production environment

423
00:21:54,510 --> 00:21:58,500
where we can run up to 12
different hurricane-modeling runs.

424
00:21:58,500 --> 00:22:02,430
So you see each of these yellow boxes

425
00:22:02,430 --> 00:22:06,510
represent the domain of
that particular model,

426
00:22:06,510 --> 00:22:10,680
and that model moves
with the storm itself,

427
00:22:10,680 --> 00:22:13,545
and it's using the global
model for boundary conditions

428
00:22:13,545 --> 00:22:15,240
as it's moving through.

429
00:22:15,240 --> 00:22:16,800
So we can run up to 12 of these

430
00:22:16,800 --> 00:22:19,920
in our operational
environment at any given time,

431
00:22:19,920 --> 00:22:21,390
four times a day.

432
00:22:21,390 --> 00:22:24,900
The problem there is
that we have to reserve

433
00:22:24,900 --> 00:22:28,680
that sort of compute capacity
from a production perspective

434
00:22:28,680 --> 00:22:32,010
to be ready anytime we
need to run those storms,

435
00:22:32,010 --> 00:22:36,270
which means that that's
space on our system

436
00:22:36,270 --> 00:22:38,130
where we can't otherwise expand

437
00:22:38,130 --> 00:22:40,530
the bulk kind of production environment

438
00:22:40,530 --> 00:22:43,080
that I showed in the slides before.

439
00:22:43,080 --> 00:22:46,260
So this is an interesting case.

440
00:22:46,260 --> 00:22:49,980
So the other interesting
piece of this as well

441
00:22:49,980 --> 00:22:53,940
is that when we need to run the models,

442
00:22:53,940 --> 00:22:55,563
we need to spin them up quickly.

443
00:22:56,640 --> 00:23:01,640
So we can't wait around for
the environment to spin up

444
00:23:02,010 --> 00:23:02,843
when we need it.

445
00:23:02,843 --> 00:23:05,880
It needs to be there when
we need to start running it.

446
00:23:05,880 --> 00:23:08,010
So that's another kind
of interesting piece

447
00:23:08,010 --> 00:23:09,900
in architecting the workload

448
00:23:09,900 --> 00:23:12,393
with these sorts of on-demand runs.

449
00:23:13,710 --> 00:23:16,590
We also have development workloads,

450
00:23:16,590 --> 00:23:19,950
which I like to call like
surge development workloads.

451
00:23:19,950 --> 00:23:22,080
So we have our developers working

452
00:23:22,080 --> 00:23:25,320
on constant improvements in the models,

453
00:23:25,320 --> 00:23:27,060
but then there are also cases

454
00:23:27,060 --> 00:23:30,330
where you might want to look back 30 years

455
00:23:30,330 --> 00:23:31,890
at the observational data

456
00:23:31,890 --> 00:23:35,220
and apply that to the latest
technology with the models

457
00:23:35,220 --> 00:23:40,080
and create what we call
a re-analysis data set.

458
00:23:40,080 --> 00:23:42,300
So you're using the current model

459
00:23:42,300 --> 00:23:44,880
to fit it to a continuous
sort of atmosphere,

460
00:23:44,880 --> 00:23:46,320
fitting it to the grid

461
00:23:46,320 --> 00:23:48,840
to make it nice and neat and tidy.

462
00:23:48,840 --> 00:23:50,550
And you're doing that

463
00:23:50,550 --> 00:23:54,030
going back 30 years and you're
doing four forecasts a day

464
00:23:54,030 --> 00:23:56,310
for 30 years moving forward,

465
00:23:56,310 --> 00:23:58,470
which creates a massive amount

466
00:23:58,470 --> 00:24:00,810
of computational workload needed.

467
00:24:00,810 --> 00:24:03,030
But we don't need to do that all the time.

468
00:24:03,030 --> 00:24:05,400
That's an example of a surge workload.

469
00:24:05,400 --> 00:24:06,690
Another example might be

470
00:24:06,690 --> 00:24:09,060
taking all of that
numerical weather prediction

471
00:24:09,060 --> 00:24:13,830
kind of physics-based model and
applying that to an AI model

472
00:24:13,830 --> 00:24:16,800
and training the model to
do that sort of workload.

473
00:24:16,800 --> 00:24:21,360
So it's kind of an example
of a one-time surge, right?

474
00:24:21,360 --> 00:24:23,880
And so on a on-prem environment,

475
00:24:23,880 --> 00:24:28,260
you really have to either
reprioritize your workloads

476
00:24:28,260 --> 00:24:31,350
or you have to go get more money,

477
00:24:31,350 --> 00:24:33,330
take the time to provision that,

478
00:24:33,330 --> 00:24:34,920
get that on the floor,

479
00:24:34,920 --> 00:24:37,440
and then maybe like
six, eight months later,

480
00:24:37,440 --> 00:24:39,993
you're able to start
running your workload.

481
00:24:41,010 --> 00:24:45,963
So that's an example of
development surge workloads.

482
00:24:47,940 --> 00:24:50,340
Shifting gears a bit, this is newer to us

483
00:24:50,340 --> 00:24:52,803
is these AI workloads,

484
00:24:53,640 --> 00:24:56,250
in particular, the inference workloads.

485
00:24:56,250 --> 00:24:59,643
So the things that we run
operationally in production,

486
00:25:01,770 --> 00:25:04,350
we're experimenting with these.

487
00:25:04,350 --> 00:25:06,450
We're in the process of
getting those running

488
00:25:06,450 --> 00:25:09,090
in real time as we speak.

489
00:25:09,090 --> 00:25:14,090
And so currently, we have
a GraphCast-based model

490
00:25:14,700 --> 00:25:17,970
that's trained with our
global forecast model.

491
00:25:17,970 --> 00:25:21,570
And there's two types of
models, modeling systems.

492
00:25:21,570 --> 00:25:23,010
You could have a deterministic system,

493
00:25:23,010 --> 00:25:26,040
which means you make a single model run

494
00:25:26,040 --> 00:25:28,920
out from one initial guess field.

495
00:25:28,920 --> 00:25:30,960
Or you can run it in an ensemble,

496
00:25:30,960 --> 00:25:33,990
which means that you take
an initial guess field,

497
00:25:33,990 --> 00:25:36,090
you perturb it slightly,

498
00:25:36,090 --> 00:25:38,430
and then you run it out into the future.

499
00:25:38,430 --> 00:25:40,020
And what that does is it helps us

500
00:25:40,020 --> 00:25:43,050
with uncertainty calculations, right?

501
00:25:43,050 --> 00:25:46,620
So if you look at a suite
of 31 different forecasts

502
00:25:46,620 --> 00:25:48,600
starting from slightly
different initial conditions

503
00:25:48,600 --> 00:25:49,890
with the same model,

504
00:25:49,890 --> 00:25:51,060
that gives you an indicator

505
00:25:51,060 --> 00:25:52,680
of the type of spread in the forecast

506
00:25:52,680 --> 00:25:54,690
that you could have in the future.

507
00:25:54,690 --> 00:25:59,673
So we have inference runs
that run out to 16 days.

508
00:26:00,990 --> 00:26:02,070
So I told you earlier

509
00:26:02,070 --> 00:26:04,500
that a run like this on
a physical-based model

510
00:26:04,500 --> 00:26:07,740
would take about 25,000
cores for an hour and a half.

511
00:26:07,740 --> 00:26:12,210
This model with just a handful of H100s

512
00:26:12,210 --> 00:26:13,593
can run in seven minutes.

513
00:26:16,020 --> 00:26:17,493
It's a game changer, right?

514
00:26:19,140 --> 00:26:24,030
And our global ensemble
forecast, very similar in nature.

515
00:26:24,030 --> 00:26:26,400
So we're very close to running these

516
00:26:26,400 --> 00:26:28,800
in real time operationally.

517
00:26:28,800 --> 00:26:30,150
We're pretty excited about that

518
00:26:30,150 --> 00:26:32,193
and we're embarking on this mission.

519
00:26:33,600 --> 00:26:38,190
So kind of a word of caution,
though, from an AI perspective

520
00:26:38,190 --> 00:26:40,590
versus physical-based models, right?

521
00:26:40,590 --> 00:26:42,060
Are we gonna go out tomorrow

522
00:26:42,060 --> 00:26:45,060
and just switch straight to AI models

523
00:26:45,060 --> 00:26:46,830
and do that straight from observations?

524
00:26:46,830 --> 00:26:47,880
No.

525
00:26:47,880 --> 00:26:51,303
No, it's a bit of a
codependency, at least for now.

526
00:26:52,170 --> 00:26:54,460
So we need physical-based models

527
00:26:56,400 --> 00:27:01,113
to train the AI models,
statistically-based model,

528
00:27:02,670 --> 00:27:06,990
and then we can run those really quickly

529
00:27:06,990 --> 00:27:09,390
from an inference perspective.

530
00:27:09,390 --> 00:27:12,130
So the model runs that we're talking about

531
00:27:14,250 --> 00:27:17,760
are performing in a very
similar performance level

532
00:27:17,760 --> 00:27:19,710
to our physical-based models,

533
00:27:19,710 --> 00:27:21,630
but just keep in mind
that those are trained

534
00:27:21,630 --> 00:27:23,250
off of the physical-based model.

535
00:27:23,250 --> 00:27:25,323
So that's a codependence that we have.

536
00:27:26,730 --> 00:27:30,600
So we still need significant resources

537
00:27:30,600 --> 00:27:35,280
to run these re-analyses or
these large training data sets

538
00:27:35,280 --> 00:27:39,063
with the physical-based models
to train the AI-based models.

539
00:27:40,200 --> 00:27:42,000
But it's an interesting context, right?

540
00:27:42,000 --> 00:27:44,670
So now from an operational perspective,

541
00:27:44,670 --> 00:27:48,540
you saw the amount of compute
capacity that we needed

542
00:27:48,540 --> 00:27:51,843
to run our physical-based
models on a regular timeframe.

543
00:27:54,270 --> 00:27:56,490
Now think about that with
a bunch of inference runs

544
00:27:56,490 --> 00:28:00,090
that run in seven minutes
versus an hour and a half.

545
00:28:00,090 --> 00:28:01,380
It really changes the nature

546
00:28:01,380 --> 00:28:03,750
of how we think about our
high-performance computing.

547
00:28:03,750 --> 00:28:05,583
And even those inference runs,

548
00:28:06,570 --> 00:28:09,390
for one, do those scale really,

549
00:28:09,390 --> 00:28:13,650
are they really like kind
of the true HPC or not?

550
00:28:13,650 --> 00:28:16,320
So we're kind of trying
to figure that out.

551
00:28:16,320 --> 00:28:19,500
Another interesting aspect of this,

552
00:28:19,500 --> 00:28:22,113
it's something that we're
trying to figure out is,

553
00:28:23,040 --> 00:28:24,870
what's the impact in terms of moving

554
00:28:24,870 --> 00:28:26,610
that data out to the public, right?

555
00:28:26,610 --> 00:28:28,350
I'll give you some statistics in a moment

556
00:28:28,350 --> 00:28:31,380
about the type of data
movement that we have.

557
00:28:31,380 --> 00:28:33,960
But now you're going from outputting

558
00:28:33,960 --> 00:28:37,440
a large amount of data over
an hour and a half period,

559
00:28:37,440 --> 00:28:38,520
with a physical-based model,

560
00:28:38,520 --> 00:28:42,090
to dropping that same amount
of data in seven minutes.

561
00:28:42,090 --> 00:28:43,230
Right.

562
00:28:43,230 --> 00:28:44,910
And from a customer perspective,

563
00:28:44,910 --> 00:28:46,740
they're all expecting that data

564
00:28:46,740 --> 00:28:50,010
to be out and to be available.

565
00:28:50,010 --> 00:28:52,470
So now you have a bit of a
data streaming issue, right?

566
00:28:52,470 --> 00:28:57,210
So you have a lot of customers looking

567
00:28:57,210 --> 00:29:00,000
for the same exact data at the same time,

568
00:29:00,000 --> 00:29:02,700
and all of that's compressed
in a seven-minute period.

569
00:29:02,700 --> 00:29:05,040
That's not really fun to think about

570
00:29:05,040 --> 00:29:07,593
from a data distribution perspective.

571
00:29:10,140 --> 00:29:11,970
The other piece of this as well is,

572
00:29:11,970 --> 00:29:14,523
just looking at the
advancements of these models,

573
00:29:16,170 --> 00:29:20,100
it's really fast paced in
terms of advancements, right?

574
00:29:20,100 --> 00:29:23,370
But the other issue we have is

575
00:29:23,370 --> 00:29:25,437
that the integrity of our data

576
00:29:25,437 --> 00:29:28,980
and the scientific integrity
is super important.

577
00:29:28,980 --> 00:29:32,130
So there's a large validation process

578
00:29:32,130 --> 00:29:35,190
that we have to go through
before we implement a model.

579
00:29:35,190 --> 00:29:38,580
You want to look back
retrospectively to understand

580
00:29:38,580 --> 00:29:40,713
how these perform in different seasons.

581
00:29:42,330 --> 00:29:45,300
If you're updating a
hurricane model, for instance,

582
00:29:45,300 --> 00:29:48,570
you might want to run that
on 90 to 100 different storms

583
00:29:48,570 --> 00:29:49,920
to make sure it's verifying.

584
00:29:49,920 --> 00:29:50,753
You need to make sure

585
00:29:50,753 --> 00:29:54,600
that if you're improving the troposphere

586
00:29:54,600 --> 00:29:56,730
or the mid-latitudes,
you want to make sure

587
00:29:56,730 --> 00:29:58,740
that you're not impacting the tropics,

588
00:29:58,740 --> 00:30:00,810
which would then impact the performance

589
00:30:00,810 --> 00:30:02,640
of hurricane runs and so on.

590
00:30:02,640 --> 00:30:06,450
So this pace of advancement
in the model improvements,

591
00:30:06,450 --> 00:30:08,460
we have to kind of come to grips with,

592
00:30:08,460 --> 00:30:10,930
adding potentially more automation

593
00:30:11,880 --> 00:30:16,200
and insight into the validation
of those models as well

594
00:30:16,200 --> 00:30:18,030
to ensure that we have
the scientific integrity,

595
00:30:18,030 --> 00:30:20,190
and that's a challenge for us.

596
00:30:20,190 --> 00:30:22,530
All right, just gonna
finish up real quick here

597
00:30:22,530 --> 00:30:25,860
on data dissemination, out.

598
00:30:25,860 --> 00:30:29,010
So this is, not sure if a lot of folks

599
00:30:29,010 --> 00:30:30,960
understand the type of
data that we produce,

600
00:30:30,960 --> 00:30:35,960
but we'll send out about
12 to 15 terabytes a day

601
00:30:37,200 --> 00:30:39,660
and make that available to the public.

602
00:30:39,660 --> 00:30:44,660
We'll serve out about 300 terabytes a day,

603
00:30:46,800 --> 00:30:49,830
so about 10 petabytes, 11
petabytes a month of data

604
00:30:49,830 --> 00:30:51,720
that we're streaming out.

605
00:30:51,720 --> 00:30:53,640
And you can see what these
model implementations

606
00:30:53,640 --> 00:30:55,263
as we get to higher resolutions.

607
00:30:55,263 --> 00:30:57,030
It's not a steady amount.

608
00:30:57,030 --> 00:31:01,440
We have to plan and plan
for growth in that as well.

609
00:31:01,440 --> 00:31:06,440
So we offload about 50%
of that data distribution

610
00:31:06,480 --> 00:31:08,520
to content delivery services.

611
00:31:08,520 --> 00:31:12,570
About 50% of that will stream out

612
00:31:12,570 --> 00:31:15,660
from our on-prem systems that we have.

613
00:31:15,660 --> 00:31:20,520
From a web kind of services
or an API-based perspective,

614
00:31:20,520 --> 00:31:25,520
we get a little over a billion
hits a day on our services.

615
00:31:26,160 --> 00:31:29,610
So it's pretty tremendous for us.

616
00:31:29,610 --> 00:31:31,620
And about 80% of those we offload

617
00:31:31,620 --> 00:31:34,110
through content delivery services.

618
00:31:34,110 --> 00:31:38,910
But it's not just that on a steady basis.

619
00:31:38,910 --> 00:31:41,230
Our load will change based on

620
00:31:42,330 --> 00:31:45,600
what we have going for weather.

621
00:31:45,600 --> 00:31:49,080
Hurricane events,
snowstorms, tsunami events.

622
00:31:49,080 --> 00:31:53,970
So this is an example of a
tsunami case that we had recently

623
00:31:53,970 --> 00:31:57,660
where there was an earthquake,
the coast of Russia,

624
00:31:57,660 --> 00:32:02,660
and that created a large amount
of warning and advisories

625
00:32:03,210 --> 00:32:04,983
for tsunamis across the basin.

626
00:32:06,060 --> 00:32:10,290
And we typically get about two
and a half million hits a day

627
00:32:10,290 --> 00:32:12,510
on our web services for tsunami.

628
00:32:12,510 --> 00:32:16,080
In 12 hours, that surged
to about a billion hits.

629
00:32:16,080 --> 00:32:17,580
So that's just a good example

630
00:32:17,580 --> 00:32:21,243
of how we have to be ready
to surge as we move through.

631
00:32:23,220 --> 00:32:27,903
So now I'd like to hand it off to Rayette.

632
00:32:30,000 --> 00:32:33,030
We'll go through some of
the more practical aspects

633
00:32:33,030 --> 00:32:35,517
on the technical aspects on the cloud.

634
00:32:35,517 --> 00:32:36,350
Thank you.

635
00:32:36,350 --> 00:32:37,500
- Thank you, thank you.

636
00:32:39,300 --> 00:32:44,250
So welcome again. My name
is Rayette Toles-Abdullah.

637
00:32:44,250 --> 00:32:45,660
I started supporting NOAA

638
00:32:45,660 --> 00:32:48,633
as their Solution Architect in April 2020.

639
00:32:50,250 --> 00:32:54,000
I was excited to become

640
00:32:54,000 --> 00:32:57,630
someone who's supporting such
a wonderful mission at NOAA

641
00:32:57,630 --> 00:33:00,420
to help save life and property.

642
00:33:00,420 --> 00:33:04,080
Certainly took it seriously,
as you can imagine.

643
00:33:04,080 --> 00:33:06,810
And also, David might not know it.

644
00:33:06,810 --> 00:33:10,230
I'm a NOAA employee who sits at AWS, okay?

645
00:33:10,230 --> 00:33:13,320
That's what we have going on there, right?

646
00:33:13,320 --> 00:33:18,060
To explain NOAA's journey with HPC here,

647
00:33:18,060 --> 00:33:22,470
we started off early 2020, late 2020,

648
00:33:22,470 --> 00:33:26,220
to start to look at benchmarking

649
00:33:26,220 --> 00:33:28,740
some of their models
that they ran on-prem,

650
00:33:28,740 --> 00:33:30,870
in the cloud, right?

651
00:33:30,870 --> 00:33:33,090
We started looking at proof of concepts

652
00:33:33,090 --> 00:33:37,950
and actually porting in
their MPI applications,

653
00:33:37,950 --> 00:33:41,100
which proved to be the longest clip

654
00:33:41,100 --> 00:33:43,920
for getting this done.

655
00:33:43,920 --> 00:33:45,810
Once we got the benchmarking done

656
00:33:45,810 --> 00:33:50,243
for models such as GFS, GEFS,

657
00:33:51,780 --> 00:33:55,590
things of that nature,
and getting the metrics

658
00:33:55,590 --> 00:33:59,370
and stringent information that we needed

659
00:33:59,370 --> 00:34:01,860
for how those models
will run in the cloud,

660
00:34:01,860 --> 00:34:04,770
we certainly were in a place

661
00:34:04,770 --> 00:34:08,430
where we started to talk to AWS partners,

662
00:34:08,430 --> 00:34:10,620
some of our force multipliers

663
00:34:10,620 --> 00:34:12,930
that I'm gonna get to in a second

664
00:34:12,930 --> 00:34:17,670
to help grow their offerings at NOAA

665
00:34:17,670 --> 00:34:20,700
for their scientists
and their researchers.

666
00:34:20,700 --> 00:34:23,850
We flattened a learning
curve along the way, right?

667
00:34:23,850 --> 00:34:26,040
There's a bit of a black box

668
00:34:26,040 --> 00:34:28,120
for some scientists and researchers

669
00:34:29,730 --> 00:34:32,370
who are doing HPC in the cloud.

670
00:34:32,370 --> 00:34:37,370
So we set up enablement sessions
to help everyone understand

671
00:34:37,410 --> 00:34:41,070
how this works in the
cloud and how does it look?

672
00:34:41,070 --> 00:34:44,790
There's less knobs for you
to be turning in the cloud,

673
00:34:44,790 --> 00:34:48,000
which, you know, we've
been able to do on-prem

674
00:34:48,000 --> 00:34:50,670
and found to be unnecessary as well.

675
00:34:50,670 --> 00:34:53,170
We work with our HPC specialists

676
00:34:54,720 --> 00:34:57,300
to help enable that as well,

677
00:34:57,300 --> 00:35:00,030
kind of a unshameless
plug for Aaron Buscher,

678
00:35:00,030 --> 00:35:03,630
who's the HPC specialist who's
not here today to help us,

679
00:35:03,630 --> 00:35:06,930
to actually enable

680
00:35:06,930 --> 00:35:11,073
what we see now at NOAA in
the cloud in terms of HPC.

681
00:35:12,090 --> 00:35:16,320
So there's two offerings at NOAA

682
00:35:16,320 --> 00:35:19,560
that they offer their
scientists and researchers.

683
00:35:19,560 --> 00:35:21,720
In the cloud, in AWS,

684
00:35:21,720 --> 00:35:26,070
they could either run their
self-managed HPC clusters

685
00:35:26,070 --> 00:35:27,240
themselves,

686
00:35:27,240 --> 00:35:29,850
build up the clusters,
interconnect those clusters,

687
00:35:29,850 --> 00:35:32,580
have their head node
and their compute nodes

688
00:35:32,580 --> 00:35:34,440
as they do on-prem,

689
00:35:34,440 --> 00:35:38,190
or they can leverage the
HPC-as-a-Service offering

690
00:35:38,190 --> 00:35:39,810
that they have there.

691
00:35:39,810 --> 00:35:42,450
In that case, we partnered
up with Parallel Works

692
00:35:42,450 --> 00:35:44,760
and GDIT as the prime

693
00:35:44,760 --> 00:35:47,580
to basically provide

694
00:35:47,580 --> 00:35:49,890
a unified, simplified interface

695
00:35:49,890 --> 00:35:51,780
for scientists to run their jobs.

696
00:35:51,780 --> 00:35:54,060
I'm gonna get to that in a second, right?

697
00:35:54,060 --> 00:35:57,393
But either way, the
architecture looks the same.

698
00:35:59,040 --> 00:36:04,040
NOAA standardized on
purpose-built HPC EC2 instances.

699
00:36:05,730 --> 00:36:10,170
These are Hpc7a, 6as.

700
00:36:10,170 --> 00:36:14,887
And introduced at Super
Compute '25 and 2026,

701
00:36:16,050 --> 00:36:19,830
Hpc8as will be released, right?

702
00:36:19,830 --> 00:36:21,720
So they standardized on that.

703
00:36:21,720 --> 00:36:26,070
These are tightly-coupled
clusters as they are on-prem.

704
00:36:26,070 --> 00:36:28,630
So cluster placements groups

705
00:36:29,520 --> 00:36:32,640
can be used so that those clusters are

706
00:36:32,640 --> 00:36:37,507
in close proximity to one
another, right, in AWS.

707
00:36:38,820 --> 00:36:42,900
They standardized on using
Elastic Fabric Adapter.

708
00:36:42,900 --> 00:36:47,850
This gives you
sub-microsecond connectivity

709
00:36:47,850 --> 00:36:50,460
between the nodes that we have

710
00:36:50,460 --> 00:36:53,460
as cluster nodes as you see on-prem.

711
00:36:53,460 --> 00:36:56,640
They standardized on using FSx for Lustre

712
00:36:56,640 --> 00:37:00,210
as the file systems that
give them high latency,

713
00:37:00,210 --> 00:37:03,330
low throughput with their storage system.

714
00:37:03,330 --> 00:37:07,720
But key to that is also the ability

715
00:37:08,670 --> 00:37:11,700
to integrate natively with S3

716
00:37:11,700 --> 00:37:14,730
to import and export data.

717
00:37:14,730 --> 00:37:19,730
Meaning they don't have to
have that expensive storage

718
00:37:20,220 --> 00:37:23,520
consistently, persistently in the cloud.

719
00:37:23,520 --> 00:37:25,980
They can also export that data,

720
00:37:25,980 --> 00:37:30,540
export the data that
the models have output,

721
00:37:30,540 --> 00:37:34,170
and actually do
post-processing with that data

722
00:37:34,170 --> 00:37:38,400
or future analysis with that data from S3,

723
00:37:38,400 --> 00:37:40,920
less expensive object storage

724
00:37:40,920 --> 00:37:45,030
instead of having that persistent
FSx for Lustre storage.

725
00:37:45,030 --> 00:37:47,370
And then, like David was saying,

726
00:37:47,370 --> 00:37:52,370
after their runs are done,
this cluster is basically gone.

727
00:37:52,800 --> 00:37:55,680
It's terminated and they
move on to the next job

728
00:37:55,680 --> 00:37:57,333
that needs to be ran, right?

729
00:37:59,250 --> 00:38:03,360
Some of the on-prem
workflow capability exists.

730
00:38:03,360 --> 00:38:07,050
They still use PBS for job management.

731
00:38:07,050 --> 00:38:08,640
They still use Slurm,

732
00:38:08,640 --> 00:38:12,930
or you can use AWS Step
Functions and Lambda Functions

733
00:38:12,930 --> 00:38:16,680
to kind of handle that
workflow management as well.

734
00:38:16,680 --> 00:38:19,650
But they tend to like to
stick with some of the tooling

735
00:38:19,650 --> 00:38:21,180
that they use now on-prem

736
00:38:21,180 --> 00:38:23,523
for workflow management
and job management.

737
00:38:25,440 --> 00:38:29,043
So going to the HPC-as-a-Service offering,

738
00:38:29,910 --> 00:38:31,360
this was key

739
00:38:32,520 --> 00:38:36,150
to enable the scientists
and researchers at NOAA

740
00:38:36,150 --> 00:38:41,150
to, one, accelerate research
to operations, right?

741
00:38:41,790 --> 00:38:44,190
They have a simplified web portal.

742
00:38:44,190 --> 00:38:46,380
They just go into the web portal.

743
00:38:46,380 --> 00:38:48,780
They select, I need this many cores,

744
00:38:48,780 --> 00:38:50,640
I need this much memory,

745
00:38:50,640 --> 00:38:54,450
I need these EC2 instance types,

746
00:38:54,450 --> 00:38:56,460
I need this type of storage,

747
00:38:56,460 --> 00:38:58,440
and they actually run their jobs.

748
00:38:58,440 --> 00:39:00,760
So what this do is it abstracts

749
00:39:01,770 --> 00:39:03,750
the scientists and the researchers

750
00:39:03,750 --> 00:39:07,170
from worrying about the
configuration of clusters

751
00:39:07,170 --> 00:39:09,990
and how they get the network going

752
00:39:09,990 --> 00:39:14,970
and how they are able to tweak the knobs

753
00:39:14,970 --> 00:39:16,680
for the configuration

754
00:39:16,680 --> 00:39:20,730
in the backend using
infrastructure as code.

755
00:39:20,730 --> 00:39:23,400
Basically, from what they're selected,

756
00:39:23,400 --> 00:39:28,290
they get deployed an entire
cluster to do their jobs, right?

757
00:39:28,290 --> 00:39:30,480
And those jobs really run

758
00:39:30,480 --> 00:39:35,480
at performance capability that
David talked about earlier.

759
00:39:35,970 --> 00:39:37,860
For example, when he was talking

760
00:39:37,860 --> 00:39:41,193
about the hurricane
analysis forecast system.

761
00:39:42,150 --> 00:39:46,950
Also, with this offering,

762
00:39:46,950 --> 00:39:50,250
there came a time where we were able

763
00:39:50,250 --> 00:39:53,080
to see the convergence

764
00:39:53,970 --> 00:39:56,400
that I'm gonna talk about
in a little bit, right?

765
00:39:56,400 --> 00:39:59,460
So the scientists were
coming in and saying,

766
00:39:59,460 --> 00:40:02,070
I need HPC clusters, I need them now.

767
00:40:02,070 --> 00:40:05,700
I can't wait for the
on-prem queues, right?

768
00:40:05,700 --> 00:40:07,770
I can't wait to do my R&D

769
00:40:07,770 --> 00:40:10,410
because I wanted to get that done now.

770
00:40:10,410 --> 00:40:13,050
We're trying to go operational

771
00:40:13,050 --> 00:40:16,440
in XYZ year, right, or month.

772
00:40:16,440 --> 00:40:20,700
And so being able to have
this capacity and capability

773
00:40:20,700 --> 00:40:22,533
was a big success at NOAA.

774
00:40:23,670 --> 00:40:27,090
We have a couple of
use cases to talk about

775
00:40:27,090 --> 00:40:29,520
in that perspective, right?

776
00:40:29,520 --> 00:40:31,320
The biggest one and the same one

777
00:40:31,320 --> 00:40:34,320
that David was talking
about earlier on-prem

778
00:40:34,320 --> 00:40:38,430
is the hurricane analysis
and forecast system.

779
00:40:38,430 --> 00:40:41,970
And we've ran this a
couple of years in AWS,

780
00:40:41,970 --> 00:40:44,553
real time, during the hurricane season.

781
00:40:45,840 --> 00:40:48,060
Real time, hurricane season,

782
00:40:48,060 --> 00:40:52,563
a 21-member ensemble
would run twice a day.

783
00:40:53,670 --> 00:40:56,550
The output of that data
was exported to S3,

784
00:40:56,550 --> 00:40:59,970
shared with scientists
so they can collaborate

785
00:40:59,970 --> 00:41:01,980
and they can look at that data

786
00:41:01,980 --> 00:41:04,050
and see where you need refinements,

787
00:41:04,050 --> 00:41:06,690
and this helped to help
them solve problems

788
00:41:06,690 --> 00:41:09,990
and even improve the
model over time, right?

789
00:41:09,990 --> 00:41:14,350
Without waiting again for when
they can run these on-prem

790
00:41:15,300 --> 00:41:19,110
as well as when they can get the analysis

791
00:41:19,110 --> 00:41:22,323
that they need done
after those runs, right?

792
00:41:23,280 --> 00:41:28,280
We ran 399 hpc6a.48xlarge at that time

793
00:41:30,930 --> 00:41:33,660
on those twice a day,

794
00:41:33,660 --> 00:41:36,570
during those twice-a-day lab results

795
00:41:36,570 --> 00:41:40,500
and lab runs that we was running as well.

796
00:41:40,500 --> 00:41:45,030
And this was beneficial when
they were doing a test bed

797
00:41:45,030 --> 00:41:49,260
and trying to analyze, in real time,

798
00:41:49,260 --> 00:41:52,143
the data that was coming
out of the models, right?

799
00:41:53,700 --> 00:41:55,143
Another good,

800
00:41:56,220 --> 00:41:59,460
or I should say great
case study that came up

801
00:41:59,460 --> 00:42:04,460
was what we call RRFS, the
Rapid Refresh Forecast System.

802
00:42:06,120 --> 00:42:08,370
Now my understanding is RRFS

803
00:42:08,370 --> 00:42:13,350
is slated to go operational in 2026.

804
00:42:13,350 --> 00:42:14,790
And the reason,

805
00:42:14,790 --> 00:42:18,480
and one of the reasons why
they've been able to do so

806
00:42:18,480 --> 00:42:22,830
is because they were able to
leverage AWS to run the model.

807
00:42:22,830 --> 00:42:26,820
Again, this too was an ensemble run

808
00:42:26,820 --> 00:42:29,640
that they ran consistently during the day.

809
00:42:29,640 --> 00:42:33,570
Again, they standardized
on the Lustre file system,

810
00:42:33,570 --> 00:42:35,220
giving that high throughput.

811
00:42:35,220 --> 00:42:40,080
They standardized on the
purpose-built HPC instances again.

812
00:42:40,080 --> 00:42:41,850
And they also, again,

813
00:42:41,850 --> 00:42:45,750
exported the data from those models

814
00:42:45,750 --> 00:42:50,750
to AWS Open Data so that further
analysis and understanding

815
00:42:51,810 --> 00:42:54,570
of how the model is run
and how it can be tweaked

816
00:42:54,570 --> 00:42:56,850
can be leveraged as well.

817
00:42:56,850 --> 00:43:01,850
They also found that it ran
15% faster than on-prem,

818
00:43:02,400 --> 00:43:06,330
and it also was 25% better

819
00:43:06,330 --> 00:43:08,433
from a cost-performance standpoint.

820
00:43:11,010 --> 00:43:14,250
But like I said before,
there was a convergence.

821
00:43:14,250 --> 00:43:18,180
So we worked through a few years

822
00:43:18,180 --> 00:43:20,970
with the HPC-as-a-Service offering,

823
00:43:20,970 --> 00:43:23,583
and then we found something interesting.

824
00:43:24,450 --> 00:43:27,210
The scientists and researchers were like,

825
00:43:27,210 --> 00:43:30,150
we want access to Jupyter Notebooks.

826
00:43:30,150 --> 00:43:34,290
We want access to AI-ML services.

827
00:43:34,290 --> 00:43:37,030
We also need to have the capability

828
00:43:37,980 --> 00:43:40,900
to analyze large data sets

829
00:43:42,120 --> 00:43:44,490
that's outputted from these models.

830
00:43:44,490 --> 00:43:46,800
So we've seen the convergence

831
00:43:46,800 --> 00:43:51,800
from using the big,
tightly-coupled HPC clusters

832
00:43:53,550 --> 00:43:57,333
to AI-ML service within
this particular offering.

833
00:43:58,290 --> 00:44:02,160
Now while we was enabling
that, we were like, why?

834
00:44:02,160 --> 00:44:04,410
And David explained it earlier.

835
00:44:04,410 --> 00:44:07,080
We didn't know early on why is it so,

836
00:44:07,080 --> 00:44:08,910
but we figured it out later,

837
00:44:08,910 --> 00:44:13,910
is that we were seeing the convergence

838
00:44:14,700 --> 00:44:17,910
into AI weather prediction coming.

839
00:44:17,910 --> 00:44:20,700
So David talked about earlier

840
00:44:20,700 --> 00:44:25,590
with the traditional numerical
weather prediction models.

841
00:44:25,590 --> 00:44:27,390
These are physical based.

842
00:44:27,390 --> 00:44:32,010
They take the data from the
sensors, from the buoys,

843
00:44:32,010 --> 00:44:35,820
from the satellites, and they
take that observation data

844
00:44:35,820 --> 00:44:40,050
and they running those
computations on a supercomputer

845
00:44:40,050 --> 00:44:42,927
on-prem to get the
analysis and the forecast

846
00:44:42,927 --> 00:44:45,030
and the re-analysis.

847
00:44:45,030 --> 00:44:47,343
With AI weather prediction, though,

848
00:44:48,330 --> 00:44:50,250
it's slightly different,

849
00:44:50,250 --> 00:44:53,820
in the sense that we have
the training of the data

850
00:44:53,820 --> 00:44:58,770
using that numerical
weather data output, right?

851
00:44:58,770 --> 00:45:03,240
And then there's inference
that happens with that data,

852
00:45:03,240 --> 00:45:06,240
with that training data
that we have there.

853
00:45:06,240 --> 00:45:07,830
So,

854
00:45:07,830 --> 00:45:12,330
what's astounding to me,
and when I looked at it,

855
00:45:12,330 --> 00:45:15,990
is that from a SageMaker Jupyter Notebook,

856
00:45:15,990 --> 00:45:18,453
in minutes instead of hours,

857
00:45:19,380 --> 00:45:21,660
your scientists and your
researchers are able

858
00:45:21,660 --> 00:45:24,010
to do those weather forecasts

859
00:45:25,140 --> 00:45:28,140
and they get the output and
the data that they need.

860
00:45:28,140 --> 00:45:31,593
This democratizes weather prediction.

861
00:45:33,750 --> 00:45:36,570
We have the challenges that
David talked about earlier.

862
00:45:36,570 --> 00:45:38,883
Now we got more data coming in!

863
00:45:39,870 --> 00:45:42,330
I think it's a good problem to have.

864
00:45:42,330 --> 00:45:44,250
But also, we can see

865
00:45:44,250 --> 00:45:49,250
how this conversion is also
helping us to solve problems

866
00:45:50,040 --> 00:45:54,150
and do other scientific advancements

867
00:45:54,150 --> 00:45:55,800
with AI weather prediction,

868
00:45:55,800 --> 00:46:00,390
because this is a single GPU
instance that I'm using here

869
00:46:00,390 --> 00:46:03,780
or that I'm showing here on this slide

870
00:46:03,780 --> 00:46:08,780
instead of a tightly-coupled
large HPC cluster, right?

871
00:46:13,080 --> 00:46:16,210
David talked about the
models that's available

872
00:46:17,340 --> 00:46:19,560
with NOAA and they're working on.

873
00:46:19,560 --> 00:46:23,130
These are some other
models that are out there.

874
00:46:23,130 --> 00:46:26,010
FourCastNet, Pangu-Weather,

875
00:46:26,010 --> 00:46:27,960
GraphCast, Aurora.

876
00:46:27,960 --> 00:46:30,040
All can be ran on AWS

877
00:46:32,953 --> 00:46:35,190
with a single GPU, or if it's an ensemble,

878
00:46:35,190 --> 00:46:37,440
like David was talking about earlier,

879
00:46:37,440 --> 00:46:41,700
you're essentially gonna be
looking at several GPUs there.

880
00:46:41,700 --> 00:46:45,930
But the idea here is, again,
from a Jupyter Notebook,

881
00:46:45,930 --> 00:46:48,720
you could run AI weather prediction

882
00:46:48,720 --> 00:46:50,463
in minutes instead of hours.

883
00:46:51,780 --> 00:46:55,650
To prove that, because the
proof is in the pudding,

884
00:46:55,650 --> 00:46:58,773
I'm a technologist, I'm
not a scientist, right?

885
00:46:59,820 --> 00:47:04,820
But I was able to run this
FourCastNetv2 Small AI Model.

886
00:47:06,750 --> 00:47:09,990
What you see here is Hurricane Helene

887
00:47:09,990 --> 00:47:13,923
coming through Florida
into North Carolina.

888
00:47:14,790 --> 00:47:16,410
And you can see my name up there.

889
00:47:16,410 --> 00:47:18,540
That's my notebook that's running there

890
00:47:18,540 --> 00:47:21,240
based off of some code that I found,

891
00:47:21,240 --> 00:47:24,330
open source and available out there.

892
00:47:24,330 --> 00:47:28,500
So even myself as a technologist
is able to do these runs.

893
00:47:28,500 --> 00:47:31,140
Just imagine this researchers

894
00:47:31,140 --> 00:47:33,870
and scientists having this capability

895
00:47:33,870 --> 00:47:36,570
to be able to run these, at will,

896
00:47:36,570 --> 00:47:38,670
with the capacity that they need

897
00:47:38,670 --> 00:47:40,710
and the compute options that they need

898
00:47:40,710 --> 00:47:41,913
available in the cloud.

899
00:47:45,150 --> 00:47:47,850
And so with that, Mickey's gonna come up

900
00:47:47,850 --> 00:47:52,023
and talk about the future of HPC and AI.

901
00:47:53,217 --> 00:47:56,384
(audience applauding)

902
00:47:57,330 --> 00:47:58,563
- Thank you, Rayette.

903
00:48:00,300 --> 00:48:04,470
So you heard a great case
study from our colleague

904
00:48:04,470 --> 00:48:07,680
from National Oceanic and
Atmospheric Administration.

905
00:48:07,680 --> 00:48:11,260
Some great examples of
the use of supercomputing

906
00:48:12,120 --> 00:48:13,750
to solve their mission issues

907
00:48:14,589 --> 00:48:16,380
and to get to the right mission outcomes

908
00:48:16,380 --> 00:48:17,670
that they're looking at.

909
00:48:17,670 --> 00:48:20,280
And then you heard from Rayette about

910
00:48:20,280 --> 00:48:22,860
how AWS is powering that infrastructure

911
00:48:22,860 --> 00:48:23,940
and providing support

912
00:48:23,940 --> 00:48:27,810
to National Oceanic and
Atmospheric Administration.

913
00:48:27,810 --> 00:48:30,813
So what does the future
of HPC and AI look like?

914
00:48:31,740 --> 00:48:34,950
We at AWS believe it is bright.

915
00:48:34,950 --> 00:48:38,610
And if you are not aware
of this, about a week ago,

916
00:48:38,610 --> 00:48:43,610
we announced $50 billion in
infrastructure investment

917
00:48:44,520 --> 00:48:47,760
to build new AI and supercomputer centers

918
00:48:47,760 --> 00:48:49,053
for the US government.

919
00:48:51,570 --> 00:48:53,620
This $50 billion investment

920
00:48:54,660 --> 00:48:59,660
is comparable to 1.3
gigawatts in compute capacity.

921
00:49:02,130 --> 00:49:05,400
And this will serve the missions

922
00:49:05,400 --> 00:49:08,370
of various government agencies,

923
00:49:08,370 --> 00:49:10,080
including those

924
00:49:10,080 --> 00:49:14,880
that have their workloads
in the US GovCloud,

925
00:49:14,880 --> 00:49:19,530
in our secret regions, in
our top-secret regions,

926
00:49:19,530 --> 00:49:23,073
as well as across all
data classifications.

927
00:49:24,960 --> 00:49:29,960
Now this investment is based
on real-world innovation

928
00:49:30,480 --> 00:49:32,880
that we've been seeing, our customers

929
00:49:32,880 --> 00:49:35,640
like National Oceanic and
Atmospheric Administration,

930
00:49:35,640 --> 00:49:39,030
as well as many other
agencies demonstrate.

931
00:49:39,030 --> 00:49:41,100
So what I wanted to do
was kind of take you

932
00:49:41,100 --> 00:49:44,220
through a couple of these examples

933
00:49:44,220 --> 00:49:47,640
just to kind of spur the innovation.

934
00:49:47,640 --> 00:49:52,640
So first of all, we have
a partner called S2 Labs

935
00:49:54,480 --> 00:49:58,380
and they do subsurface mapping.

936
00:49:58,380 --> 00:50:00,270
What does subsurface mapping mean?

937
00:50:00,270 --> 00:50:04,140
Basically, it means that
without doing any excavation,

938
00:50:04,140 --> 00:50:08,400
you're trying to figure out
what is buried underground.

939
00:50:08,400 --> 00:50:11,790
Could be an infrastructure,
could be cables,

940
00:50:11,790 --> 00:50:14,640
could be pipelines, could
be construction material,

941
00:50:14,640 --> 00:50:18,360
but you have to figure out
what it is underground,

942
00:50:18,360 --> 00:50:20,463
especially when you cannot do excavation.

943
00:50:21,480 --> 00:50:25,050
So in 2004, there was Hurricane Ivan.

944
00:50:25,050 --> 00:50:26,200
Anybody remembers that?

945
00:50:27,270 --> 00:50:28,103
Okay.

946
00:50:28,103 --> 00:50:31,360
So Hurricane Ivan actually
destroyed an offshore oil rink

947
00:50:32,370 --> 00:50:37,370
and all the critical
infrastructure fell into the ocean

948
00:50:38,010 --> 00:50:40,713
and it was buried under
meters of sediment.

949
00:50:42,270 --> 00:50:45,360
Now it took 18 years of trials,

950
00:50:45,360 --> 00:50:48,450
using traditional acoustic methods,

951
00:50:48,450 --> 00:50:52,680
to try to figure out where
that infrastructure was buried

952
00:50:52,680 --> 00:50:54,930
because it was hazardous waste as well.

953
00:50:54,930 --> 00:50:56,973
But there was no success.

954
00:50:57,990 --> 00:51:01,260
Until S2 Labs

955
00:51:01,260 --> 00:51:03,310
used deep learning

956
00:51:04,290 --> 00:51:07,410
and applied physics and inferencing

957
00:51:07,410 --> 00:51:09,810
along with supercomputing-grade APIs

958
00:51:09,810 --> 00:51:11,313
to try to solve that problem.

959
00:51:13,050 --> 00:51:16,533
And they were able to do that
with unprecedented clarity.

960
00:51:17,970 --> 00:51:22,970
So much so that they developed a workflow

961
00:51:23,160 --> 00:51:25,350
that can actually map

962
00:51:25,350 --> 00:51:30,350
an area of the size of 400
meters by 400 meters by 60 meters

963
00:51:30,900 --> 00:51:33,240
in under five seconds.

964
00:51:33,240 --> 00:51:35,733
That's 29 and a half football fields.

965
00:51:37,140 --> 00:51:39,240
So the key innovation here is

966
00:51:39,240 --> 00:51:41,490
that this has applicability

967
00:51:41,490 --> 00:51:44,010
in many, many different industries.

968
00:51:44,010 --> 00:51:45,900
Oil and gas is one,

969
00:51:45,900 --> 00:51:49,410
energy, utility, urban development,

970
00:51:49,410 --> 00:51:52,920
construction safety,
as well as many others.

971
00:51:52,920 --> 00:51:57,030
So this is exactly a great
example of what we're seeing

972
00:51:57,030 --> 00:52:00,120
for the convergence of HPC and AI

973
00:52:00,120 --> 00:52:01,720
and the results it's delivering.

974
00:52:03,150 --> 00:52:05,463
Let's look at one more example.

975
00:52:07,410 --> 00:52:11,160
Now many of you may have encountered

976
00:52:11,160 --> 00:52:12,810
some challenges when you're trying

977
00:52:12,810 --> 00:52:16,650
to provide decision-making that involves

978
00:52:16,650 --> 00:52:18,300
multiple expert teams

979
00:52:18,300 --> 00:52:20,970
that need to come together and collaborate

980
00:52:20,970 --> 00:52:24,150
and then provide some kind of an analysis.

981
00:52:24,150 --> 00:52:27,180
This happens a lot in
the engineering domain,

982
00:52:27,180 --> 00:52:28,920
in the science domain,

983
00:52:28,920 --> 00:52:31,260
even in the financial services domain

984
00:52:31,260 --> 00:52:33,810
where you're trying to
approve a loan, for example.

985
00:52:34,830 --> 00:52:37,860
So we have a partner called Synera,

986
00:52:37,860 --> 00:52:39,760
which is a process automation company,

987
00:52:40,680 --> 00:52:44,850
and they worked on a domain of engineering

988
00:52:44,850 --> 00:52:48,630
where in order to provide
a request for quotation

989
00:52:48,630 --> 00:52:50,880
for an engineering component

990
00:52:50,880 --> 00:52:52,440
that is actually comprised

991
00:52:52,440 --> 00:52:54,180
of many, many other components

992
00:52:54,180 --> 00:52:56,490
that need to be assembled together,

993
00:52:56,490 --> 00:53:00,720
each one of which has to be monitored

994
00:53:00,720 --> 00:53:02,760
and for which the information
needs to be provided

995
00:53:02,760 --> 00:53:04,860
by a different engineering expert,

996
00:53:04,860 --> 00:53:07,980
and all that information
needs to be synthesized.

997
00:53:07,980 --> 00:53:10,320
And they wanted to try to model that

998
00:53:10,320 --> 00:53:11,940
and create a solution for that

999
00:53:11,940 --> 00:53:16,940
using HPC performance-grade
APIs as well as AI-ML.

1000
00:53:18,960 --> 00:53:22,143
So they developed a solution
based on Amazon Bedrock,

1001
00:53:23,220 --> 00:53:25,383
EC2 instances running on Nvidia,

1002
00:53:26,700 --> 00:53:30,423
as well as additional
AI-ML-based inferencing.

1003
00:53:31,350 --> 00:53:34,050
And what they were able to do was

1004
00:53:34,050 --> 00:53:37,470
take this assembly of
complex engineering parts

1005
00:53:37,470 --> 00:53:39,150
from multiple different experts,

1006
00:53:39,150 --> 00:53:41,550
take all of their expertise,

1007
00:53:41,550 --> 00:53:44,850
and run a multi-agent AI system

1008
00:53:44,850 --> 00:53:48,090
that gathers this information,
reasons through it,

1009
00:53:48,090 --> 00:53:49,713
and then provides the result,

1010
00:53:51,000 --> 00:53:54,210
from three weeks to several minutes now.

1011
00:53:54,210 --> 00:53:56,890
Again, you know, great example of how

1012
00:53:57,960 --> 00:54:02,960
we applied with our
partner HPC supercomputing

1013
00:54:03,150 --> 00:54:05,880
to solve a very, very complex problem.

1014
00:54:05,880 --> 00:54:10,140
So these two are just
scratching the surface.

1015
00:54:10,140 --> 00:54:14,550
There are many, many areas of innovation

1016
00:54:14,550 --> 00:54:17,760
that are possible with this
convergence of HPC and AI-ML,

1017
00:54:17,760 --> 00:54:21,180
and certainly with the
investment that we are making,

1018
00:54:21,180 --> 00:54:23,610
especially for the US government,

1019
00:54:23,610 --> 00:54:26,520
we see that this will become

1020
00:54:26,520 --> 00:54:28,950
more advanced in the future as well.

1021
00:54:28,950 --> 00:54:31,440
So let me just do a quick recap.

1022
00:54:31,440 --> 00:54:34,080
You know, we started about talking,

1023
00:54:34,080 --> 00:54:37,830
talking about the convergence
of HPC and AI-ML in the cloud.

1024
00:54:37,830 --> 00:54:40,590
We talked about, hey, back in 2023,

1025
00:54:40,590 --> 00:54:44,550
there's about $37 billion
worth of market for HPC.

1026
00:54:44,550 --> 00:54:48,810
We see this progressing quite
rapidly, 24% growth in 2024.

1027
00:54:50,100 --> 00:54:52,170
And then by 2029,

1028
00:54:52,170 --> 00:54:54,510
the analysts are telling us
that the conversion market,

1029
00:54:54,510 --> 00:54:56,163
it's gonna be about $49 billion.

1030
00:54:57,390 --> 00:55:01,173
Then we had David come up and
talk about NOAA's mission.

1031
00:55:02,130 --> 00:55:04,500
It's not just about forecasting.
Remember that, right?

1032
00:55:04,500 --> 00:55:07,450
They do so many other things
which are extremely important.

1033
00:55:08,760 --> 00:55:11,040
And then we had Rayette
come and talk about

1034
00:55:11,040 --> 00:55:14,580
the infrastructure, the
compute, the capacity,

1035
00:55:14,580 --> 00:55:17,520
and the architecture that is
supporting NOAA's mission.

1036
00:55:17,520 --> 00:55:20,190
So we hope you enjoyed it.

1037
00:55:20,190 --> 00:55:21,699
Thank you very much for your time.

1038
00:55:21,699 --> 00:55:23,886
(audience applauding)

