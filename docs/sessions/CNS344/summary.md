# AWS re:Invent 2025 会议总结：在 Amazon EKS 上运行 AI 工作负载

## 会议概述

本次 AWS re:Invent 2025 技术分享会聚焦于如何在 Amazon EKS (Elastic Kubernetes Service) 上运行生成式 AI 工作负载。会议由 AWS 高级专家解决方案架构师 Christina Andonov 和 EKS 产品团队的 Chris Flintter 共同主讲。

会议深入探讨了当前 AI 技术革命的快速发展态势，以及如何利用 Kubernetes 平台来应对这一变革。演讲者指出，自 2023 年以来，AI 创新的步伐异常迅速，每两天就有新的基础模型发布，这给技术从业者带来了巨大的焦虑感。然而，通过合理利用 EKS 和 AWS 生态系统，可以有效地管理和部署 AI 代理（Agents）、模型推理（Inference）和微调（Fine-tuning）工作负载。

会议强调了 Kubernetes 在 AI 工作负载中的三大优势：对底层基础设施的精细控制以优化成本性能、跨云和本地环境的可移植性、以及作为统一平台管理所有工作负载的能力。演讲者通过旅行代理的实际案例，详细演示了如何使用 Strands 等代理框架、MCP 服务器、以及各种 AWS 服务来构建完整的 AI 应用。在 GPU 管理方面，会议提供了从选择合适的 GPU 实例、配置 EKS 集群、到优化成本和性能的完整指南。

## 详细时间线与关键要点

### 开场与背景介绍 (00:00 - 05:30)

00:00 - 01:30 - 会议开场，讨论 AI 技术革命的独特性
- 演讲者指出当前 AI 革命与过去的互联网、智能手机、云计算革命不同
- 强调变化速度极快，自 2023 年以来创新步伐惊人
- 承认这种快速变化给人们带来的焦虑感是正常的人类反应

01:30 - 03:00 - Christina Andonov 自我介绍与会议目标
- 介绍自己是 AWS 高级专家解决方案架构师
- 会议目标：为参会者提供方向性指导，帮助降低 AI 焦虑

03:00 - 05:30 - AWS AI 服务组合概览
- 展示 AWS 的代理式 AI 和推理服务组合
- 解释服务选择原则：越靠左侧 AWS 管理越多，越靠右侧用户控制越多
- 介绍客户选择 Kubernetes 的三大原因：基础设施控制、可移植性、统一平台

### AI 代理在 EKS 上的运行 (05:30 - 15:00)

05:30 - 07:00 - AI 代理的重要性
- 引用 Gartner 预测：到 2028 年，33% 的企业软件将具有代理行为
- 15% 的日常工作决策将由代理应用做出
- 强调代理是新型业务应用，能够解决需要推理的问题

07:00 - 09:30 - 构建天气代理示例
- 演示如何使用 Strands 代理库构建简单的天气代理
- 展示仅需 5 行 Python 代码即可创建基本代理
- 说明代理代码可以容器化并部署到 EKS

09:30 - 11:00 - 工具（Tools）的使用
- 解释代理如何通过工具与外部 API 和数据库通信
- 演示如何使用 @tool 装饰器将传统函数转换为代理工具
- 介绍 MCP (Model Context Protocol) 服务器的概念和优势

11:00 - 13:00 - 完整代理架构
- 展示使用 Amazon Cognito 进行身份验证
- 使用 Amazon S3 集成会话管理
- 使用 DynamoDB 存储长期和短期代理记忆

13:00 - 15:00 - 可观测性
- 强调日志、指标和追踪三大可观测性支柱
- 特别指出追踪对于非确定性行为的重要性
- 推荐使用 Ragas 和 Langfuse 开源库进行代理监控

### GPU 和模型推理 (15:00 - 35:00)

15:00 - 17:00 - 在 Kubernetes 上运行模型的原因
- 使用自定义知识增强开源模型
- 降低服务与模型之间的延迟（物理定律）
- 大规模优化成本性能

17:00 - 20:00 - 选择合适的 GPU（"选择正确的种子"）
- 演示如何根据模型大小选择 GPU
- 示例：40GB 模型加上 KV 缓存和内存需要约 45GB GPU 内存
- 介绍 G5 和 G6 实例系列，以及量化技术

20:00 - 22:30 - GPU 采购选项
- 按需实例、储蓄计划和 Spot 实例
- 按需容量预留（ODCR）用于生产工作负载
- 容量块（Capacity Blocks）：24 小时到 28 天的预付费容量

22:30 - 26:00 - 配置 EKS 集群（"在正确的土壤中种植"）
- 推荐两种方式：EKS Auto Mode 和开源 Karpenter
- Auto Mode 自带托管的 Karpenter 自动扩展器
- 演示如何创建 Karpenter 节点池，自动选择最具成本效益的实例

26:00 - 28:30 - 网络和存储优化
- 使用 S3 VPC 端点存储模型
- 使用 ECR VPC 端点存储容器镜像
- 通过内部网络下载以提高速度

28:30 - 31:00 - 启动时间优化
- 目标：推理工作负载在 2 分钟内完成实例启动、镜像下载和模型加载
- 2 分钟与 Spot 实例中断通知时间一致
- 推荐使用 Run:AI 开源项目缓存权重并流式传输到 GPU

31:00 - 33:00 - GPU 监控和维护（"适当的照料"）
- 监控令牌生成速度和吞吐量
- 监控 GPU 健康状况、温度和功率
- 介绍节点健康监控和自动修复功能

33:00 - 35:00 - 推理工作负载扩展
- 使用 KEDA 和自定义指标进行水平 Pod 自动扩展
- 推荐使用推理框架（AI Bricks、Ray、Dynamo）
- 这些框架提供内置指标用于扩展

### AWS 最新发布和客户案例 (35:00 - 50:00)

35:00 - 37:00 - Chris Flintter 介绍与 EKS AI 工作负载统计
- 每周有数百万个 GPU 驱动的 EC2 实例在 EKS 集群中运行
- 该指标自 2024 年以来增长超过一倍
- 引用 Gartner 预测：到 2028 年，95% 的新 AI 工作负载将在 Kubernetes 上运行

37:00 - 39:00 - 客户案例展示
- 展示多个行业的客户在 EKS 上运行推理、训练、微调和代理工作负载
- 强调 AI 采用趋势影响所有行业垂直领域和客户规模

39:00 - 42:00 - 在 Kubernetes 上运行 AI 的挑战
- GPU 容量获取困难
- 成本优化和 GPU 利用率挑战
- 复杂的硬件和软件环境

42:00 - 45:00 - 新 GPU 实例发布
- GB200 和 GB300：采用 Nvidia Grace Blackwell GPU，支持多节点通信
- P6B200 和 P6B300：Nvidia Blackwell GPU，性能提升 2 倍
- P5.4xlarge：单 GPU Nvidia H100，用于中小型推理
- G6F：分数 GPU 实例

45:00 - 47:30 - EKS 加速 AMI
- 预验证完整的 GPU 驱动程序、内核模块和软件包堆栈
- 组件内置于操作系统镜像中，减少运行时安装
- 更新至 Kernel 6.12、Containerd 2.1、Nvidia 580 LTS 驱动

47:30 - 49:30 - 动态资源分配（DRA）
- 比设备插件更灵活和表达性更强的 API
- 允许指定 GPU 和网络设备的 PCI 路由亲和性
- 在 EKS Kubernetes 1.33 中启用，上游 1.34 GA

49:30 - 50:00 - 快速容器拉取（Seekable OCI/SOCI）
- 并行拉取和解包容器镜像
- 在 EKS Auto Mode 中默认为 GPU 和 Trainium 实例启用
- 显著减少大型推理框架和模型容器的启动时间

### Karpenter 和 Auto Mode 增强 (50:00 - 55:00)

50:00 - 52:00 - EC2 容量预留支持
- 支持 ODCR 和 ML 容量块
- 可在 Karpenter 和 Auto Mode 中指定容量预留 ID

52:00 - 53:30 - 静态容量配置
- 允许定义具有固定节点数的节点池
- 适用于推理和微调的基线容量需求
- 无需部署虚拟 Pod 来维持容量

53:30 - 55:00 - 节点覆盖（Node Overlay）
- 向 Karpenter 传递额外信息用于实例选择
- 支持自定义定价协议和特定资源需求
- 目前仅在 Karpenter 中可用，计划引入 Auto Mode

### EKS 控制平面和总结 (55:00 - 结束)

55:00 - 57:00 - EKS 预配置控制平面
- 在 re:Invent 上发布的新功能
- 允许选择更高层级的 EKS 控制平面规模
- 通过 API 自助服务方式配置
- 适用于大规模（数百个节点）和高流量用例

57:00 - 结束 - 会议总结
- 强调 EKS 和 AWS 生态系统为 GPU 提供最佳运行环境
- 鼓励参会者利用这些工具和最佳实践降低 AI 焦虑
- 展望 Kubernetes 在 AI 工作负载中的持续增长