1
00:00:00,000 --> 00:00:00,840
- All right.

2
00:00:00,840 --> 00:00:03,600
Hi, good morning everyone.
Thanks for joining us today.

3
00:00:03,600 --> 00:00:05,610
I hope you guys have been enjoying

4
00:00:05,610 --> 00:00:08,220
this year's AWS re:Invent thus far

5
00:00:08,220 --> 00:00:10,650
Thanks to AWS for giving
us the opportunity

6
00:00:10,650 --> 00:00:13,620
to speak a little bit
about ScyllaDB here today.

7
00:00:13,620 --> 00:00:16,440
And of course, I would like also

8
00:00:16,440 --> 00:00:21,440
to express my deepest gratitude
to both SAS and Freshworks,

9
00:00:23,160 --> 00:00:26,121
both for their continued
partnership as well

10
00:00:26,121 --> 00:00:29,550
as taking the time to
share a bit of their story

11
00:00:29,550 --> 00:00:31,050
with us here today.

12
00:00:31,050 --> 00:00:34,500
All of which you are going
to hear about here shortly.

13
00:00:34,500 --> 00:00:35,370
My name is Felipe,

14
00:00:35,370 --> 00:00:39,540
and I work as a technical
director at ScyllaDB.

15
00:00:39,540 --> 00:00:42,510
So, just to set the scene
here real quick, I want

16
00:00:42,510 --> 00:00:47,310
to briefly recap the primary
challenges teams face when

17
00:00:47,310 --> 00:00:50,850
they try to scale their
data-intensive applications.

18
00:00:50,850 --> 00:00:52,350
These are applications

19
00:00:52,350 --> 00:00:55,680
that require extremely high throughputs

20
00:00:55,680 --> 00:00:58,710
with fast and consistent response times,

21
00:00:58,710 --> 00:01:01,680
often within single-digit milliseconds

22
00:01:01,680 --> 00:01:05,040
or lower for their tail latencies.

23
00:01:05,040 --> 00:01:07,770
As data-intensive applications scale

24
00:01:07,770 --> 00:01:10,770
and becomes critical for your business,

25
00:01:10,770 --> 00:01:13,350
the primary core challenge is ensuring

26
00:01:13,350 --> 00:01:15,450
their consistent performance.

27
00:01:15,450 --> 00:01:16,966
As you scale,

28
00:01:16,966 --> 00:01:21,870
conventional database systems
become increasingly sensitive

29
00:01:21,870 --> 00:01:25,890
to traffic fluctuations,
which yes, you can try

30
00:01:25,890 --> 00:01:29,700
to best by throwing more
money at the problem,

31
00:01:29,700 --> 00:01:32,760
but that quickly becomes unsustainable

32
00:01:32,760 --> 00:01:34,500
in the longer term.

33
00:01:34,500 --> 00:01:38,460
And this is really the context
Where ScyllaDB fits in.

34
00:01:38,460 --> 00:01:42,870
ScyllaDB is the database for applications

35
00:01:42,870 --> 00:01:46,140
that require predictable
performance at scale.

36
00:01:46,140 --> 00:01:48,780
Our architecture is specifically designed

37
00:01:48,780 --> 00:01:51,750
to eliminate performance bottlenecks

38
00:01:51,750 --> 00:01:56,730
that conventional database
systems struggle with at scale.

39
00:01:56,730 --> 00:01:59,460
Rather than compensating
with more hardware,

40
00:01:59,460 --> 00:02:02,280
ScyllaDB's close-to-the-metal architecture

41
00:02:02,280 --> 00:02:04,590
is built to maximize the performance

42
00:02:04,590 --> 00:02:06,153
of underlying infrastructure.

43
00:02:07,020 --> 00:02:09,870
That's why organizations trust ScyllaDB

44
00:02:09,870 --> 00:02:12,840
to power their most critical applications.

45
00:02:12,840 --> 00:02:14,190
With ScyllaDB,

46
00:02:14,190 --> 00:02:18,510
a small three-node cluster
running on top of AWS CC2

47
00:02:18,510 --> 00:02:22,560
is capable of sustaining
millions of operations per second

48
00:02:22,560 --> 00:02:26,640
with predictable single-digit
millisecond latencies.

49
00:02:26,640 --> 00:02:29,850
Even as workloads grow and fluctuate,

50
00:02:29,850 --> 00:02:33,840
this efficiency does not
just improve performance

51
00:02:33,840 --> 00:02:36,956
but considerably reduce
infrastructure costs,

52
00:02:36,956 --> 00:02:38,400
giving you headroom

53
00:02:38,400 --> 00:02:42,990
to scale your most demanding
data-intensive applications.

54
00:02:42,990 --> 00:02:47,580
So, how do I know whether
ScyllaDB is a database

55
00:02:47,580 --> 00:02:49,470
that's a good fit for me?

56
00:02:49,470 --> 00:02:54,210
First, if your current response
times are unpredictable,

57
00:02:54,210 --> 00:02:56,790
especially when your tail
latencies are starting

58
00:02:56,790 --> 00:02:58,560
to violate your SLAs,

59
00:02:58,560 --> 00:03:02,670
and your teams keep getting
paged at 3:00 AM just for you

60
00:03:02,670 --> 00:03:04,530
to babysit the database.

61
00:03:04,530 --> 00:03:07,230
Second, if your database cannot keep up

62
00:03:07,230 --> 00:03:08,693
with throughput demands,

63
00:03:08,693 --> 00:03:12,270
particularly during peak or spike traffic,

64
00:03:12,270 --> 00:03:15,120
or when you are forced to overprovision

65
00:03:15,120 --> 00:03:18,480
just to stay ahead of a predictable load

66
00:03:18,480 --> 00:03:21,810
because your system lacks true elasticity.

67
00:03:21,810 --> 00:03:26,520
And third, your database
expansion is spiraling,

68
00:03:26,520 --> 00:03:28,650
and adding more nodes, replicas,

69
00:03:28,650 --> 00:03:31,080
or caches has become the only way

70
00:03:31,080 --> 00:03:33,270
to mask the performance issues,

71
00:03:33,270 --> 00:03:36,780
which ends up driving
your costs out of control.

72
00:03:36,780 --> 00:03:38,940
If any of those three points,

73
00:03:38,940 --> 00:03:41,820
or a combination of
these, resonates to you,

74
00:03:41,820 --> 00:03:45,810
then you may want to
check out on ScyllaDB.

75
00:03:45,810 --> 00:03:49,140
So, this chart on this slide demonstrates

76
00:03:49,140 --> 00:03:51,480
our user base spectrum,

77
00:03:51,480 --> 00:03:54,630
where the Y-axis represents throughput

78
00:03:54,630 --> 00:03:58,200
and the X-axis represents latencies.

79
00:03:58,200 --> 00:04:01,230
As you can see, the
largest cluster of users

80
00:04:01,230 --> 00:04:03,780
really live in the top-left-hand side,

81
00:04:03,780 --> 00:04:05,550
with workloads ranging close

82
00:04:05,550 --> 00:04:08,520
to a million operations per second,

83
00:04:08,520 --> 00:04:10,590
requiring between single-digit

84
00:04:10,590 --> 00:04:15,360
to less than 20-millisecond
P99 response times.

85
00:04:15,360 --> 00:04:17,640
Now, why ScyllaDB?

86
00:04:17,640 --> 00:04:19,980
Our highly available
architecture allows teams

87
00:04:19,980 --> 00:04:23,760
to confidently run mission-critical
applications at scale.

88
00:04:23,760 --> 00:04:26,130
We are built first and foremost

89
00:04:26,130 --> 00:04:29,550
with high-performance
transactional workloads in mind,

90
00:04:29,550 --> 00:04:33,780
while we still support
real-time analytical use cases

91
00:04:33,780 --> 00:04:37,230
that need fast access to operational data.

92
00:04:37,230 --> 00:04:39,330
And, more recently this year,

93
00:04:39,330 --> 00:04:42,630
with our recent native vector search,

94
00:04:42,630 --> 00:04:46,410
ScyllaDB becomes a unified AI platform

95
00:04:46,410 --> 00:04:48,517
built for performance.

96
00:04:48,517 --> 00:04:49,710
ScyllaDB can start

97
00:04:49,710 --> 00:04:54,150
and serve massive volumes of
data while using fewer nodes,

98
00:04:54,150 --> 00:04:58,770
reducing infrastructure costs
and operational overhead.

99
00:04:58,770 --> 00:05:01,830
It's also designed to
avoid vendor lock-in,

100
00:05:01,830 --> 00:05:05,730
give you the flexibility to
run your workloads on-prem,

101
00:05:05,730 --> 00:05:08,880
in any cloud, or in a hybrid environment,

102
00:05:08,880 --> 00:05:12,120
and effectively allowing
you to serve data closer

103
00:05:12,120 --> 00:05:14,880
to your users when it matters.

104
00:05:14,880 --> 00:05:17,040
Elasticity is also built deep,

105
00:05:17,040 --> 00:05:19,320
very deep in our architecture.

106
00:05:19,320 --> 00:05:23,550
We allow cluster to scale with
demand, eliminating the waste

107
00:05:23,550 --> 00:05:26,490
of permanently overprovision
infrastructure,

108
00:05:26,490 --> 00:05:30,630
so you get predictable
performance during traffic spikes

109
00:05:30,630 --> 00:05:33,183
and cost efficiency the rest of the time.

110
00:05:34,140 --> 00:05:38,490
So, how are organizations
using ScyllaDB today?

111
00:05:38,490 --> 00:05:41,070
Beyond SAS and Freshworks,

112
00:05:41,070 --> 00:05:43,680
at last year's re:Invent,
I had the opportunity

113
00:05:43,680 --> 00:05:45,930
to speak along with TripAdvisor

114
00:05:45,930 --> 00:05:48,120
here at this very same re:Invent

115
00:05:48,120 --> 00:05:50,670
who shared how ScyllaDB powers their

116
00:05:50,670 --> 00:05:54,720
real-time machine learning
system for recommendations.

117
00:05:54,720 --> 00:05:57,690
We also have companies like Discord,

118
00:05:57,690 --> 00:06:00,810
who use ScyllaDB at massive scale

119
00:06:00,810 --> 00:06:04,800
to connect millions of users
in real-time around the world.

120
00:06:04,800 --> 00:06:07,950
Disney Plus and Hulu
are relying on ScyllaDB

121
00:06:07,950 --> 00:06:10,272
to power their media streaming services.

122
00:06:10,272 --> 00:06:12,840
ScyllaDB, having supported the Super Bowl

123
00:06:12,840 --> 00:06:15,030
for actually many years in a row now.

124
00:06:15,030 --> 00:06:17,400
We've got that far. Yeah.

125
00:06:17,400 --> 00:06:21,660
And of course, most of the
organizations listed here

126
00:06:21,660 --> 00:06:25,140
actually also run ScyllaDB within AWS,

127
00:06:25,140 --> 00:06:28,120
which makes it perfect for
me to talk a little bit about

128
00:06:28,120 --> 00:06:31,861
our existing partnership with AWS.

129
00:06:31,861 --> 00:06:36,600
ScyllaDB has been, for many
years, an AWS ISV partner,

130
00:06:36,600 --> 00:06:39,570
where we've partnered
together in demonstrating

131
00:06:39,570 --> 00:06:42,060
the superior price/performance

132
00:06:42,060 --> 00:06:47,060
of their EC2 I4i and I7i
instances ahead of their launch,

133
00:06:48,870 --> 00:06:52,260
as well as the superior
Graviton performance,

134
00:06:52,260 --> 00:06:55,470
also in AWS EC2.

135
00:06:55,470 --> 00:06:57,600
We are a Graviton-ready partner,

136
00:06:57,600 --> 00:07:02,190
and we have two listings
in the AWS Marketplace.

137
00:07:02,190 --> 00:07:05,670
We have ScyllaDB Enterprise
for users who want to run

138
00:07:05,670 --> 00:07:08,580
and self-manage their
own database deployments,

139
00:07:08,580 --> 00:07:10,260
and we have ScyllaDB Cloud,

140
00:07:10,260 --> 00:07:14,070
our fully managed
database-as-a-service offering.

141
00:07:14,070 --> 00:07:18,570
So, with that, let me
switch over to Brian Jones,

142
00:07:18,570 --> 00:07:21,630
Senior Manager of Software
Development at SAS,

143
00:07:21,630 --> 00:07:23,100
who is going to tell us a little bit

144
00:07:23,100 --> 00:07:25,980
about SAS's real-time
personalization story.

145
00:07:25,980 --> 00:07:27,450
Brian, thank you so much

146
00:07:27,450 --> 00:07:29,730
for sharing your story with us today,

147
00:07:29,730 --> 00:07:32,910
and with that, the floor is yours.

148
00:07:32,910 --> 00:07:33,743
- All right.

149
00:07:35,520 --> 00:07:37,710
Thank you, Felipe, and
thank you, everyone,

150
00:07:37,710 --> 00:07:40,173
for coming to this presentation today.

151
00:07:41,490 --> 00:07:46,490
So, the first thing I wanna
do is talk about the Big Game.

152
00:07:46,680 --> 00:07:48,720
So, you're sitting down
watching the Big Game,

153
00:07:48,720 --> 00:07:51,150
and suddenly there's
a break in the action,

154
00:07:51,150 --> 00:07:53,070
and now you get an advertisement,

155
00:07:53,070 --> 00:07:55,680
and it happens to be the same commercial

156
00:07:55,680 --> 00:07:57,660
you've already seen in previous break,

157
00:07:57,660 --> 00:07:59,310
and that's very annoying for you.

158
00:08:00,300 --> 00:08:01,503
Why does that happen?

159
00:08:03,210 --> 00:08:05,523
Well, they're probably
not a customer of SAS.

160
00:08:06,780 --> 00:08:09,480
At SAS, we've been delivering
digital advertising

161
00:08:09,480 --> 00:08:11,050
during live sporting events

162
00:08:12,000 --> 00:08:15,150
like football, soccer, tennis, rugby.

163
00:08:15,150 --> 00:08:16,383
Since 2014,

164
00:08:17,400 --> 00:08:20,758
we have a robust ad-tech
platform seamlessly integrating

165
00:08:20,758 --> 00:08:23,973
with your own data workflows
and partner ecosystem.

166
00:08:25,560 --> 00:08:29,670
Good ad decisions require
us to activate data quickly,

167
00:08:29,670 --> 00:08:33,000
which improves the outcomes
for both viewers, advertisers,

168
00:08:33,000 --> 00:08:34,473
and broadcasters.

169
00:08:38,730 --> 00:08:39,960
In plain terms,

170
00:08:39,960 --> 00:08:43,233
the core of video advertising
is composed of a few parts.

171
00:08:44,940 --> 00:08:47,216
The first of these is ad decisions.

172
00:08:47,216 --> 00:08:48,900
When we receive an ad request,

173
00:08:48,900 --> 00:08:51,650
we need to make a decision
quickly about what to serve,

174
00:08:52,710 --> 00:08:54,700
and that response in video advertising

175
00:08:55,710 --> 00:08:59,790
uses the IAB VAST standard,

176
00:08:59,790 --> 00:09:03,570
which will include some
playback signals or events.

177
00:09:03,570 --> 00:09:06,300
So, as the advertisement is played out,

178
00:09:06,300 --> 00:09:09,693
these signals come back
to SAS, other partners,

179
00:09:10,800 --> 00:09:14,530
and those become then
part of the reporting

180
00:09:15,600 --> 00:09:18,693
that broadcasters and
advertisers will utilize.

181
00:09:20,130 --> 00:09:24,870
And finally, especially
for video-on-demand,

182
00:09:24,870 --> 00:09:27,480
optimization can happen
where you can choose

183
00:09:27,480 --> 00:09:30,930
how many ads to show or
how long a break should be

184
00:09:30,930 --> 00:09:32,433
for individual users.

185
00:09:35,970 --> 00:09:38,940
So, this picture is

186
00:09:38,940 --> 00:09:41,310
from inside the Ocracoke Lighthouse.

187
00:09:41,310 --> 00:09:45,030
You can take a look inside of
there during summer months,

188
00:09:45,030 --> 00:09:48,270
and like the steps to
summit this lighthouse,

189
00:09:48,270 --> 00:09:51,400
there are a few challenges
in digital video advertising

190
00:09:52,380 --> 00:09:54,330
I want to discuss.

191
00:09:54,330 --> 00:09:58,830
So, first you need to scale to
support large traffic spikes,

192
00:09:58,830 --> 00:10:02,460
integrate with the rest of the ecosystem,

193
00:10:02,460 --> 00:10:05,013
and make advertising
relevant to each viewer.

194
00:10:10,710 --> 00:10:13,000
So, at SAS, we help organizations

195
00:10:13,860 --> 00:10:15,960
do amazing things with the data,

196
00:10:15,960 --> 00:10:19,260
and we've been doing it
for nearly five decades.

197
00:10:19,260 --> 00:10:21,750
SAS is a leader in data and AI,

198
00:10:21,750 --> 00:10:23,850
with headquarters in Cary, North Carolina.

199
00:10:25,020 --> 00:10:28,260
Our focus is on helping people to use data

200
00:10:28,260 --> 00:10:30,633
and AI responsibly and effectively.

201
00:10:34,290 --> 00:10:38,610
SAS 360 Match delivers decisions at scale

202
00:10:38,610 --> 00:10:40,470
for our customers around the world.

203
00:10:40,470 --> 00:10:44,520
So, these are some of the
numbers, billions of impressions,

204
00:10:44,520 --> 00:10:47,103
video ads, programmatic bid request,

205
00:10:48,030 --> 00:10:51,633
and all delivered with low
latency and high availability.

206
00:10:56,460 --> 00:10:59,880
So, this image captures part
of the new user interface

207
00:10:59,880 --> 00:11:02,730
for SAS 360 Match that I and
my team have been working on

208
00:11:03,630 --> 00:11:04,920
at SAS.

209
00:11:04,920 --> 00:11:07,290
It is a first party ad server

210
00:11:07,290 --> 00:11:09,180
delivered as
software-as-a-service application

211
00:11:09,180 --> 00:11:11,670
to publishers, broadcasters,

212
00:11:11,670 --> 00:11:14,400
retailers, financial institutions,

213
00:11:14,400 --> 00:11:17,910
and we have clients like desktop browsers,

214
00:11:17,910 --> 00:11:21,810
mobile applications, connected
TV, and many other platforms.

215
00:11:21,810 --> 00:11:26,040
So, I work on essentially
all aspects of this product:

216
00:11:26,040 --> 00:11:28,950
R&D, operations, security, governance,

217
00:11:28,950 --> 00:11:31,923
legal, audit, et cetera, et cetera.

218
00:11:33,570 --> 00:11:37,023
So, if you have questions,
come talk to me afterwards.

219
00:11:40,560 --> 00:11:42,240
So, what kind of data

220
00:11:42,240 --> 00:11:44,910
do we usually have when
we're making a decision?

221
00:11:44,910 --> 00:11:47,640
So, we have personal data,

222
00:11:47,640 --> 00:11:50,190
like where are you, your interests,

223
00:11:50,190 --> 00:11:53,100
maybe some data that you use
to sign up for a service,

224
00:11:53,100 --> 00:11:54,570
registration data.

225
00:11:54,570 --> 00:11:56,130
There could be analytics or scoring data

226
00:11:56,130 --> 00:11:58,030
that's also attached to your identity.

227
00:11:59,190 --> 00:12:00,953
Additionally, there's contextual data.

228
00:12:00,953 --> 00:12:04,680
Amazon has spoken about this
for a couple of years now,

229
00:12:04,680 --> 00:12:08,700
using AI to grab contextual
data from video streams

230
00:12:08,700 --> 00:12:10,380
in advance or in live.

231
00:12:10,380 --> 00:12:13,290
So, you have episode data, break data,

232
00:12:13,290 --> 00:12:15,873
also recent ad-serving history data.

233
00:12:17,400 --> 00:12:20,130
Then we also can incorporate other data.

234
00:12:20,130 --> 00:12:23,703
So, there could be product
data or potentially offer data.

235
00:12:27,090 --> 00:12:29,430
So, from all of this data, audiences

236
00:12:29,430 --> 00:12:32,313
and segments are created
that appeal to advertisers.

237
00:12:37,170 --> 00:12:40,560
So, there are many methods available

238
00:12:40,560 --> 00:12:42,780
to provide data for decisions.

239
00:12:42,780 --> 00:12:45,093
An ad request may provide this data.

240
00:12:46,920 --> 00:12:49,710
The data could come from
an external web service.

241
00:12:49,710 --> 00:12:52,530
For example, if you need
to get the current weather

242
00:12:52,530 --> 00:12:54,573
for a location based on geo,

243
00:12:56,850 --> 00:12:58,920
that data is fetched,

244
00:12:58,920 --> 00:13:01,920
cached for a brief period of
time and used for that request.

245
00:13:02,760 --> 00:13:05,730
You can also reference
data that we hold in memory

246
00:13:05,730 --> 00:13:07,773
or is stored in a persistent data store.

247
00:13:08,610 --> 00:13:11,220
And any of this data could
refer to other identifiers,

248
00:13:11,220 --> 00:13:13,410
which we'll load more data for you.

249
00:13:13,410 --> 00:13:15,780
And recently, as of earlier this year,

250
00:13:15,780 --> 00:13:18,270
we introduced the concept of datasets,

251
00:13:18,270 --> 00:13:21,150
which is basically a
hierarchy of identifiers

252
00:13:21,150 --> 00:13:25,740
where we evaluate each
set of data independently

253
00:13:25,740 --> 00:13:27,490
to determine what should be served.

254
00:13:31,170 --> 00:13:35,160
So, let's dig a little
bit into an ad break.

255
00:13:35,160 --> 00:13:39,540
So, when a video stream is
approaching an ad break,

256
00:13:39,540 --> 00:13:42,633
there will be a signal within the stream.

257
00:13:43,920 --> 00:13:45,960
SCTE is a type of signaling,

258
00:13:45,960 --> 00:13:47,640
indicating that a break is coming soon

259
00:13:47,640 --> 00:13:49,890
and we need to make an ad request.

260
00:13:49,890 --> 00:13:51,750
That ad request is usually templated.

261
00:13:51,750 --> 00:13:53,520
It doesn't usually have very much data,

262
00:13:53,520 --> 00:13:55,683
a couple of things at most, usually.

263
00:13:57,540 --> 00:13:59,103
And we'll receive this,

264
00:14:00,180 --> 00:14:03,900
and then we'll need to make a
decision about what to serve.

265
00:14:03,900 --> 00:14:05,880
And so, we will return

266
00:14:05,880 --> 00:14:10,410
a VAST-formatted XML template response.

267
00:14:10,410 --> 00:14:12,660
And inside of this, we'll
have a number of these events,

268
00:14:12,660 --> 00:14:16,230
which I mentioned earlier,
like impression, view,

269
00:14:16,230 --> 00:14:19,200
various standard quartiles,

270
00:14:19,200 --> 00:14:20,463
those sorts of signals.

271
00:14:26,370 --> 00:14:28,890
And this is an example of what
the break actually looks like

272
00:14:28,890 --> 00:14:30,690
to our software.

273
00:14:30,690 --> 00:14:33,000
It's a series of requests.

274
00:14:33,000 --> 00:14:35,910
In this example, we have
a duration server request.

275
00:14:35,910 --> 00:14:38,733
It's asking for a duration of 180 seconds.

276
00:14:40,020 --> 00:14:43,530
It says that this is the
particular break identifier.

277
00:14:43,530 --> 00:14:45,510
This is like a pseudo-random identifier.

278
00:14:45,510 --> 00:14:48,480
It'll recycle after a
couple months, probably.

279
00:14:48,480 --> 00:14:52,380
Then we have an identifier
of some name for a visitor,

280
00:14:52,380 --> 00:14:53,943
so in this case SVID.

281
00:14:55,050 --> 00:15:00,050
And then, after this ad request
returns the VAST response,

282
00:15:01,350 --> 00:15:06,180
the service-side ad insertion,
or the player device,

283
00:15:06,180 --> 00:15:08,250
whatever, was begin making
these count requests.

284
00:15:08,250 --> 00:15:12,180
These are all the events that
were in the VAST response

285
00:15:12,180 --> 00:15:14,610
to begin with for all the quartiles

286
00:15:14,610 --> 00:15:15,860
that we mentioned before.

287
00:15:19,500 --> 00:15:22,500
So then, how do we make a decision?

288
00:15:22,500 --> 00:15:24,540
Let's dive a little bit through that.

289
00:15:24,540 --> 00:15:27,780
So, if we've never seen a
request from a user before,

290
00:15:27,780 --> 00:15:30,990
from a visitor, we're gonna
need to start a session.

291
00:15:30,990 --> 00:15:33,840
So, we will start the session

292
00:15:33,840 --> 00:15:35,910
usually with an identifier
that's given to us,

293
00:15:35,910 --> 00:15:38,193
but sometimes we'll need to generate one.

294
00:15:39,540 --> 00:15:43,680
We'll then load data if we
have it for the individual,

295
00:15:43,680 --> 00:15:47,130
if we've got data to load for them,

296
00:15:47,130 --> 00:15:50,110
including all of their
previous history that we need

297
00:15:51,660 --> 00:15:54,180
to enforce other business rules.

298
00:15:54,180 --> 00:15:58,650
And then we can begin
processing the actual request.

299
00:15:58,650 --> 00:16:00,690
So, we'll activate more data.

300
00:16:00,690 --> 00:16:03,120
So, supertags, these are in-memory data,

301
00:16:03,120 --> 00:16:06,243
sometimes persisted data from identifiers,

302
00:16:08,040 --> 00:16:10,230
usually coming from expansions of things

303
00:16:10,230 --> 00:16:12,333
that are inside of the request itself.

304
00:16:13,350 --> 00:16:14,714
Then, other identifiers may be loaded.

305
00:16:14,714 --> 00:16:16,290
Remember the products and offers

306
00:16:16,290 --> 00:16:17,850
that we talked about earlier.

307
00:16:17,850 --> 00:16:19,015
Connectors are the
things where you can talk

308
00:16:19,015 --> 00:16:22,863
to any HTTP web service
to load more data, still,

309
00:16:24,420 --> 00:16:26,943
you can also talk to SAS
Intelligent Decisioning.

310
00:16:29,130 --> 00:16:31,260
Then, we need to make a decision

311
00:16:31,260 --> 00:16:33,990
so the customer can
introduce prioritization

312
00:16:33,990 --> 00:16:37,233
based on their needs for their inventory.

313
00:16:38,070 --> 00:16:40,140
SAS can perform optimizations,

314
00:16:40,140 --> 00:16:43,950
as well as targeting via Boolean logic,

315
00:16:43,950 --> 00:16:46,683
on all of this key-value
data that we have in hand.

316
00:16:48,330 --> 00:16:50,010
And if the customer is using it,

317
00:16:50,010 --> 00:16:55,010
we can talk to OpenRTB SSP
for open web creatives,

318
00:16:59,670 --> 00:17:02,320
or they can make use of
direct bookings in the system

319
00:17:03,180 --> 00:17:04,013
as well.

320
00:17:06,450 --> 00:17:09,720
So, once we have selected the creative,

321
00:17:09,720 --> 00:17:12,300
we'll render response again
from the VAST template

322
00:17:12,300 --> 00:17:14,130
for video advertising.

323
00:17:14,130 --> 00:17:18,810
We'll serve that and record
this decisions, one or more,

324
00:17:18,810 --> 00:17:21,060
as well as metrics that
need to be generated.

325
00:17:25,710 --> 00:17:29,280
So, providing the best decision
in the moment also means

326
00:17:29,280 --> 00:17:32,266
that we support the viewers, for example,

327
00:17:32,266 --> 00:17:35,610
by preventing the exposure
to the same advertiser

328
00:17:35,610 --> 00:17:37,980
or ad too often,

329
00:17:37,980 --> 00:17:39,360
ensuring that industry

330
00:17:39,360 --> 00:17:41,340
and government regulations are followed,

331
00:17:41,340 --> 00:17:43,230
and respecting the privacy rules

332
00:17:43,230 --> 00:17:45,480
and regulations our
customers must adhere to.

333
00:17:50,280 --> 00:17:52,680
So, at SAS, we approach things differently

334
00:17:52,680 --> 00:17:53,730
than many other players

335
00:17:53,730 --> 00:17:56,280
in the digital video advertising industry.

336
00:17:56,280 --> 00:17:57,570
SAS does not collect

337
00:17:57,570 --> 00:17:59,850
and aggregate data across customers,

338
00:17:59,850 --> 00:18:02,223
as others in the advertising
industry often do.

339
00:18:03,390 --> 00:18:06,357
So, the broadcasters have
control over their data

340
00:18:06,357 --> 00:18:09,000
and with whom that data shared.

341
00:18:09,000 --> 00:18:11,880
We provide our customers with the tools

342
00:18:11,880 --> 00:18:13,280
that are open to integration

343
00:18:14,370 --> 00:18:16,083
and extension to fit their needs.

344
00:18:18,420 --> 00:18:22,080
Both performance and availability
matter not just for users,

345
00:18:22,080 --> 00:18:24,873
but also advertisers and our customers.

346
00:18:29,250 --> 00:18:31,593
How do we scale our access to data?

347
00:18:33,690 --> 00:18:36,390
We require, this is a requirement,

348
00:18:36,390 --> 00:18:38,553
that the database have low-latency reads.

349
00:18:39,480 --> 00:18:42,930
It must have high
availability, data redundancy,

350
00:18:42,930 --> 00:18:44,133
and predictable cost.

351
00:18:46,590 --> 00:18:51,590
But there's not enough time
during this cycle of ad request,

352
00:18:51,720 --> 00:18:55,440
to fetch the data, make use of it,

353
00:18:55,440 --> 00:18:57,690
and make the changes
and write them back out.

354
00:18:58,830 --> 00:19:01,623
So, we introduce some other concepts.

355
00:19:03,060 --> 00:19:05,043
We activate the data on session start.

356
00:19:07,050 --> 00:19:12,050
We write changes to the
data, when necessary.

357
00:19:15,300 --> 00:19:20,010
And this reduces the interactions
necessary for sessions

358
00:19:20,010 --> 00:19:23,223
that are routed, based on,

359
00:19:24,090 --> 00:19:28,143
to find the right
session within a cluster.

360
00:19:32,820 --> 00:19:35,283
How do we scale to handle
large traffic spikes?

361
00:19:36,900 --> 00:19:40,263
So, we make use of load balancing.

362
00:19:41,940 --> 00:19:44,760
In Amazon, we've been
using provisioned capacity

363
00:19:44,760 --> 00:19:47,520
from load balancers for many years.

364
00:19:47,520 --> 00:19:49,620
We went to GA earlier this year,

365
00:19:49,620 --> 00:19:51,903
I think in January,
maybe December last year.

366
00:19:54,630 --> 00:19:58,530
We also make use of
autoscaling, previously EC2.

367
00:19:58,530 --> 00:20:00,993
Now, Kubernetes horizontal
pod autoscaling.

368
00:20:02,850 --> 00:20:04,530
Scheduled scaling is also available.

369
00:20:04,530 --> 00:20:06,750
So, broadcasters often know,

370
00:20:06,750 --> 00:20:09,300
or have an idea of, the
size of their audience

371
00:20:09,300 --> 00:20:11,703
based on time of day, program, et cetera.

372
00:20:12,990 --> 00:20:16,230
And we also introduce
CPU utilization buffers.

373
00:20:16,230 --> 00:20:20,940
So, rather than run it, let's
say, at 80% CPU utilization,

374
00:20:20,940 --> 00:20:22,980
maybe you run it at 50%.

375
00:20:22,980 --> 00:20:26,160
You give yourself some
room to handle spikes

376
00:20:26,160 --> 00:20:29,313
that autoscaling simply can't do.

377
00:20:30,870 --> 00:20:33,120
And finally, load shedding.

378
00:20:33,120 --> 00:20:35,910
So, there're several features
in this area for one.

379
00:20:35,910 --> 00:20:39,000
In one case, a customer can choose

380
00:20:39,000 --> 00:20:42,720
to prioritize certain types
of requests over others

381
00:20:42,720 --> 00:20:45,210
because some are more important to them

382
00:20:45,210 --> 00:20:47,523
for revenue than other requests are.

383
00:20:50,820 --> 00:20:52,980
So, I've only touched on part

384
00:20:52,980 --> 00:20:55,923
of what SAS 360 Match can
do and how it does it.

385
00:20:56,940 --> 00:20:59,403
We support many kinds
of client applications.

386
00:21:01,920 --> 00:21:05,245
We use many Amazon services,

387
00:21:05,245 --> 00:21:07,413
and yes, we use ScyllaDB.

388
00:21:11,340 --> 00:21:13,623
So, why do we use ScyllaDB?

389
00:21:16,020 --> 00:21:19,923
ScyllaDB provides the low
latency that we need for reads.

390
00:21:21,330 --> 00:21:24,240
It linearly scales both
vertically and horizontally

391
00:21:24,240 --> 00:21:25,073
with compute.

392
00:21:26,220 --> 00:21:28,770
Individual nodes in the
cluster can be replaced

393
00:21:28,770 --> 00:21:30,603
without data loss or downtime.

394
00:21:31,950 --> 00:21:34,320
The data can be replicated
to multiple data centers,

395
00:21:34,320 --> 00:21:37,473
AZs, or regions, if needed.

396
00:21:38,940 --> 00:21:42,420
And for us, we can also
use workload prioritization

397
00:21:42,420 --> 00:21:44,610
to ensure that ad delivery is not impacted

398
00:21:44,610 --> 00:21:47,313
by other data processing that we must do.

399
00:21:51,660 --> 00:21:55,593
Here is an example of what
a request spike looks like.

400
00:21:56,940 --> 00:22:01,800
So, when we get to a spike
seen on the bottom graph,

401
00:22:01,800 --> 00:22:05,613
there's essentially no
change in read latency,

402
00:22:07,830 --> 00:22:09,580
which is exactly what we wanna see.

403
00:22:14,220 --> 00:22:15,813
What kind of data do we store?

404
00:22:17,880 --> 00:22:22,020
So, we store viewer
data, registration data.

405
00:22:22,020 --> 00:22:24,210
This includes events for event counting,

406
00:22:24,210 --> 00:22:28,200
so we know how many times
you've done something

407
00:22:28,200 --> 00:22:31,383
that's important to the
broadcaster or advertiser.

408
00:22:33,330 --> 00:22:35,550
We also can store exposure,

409
00:22:35,550 --> 00:22:38,160
so this is how many times
you've seen a particular ad,

410
00:22:38,160 --> 00:22:42,150
and there are rules that say things like,

411
00:22:42,150 --> 00:22:44,350
let's not show this ad
more than once a day,

412
00:22:46,230 --> 00:22:47,330
those sorts of things.

413
00:22:48,630 --> 00:22:49,950
Additionally, there is data

414
00:22:49,950 --> 00:22:53,250
that can help map from
one identifier to another,

415
00:22:53,250 --> 00:22:54,630
if necessary.

416
00:22:54,630 --> 00:22:56,760
You could see this for like cases

417
00:22:56,760 --> 00:22:59,940
where you only have a device identifier,

418
00:22:59,940 --> 00:23:03,450
but someone is signing
in as a registered user.

419
00:23:03,450 --> 00:23:05,130
And so, the device becomes associated

420
00:23:05,130 --> 00:23:07,130
with the registered user in the service.

421
00:23:10,110 --> 00:23:12,510
We also make use of additional data,

422
00:23:12,510 --> 00:23:16,890
like the program and episode data,

423
00:23:16,890 --> 00:23:19,293
break identifier data,
product and offer data.

424
00:23:22,950 --> 00:23:24,690
So, it's one thing for me to talk about

425
00:23:24,690 --> 00:23:26,493
what SAS 360 Match can do.

426
00:23:27,420 --> 00:23:30,480
It's another to hear it
from one of our customers.

427
00:23:30,480 --> 00:23:31,950
So, Alex Maison,

428
00:23:31,950 --> 00:23:35,857
head of digital ad platforms
at ITV, says the following:

429
00:23:35,857 --> 00:23:38,670
"The flexibility of
SaaS 360 Match allows us

430
00:23:38,670 --> 00:23:40,440
to develop things at our own pace.

431
00:23:40,440 --> 00:23:42,900
We define our own development queue;

432
00:23:42,900 --> 00:23:45,147
we're not waiting for a third party."

433
00:23:47,280 --> 00:23:48,840
What this means is

434
00:23:48,840 --> 00:23:52,080
that Alex can often use
the existing capabilities

435
00:23:52,080 --> 00:23:54,210
within the toolbox that SAS provides

436
00:23:54,210 --> 00:23:57,453
to solve new problems without
waiting for new development.

437
00:24:03,720 --> 00:24:05,342
All right, if there's just three things

438
00:24:05,342 --> 00:24:09,783
to remember from my presentation,
here's what they are:

439
00:24:10,980 --> 00:24:14,680
With SAS, you can leverage
a multitude of data sources

440
00:24:15,570 --> 00:24:17,570
to make the best decision at the moment.

441
00:24:18,630 --> 00:24:21,730
You can help advertisers
reach their intended audience

442
00:24:23,010 --> 00:24:25,590
and provide broadcasters with the tools

443
00:24:25,590 --> 00:24:28,173
to improve the advertising
experience for viewers.

444
00:24:30,330 --> 00:24:32,700
And SAS does all of this at
a scale that can handle some

445
00:24:32,700 --> 00:24:35,643
of the largest events in broadcast sports.

446
00:24:38,340 --> 00:24:42,327
Alright, I'm gonna turn
this over to Felipe now.

447
00:24:42,327 --> 00:24:46,230
- Thank you so much, Brian,
for such an awesome talk.

448
00:24:46,230 --> 00:24:50,340
Before we let you go, allow me to ask:

449
00:24:50,340 --> 00:24:55,340
We spoke quite extensively
about the SAS 360 suite.

450
00:24:55,860 --> 00:24:59,700
Is there any particular
examples of problems

451
00:24:59,700 --> 00:25:01,113
ScyllDB helps you solve?

452
00:25:02,070 --> 00:25:06,840
- Sure. So, in ScyllDB, I
mentioned event accounts.

453
00:25:06,840 --> 00:25:11,840
So, in one case, a customer
had a satisfaction problem

454
00:25:12,330 --> 00:25:14,940
where viewers were unhappy

455
00:25:14,940 --> 00:25:19,290
that when they went to go
watch a video on-demand stream,

456
00:25:19,290 --> 00:25:22,080
they were shown advertisements.

457
00:25:22,080 --> 00:25:24,213
And let's say they didn't
actually want to watch that one,

458
00:25:24,213 --> 00:25:26,040
they wanted to switch
and watch another one,

459
00:25:26,040 --> 00:25:27,933
and then they get ads again.

460
00:25:29,160 --> 00:25:30,399
Well, why?

461
00:25:30,399 --> 00:25:35,340
There's a way to fix this
within the ad software itself.

462
00:25:35,340 --> 00:25:39,877
So, we can count when
you have last seen ads,

463
00:25:40,950 --> 00:25:42,510
a break, for instance.

464
00:25:42,510 --> 00:25:45,120
And let's say you want to implement a rule

465
00:25:45,120 --> 00:25:48,486
that says if another ad request
comes within four minutes

466
00:25:48,486 --> 00:25:51,510
of the last time you saw ads,

467
00:25:51,510 --> 00:25:54,120
we're gonna skip it, we're
just not gonna do it.

468
00:25:54,120 --> 00:25:58,170
And this greatly improved
user satisfaction

469
00:25:58,170 --> 00:26:00,450
for this broadcaster.

470
00:26:00,450 --> 00:26:04,650
It was a very small change to
make that had a huge impact.

471
00:26:04,650 --> 00:26:06,060
- And from that perspective,

472
00:26:06,060 --> 00:26:09,600
would you say ScyllaDB
somewhat sits in the hot path,

473
00:26:09,600 --> 00:26:11,157
critical path, of your new application?

474
00:26:11,157 --> 00:26:13,382
The way I'd like to frame it is,

475
00:26:13,382 --> 00:26:15,690
ScyllaDB, wherever to come down.

476
00:26:15,690 --> 00:26:18,063
What would be the impact
to your size business?

477
00:26:19,740 --> 00:26:21,270
- Well, Scylla doesn't come down.

478
00:26:21,270 --> 00:26:22,103
- Yeah, (chuckles)

479
00:26:22,103 --> 00:26:26,220
- I mean, it's not a
thing; too much redundancy.

480
00:26:26,220 --> 00:26:30,550
So, Scylla is critical
for us for data redundancy

481
00:26:31,770 --> 00:26:34,415
and ensuring we have the
data we need quickly when

482
00:26:34,415 --> 00:26:36,060
we're trying to make an ad decision,

483
00:26:36,060 --> 00:26:39,600
especially in, like, the
worst case scenarios where

484
00:26:39,600 --> 00:26:43,740
it's a new user, we've not
established a session yet,

485
00:26:43,740 --> 00:26:46,950
and so we plan for those contingencies

486
00:26:46,950 --> 00:26:51,690
and ensure that we have the
resources necessary with Scylla

487
00:26:51,690 --> 00:26:54,090
to answer those requests.

488
00:26:54,090 --> 00:26:55,860
- Well, thank you again, Brian.

489
00:26:55,860 --> 00:26:58,470
Brian will be available
later after our presentation

490
00:26:58,470 --> 00:27:00,474
to answer any questions you may have.

491
00:27:00,474 --> 00:27:02,970
With that, Brian, I'll let you go.

492
00:27:02,970 --> 00:27:04,980
So, during his presentation,

493
00:27:04,980 --> 00:27:08,850
Brian spoke about SAS
360 Match API support

494
00:27:08,850 --> 00:27:11,880
both real-time and batch updates to data.

495
00:27:11,880 --> 00:27:15,330
He also mentioned about one
particular ScyllaDB feature

496
00:27:15,330 --> 00:27:17,526
called workload prioritization.

497
00:27:17,526 --> 00:27:20,490
That's a feature that allow users

498
00:27:20,490 --> 00:27:25,260
to assign different SLA priorities
for different workloads.

499
00:27:25,260 --> 00:27:28,380
In that sense, that's a
feature geared towards

500
00:27:28,380 --> 00:27:30,990
infrastructure consolidation,
which allows you

501
00:27:30,990 --> 00:27:35,340
to prioritize real-time workloads
in contrast with workloads

502
00:27:35,340 --> 00:27:38,883
that do not have strict
latency requirements.

503
00:27:40,140 --> 00:27:45,140
So next up, allow me to call
Sreedhar, VP of Engineering,

504
00:27:45,300 --> 00:27:48,390
and Premkumar, senior
manager at Freshworks,

505
00:27:48,390 --> 00:27:50,310
who are going to share their story on

506
00:27:50,310 --> 00:27:54,150
how Freshworks redesigned
their AI software architecture

507
00:27:54,150 --> 00:27:58,603
for 10 times growth while
improving latency by 95%,

508
00:27:59,580 --> 00:28:03,720
all of which with under
significant cost savings.

509
00:28:03,720 --> 00:28:06,540
Sreedhar and Prem, thank
you again for your time,

510
00:28:06,540 --> 00:28:08,940
and with that, the floor is yours.

511
00:28:08,940 --> 00:28:10,680
- Awesome, thank you Felipe.

512
00:28:10,680 --> 00:28:12,330
Hello everyone. Good morning.

513
00:28:12,330 --> 00:28:16,680
This is Sreedhar Gade, head
of AI and data for Freshworks

514
00:28:16,680 --> 00:28:19,350
and it's Prem, my colleague.

515
00:28:19,350 --> 00:28:21,337
- Hi myself, Premkumar.

516
00:28:21,337 --> 00:28:22,878
I handle the data works at Freshworks,

517
00:28:22,878 --> 00:28:24,240
data platform engineering.

518
00:28:24,240 --> 00:28:27,120
My passion is data and
that's what I live for.

519
00:28:27,120 --> 00:28:31,620
- Fantastic, so over, you
know, next 20, 25 minutes,

520
00:28:31,620 --> 00:28:35,040
I'm going, you know, cover this
presentation in three parts.

521
00:28:35,040 --> 00:28:37,410
I'll call Prem back into the stage,

522
00:28:37,410 --> 00:28:38,455
you know, during the initial part.

523
00:28:38,455 --> 00:28:39,288
Thanks Prem.

524
00:28:40,500 --> 00:28:43,080
So, the first part would
be about the Freshworks.

525
00:28:43,080 --> 00:28:46,650
Essentially we talk about
the company and its scale.

526
00:28:46,650 --> 00:28:48,720
That will be a good segue
into the section two,

527
00:28:48,720 --> 00:28:52,973
which is about the entire
journey of our NoSQL,

528
00:28:55,710 --> 00:28:57,800
you know, the quest to
actually find ScyllaDB

529
00:28:57,800 --> 00:28:59,430
as one of the great partners.

530
00:28:59,430 --> 00:29:02,703
And then, how did we actually
go about building multiple,

531
00:29:03,900 --> 00:29:06,930
you know, use cases, and
then our migration story.

532
00:29:06,930 --> 00:29:09,420
Then I'll come back again
and, you know, wrap up

533
00:29:09,420 --> 00:29:12,273
with the AI use cases and then takeaways.

534
00:29:14,580 --> 00:29:15,453
So, that's us.

535
00:29:16,530 --> 00:29:20,460
And so, Freshworks is a company
started about 15 years ago,

536
00:29:20,460 --> 00:29:25,440
in 2010, in a city
called Chennai in India,

537
00:29:25,440 --> 00:29:28,200
with the dream of building
an uncomplicated software

538
00:29:28,200 --> 00:29:29,610
to enable companies

539
00:29:29,610 --> 00:29:32,490
to solve their business workflows easily.

540
00:29:32,490 --> 00:29:34,110
And we started as a Freshdesk,

541
00:29:34,110 --> 00:29:35,520
which is a customer support software.

542
00:29:35,520 --> 00:29:39,390
And over time, we went on
to build just the dream

543
00:29:39,390 --> 00:29:40,979
that we had about, like,
building uncomplicated software

544
00:29:40,979 --> 00:29:42,900
for all sizes of companies.

545
00:29:42,900 --> 00:29:47,760
And 10 years later, we were
the first company, you know,

546
00:29:47,760 --> 00:29:51,270
in the SaaS field to have
gone public in the US, right?

547
00:29:51,270 --> 00:29:52,590
And we are very proud of that.

548
00:29:52,590 --> 00:29:55,860
And now, we have close
to 75,000 customers,

549
00:29:55,860 --> 00:29:59,010
and we serve our products in
over 40 different languages,

550
00:29:59,010 --> 00:30:01,170
and touching, you know,

551
00:30:01,170 --> 00:30:03,090
distance to 1 billion revenues across.

552
00:30:03,090 --> 00:30:04,470
And we are very proud of that.

553
00:30:04,470 --> 00:30:05,880
And how did we build that?

554
00:30:05,880 --> 00:30:08,850
We always looked at the customer side:

555
00:30:08,850 --> 00:30:11,190
how do we create uncomplicated software?

556
00:30:11,190 --> 00:30:12,480
But at the same time,

557
00:30:12,480 --> 00:30:15,150
we also look at uncomplicated
technology stack.

558
00:30:15,150 --> 00:30:18,840
So, you'll see this is a
foundation at the bottom,

559
00:30:18,840 --> 00:30:22,860
where you have a cloud
infrastructure scaling across.

560
00:30:22,860 --> 00:30:26,820
And then you basically will
have the platform, which is,

561
00:30:26,820 --> 00:30:29,400
you know, all the foundational
capabilities, like,

562
00:30:29,400 --> 00:30:32,670
say, single sign-on, and then
analytics and everything else.

563
00:30:32,670 --> 00:30:35,220
And on top of it, you have,
again, foundational AI,

564
00:30:35,220 --> 00:30:37,050
and then you got the
products on top of it.

565
00:30:37,050 --> 00:30:39,930
So, this helped us
scale leap front bounds,

566
00:30:39,930 --> 00:30:43,230
and we are just getting started.

567
00:30:43,230 --> 00:30:45,390
And the journey has been fabulous for us.

568
00:30:45,390 --> 00:30:47,070
You know, like I said, in 2010,

569
00:30:47,070 --> 00:30:50,430
we started with a
product called Freshdesk.

570
00:30:50,430 --> 00:30:52,140
We were a single-product company,

571
00:30:52,140 --> 00:30:55,500
and with a humble, you know,
a 100 to 200 customers.

572
00:30:55,500 --> 00:30:58,560
But then, soon, we started
adding Freshservice.

573
00:30:58,560 --> 00:31:01,620
Freshservice is our ITSM,
you know, offerings.

574
00:31:01,620 --> 00:31:04,230
And now, today, we have asset management,

575
00:31:04,230 --> 00:31:05,250
and the service management,

576
00:31:05,250 --> 00:31:08,940
and even the employee workflow
management also in this.

577
00:31:08,940 --> 00:31:11,400
And this is one of the
fastest growing, you know,

578
00:31:11,400 --> 00:31:14,430
ITSM systems, you know, on the planet,

579
00:31:14,430 --> 00:31:16,410
you know, on the SaaS world.

580
00:31:16,410 --> 00:31:20,580
And as you can see, we also
have, you know, Freddie,

581
00:31:20,580 --> 00:31:22,770
which is our brand name

582
00:31:22,770 --> 00:31:24,810
for the AI offerings across the company.

583
00:31:24,810 --> 00:31:27,330
And then, you'll hear a lot
about Freddie as well, you know,

584
00:31:27,330 --> 00:31:29,670
in the upcoming slides and outset.

585
00:31:29,670 --> 00:31:30,848
So, that's about Freshworks.

586
00:31:30,848 --> 00:31:34,050
So Freshworks, you know, in a summary,

587
00:31:34,050 --> 00:31:35,340
it's a company that was built

588
00:31:35,340 --> 00:31:39,630
to revolutionize the
experience for SaaS customers.

589
00:31:39,630 --> 00:31:41,370
And we did just that.

590
00:31:41,370 --> 00:31:45,000
But then that also basically
means that we keep reinventing

591
00:31:45,000 --> 00:31:47,218
the data technologies and AI technologies

592
00:31:47,218 --> 00:31:49,563
to meet and exceed customer expectations.

593
00:31:51,330 --> 00:31:53,460
So, let me talk about,

594
00:31:53,460 --> 00:31:56,100
double-click into the
data sort of Freshworks,

595
00:31:56,100 --> 00:31:59,670
like, so one size does not fit all, right?

596
00:31:59,670 --> 00:32:03,909
So, we moved from having entire
business logic into MySQL

597
00:32:03,909 --> 00:32:08,130
or RDBMS to multiple data
technologies across the board,

598
00:32:08,130 --> 00:32:10,260
and, like, whatever actually, you know,

599
00:32:10,260 --> 00:32:11,520
fits that particular use case.

600
00:32:11,520 --> 00:32:13,470
And at the same time,
we wanted to make sure

601
00:32:13,470 --> 00:32:16,380
that whatever we have in
our Dataverse portfolio

602
00:32:16,380 --> 00:32:18,120
is the best-in-best in that space.

603
00:32:18,120 --> 00:32:20,880
And that's where, you
know, our whole partnership

604
00:32:20,880 --> 00:32:22,470
with the ScyllaDB actually comes in,

605
00:32:22,470 --> 00:32:24,060
and that's what we are gonna talk about.

606
00:32:24,060 --> 00:32:26,310
So, we care about data engineering.

607
00:32:26,310 --> 00:32:27,420
So, it's not about operations,

608
00:32:27,420 --> 00:32:29,580
or it's not about keeping
things up and running,

609
00:32:29,580 --> 00:32:31,200
it's not about keep the lights on,

610
00:32:31,200 --> 00:32:33,060
it's about engineering for data.

611
00:32:33,060 --> 00:32:35,460
Second thing is, we care
for reliability at scale.

612
00:32:35,460 --> 00:32:38,340
We spoke about scale already,
and reliability matters.

613
00:32:38,340 --> 00:32:41,460
We at Freshworks, at the Dataverse layer,

614
00:32:41,460 --> 00:32:45,480
we give 4.9 consistently, at
sometimes 4.5-4.9 as well,

615
00:32:45,480 --> 00:32:48,630
for all the products
being supported by that.

616
00:32:48,630 --> 00:32:51,510
We also care about
disaster recovery and BCP,

617
00:32:51,510 --> 00:32:52,980
and also data residency.

618
00:32:52,980 --> 00:32:55,740
The customers as increasingly
more sensitive about

619
00:32:55,740 --> 00:32:56,940
where their data is,

620
00:32:56,940 --> 00:33:00,660
and also, how up and running
in terms of, you know,

621
00:33:00,660 --> 00:33:04,350
the disaster recovery in
case of outages that happen

622
00:33:04,350 --> 00:33:05,673
on the cloud or elsewhere.

623
00:33:06,690 --> 00:33:08,580
And security is paramount for us.

624
00:33:08,580 --> 00:33:10,410
We care for the privacy

625
00:33:10,410 --> 00:33:12,720
and security of our customers' data,

626
00:33:12,720 --> 00:33:14,220
and that's where data,

627
00:33:14,220 --> 00:33:17,310
since the entire Dataverse
holds the source of truth,

628
00:33:17,310 --> 00:33:19,020
as well as the persistence of data.

629
00:33:19,020 --> 00:33:19,853
We want to make sure

630
00:33:19,853 --> 00:33:22,800
that the security is
ground up, data at rest,

631
00:33:22,800 --> 00:33:25,803
data in transit, everything is
kind of encrypted and secure.

632
00:33:26,730 --> 00:33:27,930
Last but not least,

633
00:33:27,930 --> 00:33:30,870
we also have close to
thousand-plus developer community

634
00:33:30,870 --> 00:33:32,970
in the company for us to support,

635
00:33:32,970 --> 00:33:35,040
who build our applications and products

636
00:33:35,040 --> 00:33:36,750
and AI on top of Dataverse.

637
00:33:36,750 --> 00:33:41,279
And we give them AI-enabled
self-service portals

638
00:33:41,279 --> 00:33:45,300
for them to be able to
spin off data layers

639
00:33:45,300 --> 00:33:47,460
and then be able to
scale, support, manage,

640
00:33:47,460 --> 00:33:49,080
deploy, everything else.

641
00:33:49,080 --> 00:33:50,730
So, it's very seamless,

642
00:33:50,730 --> 00:33:52,680
and it actually helps them also scale.

643
00:33:52,680 --> 00:33:56,700
So, that's a glimpse of
Dataverse at Freshworks, right?

644
00:33:56,700 --> 00:33:58,800
Now, going deeper into, you know,

645
00:33:58,800 --> 00:34:02,130
how we came about in our ScyllaDB journey.

646
00:34:02,130 --> 00:34:04,890
So, we have been talking
about Cassandra workloads,

647
00:34:04,890 --> 00:34:08,040
and there are multiple NoSQL
use cases within our company.

648
00:34:08,040 --> 00:34:10,980
You'll hear more about those use cases.

649
00:34:10,980 --> 00:34:13,920
We kind of open up our playbook for you,

650
00:34:13,920 --> 00:34:15,450
in a way selflessly,

651
00:34:15,450 --> 00:34:18,480
to talk more deeper into
exactly what are those use cases

652
00:34:18,480 --> 00:34:20,823
and how it kind of
helped along with scale.

653
00:34:21,660 --> 00:34:25,770
But, in a nutshell,
Scylla kind of, you know,

654
00:34:25,770 --> 00:34:27,558
allowed us to actually
bring our own AWS accounts,

655
00:34:27,558 --> 00:34:30,630
because we are super
complex AWS environment,

656
00:34:30,630 --> 00:34:34,080
already in a mesh architecture
with the platforms, and AI,

657
00:34:34,080 --> 00:34:35,370
and applications.

658
00:34:35,370 --> 00:34:36,990
So, we did not wanna change that.

659
00:34:36,990 --> 00:34:39,630
So, we were actually in our own VPCs,

660
00:34:39,630 --> 00:34:43,530
but actually we could bring
in Scylla into our tech stack,

661
00:34:43,530 --> 00:34:46,273
but still have all the
goodness of hosted solution.

662
00:34:46,273 --> 00:34:49,212
And you'll also see
performance and scale metrics.

663
00:34:49,212 --> 00:34:51,300
You'll see, like I said, use cases.

664
00:34:51,300 --> 00:34:53,820
I think you'll be able to
resonate with some of those

665
00:34:53,820 --> 00:34:54,900
as we speak.

666
00:34:54,900 --> 00:34:58,050
And, but what's more important
is: how did we migrate,

667
00:34:58,050 --> 00:35:00,480
with a third-party Cassandra

668
00:35:00,480 --> 00:35:04,207
or open-source Cassandra to
where the Scylla is, right?

669
00:35:04,207 --> 00:35:08,490
And, like, how was the
journey? You'll also see that.

670
00:35:08,490 --> 00:35:10,380
And last but not least,

671
00:35:10,380 --> 00:35:12,660
there are also some
future upcoming use cases

672
00:35:12,660 --> 00:35:13,493
that we are talking about.

673
00:35:13,493 --> 00:35:15,690
You know, this world is all about AI.

674
00:35:15,690 --> 00:35:17,640
So, everything to do with AI,

675
00:35:17,640 --> 00:35:19,410
and how we are empowering
it using self-service

676
00:35:19,410 --> 00:35:20,580
or agentic workflows,

677
00:35:20,580 --> 00:35:24,390
how Scylla is going to actually
help us get there, right?

678
00:35:24,390 --> 00:35:26,970
Like I said, our customers
care for data residency

679
00:35:26,970 --> 00:35:29,370
and that's one of the reasons where,

680
00:35:29,370 --> 00:35:31,260
while we use a managed service,

681
00:35:31,260 --> 00:35:34,113
the entire data plane
is within our own VPC.

682
00:35:35,400 --> 00:35:37,050
We have full control over it,

683
00:35:37,050 --> 00:35:38,110
and then we have full access to it,

684
00:35:38,110 --> 00:35:40,710
and then it is actually
within the boundary

685
00:35:40,710 --> 00:35:43,380
of our security guardrails.

686
00:35:43,380 --> 00:35:46,110
And the control plane is with Scylla,

687
00:35:46,110 --> 00:35:49,020
and there's a perfect
combination, you know,

688
00:35:49,020 --> 00:35:52,080
and it really helped us
quickly deploy, quickly scale,

689
00:35:52,080 --> 00:35:53,230
and take it from there.

690
00:35:55,260 --> 00:35:57,060
And this is, I think, a defining slide,

691
00:35:57,060 --> 00:35:59,190
because if you look at the numbers there,

692
00:35:59,190 --> 00:36:00,630
when we actually did the benchmark,

693
00:36:00,630 --> 00:36:04,500
when the executives from Scylla
came and met us in Chennai

694
00:36:04,500 --> 00:36:06,510
and said, "Hey, you know,
we are this company,

695
00:36:06,510 --> 00:36:08,520
and we actually solved
Cassandra problems."

696
00:36:08,520 --> 00:36:10,650
I said like," Hey, this problem
is already been solved."

697
00:36:10,650 --> 00:36:11,640
Apparently not.

698
00:36:11,640 --> 00:36:12,930
So, when we actually looked at it,

699
00:36:12,930 --> 00:36:14,040
and they did the benchmarks,

700
00:36:14,040 --> 00:36:16,680
and I were like, we were
just blown away with the,

701
00:36:16,680 --> 00:36:18,543
especially, you know, tail,

702
00:36:19,447 --> 00:36:22,020
you know, the performance,
is like much, much higher.

703
00:36:22,020 --> 00:36:25,083
It, like, you know, P99.99,

704
00:36:26,520 --> 00:36:27,900
you know, percentile, right?

705
00:36:27,900 --> 00:36:29,820
We generally look at P90, P95;

706
00:36:29,820 --> 00:36:31,470
here we are looking at that depth,

707
00:36:31,470 --> 00:36:33,660
which means that no single
request is actually,

708
00:36:33,660 --> 00:36:35,700
like, left behind in
terms of the performance.

709
00:36:35,700 --> 00:36:37,233
So, that was our experience.

710
00:36:38,070 --> 00:36:40,083
And, so with this,

711
00:36:42,000 --> 00:36:43,850
I basically, like, can hand over to Prem,

712
00:36:43,850 --> 00:36:46,391
to actually walk us through
some of the use cases.

713
00:36:46,391 --> 00:36:48,690
You can actually go through
it, and then I'll come back

714
00:36:48,690 --> 00:36:49,523
and wrap it up.

715
00:36:49,523 --> 00:36:50,763
Prem, over to you.

716
00:36:51,901 --> 00:36:53,901
- Thank you sir.
- Yeah,

717
00:36:58,507 --> 00:37:01,680
- I'm gonna talk about some of
the use cases at Freshworks.

718
00:37:01,680 --> 00:37:03,480
We use ScyllaDB.

719
00:37:03,480 --> 00:37:05,700
So, I'm gonna cover three different types

720
00:37:05,700 --> 00:37:08,100
of database migrations
as well along with this.

721
00:37:08,100 --> 00:37:10,470
So, the first one will be
covering Cassandra to ScyllaDB,

722
00:37:10,470 --> 00:37:13,740
which is a natural migration
for us to consider.

723
00:37:13,740 --> 00:37:16,140
And the second one will be,
like, how can MySQL data

724
00:37:16,140 --> 00:37:18,445
to be moved towards ScyllaDB?

725
00:37:18,445 --> 00:37:20,670
And the third one will be DynamoDB data,

726
00:37:20,670 --> 00:37:22,680
with limitations kind of improved

727
00:37:22,680 --> 00:37:24,230
while we move towards ScyllaDB.

728
00:37:25,110 --> 00:37:28,140
So, Conversation Store is a
platform service at Freshworks.

729
00:37:28,140 --> 00:37:30,211
This kind of covers all
the conversations, kind

730
00:37:30,211 --> 00:37:33,217
of stores all the product
conversations at one shell,

731
00:37:33,217 --> 00:37:36,780
and then this kind of also
as translation service,

732
00:37:36,780 --> 00:37:38,370
this can also be useful for us

733
00:37:38,370 --> 00:37:39,900
to store the AI conversations

734
00:37:39,900 --> 00:37:41,517
between customer and AI agents.

735
00:37:41,517 --> 00:37:44,043
And this is powered by Cassandra.

736
00:37:46,890 --> 00:37:49,170
And we also have UCR,

737
00:37:49,170 --> 00:37:52,890
which we call as unified customer portal.

738
00:37:52,890 --> 00:37:55,650
Like, we kind of actually store
all the contacts, whatever

739
00:37:55,650 --> 00:37:57,150
that is part of all these products,

740
00:37:57,150 --> 00:37:58,800
like Freshdesk and Freshservice,

741
00:37:58,800 --> 00:37:59,633
in one place.

742
00:37:59,633 --> 00:38:02,301
And we also retrieve it on
a very low latency level.

743
00:38:02,301 --> 00:38:03,630
So that's a requirement.

744
00:38:03,630 --> 00:38:05,370
So, we were using Cassandra for this.

745
00:38:05,370 --> 00:38:07,110
There were some bottlenecks in that,

746
00:38:07,110 --> 00:38:08,490
and also some availability issues.

747
00:38:08,490 --> 00:38:10,960
So, we did consider
ScyllaDB for that as well.

748
00:38:12,537 --> 00:38:14,490
And as I said, the scale is kind

749
00:38:14,490 --> 00:38:17,220
of 4 billion-plus contacts
saved in this portal,

750
00:38:17,220 --> 00:38:19,560
and the expectation is
very minimal, right?

751
00:38:19,560 --> 00:38:21,600
The latency should be
below 5ms consistently,

752
00:38:21,600 --> 00:38:25,740
like, it should be even P99.99
should be also below 5ms.

753
00:38:25,740 --> 00:38:28,803
And the current scale is 15K
ops per second in this portal.

754
00:38:30,180 --> 00:38:32,340
So, I'm gonna walk through
the migration story.

755
00:38:32,340 --> 00:38:33,450
And as I said,

756
00:38:33,450 --> 00:38:35,825
we had some difficulties in
Cassandra, like, every weekend,

757
00:38:35,825 --> 00:38:38,836
whenever repairs runs, deletion runs,

758
00:38:38,836 --> 00:38:40,980
we add fall into some latencies,

759
00:38:40,980 --> 00:38:42,630
and also timeouts in Cassandra.

760
00:38:42,630 --> 00:38:44,700
And also, we did try some
third-party Cassandras

761
00:38:44,700 --> 00:38:45,533
like DataStax.

762
00:38:45,533 --> 00:38:46,950
There was some availability issues.

763
00:38:46,950 --> 00:38:50,280
So, we did consider ScyllaDB
for this particular action.

764
00:38:50,280 --> 00:38:52,590
And I'm gonna walk through
the migration story,

765
00:38:52,590 --> 00:38:54,581
which is kind of a
little interesting for us

766
00:38:54,581 --> 00:38:57,330
and also challenging for
us to push our limits,

767
00:38:57,330 --> 00:38:59,460
because we want to do
this in a live migration,

768
00:38:59,460 --> 00:39:01,710
because we wanted to give
the customer the ease

769
00:39:01,710 --> 00:39:03,480
of continuing the use of database

770
00:39:03,480 --> 00:39:05,070
and applications even on the weekends.

771
00:39:05,070 --> 00:39:08,460
So we cannot afford any downtime
and that's our benchmark.

772
00:39:08,460 --> 00:39:12,150
We wanted to, you know, make
sure the customer is always up

773
00:39:12,150 --> 00:39:13,913
and able to serve, because we serve across

774
00:39:13,913 --> 00:39:15,330
all part of the country, right?

775
00:39:15,330 --> 00:39:17,310
And also, all part of the world.

776
00:39:17,310 --> 00:39:21,060
So customers, we kind of push
our standards that we ensure

777
00:39:21,060 --> 00:39:23,910
that there is a zero
downtime migration always.

778
00:39:23,910 --> 00:39:26,610
And we kind of taken
the first complication

779
00:39:26,610 --> 00:39:29,400
and introduced ZDM proxy
in the middle of Cassandra,

780
00:39:29,400 --> 00:39:31,590
which can actually do dual reads for us,

781
00:39:31,590 --> 00:39:33,303
for both Cassandra and Scylla.

782
00:39:34,320 --> 00:39:35,580
So, while it does dual reads,

783
00:39:35,580 --> 00:39:36,930
the reads goes through Cassandra,

784
00:39:36,930 --> 00:39:38,280
that's our existing system.

785
00:39:40,860 --> 00:39:43,620
So, when I say dual
reads, all the new data,

786
00:39:43,620 --> 00:39:45,210
like when, right from the beginning

787
00:39:45,210 --> 00:39:48,570
of the ZDM proxy gets
captured in both the places,

788
00:39:48,570 --> 00:39:49,977
Cassandra and Scylla.

789
00:39:49,977 --> 00:39:51,552
And now that CDM migrator comes

790
00:39:51,552 --> 00:39:54,720
to migrate the historical
data, like, from where it left.

791
00:39:54,720 --> 00:39:56,190
So, we started dual reads.

792
00:39:56,190 --> 00:39:57,960
Now we have to migrate the historical data

793
00:39:57,960 --> 00:39:59,730
into the new system, ScyllaDB.

794
00:39:59,730 --> 00:40:01,290
So, we used the CDM migrator.

795
00:40:01,290 --> 00:40:04,170
We migrated all the
historical data into ScyllaDB.

796
00:40:04,170 --> 00:40:05,793
And now that it has both historical data

797
00:40:05,793 --> 00:40:08,707
and the latest data through
dual reads from the ZDM.

798
00:40:10,737 --> 00:40:13,241
And this is very well put
out here as well, like,

799
00:40:13,241 --> 00:40:15,744
how the CDM migrator reads data,

800
00:40:15,744 --> 00:40:18,180
and then pushes to ScyllaDB.

801
00:40:18,180 --> 00:40:19,620
And, after we complete the migration,

802
00:40:19,620 --> 00:40:21,060
we also wanted to validate,

803
00:40:21,060 --> 00:40:22,380
and then fix the missing gaps

804
00:40:22,380 --> 00:40:24,030
in terms of repairs and things.

805
00:40:24,030 --> 00:40:26,100
So, we kind of actually
use the CDM migrator

806
00:40:26,100 --> 00:40:28,020
to validate all the data is copied

807
00:40:28,020 --> 00:40:30,630
from Cassandra to Scylla.

808
00:40:30,630 --> 00:40:33,030
So, while we do that, we
find some of the gaps,

809
00:40:33,030 --> 00:40:36,420
because the data we talk
here is close to terabytes,

810
00:40:36,420 --> 00:40:38,400
like 1.2 terabytes to be exact,

811
00:40:38,400 --> 00:40:39,990
and we find some bottlenecks,

812
00:40:39,990 --> 00:40:41,580
and job is keep on running for a while.

813
00:40:41,580 --> 00:40:44,340
And then we kind of took the
open-source CDM migrator,

814
00:40:44,340 --> 00:40:46,800
modified with batch processing options.

815
00:40:46,800 --> 00:40:50,040
This helps us to kind of
add the batch processing

816
00:40:50,040 --> 00:40:51,840
and improve the time to 10X.

817
00:40:51,840 --> 00:40:53,790
So, one advantage what
we have in this portal

818
00:40:53,790 --> 00:40:54,780
and this use case is,

819
00:40:54,780 --> 00:40:59,070
this only powers majority of
inserts, not much are updated.

820
00:40:59,070 --> 00:41:00,480
So, which we can make sure

821
00:41:00,480 --> 00:41:02,002
that only if we check
the record exists or not,

822
00:41:02,002 --> 00:41:04,440
that should be enough for us to cross

823
00:41:04,440 --> 00:41:06,360
through the validation phase.

824
00:41:06,360 --> 00:41:08,670
So, we were able to get 10X improvement.

825
00:41:08,670 --> 00:41:10,517
And now the CDM migrator
is substreamed as well,

826
00:41:10,517 --> 00:41:13,860
and ScyllaDB is using for other customers,

827
00:41:13,860 --> 00:41:17,392
while they migrate
Cassandra to Scylla as well.

828
00:41:17,392 --> 00:41:21,120
And as I said, validation
is completed now.

829
00:41:21,120 --> 00:41:25,620
We also go to kind of we wanted
to switch the traffic right?

830
00:41:25,620 --> 00:41:27,450
And we kind of switch the traffic

831
00:41:27,450 --> 00:41:29,550
from, I think I moved this slide.

832
00:41:29,550 --> 00:41:31,080
Yeah, we kind of switch the traffic

833
00:41:31,080 --> 00:41:33,270
from Cassandra to Scylla at this point.

834
00:41:33,270 --> 00:41:34,103
Now, while we do that,

835
00:41:34,103 --> 00:41:36,660
we want to ensure the
reads are switched first,

836
00:41:36,660 --> 00:41:39,450
and then validated through
logs for any errors,

837
00:41:39,450 --> 00:41:41,490
or any timeouts, and things like that.

838
00:41:41,490 --> 00:41:42,390
While we are comfortable,

839
00:41:42,390 --> 00:41:45,480
then we kind of move the right
traffic also to ScyllaDB.

840
00:41:45,480 --> 00:41:47,730
And we also ensure that,

841
00:41:47,730 --> 00:41:49,170
after the write is switched,

842
00:41:49,170 --> 00:41:51,057
some kind of ZDM proxy dual reads happens

843
00:41:51,057 --> 00:41:52,080
to Cassandra as well,

844
00:41:52,080 --> 00:41:54,810
because that gives us the
convenience and comfortness for us

845
00:41:54,810 --> 00:41:57,090
to come back to Cassandra if
there is any issues faced.

846
00:41:57,090 --> 00:41:59,182
But fortunately, there's just one path,

847
00:41:59,182 --> 00:42:01,612
once we moved to Scylla, there
is no coming back for us,

848
00:42:01,612 --> 00:42:04,362
because the system was
scaling what we expected

849
00:42:04,362 --> 00:42:07,165
and, you know, we were able
to always save the traffic

850
00:42:07,165 --> 00:42:07,998
within the SLA.

851
00:42:07,998 --> 00:42:10,740
And the tail latencies
were really helpful,

852
00:42:10,740 --> 00:42:12,693
like, Steve mentioned initially.

853
00:42:14,160 --> 00:42:15,930
And I move on to the next use case.

854
00:42:15,930 --> 00:42:17,430
This is Workflow Automator use case.

855
00:42:17,430 --> 00:42:19,560
So, when I say Workflow Automator,

856
00:42:19,560 --> 00:42:21,810
this is spun up of our all the workflows

857
00:42:21,810 --> 00:42:24,150
that runs across our
systems, all the products

858
00:42:24,150 --> 00:42:26,670
and all the platforms
services, all the AI use cases.

859
00:42:26,670 --> 00:42:28,457
So, this is a bound use case with Scylla.

860
00:42:28,457 --> 00:42:30,210
There's no migration involved here.

861
00:42:30,210 --> 00:42:32,370
The Scylla was directly supporting this.

862
00:42:32,370 --> 00:42:35,250
And the uniqueness is, this
our multiple concurrency,

863
00:42:35,250 --> 00:42:39,930
and we have close to 250k-plus
workflows stored here,

864
00:42:39,930 --> 00:42:42,150
and parallel executions has to happen.

865
00:42:42,150 --> 00:42:43,906
And this is operating on the
top of Netflix Conductor,

866
00:42:43,906 --> 00:42:48,810
the open-source software that
demands a Cassandra backend.

867
00:42:48,810 --> 00:42:50,250
So, we kind of selected Scylla

868
00:42:50,250 --> 00:42:51,690
because the throughput is very high,

869
00:42:51,690 --> 00:42:52,760
and now AI coming into
picture, there's a lot

870
00:42:52,760 --> 00:42:54,597
of even more further workflows

871
00:42:54,597 --> 00:42:57,930
that has to be run
seamlessly and optimized.

872
00:42:57,930 --> 00:43:00,240
So, we selected ScyllaDB for this use case

873
00:43:00,240 --> 00:43:03,720
and it is being served
very, very much in ease,

874
00:43:03,720 --> 00:43:05,670
and also within the latency as he said.

875
00:43:08,160 --> 00:43:10,230
Yeah, so moving on to the next one.

876
00:43:10,230 --> 00:43:12,960
So, I've covered the
Cassandra to ScyllaDB story,

877
00:43:12,960 --> 00:43:15,660
now I'm coming towards
what we wanted to migrate

878
00:43:15,660 --> 00:43:17,490
from MySQL to ScyllaDB.

879
00:43:17,490 --> 00:43:20,670
So Freshworks historically
is a MySQLcumRuby store.

880
00:43:20,670 --> 00:43:23,640
So, we say the source
of truth lies in MySQL.

881
00:43:23,640 --> 00:43:26,580
So, while we say that, I mean we are able

882
00:43:26,580 --> 00:43:28,020
to handle the scale today,

883
00:43:28,020 --> 00:43:29,970
because we are able to,

884
00:43:29,970 --> 00:43:31,890
we are always thinking about
handling next five years

885
00:43:31,890 --> 00:43:33,810
what can happen and
what will be the scale,

886
00:43:33,810 --> 00:43:36,960
and we want to see to that
this scale has to be supported

887
00:43:36,960 --> 00:43:39,120
as I said, without any downtime,

888
00:43:39,120 --> 00:43:42,030
and without any customer latencies,

889
00:43:42,030 --> 00:43:43,530
and without any,

890
00:43:43,530 --> 00:43:44,931
with all the new features
coming up with the AI,

891
00:43:44,931 --> 00:43:45,764
and things like that.

892
00:43:45,764 --> 00:43:47,610
We want to deep archive
and things like that.

893
00:43:47,610 --> 00:43:49,650
So, we thought to move
the text and BLOB data,

894
00:43:49,650 --> 00:43:51,147
that is saved in MySQL today,

895
00:43:51,147 --> 00:43:53,841
and that takes close to 2
petabytes of data for us,

896
00:43:53,841 --> 00:43:57,600
because all the historical
conversations logs

897
00:43:57,600 --> 00:43:59,548
in conversations store,
but again, the tickets

898
00:43:59,548 --> 00:44:02,240
and ticket bodies data lies in the MySQL

899
00:44:02,240 --> 00:44:03,540
at this point of time.

900
00:44:03,540 --> 00:44:06,240
So, we felt this is only a lookup content,

901
00:44:06,240 --> 00:44:08,400
there is no filtering or
anything on the top of this.

902
00:44:08,400 --> 00:44:09,726
So, this can move to a different store.

903
00:44:09,726 --> 00:44:13,050
We evaluated multiple stores
for this particular operations

904
00:44:13,050 --> 00:44:14,820
and this particular use case,

905
00:44:14,820 --> 00:44:17,880
and we finally felt a ScyllaDB

906
00:44:17,880 --> 00:44:20,193
is again offering this low latency,

907
00:44:21,120 --> 00:44:22,858
and also due to the
reliability, and availability,

908
00:44:22,858 --> 00:44:25,290
and scalability options which they have.

909
00:44:25,290 --> 00:44:29,400
We felt this can also fit into ScyllaDB.

910
00:44:29,400 --> 00:44:30,510
And while I talk,

911
00:44:30,510 --> 00:44:32,940
the problem which we face
today in MySQL is we want

912
00:44:32,940 --> 00:44:34,178
to upgrade the systems pretty often,

913
00:44:34,178 --> 00:44:35,131
like six months from today,

914
00:44:35,131 --> 00:44:37,410
because of the year-old kind of situation.

915
00:44:37,410 --> 00:44:40,470
And, to migrate this petabytes
of data without downtime

916
00:44:40,470 --> 00:44:42,330
is becoming more, and more challenging.

917
00:44:42,330 --> 00:44:44,730
And whenever there's increased
workloads from customers,

918
00:44:44,730 --> 00:44:47,280
like, you know, they wanted to
process some historical jobs

919
00:44:47,280 --> 00:44:48,666
and they wanted to process
historical contents,

920
00:44:48,666 --> 00:44:51,239
and due to some Cyber Monday
deals, the traffic increases.

921
00:44:51,239 --> 00:44:53,130
We will have to scale it on the ease.

922
00:44:53,130 --> 00:44:54,930
And all this becomes little complicated

923
00:44:54,930 --> 00:44:56,129
in the relational database world,

924
00:44:56,129 --> 00:45:00,090
but this is what is Scylla
gonna help us in benefiting.

925
00:45:00,090 --> 00:45:01,320
So, there are two advantages.

926
00:45:01,320 --> 00:45:04,410
So, we kind of bring the
MySQL storage lesser,

927
00:45:04,410 --> 00:45:06,780
which means we can handle the MySQL store

928
00:45:06,780 --> 00:45:07,950
with more further ease-ness,

929
00:45:07,950 --> 00:45:09,861
and also we can scale up,
scale down replication,

930
00:45:09,861 --> 00:45:12,634
and all that will happen within the SLA.

931
00:45:12,634 --> 00:45:15,360
And on the other end, like,

932
00:45:15,360 --> 00:45:17,807
where the majority data is
gonna shift towards Scylla,

933
00:45:17,807 --> 00:45:20,220
we can actually scale
seamlessly, vertically,

934
00:45:20,220 --> 00:45:22,140
horizontally, in however fashion we want.

935
00:45:22,140 --> 00:45:24,750
And also do version upgrades,
patching without any downtime.

936
00:45:24,750 --> 00:45:26,520
So, this is advantage we see.

937
00:45:26,520 --> 00:45:28,770
And while we see that, we also wanted

938
00:45:28,770 --> 00:45:30,390
to migrate the petabytes of data, right?

939
00:45:30,390 --> 00:45:32,580
And it's not gonna be a direct option.

940
00:45:32,580 --> 00:45:35,850
And we have the same challenges,
like, incremental data,

941
00:45:35,850 --> 00:45:38,610
and also the historical data migration

942
00:45:38,610 --> 00:45:39,450
that has to be done here.

943
00:45:39,450 --> 00:45:41,310
And while I say that,

944
00:45:41,310 --> 00:45:44,010
we kind of selected
Kafka approach for this,

945
00:45:44,010 --> 00:45:46,830
for the CDC changes,
the incremental changes.

946
00:45:46,830 --> 00:45:49,560
And then the historical data
comes through Parquet format.

947
00:45:49,560 --> 00:45:54,560
So, we like talk about
this, I also need to,

948
00:45:54,870 --> 00:45:58,293
yes, so we need to do 2
petabytes of data migration here,

949
00:45:58,293 --> 00:45:59,580
as I already said.

950
00:45:59,580 --> 00:46:02,790
And we kind of use Kafka Connect,

951
00:46:02,790 --> 00:46:06,480
Debezium-backed CDC options from MySQL.

952
00:46:06,480 --> 00:46:09,570
This will copy the bin logs
into Kafka through events,

953
00:46:09,570 --> 00:46:12,540
and then save all these
incremental changes.

954
00:46:12,540 --> 00:46:14,520
And that can be put into a ScyllaDB

955
00:46:14,520 --> 00:46:17,700
through ScyllaDB Stream Store.

956
00:46:17,700 --> 00:46:19,710
So, this will actually,

957
00:46:19,710 --> 00:46:22,440
again, part of the Kafka
Connect built by Scylla's team.

958
00:46:22,440 --> 00:46:25,110
This Sink Connector will
actually read all the data

959
00:46:25,110 --> 00:46:28,260
from Kafka and pushes
to Scylla seamlessly.

960
00:46:28,260 --> 00:46:29,940
So, the incremental data comes in,

961
00:46:29,940 --> 00:46:34,940
and from MySQL, we will use the
S3 export option to Parquet,

962
00:46:35,160 --> 00:46:37,830
and then Parque files can be
loaded into Scylla directly.

963
00:46:37,830 --> 00:46:40,383
And then we'll validate using
our custom Lambda solution so

964
00:46:40,383 --> 00:46:43,230
that the data present seamlessly,

965
00:46:43,230 --> 00:46:45,060
and we'll switch the
read traffic initially

966
00:46:45,060 --> 00:46:47,400
to make sure that the
read things are happening

967
00:46:47,400 --> 00:46:50,520
and processed fine with
the Scylla systems,

968
00:46:50,520 --> 00:46:52,620
and then we'll switch the
write traffic seamlessly.

969
00:46:52,620 --> 00:46:53,970
So that's the plan we have.

970
00:46:55,710 --> 00:46:57,810
And I've already covered MySQL use case

971
00:46:57,810 --> 00:46:58,770
and the Cassandra use case.

972
00:46:58,770 --> 00:47:00,402
Moving on to the Dynamo use case.

973
00:47:00,402 --> 00:47:03,660
Today, our activities, microservices,

974
00:47:03,660 --> 00:47:05,220
it sits on the top of Dynamo.

975
00:47:05,220 --> 00:47:07,350
So, let's see activities,
it's another use case.

976
00:47:07,350 --> 00:47:10,860
Use case number three,
which we actually see,

977
00:47:10,860 --> 00:47:12,487
some limitations because
of the size, right?

978
00:47:12,487 --> 00:47:16,800
You know, the item size in
Dynamo is limited to 400 kb,

979
00:47:16,800 --> 00:47:19,620
which is actually pushing
us to, you know, split it,

980
00:47:19,620 --> 00:47:21,240
and then write it into Dynamo suite.

981
00:47:21,240 --> 00:47:23,310
Dynamo is fine, that's completely cool,

982
00:47:23,310 --> 00:47:24,450
it's great to use as well.

983
00:47:24,450 --> 00:47:27,510
But again, this limitation
pushes into multiple options,

984
00:47:27,510 --> 00:47:29,400
like, we'll have to split
it into multiple records,

985
00:47:29,400 --> 00:47:32,310
and then write it, and
then search it again.

986
00:47:32,310 --> 00:47:35,370
All these logics are handling
some compute throttling

987
00:47:35,370 --> 00:47:36,330
in our compute layer.

988
00:47:36,330 --> 00:47:39,480
So, we wanted to see if we
can migrate this to Scylla,

989
00:47:39,480 --> 00:47:41,670
because the limitations
are not there as such.

990
00:47:41,670 --> 00:47:45,000
And also, we wanted to scale this further

991
00:47:45,000 --> 00:47:48,930
and also be FedRAMP-compliant
and things in the future.

992
00:47:48,930 --> 00:47:50,790
So, we selected this particular option.

993
00:47:50,790 --> 00:47:53,730
And for this, again, we thought to migrate

994
00:47:53,730 --> 00:47:57,690
both historical data as well
as the incremental data.

995
00:47:57,690 --> 00:47:59,107
A pretty much same approach like MySQL,

996
00:47:59,107 --> 00:48:01,440
but different connectors are used here.

997
00:48:01,440 --> 00:48:04,830
So, we kind of enabled
the Dynamo Streams here.

998
00:48:04,830 --> 00:48:05,897
So, Dynamo Streams will actually

999
00:48:05,897 --> 00:48:09,990
have the change data capture
in Dynamo, and from there,

1000
00:48:09,990 --> 00:48:12,480
we kind of actually use our custom Lambda

1001
00:48:12,480 --> 00:48:16,650
to read these events from
DybamoDB and push to Kafka events.

1002
00:48:16,650 --> 00:48:17,790
So, this will actually take care

1003
00:48:17,790 --> 00:48:20,280
of the incremental changes,
as we already said.

1004
00:48:20,280 --> 00:48:23,550
And this incremental changes
will be taken to ScyllaDB,

1005
00:48:23,550 --> 00:48:25,410
like, using the same Sink Connector

1006
00:48:25,410 --> 00:48:27,660
that is provided by Scylla team itself.

1007
00:48:27,660 --> 00:48:30,090
And then data receipt
in Scylla at this point.

1008
00:48:30,090 --> 00:48:31,620
And the historical data will be taken

1009
00:48:31,620 --> 00:48:33,275
from the Spark job this time.

1010
00:48:33,275 --> 00:48:36,090
we'll run a Spark map
produce and copy the data

1011
00:48:36,090 --> 00:48:39,368
from historical data
from Dynamo to ScyllaDB.

1012
00:48:39,368 --> 00:48:41,355
And then move the traffic to Scylla, like,

1013
00:48:41,355 --> 00:48:43,620
the way same switch, like read and write.

1014
00:48:43,620 --> 00:48:45,990
So, this is well called
out here in four phases.

1015
00:48:45,990 --> 00:48:48,420
The first phase covers
migrating the schema

1016
00:48:48,420 --> 00:48:50,790
from Dynamo to ScyllaDB,

1017
00:48:50,790 --> 00:48:53,190
and then we'll also
enable the dynamo streams,

1018
00:48:53,190 --> 00:48:56,520
as I said, that's what is
called ODC/CDC/Dual writes.

1019
00:48:56,520 --> 00:48:58,920
So, this will take the data to Kafka,

1020
00:48:58,920 --> 00:49:02,130
and from Kafka we'll
have to take it to Scylla

1021
00:49:02,130 --> 00:49:04,290
that is covered in the
phase two and phase three.

1022
00:49:04,290 --> 00:49:07,500
And then, we'll also cover the
check sync validation, right?

1023
00:49:07,500 --> 00:49:10,200
That will happen through
our custom Lambda.

1024
00:49:10,200 --> 00:49:11,997
And then, we'll have to do the read

1025
00:49:11,997 --> 00:49:13,683
and write switch periodically.

1026
00:49:15,270 --> 00:49:17,970
So, with this, I hand it over to Sree,

1027
00:49:17,970 --> 00:49:20,014
to cover the next feature
use cases at ScyllaDB

1028
00:49:20,014 --> 00:49:21,180
for Freshworks.

1029
00:49:21,180 --> 00:49:22,740
Thank you.

1030
00:49:22,740 --> 00:49:23,573
Thank you Sree.

1031
00:49:24,690 --> 00:49:26,010
- Alright, thanks Prem.

1032
00:49:26,010 --> 00:49:30,390
So, just to wrap it up, it's
not just the traditional MySQL,

1033
00:49:30,390 --> 00:49:33,480
or, you know, these use cases,

1034
00:49:33,480 --> 00:49:36,780
but we are going forward,
we are going to, you know,

1035
00:49:36,780 --> 00:49:39,330
have more, like, doubling down on AI.

1036
00:49:39,330 --> 00:49:41,220
So, what are the different use
cases you're talking about?

1037
00:49:41,220 --> 00:49:42,810
So, there are two specific use cases

1038
00:49:42,810 --> 00:49:44,430
that we are looking at going forward.

1039
00:49:44,430 --> 00:49:47,250
One is something called, you know,

1040
00:49:47,250 --> 00:49:50,040
we are talking about,
like a Feature Store where

1041
00:49:50,040 --> 00:49:53,460
essentially, like, most of
your features, most of your,

1042
00:49:53,460 --> 00:49:57,360
you know, the use cases
are trying to reach LLMs.

1043
00:49:57,360 --> 00:49:59,370
In some cases, trying to reach agents.

1044
00:49:59,370 --> 00:50:01,800
So, all these conversations
are kind of on a cached

1045
00:50:01,800 --> 00:50:03,270
in ScyllaDB.

1046
00:50:03,270 --> 00:50:06,270
So that way, your traffic
between the, you know your app

1047
00:50:06,270 --> 00:50:08,130
and the LLMs are actually kind of reduced.

1048
00:50:08,130 --> 00:50:10,770
Essentially, the overall
round trips depend upon

1049
00:50:10,770 --> 00:50:12,510
the performance of LLM as well.

1050
00:50:12,510 --> 00:50:14,725
And when you make multiple
calls that kind of will be,

1051
00:50:14,725 --> 00:50:16,890
like, a very noticeable latency.

1052
00:50:16,890 --> 00:50:19,512
So, this is one caching use
case that we are looking at.

1053
00:50:19,512 --> 00:50:23,550
So basically, ultra-low
predictable, you know, latency.

1054
00:50:23,550 --> 00:50:25,293
And the second one is,

1055
00:50:25,293 --> 00:50:27,746
we are talking about, like,
a feature caching as well.

1056
00:50:27,746 --> 00:50:30,780
So yeah, this is the one I
was just talking about where,

1057
00:50:30,780 --> 00:50:32,880
you know, how do you,
so this is basically,

1058
00:50:32,880 --> 00:50:36,300
in between you and the LLM essentially.

1059
00:50:36,300 --> 00:50:38,123
So, this is something we
are here to experiment,

1060
00:50:38,123 --> 00:50:39,450
and we are hoping

1061
00:50:39,450 --> 00:50:42,753
that I think we'll see
some good improvement here.

1062
00:50:44,160 --> 00:50:45,840
Next one is the data source for modeling.

1063
00:50:45,840 --> 00:50:47,880
So, this is more like an MLOps use case.

1064
00:50:47,880 --> 00:50:50,880
So, Freshworks has two sets of models

1065
00:50:50,880 --> 00:50:52,530
that we use in an AI use case.

1066
00:50:52,530 --> 00:50:53,700
One is a generative AI,

1067
00:50:53,700 --> 00:50:56,520
where we do not do any kind
of training, we just go,

1068
00:50:56,520 --> 00:50:59,238
this is Azure OpenAI and a few other ones.

1069
00:50:59,238 --> 00:51:00,454
These are the large language models.

1070
00:51:00,454 --> 00:51:03,090
We don't do any training, we
just do inference with them.

1071
00:51:03,090 --> 00:51:04,468
But we also have self-hosted models.

1072
00:51:04,468 --> 00:51:08,790
There are smaller models where
they're kind of optimized

1073
00:51:08,790 --> 00:51:11,310
for some kind of a very specific use cases

1074
00:51:11,310 --> 00:51:12,900
in industry verticals that we have.

1075
00:51:12,900 --> 00:51:15,300
It's like, something
like ITSM or FaceDesk.

1076
00:51:15,300 --> 00:51:17,455
So, these are the ones
that we kind of train them

1077
00:51:17,455 --> 00:51:18,854
and these training
cycles are becoming more,

1078
00:51:18,854 --> 00:51:21,450
and more frequent, and when
we are working with this,

1079
00:51:21,450 --> 00:51:22,590
we are also looking at to see,

1080
00:51:22,590 --> 00:51:25,770
can we actually have, you know, ScyllaDB,

1081
00:51:25,770 --> 00:51:27,660
like, you know, in the training pipeline.

1082
00:51:27,660 --> 00:51:31,950
So that way, Prem spoke
about 2 petabytes of data,

1083
00:51:31,950 --> 00:51:33,360
and all the data needs

1084
00:51:33,360 --> 00:51:35,550
to actually go into
this training pipeline.

1085
00:51:35,550 --> 00:51:37,500
So how do we actually kind
of do some kind of caching

1086
00:51:37,500 --> 00:51:39,570
to speed this up, right?

1087
00:51:39,570 --> 00:51:40,963
So, and in the future,

1088
00:51:40,963 --> 00:51:43,410
we are talking about, like
notification services,

1089
00:51:43,410 --> 00:51:45,960
we are talking about URL
shortening, this is again,

1090
00:51:46,920 --> 00:51:48,930
the 75,000 customers that we have.

1091
00:51:48,930 --> 00:51:51,480
We lead customers who
have customer domains,

1092
00:51:51,480 --> 00:51:53,790
and, you know, create
their own custom URLs.

1093
00:51:53,790 --> 00:51:56,130
So, in that space, you know,
how do we actually use ScyllaDB

1094
00:51:56,130 --> 00:51:58,380
to create URL shortening services as well,

1095
00:51:58,380 --> 00:51:59,520
and then attachment services.

1096
00:51:59,520 --> 00:52:02,120
So yeah, essentially, all
the products that we have,

1097
00:52:03,000 --> 00:52:05,730
you know, the fundamentally
they are ticketing solutions,

1098
00:52:05,730 --> 00:52:08,430
and they'll also have ticket
bodies and attachments.

1099
00:52:08,430 --> 00:52:12,360
So, until now we are looking
at, you know, the MySQL alone,

1100
00:52:12,360 --> 00:52:13,463
but then, you know, can
we actually use this

1101
00:52:13,463 --> 00:52:15,603
as attachment service as well.

1102
00:52:16,920 --> 00:52:18,870
So ultimate takeaways.

1103
00:52:18,870 --> 00:52:20,280
So, as you have seen,

1104
00:52:20,280 --> 00:52:22,948
Freshworks always
reinvents the technology,

1105
00:52:22,948 --> 00:52:25,666
you know, every single
year, full stack, right?

1106
00:52:25,666 --> 00:52:29,040
Whether it's a cloud layer,
storage, compute, network, data,

1107
00:52:29,040 --> 00:52:30,510
or it's the platform layer,

1108
00:52:30,510 --> 00:52:32,280
we are talking about multiple
capabilities, like email,

1109
00:52:32,280 --> 00:52:35,060
analytics, and you know, single sign-on,

1110
00:52:35,060 --> 00:52:37,230
or the AI, or the products.

1111
00:52:37,230 --> 00:52:39,750
And Scylla has been our partner

1112
00:52:39,750 --> 00:52:42,570
in actually building some
of the cool technologies,

1113
00:52:42,570 --> 00:52:44,220
and they have been
actually co-build partners

1114
00:52:44,220 --> 00:52:46,140
and co-market partners as well.

1115
00:52:46,140 --> 00:52:50,520
So, we achieved, I think, 10X
of what we thought initially

1116
00:52:50,520 --> 00:52:52,170
that we would otherwise do.

1117
00:52:52,170 --> 00:52:55,410
So, it's always like, you know,
awesome working with them.

1118
00:52:55,410 --> 00:52:58,823
So, with that, I'm calling
Felipe at the stage.

1119
00:52:58,823 --> 00:53:00,990
- Thank you so much Sree.

1120
00:53:00,990 --> 00:53:02,620
You guys basically did

1121
00:53:03,562 --> 00:53:06,577
all the value ScyllaDB
gives to Freshworks.

1122
00:53:06,577 --> 00:53:09,780
One particular question I have to you is,

1123
00:53:09,780 --> 00:53:11,610
since you guys are running Scylla cloud,

1124
00:53:11,610 --> 00:53:14,940
and Prem spoke briefly
about ScyllaDB cloud

1125
00:53:14,940 --> 00:53:16,410
and your BYOA setup,

1126
00:53:16,410 --> 00:53:18,780
what's the main value BYOA gives to you

1127
00:53:18,780 --> 00:53:22,050
as a customer, and for
your customers as well?

1128
00:53:22,050 --> 00:53:23,820
Because in the BYOA setup,

1129
00:53:23,820 --> 00:53:26,250
all the data is issue under
your own management as well.

1130
00:53:26,250 --> 00:53:27,660
Could you speak a bit more about that?

1131
00:53:27,660 --> 00:53:28,650
- Yeah, absolutely.

1132
00:53:28,650 --> 00:53:31,920
I think, bring your own
account matters a lot,

1133
00:53:31,920 --> 00:53:33,870
especially, we are talking about,

1134
00:53:33,870 --> 00:53:36,750
so Freshworks serves our products
in five different regions,

1135
00:53:36,750 --> 00:53:39,810
like us, Europe, India,
Australia, and Middle East.

1136
00:53:39,810 --> 00:53:41,758
And when we speak about
customers also care

1137
00:53:41,758 --> 00:53:44,580
for data residency, so they look at

1138
00:53:44,580 --> 00:53:47,100
where their data resides
and how it gets transferred.

1139
00:53:47,100 --> 00:53:50,370
So, we wanna make sure
all of our sub-processors,

1140
00:53:50,370 --> 00:53:52,260
also live in the same region

1141
00:53:52,260 --> 00:53:55,830
and the data doesn't go out
the storage or in transit.

1142
00:53:55,830 --> 00:53:57,491
So keeping all this in mind,

1143
00:53:57,491 --> 00:54:01,680
when we are talking about
bringing your own account,

1144
00:54:01,680 --> 00:54:04,830
essentially our entire data
plan decide within our account.

1145
00:54:04,830 --> 00:54:07,373
So that actually makes it
very easy in terms of access

1146
00:54:07,373 --> 00:54:09,930
scaling for our own engineers, right?

1147
00:54:09,930 --> 00:54:12,528
And that could be a deal breaker for us.

1148
00:54:12,528 --> 00:54:16,274
And since you enabled
us with that feature,

1149
00:54:16,274 --> 00:54:19,503
it made it so seamless
for us to, you know,

1150
00:54:20,370 --> 00:54:22,413
do the entire migration over to ScyllaDB.

1151
00:54:22,413 --> 00:54:25,350
- Prem also spoke quite
extensively about migrations

1152
00:54:25,350 --> 00:54:27,780
and the different
strategies you guys took.

1153
00:54:27,780 --> 00:54:29,940
And I don't want to choose sugar-coat it,

1154
00:54:29,940 --> 00:54:31,830
I mean database migrations are hard.

1155
00:54:31,830 --> 00:54:36,660
We often classify, we like to
describe migration database

1156
00:54:36,660 --> 00:54:38,760
like an open-heart surgery.

1157
00:54:38,760 --> 00:54:42,120
How did ScyllaDB help it
and simplify the process

1158
00:54:42,120 --> 00:54:45,780
of migrating across multiple
databases to ScyllaDB?

1159
00:54:45,780 --> 00:54:49,290
How did your support team
working with you guys

1160
00:54:49,290 --> 00:54:51,540
and helping you guys throughout
all these steps involved

1161
00:54:51,540 --> 00:54:52,373
to move your data over?

1162
00:54:52,373 --> 00:54:53,430
- Yeah, yeah.

1163
00:54:53,430 --> 00:54:55,370
No, I think the analogy
of operating on a patient

1164
00:54:55,370 --> 00:54:57,090
is actually perfect.

1165
00:54:57,090 --> 00:54:58,030
But we are operating on a patient

1166
00:54:58,030 --> 00:55:00,780
where a patient is still in all the senses

1167
00:55:00,780 --> 00:55:04,036
and actually not, you know,

1168
00:55:04,036 --> 00:55:06,330
like, you know, on an
anesthesia or anything else.

1169
00:55:06,330 --> 00:55:09,936
So, our customers expect us
not to go down at any point.

1170
00:55:09,936 --> 00:55:12,510
They don't care whether
you are doing maintenance

1171
00:55:12,510 --> 00:55:13,530
or upgrades.

1172
00:55:13,530 --> 00:55:16,170
They want the service to be
up and running all the time.

1173
00:55:16,170 --> 00:55:18,750
And that's one of the best
features of ScyllaDB, right?

1174
00:55:18,750 --> 00:55:21,960
So, we could do hot, hot scenario where

1175
00:55:21,960 --> 00:55:24,750
while the upgrades are happening
or migration is happening,

1176
00:55:24,750 --> 00:55:27,900
we could do parallel writes,
like Prem just spoke about it,

1177
00:55:27,900 --> 00:55:29,435
and then reads progressively.

1178
00:55:29,435 --> 00:55:32,370
We kind of shifted based on the accuracy

1179
00:55:32,370 --> 00:55:33,900
as we get more confidence.

1180
00:55:33,900 --> 00:55:37,080
So that way, but if it is RDBMS,

1181
00:55:37,080 --> 00:55:38,870
at least you'll have like in
a few seconds of downtime.

1182
00:55:38,870 --> 00:55:39,703
- Yeah.

1183
00:55:39,703 --> 00:55:42,300
- And that is noticeable,
you know, for our customers.

1184
00:55:42,300 --> 00:55:44,160
And then, every time there is a downtime,

1185
00:55:44,160 --> 00:55:47,610
we gotta notify 75,000
customers two weeks in advance.

1186
00:55:47,610 --> 00:55:49,800
And then if somebody says no,

1187
00:55:49,800 --> 00:55:50,979
then in some cases premium customers

1188
00:55:50,979 --> 00:55:52,860
you might have to even move the window.

1189
00:55:52,860 --> 00:55:55,799
So, in this case, it's like
you're trying to maintain

1190
00:55:55,799 --> 00:55:58,599
or replace engines of aircraft
while it is still flying.

1191
00:55:59,730 --> 00:56:01,590
- Yeah, thank you so much, Sreedhar,

1192
00:56:01,590 --> 00:56:03,000
and thank you so much, Prem.

1193
00:56:03,000 --> 00:56:06,180
I would like to tell you thank you

1194
00:56:06,180 --> 00:56:08,070
for your continued partnership,

1195
00:56:08,070 --> 00:56:12,330
and I hope we will help you out
with this upcoming use case.

1196
00:56:12,330 --> 00:56:13,787
- So, with that folks-
- Thank you so much.

1197
00:56:13,787 --> 00:56:15,780
- I would like to thank
you well for attending

1198
00:56:15,780 --> 00:56:19,530
and listening to both
of our speakers today.

1199
00:56:19,530 --> 00:56:23,760
If you like what we had to
share, please remember to rate

1200
00:56:23,760 --> 00:56:27,450
our talk in the AWS events app.

1201
00:56:27,450 --> 00:56:31,680
And with that, if in case you
have any questions to Brian,

1202
00:56:31,680 --> 00:56:36,540
or to Sree, or Prem, we would
love to catch you up here.

1203
00:56:36,540 --> 00:56:40,380
And you are also welcome
to visit our booth,

1204
00:56:40,380 --> 00:56:43,140
at the Venetian Boot, 1747.

1205
00:56:43,140 --> 00:56:45,040
Thank you folks, and have a great day.

