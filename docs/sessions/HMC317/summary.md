# AWS re:Invent 2025 制造业边缘AI代理应用会议总结

## 会议概述

本次技术会议由AWS高级解决方案架构师Karim Akhnoukh和Mohamed Salah主讲，重点介绍了如何在制造业环境中部署生成式AI应用来解决计划外停机问题。全球前500家制造商每年因计划外停机损失高达1.4万亿美元，相当于西班牙的GDP。演讲者通过一个智能饼干工厂的实际案例，展示了如何使用AWS Outpost在边缘部署AI代理应用，结合小语言模型、RAG技术和实时遥测数据来提供智能运维支持。

会议涵盖了从数据准备、模型微调到边缘部署的完整技术栈，特别强调了在网络连接不稳定的工厂环境中保持"始终在线"能力的重要性。通过部署专门的小语言模型和AI代理，操作员可以通过文本或语音与系统交互，获得实时的故障诊断和维修建议。

## 详细时间线与关键要点

### 00:00-05:00 问题背景与挑战分析
- **计划外停机成本**：全球制造业每年损失1.4万亿美元，占收入的11%
- **四大核心挑战**：
  - 数据孤岛：各机器独立数据库，缺乏端到端视图
  - 技能差距：资深专家知识无法有效传承给初级操作员
  - 生产延误：缺乏专业知识和数据连接导致无法及时响应
  - 运营中断：工厂网络连接不稳定，影响云端决策支持

### 05:00-10:00 解决方案架构设计
- **统一数据平台**：使用AWS Outpost整合所有机器数据到单一数据湖
- **多协议集成**：支持MQTT、LoRaWAN、REST API、SFTP等不同OEM集成方式
- **EKS本地集群**：部署统一API层吸收不同集成类型
- **AI消费层**：使用Bedrock进行AI分析，QuickSight提供仪表板可视化
- **边缘智能**：部署多个小语言模型实现离线AI能力

### 10:00-15:00 饼干工厂用例演示
- **生产线组成**：饼干成型机、冷冻隧道、饼干检测器三大核心设备
- **AI代理功能**：
  - 访问机器手册、标准操作程序、实时遥测数据
  - 支持文本和语音交互
  - 自动故障诊断和修复建议
- **技术架构**：
  - GPT OSS路由模型
  - 微调的Llama 3.2 10亿参数模型用于RAG
  - SmolVLM用于视觉质量检测

### 15:00-25:00 基础设施部署实践
- **EC2实例配置**：使用g4dn.xlarge实例模拟AWS Outpost环境
- **依赖项安装**：
  - 代理机器：Docker运行时、自定义Python 3.10、Nvidia工具包、Ollama
  - 模型机器：Docker、微调模型、Ollama、SLM
- **硬件约束考虑**：g4dn.12xlarge配置4个T4 GPU，每个16GB内存

### 25:00-35:00 模型部署策略
- **量化技术**：
  - 基线模型：FP16精度，40GB内存占用
  - 量化模型：MXFP4精度，13GB内存占用，节省65%内存
  - 精度损失：1-2%准确率下降
- **部署策略**：
  - 策略一：单GPU部署完整模型，支持并行处理多客户端
  - 策略二：模型权重分片到多GPU，增大KV缓存容量

### 35:00-45:00 数据准备与模型微调
- **数据源类型**：CSV文件、文本文件、PDF文档等多种格式
- **数据处理流程**：
  - 读取不同格式文档
  - 调用大模型生成结构化问答对
  - 确定性验证确保数据质量
  - 智能分割避免语义断裂
- **微调目标**：让模型理解所需技能，采用正确语调响应
- **SageMaker AI Studio**：使用Jumpstart进行Llama 3.2模型微调

### 45:00-55:00 AI代理应用开发与测试
- **LLM-as-a-Judge评估**：
  - 使用Claude 4.5 Sonnet/Haiku和Nova Pro作为评判者
  - 评估标准：准确性、完整性、相关性
  - 结果：微调模型比基础模型平均提升14%
- **RAG工具实现**：
  - 使用Chroma DB作为向量存储
  - 结合检索和生成功能
  - 集成微调模型增强响应质量
- **遥测数据工具**：
  - 读取实时传感器数据（温度、速度等）
  - 支持设备查找和时间范围查询
- **实际测试**：演示"debug freezer tunnel"查询，系统自动检测温度异常并提供修复建议