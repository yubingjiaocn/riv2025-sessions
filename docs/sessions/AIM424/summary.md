# AWS re:Invent 2025 - SageMaker AI 推理：高效部署 LLM 和生成式 AI 工作负载

## 会议概述

本次 AWS re:Invent 2025 第二天的技术分享会由 AWS SageMaker 团队的首席生成式 AI/ML 解决方案架构师 Vive Gangasani 和产品经理 Karim 主讲，Salesforce 软件架构师 Richard Chan 作为合作伙伴参与分享。会议重点介绍了如何在 SageMaker AI 上高效、经济地部署和管理大语言模型（LLM）及生成式 AI 工作负载。

演讲团队分享了基于客户反馈和实际测试的最佳实践，涵盖了从模型部署到生产环境扩展的完整流程。会议特别强调了 2025 年的两大趋势：智能体工作流（Agentic Workflows）的兴起和推理时计算（Test Time Compute）的重要性。随着推理模型采用思维链（Chain of Thought）技术，虽然准确性提高，但每次推理生成的 token 数量显著增加，对计算资源的需求也相应提升。

会议详细介绍了 SageMaker AI 推理服务的三大核心支柱：价格性能优化、灵活性和易用性。通过多项新功能发布，包括 Eagle 2/3 推测解码、动态 LoRA 适配器、双向流式传输等技术，SageMaker 帮助客户在保持高性能的同时最大化 GPU 利用率，降低运营成本。

## 详细时间线与关键要点

### 开场介绍（0:00-2:30）
- **0:00** - 会议开始，Vive Gangasani 自我介绍，担任 SageMaker 首席生成式 AI/ML 专家解决方案架构师
- **0:45** - 介绍工作职责：帮助数百家初创企业和大型企业训练、部署和扩展模型
- **1:15** - Karim 介绍，SageMaker 推理产品经理
- **1:45** - Richard Chan 介绍，来自 Salesforce 的软件架构师
- **2:15** - 会议主题：如何高效部署 LLM 和生成式 AI 工作负载

### 会议议程概览（2:30-4:00）
- **2:30** - 议程介绍：2025 年趋势、部署步骤、三大核心支柱
- **3:00** - 将讨论价格性能优化、灵活性和易用性
- **3:30** - 包含智能体工作流构建和 Salesforce Agent Force 案例分享

### 2025 年生成式 AI 趋势（4:00-8:30）
- **4:00** - 自 2022 年底 ChatGPT 发布以来，生成式 AI 的演进
- **4:30** - 趋势一：智能体工作流兴起 - 企业开始认真整合智能体到现有工作流程
- **5:00** - 智能体不仅提供响应，还能采取行动、分解问题并代表用户执行任务
- **5:45** - 趋势二：推理时计算（Test Time Compute） - 模型在生成响应前进行思维链推理
- **6:30** - 推理模型（如 DeepSeek、Qwen）通过思维链提高准确性，但生成更多 token
- **7:15** - 示例：100 token 提示 + 100 token 响应 + 200 token 推理 = 300 token 总计
- **7:45** - Gartner 预测：到 2028 年，超过三分之一的应用将包含智能体工作流

### 从概念验证到生产环境的挑战（8:30-11:00）
- **8:30** - 在小规模环境中部署模型相对简单，但扩展到生产环境面临挑战
- **9:00** - 挑战一：性能 - 随着并发增加，需要优化响应时间和吞吐量
- **9:30** - 挑战二：成本 - GPU 昂贵且稀缺，需要最大化利用率
- **10:00** - 挑战三：可扩展性 - 不能像 CPU 工作负载那样随意扩展
- **10:30** - 挑战四：复杂性 - 框架、环境变量、优化技术、基础设施配置复杂

### SageMaker vs Bedrock 对比（11:00-13:00）
- **11:00** - 客户常问问题：Bedrock 和 SageMaker 推理服务的区别
- **11:30** - 区别一：专有模型 - Claude、Nova、Cohere 等仅在 Bedrock 上可用
- **12:00** - 区别二：灵活性 - SageMaker 支持部署任何模型架构，提供底层访问权限
- **12:30** - 区别三：计费模式 - Bedrock 按 token 计费（无服务器），SageMaker 按 GPU 使用计费

### SageMaker AI 推理三大支柱介绍（13:00-15:30）
- **13:00** - Karim 接手介绍三大核心支柱
- **13:30** - 支柱一：价格性能 - 高吞吐量、低延迟、自动扩展
- **14:00** - 支柱二：灵活性 - 支持任何模型、框架、GPU 实例
- **14:30** - 支柱三：易用性 - 从模型权重到调用的简化流程
- **15:00** - 生产环境流量不恒定，需要快速自动扩展能力

### SageMaker 推理部署流程（15:30-18:00）
- **15:30** - 展示 SageMaker 推理完整功能图
- **16:00** - 模型来源：S3、JumpStart、Hugging Face、ECR 镜像
- **16:30** - 创建端点并部署模型，开始调用
- **17:00** - 提供的功能：实例选择、自动扩展、内置可观测性、自带容器
- **17:30** - 多模型支持、流式响应、预留实例和按需实例混合使用

### 部署架构层次（18:00-19:30）
- **18:00** - 部署架构说明：基础设施层（GPU/CPU）、容器层、模型层
- **18:30** - 支持单模型部署或同一 GPU 上的多模型部署
- **19:00** - 多模型部署可最大化 GPU 利用率，支持按需扩展模型副本

### Eagle 2/3 推测解码技术（19:30-25:00）
- **19:30** - 宣布 Eagle 2 和 Eagle 3 推测解码在 SageMaker 上可用
- **20:00** - 客户期望快速推理，但 LLM 逐个生成 token 速度较慢
- **20:30** - Eagle 通过草稿模型生成多个连续 token，基础模型评估并接受
- **21:00** - 工作原理：草稿模型生成 n 个 token，基础模型并行评估所有 token
- **21:45** - 示例："the cat" 后，草稿模型生成 "jumped over the moon"
- **22:30** - 基础模型根据概率接受或拒绝每个 token
- **23:00** - Eagle 3 增强 - 生成多个变体（jumped/climbed/hopped），基础模型选择最佳
- **24:00** - 测试结果：吞吐量提升 2.5 倍，无准确性损失
- **24:30** - 实现方式：运行优化作业，可使用自定义或开源数据集训练 Eagle heads

### 动态 LoRA 适配器（25:00-27:30）
- **25:00** - 介绍动态 LoRA 适配器功能
- **25:30** - 可在同一实例上部署基础模型和多个 LoRA 适配器
- **26:00** - 工作流程：LoRA 适配器初始存储在 S3，首次调用时加载到 GPU
- **26:45** - GPU 内存不足时，未使用的适配器自动卸载
- **27:15** - 实现"无限" LoRA 适配器，按需从存储快速加载到 GPU

### 推理组件与多模型部署（27:30-29:30）
- **27:30** - 推理组件功能：在同一 GPU 上部署多个模型或模型副本
- **28:00** - 示例：在同一 GPU 上部署 1 个模型 A 副本和 2 个模型 B 副本
- **28:45** - 优势：实现模型扩展和最大化 GPU 利用率
- **29:15** - NVME 卷容器缓存：容器预加载到 NVME，自动扩展速度提升超过 50%

### 负载感知路由与会话路由（29:30-31:30）
- **29:30** - 介绍延迟优化的路由改进
- **30:00** - 负载感知路由 - 无状态请求（如"西雅图天气"）路由到低流量 GPU
- **30:45** - 会话感知路由 - 有状态多轮对话，保持上下文历史
- **31:15** - 会话路由适用于聊天机器人等需要个性化响应的应用

### 灵活性：框架与容器支持（31:30-35:00）
- **31:30** - Vive 介绍灵活性支柱
- **32:00** - SageMaker 提供托管容器，基于开源框架并优化
- **32:30** - 支持自带容器（BYOC）满足安全或定制需求
- **33:00** - 支持自定义推理脚本进行预处理和后处理
- **33:30** - 多模态支持 - 文本、视频、音频生成和理解，语音智能体
- **34:15** - 开源模型性能与闭源模型相当（参考 Artificial Analysis）

### 开源模型优势（35:00-37:00）
- **35:00** - 展示 Artificial Analysis 性能对比
- **35:30** - 开源模型（如 GPT-4o）在工具调用和函数调用方面表现优秀
- **36:00** - 成本效益高：GPT-4o 可在单个 G6E 2xlarge 实例上运行（约 $2/小时）
- **36:30** - 可处理 40-50 个并发请求，大规模时 GPU 推理性价比更高

### 双向流式传输（37:00-41:00）
- **37:00** - 宣布推出双向流式传输功能
- **37:30** - 应用场景：实时音频转录、音频翻译、客户服务智能体
- **38:00** - 技术实现：使用 HTTP/2 协议连接客户端到 SageMaker 路由器
- **38:45** - 路由器通过 WebSocket 连接转发请求到模型
- **39:30** - WebSocket 维持持久连接，实现双向流式响应
- **40:00** - 优势：无需等待完整请求，实时转录和响应
- **40:30** - 与 Deepgram 合作，其模型原生集成到 SageMaker

### 双向流式传输演示（41:00-43:30）
- **41:00** - 播放虚拟药房助手演示视频
- **41:30** - 演示客户查询订单状态（会员 ID: M10001）
- **42:00** - 助手实时响应：订单 ORD01 处理中，包含 Amoxicillin 500mg 21 粒
- **42:45** - 查询剩余续配次数：0 次剩余
- **43:00** - 取货时间：12 月 6 日上午 10:00
- **43:15** - 演示展示流式对话的实时性和响应速度

### 易用性：容量预留（43:30-46:00）
- **43:30** - Vive 介绍易用性支柱
- **44:00** - 客户常问：如何确定生产环境所需实例数量？
- **44:30** - 需要简便方式测试 GPU、框架和配置
- **45:00** - 新功能：自助容量预留 - 通过 SageMaker 控制台直接预留容量
- **45:30** - 选择实例类型、时间范围和天数，无需手动流程
- **45:45** - 批准后立即可用，无需提交限额请求

### 可观测性与监控（46:00-48:00）
- **46:00** - 内置可观测性堆栈追踪 GPU 指标
- **46:30** - 每个 GPU 的利用率、CPU 利用率等详细指标
- **47:00** - 路由器导出调用次数、错误率、延迟等指标
- **47:30** - 所有指标自动发送到 CloudWatch，无需自建监控系统

### 容器与框架选择（48:00-51:00）
- **48:00** - 客户常问：最佳容器或框架是什么？
- **48:30** - 答案：取决于具体情况
- **49:00** - vLLM - 最流行选择，支持最新模型（day-zero 支持）
- **49:30** - SGLang - 适合专家混合模型和超长上下文
- **50:00** - SageMaker 提供基于这些框架的 LMI 托管容器
- **50:30** - GitHub 仓库提供 Dockerfile 示例，展示如何适配开源容器

### LMI 容器功能（51:00-53:00）
- **51:00** - 新推出的 LMI 容器基于 vLLM 构建
- **51:30** - 支持 LoRA 托管：部署基础模型 + 多个 LoRA 适配器
- **52:00** - 支持推测解码优化
- **52:30** - 兼容 OpenAI Chat Completions API 标准
- **52:45** - 托管容器经过安全扫描、漏洞检测和生产级测试

### 智能体工作流集成（53:00-55:00）
- **53:00** - 模型部署后如何使用？
- **53:30** - 虽然重点是智能体 AI，但 SageMaker 也支持个性化、计算机视觉等
- **54:00** - 主流框架（LangChain、LlamaIndex 等）都有 SageMaker 连接器
- **54:30** - 部署到 SageMaker 端点后，可直接通过框架连接器调用
- **55:00** - 示例：部署 Qwen 模型，通过框架构建智能体工作流

### 会议总结
本次会议全面展示了 SageMaker AI 推理服务在 2025 年的重大进展，特别是针对智能体工作流和推理模型的优化。通过 Eagle 推测解码、动态 LoRA、双向流式传输等创新技术，SageMaker 帮助客户在保持高性能的同时显著降低成本。自助容量预留、增强的可观测性和托管容器等易用性改进，进一步降低了从实验到生产的门槛。