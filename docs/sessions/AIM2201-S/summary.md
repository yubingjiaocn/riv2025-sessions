# AWS re:Invent 2025 会议总结：Modal AI 基础设施平台

## 会议概览

本次会议由 Modal 公司 CEO Erik Bernhardsson 主讲，介绍了 Modal 这一专为 AI 应用构建的无服务器基础设施平台。Modal 成立于 2021 年，总部位于纽约，已运营约四年时间。该平台旨在解决传统基础设施（如 Kubernetes、EC2、Docker）在处理 AI 工作负载时面临的挑战，特别是在 GPU 管理、模型部署和弹性扩展方面的痛点。

Modal 的核心价值在于让开发者能够用几行 Python 代码就将本地代码部署到云端 GPU 上运行，无需处理复杂的基础设施配置。平台采用按使用量计费模式，仅在代码实际运行时收费，能够在秒级时间内扩展到数千个 GPU。Modal 服务于多种客户，包括 Meta、Suno、Lovable、Cognition 等知名公司，应用场景涵盖推理、训练、代码沙箱、音乐生成、生物技术、天气预报等多个领域。

该平台的技术优势源于其自研的底层技术栈，包括容器运行时、文件系统、镜像构建器、编排器和调度器。通过 CPU 和 GPU 内存快照技术，Modal 能够在不到一秒的时间内启动大型模型并开始推理。作为 AWS 的大客户，Modal 在全球多个 AWS 区域管理着数万个 GPU，通过多租户架构实现了比传统预留模式更高的 GPU 利用率和成本效益。

## 详细时间线

0:00 - 0:45 - 开场介绍
- 演讲者自我介绍：Modal 公司 CEO
- Modal 是一家 AI 基础设施公司，提供无服务器推理等服务
- 客户包括 Suno、Lovable 等公司

0:45 - 1:30 - 公司背景
- Modal 总部位于纽约
- 成立于 2021 年，已运营约四年
- 定位为帮助开发者构建和扩展 AI 应用的基础设施平台

1:30 - 2:30 - 核心问题阐述
- 传统基础设施（Kubernetes、EC2、Docker）不适合 AI 应用
- GPU 成本高昂且容量有限
- 需要快速扩缩容以应对不可预测的推理需求
- 传统基础设施降低开发者生产力

2:30 - 3:30 - 技术挑战分析
- 传统基础设施为 CRUD 应用和稳定 CPU 需求设计
- AI 应用需要昂贵的 GPU 和大规模模型
- 需要新的原语如代码沙箱来执行不可信代码（如 LLM 生成的代码）
- 涉及训练、存储、批处理作业等复杂场景

3:30 - 4:30 - Modal 技术栈
- 自研容器运行时
- 自研文件系统
- 自研存储原语
- 目标是提供卓越的开发者体验
- 代码可在不到一秒内从本地部署到云端 GPU（包括 B200）

4:30 - 5:30 - 应用场景
- 推理、训练、Notebook
- 批处理作业、代码执行
- 端到端 AI 平台，支持多种用例
- 专注于机器学习开发者

5:30 - 6:30 - 客户案例
- Meta、Lovable（代码执行）
- Cognition 和 Decagon（LLM）
- Suno AI（音乐生成推理）
- 生物技术领域：化合物扫描、蛋白质折叠、序列比对
- 天气预报、癌症治疗研究
- 音频转录、文本转语音、语音转文本

6:30 - 7:30 - 平台特性
- 不是 AI API，没有固定模型集
- 支持运行任何模型（专有或开源）
- 通过 Python SDK 将函数转换为无服务器函数
- 管理所有扩展需求

7:30 - 8:30 - 客户反馈：机器人应用案例
- 引用客户评价：用于控制机器人手臂
- 强调低延迟需求
- 展示 Modal 在实时应用中的能力

8:30 - 9:30 - 架构说明
- 两部分组成：Python SDK + 基础设施层
- SDK 定义要在云端运行的函数
- 基础设施层管理容量和扩展
- 可在秒级时间内提供数千个 GPU

9:30 - 11:00 - Python SDK 代码示例
- 导入 Modal 库
- 使用装饰器将普通函数转换为无服务器函数
- 示例：在 L40S GPU 上运行
- 使用 Hugging Face Transformers 加载模型
- 零基础设施配置需求
- 运行 modal run 或 modal deploy 即可部署

11:00 - 12:00 - 客户案例：Suno
- 小团队专注于模型和消费者应用
- Modal 处理所有底层基础设施
- 几行 Python 代码即可完成
- 零 YAML、零配置文件
- 无需编写 Dockerfile

12:00 - 13:30 - 底层技术实现
- 自研容器运行时、文件系统、镜像构建器
- 自研编排器和调度器
- CPU 和 GPU 内存快照技术
- 大型模型（数十 GB）可在不到一秒内启动并开始推理
- 深度技术投资带来的优势

13:30 - 15:00 - 规模和成本优势
- 管理全球数万个 GPU
- 大量部署在 AWS 上
- 按使用量计费，仅在代码运行时收费
- 适合不可预测的推理需求
- 无流量时自动缩减到零，零成本
- 支持 CPU 和 GPU 函数部署

15:00 - 16:00 - AWS 区域覆盖
- 展示 A10G 在多个 AWS 区域的部署示例
- 处理所有底层复杂性
- 自动管理 GPU 容量获取和扩缩容

16:00 - 17:00 - 可观测性仪表板
- 内部高级可观测性工具
- 可查看延迟、GPU 温度、容器数量
- 机器学习工程师喜欢的全栈控制
- 支持堆栈跟踪和日志查看
- 实时了解 GPU 使用情况
- 实时查看成本

17:00 - 18:30 - 效率优势
- 聚合所有需求以提高效率
- 传统方式需要为峰值预留资源
- Modal 通过多租户提高 GPU 利用率
- 虽然单位 GPU 小时成本可能更高，但净成本可能更低
- 仅在 GPU 实际运行时付费

18:30 - 19:30 - 开发者体验
- 强调开发者喜爱度
- 几行代码即可投入生产
- 缩短上市时间
- 支持复杂模型、训练、微调
- 大规模批处理作业和代码沙箱
- 只需 pip install modal 即可开始

19:30 - 20:30 - 客户案例：Substack
- 扩展到数百个 GPU 用于大批处理作业
- 其他客户扩展到数千个 GPU
- 案例：几天内处理 3000 年的音频数据

20:30 - 21:30 - 产品理念
- 四年持续构建
- 专注于开发者喜爱的产品
- 演讲者有 32 年编程经验，25 年专业经验
- 目标是提高工程师生产力
- 消除基础设施的枯燥部分

21:30 - 22:30 - 目标用户
- 主要面向机器学习工程师
- 构建和微调模型的开发者
- 不剥夺控制权，而是赋能
- 使开发者能够更快行动、部署更复杂的应用
- 获得更多 GPU 和云端可扩展性

22:30 - 23:30 - 平台灵活性
- 再次强调：不是 AI API
- 没有固定模型集
- 支持任何代码或模型（专有或开源）
- 提供大量模板和示例
- 按消费计费的优势

23:30 - 24:30 - 如何开始使用
- 在终端运行 pip install modal
- 立即在云端编写和运行代码
- 每个用户每月获得 $30 免费额度
- 初创公司可获得高达 $50,000 的额度
- 许多 Y Combinator 公司使用 Modal

24:30 - 25:30 - 从原型到生产
- 支持原型开发阶段
- 帮助快速迭代
- 扩展到数千个 GPU 的生产工作负载
- 端到端支持

25:30 - 26:00 - 结束语和联系方式
- Twitter: @Burnhardson
- 博客（虽然更新不频繁）
- 邮箱: erik@modal.com
- 欢迎提问和保持联系