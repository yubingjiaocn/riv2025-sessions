1
00:00:00,779 --> 00:00:02,439
Hey, it's great to see you,

2
00:00:02,769 --> 00:00:03,289
um.

3
00:00:04,698 --> 00:00:07,059
CEO of a company called Model, we build

4
00:00:07,059 --> 00:00:08,118
AI infrastructure.

5
00:00:08,618 --> 00:00:11,009
Serverless inference is one of the many things we offer.

6
00:00:11,339 --> 00:00:13,419
Um, we, uh, power some really

7
00:00:13,419 --> 00:00:15,659
cool customers like Suno, Lovable,

8
00:00:16,019 --> 00:00:17,089
and many others,

9
00:00:17,420 --> 00:00:19,739
uh, doing things like inference, training,

10
00:00:19,899 --> 00:00:21,280
code sandboxes,

11
00:00:21,699 --> 00:00:22,818
and a lot of other stuff.

12
00:00:23,179 --> 00:00:25,280
So let's, uh, let's walk through what Model does

13
00:00:25,280 --> 00:00:26,239
and, um,

14
00:00:26,500 --> 00:00:27,199
how it works.

15
00:00:28,219 --> 00:00:29,379
Uh, we, uh,

16
00:00:29,769 --> 00:00:30,958
we're based in New York.

17
00:00:31,620 --> 00:00:33,779
Been doing this for about 4 years. I

18
00:00:33,779 --> 00:00:35,259
started the company about 2021.

19
00:00:36,728 --> 00:00:38,810
Modal is an infrastructure platform

20
00:00:38,810 --> 00:00:40,969
that makes it easy for developers to build and scale

21
00:00:40,969 --> 00:00:42,429
AI applications.

22
00:00:42,810 --> 00:00:45,270
So, when you're building these AI

23
00:00:45,270 --> 00:00:46,750
applications, you need infrastructure.

24
00:00:47,090 --> 00:00:49,679
So you need a place to run all these

25
00:00:49,679 --> 00:00:51,840
applications, right? And as it turns out, when you're

26
00:00:51,840 --> 00:00:54,009
building these things, working with large scale models,

27
00:00:54,090 --> 00:00:56,408
working with GPUs, uh, a lot of

28
00:00:56,408 --> 00:00:58,450
existing infrastructure doesn't really cut

29
00:00:58,450 --> 00:01:00,740
it. So you're trying to deploy things

30
00:01:00,740 --> 00:01:02,889
using Kubernetes, EC2,

31
00:01:03,219 --> 00:01:04,058
Docker,

32
00:01:04,409 --> 00:01:06,849
it's kind of a nightmare, uh, and it really drains

33
00:01:06,849 --> 00:01:09,058
developer productivity, and then you have a lot of these

34
00:01:09,058 --> 00:01:10,159
challenges around

35
00:01:10,459 --> 00:01:12,159
not, you know, managing.

36
00:01:13,290 --> 00:01:15,319
Predictable demand and particularly with inference, you need

37
00:01:15,319 --> 00:01:17,400
to be able to scale up and down very, very fast and

38
00:01:17,400 --> 00:01:19,900
manage the utilizations and GPUs are expensive

39
00:01:20,239 --> 00:01:22,000
and GPU capacity is limited.

40
00:01:22,480 --> 00:01:23,739
So you have all these issues around

41
00:01:24,120 --> 00:01:26,480
infrastructure management and that's what model solves.

42
00:01:26,719 --> 00:01:28,719
So we run tens of thousands of GPUs all

43
00:01:28,719 --> 00:01:30,799
over the world. We manage that for you.

44
00:01:30,879 --> 00:01:32,219
We're a big customer of AWS.

45
00:01:33,120 --> 00:01:34,159
Very happy with that.

46
00:01:34,980 --> 00:01:35,620
Um,

47
00:01:36,250 --> 00:01:38,290
And, and we, we basically started a

48
00:01:38,290 --> 00:01:39,388
company with this sort of

49
00:01:39,769 --> 00:01:41,930
thesis that today's infrastructure is

50
00:01:41,930 --> 00:01:44,010
not cut for, for

51
00:01:44,010 --> 00:01:45,168
AI applications.

52
00:01:45,980 --> 00:01:48,260
And in particular, when you go deep on it, it's, you

53
00:01:48,260 --> 00:01:50,638
look at Kubernetes in, in the past, and

54
00:01:51,138 --> 00:01:53,209
sorry to pick on Kubernetes, but it's, it's a lot of traditional

55
00:01:53,209 --> 00:01:55,250
infrastructure as a whole. It's built for a kind of a

56
00:01:55,250 --> 00:01:56,879
different era of

57
00:01:57,379 --> 00:01:59,459
uh compute. It's sort of a crud,

58
00:01:59,579 --> 00:02:01,819
you know, like kind of a steady

59
00:02:01,819 --> 00:02:04,209
demand, CPU based applications.

60
00:02:04,418 --> 00:02:06,418
It's pretty cheap. But then you go to

61
00:02:06,579 --> 00:02:08,659
AI land and now you're running very expensive

62
00:02:08,659 --> 00:02:09,819
GPUs and

63
00:02:10,219 --> 00:02:12,699
it, it, it poses a lot of challenges

64
00:02:12,699 --> 00:02:15,000
for how we deploy and scale applications.

65
00:02:15,659 --> 00:02:16,219
And so

66
00:02:16,919 --> 00:02:19,000
Some of those applications is just a high cost,

67
00:02:19,159 --> 00:02:21,360
some of them is scaling and working with these large

68
00:02:21,360 --> 00:02:21,868
models.

69
00:02:22,199 --> 00:02:24,360
Some of them are kind of new things, like

70
00:02:24,360 --> 00:02:26,258
how do you programmatically execute

71
00:02:26,879 --> 00:02:28,979
uh a code that you don't trust necessarily.

72
00:02:29,038 --> 00:02:30,520
It may be code that comes from an LLM,

73
00:02:30,879 --> 00:02:32,960
so you need a sandbox primitives. And then of course,

74
00:02:33,020 --> 00:02:33,990
there's training,

75
00:02:34,278 --> 00:02:37,000
how do you do all of the storage of all of this underneath,

76
00:02:37,240 --> 00:02:38,669
uh, big batch jobs,

77
00:02:38,960 --> 00:02:39,699
all these things.

78
00:02:41,368 --> 00:02:43,729
Uh, so, so we basically built our own stack.

79
00:02:44,050 --> 00:02:45,469
So we started at the ground level,

80
00:02:45,808 --> 00:02:47,409
we built our own container runtime,

81
00:02:47,770 --> 00:02:49,389
we built our own file system,

82
00:02:49,808 --> 00:02:51,879
uh, and then we built a bunch of storage primitives.

83
00:02:52,210 --> 00:02:54,449
And the reason we did that was that enables

84
00:02:54,449 --> 00:02:56,490
us to have this amazing developer experience

85
00:02:56,490 --> 00:02:58,689
that lets us iterate extremely fast and

86
00:02:58,689 --> 00:03:01,008
build this really, really offer like an amazing

87
00:03:01,008 --> 00:03:03,050
developer experience. Uh, you're trying to

88
00:03:03,050 --> 00:03:05,409
find something that sparks joy.

89
00:03:05,490 --> 00:03:06,649
When you're working with model,

90
00:03:06,969 --> 00:03:08,508
and I'll show you some code in a second.

91
00:03:09,139 --> 00:03:11,179
Uh, you can get, you can take code from your

92
00:03:11,179 --> 00:03:13,179
laptop and run it in the cloud in less than

93
00:03:13,179 --> 00:03:15,379
a second, uh, on GPUs, state

94
00:03:15,379 --> 00:03:16,758
of the art B-200s.

95
00:03:18,389 --> 00:03:20,569
Um, and what's cool about model

96
00:03:20,569 --> 00:03:21,139
is

97
00:03:21,439 --> 00:03:23,479
we power so many different use cases and

98
00:03:23,479 --> 00:03:24,500
so many different,

99
00:03:24,919 --> 00:03:27,199
uh, parts of the sort of AI journey,

100
00:03:27,278 --> 00:03:28,379
right? Like we do,

101
00:03:28,838 --> 00:03:31,028
we focus on machine learning developers, uh,

102
00:03:31,080 --> 00:03:33,118
primarily, but we do inference

103
00:03:33,118 --> 00:03:33,860
training,

104
00:03:34,118 --> 00:03:36,629
we even have notebooks. So you can do bass jobs,

105
00:03:36,919 --> 00:03:38,020
you can run code execution,

106
00:03:38,278 --> 00:03:39,538
we're building many more things.

107
00:03:39,919 --> 00:03:42,599
So Modal, sort of an end to end platform

108
00:03:42,599 --> 00:03:44,479
for all, a lot of different things AI.

109
00:03:46,189 --> 00:03:47,629
We have a lot of different customers,

110
00:03:48,028 --> 00:03:49,169
some may be recognizable,

111
00:03:49,830 --> 00:03:50,770
maybe you heard of Mena,

112
00:03:51,270 --> 00:03:53,308
uh, but we also have some really cool sort of

113
00:03:53,308 --> 00:03:55,629
new AI companies. I mentioned Lovable,

114
00:03:55,710 --> 00:03:56,330
they're using us

115
00:03:56,830 --> 00:03:59,528
for, uh, all the uh code execution.

116
00:03:59,949 --> 00:04:00,528
Um,

117
00:04:01,069 --> 00:04:01,729
we have some

118
00:04:01,990 --> 00:04:02,889
companies like

119
00:04:03,149 --> 00:04:05,618
Cognition and Decagon using us for LMs,

120
00:04:06,258 --> 00:04:08,270
Suno, AI generated music, they run all

121
00:04:08,270 --> 00:04:09,008
their inference on Modal.

122
00:04:10,149 --> 00:04:12,088
Uh, so, so we're on some pretty large scale.

123
00:04:14,330 --> 00:04:16,879
And it's all over the place. We're also actually doing a lot of biotech

124
00:04:16,879 --> 00:04:18,928
too. So we have a lot of customers using us

125
00:04:18,928 --> 00:04:21,410
for scanning millions of

126
00:04:21,410 --> 00:04:23,730
compounds and doing, you know, advanced protein

127
00:04:23,730 --> 00:04:26,009
folding on, on GPU or sequence

128
00:04:26,009 --> 00:04:28,209
alignment and things like that. We have people

129
00:04:28,209 --> 00:04:29,298
using us for

130
00:04:29,689 --> 00:04:31,928
weather forecasting. We have people using us for curing

131
00:04:31,928 --> 00:04:34,189
cancer. Uh, we have people using us for

132
00:04:34,369 --> 00:04:36,608
audio transcriptions, text to speech, speech

133
00:04:36,608 --> 00:04:38,649
to texts, and a lot of applications and

134
00:04:38,649 --> 00:04:40,850
audio. Uh, but, but really

135
00:04:40,850 --> 00:04:42,970
sort of any modality. LM of course, is

136
00:04:42,970 --> 00:04:45,028
a cor cornerstone to

137
00:04:45,410 --> 00:04:46,160
modern AI.

138
00:04:46,819 --> 00:04:49,428
And how does it work? So we're not an AI API.

139
00:04:49,970 --> 00:04:50,910
We don't have like

140
00:04:51,449 --> 00:04:53,608
necessarily a set of models. You can run any model

141
00:04:53,608 --> 00:04:54,160
on model.

142
00:04:54,488 --> 00:04:55,269
Some people run

143
00:04:55,730 --> 00:04:57,160
their own proprietary models,

144
00:04:57,439 --> 00:04:59,178
some people deploy an open source model.

145
00:04:59,488 --> 00:05:01,528
So it's not like you're limited to a set of models,

146
00:05:01,569 --> 00:05:02,949
you can really run anything here.

147
00:05:03,670 --> 00:05:05,678
Uh, we take any code. We take

148
00:05:05,678 --> 00:05:07,759
your code, we take a container, and we make

149
00:05:07,759 --> 00:05:09,899
it run a modal, and we manage all the scaling.

150
00:05:11,399 --> 00:05:12,178
And so,

151
00:05:12,720 --> 00:05:14,899
the general way people do this is to write

152
00:05:14,899 --> 00:05:17,540
a little bit of Python that basically takes functions

153
00:05:17,540 --> 00:05:19,040
and turn them into serverless functions.

154
00:05:19,309 --> 00:05:20,519
So you can think of modal.

155
00:05:21,369 --> 00:05:23,488
Yeah, here's a quote from, and

156
00:05:23,488 --> 00:05:25,209
I actually I think this is a really cool example.

157
00:05:26,019 --> 00:05:28,059
It's a robotics example, so they obviously have

158
00:05:28,059 --> 00:05:30,238
like very low latency demand, like we're basically

159
00:05:30,238 --> 00:05:32,040
using model to control a robotic arm.

160
00:05:32,899 --> 00:05:34,000
Incredibly cool application.

161
00:05:36,119 --> 00:05:38,278
Um, and, and so one way to

162
00:05:38,278 --> 00:05:39,298
think about modal

163
00:05:39,629 --> 00:05:40,838
is that it's, um,

164
00:05:41,619 --> 00:05:43,819
It's, it's two parts. One is a Python

165
00:05:43,819 --> 00:05:45,838
SDK where you can

166
00:05:46,139 --> 00:05:48,959
write a little bit of code that defines

167
00:05:49,619 --> 00:05:51,720
the functions you want to run and scale in the cloud.

168
00:05:52,119 --> 00:05:53,379
And underneath

169
00:05:53,720 --> 00:05:55,959
where the infrastructure substrate that basically

170
00:05:55,959 --> 00:05:58,338
scales that, manages all the capacity,

171
00:05:58,769 --> 00:06:01,238
so you don't have to think about scaling. If you need 1000

172
00:06:01,238 --> 00:06:03,358
GPUs, we can give you 1000 GPUs within

173
00:06:03,358 --> 00:06:05,059
sometimes seconds. So we have these

174
00:06:05,319 --> 00:06:07,399
crazy large scale, very bursty jobs,

175
00:06:07,838 --> 00:06:09,838
sometimes, you know, big bath shops or infer, very

176
00:06:09,838 --> 00:06:11,000
unpredictable inference.

177
00:06:11,278 --> 00:06:13,338
We can scale up and down extremely well.

178
00:06:14,459 --> 00:06:16,579
And, and how does, so let's talk about the first step. Let's

179
00:06:16,579 --> 00:06:17,528
talk about the,

180
00:06:17,899 --> 00:06:18,959
the Python SDK.

181
00:06:19,500 --> 00:06:20,838
Here's a little bit of an example.

182
00:06:21,139 --> 00:06:21,959
And hopefully,

183
00:06:22,338 --> 00:06:24,660
you're familiar with, you know, Python and how it works.

184
00:06:24,939 --> 00:06:26,939
Uh, here's an example of like how you can deploy

185
00:06:26,939 --> 00:06:27,959
a function in modal.

186
00:06:28,220 --> 00:06:29,798
You start with like a little bit of Python,

187
00:06:30,139 --> 00:06:31,059
you add a decorator,

188
00:06:31,329 --> 00:06:33,358
oh sorry, you import modal, of course, first.

189
00:06:33,778 --> 00:06:35,959
The key thing is really here, you put this decorator

190
00:06:35,959 --> 00:06:37,009
on any function,

191
00:06:37,298 --> 00:06:39,199
that turns this function into serverless

192
00:06:39,899 --> 00:06:41,980
uh function that runs in modal.

193
00:06:43,000 --> 00:06:45,160
Uh, we run a lot of stuff on AWS, but also

194
00:06:45,160 --> 00:06:45,959
other clouds,

195
00:06:46,319 --> 00:06:48,509
many neo clouds and, and the big hyperscales.

196
00:06:48,750 --> 00:06:50,500
And in particular here, as you can see,

197
00:06:51,160 --> 00:06:53,259
uh, we run this on an L40S,

198
00:06:53,480 --> 00:06:55,920
which is one of Nvidia's latest GPUs.

199
00:06:56,470 --> 00:06:58,858
Very good for imprints, especially for smaller models.

200
00:06:59,540 --> 00:07:01,559
Uh, and then, you know, we load the model,

201
00:07:01,899 --> 00:07:04,600
just, you know, using a hugging face, transformers.

202
00:07:05,259 --> 00:07:06,449
Um, and that's it.

203
00:07:07,019 --> 00:07:09,108
Zero infrastructure needed. When you get started with

204
00:07:09,108 --> 00:07:10,858
model, you just write a little bit of code,

205
00:07:11,269 --> 00:07:13,369
you run modal run or modal deploy,

206
00:07:13,509 --> 00:07:15,470
that turns it into an app, it runs in the cloud,

207
00:07:15,750 --> 00:07:16,588
and that's it.

208
00:07:17,488 --> 00:07:18,189
Underneath,

209
00:07:18,528 --> 00:07:20,750
well, here's the one kind of beauty of this.

210
00:07:21,209 --> 00:07:23,278
Put in the words of our, one of our favorite

211
00:07:23,278 --> 00:07:24,290
customers, Suno.

212
00:07:24,730 --> 00:07:26,079
So they run, as I said,

213
00:07:26,528 --> 00:07:28,608
music, AI

214
00:07:28,608 --> 00:07:30,009
generated music in the cloud.

215
00:07:30,858 --> 00:07:32,600
Uh, and, and what they love is like,

216
00:07:33,139 --> 00:07:35,369
they're a small team, they can focus on what they're really, really

217
00:07:35,369 --> 00:07:37,579
good at, which is to build models

218
00:07:37,579 --> 00:07:39,178
and build consumer applications,

219
00:07:39,540 --> 00:07:41,639
and we take care of all the infrastructure underneath

220
00:07:42,338 --> 00:07:44,178
with a few lines of Python, they get all this done.

221
00:07:45,028 --> 00:07:46,048
And it's all in code.

222
00:07:46,709 --> 00:07:47,220
You know,

223
00:07:47,509 --> 00:07:49,670
0 Yamel, 0 configuration.

224
00:07:51,000 --> 00:07:53,220
No one likes to write YAL or docket files, actually.

225
00:07:53,910 --> 00:07:55,959
Uh, so, so you write like a few lines of Python and

226
00:07:55,959 --> 00:07:58,019
it just, you know, turn, we can also take an

227
00:07:58,019 --> 00:08:00,238
existing container and just move it into Modal,

228
00:08:00,309 --> 00:08:02,480
but the, the main way people use Modal

229
00:08:02,639 --> 00:08:04,259
is just through the Python SDK.

230
00:08:06,649 --> 00:08:08,730
Uh, and underneath, how does it work? So

231
00:08:08,730 --> 00:08:10,730
we built our own container on time, we built our own

232
00:08:10,730 --> 00:08:11,428
file system,

233
00:08:12,178 --> 00:08:14,608
we built our own container image builder, we built our orchestrator,

234
00:08:14,720 --> 00:08:15,670
built our own scheduler,

235
00:08:16,059 --> 00:08:18,170
all of this stuff, right? Like we make it, you know, the

236
00:08:18,170 --> 00:08:19,949
cloud development just work,

237
00:08:20,238 --> 00:08:22,410
uh, which is a very hard problem, you

238
00:08:22,410 --> 00:08:24,649
know, and hats off to AWS for doing something

239
00:08:24,649 --> 00:08:26,790
similar, like they built the EC2 and all these things.

240
00:08:27,009 --> 00:08:28,069
And we use some of those,

241
00:08:28,449 --> 00:08:30,449
uh, but these are very hard engineering problems, and we built

242
00:08:30,449 --> 00:08:32,558
this underlying infrastructure stack we felt because

243
00:08:32,558 --> 00:08:33,119
we felt like

244
00:08:33,450 --> 00:08:35,609
the infrastructure for AI just wasn't there.

245
00:08:36,317 --> 00:08:38,518
So some of the cool stuff we can do is we can, we

246
00:08:38,518 --> 00:08:40,957
can snapshot CPU memory, we can also snapshot

247
00:08:40,957 --> 00:08:42,957
GPU memory, we can restore that. So

248
00:08:42,957 --> 00:08:44,957
we have incredibly fast container cold

249
00:08:44,957 --> 00:08:47,038
start. We can start GPU models

250
00:08:47,038 --> 00:08:49,239
and start running inference calls in less

251
00:08:49,239 --> 00:08:52,058
than 1 2nd, even with large models like 10s of gigabytes.

252
00:08:52,479 --> 00:08:54,678
Uh, and part of that is like all the deep investments

253
00:08:54,678 --> 00:08:56,038
we made in the infrastructure below.

254
00:08:56,950 --> 00:08:59,210
Uh, and we also manage all the scale.

255
00:08:59,590 --> 00:09:00,529
So like I said,

256
00:09:01,070 --> 00:09:03,149
we manage tens of thousands of GPUs

257
00:09:03,149 --> 00:09:04,389
all over the world, some of,

258
00:09:04,710 --> 00:09:05,750
a lot of them in AWS.

259
00:09:06,779 --> 00:09:08,279
It's all usage-based.

260
00:09:08,700 --> 00:09:10,979
So you only pay for the time the

261
00:09:10,979 --> 00:09:11,918
code is actually running,

262
00:09:12,340 --> 00:09:14,739
which is very nice with inference because you have very unpredictable

263
00:09:14,739 --> 00:09:17,048
demand. And especially with inference, when you're scaling

264
00:09:17,048 --> 00:09:17,960
up and down,

265
00:09:18,340 --> 00:09:20,340
you don't want to go out and make a reservation of like

266
00:09:20,340 --> 00:09:22,109
kind of a rectangle, like you want to

267
00:09:22,538 --> 00:09:24,080
just pay for what you're using.

268
00:09:24,500 --> 00:09:27,178
Maybe your app goes viral and you need 1000 GPUs

269
00:09:27,178 --> 00:09:29,279
tomorrow, maybe while you're sleeping.

270
00:09:29,570 --> 00:09:31,759
Uh, we handle that. We scale it up and

271
00:09:31,759 --> 00:09:33,619
manage that scale for you.

272
00:09:34,379 --> 00:09:36,779
And when there's no traffic, we scale down to 0

273
00:09:36,779 --> 00:09:37,599
and it costs you nothing.

274
00:09:38,729 --> 00:09:40,879
And you can also deploy CPUs, uh, CPU

275
00:09:40,879 --> 00:09:42,940
functions. And so

276
00:09:42,940 --> 00:09:44,279
this is a lot of the work underneath.

277
00:09:44,580 --> 00:09:46,859
As you can see, we're on a lot of different AWS regions.

278
00:09:46,928 --> 00:09:49,099
This is one example of A10Gs how we managed

279
00:09:49,099 --> 00:09:50,609
that. This is a lot of under the hood.

280
00:09:51,019 --> 00:09:52,029
But the point is like,

281
00:09:52,460 --> 00:09:54,619
we take care of all of that complexity for you.

282
00:09:54,700 --> 00:09:55,820
You don't have to think about it.

283
00:09:56,099 --> 00:09:56,840
Under the hood,

284
00:09:57,500 --> 00:09:59,840
all this complexity about like getting GPU capacity

285
00:09:59,840 --> 00:10:01,960
and managing that and scaling up and down.

286
00:10:02,538 --> 00:10:03,820
Don't think about it, we got it.

287
00:10:04,918 --> 00:10:05,519
Um,

288
00:10:06,469 --> 00:10:09,029
And we also have a very fancy internal

289
00:10:09,029 --> 00:10:11,070
observability dashboard. You

290
00:10:11,070 --> 00:10:11,859
can see,

291
00:10:12,190 --> 00:10:14,229
this is one screenshot, but you can, you know,

292
00:10:14,509 --> 00:10:16,509
see latencies and you can

293
00:10:16,509 --> 00:10:18,969
see GPU temperature and number of containers.

294
00:10:19,940 --> 00:10:22,229
Uh, and this is something a lot of our developers really like, the

295
00:10:22,229 --> 00:10:24,229
machine learning engineers. They love seeing and having

296
00:10:24,229 --> 00:10:24,849
full control,

297
00:10:25,509 --> 00:10:27,548
taking stack traces, looking at the logs,

298
00:10:27,619 --> 00:10:29,869
and really understanding what are my GPUs

299
00:10:29,869 --> 00:10:31,889
doing at any point in time. How do I maximize.

300
00:10:32,500 --> 00:10:33,808
Um, and so,

301
00:10:34,940 --> 00:10:36,298
Uh, so when you deploy the model,

302
00:10:36,580 --> 00:10:38,619
you can just log into your dashboard and see all these

303
00:10:38,619 --> 00:10:40,629
things for all your apps that are running and see

304
00:10:40,629 --> 00:10:42,899
exactly how much you're paying at any point in time.

305
00:10:45,119 --> 00:10:45,950
Um,

306
00:10:46,288 --> 00:10:48,489
and, and part of how, what, how we think

307
00:10:48,489 --> 00:10:49,548
about the future is

308
00:10:49,969 --> 00:10:51,969
we take all this demand and just

309
00:10:51,969 --> 00:10:54,080
manage all that complexity for you, and by

310
00:10:54,080 --> 00:10:56,450
doing that, we can run it way more efficiently

311
00:10:56,450 --> 00:10:57,830
than anyone else can do

312
00:10:58,129 --> 00:11:00,168
because traditionally, at least in the last few years, the only

313
00:11:00,168 --> 00:11:01,788
way to get GPU access has been

314
00:11:02,129 --> 00:11:03,460
to go make reservations.

315
00:11:03,849 --> 00:11:05,928
I don't think that makes sense because then everyone has to make

316
00:11:05,928 --> 00:11:07,469
a reservation for the peak,

317
00:11:07,729 --> 00:11:09,769
but by like taking all that work, all, all

318
00:11:09,769 --> 00:11:11,830
these workloads and combining them, we

319
00:11:11,830 --> 00:11:12,489
can manage that.

320
00:11:13,570 --> 00:11:15,440
Utilization way more efficiently.

321
00:11:15,769 --> 00:11:17,239
So a lot of people actually,

322
00:11:17,649 --> 00:11:18,389
even though our

323
00:11:19,029 --> 00:11:20,288
$1 per GPU hour,

324
00:11:20,570 --> 00:11:21,460
sometimes it's higher,

325
00:11:21,889 --> 00:11:23,899
but the net cost might

326
00:11:23,899 --> 00:11:25,928
actually be lower moving to modal because you're only

327
00:11:25,928 --> 00:11:27,928
paying when the GPSs are actually running. And we can

328
00:11:27,928 --> 00:11:30,070
do that because we invested a lot in driving

329
00:11:30,070 --> 00:11:31,109
GPU efficiently,

330
00:11:31,369 --> 00:11:31,889
GPU

331
00:11:32,210 --> 00:11:32,989
utilization,

332
00:11:33,769 --> 00:11:35,168
because we have this multi-tenancy.

333
00:11:36,950 --> 00:11:37,489
Um

334
00:11:38,440 --> 00:11:40,340
And, and yeah, so you see some amazing

335
00:11:40,840 --> 00:11:43,119
developer love. I really think that the developer

336
00:11:43,119 --> 00:11:44,798
experience is what sets Modal apart.

337
00:11:45,080 --> 00:11:47,158
People really comment on like, you can just

338
00:11:47,158 --> 00:11:48,729
write some, a few lines of code,

339
00:11:49,239 --> 00:11:51,279
get it out in production, you don't have to worry. So

340
00:11:51,279 --> 00:11:53,500
it's really time to market that I think people

341
00:11:53,889 --> 00:11:55,259
come to us initially for.

342
00:11:55,759 --> 00:11:57,779
They want to have their very complex models, maybe

343
00:11:57,779 --> 00:11:58,619
they're training or

344
00:11:58,918 --> 00:12:00,960
doing fine tuning. They want to scale up,

345
00:12:01,070 --> 00:12:02,580
you know, big bass shops or run.

346
00:12:03,229 --> 00:12:05,269
Code sandboxes, and all you have to

347
00:12:05,269 --> 00:12:07,168
do is pip install model and get started,

348
00:12:07,509 --> 00:12:09,590
and people really love this developer experience. The

349
00:12:09,590 --> 00:12:11,769
one thing if you talk to our customers, it's,

350
00:12:12,269 --> 00:12:13,048
you know, it's a

351
00:12:13,788 --> 00:12:16,279
We just handle all of that underneath, all the infrastructure

352
00:12:16,279 --> 00:12:18,178
and you just have to write like a few lines of Python.

353
00:12:19,279 --> 00:12:20,469
Uh, here's another example.

354
00:12:21,200 --> 00:12:21,759
You, you, we,

355
00:12:22,308 --> 00:12:24,529
we help sub stacks scale out to hundreds

356
00:12:24,529 --> 00:12:26,320
of GPUs, like big batch jobs.

357
00:12:26,649 --> 00:12:28,729
Uh, we have many other customers that scale out to thousands

358
00:12:28,729 --> 00:12:29,629
of GPUs,

359
00:12:30,009 --> 00:12:32,129
uh, maybe around, we had a customer the other

360
00:12:32,129 --> 00:12:34,250
day who processed 3000

361
00:12:34,250 --> 00:12:36,288
years of audio in a few

362
00:12:36,288 --> 00:12:38,609
days, uh, doing some advanced,

363
00:12:38,690 --> 00:12:40,408
uh, I don't know what they're doing actually.

364
00:12:41,190 --> 00:12:41,969
It's very cool.

365
00:12:45,489 --> 00:12:46,509
And the love is there.

366
00:12:47,009 --> 00:12:49,330
We've been building this for about 4 years. We're obsessed

367
00:12:49,330 --> 00:12:51,590
with building products

368
00:12:51,928 --> 00:12:54,048
that developers really love. I'm

369
00:12:54,048 --> 00:12:55,710
a developer. I've been writing code for

370
00:12:56,288 --> 00:12:58,389
32 years, I believe, something like that.

371
00:12:59,308 --> 00:13:01,779
Um, professionally for 25 years,

372
00:13:01,908 --> 00:13:02,940
uh, we, um,

373
00:13:03,269 --> 00:13:05,590
we really think about how do we make engineers

374
00:13:05,590 --> 00:13:07,489
productive and how do we take away the sort of

375
00:13:08,070 --> 00:13:10,080
boring parts of infras of, of,

376
00:13:10,190 --> 00:13:10,808
of their job,

377
00:13:11,149 --> 00:13:13,168
and which in many, many cases is the

378
00:13:13,168 --> 00:13:14,548
infrastructure, the configuration.

379
00:13:15,229 --> 00:13:17,389
We love infrastructure, so I like building

380
00:13:17,389 --> 00:13:18,009
this stuff.

381
00:13:18,308 --> 00:13:20,308
So we take the, the boring stuff away from

382
00:13:20,308 --> 00:13:22,519
you and we build all the infrastructure for you,

383
00:13:22,830 --> 00:13:25,168
so you don't have to think about it, especially with these AI

384
00:13:25,168 --> 00:13:27,048
applications, they're pretty hard to manage.

385
00:13:27,558 --> 00:13:29,759
Our typical customers tends to be machine learning

386
00:13:29,759 --> 00:13:31,960
engineers. It tends to be people who are building models

387
00:13:31,960 --> 00:13:32,950
or fine tuning,

388
00:13:33,210 --> 00:13:35,440
building quite advanced things, and they really

389
00:13:35,440 --> 00:13:37,379
love that they get this like full control.

390
00:13:37,719 --> 00:13:39,719
We're not trying to take away the power and the

391
00:13:39,719 --> 00:13:40,700
control from customers.

392
00:13:41,080 --> 00:13:43,200
In fact, we're actually enabling them to move

393
00:13:43,200 --> 00:13:45,359
faster and deploy more complex

394
00:13:45,359 --> 00:13:47,479
things, and get the power of, you know, the

395
00:13:47,479 --> 00:13:49,479
scalability in the cloud and, and get

396
00:13:49,479 --> 00:13:51,558
even more GPUs than they previously had.

397
00:13:52,440 --> 00:13:53,460
So it's really about,

398
00:13:53,798 --> 00:13:55,219
you know, making engineers

399
00:13:55,509 --> 00:13:57,639
move faster and, and

400
00:13:57,639 --> 00:13:59,090
scale up at large scale.

401
00:14:01,250 --> 00:14:03,629
And, and, and a lot of what people

402
00:14:03,889 --> 00:14:05,038
do is really like,

403
00:14:05,330 --> 00:14:07,330
again, we're not an AI API we don't have a set

404
00:14:07,330 --> 00:14:09,308
of models. You can run any code or model.

405
00:14:10,048 --> 00:14:12,200
Whatever code you have, if it's a proprietary

406
00:14:12,200 --> 00:14:13,259
model, in the case of

407
00:14:13,649 --> 00:14:15,690
uh Suno, or if it's an open source

408
00:14:15,690 --> 00:14:17,719
model, you can deploy it, we can help you with that.

409
00:14:17,969 --> 00:14:20,009
We have a lot of templates and a lot of examples for how to

410
00:14:20,009 --> 00:14:22,269
get started. Um, and

411
00:14:22,489 --> 00:14:24,210
like I said, it's consumption based,

412
00:14:24,590 --> 00:14:26,750
so you only pay for the time the GPUs

413
00:14:26,750 --> 00:14:28,788
are actually running, which is a really nice feature because

414
00:14:28,788 --> 00:14:30,908
it means with these, you know, unpredictable

415
00:14:30,908 --> 00:14:32,969
scaling demands that you often have doing it in

416
00:14:32,969 --> 00:14:35,029
France, uh, you don't have to pay for the sort

417
00:14:35,029 --> 00:14:37,340
of periods in between when the GPUs

418
00:14:37,340 --> 00:14:38,048
are sitting idle,

419
00:14:38,428 --> 00:14:40,288
because we can run other workloads on those.

420
00:14:42,759 --> 00:14:44,139
So how do you get started?

421
00:14:44,599 --> 00:14:46,359
If you want to check it out, it's very easy.

422
00:14:46,639 --> 00:14:47,460
Go to your terminal,

423
00:14:47,840 --> 00:14:49,229
do PIP install modal,

424
00:14:49,519 --> 00:14:51,548
you can start writing code and running it in the cloud

425
00:14:51,798 --> 00:14:52,500
immediately.

426
00:14:52,918 --> 00:14:55,099
Uh, every user gets $30

427
00:14:55,099 --> 00:14:56,658
a month of free credits.

428
00:14:57,099 --> 00:15:00,178
And if you're a startup, you can actually get up to $50,000

429
00:15:00,178 --> 00:15:02,239
worth of credits, which goes a pretty long way.

430
00:15:02,599 --> 00:15:04,899
That's we have many startups who initially started

431
00:15:04,899 --> 00:15:05,538
at Modal,

432
00:15:05,849 --> 00:15:07,979
many white combinator companies, etc.

433
00:15:08,479 --> 00:15:09,879
started scaling at Modal,

434
00:15:10,219 --> 00:15:12,619
uh, or, or, or doing the prototypes.

435
00:15:12,705 --> 00:15:14,783
Issue with modal and now they're running a very

436
00:15:14,783 --> 00:15:15,945
large scale in production.

437
00:15:16,344 --> 00:15:18,423
So we're there for you when you're building the prototypes.

438
00:15:18,965 --> 00:15:20,783
We, we help you with the developer experience.

439
00:15:21,104 --> 00:15:22,004
We help you move fast.

440
00:15:22,423 --> 00:15:24,465
We also help you scale it up and

441
00:15:24,465 --> 00:15:26,543
run it on thousands of GPUs when you're

442
00:15:26,543 --> 00:15:28,205
ready to put some production workloads.

443
00:15:29,139 --> 00:15:29,729
Um

444
00:15:30,450 --> 00:15:32,509
So thanks a lot. And if you want to reach out

445
00:15:32,509 --> 00:15:34,928
to me, you can follow me on Twitter. I'm Bernhardtson,

446
00:15:34,969 --> 00:15:37,019
you can check out my blog, which is,

447
00:15:37,369 --> 00:15:39,450
sadly, I don't blog as much as I should these

448
00:15:39,450 --> 00:15:41,529
days, uh, but I used to. Uh, or you

449
00:15:41,529 --> 00:15:43,529
can email me. I'm Eric with a K at

450
00:15:43,529 --> 00:15:45,070
Model Model.com.

451
00:15:45,450 --> 00:15:47,070
So we'd love to stay in touch. Any

452
00:15:47,879 --> 00:15:50,029
questions or any thoughts? I don't know if we have time for questions.

453
00:15:53,428 --> 00:15:54,000
Thank you.

