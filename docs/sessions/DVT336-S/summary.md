# AWS re:Invent 2025 - 定制Llama模型用于代码生成 (DVT 3.6)

## 会议概述

本次技术分享会由Meta AI合作伙伴工程师Eissa Jamil主讲，重点介绍如何定制Llama模型来增强代码生成能力和开发者生产力。会议涵盖了从Llama模型家族介绍到实际部署的完整流程，包括数据准备、模型训练、评估和在AWS平台上的部署策略。

演讲者详细阐述了将大语言模型适配到编程任务时面临的挑战，如编程语言语法复杂性、数据质量问题和评估困难等。同时分享了Meta开源的工具和资源，包括Llama Recipes代码库中的端到端用例示例，以及PromptOps和DataKit等实用工具，帮助开发者更高效地构建和部署定制化的代码生成解决方案。

## 详细时间线与关键要点

### 0:00-5:00 开场介绍与议程概览
- 会议主题：定制Llama模型用于代码生成
- 演讲者：Eissa Jamil，Meta AI合作伙伴工程师
- 主要议程：开发者工具、编程任务适配挑战、数据准备、模型训练评估、AWS部署策略
- 进行会前匿名调研

### 5:00-10:00 Llama模型家族介绍
- Llama开源特性：支持语言翻译、个人助手、聊天机器人、智能代理、内容创作等多种用例
- Llama发展历程：
  - Llama 1 (2023年2月)：主要用于研究
  - Llama 2：支持商业用途
  - Code Llama、Purple Llama：专注安全和内容审核
  - Llama 3系列：包括3.1 8B、3.2多模态模型、1B和3B小型模型
  - Llama 3.3：更新的70B版本
  - Llama 4 (2025年)：Scout和Maverick多模态模型

### 10:00-15:00 开源生态与模型选择
- 统计数据：超过10亿次Hugging Face下载，20万个衍生模型
- 开源优势：部署灵活性、模型微调能力、模型蒸馏选项
- 模型推荐：
  - 8B模型：快速微调、成本效益高、推理速度快
  - 70B模型：性能优秀、适合对话和深度上下文用例
  - Llama 4：首个多模态模型，支持图像和文本输入

### 15:00-20:00 AWS部署选项
- Amazon Bedrock：模型即服务，快速原型开发，无需管理基础设施
- SageMaker：生产环境，支持优化芯片组(Inferentia/Trainium)，HyperPod集群管理
- EKS和EC2：自定义部署选项
- PromptOps工具：自动优化Llama模型提示词的Python包

### 20:00-25:00 编程任务适配挑战
- 语法复杂性：不同编程语言和版本的语法差异
- 数据质量问题：编程数据集相对较小，难以获取，缺乏针对特定公司标准的定制化
- 评估困难：代码评估复杂，缺乏标准化评估方法

### 25:00-30:00 数据准备与预处理
- 数据获取：代码和编程资源、内部数据集、内部代码仓库
- 预处理步骤：
  - 按编程语言分割和过滤
  - 数据清理和敏感信息处理
  - 标记化处理
  - 隐私敏感数据转换
- 迭代优化数据集质量

### 30:00-35:00 模型微调与评估
- 微调策略：
  - 选择合适的模型大小(8B vs 70B)
  - 监督微调(SFT)技术
  - 参数高效微调(PEFT/LoRA)
  - 强化学习(RLHF)
- 评估关键指标：
  - 准确性和置信度
  - 可编译代码生成
  - 上下文感知能力
  - 人工评估和自定义内部数据集
  - 标准化指标(BLEU等)

### 35:00-36:30 部署与实际应用
- 部署选项：Bedrock自定义模型导入、EKS生产部署、IDE集成
- 实际应用场景：
  - 内部开发工具和生产力提升
  - 自动化代码审查
  - DevOps流程优化
- 开源资源：Llama Recipes/Cookbooks代码库，包含端到端用例示例
- 工具推荐：PromptOps(提示词优化)、DataKit(合成数据生成)