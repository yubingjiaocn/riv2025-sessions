1
00:00:01,680 --> 00:00:02,850
- Hi everyone, thank you.

2
00:00:02,850 --> 00:00:04,110
My name's Michael Webster.

3
00:00:04,110 --> 00:00:06,600
I'm a principal engineer at CircleCI

4
00:00:06,600 --> 00:00:07,980
and we're gonna talk a little bit today

5
00:00:07,980 --> 00:00:10,860
about kind of where we
are right now with agents

6
00:00:10,860 --> 00:00:12,840
and what they're doing to the SDLC.

7
00:00:12,840 --> 00:00:13,673
Some of the problems

8
00:00:13,673 --> 00:00:15,464
that we're seeing already being created,

9
00:00:15,464 --> 00:00:18,810
and some techniques we're
finding effective to fix it.

10
00:00:18,810 --> 00:00:22,110
I'm including some things
we're adding into our product.

11
00:00:22,110 --> 00:00:23,490
So yeah, let's go and get started.

12
00:00:23,490 --> 00:00:26,430
As a quick recap, I wanna
go over sort of like,

13
00:00:26,430 --> 00:00:28,650
kind of the history of agentic workflows.

14
00:00:28,650 --> 00:00:30,510
I won't go too in-depth,

15
00:00:30,510 --> 00:00:31,890
but I think kind of where we are now

16
00:00:31,890 --> 00:00:34,350
is starting to see a little
bit of a decoupling point

17
00:00:34,350 --> 00:00:35,820
that we didn't have, right?

18
00:00:35,820 --> 00:00:38,971
So back in the days of 2021, 2022,

19
00:00:38,971 --> 00:00:41,820
there was a lot of developer copy-pasting

20
00:00:41,820 --> 00:00:45,848
between terminal IDE, a
ChatGPT window, what have you.

21
00:00:45,848 --> 00:00:49,440
Then we start seeing IDEs come
along that are still driven

22
00:00:49,440 --> 00:00:53,070
by the developer, but are
capable of more agentic tasks.

23
00:00:53,070 --> 00:00:56,130
They can do more long-range
planning and execution.

24
00:00:56,130 --> 00:00:57,240
This was really interesting,

25
00:00:57,240 --> 00:01:00,960
unlocked a lot of potential
within the AI space.

26
00:01:00,960 --> 00:01:02,040
But now what we're seeing

27
00:01:02,040 --> 00:01:04,650
is these sort of headless agents
starting to emerge, right?

28
00:01:04,650 --> 00:01:07,800
You take a CLI, you throw it
inside of a Docker container,

29
00:01:07,800 --> 00:01:09,930
and now suddenly you can
start running cron jobs,

30
00:01:09,930 --> 00:01:11,571
webhook-triggered agent runs,

31
00:01:11,571 --> 00:01:15,540
anything you really like,
in order to impact this.

32
00:01:15,540 --> 00:01:17,070
Now, there's a lot of
products on the market

33
00:01:17,070 --> 00:01:19,470
they've launched in the
last six months to do this,

34
00:01:19,470 --> 00:01:21,030
and so I thought it was interesting

35
00:01:21,030 --> 00:01:25,140
to maybe take a look and see
how real is this trend, right?

36
00:01:25,140 --> 00:01:29,070
So I went through and I looked
at all of the bot activity

37
00:01:29,070 --> 00:01:30,945
in the public GitHub archive,

38
00:01:30,945 --> 00:01:34,023
going back to the end of last year.

39
00:01:35,100 --> 00:01:36,450
And just sort of looked into like,

40
00:01:36,450 --> 00:01:38,670
for these known coding agents,

41
00:01:38,670 --> 00:01:40,833
a small subset of them, five or six,

42
00:01:41,970 --> 00:01:43,920
what are they actually doing on GitHub?

43
00:01:43,920 --> 00:01:45,240
And you can see a normal

44
00:01:45,240 --> 00:01:48,120
sort of like really rapid
growth trend overall

45
00:01:48,120 --> 00:01:51,420
within these agents across, you know,

46
00:01:51,420 --> 00:01:53,430
all the big common ones, Copilot,

47
00:01:53,430 --> 00:01:55,928
Claude, Codex, all of those.

48
00:01:55,928 --> 00:01:58,110
But it gets more interesting.

49
00:01:58,110 --> 00:01:59,400
So there's obviously some growth,

50
00:01:59,400 --> 00:02:01,530
there's activity from these agents.

51
00:02:01,530 --> 00:02:03,360
But it gets more interesting
when you break it down

52
00:02:03,360 --> 00:02:04,680
into the event types.

53
00:02:04,680 --> 00:02:07,110
Like what are these agents actually doing?

54
00:02:07,110 --> 00:02:10,290
And you start to see a
really interesting trend

55
00:02:10,290 --> 00:02:13,950
from the first like three
months on and then forward.

56
00:02:13,950 --> 00:02:15,150
When these things first launched,

57
00:02:15,150 --> 00:02:17,190
they were just doing PR comments.

58
00:02:17,190 --> 00:02:19,380
This was just like code review bots,

59
00:02:19,380 --> 00:02:21,990
every example starter workflow
of how to use an agent

60
00:02:21,990 --> 00:02:24,930
and an action or in a CircleCI config.

61
00:02:24,930 --> 00:02:26,520
It was doing things like issue triage

62
00:02:26,520 --> 00:02:28,950
and code review, and that's what they did.

63
00:02:28,950 --> 00:02:31,950
But you can see around May
of this year, you started

64
00:02:31,950 --> 00:02:34,590
to see these agents actually
starting pushing code, right?

65
00:02:34,590 --> 00:02:36,390
This was something that
people sort of expected,

66
00:02:36,390 --> 00:02:38,520
but it took a little while to happen.

67
00:02:38,520 --> 00:02:41,670
And at this point they're
almost doing as much.

68
00:02:41,670 --> 00:02:44,200
This data is as recent as October,

69
00:02:44,200 --> 00:02:46,980
in some weeks you're seeing
just as much push activity

70
00:02:46,980 --> 00:02:49,410
as you are comment and PR activity, right?

71
00:02:49,410 --> 00:02:52,200
So these agents are
actually doing real work.

72
00:02:52,200 --> 00:02:55,440
They're pushing actual
code to real repositories,

73
00:02:55,440 --> 00:02:56,850
which is what you would expect, again,

74
00:02:56,850 --> 00:02:58,113
with the growth trends.

75
00:02:59,670 --> 00:03:01,800
So that's public GitHub activity,

76
00:03:01,800 --> 00:03:04,080
but that could simply be experiments,

77
00:03:04,080 --> 00:03:06,060
people with hobbyist projects,

78
00:03:06,060 --> 00:03:10,230
nothing necessarily like
in a real-world setting.

79
00:03:10,230 --> 00:03:12,900
So at CircleCI, we see a lot

80
00:03:12,900 --> 00:03:15,570
of build activity from multiple projects

81
00:03:15,570 --> 00:03:18,510
that aren't on the public GitHub archive.

82
00:03:18,510 --> 00:03:22,710
These are, you know, enterprises,
large and small startups.

83
00:03:22,710 --> 00:03:23,880
So we decided to look and see

84
00:03:23,880 --> 00:03:25,380
what does our activity pattern see?

85
00:03:25,380 --> 00:03:27,300
Do we see a similar growth trend

86
00:03:27,300 --> 00:03:30,600
as what we see in the public archive?

87
00:03:30,600 --> 00:03:33,261
And this is a screenshot
kind of some aggregated

88
00:03:33,261 --> 00:03:34,980
and anonymized data.

89
00:03:34,980 --> 00:03:37,211
And you see, again, we see this trend,

90
00:03:37,211 --> 00:03:39,630
we see the same pattern repeat itself.

91
00:03:39,630 --> 00:03:41,490
So there's activity happening.

92
00:03:41,490 --> 00:03:43,051
And in particular with CircleCI,

93
00:03:43,051 --> 00:03:46,530
these are cases where we
actually ran a pipeline.

94
00:03:46,530 --> 00:03:49,530
This is not a case where
someone simply updated a README

95
00:03:49,530 --> 00:03:51,630
or maybe it was building a static blog.

96
00:03:51,630 --> 00:03:55,050
This is someone took the
time to configure a pipeline

97
00:03:55,050 --> 00:03:56,728
to run unit tests and do deploys

98
00:03:56,728 --> 00:03:59,310
in response to a push event.

99
00:03:59,310 --> 00:04:00,848
All right, so this is like,

100
00:04:00,848 --> 00:04:03,630
you would think this is
economically valuable work

101
00:04:03,630 --> 00:04:04,740
being done by these agents.

102
00:04:04,740 --> 00:04:08,190
It's not just, I had a hobby
project, let me go turn on

103
00:04:08,190 --> 00:04:10,650
and see if Claude can
keep my GitHub, you know,

104
00:04:10,650 --> 00:04:12,550
activity green or something like that.

105
00:04:13,619 --> 00:04:15,030
And again, this is all looking at agents

106
00:04:15,030 --> 00:04:16,770
that we can distinctly identify.

107
00:04:16,770 --> 00:04:19,170
So this is our low-end
estimate of what's happening.

108
00:04:19,170 --> 00:04:22,560
If you use Claude Code or Codex or Kiro

109
00:04:22,560 --> 00:04:24,330
and commit under your own name,

110
00:04:24,330 --> 00:04:26,820
we're not gonna necessarily
be able to tell that it's you.

111
00:04:26,820 --> 00:04:29,370
So this is sort of like
a very low-bar estimate,

112
00:04:29,370 --> 00:04:31,500
and we've seen within really about five

113
00:04:31,500 --> 00:04:34,320
or six months, we've seen
a very fast growth clip.

114
00:04:34,320 --> 00:04:36,688
And this is reflecting in
the revenue of these tools

115
00:04:36,688 --> 00:04:39,480
and just the general trends overall

116
00:04:39,480 --> 00:04:42,420
where people are more willing to try out

117
00:04:42,420 --> 00:04:43,833
these headless agent votes.

118
00:04:45,090 --> 00:04:46,320
Okay, so why is this a problem?

119
00:04:46,320 --> 00:04:47,861
Like, this is the thing
that everybody wanted.

120
00:04:47,861 --> 00:04:50,760
We wanted these agents to
do more than just like,

121
00:04:50,760 --> 00:04:52,380
get us out of the IDE.

122
00:04:52,380 --> 00:04:53,880
Instead of multi-boxing cursor,

123
00:04:53,880 --> 00:04:56,792
you can now just have multiple
agents running in parallel.

124
00:04:56,792 --> 00:04:59,540
And the issue here really comes down to

125
00:04:59,540 --> 00:05:01,800
the code isn't really valuable

126
00:05:01,800 --> 00:05:03,360
until it's in a customer's hands.

127
00:05:03,360 --> 00:05:06,360
And all of the stuff that
happens after you write the code

128
00:05:06,360 --> 00:05:09,420
is not necessarily keeping up, right?

129
00:05:09,420 --> 00:05:12,870
PRs are getting massive, open
source projects are requiring

130
00:05:12,870 --> 00:05:15,390
AI disclosures because they
suddenly get mysterious

131
00:05:15,390 --> 00:05:17,373
2000 line PRs that all look alike.

132
00:05:19,050 --> 00:05:20,760
Reviews are taking longer,

133
00:05:20,760 --> 00:05:23,220
humans looking at those 2000 line PRs

134
00:05:23,220 --> 00:05:27,150
and in general, the build
stability isn't really improving.

135
00:05:27,150 --> 00:05:30,120
And this is kind of what you would expect.

136
00:05:30,120 --> 00:05:32,040
This is a basic kind of queuing theory.

137
00:05:32,040 --> 00:05:34,390
It's a branch of math
about how queues operate.

138
00:05:35,430 --> 00:05:39,120
To simplify it a lot,
really, if work is arriving

139
00:05:39,120 --> 00:05:41,040
into your system faster than you're able

140
00:05:41,040 --> 00:05:43,110
to process it, you get delays.

141
00:05:43,110 --> 00:05:44,700
You probably understand this intuitively.

142
00:05:44,700 --> 00:05:45,840
If you've ever been at a store

143
00:05:45,840 --> 00:05:48,210
that only had a single checkout
line when they were busy,

144
00:05:48,210 --> 00:05:49,710
everybody has to wait.

145
00:05:49,710 --> 00:05:52,101
Eventually, some people
might get tired and give up,

146
00:05:52,101 --> 00:05:55,830
but when we have, you know, a
revenue feature on the line,

147
00:05:55,830 --> 00:05:57,030
we can't really just stop.

148
00:05:57,030 --> 00:05:58,830
So the queue just builds up and builds up,

149
00:05:58,830 --> 00:06:01,350
and then people stop working
to go drain the queue

150
00:06:01,350 --> 00:06:02,763
and do all the reviews.

151
00:06:04,589 --> 00:06:07,530
So the reality is for
a lot of organizations,

152
00:06:07,530 --> 00:06:10,080
even though you can
write a lot of code now,

153
00:06:10,080 --> 00:06:12,390
you probably couldn't
actually go much faster

154
00:06:12,390 --> 00:06:15,309
if you wanted to in terms
of your delivery processes.

155
00:06:15,309 --> 00:06:18,360
To give an example of
what this looks like,

156
00:06:18,360 --> 00:06:20,370
I put together kind of
a queuing simulation

157
00:06:20,370 --> 00:06:22,680
of under various scenarios what happens

158
00:06:22,680 --> 00:06:25,770
with different speed-ups due to AI, right?

159
00:06:25,770 --> 00:06:29,520
So, on the bottom, we kinda
have a status quo baseline.

160
00:06:29,520 --> 00:06:32,130
It assumes that you can process code

161
00:06:32,130 --> 00:06:34,380
twice as fast as you can write it.

162
00:06:34,380 --> 00:06:37,080
But if you hold some of
these things constant,

163
00:06:37,080 --> 00:06:40,890
as the AI gets faster and
faster, the delays get larger

164
00:06:40,890 --> 00:06:43,950
and they come up even more quickly.

165
00:06:43,950 --> 00:06:45,600
And this is particularly a problem

166
00:06:45,600 --> 00:06:48,660
because with humans, the workday
ends at some point, right?

167
00:06:48,660 --> 00:06:50,220
You don't have an infinite backlog

168
00:06:50,220 --> 00:06:53,070
because people don't work
at an AI pace all the time.

169
00:06:53,070 --> 00:06:54,920
But the agents can, if
you wanna hook them up

170
00:06:54,920 --> 00:06:57,150
to your Jira backlog, your
linear queue, whatever,

171
00:06:57,150 --> 00:07:01,170
whatever it is, the reality is
that for most organizations,

172
00:07:01,170 --> 00:07:03,030
you're not actually gonna
be able to get the benefit

173
00:07:03,030 --> 00:07:04,317
because you're gonna be
spending all your time reviewing

174
00:07:04,317 --> 00:07:07,083
the PRs and waiting for
the deploys to go through.

175
00:07:09,240 --> 00:07:11,130
This theory aligns with
subjective feedback.

176
00:07:11,130 --> 00:07:14,400
You look at the DORA metrics,
there's actually some,

177
00:07:14,400 --> 00:07:17,820
many teams are reporting an
increase in instability from AI.

178
00:07:17,820 --> 00:07:21,390
There's minimal effects on
the product effectiveness.

179
00:07:21,390 --> 00:07:24,060
And, in general, burnout.

180
00:07:24,060 --> 00:07:27,270
If you look at other industry benchmarks,

181
00:07:27,270 --> 00:07:30,090
when you dig in further, you
see that a lot of the AI gains,

182
00:07:30,090 --> 00:07:32,760
they center around 10% improvement,

183
00:07:32,760 --> 00:07:34,050
but that's widely distributed.

184
00:07:34,050 --> 00:07:35,730
It's basically a bimodal distribution.

185
00:07:35,730 --> 00:07:37,950
People who are really good
at delivering software

186
00:07:37,950 --> 00:07:39,420
are getting most of the benefit.

187
00:07:39,420 --> 00:07:41,070
People that are average to mediocre,

188
00:07:41,070 --> 00:07:42,360
they're actually seeing no benefit

189
00:07:42,360 --> 00:07:43,803
to negative effects from AI.

190
00:07:45,720 --> 00:07:47,850
Okay, and this is a problem obviously

191
00:07:47,850 --> 00:07:49,920
because like, you know, as a technologist,

192
00:07:49,920 --> 00:07:51,120
we want these tools to work,

193
00:07:51,120 --> 00:07:54,090
but also we're not putting
10% level improvements

194
00:07:54,090 --> 00:07:56,370
from your senior engineer
investments into AI.

195
00:07:56,370 --> 00:07:58,020
We're spending hundreds
of thousands to millions

196
00:07:58,020 --> 00:08:00,347
of dollars building data
centers, all kinds of inference.

197
00:08:00,347 --> 00:08:04,410
We need more than just
incremental improvements.

198
00:08:04,410 --> 00:08:06,568
So how can we fix this?

199
00:08:06,568 --> 00:08:10,620
Well, the simple pithy answer
is to go faster, right?

200
00:08:10,620 --> 00:08:12,390
You know, in that
simulation I showed before,

201
00:08:12,390 --> 00:08:14,340
we said like, okay, the
code comes in faster,

202
00:08:14,340 --> 00:08:16,080
but the delivery goes the same.

203
00:08:16,080 --> 00:08:18,600
Well, what if we just
made the delivery faster?

204
00:08:18,600 --> 00:08:21,360
And you can see again,
expected effects here.

205
00:08:21,360 --> 00:08:22,980
If you're able to ship the code faster,

206
00:08:22,980 --> 00:08:24,568
the curve kind of bends down.

207
00:08:24,568 --> 00:08:27,750
In some cases, even if you
don't get any improvement

208
00:08:27,750 --> 00:08:30,240
from AI, if your agents
are still producing

209
00:08:30,240 --> 00:08:32,100
at the same output as your developer,

210
00:08:32,100 --> 00:08:33,810
but you've made your delivery faster,

211
00:08:33,810 --> 00:08:36,210
you can actually cut your delays.

212
00:08:36,210 --> 00:08:38,943
So this has benefit even
if you're using AI or not.

213
00:08:41,490 --> 00:08:44,670
Okay, so we have a hypothesis
of how we can improve this.

214
00:08:44,670 --> 00:08:47,130
We have some levers that
we think we can pull.

215
00:08:47,130 --> 00:08:48,030
So what does it look like?

216
00:08:48,030 --> 00:08:48,990
How do we actually do that?

217
00:08:48,990 --> 00:08:53,103
Just go faster is not a
particularly actionable task.

218
00:08:54,570 --> 00:08:56,850
To start with, like I
love AI, I love agents,

219
00:08:56,850 --> 00:08:57,930
but I wanna be clear,

220
00:08:57,930 --> 00:08:59,520
a lot of this isn't like whiz-bang

221
00:08:59,520 --> 00:09:00,840
multi-agent orchestration.

222
00:09:00,840 --> 00:09:02,940
Like it's block and tackling engineering,

223
00:09:02,940 --> 00:09:05,343
like stuff we've been
doing for 15, 25 years.

224
00:09:06,540 --> 00:09:09,570
You know, having things
like acceptance tests

225
00:09:09,570 --> 00:09:11,940
that will reliably tell
you if things broke,

226
00:09:11,940 --> 00:09:13,779
being confident that when you

227
00:09:13,779 --> 00:09:18,648
get an automated PR from a
bot, whether it's a CDE bot

228
00:09:18,648 --> 00:09:22,380
or an AI bot, that it didn't
cause a regression, right?

229
00:09:22,380 --> 00:09:23,940
Where you're comfortable trusting the fact

230
00:09:23,940 --> 00:09:25,800
that if your tests say
you're green, you're green,

231
00:09:25,800 --> 00:09:27,350
and you can go ahead and merge.

232
00:09:28,410 --> 00:09:30,510
You have to make investments
in your delivery pipelines.

233
00:09:30,510 --> 00:09:33,540
And so I don't wanna
downplay all of this work.

234
00:09:33,540 --> 00:09:36,246
This is all super necessary
and it's very tractable

235
00:09:36,246 --> 00:09:38,760
and it's definitely a thing
that everyone should invest in.

236
00:09:38,760 --> 00:09:41,090
AI's really great at helping
out with this too, right?

237
00:09:41,090 --> 00:09:42,510
If you need to convert scripts,

238
00:09:42,510 --> 00:09:44,340
if you've got a bunch of
bash scripts that are slow

239
00:09:44,340 --> 00:09:46,827
and doing sub processing, you
can rewrite it in a language

240
00:09:46,827 --> 00:09:50,460
that's compiled and the AI
will help you with that.

241
00:09:50,460 --> 00:09:53,070
So it's definitely a spot
where AI benefits you,

242
00:09:53,070 --> 00:09:54,670
but you do have to do this work.

243
00:09:55,797 --> 00:09:57,720
Okay, but let's just assume
for the sake of argument

244
00:09:57,720 --> 00:10:00,060
that's still not fast enough,
that only gets you, you know,

245
00:10:00,060 --> 00:10:03,150
a slight reduction in your queuing delay.

246
00:10:03,150 --> 00:10:04,800
Well, how can you go even faster?

247
00:10:06,390 --> 00:10:08,400
And the way we think
about this at CircleCI is

248
00:10:08,400 --> 00:10:09,990
that it really comes down to validation.

249
00:10:09,990 --> 00:10:12,780
I'm riffing on a really old
saying about amateur stocking

250
00:10:12,780 --> 00:10:15,300
strategy and professional
stocking logistics.

251
00:10:15,300 --> 00:10:17,070
But the idea here is like instead

252
00:10:17,070 --> 00:10:19,080
of obsessing about what's the latest

253
00:10:19,080 --> 00:10:20,580
and greatest way to prompt an agent,

254
00:10:20,580 --> 00:10:22,920
what's the latest and
greatest memory framework?

255
00:10:22,920 --> 00:10:24,090
What have you.

256
00:10:24,090 --> 00:10:26,734
Think about how you can
validate the code, right?

257
00:10:26,734 --> 00:10:30,750
I'll try to make the case of
what I mean by validation here.

258
00:10:30,750 --> 00:10:32,940
This is a really simple loop.

259
00:10:32,940 --> 00:10:34,650
You make a plan, you do some work,

260
00:10:34,650 --> 00:10:37,050
and you have something that
judges the quality of that work.

261
00:10:37,050 --> 00:10:39,107
And then if it passes, you go through.

262
00:10:39,107 --> 00:10:40,350
If it doesn't pass,

263
00:10:40,350 --> 00:10:42,326
you go back to the beginning
and you start over.

264
00:10:42,326 --> 00:10:44,208
This is the foundation of

265
00:10:44,208 --> 00:10:47,808
if you wanna do
red-green-refactor, classic CI,

266
00:10:47,808 --> 00:10:50,550
any type of like the local loops

267
00:10:50,550 --> 00:10:52,770
that agents go through,
this is the pattern

268
00:10:52,770 --> 00:10:56,490
that's really effective
at getting work done.

269
00:10:56,490 --> 00:10:58,128
You do something, judge the results,

270
00:10:58,128 --> 00:11:00,810
give feedback, and continue forward.

271
00:11:00,810 --> 00:11:03,783
So this is the basic recipe
when we talk about validation.

272
00:11:06,539 --> 00:11:08,550
To make a further argument
for why I think validation

273
00:11:08,550 --> 00:11:10,350
is so important.

274
00:11:10,350 --> 00:11:12,300
It has a lot of nice properties here.

275
00:11:12,300 --> 00:11:14,070
It's really scalable, right?

276
00:11:14,070 --> 00:11:17,460
So the work you put into
making sure that your code,

277
00:11:17,460 --> 00:11:19,620
that you can validate
code written by agents,

278
00:11:19,620 --> 00:11:21,660
it works for individual tasks, right?

279
00:11:21,660 --> 00:11:24,840
You probably use a version
of this in your local loops

280
00:11:24,840 --> 00:11:27,750
where Claude will run tests,
run linters, formatting.

281
00:11:27,750 --> 00:11:30,300
It works if you want to
tune the prompts, right?

282
00:11:30,300 --> 00:11:32,580
Having a set of, we know
this was a good change,

283
00:11:32,580 --> 00:11:34,680
we know this was a bad change.

284
00:11:34,680 --> 00:11:36,210
If you even want to go full on

285
00:11:36,210 --> 00:11:38,460
like training your own RL agent,

286
00:11:38,460 --> 00:11:43,315
the same, that basic recipe
I showed of task, output,

287
00:11:43,315 --> 00:11:46,740
evaluate, and loop is really
the fundamental recipe.

288
00:11:46,740 --> 00:11:48,840
So it works all the way
up from your developer

289
00:11:48,840 --> 00:11:51,660
on your machine, all the way
up until you get to the point

290
00:11:51,660 --> 00:11:54,540
where you're wanting to
train your own agent.

291
00:11:54,540 --> 00:11:56,010
It's also durable, right?

292
00:11:56,010 --> 00:11:58,140
If you've been in the AI space at all,

293
00:11:58,140 --> 00:12:00,060
you're familiar with all of the churn,

294
00:12:00,060 --> 00:12:02,549
it's chain of thought,
then it's graph of thought,

295
00:12:02,549 --> 00:12:04,500
now it's program of thoughts.

296
00:12:04,500 --> 00:12:06,780
Like, there's like,
there's so many techniques

297
00:12:06,780 --> 00:12:08,100
that people learn and I'm glad

298
00:12:08,100 --> 00:12:09,630
that people are researching them.

299
00:12:09,630 --> 00:12:10,860
But for end developers,

300
00:12:10,860 --> 00:12:12,480
what ends up happening is they get trained

301
00:12:12,480 --> 00:12:14,220
into the underlying models.

302
00:12:14,220 --> 00:12:16,290
So trying to chase sort of like techniques

303
00:12:16,290 --> 00:12:19,530
and tactics, you need to do
that to some extent, right?

304
00:12:19,530 --> 00:12:21,630
You want to use the tools
that are available to you,

305
00:12:21,630 --> 00:12:24,480
but making that the
centerpiece of your strategy,

306
00:12:24,480 --> 00:12:26,970
you can probably steer your investment

307
00:12:26,970 --> 00:12:28,170
in more productive ways.

308
00:12:29,430 --> 00:12:32,250
Finally, agents are more
tractable for most organize,

309
00:12:32,250 --> 00:12:35,340
or validation is tractable
for most organizations.

310
00:12:35,340 --> 00:12:39,586
It's possible, but not
straightforward to run

311
00:12:39,586 --> 00:12:43,770
and train your own models,
but making your tests faster,

312
00:12:43,770 --> 00:12:46,140
understanding why your deploys failed,

313
00:12:46,140 --> 00:12:48,960
understanding what good code
review feedback looks like

314
00:12:48,960 --> 00:12:51,420
so that you can give it
to the developers earlier.

315
00:12:51,420 --> 00:12:53,730
This is all information
that's readily accessible.

316
00:12:53,730 --> 00:12:55,680
It's happening inside
of your organizations.

317
00:12:55,680 --> 00:12:57,900
You're probably using a
developer experience tool

318
00:12:57,900 --> 00:12:59,430
that is mining this

319
00:12:59,430 --> 00:13:02,760
to tell you subjective
feedback on your developers.

320
00:13:02,760 --> 00:13:05,250
So when you look across all of the things

321
00:13:05,250 --> 00:13:08,520
that you could do when it comes to agents

322
00:13:08,520 --> 00:13:10,860
investing in how you
validate their output,

323
00:13:10,860 --> 00:13:12,780
to me feels like the most effective

324
00:13:12,780 --> 00:13:14,793
high ROI option that we have.

325
00:13:16,740 --> 00:13:20,160
Okay, the other thing
that I would recommend

326
00:13:20,160 --> 00:13:22,110
investing in is context, right?

327
00:13:22,110 --> 00:13:26,550
So whenever we are dealing
with agents, one of the things

328
00:13:26,550 --> 00:13:28,920
that you have to manage
is, what does it know?

329
00:13:28,920 --> 00:13:30,300
What is it having to remember

330
00:13:30,300 --> 00:13:32,370
and what is it having to parse through

331
00:13:32,370 --> 00:13:33,903
in order to complete the task?

332
00:13:35,640 --> 00:13:37,830
The nice thing about this, about context,

333
00:13:37,830 --> 00:13:40,710
is your validation
pipeline becomes an input

334
00:13:40,710 --> 00:13:42,300
for context for the agent.

335
00:13:42,300 --> 00:13:46,380
Taking a log of having Claude Code work

336
00:13:46,380 --> 00:13:48,167
on a backlog of tasks,

337
00:13:48,167 --> 00:13:51,780
having another AI summarize
the results of that task,

338
00:13:51,780 --> 00:13:54,600
and then you feeding that
back to improve the prompts,

339
00:13:54,600 --> 00:13:57,690
the tools, the sub agents
that you might be using

340
00:13:57,690 --> 00:13:59,580
is a really powerful technique.

341
00:13:59,580 --> 00:14:01,170
So yeah, at the end of the
day, you might be changing

342
00:14:01,170 --> 00:14:04,260
a prompt, but you're driving
this off of real feedback

343
00:14:04,260 --> 00:14:07,413
that's tuned to your
organization and your code basis.

344
00:14:10,408 --> 00:14:12,960
Very quickly when we talk about context,

345
00:14:12,960 --> 00:14:14,670
there's a few levers
that I think are really

346
00:14:14,670 --> 00:14:16,410
important that you can deal with.

347
00:14:16,410 --> 00:14:19,410
You really kind of can
control the tools, the task,

348
00:14:19,410 --> 00:14:22,380
or the prompting of the
agent, as well as the check.

349
00:14:22,380 --> 00:14:24,810
Aort of, you know, this is
a very simple version of

350
00:14:24,810 --> 00:14:26,910
what an agent does where
you give it a task,

351
00:14:26,910 --> 00:14:29,463
it keeps looping until
it passes some standard.

352
00:14:31,050 --> 00:14:32,070
I think this check,

353
00:14:32,070 --> 00:14:34,320
I would recommend a
bottom up approach here.

354
00:14:34,320 --> 00:14:37,680
Start with a check, then
use that as a way to improve

355
00:14:37,680 --> 00:14:39,120
how you give tasks to the agent,

356
00:14:39,120 --> 00:14:41,130
and then extract those things into tools

357
00:14:41,130 --> 00:14:42,810
that the agent can use, right?

358
00:14:42,810 --> 00:14:44,400
So you use that validation as a way

359
00:14:44,400 --> 00:14:47,310
to ripple things back up until
you end up with a library

360
00:14:47,310 --> 00:14:48,990
of things that you have
really good confidence

361
00:14:48,990 --> 00:14:51,723
that the agent can use and
execute on effectively.

362
00:14:54,480 --> 00:14:56,460
So I'll give kinda like a verbal version

363
00:14:56,460 --> 00:14:57,870
of this basic recipe that we found

364
00:14:57,870 --> 00:15:00,090
really effective on that CircleCI.

365
00:15:00,090 --> 00:15:02,250
When we're working in
our development teams,

366
00:15:02,250 --> 00:15:05,370
which is we use the
feedback from CI pipelines

367
00:15:05,370 --> 00:15:08,040
or local testing runs to get the model

368
00:15:08,040 --> 00:15:09,690
to produce the results we want.

369
00:15:09,690 --> 00:15:11,520
Once we have a baseline, once we have a,

370
00:15:11,520 --> 00:15:14,790
we know that this is good,
then we can start extracting

371
00:15:14,790 --> 00:15:16,527
that process into rules to get the model

372
00:15:16,527 --> 00:15:18,150
to do that all the time.

373
00:15:18,150 --> 00:15:20,970
So instead of everyone
starting from scratch,

374
00:15:20,970 --> 00:15:25,290
one person goes through the
process of how do I get a

375
00:15:25,290 --> 00:15:29,790
Playwright-based flaky test
fixing process working?

376
00:15:29,790 --> 00:15:32,670
And then we're able to use
that across all of our projects

377
00:15:32,670 --> 00:15:34,590
to perform a specific task.

378
00:15:34,590 --> 00:15:37,410
And then just like any library
development kind of process

379
00:15:37,410 --> 00:15:38,700
that you might have done before,

380
00:15:38,700 --> 00:15:42,090
once you've got a few one-off
encapsulated functions,

381
00:15:42,090 --> 00:15:44,820
you can then turn those into
rules that are more abstract.

382
00:15:44,820 --> 00:15:47,760
So it's taking the same
techniques that we approach

383
00:15:47,760 --> 00:15:50,636
with software and just
applying them to the prompts,

384
00:15:50,636 --> 00:15:54,390
all driven off of a clear indicator of

385
00:15:54,390 --> 00:15:56,290
if what the agent did was good or bad.

386
00:15:58,440 --> 00:16:00,750
All right, and so, yeah,
I'll talk about Chunk now.

387
00:16:00,750 --> 00:16:01,957
So Chunk, this is our CircleCI,

388
00:16:01,957 --> 00:16:04,440
the agent that we're working with CircleCI

389
00:16:04,440 --> 00:16:05,520
that's built from the ground up

390
00:16:05,520 --> 00:16:08,100
with most of these principles in mind.

391
00:16:08,100 --> 00:16:11,370
The idea behind Chunk is to help make sure

392
00:16:11,370 --> 00:16:13,680
that your software is always validated

393
00:16:13,680 --> 00:16:16,890
and to make the process of
validating that software faster,

394
00:16:16,890 --> 00:16:18,573
more reliable, more effective.

395
00:16:19,937 --> 00:16:24,000
Chunk, we built this thing
very much validation first.

396
00:16:24,000 --> 00:16:26,340
Anytime this agent touches
your code base, it's going

397
00:16:26,340 --> 00:16:29,430
to execute your CI pipelines
to verify that it works.

398
00:16:29,430 --> 00:16:30,570
This means you're getting feedback

399
00:16:30,570 --> 00:16:32,910
from real environments
that you've already built.

400
00:16:32,910 --> 00:16:35,040
You already know that this environment

401
00:16:35,040 --> 00:16:36,870
and these commands are good enough to say

402
00:16:36,870 --> 00:16:38,850
that this code can go to production.

403
00:16:38,850 --> 00:16:41,130
And so we're just gonna reuse that.

404
00:16:41,130 --> 00:16:43,316
There's no real need to
completely reinvent the wheel here

405
00:16:43,316 --> 00:16:45,990
and have to think too much about hooks

406
00:16:45,990 --> 00:16:48,420
or some other side process
of development flow.

407
00:16:48,420 --> 00:16:49,770
You've already configured a way

408
00:16:49,770 --> 00:16:52,380
to test and see if these things are good.

409
00:16:52,380 --> 00:16:54,660
We also keep your
software production-ready.

410
00:16:54,660 --> 00:16:57,210
The first thing we're doing
is going after flaky tests.

411
00:16:57,210 --> 00:17:00,540
When it comes to delivery,
flaky tests are kind of the bane

412
00:17:00,540 --> 00:17:02,310
of at least my existence.

413
00:17:02,310 --> 00:17:04,410
When you have a change
that you know is good,

414
00:17:04,410 --> 00:17:06,180
but you have a test that is unreliable

415
00:17:06,180 --> 00:17:09,475
and can't always tell you
if something is good or bad,

416
00:17:09,475 --> 00:17:11,880
that slows everything down.

417
00:17:11,880 --> 00:17:14,670
It slows down humans,
it slows down agents,

418
00:17:14,670 --> 00:17:18,005
it really grinds, it's
standing in the gears

419
00:17:18,005 --> 00:17:22,380
to any kind of smooth delivery process.

420
00:17:22,380 --> 00:17:23,460
We're also working on, you know,

421
00:17:23,460 --> 00:17:25,560
things like improving code
coverage, which again,

422
00:17:25,560 --> 00:17:28,710
is very useful context
for agents to understand

423
00:17:28,710 --> 00:17:31,047
how files relate to each other

424
00:17:31,047 --> 00:17:33,630
and how they're tested effectively

425
00:17:33,630 --> 00:17:36,330
and just generally
handling a lot of the toil.

426
00:17:36,330 --> 00:17:38,190
Nobody really likes
maintaining CI pipelines.

427
00:17:38,190 --> 00:17:39,240
I work at a CI company.

428
00:17:39,240 --> 00:17:41,280
I don't like writing YAML,

429
00:17:41,280 --> 00:17:43,290
but agents are really
good at writing YAML.

430
00:17:43,290 --> 00:17:46,680
So things like build optimization,
keeping your build fast.

431
00:17:46,680 --> 00:17:50,040
These are all really, these
are all first class tasks

432
00:17:50,040 --> 00:17:51,840
that we'll be adding into Chunk.

433
00:17:51,840 --> 00:17:53,430
The other thing that Chunk does here,

434
00:17:53,430 --> 00:17:56,880
because we have context on all
of the changes that occurred

435
00:17:56,880 --> 00:17:58,650
and whether they were good or bad, right?

436
00:17:58,650 --> 00:18:00,930
We can follow a change all the
way through from the commit

437
00:18:00,930 --> 00:18:03,716
that came into GitHub to
was this change deployed,

438
00:18:03,716 --> 00:18:07,146
merged, deployed, or
eventually rolled back.

439
00:18:07,146 --> 00:18:10,215
We're able to build a
really good understanding of

440
00:18:10,215 --> 00:18:14,700
how you can make an agent
work more effectively.

441
00:18:14,700 --> 00:18:17,790
So instead of you having
to build your own loops,

442
00:18:17,790 --> 00:18:20,790
your own feedback cycles,
your own RL environments,

443
00:18:20,790 --> 00:18:23,460
we can take the results of your builds

444
00:18:23,460 --> 00:18:25,260
and we can use them as feedback to further

445
00:18:25,260 --> 00:18:27,393
to the agents in a more automatic fashion.

446
00:18:29,790 --> 00:18:32,793
All right, so quick
recap and takeaways here.

447
00:18:33,690 --> 00:18:37,920
The AI agent adoption trend
is quite real and growing.

448
00:18:37,920 --> 00:18:39,870
This is not, we're kind
of out of the realm

449
00:18:39,870 --> 00:18:42,390
of reviewing PRs or triaging issues.

450
00:18:42,390 --> 00:18:44,520
These are writing real code.

451
00:18:44,520 --> 00:18:46,770
What we're seeing today is
probably an underestimate

452
00:18:46,770 --> 00:18:50,493
of reality, but it is a
real and growing trend.

453
00:18:52,110 --> 00:18:55,890
We're seeing that this is
causing some negative impact

454
00:18:55,890 --> 00:18:58,308
and definitely is not
uniformly beneficial.

455
00:18:58,308 --> 00:19:01,530
Not all organizations are
getting the same results

456
00:19:01,530 --> 00:19:04,770
from AI initiatives
that they would expect.

457
00:19:04,770 --> 00:19:06,180
Also, hopefully I've convinced you,

458
00:19:06,180 --> 00:19:08,940
investing in the delivery
process is a really good start

459
00:19:08,940 --> 00:19:10,350
and a way to get going.

460
00:19:10,350 --> 00:19:13,290
And that the validation
that you do in delivery

461
00:19:13,290 --> 00:19:16,533
is the foundations of keeping
your agents fast and reliable.

462
00:19:17,790 --> 00:19:19,710
Yeah, and so that's all
I have for you today.

463
00:19:19,710 --> 00:19:21,540
If you want to talk to me more about Chunk

464
00:19:21,540 --> 00:19:23,730
or see a demo, I'll be
hanging out in the back,

465
00:19:23,730 --> 00:19:27,510
and then we're in Booth 1451 at CircleCI.

466
00:19:27,510 --> 00:19:28,510
Great, thanks y'all.

