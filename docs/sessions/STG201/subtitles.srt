1
00:00:02,070 --> 00:00:02,940
- Hey everyone.

2
00:00:02,940 --> 00:00:05,640
Thanks for joining. Good crowd.

3
00:00:05,640 --> 00:00:07,290
My name is Monica Vyavahare

4
00:00:07,290 --> 00:00:10,597
and I'm a senior technical
product manager with Amazon S3.

5
00:00:10,597 --> 00:00:14,460
And I'm joined by Jordan Dolman,
a principal product manager

6
00:00:14,460 --> 00:00:16,770
with Amazon FSx.

7
00:00:16,770 --> 00:00:18,690
Today we're going to cover how customers

8
00:00:18,690 --> 00:00:20,340
are using AWS storage

9
00:00:20,340 --> 00:00:23,880
as they build and scale new AI use cases.

10
00:00:23,880 --> 00:00:25,980
It's no secret that
everyone wants to build

11
00:00:25,980 --> 00:00:28,980
with generative AI, but
the real challenge is how

12
00:00:28,980 --> 00:00:30,660
to make it work for your data

13
00:00:30,660 --> 00:00:33,900
for your business at a
cost that makes sense.

14
00:00:33,900 --> 00:00:34,830
Today we're gonna show you

15
00:00:34,830 --> 00:00:38,613
how customers are achieving
this all built with AWS storage.

16
00:00:41,760 --> 00:00:44,850
The key to making AI work
for you is your data.

17
00:00:44,850 --> 00:00:48,276
Generic AI gives you generic
answers, but your data,

18
00:00:48,276 --> 00:00:51,720
like your customer feedback,
your usage patterns,

19
00:00:51,720 --> 00:00:55,050
that's what makes AI
valuable for your business.

20
00:00:55,050 --> 00:00:58,170
What we're seeing sets customers
apart today is the ability

21
00:00:58,170 --> 00:00:59,910
to access relevant data

22
00:00:59,910 --> 00:01:03,093
to fuel multi-step AI agentic workflows.

23
00:01:04,350 --> 00:01:06,596
When your AI agents can get access

24
00:01:06,596 --> 00:01:08,910
to the right data at the right time,

25
00:01:08,910 --> 00:01:10,920
they can better understand your business

26
00:01:10,920 --> 00:01:12,240
and your customers and

27
00:01:12,240 --> 00:01:15,090
that can dramatically help
you improve your productivity.

28
00:01:18,450 --> 00:01:20,910
So let's start with the core challenge.

29
00:01:20,910 --> 00:01:24,630
I have a given task that I want
to improve productivity for

30
00:01:24,630 --> 00:01:28,390
and I wanna use an existing
large language model or an LLM.

31
00:01:28,390 --> 00:01:30,958
I wanna use a foundational
model out of the box,

32
00:01:30,958 --> 00:01:33,813
but I need it to respond based on my data.

33
00:01:34,980 --> 00:01:39,980
But here's the problem, LLMs
are trained on static data.

34
00:01:39,990 --> 00:01:41,790
They don't know your latest information

35
00:01:41,790 --> 00:01:44,670
or your business context
or your use cases,

36
00:01:44,670 --> 00:01:47,420
so they give generic answers
and sometimes hallucinate.

37
00:01:51,663 --> 00:01:53,460
So we're gonna discuss several approaches

38
00:01:53,460 --> 00:01:55,290
on how you can build AI workflows

39
00:01:55,290 --> 00:01:58,650
for your business using
existing foundational models.

40
00:01:58,650 --> 00:02:00,330
This is where most people start today

41
00:02:00,330 --> 00:02:02,599
because it's pretty
easy to go from an idea

42
00:02:02,599 --> 00:02:05,343
and the data you have
to higher productivity.

43
00:02:06,180 --> 00:02:08,416
As we cover different
approaches today, they're going

44
00:02:08,416 --> 00:02:12,010
to increase in cost, time
and complexity, but they're

45
00:02:12,010 --> 00:02:15,180
also gonna dramatically
improve in output quality.

46
00:02:15,180 --> 00:02:16,680
So you can kind of pick where

47
00:02:16,680 --> 00:02:19,953
on this scale which approach
fits right for your business.

48
00:02:21,630 --> 00:02:23,760
Let's start with the simplest approach.

49
00:02:23,760 --> 00:02:24,843
Prompt engineering.

50
00:02:28,530 --> 00:02:30,690
If you have a single task in mind

51
00:02:30,690 --> 00:02:34,440
and data odds are you're
starting with prompt engineering.

52
00:02:34,440 --> 00:02:37,560
You give the LLM examples, context

53
00:02:37,560 --> 00:02:40,440
and constraints to guide the response.

54
00:02:40,440 --> 00:02:42,690
I can give you a personal example.

55
00:02:42,690 --> 00:02:44,430
At Amazon before we build

56
00:02:44,430 --> 00:02:48,600
or launch anything, we
start with a PRFAQ document.

57
00:02:48,600 --> 00:02:50,430
This consists of a press release

58
00:02:50,430 --> 00:02:53,883
as well as a series of
internal and external FAQs.

59
00:02:54,750 --> 00:02:57,180
To anticipate what
customers are going to ask,

60
00:02:57,180 --> 00:02:59,280
what are the hard questions
they're gonna challenge us

61
00:02:59,280 --> 00:03:01,260
with after we launch?

62
00:03:01,260 --> 00:03:03,600
This helps us work
backwards from the problem

63
00:03:03,600 --> 00:03:06,300
to know exactly how to define
the right product shape

64
00:03:06,300 --> 00:03:07,600
and build the right thing.

65
00:03:08,520 --> 00:03:12,480
To help me make this PRFAQ
document, I have a saved prompt

66
00:03:12,480 --> 00:03:16,083
and a markdown file that I use with QCLI.

67
00:03:17,246 --> 00:03:21,110
In it I've included examples
so these can be good PRFAQs

68
00:03:21,110 --> 00:03:23,010
that have gotten approved in the past.

69
00:03:23,010 --> 00:03:25,923
Some big launches like you
probably heard yesterday,

70
00:03:26,820 --> 00:03:29,490
some context like notes from my meetings

71
00:03:29,490 --> 00:03:31,890
with customers so I
can specifically define

72
00:03:31,890 --> 00:03:34,980
and hone down on what the
real problem statement is,

73
00:03:34,980 --> 00:03:38,670
as well as constraints
like legal guidelines

74
00:03:38,670 --> 00:03:39,780
from our legal department

75
00:03:39,780 --> 00:03:42,183
on messaging do's and don'ts to include.

76
00:03:43,230 --> 00:03:45,450
And the result, this has really helped me

77
00:03:45,450 --> 00:03:47,220
with my personal productivity.

78
00:03:47,220 --> 00:03:49,200
A process that used to take weeks to get

79
00:03:49,200 --> 00:03:52,230
to a good first draft,
to get to a review point.

80
00:03:52,230 --> 00:03:54,780
Now I get a something and minutes

81
00:03:54,780 --> 00:03:58,110
and I get a reviewable
draft in within a few days

82
00:03:58,110 --> 00:04:01,080
that I can review with
stakeholders and my team.

83
00:04:01,080 --> 00:04:03,510
So this has really helped my productivity,

84
00:04:03,510 --> 00:04:06,060
but how do I extend this
capability to my team so

85
00:04:06,060 --> 00:04:08,490
that everyone can benefit from this?

86
00:04:08,490 --> 00:04:10,841
Does it scale If I need
to include hundreds

87
00:04:10,841 --> 00:04:13,773
of documents or multiple
different data sources,

88
00:04:14,790 --> 00:04:18,090
I can't manually add in
all of that context because

89
00:04:18,090 --> 00:04:22,110
as you must have experienced
the context window overflows

90
00:04:22,110 --> 00:04:24,513
and once that happens,
quality tends to degrade.

91
00:04:27,810 --> 00:04:30,480
What we really need is
to give the model access

92
00:04:30,480 --> 00:04:34,020
to massive amounts of data,
but give it the ability to find

93
00:04:34,020 --> 00:04:37,500
and use the only the most
relevant bits of information

94
00:04:37,500 --> 00:04:39,933
so the context window
doesn't get overwhelmed.

95
00:04:41,130 --> 00:04:45,030
RAG a retrieval, augmented
generation is a scalable solution

96
00:04:45,030 --> 00:04:46,680
to this problem.

97
00:04:46,680 --> 00:04:48,877
You retrieve the relevant information,

98
00:04:48,877 --> 00:04:52,890
augment your original prompt
with that relevant context

99
00:04:52,890 --> 00:04:55,620
and use it to generate a better response.

100
00:04:55,620 --> 00:04:58,440
Remember, we're trying to
give these foundational models

101
00:04:58,440 --> 00:05:01,620
access to your evolving
data without manually having

102
00:05:01,620 --> 00:05:03,453
to load everything into the prompt.

103
00:05:06,840 --> 00:05:10,432
So to bolster your prompt with RAG, sorry

104
00:05:10,432 --> 00:05:13,080
to bolster your prompt,
we're going to use RAG,

105
00:05:13,080 --> 00:05:16,500
which uses semantic search to
find and return relevant data

106
00:05:16,500 --> 00:05:18,360
from any size data lake.

107
00:05:18,360 --> 00:05:19,920
Here's how it works.

108
00:05:19,920 --> 00:05:22,860
We start by converting
your data into a vector.

109
00:05:22,860 --> 00:05:24,934
A vector is basically a
numeric representation

110
00:05:24,934 --> 00:05:28,080
of your data that captures its meaning.

111
00:05:28,080 --> 00:05:31,653
Once all of your existing data
is converted into vectors.

112
00:05:32,940 --> 00:05:36,360
Next, when you make a
prompt to your AI model,

113
00:05:36,360 --> 00:05:39,750
that query is also
converted into a vector.

114
00:05:39,750 --> 00:05:42,930
We use spatial similarity
to search against

115
00:05:42,930 --> 00:05:46,080
that query vector against
all of your other vectors

116
00:05:46,080 --> 00:05:48,723
to find similar other content.

117
00:05:49,680 --> 00:05:51,397
We then use those close matches

118
00:05:51,397 --> 00:05:54,060
to augment your original prompt

119
00:05:54,060 --> 00:05:56,820
and then together that
augmented prompt is then fed

120
00:05:56,820 --> 00:06:00,033
into the model to get a better response.

121
00:06:01,050 --> 00:06:02,777
This focused, relevant context instead

122
00:06:02,777 --> 00:06:05,220
of generic knowledge that
the model was trained

123
00:06:05,220 --> 00:06:08,820
upon helps you get more accurate answers

124
00:06:08,820 --> 00:06:11,613
and it's also less likely
for the model to hallucinate.

125
00:06:15,750 --> 00:06:18,810
So before you can start using
your data for semantic search

126
00:06:18,810 --> 00:06:22,192
through RAG, you do need your
data in some type of format

127
00:06:22,192 --> 00:06:26,490
to access to vectorize at
all in the first place.

128
00:06:26,490 --> 00:06:29,880
And most of our customers
use S3 to ingest their data.

129
00:06:29,880 --> 00:06:31,980
That's because S3 is low cost

130
00:06:31,980 --> 00:06:34,023
and scalable to any workload size.

131
00:06:35,010 --> 00:06:38,400
There are two main approaches
to get your data into S3.

132
00:06:38,400 --> 00:06:40,166
One is batch ingestion

133
00:06:40,166 --> 00:06:43,440
and the second is real-time ingestion.

134
00:06:43,440 --> 00:06:44,940
Batch is a useful technique

135
00:06:44,940 --> 00:06:47,340
when your data doesn't change frequently.

136
00:06:47,340 --> 00:06:49,260
So think of documentation,

137
00:06:49,260 --> 00:06:52,680
historical records, product catalogs.

138
00:06:52,680 --> 00:06:54,780
They're not evolving all the time.

139
00:06:54,780 --> 00:06:56,610
Another good reason to use batch is

140
00:06:56,610 --> 00:06:58,920
if you need to pre-process your data.

141
00:06:58,920 --> 00:07:00,930
You can imagine if you need to pre-process

142
00:07:00,930 --> 00:07:04,080
like chunking or to generate embeddings,

143
00:07:04,080 --> 00:07:07,473
this isn't something you
can do scalably real time.

144
00:07:08,640 --> 00:07:11,310
The second technique
is real time ingestion.

145
00:07:11,310 --> 00:07:12,615
Conversely, this is a good tool

146
00:07:12,615 --> 00:07:15,600
when your data is changing frequently.

147
00:07:15,600 --> 00:07:18,390
So for example, your
live social media feeds

148
00:07:18,390 --> 00:07:22,980
or live transcripts from
customer support calls.

149
00:07:22,980 --> 00:07:25,008
You don't wanna have a dated pulse

150
00:07:25,008 --> 00:07:26,850
on what your customers are saying.

151
00:07:26,850 --> 00:07:30,240
So you can't use batch
operation from a week ago

152
00:07:30,240 --> 00:07:33,273
or every month that's
gonna have stale data.

153
00:07:34,680 --> 00:07:37,852
You can use Amazon Kinesis
or SQS as a simple way

154
00:07:37,852 --> 00:07:40,740
to ingest real-time streaming data.

155
00:07:40,740 --> 00:07:44,430
It collects, processes and
loads the data into S3.

156
00:07:44,430 --> 00:07:47,553
And there are also several
ways to do batch ingestion.

157
00:07:48,840 --> 00:07:52,170
For RAG workflows, usually
batch is what customers choose

158
00:07:52,170 --> 00:07:54,723
because it's simpler
and more cost effective.

159
00:07:58,080 --> 00:08:00,330
Okay, so now we have our data into S3

160
00:08:00,330 --> 00:08:02,220
and we want to vectorize
it so we can use it

161
00:08:02,220 --> 00:08:03,903
for semantic search with RAG.

162
00:08:04,860 --> 00:08:06,330
What are vectors?

163
00:08:06,330 --> 00:08:08,280
Vectors are basically that mechanism

164
00:08:08,280 --> 00:08:11,670
that makes RAG more powerful
through semantic search.

165
00:08:11,670 --> 00:08:14,160
You can vectorize any type of data.

166
00:08:14,160 --> 00:08:16,863
For this example, let's do text documents.

167
00:08:17,850 --> 00:08:20,610
So you have your documents
and they're then converted

168
00:08:20,610 --> 00:08:23,250
into chunks, which is
basically a finite set

169
00:08:23,250 --> 00:08:25,710
of characters of text in this case.

170
00:08:25,710 --> 00:08:26,820
Those chunks then go

171
00:08:26,820 --> 00:08:31,290
through an embedding model
and some to generate vectors

172
00:08:31,290 --> 00:08:33,810
and some embedding models
also attach metadata

173
00:08:33,810 --> 00:08:36,780
to help refine your search to the vector.

174
00:08:36,780 --> 00:08:39,189
These then embeddings
and metadata are stored

175
00:08:39,189 --> 00:08:40,893
in a vector database.

176
00:08:41,730 --> 00:08:45,480
Remember these vectors now
capture your evolving data so

177
00:08:45,480 --> 00:08:47,913
that we can use it later
to augment your query.

178
00:08:49,380 --> 00:08:52,280
There's several choices you
need to make in this pipeline.

179
00:08:53,520 --> 00:08:55,410
One, you need to choose your data source.

180
00:08:55,410 --> 00:08:57,430
So this could be an S3 bucket or a prefix

181
00:08:57,430 --> 00:08:59,760
if you want more granularity.

182
00:08:59,760 --> 00:09:02,130
You also need to choose
your chunking strategy.

183
00:09:02,130 --> 00:09:04,531
So imagine you're a film studio

184
00:09:04,531 --> 00:09:06,330
and instead of text documents

185
00:09:06,330 --> 00:09:08,910
like we're doing here,
your data is movies.

186
00:09:08,910 --> 00:09:11,280
You may choose to chunk by act or by scene

187
00:09:11,280 --> 00:09:13,653
or some more granular timeframe.

188
00:09:14,700 --> 00:09:16,800
Then you need to choose
your embeddings model.

189
00:09:16,800 --> 00:09:19,393
And then finally, your vector store.

190
00:09:19,393 --> 00:09:21,633
You can manage this pipeline on your own,

191
00:09:23,100 --> 00:09:25,500
but we also have Bedrock Knowledge Bases,

192
00:09:25,500 --> 00:09:27,420
which can manage it for you.

193
00:09:27,420 --> 00:09:30,840
It has a simple way to
configure this entire pipeline.

194
00:09:30,840 --> 00:09:33,041
And the nice thing is when
you're uploading new documents,

195
00:09:33,041 --> 00:09:35,950
those documents are
already ingested as vectors

196
00:09:38,370 --> 00:09:40,270
and they land in your vector database.

197
00:09:43,110 --> 00:09:46,500
So now vectors are essential for RAG,

198
00:09:46,500 --> 00:09:47,910
but managing large volumes

199
00:09:47,910 --> 00:09:51,540
of vectors can be
challenging and expensive.

200
00:09:51,540 --> 00:09:54,240
Over the last years we've
heard three main problems

201
00:09:54,240 --> 00:09:55,680
from customers.

202
00:09:55,680 --> 00:09:58,786
First is cost, many
traditional vector databases,

203
00:09:58,786 --> 00:10:02,080
bundle storage, memory and queries

204
00:10:03,457 --> 00:10:07,530
as a single together
making it cost prohibitive

205
00:10:07,530 --> 00:10:10,110
to deploy large vector sets.

206
00:10:10,110 --> 00:10:12,990
We've also heard that
scalability is a challenge,

207
00:10:12,990 --> 00:10:15,911
so it's difficult to
scale from small proof

208
00:10:15,911 --> 00:10:20,820
of concepts again to those
large production data sets.

209
00:10:20,820 --> 00:10:23,100
And then finally, granularity.

210
00:10:23,100 --> 00:10:24,929
We've heard that many
customers need millions

211
00:10:24,929 --> 00:10:28,350
of separate indexes for
multi-tenant applications,

212
00:10:28,350 --> 00:10:30,900
and once you start to get
to that scale, costs tend

213
00:10:30,900 --> 00:10:32,223
to spiral out of control.

214
00:10:33,330 --> 00:10:35,370
The real theme that we've heard across all

215
00:10:35,370 --> 00:10:36,960
of these was cost.

216
00:10:36,960 --> 00:10:39,360
Customers needed a more effective way

217
00:10:39,360 --> 00:10:41,283
to store and manage vectors.

218
00:10:44,220 --> 00:10:46,554
That's why this week we
launched general availability

219
00:10:46,554 --> 00:10:49,230
for Amazon S3 vectors.

220
00:10:49,230 --> 00:10:50,730
We're really proud of this launch.

221
00:10:50,730 --> 00:10:53,458
It's the first cloud object
store with native support

222
00:10:53,458 --> 00:10:56,223
for vector store and query.

223
00:10:57,060 --> 00:11:00,273
And it has completely
transformed the economics of AI.

224
00:11:01,230 --> 00:11:04,046
S3 vectors offers up to 90% lower costs

225
00:11:04,046 --> 00:11:06,993
for uploading, storing
and querying vectors.

226
00:11:08,040 --> 00:11:10,759
We offer 100 millisecond
warm query latency

227
00:11:10,759 --> 00:11:13,170
because we're able to cache queries

228
00:11:13,170 --> 00:11:17,580
that are frequently made and
sub-second cold query latency.

229
00:11:17,580 --> 00:11:20,970
You can store up to 2
billion vectors per index

230
00:11:20,970 --> 00:11:23,970
and up to 10,000 indexes per bucket.

231
00:11:23,970 --> 00:11:28,200
That's over 20 trillion
vectors in a vector bucket.

232
00:11:28,200 --> 00:11:30,600
And of those you can have 10,000.

233
00:11:30,600 --> 00:11:34,323
We also offer fast ingestion
so you can get to work quickly.

234
00:11:35,520 --> 00:11:37,524
You only pay for what you use.

235
00:11:37,524 --> 00:11:40,620
And the best thing is this
is built on top of S3.

236
00:11:40,620 --> 00:11:41,860
So customers get access

237
00:11:41,860 --> 00:11:45,690
to the attributes that
they know and love about S3

238
00:11:45,690 --> 00:11:49,983
like availability, durability,
security and compliance.

239
00:11:54,300 --> 00:11:56,490
Vector pricing is fundamentally different

240
00:11:56,490 --> 00:11:58,710
from traditional vector databases.

241
00:11:58,710 --> 00:12:02,280
Like I said, most of them bundle
compute, memory and storage

242
00:12:02,280 --> 00:12:03,879
and you pay for it all together.

243
00:12:03,879 --> 00:12:05,312
You provision capacity upfront

244
00:12:05,312 --> 00:12:07,383
and then pay around the clock.

245
00:12:08,280 --> 00:12:10,309
The way S3 vectors has
changed that completely

246
00:12:10,309 --> 00:12:14,280
is we've introduced three pay
per use pricing components

247
00:12:14,280 --> 00:12:17,160
that actually align with your usage.

248
00:12:17,160 --> 00:12:20,490
So first you pay for ingestion,
you pay for the vectors

249
00:12:20,490 --> 00:12:23,400
that you're putting into
the vector database.

250
00:12:23,400 --> 00:12:25,440
The second is storage.

251
00:12:25,440 --> 00:12:27,988
You can leverage S three's,
industry leading economics

252
00:12:27,988 --> 00:12:31,350
to store vectors at a
fraction of the cost.

253
00:12:31,350 --> 00:12:33,600
And finally queries
you pay for the queries

254
00:12:33,600 --> 00:12:35,460
that you're actually making.

255
00:12:35,460 --> 00:12:37,320
There's no capacity planning necessary

256
00:12:37,320 --> 00:12:39,570
and no infrastructure overhead.

257
00:12:39,570 --> 00:12:40,590
And this is really useful

258
00:12:40,590 --> 00:12:43,470
when your application
has varying workloads.

259
00:12:43,470 --> 00:12:45,240
You can imagine during the workday

260
00:12:45,240 --> 00:12:48,060
maybe your team is making lots of queries,

261
00:12:48,060 --> 00:12:51,690
but at night there's hardly
any out of business hours.

262
00:12:51,690 --> 00:12:53,010
This way you're only paying for

263
00:12:53,010 --> 00:12:55,160
what you're using and
when you're using it.

264
00:12:59,850 --> 00:13:01,341
Here's a customer example.

265
00:13:01,341 --> 00:13:04,020
We launched public preview
of vectors this summer

266
00:13:04,020 --> 00:13:05,817
and this customer has
been with us since then

267
00:13:05,817 --> 00:13:07,311
and we've gotten to really work with them

268
00:13:07,311 --> 00:13:09,780
and seen how they've scaled.

269
00:13:09,780 --> 00:13:12,540
This is a biotech firm and
they've been using S3 vectors

270
00:13:12,540 --> 00:13:15,690
for semantic search on
scientific literature.

271
00:13:15,690 --> 00:13:19,650
Their team consists of PhD
scientists and entrepreneurs

272
00:13:19,650 --> 00:13:22,470
and really their goal is to
discover the next breakthrough

273
00:13:22,470 --> 00:13:23,583
in drug development.

274
00:13:24,840 --> 00:13:26,430
To do so, they need to know

275
00:13:26,430 --> 00:13:29,340
and absorb the large corpus
of scientific literature

276
00:13:29,340 --> 00:13:31,710
and knowledge that already exists.

277
00:13:31,710 --> 00:13:32,850
And this isn't just reading

278
00:13:32,850 --> 00:13:35,400
a few papers every morning with breakfast.

279
00:13:35,400 --> 00:13:39,093
The scope of this problem is
30 million scientific papers.

280
00:13:40,110 --> 00:13:41,280
So before they integrated

281
00:13:41,280 --> 00:13:45,480
with S3 vectors, this research
phase would take weeks.

282
00:13:45,480 --> 00:13:47,295
And still you can
imagine it's not possible

283
00:13:47,295 --> 00:13:49,533
to absorb all of this knowledge.

284
00:13:50,670 --> 00:13:52,503
Since integrating with S3 vectors,

285
00:13:52,503 --> 00:13:55,290
this is what their pipeline looks like.

286
00:13:55,290 --> 00:13:56,922
They've ingested the entire corpus

287
00:13:56,922 --> 00:14:00,390
of scientific literature
that they had access to.

288
00:14:00,390 --> 00:14:01,752
Those were then generated into millions

289
00:14:01,752 --> 00:14:06,752
of vector embeddings, which
now have landed in S3 vectors.

290
00:14:06,780 --> 00:14:09,394
Now when they're exploring
a hypothesis, it's as simple

291
00:14:09,394 --> 00:14:13,500
as performing semantic
search on these vectors

292
00:14:13,500 --> 00:14:16,203
to understand what is
nearby and what is relevant.

293
00:14:17,100 --> 00:14:19,803
This has dramatically reduced
their research timeline.

294
00:14:23,850 --> 00:14:26,970
Okay, so let's recap
what we've done so far.

295
00:14:26,970 --> 00:14:29,730
We have all of our vectorized data in S3

296
00:14:29,730 --> 00:14:31,830
and we're performing RAG workflows

297
00:14:31,830 --> 00:14:34,530
on it to get more relevant context.

298
00:14:34,530 --> 00:14:37,050
But here's the thing, when
you perform a RAG workflow

299
00:14:37,050 --> 00:14:40,593
on this data, it's searching
against all of your data.

300
00:14:41,850 --> 00:14:44,973
We can make RAGs even smarter
with metadata filtering.

301
00:14:46,650 --> 00:14:49,410
Metadata filtering helps
to narrow the search space

302
00:14:49,410 --> 00:14:51,237
so your queries are faster, more accurate,

303
00:14:51,237 --> 00:14:54,990
and more targeted to what you're
actually trying to achieve.

304
00:14:54,990 --> 00:14:58,650
So for example, instead of
those 30 million documents,

305
00:14:58,650 --> 00:14:59,896
this form could choose

306
00:14:59,896 --> 00:15:03,063
to select only once about gene synthesis.

307
00:15:05,040 --> 00:15:07,920
You can use glue to
catalog structured data

308
00:15:07,920 --> 00:15:11,223
and you can add custom
metadata for unstructured data.

309
00:15:12,090 --> 00:15:14,130
You can use metadata
filtering then directly

310
00:15:14,130 --> 00:15:16,020
in line in your RAG workflow.

311
00:15:16,020 --> 00:15:17,940
It's like adding wear
clauses to your search

312
00:15:17,940 --> 00:15:19,833
to make it faster and more effective.

313
00:15:20,880 --> 00:15:23,070
And here's where it gets really powerful.

314
00:15:23,070 --> 00:15:25,140
As you continue to process your data,

315
00:15:25,140 --> 00:15:27,780
more metadata gets generated

316
00:15:27,780 --> 00:15:30,240
and this rich metadata
becomes the nervous system

317
00:15:30,240 --> 00:15:32,163
of your entire AI operation.

318
00:15:33,390 --> 00:15:34,961
Metadata gives you context

319
00:15:34,961 --> 00:15:37,170
like what is the data I'm looking at?

320
00:15:37,170 --> 00:15:39,420
This is Q4 sales data.

321
00:15:39,420 --> 00:15:42,390
It also gives you lineage like
where the data actually came

322
00:15:42,390 --> 00:15:44,460
from and how it's been transformed

323
00:15:44,460 --> 00:15:47,580
and this has been really
useful for root causing issues.

324
00:15:47,580 --> 00:15:50,010
And finally, it can
give you classification

325
00:15:50,010 --> 00:15:52,600
like it can auto tag sensitive data such

326
00:15:52,600 --> 00:15:55,170
as personally identifiable information so

327
00:15:55,170 --> 00:15:56,613
that it's managed correctly.

328
00:15:57,450 --> 00:16:00,930
Metadata helps AI understand
not only what your data is,

329
00:16:00,930 --> 00:16:03,180
but what it actually
means for your business.

330
00:16:06,960 --> 00:16:09,660
So far we've built a complete RAG system.

331
00:16:09,660 --> 00:16:12,630
We've ingested data into
S3, we've vectorized it

332
00:16:12,630 --> 00:16:15,330
for semantic search, we've empowered it

333
00:16:15,330 --> 00:16:18,060
with metadata filtering
to make it more effective.

334
00:16:18,060 --> 00:16:21,093
And we've introduced a cost
effective vector storage.

335
00:16:22,350 --> 00:16:24,000
Many customers choose to stop here

336
00:16:24,000 --> 00:16:27,360
because this is already
a very powerful AI engine

337
00:16:27,360 --> 00:16:29,133
and it delivers results at scale.

338
00:16:30,450 --> 00:16:32,160
But what if you want more?

339
00:16:32,160 --> 00:16:34,535
What if you instead of
a single question like

340
00:16:34,535 --> 00:16:37,493
what are similar scientific documents

341
00:16:37,493 --> 00:16:40,230
to this problem I'm exploring?

342
00:16:40,230 --> 00:16:41,340
What if now you want

343
00:16:41,340 --> 00:16:44,027
to break down a complex
task, give it power

344
00:16:44,027 --> 00:16:46,870
with tools and access to other datas

345
00:16:47,730 --> 00:16:50,330
and then have it reasoned
through a complex problem?

346
00:16:51,210 --> 00:16:54,405
Earlier I introduced prompt
engineering with an example

347
00:16:54,405 --> 00:16:56,998
on how I've used it to
improve my productivity

348
00:16:56,998 --> 00:16:59,133
with PRFAQ writing.

349
00:17:00,630 --> 00:17:03,630
We are now entering roadmap
season for next year

350
00:17:03,630 --> 00:17:06,330
and it's been great to
have many conversations

351
00:17:06,330 --> 00:17:08,070
with customers this week at re:Invent

352
00:17:08,070 --> 00:17:09,150
as well as over the last year

353
00:17:09,150 --> 00:17:11,793
to help us define what
to build for next year.

354
00:17:12,840 --> 00:17:16,222
Can I make a roadmap agent
using all of this information

355
00:17:16,222 --> 00:17:18,870
and using several different tools?

356
00:17:18,870 --> 00:17:21,206
What if I want customer
feedback from my notes

357
00:17:21,206 --> 00:17:25,860
from this week as well as
product reviews we have online?

358
00:17:25,860 --> 00:17:30,240
I want to check our S3 tables
for customer usage data

359
00:17:30,240 --> 00:17:33,900
to find top customers
and I wanna combine those

360
00:17:33,900 --> 00:17:37,200
to find the top pain points
that customers are discussing.

361
00:17:37,200 --> 00:17:41,400
And I wanna write a PRFAQ
for each of those as a pitch

362
00:17:41,400 --> 00:17:43,530
for a new feature that
we should launch for each

363
00:17:43,530 --> 00:17:44,673
of those pain points.

364
00:17:46,020 --> 00:17:48,600
That requires the LLM to now reason

365
00:17:48,600 --> 00:17:51,041
through a complex task,
use multiple data sources

366
00:17:51,041 --> 00:17:55,110
and tools and perform those actions.

367
00:17:55,110 --> 00:17:58,743
RAG isn't gonna cut it
anymore. Now I need agents.

368
00:18:02,730 --> 00:18:06,990
With agents, you give your
LLMA set of tools upfront,

369
00:18:06,990 --> 00:18:10,440
things like Bedrock Knowledge
Bases for access to documents.

370
00:18:10,440 --> 00:18:14,123
S3 tables for access to
usage statistics, APIs

371
00:18:14,123 --> 00:18:17,103
to pull customer feedback
from social media.

372
00:18:18,000 --> 00:18:19,740
And here's the big shift,

373
00:18:19,740 --> 00:18:22,050
before this, with prompt
engineering, we had

374
00:18:22,050 --> 00:18:24,720
to guide the model step-by-step.

375
00:18:24,720 --> 00:18:27,870
Now agents figure out
the steps, figure out

376
00:18:27,870 --> 00:18:30,820
what data is needed and what
are the right tools to get it?

377
00:18:31,680 --> 00:18:33,360
So if we look at this diagram,

378
00:18:33,360 --> 00:18:34,620
we were already prompting

379
00:18:34,620 --> 00:18:37,680
the model with goals and
instructions and context.

380
00:18:37,680 --> 00:18:39,513
Tools are the missing piece here.

381
00:18:41,010 --> 00:18:42,690
This is all fed to the agent

382
00:18:42,690 --> 00:18:47,043
who's using this foundational
model to perform actions.

383
00:18:48,150 --> 00:18:50,820
Managing all of these tool
integrations can start

384
00:18:50,820 --> 00:18:53,070
to get very complex.

385
00:18:53,070 --> 00:18:54,843
This is where MCP comes in.

386
00:18:57,960 --> 00:19:01,037
MCP or model context protocol
is becoming the standard

387
00:19:01,037 --> 00:19:04,290
for how tools connect with agents.

388
00:19:04,290 --> 00:19:07,020
It's really similar to
how HTTP standardized

389
00:19:07,020 --> 00:19:09,870
how applications can talk to backends.

390
00:19:09,870 --> 00:19:14,310
There are two parts to this.
MCP clients and MCP servers.

391
00:19:14,310 --> 00:19:16,157
MCP clients define the how.

392
00:19:16,157 --> 00:19:18,480
How do you query a database?

393
00:19:18,480 --> 00:19:21,270
How do you search
through a knowledge base?

394
00:19:21,270 --> 00:19:24,450
And MCP servers actually execute on that.

395
00:19:24,450 --> 00:19:26,940
They check those
constraints and those rules

396
00:19:26,940 --> 00:19:28,710
and then actually execute that query

397
00:19:28,710 --> 00:19:32,313
and tailor the results based
on how you've configured it.

398
00:19:33,480 --> 00:19:37,310
AWS is building several MCP
servers for our services as well

399
00:19:37,310 --> 00:19:41,130
as external services to help
your agents find the right data

400
00:19:41,130 --> 00:19:42,330
through the right tools.

401
00:19:45,750 --> 00:19:47,910
For example, we have an MCP server

402
00:19:47,910 --> 00:19:50,010
for Bedrock Knowledge Bases.

403
00:19:50,010 --> 00:19:51,450
Now you can query knowledge bases

404
00:19:51,450 --> 00:19:54,573
with natural language
no API calls necessary.

405
00:19:55,410 --> 00:19:56,809
With this you can filter

406
00:19:56,809 --> 00:19:59,820
to target specific sections
of the knowledge base.

407
00:19:59,820 --> 00:20:02,372
It can configure the result
size and re-rank outputs

408
00:20:02,372 --> 00:20:05,313
to improve relevance to
what you're searching for.

409
00:20:06,510 --> 00:20:09,270
You can also do this conversationally.

410
00:20:09,270 --> 00:20:12,630
So now my road mapping
agent can ask, okay,

411
00:20:12,630 --> 00:20:15,100
what are the key limitations
that customers are reporting

412
00:20:15,100 --> 00:20:18,633
and our product reviews
and it's all configurable.

413
00:20:22,680 --> 00:20:25,350
So in addition to RAG
for semantic search, we

414
00:20:25,350 --> 00:20:29,370
also need traditional ways
to filter and find data.

415
00:20:29,370 --> 00:20:31,230
That's where metadata comes in.

416
00:20:31,230 --> 00:20:34,830
You can generate and search
metadata with S3 metadata

417
00:20:34,830 --> 00:20:37,680
and this works for both
structured and unstructured data.

418
00:20:38,760 --> 00:20:40,230
So here's how it works.

419
00:20:40,230 --> 00:20:42,900
Let's say you have lots of types of data

420
00:20:42,900 --> 00:20:47,280
like PDFs, CSVs, audio files,
video files, you name it.

421
00:20:47,280 --> 00:20:49,380
And they're all landing in your S3 bucket.

422
00:20:51,390 --> 00:20:53,970
First you wanna configure
your source bucket.

423
00:20:53,970 --> 00:20:56,703
You wanna configure S3
metadata on your source bucket.

424
00:20:58,290 --> 00:21:01,500
Second, you wanna create
an S3 table bucket.

425
00:21:01,500 --> 00:21:05,070
This is where a queryable
metadata table will live.

426
00:21:05,070 --> 00:21:07,710
You can do both of these
through a single API call

427
00:21:07,710 --> 00:21:10,083
or just a few clicks in the S3 console.

428
00:21:11,670 --> 00:21:14,880
Finally, S3 will generate
a metadata table in

429
00:21:14,880 --> 00:21:16,230
that table bucket

430
00:21:16,230 --> 00:21:18,720
and it automatically
updates every few minutes

431
00:21:18,720 --> 00:21:20,193
as your data is evolving.

432
00:21:21,960 --> 00:21:23,760
It auto updates for new objects

433
00:21:23,760 --> 00:21:26,850
that enter, it auto tracks system metadata

434
00:21:26,850 --> 00:21:29,643
as well as you can configure
your own custom metadata.

435
00:21:30,510 --> 00:21:33,354
RAG workflows can leverage this
data for metadata filtering

436
00:21:33,354 --> 00:21:36,570
for more optimized and targeted searches

437
00:21:36,570 --> 00:21:38,640
and you can also query it on your own.

438
00:21:38,640 --> 00:21:40,290
For your own use cases.

439
00:21:40,290 --> 00:21:42,540
You can use Athena, Quick Suite, Spark

440
00:21:42,540 --> 00:21:44,445
or any SQL based process

441
00:21:44,445 --> 00:21:47,553
to gain valuable insights
from your metadata.

442
00:21:48,480 --> 00:21:51,450
So this is very useful
for RAG and for analytics,

443
00:21:51,450 --> 00:21:53,283
but it's also useful for agents.

444
00:21:56,100 --> 00:21:59,280
Now you have this rich cataloged metadata

445
00:21:59,280 --> 00:22:02,340
and your agents can also access this.

446
00:22:02,340 --> 00:22:06,660
This year we also launched MCP
server for Amazon S3 tables.

447
00:22:06,660 --> 00:22:09,090
So now you can use natural
language to interact

448
00:22:09,090 --> 00:22:14,090
with S3 tables and S3 metadata,
no SQL required anymore.

449
00:22:15,120 --> 00:22:17,790
So coming back to my
roadmap agent, it can ask,

450
00:22:17,790 --> 00:22:20,160
it can check the table of customer usage

451
00:22:20,160 --> 00:22:22,440
and see who are the top 10 customers

452
00:22:22,440 --> 00:22:24,570
for a given problem I'm solving

453
00:22:24,570 --> 00:22:26,770
and what does their
monthly usage look like?

454
00:22:27,690 --> 00:22:30,880
This is all available in
the AWS MCP open source

455
00:22:30,880 --> 00:22:34,323
repository and it's really easy to set up.

456
00:22:40,290 --> 00:22:42,900
Okay, let's bring this all together.

457
00:22:42,900 --> 00:22:46,247
We started with prompt engineering,
giving your LLM examples

458
00:22:46,247 --> 00:22:49,743
and constraints and context
to get better responses.

459
00:22:51,030 --> 00:22:54,180
Then we added RAG giving your LLM access

460
00:22:54,180 --> 00:22:56,460
to more relevant data to fill that context

461
00:22:56,460 --> 00:22:58,860
and augment your original prompt.

462
00:22:58,860 --> 00:23:01,590
We vectorized our data to support this.

463
00:23:01,590 --> 00:23:04,890
We discussed metadata filtering
to target the RAG search

464
00:23:04,890 --> 00:23:07,934
and we also talked about S3
vectors as a cost effective way

465
00:23:07,934 --> 00:23:09,873
to manage your vector storage.

466
00:23:12,270 --> 00:23:16,020
Then we leveled up to agents,
we gave our LLM the access

467
00:23:16,020 --> 00:23:19,683
to connect with multiple tools
to perform complex tasks.

468
00:23:22,380 --> 00:23:24,967
And finally we talked
about MCP, the standard

469
00:23:24,967 --> 00:23:28,053
that makes it easy for your
agents to connect to tools.

470
00:23:28,950 --> 00:23:31,380
This here is the modern AI stack

471
00:23:31,380 --> 00:23:32,940
and it's where many customers stop

472
00:23:32,940 --> 00:23:35,463
because it's really
powerful and it's scalable.

473
00:23:36,540 --> 00:23:38,343
Some customers want to go further.

474
00:23:41,290 --> 00:23:43,680
To discuss more advanced workloads,

475
00:23:43,680 --> 00:23:46,323
I'm gonna invite Jordan
to the stage, thank you.

476
00:23:50,524 --> 00:23:53,910
(audience clapping)

477
00:23:53,910 --> 00:23:56,220
- Alright, so Monica talked

478
00:23:56,220 --> 00:23:59,010
a lot about how you can
use prompt engineering

479
00:23:59,010 --> 00:24:01,860
to provide better structure and RAG

480
00:24:01,860 --> 00:24:05,253
to provide more data to improve
the outputs of your model.

481
00:24:06,706 --> 00:24:10,350
Now this works in many cases,
however, a lot of that data

482
00:24:10,350 --> 00:24:12,480
and structure we just talked about lives

483
00:24:12,480 --> 00:24:14,490
in the context window of your application.

484
00:24:14,490 --> 00:24:17,490
That's kind of like the short
term memory of the model.

485
00:24:17,490 --> 00:24:21,210
And because that's fixed
sometimes you have to go further

486
00:24:21,210 --> 00:24:23,640
and actually embeds some of that content

487
00:24:23,640 --> 00:24:25,230
in the model itself rather

488
00:24:25,230 --> 00:24:27,600
than using something off the shelf.

489
00:24:27,600 --> 00:24:30,510
So let's think about how models work today

490
00:24:30,510 --> 00:24:31,610
and how they're built.

491
00:24:32,550 --> 00:24:35,280
Every model that that we have access to

492
00:24:35,280 --> 00:24:37,230
that's already preexisting,
has been trained

493
00:24:37,230 --> 00:24:40,410
on a large data set and
the knowledge that was used

494
00:24:40,410 --> 00:24:42,810
for training is now deeply embedded

495
00:24:42,810 --> 00:24:45,330
within the weights of the model itself.

496
00:24:45,330 --> 00:24:48,540
That data is accessible
when you introduce a prompt

497
00:24:48,540 --> 00:24:50,430
to the model and get a response,

498
00:24:50,430 --> 00:24:53,310
but if the knowledge you're
looking for isn't available,

499
00:24:53,310 --> 00:24:56,760
if the structure doesn't
match your application,

500
00:24:56,760 --> 00:24:59,640
then we actually need to
try to tweak those weights,

501
00:24:59,640 --> 00:25:02,440
update the model itself to
get the right kind of output.

502
00:25:03,840 --> 00:25:06,000
Now of course that takes data

503
00:25:06,000 --> 00:25:08,520
and depending on how much data you have,

504
00:25:08,520 --> 00:25:09,690
there's different techniques

505
00:25:09,690 --> 00:25:13,380
that you'll be able to use
in order to actually train

506
00:25:13,380 --> 00:25:17,130
and improve the actual
outputs of your model.

507
00:25:17,130 --> 00:25:18,900
If you have a small amount

508
00:25:18,900 --> 00:25:21,960
of data, rather than trying
to update the weights

509
00:25:21,960 --> 00:25:24,660
across the whole model,
we really wanna focus

510
00:25:24,660 --> 00:25:28,620
on just updating portions
of the model itself.

511
00:25:28,620 --> 00:25:30,483
And if you're really gonna
get a big bang for your buck

512
00:25:30,483 --> 00:25:33,660
with your data, you wanna
provide even more guidance

513
00:25:33,660 --> 00:25:35,220
by labeling the data.

514
00:25:35,220 --> 00:25:37,830
So it's very clear you have an input

515
00:25:37,830 --> 00:25:40,020
and a desired output
that you're looking for.

516
00:25:40,020 --> 00:25:42,120
You're trying to guide the model

517
00:25:42,120 --> 00:25:43,770
to produce certain outputs based on

518
00:25:43,770 --> 00:25:45,750
that small labeled data set.

519
00:25:45,750 --> 00:25:46,770
And you're gonna focus it

520
00:25:46,770 --> 00:25:49,650
on a small portion, a subset of the model.

521
00:25:49,650 --> 00:25:53,220
If you have more data, then
you can actually still with

522
00:25:53,220 --> 00:25:55,642
that labeling to provide
that guidance, expand

523
00:25:55,642 --> 00:25:59,490
and actually update more
of the model weights.

524
00:25:59,490 --> 00:26:00,540
That's kind of like, rather

525
00:26:00,540 --> 00:26:03,223
than teaching the model a
pointed piece of information,

526
00:26:03,223 --> 00:26:07,050
it's like helping it think in your domain

527
00:26:07,050 --> 00:26:10,110
that you're working with that's
maybe not already embedded

528
00:26:10,110 --> 00:26:11,670
in the model itself.

529
00:26:11,670 --> 00:26:14,100
And then if you have
a huge volume of data,

530
00:26:14,100 --> 00:26:17,790
you can actually skip
that kind of labeling step

531
00:26:17,790 --> 00:26:19,110
and go straight to something

532
00:26:19,110 --> 00:26:21,660
that's more akin to
continuous pre-training.

533
00:26:21,660 --> 00:26:23,280
It's kind of like picking up

534
00:26:23,280 --> 00:26:26,130
where the original model
developers left off

535
00:26:26,130 --> 00:26:29,253
and then embedding that new
content in those model weights.

536
00:26:32,220 --> 00:26:34,807
Now the services that we
have available to help

537
00:26:34,807 --> 00:26:39,807
with this effort are Amazon
Bedrock and Amazon SageMaker AI.

538
00:26:40,379 --> 00:26:43,650
If you find yourself mostly
on the inference side,

539
00:26:43,650 --> 00:26:46,530
the application builder
side of the equation,

540
00:26:46,530 --> 00:26:49,680
Bedrock is really where you'll
probably want to operate.

541
00:26:49,680 --> 00:26:50,880
And if you think of yourself more

542
00:26:50,880 --> 00:26:53,790
as a model build, model
builder, model developer,

543
00:26:53,790 --> 00:26:56,790
then SageMaker AI is is the place to go.

544
00:26:56,790 --> 00:27:00,030
In both of these cases,
you have capabilities

545
00:27:00,030 --> 00:27:01,530
to do what we're gonna talk about now,

546
00:27:01,530 --> 00:27:03,930
which is updating the model itself.

547
00:27:03,930 --> 00:27:05,937
But each of these services
has really been refined

548
00:27:05,937 --> 00:27:10,533
or tuned for specific
types of user experiences.

549
00:27:12,120 --> 00:27:15,210
So let's go into some of the
different techniques we have

550
00:27:15,210 --> 00:27:18,060
to actually update the model itself.

551
00:27:18,060 --> 00:27:19,983
In this case with that labeled data.

552
00:27:23,160 --> 00:27:25,740
We'll start with three techniques,

553
00:27:25,740 --> 00:27:29,790
supervised fine tuning,
distillation and alignment.

554
00:27:29,790 --> 00:27:31,740
And I know that there's
a lot of these words

555
00:27:31,740 --> 00:27:33,630
that get thrown around in this ML space.

556
00:27:33,630 --> 00:27:35,992
So one of the goals I have
today is to try to walk through

557
00:27:35,992 --> 00:27:38,760
and make sure that everyone
has a clear understanding

558
00:27:38,760 --> 00:27:40,080
of how each of these differs

559
00:27:40,080 --> 00:27:42,903
and what it means for data,
what it means for storage.

560
00:27:45,600 --> 00:27:48,270
Okay, so supervised, fine tuning.

561
00:27:48,270 --> 00:27:50,580
Again, I mentioned before, labeled data

562
00:27:50,580 --> 00:27:52,440
is really helpful when you're trying

563
00:27:52,440 --> 00:27:54,720
to tell the model, here's an input,

564
00:27:54,720 --> 00:27:57,060
here's the output I'd like to see.

565
00:27:57,060 --> 00:27:59,610
So this is kind of in that same vein,

566
00:27:59,610 --> 00:28:01,110
what is the input going to be?

567
00:28:01,110 --> 00:28:02,610
What's the user saying?

568
00:28:02,610 --> 00:28:04,710
What other context do we have available

569
00:28:04,710 --> 00:28:06,270
as part of this exchange?

570
00:28:06,270 --> 00:28:08,820
And then what's the single
best response or output?

571
00:28:11,040 --> 00:28:15,000
Simple model maybe used for
something like translation

572
00:28:15,000 --> 00:28:19,920
or summarization, that's
text to text, text input,

573
00:28:19,920 --> 00:28:23,790
text output, might have label
data that looks like this.

574
00:28:23,790 --> 00:28:26,010
Just a very simple, here's the prompt

575
00:28:26,010 --> 00:28:28,353
and here's the desired
output I'd like to see.

576
00:28:29,430 --> 00:28:31,743
This is a non-conversation model.

577
00:28:34,320 --> 00:28:37,320
If you have some more
conversational models,

578
00:28:37,320 --> 00:28:38,153
you know this is more of

579
00:28:38,153 --> 00:28:40,260
like the chat bot type of experience.

580
00:28:40,260 --> 00:28:43,290
You'll notice that the, the
labeled data is actually going

581
00:28:43,290 --> 00:28:46,500
to need to have tags for the user,

582
00:28:46,500 --> 00:28:47,730
the assistant kind of like

583
00:28:47,730 --> 00:28:51,270
who's actually introducing
the prompt, who's responding.

584
00:28:51,270 --> 00:28:54,090
And some of that labeled data
can also have multiple turns.

585
00:28:54,090 --> 00:28:56,790
It can go back and forth between the agent

586
00:28:56,790 --> 00:29:00,090
and the user as part of
that labeled dataset.

587
00:29:00,090 --> 00:29:02,370
Now I should say the
reason why I'm putting all

588
00:29:02,370 --> 00:29:04,230
this up here on the slide is

589
00:29:04,230 --> 00:29:06,270
because when you're
thinking about the data

590
00:29:06,270 --> 00:29:07,980
that you're generating and collecting

591
00:29:07,980 --> 00:29:10,710
across your organizations, thinking ahead

592
00:29:10,710 --> 00:29:13,590
to how this data might be
used can be really helpful,

593
00:29:13,590 --> 00:29:17,550
especially if you aren't
expecting to be generating,

594
00:29:17,550 --> 00:29:20,580
you know, hundreds of
thousands of data points to use

595
00:29:20,580 --> 00:29:23,490
for future training, and
you're going to be relying

596
00:29:23,490 --> 00:29:25,380
on labeled data sets,

597
00:29:25,380 --> 00:29:28,410
thinking in advance about how
you might wanna label data

598
00:29:28,410 --> 00:29:31,200
that's coming out of
maybe a call center log

599
00:29:31,200 --> 00:29:34,053
or any other kind of interaction
can be really helpful.

600
00:29:36,690 --> 00:29:39,270
Another example here,
different type of model.

601
00:29:39,270 --> 00:29:43,800
This is image to text, but
we have an image reference

602
00:29:43,800 --> 00:29:45,807
and then an explanation, a caption

603
00:29:45,807 --> 00:29:48,660
that basically says when
you see this kind of image,

604
00:29:48,660 --> 00:29:52,020
I'd like you to respond with a cartoon

605
00:29:52,020 --> 00:29:53,853
of an orange cat with white spots.

606
00:29:55,230 --> 00:29:57,030
This supervised fine tuning example

607
00:29:57,030 --> 00:30:00,210
that I've shared today is
available in Amazon Bedrock.

608
00:30:00,210 --> 00:30:02,670
So again, even though
it's intended in general

609
00:30:02,670 --> 00:30:05,130
for app builders, you still have this kind

610
00:30:05,130 --> 00:30:08,490
of capability available
to you when you're working

611
00:30:08,490 --> 00:30:11,103
with Bedrock to customize those models.

612
00:30:13,800 --> 00:30:16,650
The next capability where
you might be tweaking

613
00:30:16,650 --> 00:30:19,740
the underlying model
itself is distillation.

614
00:30:19,740 --> 00:30:21,872
So this is something that might be useful

615
00:30:21,872 --> 00:30:24,840
if you're working with a large model

616
00:30:24,840 --> 00:30:27,960
and you really like the
outputs that you're getting,

617
00:30:27,960 --> 00:30:31,350
but the large model
isn't quite fast enough

618
00:30:31,350 --> 00:30:34,890
for your user behavior, for
your desired application.

619
00:30:34,890 --> 00:30:38,880
Maybe it costs more, but
again, you like the output.

620
00:30:38,880 --> 00:30:40,500
And when you try to generate the same kind

621
00:30:40,500 --> 00:30:43,710
of outputs from a smaller,
more nimble lower cost model,

622
00:30:43,710 --> 00:30:46,230
you're not getting the results you like.

623
00:30:46,230 --> 00:30:47,370
Distillation is kind of

624
00:30:47,370 --> 00:30:51,390
like the cousin of supervised fine tuning.

625
00:30:51,390 --> 00:30:54,360
So again, we have the prompts coming in,

626
00:30:54,360 --> 00:30:56,280
but in this case you
don't actually provide

627
00:30:56,280 --> 00:30:59,670
the output responses,
the outputs get generated

628
00:30:59,670 --> 00:31:02,992
by the larger teacher model
and then get fused back

629
00:31:02,992 --> 00:31:07,320
with the input and then used
as basically labeled data

630
00:31:07,320 --> 00:31:09,213
to train the small student model.

631
00:31:10,890 --> 00:31:13,200
So again, this gets a bit messier here.

632
00:31:13,200 --> 00:31:15,480
I'm trying not to overwhelm
with a lot of text,

633
00:31:15,480 --> 00:31:16,920
but the key here is you see

634
00:31:16,920 --> 00:31:21,750
in this third row there's
this role of a user

635
00:31:21,750 --> 00:31:25,080
and then there's some text,
some context and a question,

636
00:31:25,080 --> 00:31:27,750
but there's no assistant
response and that's

637
00:31:27,750 --> 00:31:30,060
because again, the assistant
response is gonna come

638
00:31:30,060 --> 00:31:32,610
from the larger teacher model.

639
00:31:32,610 --> 00:31:34,110
And so again, this is pretty useful

640
00:31:34,110 --> 00:31:36,381
if you don't actually have
all the answers, but you know

641
00:31:36,381 --> 00:31:38,942
that a large model would
produce content that you

642
00:31:38,942 --> 00:31:42,213
like, you can use something
like distillation.

643
00:31:47,160 --> 00:31:49,170
Alright. And then the third technique

644
00:31:49,170 --> 00:31:53,010
with labeled data that can be
really helpful is alignment.

645
00:31:53,010 --> 00:31:55,920
This is less about training for knowledge

646
00:31:55,920 --> 00:31:58,696
and more about training for tone or

647
00:31:58,696 --> 00:32:03,696
for maybe compliance, adding
guardrails into the response.

648
00:32:04,110 --> 00:32:06,360
Sometimes this can be
something that's really helpful

649
00:32:06,360 --> 00:32:09,540
for like the branding
of of an organization.

650
00:32:09,540 --> 00:32:12,210
And similar to fine
tuning, we wanna provide

651
00:32:12,210 --> 00:32:14,580
what's the prompt, what's the context?

652
00:32:14,580 --> 00:32:15,860
But in this case we want

653
00:32:15,860 --> 00:32:19,170
to provide preferred and
non-preferred responses.

654
00:32:19,170 --> 00:32:20,250
And so this is where I think

655
00:32:20,250 --> 00:32:22,263
the example is actually quite helpful.

656
00:32:23,670 --> 00:32:25,890
This labeled data that
you're gonna be providing

657
00:32:25,890 --> 00:32:29,700
also includes kind of a score or guidance

658
00:32:29,700 --> 00:32:33,000
of different responses
and which one it prefers.

659
00:32:33,000 --> 00:32:34,230
Here you're really teaching

660
00:32:34,230 --> 00:32:37,740
the model not just what
output you want to see,

661
00:32:37,740 --> 00:32:40,770
but how it compares to another output so

662
00:32:40,770 --> 00:32:43,050
that it can really
start to understand like

663
00:32:43,050 --> 00:32:45,813
and shape towards the
preferred outputs over time.

664
00:32:46,920 --> 00:32:49,500
This is something that
if you were interested

665
00:32:49,500 --> 00:32:52,140
in doing this kind of direct
preference optimization,

666
00:32:52,140 --> 00:32:54,690
this kind of alignment
technique, you would need

667
00:32:54,690 --> 00:32:57,060
to be shifting over to SageMaker AI.

668
00:32:57,060 --> 00:32:59,403
This one isn't available in Bedrock today.

669
00:33:03,030 --> 00:33:07,290
So we have three different
techniques using labeled data.

670
00:33:07,290 --> 00:33:08,123
It's also worth noting

671
00:33:08,123 --> 00:33:12,450
that the examples I shared
today are real examples.

672
00:33:12,450 --> 00:33:15,180
You could use that kind
of structure and syntax

673
00:33:15,180 --> 00:33:17,940
for supervised fine tuning distillation

674
00:33:17,940 --> 00:33:21,540
and alignment, but they
don't work with every model.

675
00:33:21,540 --> 00:33:25,020
And so you'll want to look
at these specific inputs,

676
00:33:25,020 --> 00:33:28,170
specific labeled data
requirements for the models

677
00:33:28,170 --> 00:33:30,780
that you're planning on
working with in advance so

678
00:33:30,780 --> 00:33:32,550
that you're actually
structuring your data properly

679
00:33:32,550 --> 00:33:35,283
or you can obviously
modify it after the fact.

680
00:33:37,721 --> 00:33:40,290
Okay, so we've gone from the left here

681
00:33:40,290 --> 00:33:41,580
through the labeled data.

682
00:33:41,580 --> 00:33:42,420
Now we're all the way

683
00:33:42,420 --> 00:33:45,300
on the right side, continued pre-training

684
00:33:45,300 --> 00:33:47,733
and training a new model from scratch.

685
00:33:50,370 --> 00:33:52,827
We'll come back with one of
those slides at the end as well.

686
00:33:52,827 --> 00:33:54,930
(chuckles)

687
00:33:54,930 --> 00:33:58,980
So here again, we have a lot
of data, we're not happy yet

688
00:33:58,980 --> 00:34:03,390
with the output of the model
and we want to take it forward.

689
00:34:03,390 --> 00:34:06,870
The best place to go here would
be continued pre-training.

690
00:34:06,870 --> 00:34:09,300
This is where you don't have
to have that labeled data set,

691
00:34:09,300 --> 00:34:13,200
but you can deeply embed
new knowledge, new tone,

692
00:34:13,200 --> 00:34:17,940
new relationships between your
data into the model itself.

693
00:34:17,940 --> 00:34:21,990
But the fact is, there are
also cases where the model

694
00:34:21,990 --> 00:34:25,200
that you're working with
was actually trained on data

695
00:34:25,200 --> 00:34:28,050
that is just structurally so different

696
00:34:28,050 --> 00:34:30,030
from the model you're trying to create.

697
00:34:30,030 --> 00:34:31,660
Maybe it's a different language

698
00:34:32,790 --> 00:34:35,280
or maybe it's not a large
language model at all.

699
00:34:35,280 --> 00:34:38,370
It's something like a
weather forecasting model

700
00:34:38,370 --> 00:34:42,930
or a foundation model for drug discovery.

701
00:34:42,930 --> 00:34:44,730
There you're really going to be starting

702
00:34:44,730 --> 00:34:47,490
from scratch with building
a model and that's fine

703
00:34:47,490 --> 00:34:49,410
as long as you have all that data.

704
00:34:49,410 --> 00:34:51,510
And that's what we're gonna
be talking about here.

705
00:34:51,510 --> 00:34:53,760
One of the big differences if you're

706
00:34:53,760 --> 00:34:57,180
in this kind of continued
pre-training or training a model

707
00:34:57,180 --> 00:34:59,340
from scratch, the big difference from some

708
00:34:59,340 --> 00:35:00,950
of the previous phases is this

709
00:35:00,950 --> 00:35:04,500
is a much more resource intensive effort.

710
00:35:04,500 --> 00:35:08,430
And the integration between
your data, the compute

711
00:35:08,430 --> 00:35:11,043
and storage becomes
really, really important.

712
00:35:12,900 --> 00:35:14,670
And the reason that that's the case is

713
00:35:14,670 --> 00:35:16,680
because getting all of that data

714
00:35:16,680 --> 00:35:18,390
that we just talked
about, that vast amount

715
00:35:18,390 --> 00:35:20,880
of data you need to train
a model from scratch

716
00:35:20,880 --> 00:35:22,995
or do continued pre-training needs

717
00:35:22,995 --> 00:35:26,790
to be loaded from storage
efficiently into your GPU

718
00:35:26,790 --> 00:35:30,540
or accelerator instances
and then periodically

719
00:35:30,540 --> 00:35:32,880
for a number of different
reasons, you'll also want

720
00:35:32,880 --> 00:35:35,040
to write these kind of
intermediate checkpoints

721
00:35:35,040 --> 00:35:37,080
back to storage.

722
00:35:37,080 --> 00:35:39,090
Sometimes we wanna write checkpoints

723
00:35:39,090 --> 00:35:41,430
because you wanna make
sure there's a safe point

724
00:35:41,430 --> 00:35:43,380
to restore back to in
case there's some kind

725
00:35:43,380 --> 00:35:45,390
of infrastructure issue.

726
00:35:45,390 --> 00:35:47,850
And in other cases it
might just be a point

727
00:35:47,850 --> 00:35:51,090
for you to evaluate the
model and maybe go back to

728
00:35:51,090 --> 00:35:51,984
if you want to try training

729
00:35:51,984 --> 00:35:54,600
with different data sets over time.

730
00:35:54,600 --> 00:35:57,090
In all of those cases,
there's a lot of data

731
00:35:57,090 --> 00:35:59,130
that you need to get into the GPUs

732
00:35:59,130 --> 00:36:00,390
and there's that checkpoint data

733
00:36:00,390 --> 00:36:02,640
that you wanna get off
the GPUs very quickly.

734
00:36:04,770 --> 00:36:07,260
To give you a little bit
of a sense of the rates

735
00:36:07,260 --> 00:36:11,460
that these GPU and accelerator
instances can process data.

736
00:36:11,460 --> 00:36:13,500
If it's text-based models you're working

737
00:36:13,500 --> 00:36:17,310
with, typically we see around
128 megabytes per second

738
00:36:17,310 --> 00:36:19,260
for every GPU.

739
00:36:19,260 --> 00:36:21,600
So if you imagine distributed training

740
00:36:21,600 --> 00:36:23,640
on a number of different GPUs,

741
00:36:23,640 --> 00:36:25,620
that number can get pretty high gigabytes,

742
00:36:25,620 --> 00:36:28,080
tens of gigabytes per second.

743
00:36:28,080 --> 00:36:30,948
If you're working with richer
media, multimodal models

744
00:36:30,948 --> 00:36:34,140
or video, the throughput required

745
00:36:34,140 --> 00:36:37,860
to actually drive data
into these accelerator

746
00:36:37,860 --> 00:36:40,053
and GPU instances can be quite material.

747
00:36:42,810 --> 00:36:45,450
The other thing that
can impact performance

748
00:36:45,450 --> 00:36:48,000
is actually the size of the IO.

749
00:36:48,000 --> 00:36:50,460
The size of the data that
you're trying to retrieve

750
00:36:50,460 --> 00:36:53,520
at a given point in time
for a given request.

751
00:36:53,520 --> 00:36:56,490
If you think about
reading data from storage,

752
00:36:56,490 --> 00:36:58,920
the time it takes to get that data back

753
00:36:58,920 --> 00:37:02,160
is a combination of
overhead for the request

754
00:37:02,160 --> 00:37:05,460
and then moving that
data itself, the payload.

755
00:37:05,460 --> 00:37:07,200
Moving or getting the request,

756
00:37:07,200 --> 00:37:08,970
that overhead is authenticating

757
00:37:08,970 --> 00:37:12,390
that you have access to the
data, it's the network latency

758
00:37:12,390 --> 00:37:15,030
to actually go and retrieve
the data between the GPU

759
00:37:15,030 --> 00:37:17,370
or accelerator instance and the storage

760
00:37:17,370 --> 00:37:19,230
and then it's the metadata
lookup to figure out

761
00:37:19,230 --> 00:37:21,580
within the storage system
where the data lives.

762
00:37:23,580 --> 00:37:26,400
Now if you're working with small files

763
00:37:26,400 --> 00:37:29,850
or small IO small objects, then the amount

764
00:37:29,850 --> 00:37:30,720
of time you spend on

765
00:37:30,720 --> 00:37:34,200
that overhead actually dominates the read.

766
00:37:34,200 --> 00:37:35,670
And this is where having storage

767
00:37:35,670 --> 00:37:39,000
that's low latency, storage
that is able to respond

768
00:37:39,000 --> 00:37:42,840
and do that authentication,
shorten that network path

769
00:37:42,840 --> 00:37:45,630
and do that metadata look up
really quickly can have quite

770
00:37:45,630 --> 00:37:48,030
an impact on the job completion time.

771
00:37:48,030 --> 00:37:50,460
And again, training these models,

772
00:37:50,460 --> 00:37:52,080
doing this continued pre-training

773
00:37:52,080 --> 00:37:55,140
or training a model from
scratch can be quite expensive.

774
00:37:55,140 --> 00:37:58,650
And so making sure these
instances are kept busy,

775
00:37:58,650 --> 00:38:01,080
that the training is
efficient, it's helpful

776
00:38:01,080 --> 00:38:03,510
on the infrastructure side,
it's also helpful just

777
00:38:03,510 --> 00:38:05,130
to GET responses to the people

778
00:38:05,130 --> 00:38:07,410
who are developing these
models in the first place

779
00:38:07,410 --> 00:38:10,290
and iterate quickly because
this is really collaborative

780
00:38:10,290 --> 00:38:12,963
and iterative workload.

781
00:38:15,330 --> 00:38:18,390
So within the storage
portfolio that we have

782
00:38:18,390 --> 00:38:22,230
at AWS on the file side,
we have Amazon, EFS,

783
00:38:22,230 --> 00:38:24,210
our elastic file system.

784
00:38:24,210 --> 00:38:27,120
We have Amazon FSx, which is commercial

785
00:38:27,120 --> 00:38:29,520
and open source file
systems that we manage

786
00:38:29,520 --> 00:38:32,070
on behalf of our customers in the cloud.

787
00:38:32,070 --> 00:38:34,230
This is kind of akin to RDS.

788
00:38:34,230 --> 00:38:36,930
We have a of different
file system offerings

789
00:38:36,930 --> 00:38:38,433
within the FSx family.

790
00:38:39,630 --> 00:38:42,060
We have object storage with Amazon S3

791
00:38:42,060 --> 00:38:44,403
and then we have block storage EBS.

792
00:38:46,260 --> 00:38:47,520
For model training

793
00:38:47,520 --> 00:38:50,430
in particular, the
shared storage offerings

794
00:38:50,430 --> 00:38:53,040
that our customers use fall within the FSx

795
00:38:53,040 --> 00:38:56,313
and Amazon S3 families in particular.

796
00:38:58,080 --> 00:39:00,930
And then just to kind of
go maybe one step further,

797
00:39:00,930 --> 00:39:04,980
how customers think about
which storage services to use.

798
00:39:04,980 --> 00:39:06,750
Typically if you're
working on the research

799
00:39:06,750 --> 00:39:09,060
and development side, file systems

800
00:39:09,060 --> 00:39:12,270
are the preferred approach
for shared storage.

801
00:39:12,270 --> 00:39:14,850
And if you're kind of looking
for something more optimized

802
00:39:14,850 --> 00:39:17,340
for production use cases, data pipelines

803
00:39:17,340 --> 00:39:19,590
that are more fixed,
that's where something

804
00:39:19,590 --> 00:39:23,373
like Amazon S3 and object
storage becomes more optimized.

805
00:39:24,360 --> 00:39:26,490
So let's dive into the file side.

806
00:39:26,490 --> 00:39:28,560
Again, my goal here is to make sure

807
00:39:28,560 --> 00:39:30,660
that you understand all
these different terms

808
00:39:30,660 --> 00:39:32,970
that are typically thrown around with AI

809
00:39:32,970 --> 00:39:35,700
and machine learning and
that you know which services

810
00:39:35,700 --> 00:39:38,043
to use for each given use case.

811
00:39:38,910 --> 00:39:40,080
In the case of research

812
00:39:40,080 --> 00:39:43,500
and development, we have
different file systems

813
00:39:43,500 --> 00:39:45,660
that have been basically built

814
00:39:45,660 --> 00:39:49,560
and architected for specific use cases.

815
00:39:49,560 --> 00:39:52,620
Scale up file systems,
file systems that rely

816
00:39:52,620 --> 00:39:55,020
on a single server that can be larger

817
00:39:55,020 --> 00:39:57,450
or smaller are one part

818
00:39:57,450 --> 00:40:00,150
of the portfolio, one
type of architecture.

819
00:40:00,150 --> 00:40:02,640
And then we have these
scale out file systems

820
00:40:02,640 --> 00:40:05,580
that rely on multiple
servers stitched together

821
00:40:05,580 --> 00:40:07,773
to deliver higher levels of performance.

822
00:40:10,800 --> 00:40:15,630
Amazon FSx for open ZFS is
an open source file system

823
00:40:15,630 --> 00:40:17,730
that we fully manage for our customers.

824
00:40:17,730 --> 00:40:21,412
It's a scale up file system
that uses NFS for communication.

825
00:40:21,412 --> 00:40:24,960
This is the most standard
way for file systems

826
00:40:24,960 --> 00:40:26,850
to communicate with EC2 instances

827
00:40:26,850 --> 00:40:29,430
or any other client instances.

828
00:40:29,430 --> 00:40:33,960
And it's ideal for ultra
low latency workloads

829
00:40:33,960 --> 00:40:37,440
like home directories, storing
your conda environments

830
00:40:37,440 --> 00:40:39,030
or GIT repositories.

831
00:40:39,030 --> 00:40:40,800
This is where your
developers are gonna want

832
00:40:40,800 --> 00:40:44,070
to keep their data so that if you have a,

833
00:40:44,070 --> 00:40:47,970
let's say a Kubernetes based
developer environment stack

834
00:40:47,970 --> 00:40:51,600
where researchers are just
having their instances spun up

835
00:40:51,600 --> 00:40:53,910
and down, but you wanna
give them shared storage

836
00:40:53,910 --> 00:40:56,062
to work with in between something

837
00:40:56,062 --> 00:41:00,003
like an open ZFS shared file
system is incredibly valuable.

838
00:41:02,550 --> 00:41:05,580
On the flip side, stitching
a bunch of servers together

839
00:41:05,580 --> 00:41:08,610
is how you get to something
like Amazon FSx for luster.

840
00:41:08,610 --> 00:41:11,340
Again, this is a file
system we manage on behalf

841
00:41:11,340 --> 00:41:13,710
of our customers so they
don't have to think about

842
00:41:13,710 --> 00:41:16,170
what is luster, how does it work?

843
00:41:16,170 --> 00:41:18,617
For our customers, this
just becomes a mount point

844
00:41:18,617 --> 00:41:22,710
that you mount on your
instances and you do file APIs,

845
00:41:22,710 --> 00:41:26,940
you open read, write close
files, very simple in terms

846
00:41:26,940 --> 00:41:30,030
of the interface, but the
power you get is the ability

847
00:41:30,030 --> 00:41:34,590
to drive very high levels
of throughput and IOPS

848
00:41:34,590 --> 00:41:37,500
or transactions to your storage.

849
00:41:37,500 --> 00:41:39,978
And so this is the
solution that we recommend

850
00:41:39,978 --> 00:41:43,230
for customers to store their training data

851
00:41:43,230 --> 00:41:46,200
to receive and restore
their checkpoint data

852
00:41:46,200 --> 00:41:49,173
because it offers this
really scalable performance.

853
00:41:50,790 --> 00:41:53,250
Maybe one example of the
type of thing we've done

854
00:41:53,250 --> 00:41:57,090
with these offerings is
allowed our customers

855
00:41:57,090 --> 00:42:00,030
to actually use some enhanced
networking capabilities

856
00:42:00,030 --> 00:42:04,650
like Elastic Fabric adapter
or GPU direct storage

857
00:42:04,650 --> 00:42:07,170
with our FSx file systems.

858
00:42:07,170 --> 00:42:09,990
This offering or this networking
stack allows customers

859
00:42:09,990 --> 00:42:14,730
to read data directly from our
FSx four luster file systems

860
00:42:14,730 --> 00:42:19,530
into the CPU memory or the
GPU memory on those GPU

861
00:42:19,530 --> 00:42:21,600
and accelerator instances.

862
00:42:21,600 --> 00:42:24,180
And again, this is
optimized for performance.

863
00:42:24,180 --> 00:42:26,112
This is why we recommend
these file systems

864
00:42:26,112 --> 00:42:28,560
for these particular workloads.

865
00:42:28,560 --> 00:42:30,420
And you don't have to take these steps

866
00:42:30,420 --> 00:42:32,490
to use these networking
stacks, but they're available

867
00:42:32,490 --> 00:42:34,740
to you and once you set
them up, you don't have

868
00:42:34,740 --> 00:42:37,440
to do anything special to
achieve the high levels

869
00:42:37,440 --> 00:42:39,720
of throughput that are actually enabled

870
00:42:39,720 --> 00:42:41,313
by these networking stacks.

871
00:42:44,670 --> 00:42:46,650
So there is one challenge

872
00:42:46,650 --> 00:42:50,403
that has historically been
associated with file systems.

873
00:42:51,810 --> 00:42:55,020
Many file systems are
based on SSD based discs,

874
00:42:55,020 --> 00:42:57,030
solid state discs because a lot

875
00:42:57,030 --> 00:43:00,930
of file-based applications
expect very low latency responses

876
00:43:00,930 --> 00:43:03,030
and high transaction rates.

877
00:43:03,030 --> 00:43:03,990
And one of the challenges

878
00:43:03,990 --> 00:43:08,013
with SSD based file systems
is they can be expensive.

879
00:43:08,940 --> 00:43:11,460
If you have large amounts of data

880
00:43:11,460 --> 00:43:14,760
and you're some of it's hot,
some of it's cold, storing all

881
00:43:14,760 --> 00:43:18,180
of that on an SSD based
file system isn't ideal.

882
00:43:18,180 --> 00:43:20,940
And if your data volume is increasing

883
00:43:20,940 --> 00:43:24,180
and decreasing quickly,
having the right amount

884
00:43:24,180 --> 00:43:26,490
of storage on SSD discs to support

885
00:43:26,490 --> 00:43:29,040
that data can also be challenging.

886
00:43:29,040 --> 00:43:30,630
And for those of you who are taking on

887
00:43:30,630 --> 00:43:32,340
that challenge and trying to move data

888
00:43:32,340 --> 00:43:35,340
between hotter and
colder storage offerings,

889
00:43:35,340 --> 00:43:37,890
that's just more operational overhead.

890
00:43:37,890 --> 00:43:39,210
And so one of the things we've done

891
00:43:39,210 --> 00:43:42,720
over the last year with our FSx offering,

892
00:43:42,720 --> 00:43:46,380
is we've added FSx intelligent tiering.

893
00:43:46,380 --> 00:43:48,870
This is very similar to the concepts

894
00:43:48,870 --> 00:43:51,690
that we see on S3 intelligent tiering.

895
00:43:51,690 --> 00:43:54,450
We have virtually
unlimited elastic storage

896
00:43:54,450 --> 00:43:56,040
on our file systems.

897
00:43:56,040 --> 00:43:58,800
We have that half a
penny kind of price point

898
00:43:58,800 --> 00:44:01,230
that you might be familiar with something

899
00:44:01,230 --> 00:44:03,720
like Glacier Instant Retrieval.

900
00:44:03,720 --> 00:44:06,510
And then again, we're
automatically moving data

901
00:44:06,510 --> 00:44:10,200
between hotter tiers including
an SSD based tier all the way

902
00:44:10,200 --> 00:44:14,220
down to that colder archival
instant access tier.

903
00:44:14,220 --> 00:44:17,250
And so for quite a while,
customers who were interested

904
00:44:17,250 --> 00:44:19,050
in working with file systems

905
00:44:19,050 --> 00:44:21,870
but had a lot of data,
they would do something

906
00:44:21,870 --> 00:44:24,687
where they'd split their
data between Amazon S3

907
00:44:24,687 --> 00:44:26,610
and their file system really just

908
00:44:26,610 --> 00:44:29,430
for cost reasons even
though everything was going

909
00:44:29,430 --> 00:44:31,800
to be accessed through a interface.

910
00:44:31,800 --> 00:44:34,740
And so this really allows our customers

911
00:44:34,740 --> 00:44:37,950
to just work with one single file system

912
00:44:37,950 --> 00:44:41,790
and be able to take advantage
of all the different tiers

913
00:44:41,790 --> 00:44:45,300
and get the performance of
SSDs for their hottest data,

914
00:44:45,300 --> 00:44:48,840
get the cost of a frequent access tier

915
00:44:48,840 --> 00:44:52,497
for their hotter data that's
kind of staying, you know,

916
00:44:52,497 --> 00:44:55,080
really being used over the last few days.

917
00:44:55,080 --> 00:44:59,100
And then as data gets colder
and colder, we archive it down

918
00:44:59,100 --> 00:45:01,710
and you get that automatic
cost savings again down to

919
00:45:01,710 --> 00:45:03,270
that half a penny price point.

920
00:45:03,270 --> 00:45:06,600
So super easy and super simple
to work with if you've got

921
00:45:06,600 --> 00:45:09,840
that mix of hot and cold
data and again, accessible on

922
00:45:09,840 --> 00:45:14,403
that FSx for luster or FSx
for open ZFS file system.

923
00:45:16,890 --> 00:45:19,797
Now if you are a customer
who has data stored in S3

924
00:45:19,797 --> 00:45:23,250
and an S3 data lake, we
also have other capabilities

925
00:45:23,250 --> 00:45:27,060
to integrate FSx with your S3 data lakes.

926
00:45:27,060 --> 00:45:30,060
One option you can do is
connect your file system

927
00:45:30,060 --> 00:45:31,383
to your S3 bucket.

928
00:45:32,760 --> 00:45:35,190
You can load the metadata

929
00:45:35,190 --> 00:45:38,640
so those pointers to your
data onto the file system

930
00:45:38,640 --> 00:45:40,740
and they just show up as files.

931
00:45:40,740 --> 00:45:44,430
So you open a directory and you
can kind of see your S3 data

932
00:45:44,430 --> 00:45:47,133
on your instance just
as a regular directory.

933
00:45:48,240 --> 00:45:51,000
When you go and access
that data, it gets pulled,

934
00:45:51,000 --> 00:45:54,630
lazy loaded behind the
scenes onto your file system

935
00:45:54,630 --> 00:45:57,510
and obviously communicated or shared back

936
00:45:57,510 --> 00:45:59,223
to your EC2 instance.

937
00:46:00,690 --> 00:46:02,760
If you add new data to S3,

938
00:46:02,760 --> 00:46:06,000
it can automatically show
up on your file system and

939
00:46:06,000 --> 00:46:08,940
if you write new data to your file system,

940
00:46:08,940 --> 00:46:11,850
it can be pushed automatically back to S3.

941
00:46:11,850 --> 00:46:14,550
So super powerful architecture.

942
00:46:14,550 --> 00:46:17,970
For those of you who watched
Matt Garmin's keynote,

943
00:46:17,970 --> 00:46:20,520
you may have seen this
architecture a couple of times

944
00:46:20,520 --> 00:46:22,920
and customers like this
a lot because the storage

945
00:46:22,920 --> 00:46:25,470
and IT administrators
can think about working

946
00:46:25,470 --> 00:46:27,630
with their data in an S3 data lake

947
00:46:27,630 --> 00:46:28,980
and the researchers can have

948
00:46:28,980 --> 00:46:31,350
the intuitive collaborative interface

949
00:46:31,350 --> 00:46:32,973
that file systems provide.

950
00:46:34,980 --> 00:46:38,040
One of the examples that was
in the keynote was Adobe.

951
00:46:38,040 --> 00:46:41,070
They work with this specific
architecture actually

952
00:46:41,070 --> 00:46:43,710
to store a lot of their data on S3

953
00:46:43,710 --> 00:46:46,470
and access it through a file system.

954
00:46:46,470 --> 00:46:48,360
And they use this for
a lot of their research

955
00:46:48,360 --> 00:46:51,360
to figure out what models are
actually going to be moved

956
00:46:51,360 --> 00:46:54,390
into production, what models
are helpful and add value?

957
00:46:54,390 --> 00:46:57,990
And eventually, they use S3 to do some

958
00:46:57,990 --> 00:46:59,440
of their production training.

959
00:47:04,260 --> 00:47:07,530
Now if you do choose to store your data

960
00:47:07,530 --> 00:47:10,560
in a file system, it's
probably also worth noting

961
00:47:10,560 --> 00:47:13,020
that we've added some new
capabilities this year,

962
00:47:13,020 --> 00:47:16,290
in fact this week, to
make that data accessible

963
00:47:16,290 --> 00:47:20,610
to a whole wide range of
Amazon analytics services.

964
00:47:20,610 --> 00:47:23,520
So in the last example I mentioned,

965
00:47:23,520 --> 00:47:26,280
you might have an a data lake with S3 data

966
00:47:26,280 --> 00:47:27,900
and you can access it through your FSx

967
00:47:27,900 --> 00:47:29,760
for luster file system.

968
00:47:29,760 --> 00:47:33,480
In this case, if you have your
data in an FSx file system,

969
00:47:33,480 --> 00:47:36,990
but you wanna access it with Amazon S3,

970
00:47:36,990 --> 00:47:40,050
now we have this concept
of an access point

971
00:47:40,050 --> 00:47:44,340
that you can actually attach
to your FSx file systems

972
00:47:44,340 --> 00:47:47,550
and then you can actually
work to access data

973
00:47:47,550 --> 00:47:50,760
that might be stored in
your open ZFS file system

974
00:47:50,760 --> 00:47:53,910
or now in a NetApp ONTAP file system.

975
00:47:53,910 --> 00:47:56,670
This is super helpful if you
are a customer who's maybe

976
00:47:56,670 --> 00:47:58,980
migrated data from on-premises

977
00:47:58,980 --> 00:48:01,448
with a NetApp file system into the cloud

978
00:48:01,448 --> 00:48:05,016
and you wanna make that data
accessible to Amazon Bedrock

979
00:48:05,016 --> 00:48:07,860
for some of that fine
tuning I mentioned before

980
00:48:07,860 --> 00:48:10,110
or for that distillation use case before.

981
00:48:10,110 --> 00:48:12,701
So all of that data is
now accessible to all

982
00:48:12,701 --> 00:48:17,493
of these AWS services as well
using these S3 access points.

983
00:48:20,370 --> 00:48:24,900
Now we've talked about using
Amazon FSx to accelerate

984
00:48:24,900 --> 00:48:28,200
and deliver the performance
we need for these research

985
00:48:28,200 --> 00:48:30,210
and development applications.

986
00:48:30,210 --> 00:48:33,420
Let's talk a little bit
about what we've done on S3

987
00:48:33,420 --> 00:48:35,973
to optimize for these
ML workloads as well.

988
00:48:37,980 --> 00:48:41,820
So Amazon S3 has a lot of storage classes.

989
00:48:41,820 --> 00:48:44,430
The ones on the left are the
ones that are gonna be used for

990
00:48:44,430 --> 00:48:49,170
that hotter data, both
Amazon S3 Express One Zone,

991
00:48:49,170 --> 00:48:50,580
which is all the way on the left.

992
00:48:50,580 --> 00:48:53,490
And Amazon S3 standard
provide very high levels

993
00:48:53,490 --> 00:48:57,210
of throughput for a bunch
of different applications.

994
00:48:57,210 --> 00:49:01,140
Amazon S3 Express one zone
differs from Amazon S3 in

995
00:49:01,140 --> 00:49:03,030
that it provides much lower latencies

996
00:49:03,030 --> 00:49:05,250
and we'll talk about that in a little bit.

997
00:49:05,250 --> 00:49:08,910
But really if you're working
on continued pre-training

998
00:49:08,910 --> 00:49:11,820
or training a model from scratch,
you should expect that all

999
00:49:11,820 --> 00:49:14,610
of your data is going to be in
one of those two classes just

1000
00:49:14,610 --> 00:49:16,980
because it's going to
be accessed frequently

1001
00:49:16,980 --> 00:49:18,060
and you're not going to want

1002
00:49:18,060 --> 00:49:21,630
to have the the request cost profile

1003
00:49:21,630 --> 00:49:24,483
of anything in those kind of
further to the right tiers.

1004
00:49:27,000 --> 00:49:28,050
One of the ways

1005
00:49:28,050 --> 00:49:31,706
that Amazon S3 Express One
Zone has actually optimized

1006
00:49:31,706 --> 00:49:34,500
itself for these use cases

1007
00:49:34,500 --> 00:49:38,770
is it offers lower latency
profile by not making

1008
00:49:40,110 --> 00:49:44,160
an authorization request
on every single GET request

1009
00:49:44,160 --> 00:49:45,360
or PUT request,

1010
00:49:45,360 --> 00:49:48,210
it actually uses a session
based authorization.

1011
00:49:48,210 --> 00:49:51,810
So you do one authorization
and then you can read and write

1012
00:49:51,810 --> 00:49:55,500
or get and put from your
object storage to get that,

1013
00:49:55,500 --> 00:49:57,090
that lower latency.

1014
00:49:57,090 --> 00:49:58,800
The other thing is in the name you can see

1015
00:49:58,800 --> 00:50:01,470
it's Amazon S3 Express One Zone.

1016
00:50:01,470 --> 00:50:04,950
So we've actually deployed
this S3 storage class

1017
00:50:04,950 --> 00:50:06,630
within the availability zone,

1018
00:50:06,630 --> 00:50:08,550
so it's co-located with your GPU

1019
00:50:08,550 --> 00:50:10,980
or accelerator instances shortening those

1020
00:50:10,980 --> 00:50:12,627
network paths as well.

1021
00:50:12,627 --> 00:50:14,550
And so those two capabilities,

1022
00:50:14,550 --> 00:50:17,010
that session based authorization

1023
00:50:17,010 --> 00:50:19,650
and the fact that we've
co-located the clusters

1024
00:50:19,650 --> 00:50:22,961
with your GPU or accelerator
instances allow us to deliver

1025
00:50:22,961 --> 00:50:25,650
that single digit millisecond latencies

1026
00:50:25,650 --> 00:50:27,270
that can be really helpful
when you're working

1027
00:50:27,270 --> 00:50:28,773
with small objects.

1028
00:50:29,610 --> 00:50:32,010
The other thing that
kind of goes hand in hand

1029
00:50:32,010 --> 00:50:34,449
with small objects is
having very high levels

1030
00:50:34,449 --> 00:50:36,900
of TPS out of the box.

1031
00:50:36,900 --> 00:50:39,180
So hundreds of thousands of transactions

1032
00:50:39,180 --> 00:50:42,183
per second available
with this storage class.

1033
00:50:43,080 --> 00:50:45,330
One interesting example of a customer

1034
00:50:45,330 --> 00:50:48,570
that's actually done a lot of
this kind of training working

1035
00:50:48,570 --> 00:50:52,680
with the S3 Express One
Zone offering is Meta.

1036
00:50:52,680 --> 00:50:54,060
They came to us asking

1037
00:50:54,060 --> 00:50:58,560
for a very high throughput
object storage class

1038
00:50:58,560 --> 00:51:01,230
that they could use with very high levels

1039
00:51:01,230 --> 00:51:02,700
of transaction per second

1040
00:51:02,700 --> 00:51:05,220
and very low latencies, this kind

1041
00:51:05,220 --> 00:51:07,800
of single digit millisecond latencies.

1042
00:51:07,800 --> 00:51:10,290
And so we worked with them to scale some

1043
00:51:10,290 --> 00:51:12,436
of our S3 express clusters in order

1044
00:51:12,436 --> 00:51:17,100
to meet this 140 terabit
per second throughput level.

1045
00:51:17,100 --> 00:51:19,920
That's 17 terabytes per second.

1046
00:51:19,920 --> 00:51:23,940
We are in clear super
computing storage range here

1047
00:51:23,940 --> 00:51:26,430
available in the cloud
to train these models.

1048
00:51:26,430 --> 00:51:27,817
Now I don't expect everyone

1049
00:51:27,817 --> 00:51:31,260
to be doing anything
near this kind of scale,

1050
00:51:31,260 --> 00:51:34,950
but it does speak to the
capabilities that we have both

1051
00:51:34,950 --> 00:51:37,860
for small scale and the ability
to scale to meet the needs

1052
00:51:37,860 --> 00:51:39,903
of even the most demanding workloads.

1053
00:51:42,240 --> 00:51:45,600
In addition to moving some
of the infrastructure closer

1054
00:51:45,600 --> 00:51:49,290
to our compute physically
in our data centers.

1055
00:51:49,290 --> 00:51:51,810
The other thing we've tried
to do over the last few years

1056
00:51:51,810 --> 00:51:55,290
is bring the APIs a little
bit closer to the workloads

1057
00:51:55,290 --> 00:51:59,100
that you might be running
with S3 for machine learning.

1058
00:51:59,100 --> 00:52:02,520
Two examples of that are
Mount Point for Amazon S3

1059
00:52:02,520 --> 00:52:05,670
and the Amazon S3 connector for PyTorch

1060
00:52:05,670 --> 00:52:08,220
because a lot of machine
learning workloads

1061
00:52:08,220 --> 00:52:10,059
and libraries rely on

1062
00:52:10,059 --> 00:52:15,059
and expect file interfaces
to be used to access data.

1063
00:52:15,360 --> 00:52:19,080
We added Amazon Mount Point
for S3, which is a connector

1064
00:52:19,080 --> 00:52:22,080
that's fully optimized for Amazon S3.

1065
00:52:22,080 --> 00:52:24,630
There's a lot of connectors out
there in the world that work

1066
00:52:24,630 --> 00:52:27,210
with object and provide a file interface.

1067
00:52:27,210 --> 00:52:29,160
We wanted one that was fully optimized

1068
00:52:29,160 --> 00:52:32,490
to take advantage of how
S3 works under the hood.

1069
00:52:32,490 --> 00:52:34,050
And so this is super helpful

1070
00:52:34,050 --> 00:52:36,480
for read only machine learning workloads.

1071
00:52:36,480 --> 00:52:37,920
If you're gonna train a model

1072
00:52:37,920 --> 00:52:40,683
and you're gonna read
through a lot of data on S3,

1073
00:52:41,610 --> 00:52:43,276
it's a very simple plugin to just say,

1074
00:52:43,276 --> 00:52:45,960
let me use Mount Point
for Amazon S3 to access

1075
00:52:45,960 --> 00:52:50,460
that data read only and to
just translate those file open

1076
00:52:50,460 --> 00:52:54,573
and read APIs into an S3 get effectively.

1077
00:52:55,740 --> 00:52:58,980
On the other side, there's also
some workloads where we know

1078
00:52:58,980 --> 00:53:02,733
that just swapping the
interface isn't sufficient.

1079
00:53:03,720 --> 00:53:07,050
When that happens, we wanted
to go further up in the stack

1080
00:53:07,050 --> 00:53:08,850
and that's where something
like the connector

1081
00:53:08,850 --> 00:53:10,410
for PyTorch comes in,

1082
00:53:10,410 --> 00:53:14,310
where we actually swapped
out the whole file interface

1083
00:53:14,310 --> 00:53:16,530
and develop something
that was really optimized

1084
00:53:16,530 --> 00:53:19,830
for Amazon S3 and allows
us to do more streaming

1085
00:53:19,830 --> 00:53:24,510
and prefetching to get data
from S3 into those EC2 nodes

1086
00:53:24,510 --> 00:53:25,773
for that effort.

1087
00:53:28,530 --> 00:53:29,363
Okay, so now we've, I think, finished

1088
00:53:29,363 --> 00:53:33,930
the whole spectrum here of
different ways for you to work

1089
00:53:33,930 --> 00:53:38,130
with your data to improve your
models, both for inference

1090
00:53:38,130 --> 00:53:40,320
if you're not really gonna
change the model itself

1091
00:53:40,320 --> 00:53:41,153
and you're just trying

1092
00:53:41,153 --> 00:53:43,290
to get a better output at inference time,

1093
00:53:43,290 --> 00:53:47,760
that's your prompts, your
knowledge bases and your metadata.

1094
00:53:47,760 --> 00:53:51,174
Or if you need to change the
underlying model itself, that's

1095
00:53:51,174 --> 00:53:54,540
where getting that labeled
data or unlabeled data

1096
00:53:54,540 --> 00:53:57,900
and doing some of the
techniques here, fine tuning,

1097
00:53:57,900 --> 00:54:02,370
distillation, alignment
or continued pre-training

1098
00:54:02,370 --> 00:54:03,840
or training your own model from scratch.

1099
00:54:03,840 --> 00:54:05,430
That's where that comes in.

1100
00:54:05,430 --> 00:54:09,693
More of the the model
builder kind of persona.

1101
00:54:11,160 --> 00:54:13,410
If you're interested in
learning more, here's a couple

1102
00:54:13,410 --> 00:54:15,600
of sessions over the next 24 hours

1103
00:54:15,600 --> 00:54:17,430
that might be interesting.

1104
00:54:17,430 --> 00:54:20,160
And then my speaker, Monica,
my co-speaker, Monica,

1105
00:54:20,160 --> 00:54:22,823
and I'll be off on the side
if you do have any questions.

1106
00:54:23,700 --> 00:54:24,858
Thank you.

1107
00:54:24,858 --> 00:54:27,858
(audience clapping)

