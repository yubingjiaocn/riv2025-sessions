1
00:00:01,530 --> 00:00:03,600
- Thank you, Chris, thanks.

2
00:00:03,600 --> 00:00:04,680
So we're gonna be...

3
00:00:04,680 --> 00:00:06,780
We are all here in the
Gen AI, is that right?

4
00:00:06,780 --> 00:00:10,050
Everybody's building
yourself and 10 other agents.

5
00:00:10,050 --> 00:00:11,550
So we're gonna be talking about that,

6
00:00:11,550 --> 00:00:14,010
and I'll hand it over to Aneesh.

7
00:00:14,010 --> 00:00:15,960
My name is Mohammad Zaman. I go by Mo.

8
00:00:15,960 --> 00:00:19,800
I lead the AWS Strategic
Partnership for Cybage.

9
00:00:19,800 --> 00:00:21,180
And with me, Aneesh,

10
00:00:21,180 --> 00:00:25,230
he leads our cloud, data,
and AI practice at Cybage.

11
00:00:25,230 --> 00:00:26,880
And we're gonna be building some stuff,

12
00:00:26,880 --> 00:00:27,990
and you get to know it.

13
00:00:27,990 --> 00:00:30,420
And, also, after this, if
you have more questions,

14
00:00:30,420 --> 00:00:32,133
come to Booth 12.

15
00:00:33,960 --> 00:00:34,960
- Thank you so much.

16
00:00:37,590 --> 00:00:40,403
Oh, I'll use the handheld
maybe. Hello? Hey.

17
00:00:40,403 --> 00:00:44,100
Okay, I will use this.
Thanks everyone for coming.

18
00:00:44,100 --> 00:00:44,970
We're gonna talk quickly

19
00:00:44,970 --> 00:00:47,670
about production grade AI workloads,

20
00:00:47,670 --> 00:00:50,340
the first and only AI
session at re:Invent.

21
00:00:50,340 --> 00:00:53,310
That's a lie, but we'll cover
some interesting things here.

22
00:00:53,310 --> 00:00:58,260
So there are key ways that
different organizations,

23
00:00:58,260 --> 00:00:59,940
both software companies and enterprises,

24
00:00:59,940 --> 00:01:02,310
are thinking about AI in their stack.

25
00:01:02,310 --> 00:01:04,350
You think about typical
three-tiered stack,

26
00:01:04,350 --> 00:01:06,720
either we have some people
working on their data layer

27
00:01:06,720 --> 00:01:10,560
to get it prepared to sell to
foundation model providers.

28
00:01:10,560 --> 00:01:13,350
So this is a lot of
publications, media houses,

29
00:01:13,350 --> 00:01:15,540
are working on sending their data

30
00:01:15,540 --> 00:01:17,940
to model providers in a
more effective manner.

31
00:01:17,940 --> 00:01:20,220
You have people who are
working on, obviously,

32
00:01:20,220 --> 00:01:22,658
AI-assisted development and code.

33
00:01:22,658 --> 00:01:24,180
And it's also changing the way

34
00:01:24,180 --> 00:01:26,070
you view an application stack.

35
00:01:26,070 --> 00:01:28,620
In the sense that,
previously, you had APIs,

36
00:01:28,620 --> 00:01:30,060
which are doing the most heavy lifting

37
00:01:30,060 --> 00:01:32,490
of business logic in an application.

38
00:01:32,490 --> 00:01:34,590
Now, APIs are getting much thinner.

39
00:01:34,590 --> 00:01:38,970
You have AI agents that work
on top of lighter-weight APIs

40
00:01:38,970 --> 00:01:40,620
that are largely doing crowd operations,

41
00:01:40,620 --> 00:01:43,470
where agents can take on
more of that workload,

42
00:01:43,470 --> 00:01:46,230
more of that logic, more
of that orchestration,

43
00:01:46,230 --> 00:01:47,490
more of what the platform

44
00:01:47,490 --> 00:01:49,800
and the meat of the platform really is.

45
00:01:49,800 --> 00:01:51,960
And this is being seen across the bottom.

46
00:01:51,960 --> 00:01:53,550
We'll talk a little bit
about what this means

47
00:01:53,550 --> 00:01:57,153
in terms of actual real
production-grade architectures.

48
00:01:58,350 --> 00:02:02,850
AWS services have evolved
immensely alongside this journey.

49
00:02:02,850 --> 00:02:03,986
So back in the day,

50
00:02:03,986 --> 00:02:06,900
it started with foundation
models on Bedrock.

51
00:02:06,900 --> 00:02:09,150
Now, we have a whole host of services.

52
00:02:09,150 --> 00:02:10,650
We have AgentCore,

53
00:02:10,650 --> 00:02:14,070
which gives you real-time
runtime with agents as well.

54
00:02:14,070 --> 00:02:15,720
So people are adapting,

55
00:02:15,720 --> 00:02:18,360
and AWS is providing
the end-to-end services

56
00:02:18,360 --> 00:02:20,210
for all of those different workloads.

57
00:02:21,210 --> 00:02:23,795
We'll jump into a few quick examples of

58
00:02:23,795 --> 00:02:27,210
real production-grade implementations

59
00:02:27,210 --> 00:02:28,500
that we're working on at Cybage.

60
00:02:28,500 --> 00:02:30,630
And, hopefully, you can
go away with some tips,

61
00:02:30,630 --> 00:02:31,833
ideas, as well.

62
00:02:33,210 --> 00:02:37,290
So one massive and recurring
issue that comes up

63
00:02:37,290 --> 00:02:41,880
is the ability to separate
what is a prototype,

64
00:02:41,880 --> 00:02:44,910
or a proof of concept,
within AI development

65
00:02:44,910 --> 00:02:47,460
versus what is a production-grade

66
00:02:47,460 --> 00:02:50,310
effort implementation architecture.

67
00:02:50,310 --> 00:02:53,220
We think about this a lot in Cybage.

68
00:02:53,220 --> 00:02:56,640
And what the main takeaway
from this slide is,

69
00:02:56,640 --> 00:03:01,050
don't over-engineer prototypes
in the space of Gen AI.

70
00:03:01,050 --> 00:03:04,230
AWS has great low-code services,

71
00:03:04,230 --> 00:03:07,200
we have Bedrock knowledge
bases, we have Kendra.

72
00:03:07,200 --> 00:03:09,720
We have a bunch of services
which are meant for you

73
00:03:09,720 --> 00:03:12,750
to prove a concept, and it ends at that.

74
00:03:12,750 --> 00:03:14,280
You're not meant to take
those implementation

75
00:03:14,280 --> 00:03:16,050
and take them to production.

76
00:03:16,050 --> 00:03:19,410
So what we usually design
is a team that works on

77
00:03:19,410 --> 00:03:21,600
design and experimentation
in an enterprise.

78
00:03:21,600 --> 00:03:24,600
They are only focusing on
sandbox AWS environments,

79
00:03:24,600 --> 00:03:26,280
on using low-code services,

80
00:03:26,280 --> 00:03:29,070
and proving a concept
for different use cases.

81
00:03:29,070 --> 00:03:33,330
Then, you have a higher complexity
data ingestion workflow.

82
00:03:33,330 --> 00:03:36,870
All your AI use cases
in the enterprise need

83
00:03:36,870 --> 00:03:39,267
real-time data connectors, ingestion.

84
00:03:39,267 --> 00:03:43,080
Sometimes, it has to be
custom built from ground up.

85
00:03:43,080 --> 00:03:46,290
And then, you have your agents
that work from that data,

86
00:03:46,290 --> 00:03:48,210
at least in typical RAG applications,

87
00:03:48,210 --> 00:03:50,580
as well as in agentic applications.

88
00:03:50,580 --> 00:03:52,020
Where, with AgentCore, now,

89
00:03:52,020 --> 00:03:57,020
you can have agents that work
across different MCP servers.

90
00:03:57,150 --> 00:03:58,530
So one really useful aspect,

91
00:03:58,530 --> 00:04:00,210
and we try to implement
that in our designs,

92
00:04:00,210 --> 00:04:03,930
are differentiating between
MCP servers or tools,

93
00:04:03,930 --> 00:04:06,630
and then the agents to use those servers.

94
00:04:06,630 --> 00:04:08,790
The same MCP server, for
example, in this case,

95
00:04:08,790 --> 00:04:12,150
web browsing, or context retrieval,

96
00:04:12,150 --> 00:04:14,640
can be utilized by multiple agents.

97
00:04:14,640 --> 00:04:18,450
So this separation of tools
with agentic workflows

98
00:04:18,450 --> 00:04:21,660
is something that we really,
really focus on in our designs

99
00:04:21,660 --> 00:04:23,763
and in our applications.

100
00:04:24,840 --> 00:04:27,990
AI gateways are really picking up as well.

101
00:04:27,990 --> 00:04:29,400
So there are tools like Lite LLM.

102
00:04:29,400 --> 00:04:31,203
AWS also has many offerings here.

103
00:04:32,970 --> 00:04:36,150
Having a centralized
place to log, monitor,

104
00:04:36,150 --> 00:04:40,680
and observe your workloads is
becoming extremely essential.

105
00:04:40,680 --> 00:04:43,260
And the way to do that is
through observability tooling.

106
00:04:43,260 --> 00:04:45,870
So AWS now allows you to ingest

107
00:04:45,870 --> 00:04:48,300
your Gen AI logs in CloudWatch,

108
00:04:48,300 --> 00:04:51,450
which lets you do end-to-end
traceability of sessions,

109
00:04:51,450 --> 00:04:52,830
of traces.

110
00:04:52,830 --> 00:04:57,030
And observability has
massive importance here.

111
00:04:57,030 --> 00:04:59,460
You need to know whether the
workloads you're running,

112
00:04:59,460 --> 00:05:00,570
and they're facing issues,

113
00:05:00,570 --> 00:05:03,330
are they happening at
the stage of retrieval?

114
00:05:03,330 --> 00:05:05,610
Are they happening at
the stage of generation?

115
00:05:05,610 --> 00:05:07,680
Are they happening because
users are still figuring out

116
00:05:07,680 --> 00:05:09,540
how to use your platform?

117
00:05:09,540 --> 00:05:11,970
And observability helps
you separate that as well.

118
00:05:11,970 --> 00:05:15,626
So this last layer here,
in terms of LiteLLM,

119
00:05:15,626 --> 00:05:17,190
and orchestration and observability,

120
00:05:17,190 --> 00:05:19,830
becomes extremely essential.

121
00:05:19,830 --> 00:05:23,580
So just the takeaway here
is, don't over engineer POCs.

122
00:05:23,580 --> 00:05:25,290
They're meant to prove a concept.

123
00:05:25,290 --> 00:05:26,280
Once you've proved that,

124
00:05:26,280 --> 00:05:28,320
think about production-grade
workloads on AWS,

125
00:05:28,320 --> 00:05:29,463
and what that means.

126
00:05:31,140 --> 00:05:32,433
In some other cases,

127
00:05:34,290 --> 00:05:37,290
one of our largest
implementations recently

128
00:05:37,290 --> 00:05:41,813
was focused on building AI
layers on top of legacy APIs

129
00:05:41,813 --> 00:05:44,640
and legacy software products.

130
00:05:44,640 --> 00:05:47,310
A lot of us here, even
every booth that you see,

131
00:05:47,310 --> 00:05:50,490
we ship software. That's
our bread and butter.

132
00:05:50,490 --> 00:05:54,597
It's about, can I ingrain, and
can I infuse agentic layers,

133
00:05:54,597 --> 00:05:56,880
on top of those product APIs?

134
00:05:56,880 --> 00:06:00,450
But in that process, multiple
challenges do come up.

135
00:06:00,450 --> 00:06:03,150
APIs that we've currently
built, and host in the world,

136
00:06:03,150 --> 00:06:05,850
are not made for agentic consumption.

137
00:06:05,850 --> 00:06:09,393
They are made for consumption
by typical user interfaces.

138
00:06:10,710 --> 00:06:12,330
And this can cause multiple issues.

139
00:06:12,330 --> 00:06:14,190
I'll give you a simple example.

140
00:06:14,190 --> 00:06:17,280
A lot of product APIs
return 100-plus results

141
00:06:17,280 --> 00:06:19,950
in their responses, very noisy responses,

142
00:06:19,950 --> 00:06:24,270
a lot of garbage-adjacent
responses coming from those APIs.

143
00:06:24,270 --> 00:06:27,720
Neither would an agentic
solution ever want

144
00:06:27,720 --> 00:06:30,090
to use an API like that,

145
00:06:30,090 --> 00:06:31,860
because the way you interact with an agent

146
00:06:31,860 --> 00:06:33,870
is probably chat based,

147
00:06:33,870 --> 00:06:36,480
which means that you're
sending smaller workloads

148
00:06:36,480 --> 00:06:38,550
and smaller results to that chat.

149
00:06:38,550 --> 00:06:42,420
Nor would an agent successfully
be able to reason through

150
00:06:42,420 --> 00:06:44,580
that massive API response.

151
00:06:44,580 --> 00:06:48,450
So API readiness is something
that is extremely essential

152
00:06:48,450 --> 00:06:51,240
before you build agentic
workloads on top of it.

153
00:06:51,240 --> 00:06:54,750
Another example, APIs
often have dependencies.

154
00:06:54,750 --> 00:06:56,250
So we built this application,

155
00:06:56,250 --> 00:07:00,870
which did agentic
orchestration on legacy APIs,

156
00:07:00,870 --> 00:07:03,150
and one issue we faced was latency.

157
00:07:03,150 --> 00:07:05,726
Many people are currently
struggling with it.

158
00:07:05,726 --> 00:07:10,290
Our LLM would hop and would
do two consecutive trips

159
00:07:10,290 --> 00:07:13,650
to first get authentication
tokens to first call APIs,

160
00:07:13,650 --> 00:07:17,280
and then, with the results of
that, call downstream APIs.

161
00:07:17,280 --> 00:07:18,720
Again, these APIs have not been built

162
00:07:18,720 --> 00:07:20,280
for agentic consumption.

163
00:07:20,280 --> 00:07:22,620
If you know how users
are going to interact

164
00:07:22,620 --> 00:07:23,820
with your agents,

165
00:07:23,820 --> 00:07:25,650
what type of workforce
are they going to run,

166
00:07:25,650 --> 00:07:27,630
what type of prompts
are they going to run,

167
00:07:27,630 --> 00:07:29,350
you can better design your APIs

168
00:07:29,350 --> 00:07:31,593
with an agentic-first mindset.

169
00:07:32,730 --> 00:07:34,950
That has nothing to do with LLMs,

170
00:07:34,950 --> 00:07:37,200
that has nothing to do with generation,

171
00:07:37,200 --> 00:07:41,100
that is about logical
design in your API schemas.

172
00:07:41,100 --> 00:07:43,290
So that's something that we focus a lot on

173
00:07:43,290 --> 00:07:44,403
in Cybage as well.

174
00:07:45,570 --> 00:07:46,680
Out here as well,

175
00:07:46,680 --> 00:07:50,340
observability and monitoring
become extremely essential.

176
00:07:50,340 --> 00:07:52,770
The good part about tool
calling and function calling

177
00:07:52,770 --> 00:07:55,440
is that observability
is more deterministic.

178
00:07:55,440 --> 00:07:56,760
What do I mean by that?

179
00:07:56,760 --> 00:07:58,800
With open-ended chat responses,

180
00:07:58,800 --> 00:08:03,210
you have to create evaluation
matrices that are subjective,

181
00:08:03,210 --> 00:08:05,280
that are not completely accurate.

182
00:08:05,280 --> 00:08:07,950
With tool calling, and agent
calling, and API calling,

183
00:08:07,950 --> 00:08:10,890
you can have more deterministic
evaluation metrics.

184
00:08:10,890 --> 00:08:14,460
So if I build an agent that
sits on top of my APIs,

185
00:08:14,460 --> 00:08:16,620
I can get to accuracy scores about

186
00:08:16,620 --> 00:08:19,530
how successfully it's
able to call the APIs

187
00:08:19,530 --> 00:08:22,200
I need it to call for a
certain set of prompts.

188
00:08:22,200 --> 00:08:25,920
That determinism is super
useful in evaluation,

189
00:08:25,920 --> 00:08:28,110
which you can't get in basic chat.

190
00:08:28,110 --> 00:08:30,330
You can't get in just freeform chat.

191
00:08:30,330 --> 00:08:33,465
For that, you have to use
custom evaluation metrics.

192
00:08:33,465 --> 00:08:36,300
So evaluation, again, I'll focus on,

193
00:08:36,300 --> 00:08:38,310
and it's something that we
work on a lot at Cybage.

194
00:08:38,310 --> 00:08:39,780
It's something that we infuse in a lot

195
00:08:39,780 --> 00:08:41,530
of our agentic development as well.

196
00:08:45,000 --> 00:08:50,000
So moving to some of the
real production grade issues

197
00:08:51,090 --> 00:08:53,400
that people are currently facing.

198
00:08:53,400 --> 00:08:54,570
And we'll talk about each one.

199
00:08:54,570 --> 00:08:57,870
We'll talk about how those
are being solved as well.

200
00:08:57,870 --> 00:08:59,100
One massive issue,

201
00:08:59,100 --> 00:09:01,170
and I'll start in the third box actually,

202
00:09:01,170 --> 00:09:02,733
security and governance.

203
00:09:04,140 --> 00:09:08,820
The moment you introduce tools
to an agentic implementation,

204
00:09:08,820 --> 00:09:13,820
the security concerns, they
multiply by magnitudes.

205
00:09:14,700 --> 00:09:17,100
You have your CISOs
getting extremely concerned

206
00:09:17,100 --> 00:09:19,740
with context mixing
between different tools

207
00:09:19,740 --> 00:09:21,640
that have different permission levels.

208
00:09:22,800 --> 00:09:23,850
And the other part there is

209
00:09:23,850 --> 00:09:26,190
that people are extremely
apprehensive, obviously,

210
00:09:26,190 --> 00:09:30,150
and rightfully so, of sending
PII for LLM model calls.

211
00:09:30,150 --> 00:09:31,110
How do you screen out?

212
00:09:31,110 --> 00:09:35,160
How do you put guardrails, both
at inputs and output stages?

213
00:09:35,160 --> 00:09:36,810
So that's something
that we focus a lot on.

214
00:09:36,810 --> 00:09:39,450
You can use services like Macie in AWS

215
00:09:39,450 --> 00:09:41,970
for PII detection and PII masking.

216
00:09:41,970 --> 00:09:43,500
You can use guardrails.

217
00:09:43,500 --> 00:09:44,790
There are some open source frameworks.

218
00:09:44,790 --> 00:09:47,970
You can use custom guardrails
to rail your applications,

219
00:09:47,970 --> 00:09:51,510
both pre-generation and post-generation.

220
00:09:51,510 --> 00:09:55,950
And you can bind user
groups to the right tools

221
00:09:55,950 --> 00:09:57,930
that they have access to.

222
00:09:57,930 --> 00:09:59,610
This is an extremely essential part

223
00:09:59,610 --> 00:10:01,830
of building these applications is that

224
00:10:01,830 --> 00:10:05,160
we need to reflect enterprise
access permission levels

225
00:10:05,160 --> 00:10:07,530
with agentic permissions.

226
00:10:07,530 --> 00:10:09,900
If you're building an
application for internal users,

227
00:10:09,900 --> 00:10:12,510
or for your end customer users,

228
00:10:12,510 --> 00:10:14,760
each of them have tiered permissions,

229
00:10:14,760 --> 00:10:16,470
those need to be reflected with the tools

230
00:10:16,470 --> 00:10:18,240
that they have access to.

231
00:10:18,240 --> 00:10:21,180
With AgentCore, this
becomes extremely powerful

232
00:10:21,180 --> 00:10:23,910
because you can use dynamic tool binding.

233
00:10:23,910 --> 00:10:28,910
So AgentCore allows you to bind
tools dynamically for a user

234
00:10:29,100 --> 00:10:31,980
so you can build in that pipeline

235
00:10:31,980 --> 00:10:35,340
binding that respects
permission levels of that user.

236
00:10:35,340 --> 00:10:37,290
So that's something that we focus a lot on

237
00:10:37,290 --> 00:10:38,123
in Cybage as well.

238
00:10:38,123 --> 00:10:40,160
So that's the security
and governance side.

239
00:10:41,340 --> 00:10:43,050
I'll focus, also, a
little bit on the adoption

240
00:10:43,050 --> 00:10:45,870
and monetization side
because we have some time.

241
00:10:45,870 --> 00:10:47,220
Again, it's not as technical,

242
00:10:47,220 --> 00:10:52,220
but token-based pricing is
not working out very well

243
00:10:52,770 --> 00:10:55,383
for people trying to ship
out products with AI.

244
00:10:56,430 --> 00:10:58,650
What I mean by that is,
if I'm a software company,

245
00:10:58,650 --> 00:11:02,160
and a lot of us are
representing software companies,

246
00:11:02,160 --> 00:11:06,060
and I push out a feature that
uses LLMs at the backend,

247
00:11:06,060 --> 00:11:09,330
people are not willing to
spend on token-based pricing

248
00:11:09,330 --> 00:11:10,773
for those end features.

249
00:11:12,420 --> 00:11:14,940
People are trying to bake that
into subscription pricing.

250
00:11:14,940 --> 00:11:16,890
Either you have a new tier of subscription

251
00:11:16,890 --> 00:11:20,430
that demands higher AI workloads.

252
00:11:20,430 --> 00:11:22,440
But there are innovative
ways that we've seen

253
00:11:22,440 --> 00:11:25,500
some of our clients work
on that pricing mechanism.

254
00:11:25,500 --> 00:11:28,350
So imagine you've built an agent

255
00:11:28,350 --> 00:11:30,780
that runs a certain defined workload.

256
00:11:30,780 --> 00:11:33,450
Maybe it runs a certain
content generation workload,

257
00:11:33,450 --> 00:11:35,190
a report generation workload.

258
00:11:35,190 --> 00:11:37,230
Maybe it runs a certain action,

259
00:11:37,230 --> 00:11:39,273
an actionable output, a research report.

260
00:11:40,290 --> 00:11:45,290
Try to price AI-based features
on output and not tokens.

261
00:11:45,330 --> 00:11:46,830
That's something that
we're seeing a lot of

262
00:11:46,830 --> 00:11:48,600
in different companies, is that,

263
00:11:48,600 --> 00:11:52,530
the only way to get
adoption from your end users

264
00:11:52,530 --> 00:11:55,830
is to price these features
based off end outputs,

265
00:11:55,830 --> 00:11:56,910
instead of on tokens.

266
00:11:56,910 --> 00:12:00,240
So that's another interesting
feature that we've learned.

267
00:12:00,240 --> 00:12:01,440
And, for that,

268
00:12:01,440 --> 00:12:05,130
your API and AI gateway
layers become important.

269
00:12:05,130 --> 00:12:08,220
What I mentioned earlier
around platforms like LiteLLM,

270
00:12:08,220 --> 00:12:10,950
integrating CloudWatch in AWS,

271
00:12:10,950 --> 00:12:13,050
these aspects are required before

272
00:12:13,050 --> 00:12:15,720
you differentially throttle AI workloads

273
00:12:15,720 --> 00:12:18,000
between different user groups.

274
00:12:18,000 --> 00:12:21,150
If I've decided to price an
AI feature differentially

275
00:12:21,150 --> 00:12:22,440
by user groups,

276
00:12:22,440 --> 00:12:24,630
I need to make sure that I'm
throttling the right groups

277
00:12:24,630 --> 00:12:26,490
by their AI usage.

278
00:12:26,490 --> 00:12:30,030
So that gateway layer
that gates your calls

279
00:12:30,030 --> 00:12:31,710
for agentic layers with end LLMs

280
00:12:31,710 --> 00:12:33,573
becomes extremely essential as well.

281
00:12:34,590 --> 00:12:36,540
So these are some real
production grade issues

282
00:12:36,540 --> 00:12:39,990
and solutions that we're
working on in Cybage.

283
00:12:39,990 --> 00:12:42,960
If there are any questions,
any further topics,

284
00:12:42,960 --> 00:12:45,210
you can find us at Booth 1231.

285
00:12:45,210 --> 00:12:48,090
We're building, with AWS, these
production grade solutions.

286
00:12:48,090 --> 00:12:50,760
We'd love to get more into the details.

287
00:12:50,760 --> 00:12:53,670
And that is all from us today,

288
00:12:53,670 --> 00:12:55,200
so thank you so much for coming.

289
00:12:55,200 --> 00:12:58,260
Hopefully, you're leaving
with some tangible learnings,

290
00:12:58,260 --> 00:12:59,910
and excited to talk to you all further.

291
00:12:59,910 --> 00:13:00,743
Thank you.

