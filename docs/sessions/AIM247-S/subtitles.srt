1
00:00:00,315 --> 00:00:03,833
(audience clapping)

2
00:00:03,833 --> 00:00:05,046
- What's going on everyone?

3
00:00:05,046 --> 00:00:06,210
So thanks for taking 20 minutes out

4
00:00:06,210 --> 00:00:08,040
of a crazy conference schedule

5
00:00:08,040 --> 00:00:11,340
to talk with me about AI initiatives,

6
00:00:11,340 --> 00:00:13,110
and building solid data foundations

7
00:00:13,110 --> 00:00:15,123
on Open Lakehouse Technologies.

8
00:00:16,170 --> 00:00:18,852
So we'll introduce you to Qlik.
If you haven't heard of us.

9
00:00:18,852 --> 00:00:20,223
We'll spend the bulk of the time today

10
00:00:20,223 --> 00:00:22,200
talking about kind of where
the current AI landscape is,

11
00:00:22,200 --> 00:00:24,840
and most specifically
how to build scalable,

12
00:00:24,840 --> 00:00:27,480
but also cost effective data platforms

13
00:00:27,480 --> 00:00:30,690
to drive really any data
initiative that you might have.

14
00:00:30,690 --> 00:00:34,230
But specifically those focused around AI.

15
00:00:34,230 --> 00:00:36,270
Qlik as a company or a
global software company,

16
00:00:36,270 --> 00:00:38,160
offices around the world.

17
00:00:38,160 --> 00:00:39,480
Many of you likely use us

18
00:00:39,480 --> 00:00:41,820
even if you don't know that you use us.

19
00:00:41,820 --> 00:00:44,220
Fundamentally, Qlik
has a product portfolio

20
00:00:44,220 --> 00:00:47,610
that provides customers with
an end-to-end data platform

21
00:00:47,610 --> 00:00:50,400
focused on both data
integration and transformation

22
00:00:50,400 --> 00:00:53,234
as well as data analytics.

23
00:00:53,234 --> 00:00:54,857
So wherever you are in your data journey,

24
00:00:54,857 --> 00:00:56,790
we can likely help get
data into digestible

25
00:00:56,790 --> 00:00:58,889
and consumable formats,

26
00:00:58,889 --> 00:01:01,350
or if your data formats are
in a good state as we speak,

27
00:01:01,350 --> 00:01:04,226
we can help you analyze
and provide analytics

28
00:01:04,226 --> 00:01:05,793
on top of your data.

29
00:01:06,840 --> 00:01:09,346
We are very proud of our AWS partnership.

30
00:01:09,346 --> 00:01:11,100
It's been long lasting

31
00:01:11,100 --> 00:01:14,400
and we hope it will continue
to be into the future.

32
00:01:14,400 --> 00:01:17,850
We've been strategic partners
with AWS for many, many years

33
00:01:17,850 --> 00:01:21,600
and we rely on AWS to power
much of the capabilities

34
00:01:21,600 --> 00:01:23,749
that we're gonna talk through today.

35
00:01:23,749 --> 00:01:25,295
So we're both a partner of AWS,

36
00:01:25,295 --> 00:01:27,150
we're also a consumer of AWS services,

37
00:01:27,150 --> 00:01:31,110
and one of our primary
objectives in partnering with AWS

38
00:01:31,110 --> 00:01:34,140
is to help customers deliver
successful data projects

39
00:01:34,140 --> 00:01:38,530
on AWS as fast as possible, right?

40
00:01:38,530 --> 00:01:40,410
We want to get you from data
to outcome very quickly,

41
00:01:40,410 --> 00:01:44,550
very easily, and really
enable all of your data

42
00:01:44,550 --> 00:01:46,710
professionals to be successful,

43
00:01:46,710 --> 00:01:50,220
driving data projects on AWS.

44
00:01:50,220 --> 00:01:53,380
So AI's happened, right?

45
00:01:53,380 --> 00:01:56,310
I don't think I'm out of
bounds by saying that,

46
00:01:56,310 --> 00:01:58,492
if you look at the maturity curve,

47
00:01:58,492 --> 00:02:00,540
we're kind of inside of it, right?

48
00:02:00,540 --> 00:02:03,930
Most organizations are
starting, continuing,

49
00:02:03,930 --> 00:02:07,882
starting to deliver AI projects
within their organization.

50
00:02:07,882 --> 00:02:09,870
So it's really up to us
as data professionals

51
00:02:09,870 --> 00:02:12,573
and as data teams to figure out

52
00:02:12,573 --> 00:02:14,040
how we're gonna build
strong data platforms

53
00:02:14,040 --> 00:02:15,690
and strong data foundations,

54
00:02:15,690 --> 00:02:19,140
to deliver successful AI initiatives.

55
00:02:19,140 --> 00:02:22,440
And many organizations are
starting to make progress, right?

56
00:02:22,440 --> 00:02:26,250
I think it's good to see that
almost 70% of organizations

57
00:02:26,250 --> 00:02:30,259
have started to build a
formalized AI strategy, right?

58
00:02:30,259 --> 00:02:33,060
Pretty rare these days to
approach an organization

59
00:02:33,060 --> 00:02:34,080
and they have no strategy

60
00:02:34,080 --> 00:02:38,280
or no developed plans
for accommodating AI.

61
00:02:38,280 --> 00:02:40,230
What has become really challenging though,

62
00:02:40,230 --> 00:02:41,880
and one of the factors

63
00:02:41,880 --> 00:02:43,362
that I think is starting to hold it back,

64
00:02:43,362 --> 00:02:45,000
is that ROI can be really hard to predict

65
00:02:45,000 --> 00:02:47,280
and quantify on AI projects.

66
00:02:47,280 --> 00:02:49,740
And many organizations are
struggling to kind of dip

67
00:02:49,740 --> 00:02:51,480
their toe all the way into the water,

68
00:02:51,480 --> 00:02:53,943
'cause they're not sure
whether they're gonna get

69
00:02:53,943 --> 00:02:56,725
the perceived benefit
out of those projects.

70
00:02:56,725 --> 00:02:58,283
So that has started to
slow down the rollout

71
00:02:58,283 --> 00:03:01,770
across the industry, though
you're seeing lots of successful

72
00:03:01,770 --> 00:03:04,878
AI initiatives, a lot of
them around kind of chatbot

73
00:03:04,878 --> 00:03:07,950
type engagements, code
reading and development.

74
00:03:07,950 --> 00:03:10,440
And we're starting to
see more AI initiatives

75
00:03:10,440 --> 00:03:13,773
fall into the analytics
perspective and realm.

76
00:03:14,640 --> 00:03:17,250
If you talk to organizations
about what's holding them back,

77
00:03:17,250 --> 00:03:19,870
though, we talked about
kind of the difficulty

78
00:03:20,995 --> 00:03:21,900
in predicting ROI,

79
00:03:21,900 --> 00:03:23,951
but a lot of the barriers

80
00:03:23,951 --> 00:03:25,896
are around that data foundation, right?

81
00:03:25,896 --> 00:03:27,360
You can't really do
anything with your data

82
00:03:27,360 --> 00:03:29,760
if you can't trust your data, right?

83
00:03:29,760 --> 00:03:32,010
So how do you build that
strong data foundation,

84
00:03:32,010 --> 00:03:33,450
and that strong data platform,

85
00:03:33,450 --> 00:03:37,499
that you can then deliver
successful AI projects on top of?

86
00:03:37,499 --> 00:03:38,370
And that is something
that we're talking to

87
00:03:38,370 --> 00:03:39,750
a lot of customers about,

88
00:03:39,750 --> 00:03:41,730
and they're telling us
that it's holding them back

89
00:03:41,730 --> 00:03:43,710
on achieving a lot of their vision

90
00:03:43,710 --> 00:03:46,743
around some of these projects
that they want to implement.

91
00:03:47,820 --> 00:03:51,060
So how do we start to
make progress, right?

92
00:03:51,060 --> 00:03:54,240
How can we start to build
on these data foundations

93
00:03:54,240 --> 00:03:55,800
and these data platforms to deliver

94
00:03:55,800 --> 00:03:57,510
on our successful projects?

95
00:03:57,510 --> 00:03:59,940
We at Qlik believe that it all starts

96
00:03:59,940 --> 00:04:02,520
with an open data architecture.

97
00:04:02,520 --> 00:04:03,870
And we'll talk about what that means,

98
00:04:03,870 --> 00:04:06,060
and how we can deliver on
that through the remainder

99
00:04:06,060 --> 00:04:07,683
of today's presentation.

100
00:04:09,124 --> 00:04:10,680
So what does an architecture look like

101
00:04:10,680 --> 00:04:13,320
in terms of these open foundations?

102
00:04:13,320 --> 00:04:15,000
So if you look across
the left of the screen,

103
00:04:15,000 --> 00:04:16,350
that's your data.

104
00:04:16,350 --> 00:04:18,502
Those are your data sources, right?

105
00:04:18,502 --> 00:04:20,241
This is where the data's gonna come from

106
00:04:20,241 --> 00:04:22,137
that you want to build
your projects on top of.

107
00:04:22,137 --> 00:04:24,270
And we believe in open architecture

108
00:04:24,270 --> 00:04:27,420
should support as many
sources as you might have,

109
00:04:27,420 --> 00:04:29,610
from the simple maybe API endpoints,

110
00:04:29,610 --> 00:04:31,470
or SAS type applications,

111
00:04:31,470 --> 00:04:35,190
to the more complicated maybe
database transactional systems

112
00:04:35,190 --> 00:04:37,290
that you may want to bring
into your data architectures,

113
00:04:37,290 --> 00:04:39,990
the Oracles of the world,
SQL servers of the world,

114
00:04:39,990 --> 00:04:43,212
PostgreSQL, all the way down
to the much more complicated

115
00:04:43,212 --> 00:04:46,620
streaming realtime data sources, right?

116
00:04:46,620 --> 00:04:49,680
Those sources that deliver
semi-structured data,

117
00:04:49,680 --> 00:04:52,440
those sources that
deliver data in real time,

118
00:04:52,440 --> 00:04:54,780
those sources who schema
evolves over time.

119
00:04:54,780 --> 00:04:57,180
All of these sources need to be brought in

120
00:04:57,180 --> 00:05:00,120
to a particular data architecture.

121
00:05:00,120 --> 00:05:02,970
Now that then data architecture
then needs to provide

122
00:05:02,970 --> 00:05:06,480
capabilities to help you
move and transform that data

123
00:05:06,480 --> 00:05:10,860
into that, that is of high quality.

124
00:05:10,860 --> 00:05:12,450
So a data platform that is open,

125
00:05:12,450 --> 00:05:13,770
needs to handle the data movement

126
00:05:13,770 --> 00:05:16,560
and that also needs to be
able to analyze and act

127
00:05:16,560 --> 00:05:19,830
on that data in the most
efficient way possible.

128
00:05:19,830 --> 00:05:22,500
And we believe that an
open data lake architecture

129
00:05:22,500 --> 00:05:26,430
is the best place to store
and manage your data,

130
00:05:26,430 --> 00:05:29,490
so that it is available to
as many different consumers

131
00:05:29,490 --> 00:05:32,253
as might want to consume it.

132
00:05:33,267 --> 00:05:34,920
These could be AI agents,

133
00:05:34,920 --> 00:05:37,560
these could be traditional
data consumers like analytical

134
00:05:37,560 --> 00:05:39,930
and query engines, right?

135
00:05:39,930 --> 00:05:42,900
So this is what a proposed
architecture looks like,

136
00:05:42,900 --> 00:05:46,110
and we think that iceberg
formatted lakehouse

137
00:05:46,110 --> 00:05:49,740
is the best approach
for storing your data.

138
00:05:49,740 --> 00:05:52,505
And there's two primary reasons for that.

139
00:05:52,505 --> 00:05:54,774
If you talk to customers
about why they're considering

140
00:05:54,774 --> 00:05:56,730
adopting iceberg or Lakehouse
solutions in general,

141
00:05:56,730 --> 00:06:00,450
most of those conversations
drive towards one,

142
00:06:00,450 --> 00:06:02,790
or two particular drivers.

143
00:06:02,790 --> 00:06:04,500
The first is cost, right?

144
00:06:04,500 --> 00:06:07,046
When building out these architectures

145
00:06:07,046 --> 00:06:09,630
that have to accommodate any
data source that we might have,

146
00:06:09,630 --> 00:06:11,943
from the small scale to the large scale,

147
00:06:12,785 --> 00:06:14,280
from the batch oriented to the real time,

148
00:06:14,280 --> 00:06:17,430
cost is going to be a factor
in bringing that data in.

149
00:06:17,430 --> 00:06:18,630
And Lakehouse technology,

150
00:06:18,630 --> 00:06:21,188
specifically those based on Iceberg,

151
00:06:21,188 --> 00:06:24,120
have proven to deliver the
most flexible data storage

152
00:06:24,120 --> 00:06:26,430
at the lowest possible cost.

153
00:06:26,430 --> 00:06:29,463
The second value proposition
is interoperability.

154
00:06:30,480 --> 00:06:32,580
We are trying to help organizations shift

155
00:06:32,580 --> 00:06:35,550
from what I would call platform
oriented architectures,

156
00:06:35,550 --> 00:06:38,370
where all data sits
inside of the platform,

157
00:06:38,370 --> 00:06:41,430
consuming that data, to
a more open architecture,

158
00:06:41,430 --> 00:06:44,160
where the data can sit inside
of an open architecture

159
00:06:44,160 --> 00:06:46,530
and then as many platforms as you want,

160
00:06:46,530 --> 00:06:49,227
can connect you and consume that data.

161
00:06:49,227 --> 00:06:52,860
And Iceberg serves as a very
powerful framework for that,

162
00:06:52,860 --> 00:06:55,470
as proven by what you
see over on the right,

163
00:06:55,470 --> 00:06:59,550
right across the industry
most vendors, AWS of course,

164
00:06:59,550 --> 00:07:02,130
leading the charge, have chosen Iceberg

165
00:07:02,130 --> 00:07:04,170
as a Lakehouse format that they're putting

166
00:07:04,170 --> 00:07:06,900
a lot of eggs into that basket.

167
00:07:06,900 --> 00:07:10,620
AWS has services like S3 tables,
a variety of glue services,

168
00:07:10,620 --> 00:07:15,180
SageMaker, EMR, Glue, Athena,
all of them very well support

169
00:07:15,180 --> 00:07:17,460
Iceberg within the AWS ecosystem.

170
00:07:17,460 --> 00:07:20,460
And across the industry, whether
data warehouse providers,

171
00:07:20,460 --> 00:07:24,210
other hyper scalers, variety
of consumption engines,

172
00:07:24,210 --> 00:07:27,030
they all have chosen Iceberg as a format

173
00:07:27,030 --> 00:07:29,730
and a technology that
they will support, right?

174
00:07:29,730 --> 00:07:32,550
So this interoperability
across the data ecosystem

175
00:07:32,550 --> 00:07:35,340
is a huge value add of
any Lakehouse technology,

176
00:07:35,340 --> 00:07:38,073
specifically those based on Iceberg.

177
00:07:38,910 --> 00:07:41,790
And Qlik, of course is
now in that world, right?

178
00:07:41,790 --> 00:07:45,600
We released earlier this
year the Qlik Open Lakehouse,

179
00:07:45,600 --> 00:07:48,330
that has introduced a
managed Iceberg offering

180
00:07:48,330 --> 00:07:51,483
within the Qlik product portfolio,

181
00:07:51,483 --> 00:07:52,770
that we can help customers
ingest data into,

182
00:07:52,770 --> 00:07:54,360
transform data within,

183
00:07:54,360 --> 00:07:57,150
and then present out to
their consumption engines.

184
00:07:57,150 --> 00:07:59,640
If I layer this on top of
the architecture diagram

185
00:07:59,640 --> 00:08:02,910
that you saw earlier, you can
see the Qlik Open Lakehouse,

186
00:08:02,910 --> 00:08:05,790
powered by Apache
Iceberg that runs on AWS,

187
00:08:05,790 --> 00:08:08,670
provides three primary capabilities.

188
00:08:08,670 --> 00:08:11,608
So first you'll see high
throughput ingestion,

189
00:08:11,608 --> 00:08:13,830
that's ingestion directly
into Iceberg tables,

190
00:08:13,830 --> 00:08:16,290
from all of the various
sources that I mentioned,

191
00:08:16,290 --> 00:08:18,540
from the simple API type endpoints,

192
00:08:18,540 --> 00:08:22,020
to the more complicated
database and CDC type endpoints,

193
00:08:22,020 --> 00:08:22,950
all the way through,

194
00:08:22,950 --> 00:08:25,380
we're happy to announce at this show,

195
00:08:25,380 --> 00:08:28,440
new support for streaming data ingestions,

196
00:08:28,440 --> 00:08:31,650
from streaming providers
like Amazon Kinesis,

197
00:08:31,650 --> 00:08:35,769
various versions of Kafka,
even Micro-Batch ingestions

198
00:08:35,769 --> 00:08:37,500
for many files that might
exist on object stores.

199
00:08:37,500 --> 00:08:39,390
All of that data can be very simply

200
00:08:39,390 --> 00:08:43,230
and cost-effectively
ingested into Iceberg tables.

201
00:08:43,230 --> 00:08:45,840
Now, beyond that Iceberg ingestion though,

202
00:08:45,840 --> 00:08:49,320
any customer who's tried
to use Iceberg at scale,

203
00:08:49,320 --> 00:08:52,890
will know that Iceberg,
if it's not optimized,

204
00:08:52,890 --> 00:08:54,900
will not deliver on its core value,

205
00:08:54,900 --> 00:08:57,210
especially real-time ingestion.

206
00:08:57,210 --> 00:08:59,220
Iceberg tables need to be optimized.

207
00:08:59,220 --> 00:09:02,040
You have to compact files for
efficient file level access.

208
00:09:02,040 --> 00:09:04,178
You have to expire snapshots,

209
00:09:04,178 --> 00:09:06,480
so your metadata files
remain reasonably sized.

210
00:09:06,480 --> 00:09:07,920
You have to delete orphan files,

211
00:09:07,920 --> 00:09:10,530
so that you're not
consuming terabytes on disc

212
00:09:10,530 --> 00:09:12,543
to process a hundred gigabyte data set.

213
00:09:13,645 --> 00:09:14,790
And the click adaptive optimizer will do

214
00:09:14,790 --> 00:09:16,260
all of that for you.

215
00:09:16,260 --> 00:09:19,710
As you ingest data into
Iceberg, our optimizer kicks in,

216
00:09:19,710 --> 00:09:23,760
and we continuously optimize
Iceberg for low cost,

217
00:09:23,760 --> 00:09:26,490
and great performance.

218
00:09:26,490 --> 00:09:28,950
And then finally we have
warehouse mirroring.

219
00:09:28,950 --> 00:09:30,600
And this is an important component

220
00:09:30,600 --> 00:09:34,230
of the interoperability
function that I mentioned.

221
00:09:34,230 --> 00:09:37,050
It's not good enough to
store your data in Iceberg,

222
00:09:37,050 --> 00:09:39,090
and just query it from a single engine.

223
00:09:39,090 --> 00:09:42,390
Iceberg tables need to be
consumable by as many possible

224
00:09:42,390 --> 00:09:45,120
consumption engines as
possible in order to deliver

225
00:09:45,120 --> 00:09:48,254
on that open perspective.

226
00:09:48,254 --> 00:09:50,160
So what we offer is we
ingest data into Iceberg,

227
00:09:50,160 --> 00:09:51,360
we optimize Iceberg,

228
00:09:51,360 --> 00:09:53,430
and then we can present that data

229
00:09:53,430 --> 00:09:55,560
to a variety of consumption engines

230
00:09:55,560 --> 00:09:57,870
from your data warehouses,
to your query engines,

231
00:09:57,870 --> 00:10:00,270
so that they can reach into
the Qlik Open Lakehouse,

232
00:10:00,270 --> 00:10:03,420
and run optimized queries
against Iceberg tables.

233
00:10:03,420 --> 00:10:05,553
And all of this runs on AWS.

234
00:10:06,990 --> 00:10:09,060
As an example of the cost efficiency

235
00:10:09,060 --> 00:10:10,560
that we're helping customers achieve,

236
00:10:10,560 --> 00:10:11,760
we actually ran a benchmark,

237
00:10:11,760 --> 00:10:14,340
we just published this
benchmark a few weeks ago.

238
00:10:14,340 --> 00:10:15,930
We took the exact same data,

239
00:10:15,930 --> 00:10:18,720
it was a real time streaming
ingestion into Iceberg,

240
00:10:18,720 --> 00:10:22,350
relatively, I'll call it
moderate volume, right?

241
00:10:22,350 --> 00:10:24,000
So not low scale, not high scale,

242
00:10:24,000 --> 00:10:26,025
kind of right in the middle.

243
00:10:26,025 --> 00:10:26,940
Probably a volume similar
to what many of you

244
00:10:26,940 --> 00:10:28,410
are working with today.

245
00:10:28,410 --> 00:10:31,170
We ingested that data in real time.

246
00:10:31,170 --> 00:10:33,450
This is a real time use case into a

247
00:10:33,450 --> 00:10:35,610
Qlik Open Lakehouse Iceberg table.

248
00:10:35,610 --> 00:10:37,950
And then conversely, we
ingested the same data

249
00:10:37,950 --> 00:10:39,990
directly into a data warehouse.

250
00:10:39,990 --> 00:10:43,050
To just showcase the cost
efficiency that you can see

251
00:10:43,050 --> 00:10:45,570
with Open Lakehouse
ingestion into Iceberg.

252
00:10:45,570 --> 00:10:47,880
The conclusion of the benchmark

253
00:10:47,880 --> 00:10:51,420
was that we were able to
deliver data freshness

254
00:10:51,420 --> 00:10:54,060
at a magnitude of five x faster, right?

255
00:10:54,060 --> 00:10:56,490
So data latency you can
see in the Open Lakehouse

256
00:10:56,490 --> 00:10:58,320
was in the one to three minute range,

257
00:10:58,320 --> 00:11:01,503
and the data warehouse was in
the five to 15 minute range.

258
00:11:04,135 --> 00:11:05,751
So again, five x faster data,

259
00:11:05,751 --> 00:11:10,452
and we could do that at a lower
cost in the 75 to 90% range.

260
00:11:10,452 --> 00:11:12,780
We actually ran the test on a
few different warehouse sizes

261
00:11:12,780 --> 00:11:15,780
from the very small to
the slightly larger.

262
00:11:15,780 --> 00:11:18,210
And we were able to showcase
not only fresher data

263
00:11:18,210 --> 00:11:20,730
into Iceberg, but at a much lower cost.

264
00:11:20,730 --> 00:11:22,410
And then again, you get to take advantage

265
00:11:22,410 --> 00:11:25,050
of the interoperability
of those Iceberg tables,

266
00:11:25,050 --> 00:11:27,270
that are now consumable across

267
00:11:27,270 --> 00:11:29,550
the widest range of query engines.

268
00:11:29,550 --> 00:11:31,290
So if you look online,

269
00:11:31,290 --> 00:11:33,030
we published this
benchmark a few weeks ago.

270
00:11:33,030 --> 00:11:35,190
There's a lot of detail
information on how you can

271
00:11:35,190 --> 00:11:36,870
replicate it yourself, but again,

272
00:11:36,870 --> 00:11:40,530
delivering on that cost
efficiency at scale objective,

273
00:11:40,530 --> 00:11:44,490
of getting data into Iceberg
in an efficient manner.

274
00:11:44,490 --> 00:11:48,090
So again, Open Lakehouse
benefits, very fresh,

275
00:11:48,090 --> 00:11:51,510
high speed realtime data
ingestion into Iceberg,

276
00:11:51,510 --> 00:11:53,250
at a very low cost.

277
00:11:53,250 --> 00:11:55,650
And with the adaptive
optimizer layered on top,

278
00:11:55,650 --> 00:11:57,750
so that you get great
query performance out

279
00:11:57,750 --> 00:12:00,360
of your Iceberg tables,
again, while benefiting

280
00:12:00,360 --> 00:12:02,313
from the cost savings that we see,

281
00:12:03,170 --> 00:12:05,220
by ingesting data into an Open Lakehouse.

282
00:12:06,330 --> 00:12:08,340
So that's what I wanted
to talk about in slides.

283
00:12:08,340 --> 00:12:10,920
I did prepare a quick demo if
you wanna see how this works,

284
00:12:10,920 --> 00:12:14,040
but I will say if you wanna
see an even better demo

285
00:12:14,040 --> 00:12:16,777
than the one I'm gonna show
you, come to the click booth.

286
00:12:16,777 --> 00:12:19,260
It is literally as far
that way as you can walk

287
00:12:19,260 --> 00:12:20,610
in the expo hall.

288
00:12:20,610 --> 00:12:24,716
So if you go way that way, you
can listen for the cowbells

289
00:12:24,716 --> 00:12:26,100
or see people huffing
and puffing on bikes,

290
00:12:26,100 --> 00:12:27,780
then you'll know you're
in the right space.

291
00:12:27,780 --> 00:12:31,020
What this demo is, this
is a real streaming data,

292
00:12:31,020 --> 00:12:35,280
IoT style use case where you
can generate your own data.

293
00:12:35,280 --> 00:12:37,860
So you get on the bikes, you
pedal as fast as you want,

294
00:12:37,860 --> 00:12:39,630
during your ride, you're gonna generate

295
00:12:39,630 --> 00:12:41,730
about 300 to 500 meter readings

296
00:12:41,730 --> 00:12:43,970
that we're pulling off of the bike,

297
00:12:43,970 --> 00:12:47,424
realtime sensor data that we
then load into Amazon Kinesis,

298
00:12:47,424 --> 00:12:48,630
a realtime streaming platform.

299
00:12:48,630 --> 00:12:51,600
We ingest that data into
Iceberg tables in real time.

300
00:12:51,600 --> 00:12:52,890
We optimize those tables

301
00:12:52,890 --> 00:12:54,810
so that we get great query performance,

302
00:12:54,810 --> 00:12:56,790
and then we're providing
analytics capabilities

303
00:12:56,790 --> 00:12:58,620
on top of the data that you've generated

304
00:12:58,620 --> 00:13:00,120
while riding the bike.

305
00:13:00,120 --> 00:13:02,040
So it's both kind of
a fun user experience,

306
00:13:02,040 --> 00:13:06,180
but also a real world example
of real time streaming data

307
00:13:06,180 --> 00:13:07,893
ingestion into Iceberg.

308
00:13:08,790 --> 00:13:11,721
And if you wanna see how this works,

309
00:13:11,721 --> 00:13:13,320
I'm going to keep my fingers crossed

310
00:13:13,320 --> 00:13:15,840
that I can switch to a demo. Perfect.

311
00:13:15,840 --> 00:13:18,810
Now those of you know, the
wifi in here is not great.

312
00:13:18,810 --> 00:13:21,695
So I am gonna take
advantage of a recording.

313
00:13:21,695 --> 00:13:22,528
I normally don't do this.

314
00:13:22,528 --> 00:13:24,090
I like to do demos live,

315
00:13:24,090 --> 00:13:27,300
but so that I'm not
suffering from wifi issues.

316
00:13:27,300 --> 00:13:28,770
I'm just gonna run the recording,

317
00:13:28,770 --> 00:13:31,230
and I'll kind of talk you
through what we're seeing here.

318
00:13:31,230 --> 00:13:34,740
So this is our click town cloud platform,

319
00:13:34,740 --> 00:13:38,160
specifically our Open Lakehouse solution.

320
00:13:38,160 --> 00:13:40,680
And we're gonna build a
streaming ingestion project.

321
00:13:40,680 --> 00:13:42,120
We'll give our product a name,

322
00:13:42,120 --> 00:13:45,120
and we're gonna define the Iceberg details

323
00:13:45,120 --> 00:13:47,736
of how we want to ingest this data.

324
00:13:47,736 --> 00:13:49,530
So the catalog, we're gonna use AWS glue,

325
00:13:49,530 --> 00:13:52,133
the storage engine, AWS S3,

326
00:13:52,133 --> 00:13:56,160
and then a compute cluster
that runs in AWS to process

327
00:13:56,160 --> 00:13:59,910
that data into Iceberg and
optimize that data continuously

328
00:13:59,910 --> 00:14:01,323
as it's being ingested.

329
00:14:02,524 --> 00:14:04,389
We step through a data onboarding wizard.

330
00:14:04,389 --> 00:14:05,970
This is where we select our
Amazon Kinesis connection.

331
00:14:05,970 --> 00:14:08,010
This is where those real time events

332
00:14:08,010 --> 00:14:09,480
are gonna get loaded to.

333
00:14:09,480 --> 00:14:11,730
And we have two Kinesis
streams in this account.

334
00:14:11,730 --> 00:14:14,190
This is like an e-commerce type data set.

335
00:14:14,190 --> 00:14:17,347
And we're gonna pull those
two different Kinesis streams

336
00:14:17,347 --> 00:14:19,042
into Open Lakehouse.

337
00:14:19,042 --> 00:14:19,875
Our data's in JSON format.

338
00:14:19,875 --> 00:14:21,570
We do support a variety of formats.

339
00:14:21,570 --> 00:14:23,880
So any type of data, whether
it's highly structured,

340
00:14:23,880 --> 00:14:25,920
or less structured, we can pull in

341
00:14:25,920 --> 00:14:28,140
and ingest as a part of this process.

342
00:14:28,140 --> 00:14:30,840
And we have options to
determine how far back

343
00:14:30,840 --> 00:14:34,050
do we wanna read, how do we
want to handle nested data?

344
00:14:34,050 --> 00:14:37,114
If you're using like JSON,
and if you've got structs,

345
00:14:37,114 --> 00:14:39,450
or arrays, we can either keep
it nested or flatten it out,

346
00:14:39,450 --> 00:14:40,800
we can append or merge.

347
00:14:40,800 --> 00:14:43,170
And then we can also define
the partitioning of the table

348
00:14:43,170 --> 00:14:45,090
that we're ingesting into.

349
00:14:45,090 --> 00:14:46,320
So we build out the project,

350
00:14:46,320 --> 00:14:49,860
here's a picture of an
end-to-end pipeline from Kinesis

351
00:14:49,860 --> 00:14:53,670
into landing for raw data and
then processed into Iceberg.

352
00:14:53,670 --> 00:14:55,920
And from here now we've
got a running pipeline

353
00:14:55,920 --> 00:14:59,010
that in real time is taking
data from source event

354
00:14:59,010 --> 00:15:02,280
in Kinesis into a queryable Iceberg table

355
00:15:02,280 --> 00:15:04,290
at very low latency, right?

356
00:15:04,290 --> 00:15:06,200
Over the course of the event,

357
00:15:06,200 --> 00:15:08,670
we've done almost 2 million
bike sensor readings,

358
00:15:08,670 --> 00:15:11,190
and growing, so we'll probably
be much higher than that

359
00:15:11,190 --> 00:15:12,600
after the event finishes.

360
00:15:12,600 --> 00:15:14,790
But this is just an example
of a running pipeline

361
00:15:14,790 --> 00:15:17,130
processing data into Iceberg.

362
00:15:17,130 --> 00:15:19,950
And here's an example of
AWS Athena then querying

363
00:15:19,950 --> 00:15:23,730
that Iceberg table with near
realtime freshness, right?

364
00:15:23,730 --> 00:15:27,506
So whether you're using Athena
or any other query engine

365
00:15:27,506 --> 00:15:28,380
that you might have access to,

366
00:15:28,380 --> 00:15:31,890
you can run those queries
against the Iceberg tables.

367
00:15:31,890 --> 00:15:34,110
Lastly, how do you trust that data?

368
00:15:34,110 --> 00:15:35,560
The click town cloud platform

369
00:15:36,903 --> 00:15:37,860
has this concept of data quality.

370
00:15:37,860 --> 00:15:40,290
Every data set that we
ingest data into is assigned

371
00:15:40,290 --> 00:15:42,270
a trust score that's based on accuracy,

372
00:15:42,270 --> 00:15:46,110
validity, semantic quality
of that data, completeness,

373
00:15:46,110 --> 00:15:48,210
so that you can also know
not only are we ingesting

374
00:15:48,210 --> 00:15:50,790
this data Into Iceberg,
but we're delivering data

375
00:15:50,790 --> 00:15:52,541
that you can trust.

376
00:15:52,541 --> 00:15:53,550
It's of the highest quality.

377
00:15:53,550 --> 00:15:56,250
And you can measure that
and monitor that over time.

378
00:15:56,250 --> 00:15:57,990
So that's the click town cloud platform.

379
00:15:57,990 --> 00:16:01,170
And that's the example of a
real-time streaming ingestion

380
00:16:01,170 --> 00:16:02,610
into Iceberg.

381
00:16:02,610 --> 00:16:05,010
But very simple. You set it up once.

382
00:16:05,010 --> 00:16:07,860
You set it in, forget it,
everything runs continuously.

383
00:16:07,860 --> 00:16:09,420
Everything is optimized for you.

384
00:16:09,420 --> 00:16:11,400
Cost is managed and kept low,

385
00:16:11,400 --> 00:16:14,310
but that data is accessible
to the widest possible

386
00:16:14,310 --> 00:16:17,460
variety of query engines.

387
00:16:17,460 --> 00:16:19,800
So again, if you'd like
to try out this use case

388
00:16:19,800 --> 00:16:21,960
for yourself, definitely
come by the booth.

389
00:16:21,960 --> 00:16:24,090
You can either just ride
the bikes and work up

390
00:16:24,090 --> 00:16:26,070
a little sweat or it's only
about a 30 second ride,

391
00:16:26,070 --> 00:16:28,800
you probably won't work
up too much of a sweat,

392
00:16:28,800 --> 00:16:30,690
but you can kind of try
out the bikes for yourself,

393
00:16:30,690 --> 00:16:33,000
generate some data, and
then we can walk you through

394
00:16:33,000 --> 00:16:35,340
the end-to-end data
pipeline that you yourself

395
00:16:35,340 --> 00:16:37,350
have participated in and kind of see

396
00:16:37,350 --> 00:16:40,320
how that data's ingested, processed,

397
00:16:40,320 --> 00:16:41,910
and then finally stored an Iceberg

398
00:16:41,910 --> 00:16:46,063
and served up to a variety
of analytics use cases.

399
00:16:46,063 --> 00:16:48,570
So I do wanna thank everyone
for your time today.

400
00:16:48,570 --> 00:16:52,020
I'll peel off over there
if anyone has any questions

401
00:16:52,020 --> 00:16:54,840
that you want to ask before
the next session starts.

402
00:16:54,840 --> 00:16:57,102
Again, if you wanna learn more about Qlik,

403
00:16:57,102 --> 00:16:57,935
please come find us in the booth.

404
00:16:57,935 --> 00:16:59,760
Walk that way as far as you can.

405
00:16:59,760 --> 00:17:03,333
Listen for the cowboys, cowboys, cowbells.

406
00:17:04,658 --> 00:17:07,950
Listen or watch for the
spinning click logo, come over,

407
00:17:07,950 --> 00:17:09,450
ride the bike, see a demo.

408
00:17:09,450 --> 00:17:11,160
We'd love to talk further.

409
00:17:11,160 --> 00:17:12,360
Thank you all for your time today.

410
00:17:12,360 --> 00:17:16,110
I know it's a jam packed agenda,

411
00:17:16,110 --> 00:17:18,330
and I'm very thankful that
you were able to spend

412
00:17:18,330 --> 00:17:19,860
20 minutes with me here today.

413
00:17:19,860 --> 00:17:21,150
You can come find me right over there,

414
00:17:21,150 --> 00:17:24,000
or in the click booth over
the rest of the conference.

415
00:17:24,000 --> 00:17:24,950
So thanks everyone.

