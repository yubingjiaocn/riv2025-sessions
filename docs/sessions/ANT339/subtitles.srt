1
00:00:01,170 --> 00:00:04,540
- Thank you all for attending our session

2
00:00:05,730 --> 00:00:08,610
that talks about unstructured data

3
00:00:08,610 --> 00:00:10,710
and different data modalities

4
00:00:10,710 --> 00:00:13,863
and how can you build a
strong foundations for AI.

5
00:00:15,450 --> 00:00:17,147
We're very excited about this session.

6
00:00:17,147 --> 00:00:19,350
We are excited to share
some of our experience

7
00:00:19,350 --> 00:00:21,720
from the real world as well.

8
00:00:21,720 --> 00:00:24,720
And we couldn't be more
thrilled to be here with you.

9
00:00:24,720 --> 00:00:25,830
With me on the stage,

10
00:00:25,830 --> 00:00:28,830
I have Avinash Reddy Erupaka,

11
00:00:28,830 --> 00:00:33,450
who's our speaker from customer side.

12
00:00:33,450 --> 00:00:37,800
He's a leading principal
engineering at Bayer.

13
00:00:37,800 --> 00:00:39,300
I also have Shiv Narayanan.

14
00:00:39,300 --> 00:00:42,303
He's our product lead for
SageMaker Unified Studio.

15
00:00:43,140 --> 00:00:46,320
Throughout our session, we're gonna have,

16
00:00:46,320 --> 00:00:47,790
be looking at multiple demos

17
00:00:47,790 --> 00:00:50,610
as well as some of the
real-world examples.

18
00:00:50,610 --> 00:00:53,493
So thank you, Avinash and Shiv.

19
00:00:54,900 --> 00:00:57,240
So I just wanna start by
asking, how's everyone doing?

20
00:00:57,240 --> 00:00:58,950
I know it's a speaker,

21
00:00:58,950 --> 00:01:00,210
I cannot hear you and all that,

22
00:01:00,210 --> 00:01:01,110
but it's all good.

23
00:01:01,110 --> 00:01:02,190
Thumbs up is good.

24
00:01:02,190 --> 00:01:03,540
So what we have done today

25
00:01:03,540 --> 00:01:06,180
is we have structured our conversation

26
00:01:06,180 --> 00:01:10,470
and story in these five or
four different sections.

27
00:01:10,470 --> 00:01:12,810
We're gonna talk about
a very important topic

28
00:01:12,810 --> 00:01:15,600
called data readiness for AI, right?

29
00:01:15,600 --> 00:01:18,030
We are talking about
unstructured data today,

30
00:01:18,030 --> 00:01:19,800
among other modalities,

31
00:01:19,800 --> 00:01:21,690
but it is important for us to align

32
00:01:21,690 --> 00:01:23,700
and land on this foundation

33
00:01:23,700 --> 00:01:25,680
that talks about how you
build your data platform

34
00:01:25,680 --> 00:01:28,590
which is ready for
agentic AI and workflows

35
00:01:28,590 --> 00:01:29,423
and things like that.

36
00:01:29,423 --> 00:01:31,890
We're also gonna look at
different data modalities.

37
00:01:31,890 --> 00:01:33,000
So my name is Navneet.

38
00:01:33,000 --> 00:01:35,820
I lead the data and analytics for AWS

39
00:01:35,820 --> 00:01:37,440
for life sciences customers.

40
00:01:37,440 --> 00:01:38,640
So most of my examples

41
00:01:38,640 --> 00:01:40,890
are coming from the life sciences world.

42
00:01:40,890 --> 00:01:42,570
Having said that,

43
00:01:42,570 --> 00:01:44,310
a lot of my other partners and colleagues

44
00:01:44,310 --> 00:01:46,950
in the organization work
with different industries.

45
00:01:46,950 --> 00:01:48,180
I see a lot of similarities.

46
00:01:48,180 --> 00:01:50,610
We're gonna talk about that too.

47
00:01:50,610 --> 00:01:52,950
Then we're gonna look in the demos

48
00:01:52,950 --> 00:01:56,010
for Amazon SageMaker Unified Studio,

49
00:01:56,010 --> 00:01:57,780
Amazon SageMaker Governance, that:

50
00:01:57,780 --> 00:02:01,050
how can you build a
platform that can serve

51
00:02:01,050 --> 00:02:01,920
not only structured,

52
00:02:01,920 --> 00:02:02,910
but unstructured data?

53
00:02:02,910 --> 00:02:04,800
And finally, we're gonna hear from Avinash

54
00:02:04,800 --> 00:02:06,673
on the real-world examples,

55
00:02:06,673 --> 00:02:09,063
real-world use cases of Bayer.

56
00:02:11,310 --> 00:02:12,710
I wanna start by this slide,

57
00:02:13,680 --> 00:02:16,200
which I'm sure you all
are very familiar with,

58
00:02:16,200 --> 00:02:18,810
not maybe the slide,
but the concept, right?

59
00:02:18,810 --> 00:02:19,740
So as you know,

60
00:02:19,740 --> 00:02:22,470
that the lot of interest

61
00:02:22,470 --> 00:02:25,080
and a lot of work that is being done

62
00:02:25,080 --> 00:02:28,230
in building generative
AI and AI applications,

63
00:02:28,230 --> 00:02:29,793
and agentic AI applications.

64
00:02:30,660 --> 00:02:32,730
This is a 40,000-feet view maybe.

65
00:02:32,730 --> 00:02:34,110
Of course we are not drilling down.

66
00:02:34,110 --> 00:02:35,760
But if you look at this pyramid,

67
00:02:35,760 --> 00:02:38,640
at the bottom of all this is
the data foundations, right?

68
00:02:38,640 --> 00:02:42,000
And data foundation is
something which is evolving

69
00:02:42,000 --> 00:02:45,330
and changing with the need and, you know,

70
00:02:45,330 --> 00:02:48,270
the kind of modalities that we have

71
00:02:48,270 --> 00:02:50,880
and also the type of
agents that we are writing.

72
00:02:50,880 --> 00:02:52,350
So to write an AI application,

73
00:02:52,350 --> 00:02:53,370
to write agentic AI application,

74
00:02:53,370 --> 00:02:55,680
you need to rely on the data foundations.

75
00:02:55,680 --> 00:02:58,440
And today we're gonna talk
about how you build that,

76
00:02:58,440 --> 00:03:00,000
what are some of the best practices,

77
00:03:00,000 --> 00:03:02,370
what can you use on AWS, as an example.

78
00:03:02,370 --> 00:03:04,980
So again, we're gonna
start with this today.

79
00:03:04,980 --> 00:03:06,330
So a few years back,

80
00:03:06,330 --> 00:03:09,390
there was a paradigm in
data called data mesh.

81
00:03:09,390 --> 00:03:10,740
Have you heard of that?

82
00:03:10,740 --> 00:03:13,110
Some of you might have heard of data mesh.

83
00:03:13,110 --> 00:03:15,420
And it was an interesting
paradigm, actually, very,

84
00:03:15,420 --> 00:03:17,640
very timely at that point

85
00:03:17,640 --> 00:03:19,170
because that was the first time

86
00:03:19,170 --> 00:03:23,310
when someone was building
a paradigm shift on data

87
00:03:23,310 --> 00:03:25,200
that involves people,
process, and governance.

88
00:03:25,200 --> 00:03:27,210
Not that the governance
wasn't done before,

89
00:03:27,210 --> 00:03:28,043
it was done before.

90
00:03:28,043 --> 00:03:29,790
There were MDMs, there
were data governance leads,

91
00:03:29,790 --> 00:03:30,960
and things like that,

92
00:03:30,960 --> 00:03:33,120
but the distribution of data assets

93
00:03:33,120 --> 00:03:34,380
across your organization

94
00:03:34,380 --> 00:03:37,530
was something that data
mesh was targeting,

95
00:03:37,530 --> 00:03:39,930
along with people,
process, and technology.

96
00:03:39,930 --> 00:03:41,070
In last couple of years,

97
00:03:41,070 --> 00:03:42,660
we have seen the same paradigm shift

98
00:03:42,660 --> 00:03:43,980
on the generative AI side, right?

99
00:03:43,980 --> 00:03:46,680
So keep that data mesh concept in mind.

100
00:03:46,680 --> 00:03:48,630
I'll try to merge them together here,

101
00:03:48,630 --> 00:03:51,390
because how can you leverage
your existing platform

102
00:03:51,390 --> 00:03:53,250
in actually serving up your agents?

103
00:03:53,250 --> 00:03:55,710
So we started from traditional AI/ML.

104
00:03:55,710 --> 00:03:58,200
This is an era where
you're writing neural nets,

105
00:03:58,200 --> 00:04:00,120
deep learning models,
machine learning models,

106
00:04:00,120 --> 00:04:02,550
statistical analysis,
regressions, you name it.

107
00:04:02,550 --> 00:04:03,870
And most of the uses there

108
00:04:03,870 --> 00:04:06,030
was you have a notebook

109
00:04:06,030 --> 00:04:08,550
or RStudio or something like that,

110
00:04:08,550 --> 00:04:10,890
which is enabling your data
scientists to run analysis;

111
00:04:10,890 --> 00:04:12,480
and this is the typical.

112
00:04:12,480 --> 00:04:14,370
Now then came the generative AI,

113
00:04:14,370 --> 00:04:16,620
so large language model
building summarization,

114
00:04:16,620 --> 00:04:17,670
building notes,

115
00:04:17,670 --> 00:04:19,290
and building other sort of content

116
00:04:19,290 --> 00:04:21,810
that can be leveraged for your business.

117
00:04:21,810 --> 00:04:26,700
But with the introduction of
generative AI-based workload,

118
00:04:26,700 --> 00:04:29,130
there's another thing that
was very much important

119
00:04:29,130 --> 00:04:29,963
along with the data,

120
00:04:29,963 --> 00:04:31,020
is the context, right?

121
00:04:31,020 --> 00:04:33,750
That's where you have
technologies, RAG and stuff.

122
00:04:33,750 --> 00:04:36,000
Lately we are seeing a lot of interest

123
00:04:36,000 --> 00:04:38,700
and a lot of workloads

124
00:04:38,700 --> 00:04:40,290
that customers running building agents,

125
00:04:40,290 --> 00:04:42,600
which is basically, in my opinion,

126
00:04:42,600 --> 00:04:46,440
a unit of work that gets
a specific business metric

127
00:04:46,440 --> 00:04:48,180
or a specific work done for you.

128
00:04:48,180 --> 00:04:50,030
And we are looking at
multi-agent collaboration now.

129
00:04:50,030 --> 00:04:51,480
So it's not only one agent,

130
00:04:51,480 --> 00:04:54,000
but it's a combination of
agent with orchestration,

131
00:04:54,000 --> 00:04:56,010
with chain-of-thought
reasoning, and all those things,

132
00:04:56,010 --> 00:04:57,660
that is helping you building a workload

133
00:04:57,660 --> 00:04:59,550
that serve up for your business.

134
00:04:59,550 --> 00:05:02,010
And there are several
examples in the space.

135
00:05:02,010 --> 00:05:05,250
What is important when you
think about building data

136
00:05:05,250 --> 00:05:07,200
and you think about building these agents?

137
00:05:07,200 --> 00:05:09,000
The one common theme between the two

138
00:05:09,000 --> 00:05:11,730
is that how are you
utilizing your data platform

139
00:05:11,730 --> 00:05:12,720
to serve up these agents

140
00:05:12,720 --> 00:05:14,160
and how that is any different

141
00:05:14,160 --> 00:05:15,870
from what you were doing previously.

142
00:05:15,870 --> 00:05:17,340
As you see in this slide, at the top,

143
00:05:17,340 --> 00:05:18,660
we have the foundation model,

144
00:05:18,660 --> 00:05:20,220
the models that you would be utilizing

145
00:05:20,220 --> 00:05:22,020
to build those agents, right?

146
00:05:22,020 --> 00:05:23,460
But that alone is not sufficient.

147
00:05:23,460 --> 00:05:25,140
You need enterprise knowledge,

148
00:05:25,140 --> 00:05:27,690
as it says in the bottom of this slide.

149
00:05:27,690 --> 00:05:29,700
So think about the shift in the data,

150
00:05:29,700 --> 00:05:31,350
a paradigm shift with data mesh,

151
00:05:31,350 --> 00:05:34,050
and think about the paradigm
shift in the gen AI, which is,

152
00:05:34,050 --> 00:05:35,040
in this slide we're calling it

153
00:05:35,040 --> 00:05:36,750
multi-agent collaboration or a mesh.

154
00:05:36,750 --> 00:05:39,450
So data is no longer the only product

155
00:05:39,450 --> 00:05:41,310
that you're sharing
across the organization.

156
00:05:41,310 --> 00:05:44,190
You are also sharing
agents and other things.

157
00:05:44,190 --> 00:05:46,410
So AI is only smart as your data.

158
00:05:46,410 --> 00:05:48,210
Like, we all understand that.

159
00:05:48,210 --> 00:05:49,380
And data can be a different type.

160
00:05:49,380 --> 00:05:51,840
Now, this is a life sciences example.

161
00:05:51,840 --> 00:05:53,220
In your organization,

162
00:05:53,220 --> 00:05:55,200
you may be having your existing data,

163
00:05:55,200 --> 00:05:56,550
that is your relational databases,

164
00:05:56,550 --> 00:05:58,950
your warehouses, your S3 data,

165
00:05:58,950 --> 00:05:59,970
your your S3 tables,

166
00:05:59,970 --> 00:06:01,570
or one of our partner solutions.

167
00:06:02,490 --> 00:06:04,980
This data typically is largely structured

168
00:06:04,980 --> 00:06:06,090
and also unstructured.

169
00:06:06,090 --> 00:06:07,740
But largely what we have seen mainly

170
00:06:07,740 --> 00:06:09,900
is that you have rows, columns,

171
00:06:09,900 --> 00:06:11,250
some sort of a packet document,

172
00:06:11,250 --> 00:06:12,780
though stored raw,

173
00:06:12,780 --> 00:06:15,090
but you can query that.

174
00:06:15,090 --> 00:06:16,230
The new data that you're creating...

175
00:06:16,230 --> 00:06:18,543
And this is where we're
seeing a lot of data

176
00:06:18,543 --> 00:06:21,000
which is getting created
in an unstructured way.

177
00:06:21,000 --> 00:06:22,380
In my example

178
00:06:22,380 --> 00:06:25,200
and then Avinash will talk
about that later also,

179
00:06:25,200 --> 00:06:27,180
that a lot of data coming
from these instruments,

180
00:06:27,180 --> 00:06:28,020
the lab instruments,

181
00:06:28,020 --> 00:06:30,270
a lot of data coming from
your genomics information

182
00:06:30,270 --> 00:06:31,890
and gene information,

183
00:06:31,890 --> 00:06:34,320
and other verticals too,
like social networking,

184
00:06:34,320 --> 00:06:35,970
your clinical notes, your images, PDFs,

185
00:06:35,970 --> 00:06:37,200
and things like that,

186
00:06:37,200 --> 00:06:38,310
all the data that you're creating

187
00:06:38,310 --> 00:06:40,560
is largely unstructured, along with...

188
00:06:40,560 --> 00:06:41,700
Now, you have to work with both.

189
00:06:41,700 --> 00:06:43,680
You cannot leave the structured data.

190
00:06:43,680 --> 00:06:45,870
You have to make sure that your guidance

191
00:06:45,870 --> 00:06:48,570
and your implementation
has both of those things.

192
00:06:48,570 --> 00:06:49,890
And then the third-party data.

193
00:06:49,890 --> 00:06:52,590
This is what you acquire
from other partners,

194
00:06:52,590 --> 00:06:54,720
other sources, and things like that.

195
00:06:54,720 --> 00:06:57,300
So how you combine these
different data assets

196
00:06:57,300 --> 00:06:59,910
and spread that in a
data mesh-like setting

197
00:06:59,910 --> 00:07:02,430
with the right pillars

198
00:07:02,430 --> 00:07:04,950
so that you can serve
up your agentic platform

199
00:07:04,950 --> 00:07:06,720
as well as the human usage

200
00:07:06,720 --> 00:07:08,070
and things like that?

201
00:07:08,070 --> 00:07:10,620
An example of a multimodal data science...

202
00:07:10,620 --> 00:07:13,110
Now, even though you're
not, say, for example,

203
00:07:13,110 --> 00:07:15,390
if you're not from the life
sciences/healthcare industry,

204
00:07:15,390 --> 00:07:16,920
it still is very much relevant for you.

205
00:07:16,920 --> 00:07:18,810
At the center of this diagram,

206
00:07:18,810 --> 00:07:21,420
we have somebody called a patient, right?

207
00:07:21,420 --> 00:07:24,750
A patient is someone who
has certain data records

208
00:07:24,750 --> 00:07:26,490
like admissions, discharges,

209
00:07:26,490 --> 00:07:28,980
labs, you know, conditions, procedures.

210
00:07:28,980 --> 00:07:29,813
We all have that, right?

211
00:07:29,813 --> 00:07:32,850
We've all been to different
healthcare facilities

212
00:07:32,850 --> 00:07:33,870
and we have this data.

213
00:07:33,870 --> 00:07:35,100
Now, if you note that,

214
00:07:35,100 --> 00:07:36,450
that you have the EHR data,

215
00:07:36,450 --> 00:07:38,040
which is largely structured,

216
00:07:38,040 --> 00:07:40,020
other than the clinical
notes and the summaries,

217
00:07:40,020 --> 00:07:42,330
but you also have the clinical notes,

218
00:07:42,330 --> 00:07:44,190
the dictation in physician room

219
00:07:44,190 --> 00:07:45,600
or a vaccination data,

220
00:07:45,600 --> 00:07:47,610
or a social/geographic data.

221
00:07:47,610 --> 00:07:49,710
This comprises of a multimodality

222
00:07:49,710 --> 00:07:51,120
in any organization, right?

223
00:07:51,120 --> 00:07:54,150
So you wanna make sure that
as you look at this example,

224
00:07:54,150 --> 00:07:56,250
is that identify the modalities

225
00:07:56,250 --> 00:07:58,950
in your specific lines of business

226
00:07:58,950 --> 00:08:01,140
and identify the
different type of modality

227
00:08:01,140 --> 00:08:03,360
and then create a common model

228
00:08:03,360 --> 00:08:04,830
that can help you govern the data,

229
00:08:04,830 --> 00:08:06,300
that can help help you build the agents

230
00:08:06,300 --> 00:08:07,380
and things like that.

231
00:08:07,380 --> 00:08:08,213
On the right hand side of the patient,

232
00:08:08,213 --> 00:08:09,630
you have the omics data.

233
00:08:09,630 --> 00:08:12,000
Now, this also has a large
volume, like, for example,

234
00:08:12,000 --> 00:08:16,410
microbiome or metabolome or the proteome.

235
00:08:16,410 --> 00:08:20,400
All this data that you have
is largely unstructured

236
00:08:20,400 --> 00:08:24,750
and requires a great deal
of analysis that you need.

237
00:08:24,750 --> 00:08:26,347
Now, you might ask that,

238
00:08:26,347 --> 00:08:27,930
"I've been building my data agents,

239
00:08:27,930 --> 00:08:29,100
I've been building my data platform

240
00:08:29,100 --> 00:08:29,940
for a number of years now.

241
00:08:29,940 --> 00:08:31,770
My data is very good.

242
00:08:31,770 --> 00:08:33,060
I don't have to do anything else.

243
00:08:33,060 --> 00:08:36,780
What is that I need to do to make my data,

244
00:08:36,780 --> 00:08:38,220
unstructured and structured data,

245
00:08:38,220 --> 00:08:40,200
something like what we've
seen in previous slide,

246
00:08:40,200 --> 00:08:41,310
ready for the AI?"

247
00:08:41,310 --> 00:08:43,380
Now, this is a very small example.

248
00:08:43,380 --> 00:08:44,370
On the left, you see,

249
00:08:44,370 --> 00:08:45,450
which is the current state,

250
00:08:45,450 --> 00:08:47,310
which is a state of data scientists,

251
00:08:47,310 --> 00:08:49,920
data engineers interacting with the data.

252
00:08:49,920 --> 00:08:50,940
As you can see,

253
00:08:50,940 --> 00:08:53,610
a researcher investigating
drug interaction

254
00:08:53,610 --> 00:08:57,000
knows where to start: a
database, a molecular database.

255
00:08:57,000 --> 00:08:58,740
They can then go and
understand the significance

256
00:08:58,740 --> 00:09:01,080
of specific protein bindings
and things like that.

257
00:09:01,080 --> 00:09:04,230
So they start with a very
narrow focus approach

258
00:09:04,230 --> 00:09:06,510
and they know exactly
what they're looking for.

259
00:09:06,510 --> 00:09:09,027
Now, you may be having in
the right best possible way,

260
00:09:09,027 --> 00:09:10,470
the right quality, the right access,

261
00:09:10,470 --> 00:09:11,730
and things like that,

262
00:09:11,730 --> 00:09:12,563
and they're good with that.

263
00:09:12,563 --> 00:09:14,970
The challenge: that it
doesn't scale, right?

264
00:09:14,970 --> 00:09:16,830
So if you scale that to hundreds

265
00:09:16,830 --> 00:09:18,120
and thousands of data scientists,

266
00:09:18,120 --> 00:09:19,020
it you scale it to hundreds

267
00:09:19,020 --> 00:09:20,520
and thousands different data models,

268
00:09:20,520 --> 00:09:23,310
chances are you need something
that can help you scale.

269
00:09:23,310 --> 00:09:25,297
Which brings us back on the
other side, which is that,

270
00:09:25,297 --> 00:09:27,960
"Hey, I can write agents
that can go against this data

271
00:09:27,960 --> 00:09:29,400
and really scale it further."

272
00:09:29,400 --> 00:09:30,750
What is the challenge there?

273
00:09:30,750 --> 00:09:32,730
It starts with speculative
exploration, right?

274
00:09:32,730 --> 00:09:33,750
Because you're pretty much throwing

275
00:09:33,750 --> 00:09:35,670
every single data elements to those agents

276
00:09:35,670 --> 00:09:37,110
and then they're doing some...

277
00:09:37,110 --> 00:09:38,617
For example, in this case I have:

278
00:09:38,617 --> 00:09:40,440
"An agent is tasked with analyzing

279
00:09:40,440 --> 00:09:42,870
clinical trial effectiveness,

280
00:09:42,870 --> 00:09:45,540
might initially request all
the available trial data

281
00:09:45,540 --> 00:09:47,340
and then further filter it down."

282
00:09:47,340 --> 00:09:48,930
So, what is the right balance?

283
00:09:48,930 --> 00:09:50,340
And the reason it's important for us

284
00:09:50,340 --> 00:09:52,140
to know it now more than ever

285
00:09:52,140 --> 00:09:54,270
is that when you think
about the right balance

286
00:09:54,270 --> 00:09:56,979
between how much you want
to provide to the agents

287
00:09:56,979 --> 00:09:59,517
and how much you don't want
to provide to the agents,

288
00:09:59,517 --> 00:10:01,350
the data modality plays a big role.

289
00:10:01,350 --> 00:10:03,420
For unstructured data, a lot of data,

290
00:10:03,420 --> 00:10:04,885
something that an agent can

291
00:10:04,885 --> 00:10:05,910
(crackling drowns out Navneet)

292
00:10:05,910 --> 00:10:06,743
the value out of it.

293
00:10:06,743 --> 00:10:08,280
For structured, you may be requiring

294
00:10:08,280 --> 00:10:10,730
far more governance control
and things like that.

295
00:10:11,910 --> 00:10:13,470
So we have these building blocks

296
00:10:13,470 --> 00:10:16,050
that you think about,
again, at a high level.

297
00:10:16,050 --> 00:10:17,550
My colleague is gonna go deeper

298
00:10:17,550 --> 00:10:18,930
into some of these topics.

299
00:10:18,930 --> 00:10:20,160
But if you look at this one,

300
00:10:20,160 --> 00:10:22,740
we are saying that how you
build data ready for AI

301
00:10:22,740 --> 00:10:24,570
using three building blocks, right?

302
00:10:24,570 --> 00:10:27,540
The first building blocks
is contextual information.

303
00:10:27,540 --> 00:10:29,430
So how do you create data ready for AI?

304
00:10:29,430 --> 00:10:33,060
By bringing the right
context to the data, okay?

305
00:10:33,060 --> 00:10:34,440
The right context means several things.

306
00:10:34,440 --> 00:10:35,880
It could mean your enterprise knowledge,

307
00:10:35,880 --> 00:10:37,710
your ontologies, your metadata,

308
00:10:37,710 --> 00:10:40,590
your business metadata,
and things like that.

309
00:10:40,590 --> 00:10:43,320
The second thing that you
need in your data platform

310
00:10:43,320 --> 00:10:44,310
is the tool provisioning.

311
00:10:44,310 --> 00:10:46,890
This is how someone is going
to be accessing your data.

312
00:10:46,890 --> 00:10:48,300
Now, remember data mesh?

313
00:10:48,300 --> 00:10:50,940
They had this concept called endpoints,

314
00:10:50,940 --> 00:10:52,020
data product endpoints.

315
00:10:52,020 --> 00:10:55,050
So data product can be called using an API

316
00:10:55,050 --> 00:10:57,120
or a SQL or something like that, right?

317
00:10:57,120 --> 00:10:58,110
Now you wanna make sure

318
00:10:58,110 --> 00:11:00,480
that your data platform
is ready to be accessible

319
00:11:00,480 --> 00:11:02,160
by different toolings and resources,

320
00:11:02,160 --> 00:11:03,540
something like an MCP server,

321
00:11:03,540 --> 00:11:04,680
something like another agent,

322
00:11:04,680 --> 00:11:06,030
and things like that.

323
00:11:06,030 --> 00:11:08,820
And then finally, the maturity.

324
00:11:08,820 --> 00:11:10,260
Now, this is a bit tricky

325
00:11:10,260 --> 00:11:13,110
because data maturity for structured data

326
00:11:13,110 --> 00:11:13,943
means different things,

327
00:11:13,943 --> 00:11:15,090
for unstructured means different things.

328
00:11:15,090 --> 00:11:16,500
But it doesn't matter
to your data scientist.

329
00:11:16,500 --> 00:11:18,900
Like, they want data to be fully matured.

330
00:11:18,900 --> 00:11:22,380
So as you think about modalities
and unstructured data,

331
00:11:22,380 --> 00:11:23,580
I would highly recommend

332
00:11:23,580 --> 00:11:25,440
that look through these
three different pillars.

333
00:11:25,440 --> 00:11:27,630
Now, just a show of hands.

334
00:11:27,630 --> 00:11:30,750
How many of you are actually utilizing

335
00:11:30,750 --> 00:11:31,583
your business metadata

336
00:11:31,583 --> 00:11:34,113
as the context information
for your data assets?

337
00:11:36,750 --> 00:11:38,730
And I can tell you that the metadata

338
00:11:38,730 --> 00:11:42,660
is one of the largest
wealth of information

339
00:11:42,660 --> 00:11:43,800
outside of your data

340
00:11:43,800 --> 00:11:45,510
that can provide a lot of information

341
00:11:45,510 --> 00:11:48,420
for your agents to work for the, you know,

342
00:11:48,420 --> 00:11:49,560
for your outcomes.

343
00:11:49,560 --> 00:11:50,490
Which is my next slide,

344
00:11:50,490 --> 00:11:53,790
that if you look at the
metadata a few years back,

345
00:11:53,790 --> 00:11:56,100
it was largely treated as documentation.

346
00:11:56,100 --> 00:11:58,230
Like, I remember sitting with my team,

347
00:11:58,230 --> 00:12:00,210
working on metadata,
building data dictionaries.

348
00:12:00,210 --> 00:12:01,740
That kinda explains what data is:

349
00:12:01,740 --> 00:12:03,390
documentation pretty much, right?

350
00:12:03,390 --> 00:12:05,430
You go through multiple word docs,

351
00:12:05,430 --> 00:12:06,960
you go through multiple data dictionary.

352
00:12:06,960 --> 00:12:07,950
Not that it was bad;

353
00:12:07,950 --> 00:12:09,780
very useful, right?

354
00:12:09,780 --> 00:12:11,490
But it was largely a documentation.

355
00:12:11,490 --> 00:12:14,340
Whereas now in the contextual world...

356
00:12:14,340 --> 00:12:15,780
And by the way,

357
00:12:15,780 --> 00:12:18,600
the data context metadata
is just one of the contexts.

358
00:12:18,600 --> 00:12:19,433
There are other contexts,

359
00:12:19,433 --> 00:12:21,900
depending on the industry
that you work with.

360
00:12:21,900 --> 00:12:23,460
Now, in the gen AI era,

361
00:12:23,460 --> 00:12:26,310
metadata is actually an
intelligent amplifier, right?

362
00:12:26,310 --> 00:12:28,500
So you can use metadata
for business context,

363
00:12:28,500 --> 00:12:30,510
your glossaries or enterprise knowledge,

364
00:12:30,510 --> 00:12:32,580
something the data steward can work with;

365
00:12:32,580 --> 00:12:34,680
and the relationship
discovery across domains,

366
00:12:34,680 --> 00:12:37,050
as an example; and, of
course, real-time trust.

367
00:12:37,050 --> 00:12:38,370
So think about your metadata.

368
00:12:38,370 --> 00:12:41,250
And it's even more important
for your unstructured data

369
00:12:41,250 --> 00:12:42,360
because structured data,

370
00:12:42,360 --> 00:12:44,970
I can still query and make
some sense out of it, right,

371
00:12:44,970 --> 00:12:47,220
or I may be having at
least basic information

372
00:12:47,220 --> 00:12:48,930
of the columns that it has.

373
00:12:48,930 --> 00:12:49,830
But for unstructured,

374
00:12:49,830 --> 00:12:50,910
it's largely unknown for me

375
00:12:50,910 --> 00:12:52,350
until I actually tap inside the file

376
00:12:52,350 --> 00:12:53,850
and see what is going on.

377
00:12:53,850 --> 00:12:55,470
So therefore it is more relevant

378
00:12:55,470 --> 00:12:58,200
for you to think about a
contextual information.

379
00:12:58,200 --> 00:12:59,430
So you build your data platform,

380
00:12:59,430 --> 00:13:00,570
have those three pillars.

381
00:13:00,570 --> 00:13:02,820
In the context, start
with business metadata.

382
00:13:03,660 --> 00:13:05,250
So how do you get your unstructured data

383
00:13:05,250 --> 00:13:06,750
ready for for AI, right?

384
00:13:06,750 --> 00:13:07,770
So on this screen,

385
00:13:07,770 --> 00:13:11,100
you see that you have
different types of sources.

386
00:13:11,100 --> 00:13:12,780
Some video transcription is coming,

387
00:13:12,780 --> 00:13:14,640
some plain documentation is coming.

388
00:13:14,640 --> 00:13:16,980
The two things that I
would like you to take

389
00:13:16,980 --> 00:13:18,540
as an outcome of this talk...

390
00:13:18,540 --> 00:13:20,670
Of course it's a complex topic, you know,

391
00:13:20,670 --> 00:13:21,503
as you start working.

392
00:13:21,503 --> 00:13:23,550
I'm not overly simplify any of it.

393
00:13:23,550 --> 00:13:26,970
But the two areas that we will
focusing a lot today is, one,

394
00:13:26,970 --> 00:13:31,470
that how do you build a
unified governance of your data

395
00:13:31,470 --> 00:13:34,290
irrespective of its type
and modality, right?

396
00:13:34,290 --> 00:13:36,090
Even it is structured or unstructured,

397
00:13:36,090 --> 00:13:37,920
your data needs to be
treated the same way,

398
00:13:37,920 --> 00:13:40,320
and so as the agents, for example, right?

399
00:13:40,320 --> 00:13:41,580
And the second thing is,

400
00:13:41,580 --> 00:13:43,650
specifically for unstructured data,

401
00:13:43,650 --> 00:13:46,260
how do you use some of the
capabilities that we have

402
00:13:46,260 --> 00:13:49,080
in Amazon Bedrock and Amazon, you know,

403
00:13:49,080 --> 00:13:50,040
Bedrock Agentcore,

404
00:13:50,040 --> 00:13:51,720
and Amazon SageMaker that it can utilize

405
00:13:51,720 --> 00:13:53,280
to build insights from the data?

406
00:13:53,280 --> 00:13:55,290
Remember, governance is first,

407
00:13:55,290 --> 00:13:56,280
building insight is second.

408
00:13:56,280 --> 00:13:59,730
So those are the two areas
that we uniquely focus today

409
00:13:59,730 --> 00:14:01,050
in regards to unstructured data.

410
00:14:01,050 --> 00:14:02,220
But as you can see,

411
00:14:02,220 --> 00:14:04,650
there could be structured plus
unstructured here as well.

412
00:14:04,650 --> 00:14:05,850
And, of course, in most cases,

413
00:14:05,850 --> 00:14:08,490
like in my patients example,
you have both of them.

414
00:14:08,490 --> 00:14:10,830
And so it's important
for you to understand

415
00:14:10,830 --> 00:14:12,750
that how you can most experience that,

416
00:14:12,750 --> 00:14:13,583
as an example.

417
00:14:14,520 --> 00:14:18,000
So I'm gonna talk a little bit
on the data founding layer,

418
00:14:18,000 --> 00:14:19,110
which is something that,

419
00:14:19,110 --> 00:14:22,110
very much linked with
what I presented earlier

420
00:14:22,110 --> 00:14:22,943
on the three topics.

421
00:14:22,943 --> 00:14:24,450
So you look at the bottom of this layer,

422
00:14:24,450 --> 00:14:26,610
we call it data readiness for AI, right?

423
00:14:26,610 --> 00:14:28,500
This is where you have
the data foundations:

424
00:14:28,500 --> 00:14:31,140
the structured/unstructured,
interoperability,

425
00:14:31,140 --> 00:14:32,250
and things like that.

426
00:14:32,250 --> 00:14:33,570
This is how you store your data:

427
00:14:33,570 --> 00:14:35,940
best performance, cost, modality.

428
00:14:35,940 --> 00:14:37,260
If I have a data in Iceberg,

429
00:14:37,260 --> 00:14:39,180
I would store that in S3
tables, as an example.

430
00:14:39,180 --> 00:14:40,792
If I have data, unstructured data,

431
00:14:40,792 --> 00:14:42,090
store that in S3 general purpose bucket.

432
00:14:42,090 --> 00:14:44,160
If I have warehouse
data or structured data,

433
00:14:44,160 --> 00:14:45,060
I'll store that in RDS

434
00:14:45,060 --> 00:14:46,890
or Redshift warehouse
or something like that.

435
00:14:46,890 --> 00:14:47,760
If I have unstructured,

436
00:14:47,760 --> 00:14:49,410
I'll just simply store
that in my S3 buckets

437
00:14:49,410 --> 00:14:50,820
and then utilize.

438
00:14:50,820 --> 00:14:52,410
But however we store the data

439
00:14:52,410 --> 00:14:54,660
shouldn't guide the
architecture upstream, right?

440
00:14:54,660 --> 00:14:56,790
It's just a storage at the end of the day.

441
00:14:56,790 --> 00:14:58,770
What guides the architecture upstream

442
00:14:58,770 --> 00:15:00,180
is that how you interact with the data.

443
00:15:00,180 --> 00:15:01,980
That's where the
interoperability is very handy.

444
00:15:01,980 --> 00:15:04,380
So it's something like
Iceberg-compatible data,

445
00:15:04,380 --> 00:15:05,520
as an example.

446
00:15:05,520 --> 00:15:07,980
On top of that, to have
your data ready for AI,

447
00:15:07,980 --> 00:15:09,510
you have context, tools, and maturity.

448
00:15:09,510 --> 00:15:10,860
Remember those three pillars?

449
00:15:10,860 --> 00:15:12,780
Context is extremely important

450
00:15:12,780 --> 00:15:14,430
for your agents to work with your data.

451
00:15:14,430 --> 00:15:15,481
Without that,

452
00:15:15,481 --> 00:15:16,770
it's going to be speculative exploration

453
00:15:16,770 --> 00:15:17,880
for the most part, right?

454
00:15:17,880 --> 00:15:19,620
And context is not only business metadata.

455
00:15:19,620 --> 00:15:22,040
There are other things like
clinical ontologies in my world.

456
00:15:22,040 --> 00:15:23,640
In your world, it could
be other ontologies

457
00:15:23,640 --> 00:15:25,680
and enterprise knowledge
and things like that.

458
00:15:25,680 --> 00:15:27,750
You have that platform, then
you start building agents.

459
00:15:27,750 --> 00:15:30,780
So we have agents, in my case,

460
00:15:30,780 --> 00:15:31,890
healthcare/life sciences agents,

461
00:15:31,890 --> 00:15:33,737
but you may be having some agents

462
00:15:33,737 --> 00:15:34,570
which are specific to your industry.

463
00:15:34,570 --> 00:15:37,650
We have, you know, a offering
there which you can use:

464
00:15:37,650 --> 00:15:40,230
the accelerators that our team has built.

465
00:15:40,230 --> 00:15:42,150
You have agentic workflow,
you have data accelerators.

466
00:15:42,150 --> 00:15:43,110
All that, you put in there.

467
00:15:43,110 --> 00:15:44,370
Then you build a governance layer.

468
00:15:44,370 --> 00:15:45,203
FLAIR principle.

469
00:15:45,203 --> 00:15:46,980
Everybody knows what
FLAIR is, I'm assuming:

470
00:15:46,980 --> 00:15:49,530
findable, accessible,
interoperable, reusable.

471
00:15:49,530 --> 00:15:50,730
L is the lineage,

472
00:15:50,730 --> 00:15:52,380
which is also something that we've added

473
00:15:52,380 --> 00:15:53,460
as part of the...

474
00:15:53,460 --> 00:15:54,390
And it's interesting to know

475
00:15:54,390 --> 00:15:57,420
that even lineage can act
as a context to your data:

476
00:15:57,420 --> 00:15:59,850
the provenance, the transformation,
and things like that.

477
00:15:59,850 --> 00:16:01,860
Then after that, you
build an experience layer.

478
00:16:01,860 --> 00:16:03,030
This is your end users.

479
00:16:03,030 --> 00:16:06,600
Like, you, as a developer,
as an IT governance lead,

480
00:16:06,600 --> 00:16:08,160
you're building all this platform

481
00:16:08,160 --> 00:16:11,340
and then you build experience
layer with analytics,

482
00:16:11,340 --> 00:16:13,860
with visualization, and with chat.

483
00:16:13,860 --> 00:16:16,170
Those are the typical three
what I've seen lately:

484
00:16:16,170 --> 00:16:19,740
SQL analysis, notebooks, or
the chat agent experience.

485
00:16:19,740 --> 00:16:21,030
How you do that on AWS?

486
00:16:21,030 --> 00:16:22,713
You have Amazon SageMaker Lakehouse.

487
00:16:22,713 --> 00:16:26,280
This is how you store the
data, structured/unstructured,

488
00:16:26,280 --> 00:16:29,130
federated into existing
sources and things like that.

489
00:16:29,130 --> 00:16:32,640
You have the context as
the ontology, metadata,

490
00:16:32,640 --> 00:16:33,750
knowledge graph.

491
00:16:33,750 --> 00:16:34,983
Your tooling is MCP servers, APIs.

492
00:16:34,983 --> 00:16:37,560
And, of course, the
maturity is the quality.

493
00:16:37,560 --> 00:16:39,090
There are others, of course,

494
00:16:39,090 --> 00:16:40,890
maturity principle that you can follow.

495
00:16:40,890 --> 00:16:42,300
Amazon Bedrock and Agentcore

496
00:16:42,300 --> 00:16:43,710
for running or building your agents

497
00:16:43,710 --> 00:16:45,562
and running or
operationalizing production.

498
00:16:45,562 --> 00:16:47,280
Amazon SageMaker Catalog,

499
00:16:47,280 --> 00:16:49,410
a unified governance model

500
00:16:49,410 --> 00:16:51,660
through which you can
unify your governance

501
00:16:51,660 --> 00:16:53,940
across any type of modalities of data,

502
00:16:53,940 --> 00:16:55,950
including unstructured data.

503
00:16:55,950 --> 00:16:56,783
And then at the top,

504
00:16:56,783 --> 00:16:58,020
you have SageMaker Unified Studio

505
00:16:58,020 --> 00:16:59,730
for building your experience.

506
00:16:59,730 --> 00:17:02,160
You have Amazon SageMaker
Notebooks and Data Agents

507
00:17:02,160 --> 00:17:05,490
to accelerate your development using AI.

508
00:17:05,490 --> 00:17:07,020
And then your Amazon QuickSuite

509
00:17:07,020 --> 00:17:10,020
for you to build a chat agent
experience for your customers.

510
00:17:11,116 --> 00:17:12,720
Really quickly,

511
00:17:12,720 --> 00:17:15,083
I have the sample use
case for digital labs.

512
00:17:15,083 --> 00:17:16,680
This is very healthcare-specific.

513
00:17:16,680 --> 00:17:18,200
The one thing I just wanna call here

514
00:17:18,200 --> 00:17:20,340
is that on the left hand side,

515
00:17:20,340 --> 00:17:22,770
you see there are different
type of instruments

516
00:17:22,770 --> 00:17:24,420
from where the data is coming out, right?

517
00:17:24,420 --> 00:17:28,530
And like I said, every industry
has something very specific

518
00:17:28,530 --> 00:17:29,730
to how they're collecting data.

519
00:17:29,730 --> 00:17:31,890
In my case, it's coming through that.

520
00:17:31,890 --> 00:17:33,660
All that part is not that super important.

521
00:17:33,660 --> 00:17:34,493
What is important

522
00:17:34,493 --> 00:17:35,910
is that it's getting stored
in the storage layer.

523
00:17:35,910 --> 00:17:37,950
Remember the previous
slide where I showed you

524
00:17:37,950 --> 00:17:39,810
the data can be structured/unstructured.

525
00:17:39,810 --> 00:17:41,610
Focus on the storage,
how you store the best.

526
00:17:41,610 --> 00:17:42,560
Don't store your unstructured data

527
00:17:42,560 --> 00:17:43,710
in a relation database.

528
00:17:43,710 --> 00:17:44,910
It's not a good idea.

529
00:17:44,910 --> 00:17:47,010
It's not cost-effective solution, right?

530
00:17:47,010 --> 00:17:50,100
Then we have the lineage,
metadata, catalog,

531
00:17:50,100 --> 00:17:52,320
and the governance built, right?

532
00:17:52,320 --> 00:17:53,580
So just those three pieces.

533
00:17:53,580 --> 00:17:54,540
Start small.

534
00:17:54,540 --> 00:17:55,830
Think big, start small.

535
00:17:55,830 --> 00:17:57,510
You have to refactor everything.

536
00:17:57,510 --> 00:17:58,920
Your environment currently is good enough

537
00:17:58,920 --> 00:18:01,323
for you to build these
components starting now.

538
00:18:02,220 --> 00:18:03,750
Then you have the semantic layer.

539
00:18:03,750 --> 00:18:05,430
Extremely important, the semantic,

540
00:18:05,430 --> 00:18:07,950
which is an extended
concept there of context.

541
00:18:07,950 --> 00:18:10,020
What are the semantics for
my own organizations, right?

542
00:18:10,020 --> 00:18:11,940
In my case, it might be something like:

543
00:18:11,940 --> 00:18:13,200
What is a brain tumor?

544
00:18:13,200 --> 00:18:14,220
What is a diabetes?

545
00:18:14,220 --> 00:18:16,950
And what is something like a carcinoma?

546
00:18:16,950 --> 00:18:18,330
In your world, it could be something else.

547
00:18:18,330 --> 00:18:20,490
The point here is that
you build a semantic layer

548
00:18:20,490 --> 00:18:22,950
and then you build all
these consumption models.

549
00:18:22,950 --> 00:18:24,180
So in a cloud environment,

550
00:18:24,180 --> 00:18:27,780
store data effectively,
build the right context,

551
00:18:27,780 --> 00:18:29,883
and then go on from from there.

552
00:18:31,980 --> 00:18:34,020
So some of the challenges I want call out

553
00:18:34,020 --> 00:18:36,660
before I call upon Shiv.

554
00:18:36,660 --> 00:18:38,610
The challenges in processing
unstructured data.

555
00:18:38,610 --> 00:18:40,410
So far we talked about, you know,

556
00:18:40,410 --> 00:18:41,970
the approach of data foundation

557
00:18:41,970 --> 00:18:43,170
and what is important for readiness

558
00:18:43,170 --> 00:18:45,060
and why unstructured
data needs to be treated

559
00:18:45,060 --> 00:18:45,893
very similar way.

560
00:18:45,893 --> 00:18:48,093
Bt it should also,

561
00:18:49,220 --> 00:18:51,870
it shouldn't drive your
architectural decision drastically.

562
00:18:51,870 --> 00:18:53,460
What are the some of the challenges?

563
00:18:53,460 --> 00:18:54,480
Governance, of course.

564
00:18:54,480 --> 00:18:56,732
The governance of unstructured
data is very different

565
00:18:56,732 --> 00:18:58,140
at the bottom level.

566
00:18:58,140 --> 00:18:59,910
It shouldn't be different
at the top level.

567
00:18:59,910 --> 00:19:02,130
So the findability, the
access provisioning.

568
00:19:02,130 --> 00:19:04,290
The model and tool selection
is a challenge too,

569
00:19:04,290 --> 00:19:06,450
where you have the
selecting optimal solutions

570
00:19:06,450 --> 00:19:08,931
and modality/use cases.

571
00:19:08,931 --> 00:19:11,850
There's a lot of manual
processing for unstructured data,

572
00:19:11,850 --> 00:19:13,050
like data extraction tuning

573
00:19:13,050 --> 00:19:14,580
and parameters and things like that.

574
00:19:14,580 --> 00:19:16,140
And finally, the orchestration.

575
00:19:16,140 --> 00:19:18,810
Managing multiple models for
unstructured data is not easy,

576
00:19:18,810 --> 00:19:20,160
it's a challenge,

577
00:19:20,160 --> 00:19:21,540
output integration is a challenge,

578
00:19:21,540 --> 00:19:22,470
and things like that.

579
00:19:22,470 --> 00:19:24,600
So how do we address these challenges

580
00:19:24,600 --> 00:19:27,000
that can span across
the governance of data

581
00:19:27,000 --> 00:19:28,620
and the usage of it?

582
00:19:28,620 --> 00:19:30,900
For that, I'll call upon my colleague Shiv

583
00:19:30,900 --> 00:19:32,040
to talk about the next portion,

584
00:19:32,040 --> 00:19:34,470
where we'll see some of
the SageMaker capabilities

585
00:19:34,470 --> 00:19:35,760
to address these challenges.

586
00:19:35,760 --> 00:19:36,593
Shiv.

587
00:19:36,593 --> 00:19:37,500
- All right. Hey, folks.

588
00:19:37,500 --> 00:19:39,233
Hopefully you'll be able to hear me.

589
00:19:40,200 --> 00:19:41,070
Great.

590
00:19:41,070 --> 00:19:42,360
So my name is Shiv Narayanan.

591
00:19:42,360 --> 00:19:44,910
I'm a product manager
on the SageMaker team,

592
00:19:44,910 --> 00:19:46,143
the next-gen SageMaker.

593
00:19:47,760 --> 00:19:50,010
So there are two services: SageMaker AI,

594
00:19:50,010 --> 00:19:53,580
which is the machine learning
and artificial intelligence.

595
00:19:53,580 --> 00:19:55,950
The next-gen SageMaker
puts everything together.

596
00:19:55,950 --> 00:19:58,503
How many of you use SageMaker today?

597
00:19:59,940 --> 00:20:00,773
Awesome.

598
00:20:02,490 --> 00:20:04,080
I wanted to actually put up my picture,

599
00:20:04,080 --> 00:20:06,330
but then I figured I might
as well just put somebody

600
00:20:06,330 --> 00:20:07,773
who's from Adobe, you know?

601
00:20:10,020 --> 00:20:12,960
So today my goal is to kind of expand

602
00:20:12,960 --> 00:20:14,040
upon what Navneet said

603
00:20:14,040 --> 00:20:17,703
and show you a bunch of demos
on SageMaker Unified Studio.

604
00:20:19,080 --> 00:20:20,130
So this is Dr. Smith.

605
00:20:20,130 --> 00:20:23,490
And we're gonna be
walking through a journey.

606
00:20:23,490 --> 00:20:27,240
Dr. Smith is a neuro-oncologist
treating brain cancer,

607
00:20:27,240 --> 00:20:30,000
and we are gonna help him
prepare for a meeting tomorrow

608
00:20:30,000 --> 00:20:31,500
with one of his patients.

609
00:20:31,500 --> 00:20:33,150
And one of the patients
he's gonna be meeting

610
00:20:33,150 --> 00:20:36,030
is a 25-year-old person, a female,

611
00:20:36,030 --> 00:20:37,740
who has been recently diagnosed

612
00:20:37,740 --> 00:20:39,810
with a neurological cancer.

613
00:20:39,810 --> 00:20:44,190
And the meeting tomorrow is
super important for Dr. Smith

614
00:20:44,190 --> 00:20:45,270
because that's the first time

615
00:20:45,270 --> 00:20:47,760
that she's actually
meeting with Dr. Smith.

616
00:20:47,760 --> 00:20:49,500
And the most important thing

617
00:20:49,500 --> 00:20:51,600
that Dr. Smith wants to do in that meeting

618
00:20:51,600 --> 00:20:53,160
is to instill hope.

619
00:20:53,160 --> 00:20:55,650
And the way you do that
in the most effective way

620
00:20:55,650 --> 00:20:57,187
is to tell the patient that,

621
00:20:57,187 --> 00:20:59,580
"I've actually seen patients like you.

622
00:20:59,580 --> 00:21:00,870
And I have,

623
00:21:00,870 --> 00:21:03,390
you have hope simply because
we've actually done this

624
00:21:03,390 --> 00:21:04,223
over and over

625
00:21:04,223 --> 00:21:06,450
and there is really light
at the end of the tunnel."

626
00:21:06,450 --> 00:21:08,880
But what he's actually doing right now

627
00:21:08,880 --> 00:21:11,130
is actually to stare at those documents,

628
00:21:11,130 --> 00:21:13,170
the unstructured data, right?

629
00:21:13,170 --> 00:21:14,160
And in those documents,

630
00:21:14,160 --> 00:21:16,260
one of the couple of things
that he's really scared of,

631
00:21:16,260 --> 00:21:18,590
like how you probably
might be also scared of,

632
00:21:18,590 --> 00:21:22,380
is the amount of in incredibly
insightful information

633
00:21:22,380 --> 00:21:23,820
that's scattered across.

634
00:21:23,820 --> 00:21:27,960
And this data changes every
single document he changes.

635
00:21:27,960 --> 00:21:30,930
This is actually the
reality of healthcare today.

636
00:21:30,930 --> 00:21:33,040
And more importantly,
there is demographics data,

637
00:21:33,040 --> 00:21:36,330
there is medications, there is procedures,

638
00:21:36,330 --> 00:21:37,560
there's treatments,

639
00:21:37,560 --> 00:21:40,410
but there is also sensitive data.

640
00:21:40,410 --> 00:21:42,930
Now, it's super important for us

641
00:21:42,930 --> 00:21:46,980
to be able to provide Dr.
Smith the required tools

642
00:21:46,980 --> 00:21:49,780
in order to make sure that
he gains access to this data,

643
00:21:51,030 --> 00:21:54,810
the capabilities to use the
latest generation technologies

644
00:21:54,810 --> 00:21:55,980
like large language models

645
00:21:55,980 --> 00:21:57,600
to help process/parse this

646
00:21:57,600 --> 00:21:59,760
and eventually help him
prepare for this meeting.

647
00:21:59,760 --> 00:22:01,320
And that's what I'm gonna do now.

648
00:22:01,320 --> 00:22:02,153
We, as a team,

649
00:22:02,153 --> 00:22:03,660
are gonna actually go through that

650
00:22:03,660 --> 00:22:06,870
to see how to actually help
him prepare for that meeting.

651
00:22:06,870 --> 00:22:08,643
So in order to do this,

652
00:22:09,900 --> 00:22:10,800
the most important things

653
00:22:10,800 --> 00:22:13,260
is that obviously you have to
have the right AWS services

654
00:22:13,260 --> 00:22:14,280
to do it.

655
00:22:14,280 --> 00:22:15,113
And end of the day,

656
00:22:15,113 --> 00:22:17,700
our goal is to actually
build him a chat agent.

657
00:22:17,700 --> 00:22:19,830
And this chat agent needs to be built

658
00:22:19,830 --> 00:22:22,590
with all the right
governance controls in place.

659
00:22:22,590 --> 00:22:23,970
And traditionally,

660
00:22:23,970 --> 00:22:26,310
it was also a little
difficult in AWS to do this

661
00:22:26,310 --> 00:22:28,710
simply because while
we have all the breadth

662
00:22:28,710 --> 00:22:29,760
of these services,

663
00:22:29,760 --> 00:22:32,040
you have to hop onto different consoles.

664
00:22:32,040 --> 00:22:34,110
So what is the innovation that we've done

665
00:22:34,110 --> 00:22:38,103
in the past year to
simplify these problems?

666
00:22:39,690 --> 00:22:41,460
With that, I wanted to talk a little bit

667
00:22:41,460 --> 00:22:45,020
about the team that I'm
part of and very proud,

668
00:22:45,020 --> 00:22:47,160
of the next-generation SageMaker.

669
00:22:47,160 --> 00:22:51,603
So this is actually your center
for data analytics and AI.

670
00:22:52,872 --> 00:22:56,520
SageMaker is comprised of
three important components.

671
00:22:56,520 --> 00:22:58,830
The first one is the
lakehouse architecture,

672
00:22:58,830 --> 00:23:01,770
where you actually store all
these documents that you saw.

673
00:23:01,770 --> 00:23:04,710
The next one is a catalog.

674
00:23:04,710 --> 00:23:06,930
The ability for you to make sure

675
00:23:06,930 --> 00:23:08,610
that you actually take
all these unstructured

676
00:23:08,610 --> 00:23:11,310
and structured documents,
catalog them, annotate them,

677
00:23:12,240 --> 00:23:14,640
provide the visibility
of data quality metrics,

678
00:23:14,640 --> 00:23:16,650
associate glossary terms.

679
00:23:16,650 --> 00:23:19,380
And you have a unified studio

680
00:23:19,380 --> 00:23:22,860
that allows customers
to build applications

681
00:23:22,860 --> 00:23:25,080
all the way from simple
data engineering pipelines

682
00:23:25,080 --> 00:23:28,290
to extremely complicated
machine learning models

683
00:23:28,290 --> 00:23:31,320
and also chat agents with ease,

684
00:23:31,320 --> 00:23:33,963
all without you having to
switch different consoles.

685
00:23:35,130 --> 00:23:38,700
Before we dive into a
demo of how we do this,

686
00:23:38,700 --> 00:23:40,620
let's just walk through
a little bit more details

687
00:23:40,620 --> 00:23:42,240
about the catalog first.

688
00:23:42,240 --> 00:23:44,643
How many of you use the
SageMaker Catalog today?

689
00:23:46,200 --> 00:23:47,400
Cool.

690
00:23:47,400 --> 00:23:48,450
Very good.

691
00:23:48,450 --> 00:23:50,310
So, what's different

692
00:23:50,310 --> 00:23:52,140
about the SageMaker
Catalog is, first of all,

693
00:23:52,140 --> 00:23:55,983
it's actually built on
DataZone's business data catalog.

694
00:23:57,270 --> 00:24:01,233
It really is built on a very
scalable model to begin with.

695
00:24:02,430 --> 00:24:04,380
The second important
thing about the catalog

696
00:24:04,380 --> 00:24:07,860
is it's just not like a catalog
for structured data assets.

697
00:24:07,860 --> 00:24:09,810
It's a catalog for your
unstructured data assets,

698
00:24:09,810 --> 00:24:12,390
it's a catalog for your
business intelligence assets.

699
00:24:12,390 --> 00:24:14,700
So it's really much more broader

700
00:24:14,700 --> 00:24:16,080
and we continue to enhance

701
00:24:16,080 --> 00:24:18,830
the number of types of objects
that we support on that.

702
00:24:20,010 --> 00:24:22,440
You can also understand
the quality metrics,

703
00:24:22,440 --> 00:24:26,070
you can understand the data
lineage from that catalog.

704
00:24:26,070 --> 00:24:30,330
You can also understand the
ability for you to search.

705
00:24:30,330 --> 00:24:33,780
And you can subscribe to these assets

706
00:24:33,780 --> 00:24:35,700
and gain permissions to data

707
00:24:35,700 --> 00:24:37,590
in an auditable and compliant way.

708
00:24:37,590 --> 00:24:40,830
So that is very unique about
the permissioning model

709
00:24:40,830 --> 00:24:43,890
that has been integrated
with the SageMaker Catalog.

710
00:24:43,890 --> 00:24:44,730
In addition to it,

711
00:24:44,730 --> 00:24:45,840
there are other components

712
00:24:45,840 --> 00:24:47,340
that I'll be talking through my talk

713
00:24:47,340 --> 00:24:48,600
in terms of the guardrails

714
00:24:48,600 --> 00:24:50,400
and the responsible AI aspects

715
00:24:50,400 --> 00:24:52,830
that you'll actually see in a demo also.

716
00:24:52,830 --> 00:24:54,870
So the catalog is an
extremely important component

717
00:24:54,870 --> 00:24:56,100
for Dr. Smith and his team

718
00:24:56,100 --> 00:24:57,480
because as a first step,

719
00:24:57,480 --> 00:24:59,010
what we wanna do is to be able to take

720
00:24:59,010 --> 00:25:00,630
all this unstructured data, catalog them,

721
00:25:00,630 --> 00:25:02,040
make them discoverable,

722
00:25:02,040 --> 00:25:04,770
provide the right guardrails
so that he can actually,

723
00:25:04,770 --> 00:25:07,613
him and his team can actually
get the right access controls.

724
00:25:08,700 --> 00:25:12,090
The next part is: how do
you build your applications?

725
00:25:12,090 --> 00:25:15,300
So that's where we have
the Unified Studio.

726
00:25:15,300 --> 00:25:17,880
So Unified Studio gives you the ability,

727
00:25:17,880 --> 00:25:19,380
in a single console,

728
00:25:19,380 --> 00:25:23,130
to build all required apps

729
00:25:23,130 --> 00:25:25,470
that is needed for your
data engineering work

730
00:25:25,470 --> 00:25:26,460
to the AI work.

731
00:25:26,460 --> 00:25:28,767
For example, if you're a visual person,

732
00:25:28,767 --> 00:25:31,380
if you are like more into
visual ETL development,

733
00:25:31,380 --> 00:25:34,440
we have a visual ETL that
is available for you.

734
00:25:34,440 --> 00:25:38,029
If you are more of
code-based data engineering,

735
00:25:38,029 --> 00:25:40,263
you have notebooks that is embedded in it.

736
00:25:41,310 --> 00:25:43,440
You wanna build machine learning models,

737
00:25:43,440 --> 00:25:44,760
we have foundational models,

738
00:25:44,760 --> 00:25:47,130
we have the SageMaker AI models

739
00:25:47,130 --> 00:25:48,660
that are available in a model hub,

740
00:25:48,660 --> 00:25:51,180
that you can actually
take them, train them,

741
00:25:51,180 --> 00:25:53,070
experiment them with ML flow,

742
00:25:53,070 --> 00:25:54,570
and then deploy them.

743
00:25:54,570 --> 00:25:56,220
And if you're a gen AI developer,

744
00:25:56,220 --> 00:25:57,053
we have the ability

745
00:25:57,053 --> 00:26:00,240
for you to build chat
applications and prompts,

746
00:26:00,240 --> 00:26:02,940
the necessary guardrails,
all of them together.

747
00:26:02,940 --> 00:26:04,200
So that is the Unified Studio.

748
00:26:04,200 --> 00:26:06,957
Now, what we are gonna do
is actually watch a demo,

749
00:26:06,957 --> 00:26:09,210
and I'm gonna walk you through this.

750
00:26:09,210 --> 00:26:10,320
And as a first step

751
00:26:10,320 --> 00:26:12,570
to help Dr. Smith prepare for his meeting,

752
00:26:12,570 --> 00:26:14,610
we are going to take all
this unstructured data

753
00:26:14,610 --> 00:26:18,870
that's already available
in S3, catalog them,

754
00:26:18,870 --> 00:26:21,870
and make sure that it's
actually discoverable.

755
00:26:21,870 --> 00:26:23,160
That's the step number one.

756
00:26:23,160 --> 00:26:24,860
And here's how we are gonna do it.

757
00:26:25,890 --> 00:26:28,230
Before that, let me just quickly walk you

758
00:26:28,230 --> 00:26:30,030
through an important launch that we did

759
00:26:30,030 --> 00:26:31,950
a couple of months ago,

760
00:26:31,950 --> 00:26:33,780
which is the ability for
you to actually catalog

761
00:26:33,780 --> 00:26:34,650
these unstructured assets.

762
00:26:34,650 --> 00:26:38,580
With this, you can connect to S3,

763
00:26:38,580 --> 00:26:40,233
either to a bucket or a folder,

764
00:26:41,460 --> 00:26:44,850
take those assets,
provide business context,

765
00:26:44,850 --> 00:26:46,890
publish those to the data catalog,

766
00:26:46,890 --> 00:26:48,900
and then make it discoverable.

767
00:26:48,900 --> 00:26:51,660
So with that, let's just watch this demo.

768
00:26:51,660 --> 00:26:54,180
So I'm showing you the Unified Studio.

769
00:26:54,180 --> 00:26:55,110
And in here,

770
00:26:55,110 --> 00:26:57,630
what we are gonna do is
to go to the Data tab

771
00:26:57,630 --> 00:26:58,732
of that project.

772
00:26:58,732 --> 00:27:01,200
And as you can see here,
I have my Lakehouse,

773
00:27:01,200 --> 00:27:02,250
which is structured assets,

774
00:27:02,250 --> 00:27:03,420
and then I have Buckets.

775
00:27:03,420 --> 00:27:06,330
I'm gonna go ahead and
connect to my S3 location,

776
00:27:06,330 --> 00:27:08,130
provide a name for the data lake,

777
00:27:08,130 --> 00:27:09,840
provide a description,

778
00:27:09,840 --> 00:27:14,130
specify the URL where the
medication records are stored,

779
00:27:14,130 --> 00:27:15,480
provide a role,

780
00:27:15,480 --> 00:27:18,930
and then we are going to
hit the ability to connect.

781
00:27:18,930 --> 00:27:20,730
So what this does in the backend,

782
00:27:20,730 --> 00:27:23,310
this is actually going to
create a connection to S3

783
00:27:23,310 --> 00:27:25,290
and then it is actually
going to first bring in

784
00:27:25,290 --> 00:27:26,910
all the assets that you actually watched

785
00:27:26,910 --> 00:27:27,810
in the previous screen,

786
00:27:27,810 --> 00:27:29,970
which Dr. Smith was actually staring at.

787
00:27:29,970 --> 00:27:32,250
So these are all the unstructured data

788
00:27:32,250 --> 00:27:33,900
that you were actually seeing.

789
00:27:33,900 --> 00:27:35,160
And these are all the medications

790
00:27:35,160 --> 00:27:37,620
that you just watched in that.

791
00:27:37,620 --> 00:27:41,940
So as a next step, what I'm
gonna do is actually to,

792
00:27:41,940 --> 00:27:43,110
for brevity of this demo,

793
00:27:43,110 --> 00:27:44,880
I'm just gonna go ahead and
catalog the entire bucket.

794
00:27:44,880 --> 00:27:47,880
You can actually catalog
individual folders.

795
00:27:47,880 --> 00:27:49,920
But what we're gonna do is
to add some business metadata

796
00:27:49,920 --> 00:27:51,213
to the structured assets.

797
00:27:52,080 --> 00:27:54,930
So this is the screen
where you can actually add

798
00:27:54,930 --> 00:27:57,210
a whole lot of business metadata.

799
00:27:57,210 --> 00:27:58,043
To begin with,

800
00:27:58,043 --> 00:28:00,780
what I'm gonna do is
to start adding context

801
00:28:00,780 --> 00:28:02,460
in terms of the readme file.

802
00:28:02,460 --> 00:28:03,780
And this is what Navneet talked about:

803
00:28:03,780 --> 00:28:04,920
the context that is needed

804
00:28:04,920 --> 00:28:06,870
for the generative AI purposes, right?

805
00:28:06,870 --> 00:28:09,900
So I've added what exactly
are these documents.

806
00:28:09,900 --> 00:28:11,790
I've provided detailed descriptions,

807
00:28:11,790 --> 00:28:13,170
the type of diagnosis,

808
00:28:13,170 --> 00:28:14,850
the medications that's there.

809
00:28:14,850 --> 00:28:16,908
I'm also associating glossary terms.

810
00:28:16,908 --> 00:28:18,180
So in the catalog,

811
00:28:18,180 --> 00:28:19,920
you have the ability to create a glossary

812
00:28:19,920 --> 00:28:21,150
in a hierarchical way

813
00:28:21,150 --> 00:28:23,670
and then associate terms to these.

814
00:28:23,670 --> 00:28:25,350
This allows you to search

815
00:28:25,350 --> 00:28:27,690
and it also provides
meaningful associations

816
00:28:27,690 --> 00:28:30,180
when your other customers,

817
00:28:30,180 --> 00:28:32,550
who are actually working on SageMaker,

818
00:28:32,550 --> 00:28:34,800
to understand and discover
these assets pretty easily.

819
00:28:34,800 --> 00:28:37,260
So I'm going to go
through a variety of terms

820
00:28:37,260 --> 00:28:40,533
and then select them and
associate them for these assets.

821
00:28:41,790 --> 00:28:44,610
So now that we are happy
with adding all those assets,

822
00:28:44,610 --> 00:28:46,830
let's go ahead and click yes.

823
00:28:46,830 --> 00:28:49,260
And you also have the ability
to extend this metadata

824
00:28:49,260 --> 00:28:51,270
with additional concepts
called metadata forms.

825
00:28:51,270 --> 00:28:54,150
I'm not gonna do that
to keep this demo short.

826
00:28:54,150 --> 00:28:55,920
And let's go ahead and publish this asset.

827
00:28:55,920 --> 00:28:57,270
So when you publish this asset...

828
00:28:57,270 --> 00:28:58,890
So long you've been actually
just kind of working

829
00:28:58,890 --> 00:29:01,590
in your own local copy, so to speak.

830
00:29:01,590 --> 00:29:02,460
When you publish this,

831
00:29:02,460 --> 00:29:05,640
this now is discoverable
by anybody and everybody,

832
00:29:05,640 --> 00:29:07,350
not necessarily the data,

833
00:29:07,350 --> 00:29:09,600
but they can actually view the metadata.

834
00:29:09,600 --> 00:29:11,520
So now I just want to go to the catalog,

835
00:29:11,520 --> 00:29:13,860
make sure that I've actually
published this right.

836
00:29:13,860 --> 00:29:16,050
And you can actually
browse through the assets

837
00:29:16,050 --> 00:29:18,960
and you can see the type of assets,

838
00:29:18,960 --> 00:29:20,700
the glossary terms that I associated.

839
00:29:20,700 --> 00:29:22,657
Very easy for me now to basically say,

840
00:29:22,657 --> 00:29:23,790
"Give me the patient data.

841
00:29:23,790 --> 00:29:26,220
Give me the cancer diagnosis data."

842
00:29:26,220 --> 00:29:28,650
I can go through and look
at different asset types.

843
00:29:28,650 --> 00:29:29,483
You might have seen, like,

844
00:29:29,483 --> 00:29:31,800
the different types of
assets that we support there.

845
00:29:31,800 --> 00:29:35,850
We also provide couple
of other capabilities

846
00:29:35,850 --> 00:29:37,530
and extensions for this.

847
00:29:37,530 --> 00:29:39,810
It looks really great from my perspective

848
00:29:39,810 --> 00:29:41,610
of how I've published metadata.

849
00:29:41,610 --> 00:29:43,590
I also have lineage,

850
00:29:43,590 --> 00:29:46,601
but this is basic data
lineage at this point in time

851
00:29:46,601 --> 00:29:48,360
because we just have started to work.

852
00:29:48,360 --> 00:29:49,470
And as lineage builds up,

853
00:29:49,470 --> 00:29:52,260
it's gonna be built up
pretty well in this.

854
00:29:52,260 --> 00:29:55,440
So what I've done so
far is taken that assets

855
00:29:55,440 --> 00:29:57,120
that was just an S3,

856
00:29:57,120 --> 00:29:59,190
that was not discoverable by anybody,

857
00:29:59,190 --> 00:30:01,950
and I've made them discoverable
at this point in time

858
00:30:01,950 --> 00:30:04,203
for people to come in and start working.

859
00:30:05,730 --> 00:30:06,563
So next,

860
00:30:06,563 --> 00:30:08,940
we are gonna have a generative
AI developer come in

861
00:30:08,940 --> 00:30:10,350
and do some work with this data.

862
00:30:10,350 --> 00:30:11,760
But before that,

863
00:30:11,760 --> 00:30:14,603
let's just look at a few more concepts.

864
00:30:14,603 --> 00:30:17,460
SageMaker Unified Studio
is one of the services

865
00:30:17,460 --> 00:30:19,707
that brings all the structured

866
00:30:19,707 --> 00:30:21,750
and unstructured data together

867
00:30:21,750 --> 00:30:25,140
and allows you to build
knowledge bases with ease.

868
00:30:25,140 --> 00:30:28,440
So we provide the ability,
in one single place,

869
00:30:28,440 --> 00:30:31,350
for you to come in and source structured

870
00:30:31,350 --> 00:30:32,700
and unstructured data

871
00:30:32,700 --> 00:30:34,860
and then easily build knowledge bases

872
00:30:34,860 --> 00:30:36,543
for your gen AI applications.

873
00:30:39,540 --> 00:30:43,683
How many of you have built
a knowledge base in Bedrock?

874
00:30:45,150 --> 00:30:46,470
Cool.

875
00:30:46,470 --> 00:30:48,060
So one of the things...

876
00:30:48,060 --> 00:30:50,100
And I think, like, all of us know...

877
00:30:50,100 --> 00:30:51,870
Navneet talked adequately about RAGs

878
00:30:51,870 --> 00:30:54,660
and what's the usefulness of RAGs.

879
00:30:54,660 --> 00:30:56,220
These are the concepts

880
00:30:56,220 --> 00:30:57,150
or these are the objects

881
00:30:57,150 --> 00:31:00,090
that make your foundational
model more intelligent

882
00:31:00,090 --> 00:31:02,580
and your organization aware, right?

883
00:31:02,580 --> 00:31:04,260
So by using a knowledge base,

884
00:31:04,260 --> 00:31:05,730
what you can do is to enhance

885
00:31:05,730 --> 00:31:08,010
your existing large language models

886
00:31:08,010 --> 00:31:11,583
to provide more context of
your organizational data.

887
00:31:12,840 --> 00:31:14,610
And what we,

888
00:31:14,610 --> 00:31:18,600
from SageMaker Unified Studio
and SageMaker as a whole,

889
00:31:18,600 --> 00:31:20,760
do is to allow for you

890
00:31:20,760 --> 00:31:22,830
to create these knowledge bases with ease,

891
00:31:22,830 --> 00:31:26,250
without having to navigate
through multiple APIs,

892
00:31:26,250 --> 00:31:27,900
just simply with a few click of buttons.

893
00:31:27,900 --> 00:31:31,170
And I'm gonna show you how
you can actually do that.

894
00:31:31,170 --> 00:31:33,750
Now, it's one thing to
have a knowledge base,

895
00:31:33,750 --> 00:31:36,960
but the most important
aspect is even governing

896
00:31:36,960 --> 00:31:39,360
to make sure these models are grounded,

897
00:31:39,360 --> 00:31:41,280
to make sure that it's not hallucinating,

898
00:31:41,280 --> 00:31:45,090
to make sure that it is not being hateful,

899
00:31:45,090 --> 00:31:47,730
to make sure that it is
actually providing the context

900
00:31:47,730 --> 00:31:50,130
or the right data

901
00:31:50,130 --> 00:31:53,070
by looking at the data
that you have provided.

902
00:31:53,070 --> 00:31:57,240
So for this, Bedrock offers guardrails.

903
00:31:57,240 --> 00:31:59,790
With guardrails, what you can do is...

904
00:31:59,790 --> 00:32:02,040
It comes out of the box
with a bunch of filters

905
00:32:02,040 --> 00:32:03,300
that you can apply to make sure

906
00:32:03,300 --> 00:32:04,860
that your models are grounded.

907
00:32:04,860 --> 00:32:07,620
But in addition, you can
also extend them with ease,

908
00:32:07,620 --> 00:32:09,090
with simple natural language,

909
00:32:09,090 --> 00:32:11,640
to make sure that your prompts are,

910
00:32:11,640 --> 00:32:15,540
the result of the large language
models that you're using

911
00:32:15,540 --> 00:32:16,800
is appropriate.

912
00:32:16,800 --> 00:32:17,633
So, for instance,

913
00:32:17,633 --> 00:32:19,140
a couple of things that you
could do is, for instance,

914
00:32:19,140 --> 00:32:21,817
you could say, "Don't
show PII information," or,

915
00:32:21,817 --> 00:32:23,437
"Don't show like," you know,

916
00:32:23,437 --> 00:32:24,300
"I don't want you

917
00:32:24,300 --> 00:32:26,760
to basically be always
grounded with facts."

918
00:32:26,760 --> 00:32:30,030
And you can do a lot with
with with guardrails.

919
00:32:30,030 --> 00:32:32,490
And what is unique about
SageMaker's integration

920
00:32:32,490 --> 00:32:35,460
with guardrails is the
fact that it allows you

921
00:32:35,460 --> 00:32:39,060
to basically take your
data from your catalog,

922
00:32:39,060 --> 00:32:41,130
build that knowledge base,
build that guardrails,

923
00:32:41,130 --> 00:32:43,500
all in one single unified console,

924
00:32:43,500 --> 00:32:45,750
and then be able to deploy these

925
00:32:45,750 --> 00:32:48,600
to your business users with ease.

926
00:32:48,600 --> 00:32:52,260
And that exactly is what
we are gonna do next.

927
00:32:52,260 --> 00:32:54,570
So we have a generative AI developer.

928
00:32:54,570 --> 00:32:57,030
Remember we just cataloged,
as a data producer,

929
00:32:57,030 --> 00:32:59,130
all these assets in our catalog?

930
00:32:59,130 --> 00:33:00,630
As a generative AI developer,

931
00:33:00,630 --> 00:33:01,890
John comes in,

932
00:33:01,890 --> 00:33:03,960
just searches for a term diagnosis.

933
00:33:03,960 --> 00:33:06,360
This is a very common term
that occurs for many people.

934
00:33:06,360 --> 00:33:07,500
They just come and search

935
00:33:07,500 --> 00:33:10,440
and immediately they're
able to discover that asset.

936
00:33:10,440 --> 00:33:11,280
When they discover,

937
00:33:11,280 --> 00:33:13,200
they're able to see the metadata,

938
00:33:13,200 --> 00:33:14,730
they're able to see the glossary terms

939
00:33:14,730 --> 00:33:16,080
that they've associated.

940
00:33:16,080 --> 00:33:17,460
And with a lot of confidence

941
00:33:17,460 --> 00:33:18,840
and discussions with their colleagues,

942
00:33:18,840 --> 00:33:19,950
this is the right data set

943
00:33:19,950 --> 00:33:21,750
that we have to build our knowledge base.

944
00:33:21,750 --> 00:33:23,700
So one thing that John does here

945
00:33:23,700 --> 00:33:26,640
is he's gonna ask for access to that data,

946
00:33:26,640 --> 00:33:28,290
and this is the unstructured data access

947
00:33:28,290 --> 00:33:30,240
that he's requesting for.

948
00:33:30,240 --> 00:33:31,800
So he has requested for that access

949
00:33:31,800 --> 00:33:33,750
and he can actually view that,

950
00:33:33,750 --> 00:33:36,330
that he's actually requested for access.

951
00:33:36,330 --> 00:33:39,517
Now, the data producer gets
this as a notification, that,

952
00:33:39,517 --> 00:33:41,520
"Oh, a generative AI developer

953
00:33:41,520 --> 00:33:43,620
actually has asked me access."

954
00:33:43,620 --> 00:33:44,970
Now, this is at this point in time,

955
00:33:44,970 --> 00:33:47,160
he can click, talk to the legal teams,

956
00:33:47,160 --> 00:33:48,390
make sure that they understand

957
00:33:48,390 --> 00:33:50,850
what the use case is for
this particular asset,

958
00:33:50,850 --> 00:33:55,110
and then approve or reject
this particular access.

959
00:33:55,110 --> 00:33:59,220
So in this case, after
discussions with legal

960
00:33:59,220 --> 00:34:02,310
and other compliance organizations,

961
00:34:02,310 --> 00:34:03,300
they are approving

962
00:34:03,300 --> 00:34:05,700
to make sure that they
can actually go ahead

963
00:34:05,700 --> 00:34:07,560
and build a chat application

964
00:34:07,560 --> 00:34:09,460
or to feed this into a knowledge base.

965
00:34:10,620 --> 00:34:14,700
So now we've gotten John
access to that data.

966
00:34:14,700 --> 00:34:15,600
This is great.

967
00:34:15,600 --> 00:34:18,270
So John goes into his project,

968
00:34:18,270 --> 00:34:19,770
goes to the Data tab,

969
00:34:19,770 --> 00:34:24,630
and now confirms that he now
has access to that S3 buckets,

970
00:34:24,630 --> 00:34:27,270
which is great; he now has access.

971
00:34:27,270 --> 00:34:30,330
So the next natural step is to go ahead

972
00:34:30,330 --> 00:34:31,800
and start building the chat application.

973
00:34:31,800 --> 00:34:34,410
And remember, I talked about two concepts,

974
00:34:34,410 --> 00:34:36,120
the knowledge base and guardrails,

975
00:34:36,120 --> 00:34:37,650
and that's exactly what
we are gonna build.

976
00:34:37,650 --> 00:34:39,690
Look at how easy it is to build that.

977
00:34:39,690 --> 00:34:40,680
With Unified Studio,

978
00:34:40,680 --> 00:34:43,653
you just go in there and
create a knowledge base,

979
00:34:45,060 --> 00:34:47,460
provide the name of the knowledge base,

980
00:34:47,460 --> 00:34:49,860
and what differentiates things here

981
00:34:49,860 --> 00:34:53,160
is the fact that you can
now use the data sources

982
00:34:53,160 --> 00:34:56,280
that you have gotten
access directly from here.

983
00:34:56,280 --> 00:34:59,010
So I just got access to my S3 locations,

984
00:34:59,010 --> 00:34:59,880
I just provide that,

985
00:34:59,880 --> 00:35:01,740
I'm selecting the embeddings I need.

986
00:35:01,740 --> 00:35:03,570
SageMaker Unified Studio takes care

987
00:35:03,570 --> 00:35:05,610
of all the other required components,

988
00:35:05,610 --> 00:35:07,680
like making sure that it
creates a vector database

989
00:35:07,680 --> 00:35:09,485
in Amazon OpenSearch,

990
00:35:09,485 --> 00:35:11,100
which is a serverless offering here,

991
00:35:11,100 --> 00:35:13,590
and makes sure that it
indexes all the data

992
00:35:13,590 --> 00:35:15,090
and creates that knowledge base for you

993
00:35:15,090 --> 00:35:17,040
without you having to
do that heavy lifting.

994
00:35:17,040 --> 00:35:18,573
It's just a few simple clicks.

995
00:35:20,160 --> 00:35:23,100
So, once the knowledge base is created,

996
00:35:23,100 --> 00:35:24,870
the next step for us
is actually to go ahead

997
00:35:24,870 --> 00:35:26,160
and create the guardrails

998
00:35:26,160 --> 00:35:27,870
because this is the governance application

999
00:35:27,870 --> 00:35:30,660
that we need to make sure that, you know,

1000
00:35:30,660 --> 00:35:32,550
we are applying the right
governance controls.

1001
00:35:32,550 --> 00:35:34,110
And as you can see, out of the box,

1002
00:35:34,110 --> 00:35:36,720
there is a whole bunch of filters

1003
00:35:36,720 --> 00:35:40,350
that allow you to control the responses

1004
00:35:40,350 --> 00:35:41,430
of the knowledge base.

1005
00:35:41,430 --> 00:35:45,720
But you can also add your custom
messages or custom prompts

1006
00:35:45,720 --> 00:35:47,370
to make sure that you are applying

1007
00:35:47,370 --> 00:35:48,480
your own governance controls.

1008
00:35:48,480 --> 00:35:50,850
In this case, I don't want anyone here

1009
00:35:50,850 --> 00:35:53,910
to prompt for address
information, the demographics.

1010
00:35:53,910 --> 00:35:57,270
So I'm gonna go ahead
and create a deny address

1011
00:35:57,270 --> 00:35:59,630
and I'm gonna create a definition for it.

1012
00:35:59,630 --> 00:36:01,207
So I'm just basically saying like,

1013
00:36:01,207 --> 00:36:02,100
"Don't respond.

1014
00:36:02,100 --> 00:36:03,840
If there's patient address information,

1015
00:36:03,840 --> 00:36:05,100
you can provide some sample phrases."

1016
00:36:05,100 --> 00:36:06,630
That's optional for you.

1017
00:36:06,630 --> 00:36:09,413
And we are just gonna go ahead
and create this guardrail.

1018
00:36:10,590 --> 00:36:12,780
So once the guardrail is created,

1019
00:36:12,780 --> 00:36:14,310
once the knowledge base is created,

1020
00:36:14,310 --> 00:36:17,130
it's now time for us
to create the chat app.

1021
00:36:17,130 --> 00:36:18,630
And again, it's not too difficult.

1022
00:36:18,630 --> 00:36:21,633
You simply go to the My
apps, create a chat app.

1023
00:36:24,570 --> 00:36:27,360
And here, we provide a
name for the chat app.

1024
00:36:27,360 --> 00:36:31,110
In this case, we'll just
say it's Patient 360.

1025
00:36:31,110 --> 00:36:33,990
And we select the foundational model.

1026
00:36:33,990 --> 00:36:35,700
We now provide the knowledge base

1027
00:36:35,700 --> 00:36:37,680
that we want to augment
this foundation model,

1028
00:36:37,680 --> 00:36:38,940
which is pretty straightforward

1029
00:36:38,940 --> 00:36:41,190
because we created the knowledge
base of the previous step.

1030
00:36:41,190 --> 00:36:42,870
We basically select that.

1031
00:36:42,870 --> 00:36:45,000
We can apply multiple guardrails.

1032
00:36:45,000 --> 00:36:45,870
So we'll just go ahead

1033
00:36:45,870 --> 00:36:48,360
and apply that specific guardrail here.

1034
00:36:48,360 --> 00:36:53,010
And what we are gonna do
is to save this chat app.

1035
00:36:53,010 --> 00:36:55,960
And so

1036
00:36:57,750 --> 00:36:59,970
we'll just go ahead and save.

1037
00:36:59,970 --> 00:37:02,100
And next step is that...

1038
00:37:02,100 --> 00:37:04,590
Now that I've been working on
my local copy in my project,

1039
00:37:04,590 --> 00:37:06,180
the most important step
is to actually go ahead

1040
00:37:06,180 --> 00:37:09,720
and deploy it so that it's
basically visible to others

1041
00:37:09,720 --> 00:37:10,830
and I'm in a position

1042
00:37:10,830 --> 00:37:12,960
where I can actually start
sharing this with others.

1043
00:37:12,960 --> 00:37:15,870
So we'll just go ahead
and provide a name for it

1044
00:37:15,870 --> 00:37:18,720
or the alias name that
represents that certain version

1045
00:37:18,720 --> 00:37:20,070
or any changes to that version.

1046
00:37:20,070 --> 00:37:21,810
Because it's the first
time I'm just deploying it,

1047
00:37:21,810 --> 00:37:25,470
I'm just gonna provide the
right description for this app.

1048
00:37:25,470 --> 00:37:27,140
And we just simply deploy that app.

1049
00:37:27,140 --> 00:37:29,700
It is pretty straightforward
as you can see.

1050
00:37:29,700 --> 00:37:30,900
In few steps,

1051
00:37:30,900 --> 00:37:35,900
we basically took the data, got access,

1052
00:37:36,420 --> 00:37:38,490
and basically built a knowledge base.

1053
00:37:38,490 --> 00:37:40,560
And next step, we can
actually start sharing this.

1054
00:37:40,560 --> 00:37:42,390
We're gonna share this
with Dr. Smith; however,

1055
00:37:42,390 --> 00:37:44,760
you can also share it with
a group of individuals.

1056
00:37:44,760 --> 00:37:47,310
Pretty straightforward in terms
of how you can share this.

1057
00:37:47,310 --> 00:37:50,013
And then we just went ahead
and published this app.

1058
00:37:51,840 --> 00:37:52,890
So in this demo,

1059
00:37:52,890 --> 00:37:55,200
what we have done is
we've shown you, like,

1060
00:37:55,200 --> 00:37:57,960
how easy it is for a gen AI developer

1061
00:37:57,960 --> 00:38:02,550
to take assets in a very
secure, compliant manner,

1062
00:38:02,550 --> 00:38:04,260
in an auditable way,

1063
00:38:04,260 --> 00:38:06,660
and take that and create a
knowledge base out of it,

1064
00:38:06,660 --> 00:38:08,670
apply the right governance and guardrails,

1065
00:38:08,670 --> 00:38:11,910
and then deploy it using
SageMaker Unified studio.

1066
00:38:11,910 --> 00:38:16,350
So now is a time when Dr. Smith
comes in after his rounds.

1067
00:38:16,350 --> 00:38:18,480
He got an email saying, "Look,

1068
00:38:18,480 --> 00:38:21,330
the app is ready, you know, to share.

1069
00:38:21,330 --> 00:38:23,160
You can start asking the questions."

1070
00:38:23,160 --> 00:38:24,180
So he doesn't click on the link,

1071
00:38:24,180 --> 00:38:26,550
instead he goes and discovers that app.

1072
00:38:26,550 --> 00:38:27,840
And the app is there.

1073
00:38:27,840 --> 00:38:29,880
The first thing that
always prompts us is, like,

1074
00:38:29,880 --> 00:38:30,937
to test the app, right, like,

1075
00:38:30,937 --> 00:38:32,940
"Oh are you compliant?"

1076
00:38:32,940 --> 00:38:33,787
So let's go ahead and say like,

1077
00:38:33,787 --> 00:38:36,660
"Hey give the address of
a patient whom I know:

1078
00:38:36,660 --> 00:38:38,070
Owen Anderson.

1079
00:38:38,070 --> 00:38:39,390
Can I get the address?"

1080
00:38:39,390 --> 00:38:40,957
The guardrails kicks in, it says,

1081
00:38:40,957 --> 00:38:43,650
"Sorry, I can't give
you that information,"

1082
00:38:43,650 --> 00:38:46,110
enforcing that governance controls.

1083
00:38:46,110 --> 00:38:48,270
The next thing is I'm happy, as Dr. Smith,

1084
00:38:48,270 --> 00:38:50,130
to see that it's actually
a chat application

1085
00:38:50,130 --> 00:38:50,963
that's compliant,

1086
00:38:50,963 --> 00:38:52,890
but I know of a patient,

1087
00:38:52,890 --> 00:38:54,330
let me make sure that I can see

1088
00:38:54,330 --> 00:38:56,550
if that patient actually
has that information

1089
00:38:56,550 --> 00:38:57,930
that I can validate, right?

1090
00:38:57,930 --> 00:38:59,310
Like, is the data right?

1091
00:38:59,310 --> 00:39:00,697
So he basically asked like,

1092
00:39:00,697 --> 00:39:03,780
"Okay, tell me something
about Own Anderson."

1093
00:39:03,780 --> 00:39:06,630
The chat app is able to use
the knowledge base we created

1094
00:39:06,630 --> 00:39:09,580
and rightly summarize that
information in a very clean way.

1095
00:39:10,590 --> 00:39:13,140
The next thing, getting more confident,

1096
00:39:13,140 --> 00:39:14,647
he asks another question:

1097
00:39:14,647 --> 00:39:17,460
"How many patients are
there with brain cancer?"

1098
00:39:17,460 --> 00:39:18,930
He uses just a very simple term,

1099
00:39:18,930 --> 00:39:21,480
not even, like, anything
like oligodendroglioma,

1100
00:39:21,480 --> 00:39:23,820
none of that, just a simple brain cancer.

1101
00:39:23,820 --> 00:39:25,980
The chat app, with the help
of large language models,

1102
00:39:25,980 --> 00:39:28,200
is intelligent enough to
understand the context

1103
00:39:28,200 --> 00:39:29,760
with all the metadata that we are provided

1104
00:39:29,760 --> 00:39:31,020
and provide, like,

1105
00:39:31,020 --> 00:39:34,170
really solid responses
interacting with a physician,

1106
00:39:34,170 --> 00:39:37,080
a neuro-oncologist, which
is, I think, pretty cool.

1107
00:39:37,080 --> 00:39:39,397
And now he basically says like,

1108
00:39:39,397 --> 00:39:40,560
"Look, I have a meeting tomorrow

1109
00:39:40,560 --> 00:39:42,360
with a 24-year-old patient.

1110
00:39:42,360 --> 00:39:43,440
She's a female.

1111
00:39:43,440 --> 00:39:46,080
Find me patients like her.

1112
00:39:46,080 --> 00:39:48,930
And tell me the number
of comparable patients.

1113
00:39:48,930 --> 00:39:50,970
What is the histology of those patients?

1114
00:39:50,970 --> 00:39:51,803
Tell me, like,

1115
00:39:51,803 --> 00:39:53,460
what kind of treatments
they've gone through.

1116
00:39:53,460 --> 00:39:55,500
And just be grounded with facts.

1117
00:39:55,500 --> 00:39:56,670
Don't hallucinate," right?

1118
00:39:56,670 --> 00:39:58,711
Like, he's kind of prompting the system.

1119
00:39:58,711 --> 00:40:01,620
And the model really behaves well.

1120
00:40:01,620 --> 00:40:03,300
It basically gets that data,

1121
00:40:03,300 --> 00:40:05,010
provides the necessary information.

1122
00:40:05,010 --> 00:40:07,170
It says like, "Look, I found
a patient similar to her.

1123
00:40:07,170 --> 00:40:08,190
She's on this protocol,"

1124
00:40:08,190 --> 00:40:09,720
and all kinds of information.

1125
00:40:09,720 --> 00:40:12,480
Again, all of this is synthetic
data that we manufactured.

1126
00:40:12,480 --> 00:40:13,710
I can only imagine, you know,

1127
00:40:13,710 --> 00:40:16,020
if you start using it on your real data,

1128
00:40:16,020 --> 00:40:18,570
I'm sure you'll get pretty
good results of this.

1129
00:40:18,570 --> 00:40:19,800
But at this point,

1130
00:40:19,800 --> 00:40:23,910
Dr. Smith is able to get
the required responses

1131
00:40:23,910 --> 00:40:25,860
that he wanted

1132
00:40:25,860 --> 00:40:28,320
and he's ready to prepare for his meeting

1133
00:40:28,320 --> 00:40:30,450
and he's ready to go for tomorrow.

1134
00:40:30,450 --> 00:40:33,960
So I know we took a healthcare example.

1135
00:40:33,960 --> 00:40:35,580
You may all be from different industries,

1136
00:40:35,580 --> 00:40:37,620
but in every part of your journey

1137
00:40:37,620 --> 00:40:40,860
there is SageMaker there to help you,

1138
00:40:40,860 --> 00:40:42,360
help accelerate through that journey.

1139
00:40:42,360 --> 00:40:44,370
And I'm hoping that after the talk

1140
00:40:44,370 --> 00:40:47,780
you will go and you will try
out SageMaker Unified Studio

1141
00:40:47,780 --> 00:40:48,870
or Catalog.

1142
00:40:48,870 --> 00:40:52,260
And we just can't wait to
hear what you do with that.

1143
00:40:52,260 --> 00:40:56,580
With that, I wanted to
invite our customer speaker,

1144
00:40:56,580 --> 00:40:57,990
Avinash, from Bayer,

1145
00:40:57,990 --> 00:41:00,900
to tell you how they're
actually doing these things

1146
00:41:00,900 --> 00:41:03,420
in real world at Bayer.

1147
00:41:03,420 --> 00:41:05,830
And thank you very much.

1148
00:41:05,830 --> 00:41:07,951
(crowd chattering)

1149
00:41:07,951 --> 00:41:11,201
(attendees applauding)

1150
00:41:13,620 --> 00:41:14,453
- Good afternoon, everyone.

1151
00:41:14,453 --> 00:41:16,140
Can you guys hear me okay?

1152
00:41:16,140 --> 00:41:17,340
Wonderful.

1153
00:41:17,340 --> 00:41:18,240
I'm Avinash Erupaka.

1154
00:41:18,240 --> 00:41:21,540
I'm a principal engineering
lead at Bayer Pharmaceutical.

1155
00:41:21,540 --> 00:41:24,720
So Bayer is 150-year-old
life sciences organization

1156
00:41:24,720 --> 00:41:27,600
that is driven by the
mission of health for all,

1157
00:41:27,600 --> 00:41:28,770
hunger for none.

1158
00:41:28,770 --> 00:41:32,190
We operate across different
life science verticals

1159
00:41:32,190 --> 00:41:33,420
combining crop science,

1160
00:41:33,420 --> 00:41:35,343
consumer health, and pharmaceutical.

1161
00:41:36,450 --> 00:41:39,420
So today I'm here to talk

1162
00:41:39,420 --> 00:41:42,360
about one of the biggest challenge

1163
00:41:42,360 --> 00:41:44,550
that we have in modern-day drug discovery,

1164
00:41:44,550 --> 00:41:48,450
that is data fragmentation
and contextualization.

1165
00:41:48,450 --> 00:41:49,503
For decades,

1166
00:41:50,580 --> 00:41:53,280
search for new therapies has
been a race against time,

1167
00:41:53,280 --> 00:41:55,980
with massive investments pouring into R&D.

1168
00:41:55,980 --> 00:41:59,250
But one of the biggest
non-scientific challenge

1169
00:41:59,250 --> 00:42:03,570
that we have is not the
complexity of biology,

1170
00:42:03,570 --> 00:42:06,210
but our ability to leverage existing data

1171
00:42:06,210 --> 00:42:07,920
and accelerating AI.

1172
00:42:07,920 --> 00:42:11,520
Data that's trapped in silos
slows down our scientists

1173
00:42:11,520 --> 00:42:13,983
and bog down critical decision making.

1174
00:42:16,320 --> 00:42:18,660
We, our mission,

1175
00:42:18,660 --> 00:42:21,180
our team's mission is
very straightforward.

1176
00:42:21,180 --> 00:42:24,690
We are here to break down the silos,

1177
00:42:24,690 --> 00:42:27,543
unlock speed, trust, and scale.

1178
00:42:28,380 --> 00:42:31,770
When the data is locked
in silos, we lose speed.

1179
00:42:31,770 --> 00:42:35,520
When compliance, lineage,

1180
00:42:35,520 --> 00:42:38,160
and consent are not clear, we lose trust.

1181
00:42:38,160 --> 00:42:40,530
And without platform-level patterns,

1182
00:42:40,530 --> 00:42:41,523
we cannot scale.

1183
00:42:43,920 --> 00:42:45,180
So the vision that we have

1184
00:42:45,180 --> 00:42:48,450
for our Bayer data science ecosystem

1185
00:42:48,450 --> 00:42:50,310
is a platform built on the principles

1186
00:42:50,310 --> 00:42:53,010
of distributed data mesh,

1187
00:42:53,010 --> 00:42:57,150
along with analytical and AI workbenches

1188
00:42:57,150 --> 00:43:01,710
plus a central governance
spine and a central catalog.

1189
00:43:01,710 --> 00:43:04,710
We operate across vital data domains,

1190
00:43:04,710 --> 00:43:07,023
like omics, clinical, chemistry,

1191
00:43:07,980 --> 00:43:12,240
and we operate in a distributed
data mesh architecture.

1192
00:43:12,240 --> 00:43:16,200
Think of DSE as a supply chain fabric

1193
00:43:16,200 --> 00:43:19,773
that connects data and AI across R&D.

1194
00:43:22,740 --> 00:43:24,240
In a data mesh architecture,

1195
00:43:24,240 --> 00:43:26,100
the ownership of these data domains

1196
00:43:26,100 --> 00:43:28,260
is federated to the data owners.

1197
00:43:28,260 --> 00:43:31,560
For example, the biomarker
team owns the biomarker data,

1198
00:43:31,560 --> 00:43:34,230
the omics team owns the sequencing data,

1199
00:43:34,230 --> 00:43:35,160
so on and so forth,

1200
00:43:35,160 --> 00:43:37,110
while the central platform

1201
00:43:37,110 --> 00:43:41,400
manages the the interoperability

1202
00:43:41,400 --> 00:43:43,000
and scalability of the platform.

1203
00:43:44,280 --> 00:43:48,780
Our team solves these
cross-cutting concerns

1204
00:43:48,780 --> 00:43:52,800
by providing turnkey
solutions around data ops,

1205
00:43:52,800 --> 00:43:56,850
AI/ML ops, cost hygiene,
observability, monitoring,

1206
00:43:56,850 --> 00:44:00,210
and also validated reusable templates

1207
00:44:00,210 --> 00:44:02,550
by removing all the
infrastructural concerns

1208
00:44:02,550 --> 00:44:06,690
that bog down our scientists,
where our practitioners,

1209
00:44:06,690 --> 00:44:09,090
like machine learning researchers,

1210
00:44:09,090 --> 00:44:11,460
bioinformaticians, biostatisticians,

1211
00:44:11,460 --> 00:44:14,970
can focus 100% of their time on science

1212
00:44:14,970 --> 00:44:16,113
and not service.

1213
00:44:17,340 --> 00:44:20,100
And the glue that brings
all of this together

1214
00:44:20,100 --> 00:44:22,050
is SageMaker Unified Studio.

1215
00:44:22,050 --> 00:44:23,700
SageMaker Unified Studio

1216
00:44:23,700 --> 00:44:27,273
acts as a central
governance control plane,

1217
00:44:28,230 --> 00:44:29,400
a front door,

1218
00:44:29,400 --> 00:44:33,870
where it enables all of the capabilities

1219
00:44:33,870 --> 00:44:37,530
and enables these data
domain owners to engineer

1220
00:44:37,530 --> 00:44:40,053
and publish these assets into the catalog.

1221
00:44:40,980 --> 00:44:44,850
These assets are unstructured S3 tables,

1222
00:44:44,850 --> 00:44:46,890
structured S3 assets,

1223
00:44:46,890 --> 00:44:49,500
analytical assets like
machine learning features,

1224
00:44:49,500 --> 00:44:53,190
machine learning models,
inference endpoints, agents,

1225
00:44:53,190 --> 00:44:55,050
so on and so forth.

1226
00:44:55,050 --> 00:44:58,470
So the seamless integration

1227
00:44:58,470 --> 00:45:00,750
where these distributed data domains

1228
00:45:00,750 --> 00:45:03,720
kind of merge into a unified catalog

1229
00:45:03,720 --> 00:45:07,080
is what enables the holy grail of R&D,

1230
00:45:07,080 --> 00:45:09,450
which is multimodal analytics.

1231
00:45:09,450 --> 00:45:12,900
So just to quickly...

1232
00:45:12,900 --> 00:45:13,740
Quick raise of hands.

1233
00:45:13,740 --> 00:45:16,660
How many of you are dealing
with multimodal data

1234
00:45:17,670 --> 00:45:18,993
at your organizations?

1235
00:45:19,950 --> 00:45:21,210
Great. Great.

1236
00:45:21,210 --> 00:45:25,050
So search for new therapies

1237
00:45:25,050 --> 00:45:27,810
is fundamentally a multimodal challenge.

1238
00:45:27,810 --> 00:45:29,943
In order for us to understand a disease,

1239
00:45:30,990 --> 00:45:32,493
drug mechanism of action,

1240
00:45:33,540 --> 00:45:35,790
and how a patient is responding,

1241
00:45:35,790 --> 00:45:38,670
we cannot look at this data in isolation.

1242
00:45:38,670 --> 00:45:43,380
So if you look at a critical
domain like oncology,

1243
00:45:43,380 --> 00:45:45,415
approximately 80% of the data

1244
00:45:45,415 --> 00:45:49,143
across key therapeutic areas
is multimodal in nature.

1245
00:45:50,670 --> 00:45:52,710
None of this would be possible if we have,

1246
00:45:52,710 --> 00:45:55,023
our data is in silos, okay?

1247
00:45:56,310 --> 00:45:58,620
Our internal biomarker archive,

1248
00:45:58,620 --> 00:46:02,340
a terabyte-scale of
valuable study-specific data

1249
00:46:02,340 --> 00:46:03,780
is a clear example.

1250
00:46:03,780 --> 00:46:07,560
It was trapped in silos, hard
to access, barely findable,

1251
00:46:07,560 --> 00:46:09,900
and generally slowing down the boat.

1252
00:46:09,900 --> 00:46:13,650
So the challenge we had at
hand was to handle the scale

1253
00:46:13,650 --> 00:46:16,200
by still maintaining the
regulatory compliance

1254
00:46:16,200 --> 00:46:18,510
and dealing with these
multimodal capabilities

1255
00:46:18,510 --> 00:46:20,110
that we are planning to deliver.

1256
00:46:21,270 --> 00:46:23,250
So the solution is a three-step process.

1257
00:46:23,250 --> 00:46:26,490
The first step is where
we start by ingesting

1258
00:46:26,490 --> 00:46:29,520
our on-prem biomarker data

1259
00:46:29,520 --> 00:46:31,863
into S3 study-specific buckets.

1260
00:46:33,150 --> 00:46:36,210
Then we enable the trust layer.

1261
00:46:36,210 --> 00:46:38,850
Because this data is GXP in nature,

1262
00:46:38,850 --> 00:46:41,880
we enable automated ALCOA+ data controls,

1263
00:46:41,880 --> 00:46:45,180
making this data attributable, legible,

1264
00:46:45,180 --> 00:46:48,390
contemporaneous, original, and accurate.

1265
00:46:48,390 --> 00:46:52,080
And we achieve this entirely
by AWS-native controls.

1266
00:46:52,080 --> 00:46:53,100
Trust.

1267
00:46:53,100 --> 00:46:55,200
By using S3 object locking mechanism

1268
00:46:55,200 --> 00:46:56,040
in compliance mode,

1269
00:46:56,040 --> 00:46:58,860
we ensure immutability of this data,

1270
00:46:58,860 --> 00:47:00,810
critical for regulatory compliance.

1271
00:47:00,810 --> 00:47:01,643
Security.

1272
00:47:01,643 --> 00:47:05,340
With S3 server-side encryption with KMS,

1273
00:47:05,340 --> 00:47:07,590
we ensure our data is at rest.

1274
00:47:07,590 --> 00:47:09,810
By leveraging S3 access grants,

1275
00:47:09,810 --> 00:47:11,700
we make sure that there
is a tight boundary

1276
00:47:11,700 --> 00:47:13,800
of authentication and
authorization controls

1277
00:47:13,800 --> 00:47:15,120
that are in place.

1278
00:47:15,120 --> 00:47:16,140
Auditability.

1279
00:47:16,140 --> 00:47:19,290
By leveraging versioning and CloudTrail,

1280
00:47:19,290 --> 00:47:22,740
we make sure the
technical lineage of data,

1281
00:47:22,740 --> 00:47:24,993
how the data is accessed is captured,

1282
00:47:25,958 --> 00:47:27,723
which is critical for GXP data.

1283
00:47:28,950 --> 00:47:32,460
The second step is where our
automation engine picks up.

1284
00:47:32,460 --> 00:47:34,830
So our automation engine gets into action

1285
00:47:34,830 --> 00:47:38,250
by provisioning all the
infrastructure peripherals

1286
00:47:38,250 --> 00:47:40,590
and the components like
SageMaker Unified Studio

1287
00:47:40,590 --> 00:47:42,280
and its underlying components

1288
00:47:44,148 --> 00:47:46,050
for the study projects.

1289
00:47:46,050 --> 00:47:49,950
And then it starts off by
creating an authentication

1290
00:47:49,950 --> 00:47:51,690
and authorization boundary

1291
00:47:51,690 --> 00:47:53,580
by leveraging the S3 access grants

1292
00:47:53,580 --> 00:47:56,580
and directly mapping
them to these projects.

1293
00:47:56,580 --> 00:48:00,270
Third step is where we register

1294
00:48:00,270 --> 00:48:03,960
the study specific-unstructured
S3 locations

1295
00:48:03,960 --> 00:48:07,080
as assets into the catalog,

1296
00:48:07,080 --> 00:48:10,530
immediately making it
findable and self-describing.

1297
00:48:10,530 --> 00:48:11,700
Through this process,

1298
00:48:11,700 --> 00:48:15,120
we also capture multifaceted metadata,

1299
00:48:15,120 --> 00:48:18,510
administrative metadata in
terms of data ownership,

1300
00:48:18,510 --> 00:48:22,350
data access, licensing needs of the data,

1301
00:48:22,350 --> 00:48:25,720
and also domain and
asset-specific metadata

1302
00:48:26,640 --> 00:48:29,640
with rich business
contextual metadata tags,

1303
00:48:29,640 --> 00:48:31,290
creating a control layer,

1304
00:48:31,290 --> 00:48:33,303
making this data ready for AI.

1305
00:48:35,580 --> 00:48:37,050
We also...

1306
00:48:37,050 --> 00:48:38,070
Then the third phase

1307
00:48:38,070 --> 00:48:41,160
is where the closed-loop
execution comes into the picture.

1308
00:48:41,160 --> 00:48:46,160
Now users start acting on
this secured, governed data.

1309
00:48:46,530 --> 00:48:49,020
The AI consumers now consume this data

1310
00:48:49,020 --> 00:48:52,950
from the central catalog in
a secured, governed manner,

1311
00:48:52,950 --> 00:48:55,890
and they will start
engineering additional assets

1312
00:48:55,890 --> 00:48:57,300
by leveraging this data.

1313
00:48:57,300 --> 00:49:00,180
These assets can be
cross-study fact tables

1314
00:49:00,180 --> 00:49:02,130
by exploring dimensional data,

1315
00:49:02,130 --> 00:49:05,100
which is critical for
future consent management.

1316
00:49:05,100 --> 00:49:08,490
AI consumers can also produce
additional analytical assets

1317
00:49:08,490 --> 00:49:09,930
like machine learning models,

1318
00:49:09,930 --> 00:49:13,133
machine learning features,
agents, et cetera,

1319
00:49:13,133 --> 00:49:14,460
and in turn,

1320
00:49:14,460 --> 00:49:18,330
all of these assets are
published back into the catalog.

1321
00:49:18,330 --> 00:49:22,560
So this powerful phenomena
of closed-loop execution

1322
00:49:22,560 --> 00:49:24,030
where you consume

1323
00:49:24,030 --> 00:49:26,670
and produce back into the same catalog

1324
00:49:26,670 --> 00:49:30,990
really creates a lineage-driven
closed-loop execution,

1325
00:49:30,990 --> 00:49:32,860
which helps us answer question

1326
00:49:33,780 --> 00:49:36,000
not only who has access to this data,

1327
00:49:36,000 --> 00:49:38,310
but what is part of this data packet.

1328
00:49:38,310 --> 00:49:41,280
It helps us to track
the journey of the data

1329
00:49:41,280 --> 00:49:42,360
from a raw file,

1330
00:49:42,360 --> 00:49:45,210
from a raw unstructured file to a model,

1331
00:49:45,210 --> 00:49:48,153
really solving the provenance
concerns of a model.

1332
00:49:49,680 --> 00:49:51,930
None of this would be possible, you know,

1333
00:49:51,930 --> 00:49:55,200
if we were operating in
a siloed environment.

1334
00:49:55,200 --> 00:49:59,960
This truly unleashes the
capability of AI and agentic AI

1335
00:50:01,742 --> 00:50:03,750
in the space of R&D.

1336
00:50:03,750 --> 00:50:05,070
To demonstrate this,

1337
00:50:05,070 --> 00:50:09,600
I wanna showcase a real-time,

1338
00:50:09,600 --> 00:50:13,140
high-value agentic AI use
case that we are pursuing

1339
00:50:13,140 --> 00:50:15,603
in the space of biomarker
data engineering.

1340
00:50:17,340 --> 00:50:21,990
So one of the challenge
in early clinical trials

1341
00:50:21,990 --> 00:50:24,660
is time to harmonize data.

1342
00:50:24,660 --> 00:50:26,970
The ability for us to get data

1343
00:50:26,970 --> 00:50:29,280
from our laboratory
information management systems,

1344
00:50:29,280 --> 00:50:30,693
from our clinical systems,

1345
00:50:31,980 --> 00:50:34,290
running it through data
engineering pipelines,

1346
00:50:34,290 --> 00:50:39,290
QCing data is a week-long
effort, manual effort,

1347
00:50:39,660 --> 00:50:43,293
critically impacting
our go/no-go decisions.

1348
00:50:44,580 --> 00:50:47,160
So we deployed an agentic AI solution,

1349
00:50:47,160 --> 00:50:48,870
that's deployed on Bedrock,

1350
00:50:48,870 --> 00:50:53,460
which automates this
biomarker data ETL process.

1351
00:50:53,460 --> 00:50:55,170
So first,

1352
00:50:55,170 --> 00:50:58,080
the agent starts off by
gathering information

1353
00:50:58,080 --> 00:51:02,313
from our laboratory information
management system, LIMS,

1354
00:51:04,140 --> 00:51:05,910
our clinical systems,

1355
00:51:05,910 --> 00:51:08,160
and our study specific data.

1356
00:51:08,160 --> 00:51:11,940
Then next it starts to
parse this information.

1357
00:51:11,940 --> 00:51:14,190
The agent leverages LLM

1358
00:51:14,190 --> 00:51:16,170
that is fine-tuned on vector

1359
00:51:16,170 --> 00:51:19,440
indexed on internal protocols and SOPs,

1360
00:51:19,440 --> 00:51:24,440
and it starts to parse this
data and identify the patterns.

1361
00:51:25,290 --> 00:51:26,610
Second.

1362
00:51:26,610 --> 00:51:31,260
Third, the agent not
only acts, but, you know,

1363
00:51:31,260 --> 00:51:33,510
it orchestrates the whole flow,

1364
00:51:33,510 --> 00:51:36,750
where it picks the right
data engineering pipeline,

1365
00:51:36,750 --> 00:51:38,970
biomarker data engineering pipeline,

1366
00:51:38,970 --> 00:51:43,200
and engineers the product
and QCs the metrics.

1367
00:51:43,200 --> 00:51:47,220
The final step is a result
of a strategic asset

1368
00:51:47,220 --> 00:51:51,690
where we have a multidimensional
biomarker data cube

1369
00:51:51,690 --> 00:51:55,500
that is further published
into the catalog,

1370
00:51:55,500 --> 00:51:58,263
which is very valuable
for further analysis.

1371
00:51:59,130 --> 00:52:01,350
So as an outcome,

1372
00:52:01,350 --> 00:52:03,330
here we are able to remove

1373
00:52:03,330 --> 00:52:06,240
weeks' long worth of manual effort.

1374
00:52:06,240 --> 00:52:09,150
We are able to really accelerate

1375
00:52:09,150 --> 00:52:11,400
the time to harmonize data

1376
00:52:11,400 --> 00:52:16,020
for pharmacokinetic and
pharmacodynamic analysis,

1377
00:52:16,020 --> 00:52:20,490
in turn accelerating
our proof of mechanism

1378
00:52:20,490 --> 00:52:23,553
and go/no-go decisions,

1379
00:52:24,720 --> 00:52:27,870
which is very critical for
clinical trial success.

1380
00:52:27,870 --> 00:52:32,340
We are also leveraging
patient data in real time,

1381
00:52:32,340 --> 00:52:34,560
understanding the drug response,

1382
00:52:34,560 --> 00:52:37,503
improving our success of clinical trials.

1383
00:52:38,790 --> 00:52:42,960
So this closed-loop
architecture really provides us

1384
00:52:42,960 --> 00:52:47,280
the much-needed value unlocking that we do

1385
00:52:47,280 --> 00:52:51,183
in terms of, you know,
trust, speed, and scale.

1386
00:52:53,550 --> 00:52:56,490
Beyond the quantitative benefits,

1387
00:52:56,490 --> 00:53:01,080
this architectural shift
also delivers, you know,

1388
00:53:01,080 --> 00:53:03,240
quantifiable business value

1389
00:53:03,240 --> 00:53:06,420
in terms of capturing
multifaceted metadata,

1390
00:53:06,420 --> 00:53:10,170
technical metadata
guaranteed by S3 versioning

1391
00:53:10,170 --> 00:53:13,740
and CloudTrail logs and
closed-loop execution.

1392
00:53:13,740 --> 00:53:16,530
We are exactly able to track
the journey of the data

1393
00:53:16,530 --> 00:53:19,263
and exactly answer who
has access to this data.

1394
00:53:20,310 --> 00:53:23,610
With rich business metadata tags

1395
00:53:23,610 --> 00:53:27,300
and contextual information
that we are capturing,

1396
00:53:27,300 --> 00:53:29,760
plus the cross-study fact tables,

1397
00:53:29,760 --> 00:53:32,520
we are able to answer which data

1398
00:53:32,520 --> 00:53:35,040
is actually used for this analysis.

1399
00:53:35,040 --> 00:53:38,250
Let's say in case of
a consent-based study,

1400
00:53:38,250 --> 00:53:41,070
a patient pulls consent, we are able to,

1401
00:53:41,070 --> 00:53:42,660
the system is able to determine

1402
00:53:42,660 --> 00:53:46,290
exactly which part of
the analysis is impacted,

1403
00:53:46,290 --> 00:53:49,113
which is critical for
consent-based reusability.

1404
00:53:50,580 --> 00:53:52,860
Our roadmap is very clear.

1405
00:53:52,860 --> 00:53:56,670
We intend to expand our ecosystem

1406
00:53:56,670 --> 00:54:00,150
by onboarding more data domains,

1407
00:54:00,150 --> 00:54:03,660
industrialize our AI use cases,

1408
00:54:03,660 --> 00:54:08,660
and truly explore the potential
of multimodal analytics

1409
00:54:08,790 --> 00:54:11,760
by impacting our pipeline,

1410
00:54:11,760 --> 00:54:15,480
optimizing the cost and time that it takes

1411
00:54:15,480 --> 00:54:18,510
for bringing critical
life-saving therapies

1412
00:54:18,510 --> 00:54:19,593
to our patients.

1413
00:54:21,030 --> 00:54:21,963
In closing,

1414
00:54:22,830 --> 00:54:26,850
our journey of migrating

1415
00:54:26,850 --> 00:54:29,680
a 300-terabyte unstructured data

1416
00:54:31,140 --> 00:54:34,440
which is stuck in an on-prem environment

1417
00:54:34,440 --> 00:54:37,350
and turning into an
AI-ready strategic asset

1418
00:54:37,350 --> 00:54:39,450
is not just a technology story,

1419
00:54:39,450 --> 00:54:42,033
it is a story about empowering science.

1420
00:54:43,290 --> 00:54:47,040
We have turned an asset
that was barely accessible,

1421
00:54:47,040 --> 00:54:48,150
barely findable,

1422
00:54:48,150 --> 00:54:50,910
and accessible only through friction,

1423
00:54:50,910 --> 00:54:54,780
and turned it into a strategic AI asset

1424
00:54:54,780 --> 00:54:58,560
which could streamline
our pipeline activities,

1425
00:54:58,560 --> 00:55:02,433
it could improve efficiency
of our R&D decision making,

1426
00:55:03,840 --> 00:55:07,080
and overall laying a solid foundation

1427
00:55:07,080 --> 00:55:11,343
for our future precision
medicine activities.

1428
00:55:12,630 --> 00:55:15,603
So with this, I thank
you all for tuning in.

1429
00:55:16,890 --> 00:55:20,490
So feel free to let us know your feedback

1430
00:55:20,490 --> 00:55:21,480
so we can improve

1431
00:55:21,480 --> 00:55:23,610
and deliver more talks of this sort.

1432
00:55:23,610 --> 00:55:24,610
Thank you very much.

