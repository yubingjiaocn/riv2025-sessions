1
00:00:00,900 --> 00:00:03,780
- Hello, everyone, thanks
for joining the session.

2
00:00:03,780 --> 00:00:06,060
So today, we're gonna
talk a little bit about

3
00:00:06,060 --> 00:00:10,350
one of the growing performance
challenges with AWS customers

4
00:00:10,350 --> 00:00:13,560
and really the hybrid
cloud customers in general.

5
00:00:13,560 --> 00:00:15,570
And that has to do with agentic AI

6
00:00:15,570 --> 00:00:18,330
and AI applications in general,

7
00:00:18,330 --> 00:00:20,130
which, although they're very powerful,

8
00:00:20,130 --> 00:00:23,910
they tend to be unpredictable in usage

9
00:00:23,910 --> 00:00:25,770
from a resource perspective.

10
00:00:25,770 --> 00:00:29,340
So we're gonna spend a
bit of time talking about

11
00:00:29,340 --> 00:00:32,160
how this problem kind of manifests,

12
00:00:32,160 --> 00:00:34,650
a little bit more about
the problem itself,

13
00:00:34,650 --> 00:00:37,410
and then as well discuss
some potential solutions,

14
00:00:37,410 --> 00:00:39,900
because, ultimately, you wanna make sure

15
00:00:39,900 --> 00:00:43,620
that you maintain the
performance of the end users

16
00:00:43,620 --> 00:00:46,680
who are using the agentic AI,

17
00:00:46,680 --> 00:00:49,890
but also make sure that you
are as efficient as possible

18
00:00:49,890 --> 00:00:52,620
when it comes to the cost allocations,

19
00:00:52,620 --> 00:00:54,060
so you can avoid that bloat.

20
00:00:54,060 --> 00:00:55,660
So all right, let's get into it.

21
00:00:57,150 --> 00:00:58,320
So before I get started,

22
00:00:58,320 --> 00:01:00,630
would love to get some
feedback from the audience,

23
00:01:00,630 --> 00:01:03,360
just a quick poll in
terms of where you are

24
00:01:03,360 --> 00:01:05,253
with adoption of agentic AI.

25
00:01:06,450 --> 00:01:08,880
Raise your hands if you're just exploring,

26
00:01:08,880 --> 00:01:10,170
like you're just getting started

27
00:01:10,170 --> 00:01:12,195
with learning about agentic AI

28
00:01:12,195 --> 00:01:15,060
and where it might apply
for your environment.

29
00:01:15,060 --> 00:01:16,860
Okay, make sense.

30
00:01:16,860 --> 00:01:18,840
How many of you are
running proof of concepts

31
00:01:18,840 --> 00:01:20,670
where you actually have some workloads

32
00:01:20,670 --> 00:01:24,660
and it's in development
and you're seeing that?

33
00:01:24,660 --> 00:01:26,010
Okay, great.

34
00:01:26,010 --> 00:01:27,600
How many of you are in early production

35
00:01:27,600 --> 00:01:30,180
where you actually have some real users

36
00:01:30,180 --> 00:01:33,360
and a number of work
streams applied to that?

37
00:01:33,360 --> 00:01:34,193
Great.

38
00:01:34,193 --> 00:01:35,880
And then last, business critical

39
00:01:35,880 --> 00:01:38,370
where like you've got real users,

40
00:01:38,370 --> 00:01:39,390
you're using this at scale,

41
00:01:39,390 --> 00:01:41,250
it's a big part of your
ROI, you're seeing that.

42
00:01:41,250 --> 00:01:42,510
Anybody in that category?

43
00:01:42,510 --> 00:01:43,590
Okay. Alright.

44
00:01:43,590 --> 00:01:46,140
So not quite yet that at
that maturity adoption level,

45
00:01:46,140 --> 00:01:47,610
but a lot of that kind of makes sense

46
00:01:47,610 --> 00:01:49,713
given what we see from our customers, too.

47
00:01:51,240 --> 00:01:56,240
Okay, so just expanding on
the hidden cost of agentic AI.

48
00:01:56,460 --> 00:01:57,293
And a lot of times,

49
00:01:57,293 --> 00:02:00,150
what you see is overprovisioning
and resource bloat.

50
00:02:00,150 --> 00:02:01,710
And primarily, this comes down to

51
00:02:01,710 --> 00:02:06,710
because people don't make
performance or resource decisions

52
00:02:09,000 --> 00:02:10,650
based upon averages.

53
00:02:10,650 --> 00:02:13,140
Usually, they'll take
the worst-case scenario,

54
00:02:13,140 --> 00:02:16,350
because they're worried that
performance might suffer.

55
00:02:16,350 --> 00:02:17,610
And what that leads to

56
00:02:17,610 --> 00:02:20,550
is really conservative capacity decisions,

57
00:02:20,550 --> 00:02:22,680
because ultimately they might be afraid

58
00:02:22,680 --> 00:02:25,290
of this performance degradation,

59
00:02:25,290 --> 00:02:29,523
which will lead them to
oversized GPU instances,

60
00:02:30,600 --> 00:02:35,600
overallocate kind of RAM
farms, overprovision storage.

61
00:02:35,790 --> 00:02:39,810
And this leads to a lot of idle costs

62
00:02:39,810 --> 00:02:41,550
for these expensive resources.

63
00:02:41,550 --> 00:02:43,230
And so what you'll see from a lot of teams

64
00:02:43,230 --> 00:02:46,770
is that for fear of creating
these performance buffers,

65
00:02:46,770 --> 00:02:49,650
they will overallocate
these expensive resources.

66
00:02:49,650 --> 00:02:51,630
And then many times,

67
00:02:51,630 --> 00:02:53,730
just because they're afraid of that peak

68
00:02:53,730 --> 00:02:55,023
that might happen, right?

69
00:02:56,820 --> 00:03:00,720
There are static scaling
policies which are in place,

70
00:03:00,720 --> 00:03:02,880
but some of the problems
with these scaling policies

71
00:03:02,880 --> 00:03:04,320
is they tend to be reactive.

72
00:03:04,320 --> 00:03:06,780
And so if you have like
a peak that happens,

73
00:03:06,780 --> 00:03:08,430
then it's already too late

74
00:03:08,430 --> 00:03:10,950
for these scaling policies to take effect,

75
00:03:10,950 --> 00:03:14,580
because like I said, with
agentic AI and AI apps,

76
00:03:14,580 --> 00:03:17,640
a lot of times, the usage
is unpredictable, right?

77
00:03:17,640 --> 00:03:20,880
And it also requires a fair
amount of human intervention

78
00:03:20,880 --> 00:03:22,170
to stay on top of that.

79
00:03:22,170 --> 00:03:25,110
So you are consistently looking
at those scaling policies

80
00:03:25,110 --> 00:03:27,123
or taking remediation to deal with that.

81
00:03:28,680 --> 00:03:31,440
All right, so talking a little bit about

82
00:03:31,440 --> 00:03:35,292
how agentic AI is changing
how workloads behave.

83
00:03:35,292 --> 00:03:37,260
A lot of this comes down to, you know,

84
00:03:37,260 --> 00:03:39,511
with traditional legacy apps,

85
00:03:39,511 --> 00:03:43,020
they typically follow
like a linear process:

86
00:03:43,020 --> 00:03:45,900
a request comes in, it gets processed,

87
00:03:45,900 --> 00:03:47,277
and it consumes resources.

88
00:03:47,277 --> 00:03:49,470
And this typically follows
a straight line, right?

89
00:03:49,470 --> 00:03:52,530
As more users show up and
hit the site for example,

90
00:03:52,530 --> 00:03:56,460
then resources get consumed
in a linear fashion.

91
00:03:56,460 --> 00:03:58,800
With agentic AI and AI apps,

92
00:03:58,800 --> 00:04:00,300
it doesn't necessarily work that way.

93
00:04:00,300 --> 00:04:02,100
It's very unpredictable.

94
00:04:02,100 --> 00:04:06,690
It can plan, branch, and
then spawn many times,

95
00:04:06,690 --> 00:04:10,170
like a lot of different
hundreds of microworkloads.

96
00:04:10,170 --> 00:04:13,710
You could have gen AI talking
to another agentic AI,

97
00:04:13,710 --> 00:04:15,300
talking to another agentic AI,

98
00:04:15,300 --> 00:04:17,910
agentic AI talking to a number of APIs.

99
00:04:17,910 --> 00:04:20,100
And all this is kind
happening in the background,

100
00:04:20,100 --> 00:04:23,040
and it's creating these resource spikes

101
00:04:23,040 --> 00:04:24,900
that can sometimes bring applications down

102
00:04:24,900 --> 00:04:26,163
to its knees, right?

103
00:04:27,570 --> 00:04:30,030
One user request for example can come in

104
00:04:30,030 --> 00:04:33,480
and then create hundreds
of downstream tasks.

105
00:04:33,480 --> 00:04:38,480
And this leads to
overprovisioning of GPUs.

106
00:04:38,970 --> 00:04:42,030
You see issues with throttling and spiking

107
00:04:42,030 --> 00:04:43,980
and as evidenced here,

108
00:04:43,980 --> 00:04:45,000
like we've seen customers

109
00:04:45,000 --> 00:04:50,000
who have to deal with overprovisioning

110
00:04:50,910 --> 00:04:55,410
and sometimes utilization of 30%

111
00:04:55,410 --> 00:04:56,730
of what they should be,

112
00:04:56,730 --> 00:05:00,720
and 10 to 20 times the GPU
hours and really what's needed.

113
00:05:00,720 --> 00:05:02,490
And ultimately, you
end up with a situation

114
00:05:02,490 --> 00:05:05,790
of where these idle
resources just sit there

115
00:05:05,790 --> 00:05:08,370
because they're afraid
of this performance risk

116
00:05:08,370 --> 00:05:11,580
and then, ultimately,
become very wasteful,

117
00:05:11,580 --> 00:05:13,740
and a hidden tax upon the organization.

118
00:05:13,740 --> 00:05:16,950
So you really need to
consider how can I make sure

119
00:05:16,950 --> 00:05:18,870
that my resource usage is dynamic

120
00:05:18,870 --> 00:05:22,680
just as the agentic AI and AI
apps are dynamic themselves

121
00:05:22,680 --> 00:05:23,880
based upon their nature.

122
00:05:25,710 --> 00:05:28,650
All right, so talking about the gap

123
00:05:28,650 --> 00:05:29,700
between insight and action

124
00:05:29,700 --> 00:05:31,110
and just elaborating there.

125
00:05:31,110 --> 00:05:32,850
You know, you do have
observability solutions,

126
00:05:32,850 --> 00:05:35,310
which will do a great job
to give you information,

127
00:05:35,310 --> 00:05:38,100
such as is there CPU throttling happening?

128
00:05:38,100 --> 00:05:41,430
Is there GPU contention, or
are there latency spikes?

129
00:05:41,430 --> 00:05:43,890
So you can see observation there,

130
00:05:43,890 --> 00:05:45,720
but it doesn't necessarily do anything

131
00:05:45,720 --> 00:05:48,720
for, okay, so when that happens,
like what do I actually do?

132
00:05:48,720 --> 00:05:53,160
Maybe I can do an RCA, and
after the fact, go and fix that,

133
00:05:53,160 --> 00:05:56,400
but it doesn't actually take actions

134
00:05:56,400 --> 00:05:57,233
when you're dealing

135
00:05:57,233 --> 00:06:00,300
with these types of
agentic AI applications.

136
00:06:00,300 --> 00:06:02,580
On the FinOps side,
FinOps does a great job

137
00:06:02,580 --> 00:06:05,220
in terms of, okay, well how
much am I spending here?

138
00:06:05,220 --> 00:06:08,670
Or if I need to do reporting
for showback or chargeback,

139
00:06:08,670 --> 00:06:10,920
I can see that and
allocate it appropriately

140
00:06:10,920 --> 00:06:12,630
for these applications.

141
00:06:12,630 --> 00:06:14,760
But again, it's primarily
around reporting.

142
00:06:14,760 --> 00:06:17,310
It's not meant to do resizing of GPUs

143
00:06:17,310 --> 00:06:20,250
or to handle these
dynamic scenarios, right?

144
00:06:20,250 --> 00:06:22,980
It's really more about the cost allocation

145
00:06:22,980 --> 00:06:25,350
versus actually like solving the problem.

146
00:06:25,350 --> 00:06:27,780
So again, you have this problem

147
00:06:27,780 --> 00:06:30,540
where even with some of
these other solutions,

148
00:06:30,540 --> 00:06:33,660
in general, just oversizing
becomes the default.

149
00:06:33,660 --> 00:06:36,960
Where many of our customers
who come to talk to us

150
00:06:36,960 --> 00:06:39,420
about this problem will
essentially say, you know,

151
00:06:39,420 --> 00:06:40,830
because I'm worried about performance

152
00:06:40,830 --> 00:06:42,450
and maintain my SLOs,

153
00:06:42,450 --> 00:06:45,630
I'll just oversize what I
have in the public cloud

154
00:06:45,630 --> 00:06:48,630
or in even certain cases
in the private cloud.

155
00:06:48,630 --> 00:06:52,350
And then again, back to static
scaling, this is in place,

156
00:06:52,350 --> 00:06:54,210
but because of the nature of these apps,

157
00:06:54,210 --> 00:06:55,080
they're so dynamic,

158
00:06:55,080 --> 00:06:57,720
as I mentioned, in terms of
the way they do their work,

159
00:06:57,720 --> 00:06:59,790
they can't keep up with the demand.

160
00:06:59,790 --> 00:07:01,860
And so again, making sure

161
00:07:01,860 --> 00:07:03,690
that you really look at the problem

162
00:07:03,690 --> 00:07:07,050
from a resource perspective
and have dynamic resources

163
00:07:07,050 --> 00:07:09,810
for the platforms resource that support it

164
00:07:09,810 --> 00:07:13,023
to meet that ever-changing
demand is critically important.

165
00:07:15,060 --> 00:07:18,180
We're gonna skip this
poll just based on timing.

166
00:07:18,180 --> 00:07:19,323
So I'll move forward.

167
00:07:21,990 --> 00:07:25,950
So in our particular case,
so what Turbonomic does

168
00:07:25,950 --> 00:07:27,210
to deal with these problems

169
00:07:27,210 --> 00:07:30,000
is we will deliver realtime optimization

170
00:07:30,000 --> 00:07:31,920
to agentic AI workloads.

171
00:07:31,920 --> 00:07:36,150
And what we will do is
we will provide actions

172
00:07:36,150 --> 00:07:40,050
that look at the continuous
optimization by analyzing data

173
00:07:40,050 --> 00:07:43,740
that comes from the GPU instances itself,

174
00:07:43,740 --> 00:07:48,740
vCPU and vMem saturation,
throughput for storage and network

175
00:07:49,860 --> 00:07:52,920
and look over time at these metrics

176
00:07:52,920 --> 00:07:57,660
and then ultimately make
decisions across your environment

177
00:07:57,660 --> 00:08:01,770
to rightsize based upon what
we see from these metrics.

178
00:08:01,770 --> 00:08:03,990
So not only will we
analyze that information,

179
00:08:03,990 --> 00:08:05,550
we'll look at the entire supply chain

180
00:08:05,550 --> 00:08:07,140
from the agentic AI app,

181
00:08:07,140 --> 00:08:08,850
but also look at the resources
that are supporting it

182
00:08:08,850 --> 00:08:11,130
and real time scaled up and scaled down.

183
00:08:11,130 --> 00:08:14,130
And we look across EC2,
the GPUs themselves, EKS,

184
00:08:14,130 --> 00:08:15,570
and hybrid environments,

185
00:08:15,570 --> 00:08:16,650
of course, AWS,

186
00:08:16,650 --> 00:08:19,770
but also if you have
on-prem workloads as well

187
00:08:19,770 --> 00:08:21,183
or in other public clouds.

188
00:08:22,200 --> 00:08:23,033
And as we do this,

189
00:08:23,033 --> 00:08:24,750
we make sure we maintain performance

190
00:08:24,750 --> 00:08:26,490
and make sure we meet your SLOs

191
00:08:26,490 --> 00:08:30,540
but also keep things efficient,
so cost is under control.

192
00:08:30,540 --> 00:08:33,120
And as I mentioned, GPU optimization

193
00:08:33,120 --> 00:08:36,060
is one of the most impactful
use cases that can help

194
00:08:36,060 --> 00:08:37,020
with agentic AI,

195
00:08:37,020 --> 00:08:39,270
given how much of the
resources come from there.

196
00:08:39,270 --> 00:08:43,080
So as you see peaks come in

197
00:08:43,080 --> 00:08:45,540
where more of the resources are needed,

198
00:08:45,540 --> 00:08:46,800
we will rightsize them

199
00:08:46,800 --> 00:08:50,313
and bring in additional
GPU instances, if required,

200
00:08:51,420 --> 00:08:53,880
and leave them there.

201
00:08:53,880 --> 00:08:57,390
But as demand goes down, in real time,

202
00:08:57,390 --> 00:09:00,300
we'll rightsize those
instances and reduce them,

203
00:09:00,300 --> 00:09:02,670
as well as other resources like memory

204
00:09:02,670 --> 00:09:04,830
to reduce that overprovisioning

205
00:09:04,830 --> 00:09:06,360
and deal with some of the challenges

206
00:09:06,360 --> 00:09:07,740
that our customers face.

207
00:09:07,740 --> 00:09:09,900
So by doing this on a continuous basis,

208
00:09:09,900 --> 00:09:12,210
it takes a lot of the
human error out of it

209
00:09:12,210 --> 00:09:15,900
and allows you to know
that these agentic AI apps

210
00:09:15,900 --> 00:09:17,730
are going to run in a performant manner,

211
00:09:17,730 --> 00:09:19,683
but also as efficiently as possible.

212
00:09:20,880 --> 00:09:23,250
Alright, so I'm gonna take
you through an example.

213
00:09:23,250 --> 00:09:25,140
This is a screenshot from our product,

214
00:09:25,140 --> 00:09:27,060
and what we're looking at here

215
00:09:27,060 --> 00:09:30,210
is a pending action,

216
00:09:30,210 --> 00:09:32,700
which is essentially an
action that the user can take.

217
00:09:32,700 --> 00:09:35,580
So this is an example
of, like, a manual action

218
00:09:35,580 --> 00:09:39,090
and what Turbonomic has
been doing in this scenario

219
00:09:39,090 --> 00:09:44,090
is it's been analyzing the
GPU usage, the GPU memory,

220
00:09:44,370 --> 00:09:49,230
the instant sizing and
utilization in general.

221
00:09:49,230 --> 00:09:51,450
And it's looking at this particular GPU,

222
00:09:51,450 --> 00:09:55,170
which is a p3dn.24xlarge,

223
00:09:55,170 --> 00:10:00,170
and which is one of the
more expensive GPUs for AWS.

224
00:10:00,720 --> 00:10:02,520
And then it's essentially saying,

225
00:10:02,520 --> 00:10:05,490
based upon the utilization
that we're seeing,

226
00:10:05,490 --> 00:10:10,110
you can scale this down to a p3.8xlarge.

227
00:10:10,110 --> 00:10:11,850
And we're able to do that

228
00:10:11,850 --> 00:10:13,590
because we're looking
at the utilization data

229
00:10:13,590 --> 00:10:15,960
and matching this up with the SLOs

230
00:10:15,960 --> 00:10:19,200
and knowing that this is
going to be a safe action

231
00:10:19,200 --> 00:10:20,033
that could be taken.

232
00:10:20,033 --> 00:10:22,860
And as you can see just
from one single GPU,

233
00:10:22,860 --> 00:10:27,780
you can get a savings of
$13,800 per month, right?

234
00:10:27,780 --> 00:10:30,660
And this is still
keeping your performance,

235
00:10:30,660 --> 00:10:32,700
maintaining the SLOs.

236
00:10:32,700 --> 00:10:35,370
So just getting down to a
little bit of the details

237
00:10:35,370 --> 00:10:37,260
because, you know, it's
a great recommendation

238
00:10:37,260 --> 00:10:39,847
but our customers, our
operators want to know,

239
00:10:39,847 --> 00:10:41,820
"Well, how do I know
that I can kind of trust

240
00:10:41,820 --> 00:10:43,800
that that action is gonna
maintain performance?

241
00:10:43,800 --> 00:10:46,320
So I need to see an
additional level of details."

242
00:10:46,320 --> 00:10:48,210
So what you see here on the left hand side

243
00:10:48,210 --> 00:10:51,300
and some of the charts are, okay, look,

244
00:10:51,300 --> 00:10:54,660
starting with the GPU count
percentile and utilization,

245
00:10:54,660 --> 00:10:57,510
what it's saying is that utilization

246
00:10:57,510 --> 00:11:01,710
for this observation time
period is around 13%.

247
00:11:01,710 --> 00:11:05,100
So hasn't really been used that much,

248
00:11:05,100 --> 00:11:08,850
even though it's tied to
this EC2 instance, right?

249
00:11:08,850 --> 00:11:11,250
And then you look at the
GPU memory utilization

250
00:11:11,250 --> 00:11:13,950
and that's hovering around 22%.

251
00:11:13,950 --> 00:11:16,076
Again, not really something

252
00:11:16,076 --> 00:11:20,460
that is speaking to a
lot of usage in general,

253
00:11:20,460 --> 00:11:22,230
and it's kind of hard
to see with the vCPU,

254
00:11:22,230 --> 00:11:24,900
but I think that hovers around 3 to 4%.

255
00:11:24,900 --> 00:11:28,830
So not a ton of of resources
are being used for this app.

256
00:11:28,830 --> 00:11:30,570
So if you get into the resource impact,

257
00:11:30,570 --> 00:11:32,370
the red box on the right,

258
00:11:32,370 --> 00:11:34,620
you can see the current

259
00:11:34,620 --> 00:11:37,740
and then post-recommendation
for this action.

260
00:11:37,740 --> 00:11:42,300
And Turbonomic is recommending
to take the current GPU count

261
00:11:42,300 --> 00:11:43,890
from eight to four

262
00:11:43,890 --> 00:11:46,380
and predicting that this will lead

263
00:11:46,380 --> 00:11:49,440
from 13 to 26% utilization.

264
00:11:49,440 --> 00:11:50,910
That's a projection.

265
00:11:50,910 --> 00:11:54,720
And then on the memory, it's
going from 32 to 16 gigabytes

266
00:11:54,720 --> 00:11:58,110
and also predicting that the
utilization will go up to 44%

267
00:11:58,110 --> 00:11:59,370
after taking its action.

268
00:11:59,370 --> 00:12:00,510
Again, very safe,

269
00:12:00,510 --> 00:12:02,580
you're not gonna see a
performance degradation

270
00:12:02,580 --> 00:12:06,060
and then vCPU, it's around I think 13%

271
00:12:06,060 --> 00:12:09,630
and then you see storage
throughput, network throughput,

272
00:12:09,630 --> 00:12:11,340
in a similar situation.

273
00:12:11,340 --> 00:12:12,960
So again, the nice thing here

274
00:12:12,960 --> 00:12:15,810
is that, as an operator,
I can look at this action,

275
00:12:15,810 --> 00:12:16,650
I feel good about it

276
00:12:16,650 --> 00:12:18,300
because I know that the
performance is there

277
00:12:18,300 --> 00:12:20,370
and we're still gonna maintain our SLOs,

278
00:12:20,370 --> 00:12:21,570
and I feel comfortable doing that.

279
00:12:21,570 --> 00:12:24,300
So it's not just like a,
"Hey, you're gonna save,

280
00:12:24,300 --> 00:12:27,600
you know, the cost on this
action, you should just take it."

281
00:12:27,600 --> 00:12:29,340
It's giving that user the confidence

282
00:12:29,340 --> 00:12:31,050
that if I take this action,

283
00:12:31,050 --> 00:12:32,610
and potentially even automate it,

284
00:12:32,610 --> 00:12:34,394
I feel confident that
things will move forward

285
00:12:34,394 --> 00:12:35,973
in a positive manner.

286
00:12:36,930 --> 00:12:39,360
So I'll just wrap it up here
on this potential action.

287
00:12:39,360 --> 00:12:41,640
So you can see kind of
overall the summary,

288
00:12:41,640 --> 00:12:44,940
the on-demand rate goes
from $31.21 an hour

289
00:12:44,940 --> 00:12:47,692
down to $12.

290
00:12:47,692 --> 00:12:50,550
It does take it to account
ROI and savings plans.

291
00:12:50,550 --> 00:12:52,470
And then you see the on-demand cost,

292
00:12:52,470 --> 00:12:57,470
which again rolls up to around
$13,800 savings per month.

293
00:12:58,200 --> 00:13:01,080
And this is just from
one single GPU instance.

294
00:13:01,080 --> 00:13:04,020
So imagine if you have this
quite a big of investment,

295
00:13:04,020 --> 00:13:06,020
then you're gonna see even more savings.

296
00:13:07,320 --> 00:13:08,610
Alright, so let's talk a little bit

297
00:13:08,610 --> 00:13:11,280
about some more of the details

298
00:13:11,280 --> 00:13:13,860
in terms of how intelligent automation

299
00:13:13,860 --> 00:13:16,920
can really transform your AWS environment.

300
00:13:16,920 --> 00:13:19,440
Talked a lot about smart GPU optimization.

301
00:13:19,440 --> 00:13:22,590
So what we will do is
automatically tune those GPUs.

302
00:13:22,590 --> 00:13:24,030
We will rightsize, like I said,

303
00:13:24,030 --> 00:13:28,530
based upon as demand
spikes up in real time.

304
00:13:28,530 --> 00:13:30,120
And then we will bring
it down to make sure

305
00:13:30,120 --> 00:13:32,370
we're as efficient from a cost perspective

306
00:13:32,370 --> 00:13:33,330
as much as possible

307
00:13:33,330 --> 00:13:35,760
so you can eliminate that idle capacity.

308
00:13:35,760 --> 00:13:37,410
We also have real-time visibility.

309
00:13:37,410 --> 00:13:40,230
So our application looks
at all of the resources

310
00:13:40,230 --> 00:13:41,640
running in real time,

311
00:13:41,640 --> 00:13:43,020
and we have a supply chain

312
00:13:43,020 --> 00:13:45,750
that will map the business apps themselves

313
00:13:45,750 --> 00:13:47,580
all the way down to the resources.

314
00:13:47,580 --> 00:13:50,670
And we look across those resources

315
00:13:50,670 --> 00:13:55,080
and how they work together
ranging from EC2 and EKS

316
00:13:55,080 --> 00:13:57,120
and then the GPU environments
as I was mentioning,

317
00:13:57,120 --> 00:13:58,830
a ton of different resources.

318
00:13:58,830 --> 00:14:00,750
But we're able to correlate those metrics

319
00:14:00,750 --> 00:14:03,270
and make sure that no harm is done

320
00:14:03,270 --> 00:14:05,880
and that, ultimately, those applications

321
00:14:05,880 --> 00:14:07,410
are getting the resources that they need.

322
00:14:07,410 --> 00:14:09,090
And we'll also take into account,

323
00:14:09,090 --> 00:14:10,890
there might be a potential issue

324
00:14:10,890 --> 00:14:12,870
or a bottleneck in that supply chain,

325
00:14:12,870 --> 00:14:14,550
and it may not be hitting
one specific resource,

326
00:14:14,550 --> 00:14:16,173
but we'll identify that

327
00:14:16,173 --> 00:14:20,670
and take account to be proactive
about what we're doing.

328
00:14:20,670 --> 00:14:23,790
We also have orchestrator integration,

329
00:14:23,790 --> 00:14:26,580
which will understand
things like pod placement,

330
00:14:26,580 --> 00:14:29,700
affinity, resource quotas, and scheduling.

331
00:14:29,700 --> 00:14:32,050
So we'll optimize the
container infrastructure

332
00:14:33,180 --> 00:14:36,060
and the traditional
infrastructure layers together

333
00:14:36,060 --> 00:14:37,530
for those business apps.

334
00:14:37,530 --> 00:14:39,810
And we also have proactive
automation as well

335
00:14:39,810 --> 00:14:42,030
so the previous example I gave

336
00:14:42,030 --> 00:14:43,287
was like a single action,

337
00:14:43,287 --> 00:14:48,287
but you can completely
automate the actions

338
00:14:48,450 --> 00:14:52,383
that we're taking, so no human
intervention is required.

339
00:14:53,400 --> 00:14:56,220
And like I said, if we are
identifying a potential issue,

340
00:14:56,220 --> 00:14:57,780
we will go ahead and take care of it.

341
00:14:57,780 --> 00:15:00,210
So you don't have to have those incidents

342
00:15:00,210 --> 00:15:02,070
that happen that require like an RCA

343
00:15:02,070 --> 00:15:05,103
and trying to problem-solve
after the fact.

344
00:15:06,150 --> 00:15:08,130
Now at the end of the day, like I said,

345
00:15:08,130 --> 00:15:10,080
part of what we do is around efficiency

346
00:15:10,080 --> 00:15:11,820
and the cost savings and cost allocation

347
00:15:11,820 --> 00:15:13,140
is a big part of that.

348
00:15:13,140 --> 00:15:15,600
And we sell the businesses,

349
00:15:15,600 --> 00:15:18,150
and so we will sum up individual actions,

350
00:15:18,150 --> 00:15:21,360
we will sum up the overall ROI

351
00:15:21,360 --> 00:15:22,890
of all of the automation that's happening

352
00:15:22,890 --> 00:15:25,380
and present it back to our customers,

353
00:15:25,380 --> 00:15:26,853
and that's available as well.

354
00:15:29,098 --> 00:15:32,070
Want to go through a couple
case studies that apply directly

355
00:15:32,070 --> 00:15:34,200
to some of this work that we've done.

356
00:15:34,200 --> 00:15:36,780
And one of our internal customers

357
00:15:36,780 --> 00:15:40,950
is the big AI models team,
or BAM, short for BAM,

358
00:15:40,950 --> 00:15:45,300
which supports the LLM behind Watsonx.

359
00:15:45,300 --> 00:15:46,710
And really what they were looking at

360
00:15:46,710 --> 00:15:51,510
is they needed to
improve their environment

361
00:15:51,510 --> 00:15:54,150
so that there was less
manual tuning required

362
00:15:54,150 --> 00:15:57,120
to look after their environment.

363
00:15:57,120 --> 00:16:00,000
And they have hundreds of containers

364
00:16:00,000 --> 00:16:04,800
or running Kubernetes around
a hundred A100 NVIDIA GPUs.

365
00:16:04,800 --> 00:16:07,350
And really what they wanted to do as well

366
00:16:07,350 --> 00:16:10,380
was also, like, how
can we potentially make

367
00:16:10,380 --> 00:16:11,610
these GPUs more dense

368
00:16:11,610 --> 00:16:13,760
and get more ROI out
of what we were doing?

369
00:16:15,120 --> 00:16:16,680
And so after using Turbonomic,

370
00:16:16,680 --> 00:16:18,168
they saw some great results,

371
00:16:18,168 --> 00:16:22,200
5.3 times in terms of idle GPU resources.

372
00:16:22,200 --> 00:16:25,350
This took the headroom from 3 to 16.

373
00:16:25,350 --> 00:16:27,860
So these 16 GPUs were...

374
00:16:29,010 --> 00:16:31,710
These resources were
available for other workloads.

375
00:16:31,710 --> 00:16:34,770
They saw two times throughput improvements

376
00:16:34,770 --> 00:16:36,420
without impacting latency.

377
00:16:36,420 --> 00:16:39,720
So that was a big benefit for them.

378
00:16:39,720 --> 00:16:44,220
And again, they had 13
fewer GPUs that were needed,

379
00:16:44,220 --> 00:16:46,140
and they were able to allocate these GPUs

380
00:16:46,140 --> 00:16:47,370
to other workloads.

381
00:16:47,370 --> 00:16:49,260
So huge savings for them

382
00:16:49,260 --> 00:16:52,050
just by making the other
GPUs they had denser

383
00:16:52,050 --> 00:16:55,170
and being able to allocate those new GPUs

384
00:16:55,170 --> 00:16:57,330
as well as some of the
additional resources

385
00:16:57,330 --> 00:16:58,320
from the GPUs they still had

386
00:16:58,320 --> 00:17:01,050
allocated to other new AI workloads.

387
00:17:01,050 --> 00:17:03,360
You can scan the QR code if
you want to hear more about it,

388
00:17:03,360 --> 00:17:05,190
or read more about it.

389
00:17:05,190 --> 00:17:08,703
But again, great example of
us working together on that.

390
00:17:10,020 --> 00:17:12,870
Alright, so three things to remember today

391
00:17:12,870 --> 00:17:15,060
I'd love to leave you with.

392
00:17:15,060 --> 00:17:18,030
Agentic AI, again, because of its nature,

393
00:17:18,030 --> 00:17:19,740
it's very unpredictable, right?

394
00:17:19,740 --> 00:17:21,120
It requires a lot of resources,

395
00:17:21,120 --> 00:17:22,950
it doesn't scale in a linear fashion.

396
00:17:22,950 --> 00:17:27,950
So remember when you're
thinking about projects

397
00:17:28,020 --> 00:17:31,260
along your maturity curve
of adopting agentic AI,

398
00:17:31,260 --> 00:17:32,310
keep this in mind,

399
00:17:32,310 --> 00:17:33,870
that you're gonna have
to be sensitive to this

400
00:17:33,870 --> 00:17:36,270
because their bursts are
either unpredictable.

401
00:17:36,270 --> 00:17:38,315
And visibility alone is not enough, right?

402
00:17:38,315 --> 00:17:42,270
Just having access to
the resources behind it

403
00:17:42,270 --> 00:17:43,620
and seeing some of the issues

404
00:17:43,620 --> 00:17:46,590
can still create a lot
of manual work for you

405
00:17:46,590 --> 00:17:47,790
to have to support it.

406
00:17:47,790 --> 00:17:52,620
So being able to have a solution
that takes those insights

407
00:17:52,620 --> 00:17:56,070
and creates continuous action
to rightsize your environment

408
00:17:56,070 --> 00:17:57,270
and make sure those applications

409
00:17:57,270 --> 00:18:00,900
have the resource that they
need is incredibly important.

410
00:18:00,900 --> 00:18:03,750
You can really start with
one high-value workload.

411
00:18:03,750 --> 00:18:08,750
So look at for a potential application

412
00:18:09,240 --> 00:18:11,800
where you can still meet performance

413
00:18:12,780 --> 00:18:14,733
but have GPUs be more efficient.

414
00:18:15,690 --> 00:18:18,330
Look to target that and then scale it out

415
00:18:18,330 --> 00:18:19,830
and see some of those results.

416
00:18:20,700 --> 00:18:21,960
As far as working with us,

417
00:18:21,960 --> 00:18:22,860
we'd love to work with you

418
00:18:22,860 --> 00:18:27,000
in terms of let's look
at solution together,

419
00:18:27,000 --> 00:18:29,040
identifying a pilot workload

420
00:18:29,040 --> 00:18:33,480
and whether that's agentic
AI or a GPU service

421
00:18:33,480 --> 00:18:35,130
where performance or cost matters most,

422
00:18:35,130 --> 00:18:36,720
we can balance both of them.

423
00:18:36,720 --> 00:18:38,880
We also have a broad set of integrations

424
00:18:38,880 --> 00:18:40,860
across the hybrid cloud.

425
00:18:40,860 --> 00:18:44,490
So we have a ton of AWS
services that we optimize.

426
00:18:44,490 --> 00:18:47,973
We also do private cloud with
VMware and Nutanix and others,

427
00:18:49,590 --> 00:18:53,640
OpenShift and hybrid
environments and other CSPs

428
00:18:53,640 --> 00:18:55,290
that we support.

429
00:18:55,290 --> 00:19:00,240
Also an opportunity to combine
observability and automation.

430
00:19:00,240 --> 00:19:03,420
So our sister product, Instana,
provides even better metrics

431
00:19:03,420 --> 00:19:05,130
to power our automation

432
00:19:05,130 --> 00:19:09,570
and make our rightsizing
actions even better.

433
00:19:09,570 --> 00:19:14,160
And I'll just leave you with
thank you for your time today.

434
00:19:14,160 --> 00:19:16,710
I really appreciate all the feedback

435
00:19:16,710 --> 00:19:18,540
and responding to the poll.

436
00:19:18,540 --> 00:19:20,010
Our booth is over there
if you want to come

437
00:19:20,010 --> 00:19:21,630
and get a demo and see us in action.

438
00:19:21,630 --> 00:19:23,453
So please stop by.

439
00:19:23,453 --> 00:19:25,890
And again, thank you for your time,

440
00:19:25,890 --> 00:19:27,953
and have a great rest of the show.

441
00:19:27,953 --> 00:19:29,881
(audience applauding)

