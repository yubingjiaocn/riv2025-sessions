1
00:00:00,720 --> 00:00:02,320
- So today's session is gonna be

2
00:00:03,600 --> 00:00:06,720
what's new in Apache
Iceberg V3 and beyond.

3
00:00:06,720 --> 00:00:10,980
My name's Ron Ortloff, I'm
a product manager at AWS.

4
00:00:10,980 --> 00:00:14,340
I'm joined here today by Yuri Zarubin,

5
00:00:14,340 --> 00:00:18,723
who is a principal software
engineer also at AWS.

6
00:00:20,070 --> 00:00:22,443
So just a quick side note,

7
00:00:23,490 --> 00:00:25,770
you're in an open source track session.

8
00:00:25,770 --> 00:00:28,143
So this is what's new in Apache Iceberg.

9
00:00:29,550 --> 00:00:33,573
More focused on open source
and what's new in the spec.

10
00:00:35,130 --> 00:00:38,940
There are a bunch of other
Iceberg-focused sessions,

11
00:00:38,940 --> 00:00:43,940
specifically on AWS services
leveraging V3 features,

12
00:00:44,190 --> 00:00:45,480
things like deletion vectors.

13
00:00:45,480 --> 00:00:50,070
There's a great session on
how you can use AWS services

14
00:00:50,070 --> 00:00:53,160
with deletion vectors and a bunch more.

15
00:00:53,160 --> 00:00:56,403
So just quick side note on that one.

16
00:00:57,330 --> 00:00:59,700
So what are we gonna talk about today?

17
00:00:59,700 --> 00:01:03,450
It's really Iceberg V3,
why should you care?

18
00:01:03,450 --> 00:01:05,880
There's gonna be the three top features

19
00:01:05,880 --> 00:01:08,640
that we see most of our
customers talking about:

20
00:01:08,640 --> 00:01:10,800
the variant data type,

21
00:01:10,800 --> 00:01:12,420
deletion vectors,

22
00:01:12,420 --> 00:01:13,830
and row lineage.

23
00:01:13,830 --> 00:01:16,980
Those are really kind of bubbling
up to the top of the stack

24
00:01:16,980 --> 00:01:19,590
in terms of what people
are most interested in,

25
00:01:19,590 --> 00:01:21,540
where they're seeing the most alignment

26
00:01:21,540 --> 00:01:22,683
with their use cases.

27
00:01:23,520 --> 00:01:27,180
We'll wrap up the V3 talk
with additional features.

28
00:01:27,180 --> 00:01:30,690
There's a couple of buckets
of great additional features

29
00:01:30,690 --> 00:01:32,550
that are in the V3 spec.

30
00:01:32,550 --> 00:01:34,740
We'll talk a bit about those.

31
00:01:34,740 --> 00:01:36,390
And then Yuri's gonna come on

32
00:01:36,390 --> 00:01:41,340
and talk a bit more forward
looking about Apache Iceberg V4,

33
00:01:41,340 --> 00:01:43,500
where the community's starting to go.

34
00:01:43,500 --> 00:01:45,840
We're starting to see some proposals,

35
00:01:45,840 --> 00:01:49,170
formulate some momentum
around those proposals

36
00:01:49,170 --> 00:01:51,240
as things are starting to shape up

37
00:01:51,240 --> 00:01:54,363
and look forward towards
Apache Iceberg V4.

38
00:01:55,800 --> 00:01:58,530
We'll have call to action, some resources,

39
00:01:58,530 --> 00:02:02,043
and then you'll be on your
merry way to your next session.

40
00:02:03,420 --> 00:02:06,003
Okay, so before we get into V3,

41
00:02:07,980 --> 00:02:10,410
I want to talk a little
bit about Apache Iceberg

42
00:02:10,410 --> 00:02:13,080
as a project and the project history.

43
00:02:13,080 --> 00:02:17,610
So everything started
back in 2017 at Netflix.

44
00:02:17,610 --> 00:02:20,970
Dan Weeks, Ryan Blue,
Jason Reid noticed a lot

45
00:02:20,970 --> 00:02:23,580
of common patterns around
the big data solutions

46
00:02:23,580 --> 00:02:25,320
that they had at Netflix.

47
00:02:25,320 --> 00:02:28,350
Started a project that
they called Iceberg.

48
00:02:28,350 --> 00:02:31,200
In 2018, they started a process of getting

49
00:02:31,200 --> 00:02:34,590
that moved into the Apache
Software Foundation.

50
00:02:34,590 --> 00:02:39,060
So 2018, Iceberg became
an incubator project,

51
00:02:39,060 --> 00:02:42,690
which is kind of the
first sort of toe step

52
00:02:42,690 --> 00:02:45,990
into the Apache Software Foundation.

53
00:02:45,990 --> 00:02:49,200
In 2020, it became a top level project.

54
00:02:49,200 --> 00:02:51,750
So this is where you get
the formal governance,

55
00:02:51,750 --> 00:02:52,710
and you're following

56
00:02:52,710 --> 00:02:55,923
the Apache Software Foundation processes.

57
00:02:57,750 --> 00:03:02,403
2021, that's when the version
two of the spec came out.

58
00:03:04,320 --> 00:03:07,110
At that time, the big kind
of release and feature

59
00:03:07,110 --> 00:03:12,110
that came in V2 was the
capability for merge-on-read.

60
00:03:12,330 --> 00:03:14,730
So doing row level deletes,

61
00:03:14,730 --> 00:03:17,130
which is kind of the
precursor to deletion vectors,

62
00:03:17,130 --> 00:03:20,193
which is what we'll
talk about today for V3.

63
00:03:21,030 --> 00:03:23,700
2025, so this year in May,

64
00:03:23,700 --> 00:03:26,673
the version three of
the spec was ratified.

65
00:03:27,720 --> 00:03:29,310
And then as I mentioned,

66
00:03:29,310 --> 00:03:31,440
we're starting to see some formulation

67
00:03:31,440 --> 00:03:33,990
around where things are going

68
00:03:33,990 --> 00:03:37,773
with the end development proposals for V4.

69
00:03:43,560 --> 00:03:47,040
So with that foundation established,

70
00:03:47,040 --> 00:03:50,190
when it comes to Apache
Software Foundation projects,

71
00:03:50,190 --> 00:03:53,370
there's a couple terms when
we're talking about versions

72
00:03:53,370 --> 00:03:55,170
and what's new and those sorts of things

73
00:03:55,170 --> 00:03:58,170
that it's important to
understand how things operate

74
00:03:58,170 --> 00:04:00,360
within Apache Software Foundation.

75
00:04:00,360 --> 00:04:04,920
So you have specifications,
those are basically documents.

76
00:04:04,920 --> 00:04:07,020
It's a contract that
anybody that's working

77
00:04:07,020 --> 00:04:08,760
with Iceberg needs to adhere to

78
00:04:08,760 --> 00:04:10,890
to be spec compliant.

79
00:04:10,890 --> 00:04:13,470
This is a little bit more
important, I believe,

80
00:04:13,470 --> 00:04:15,540
in the Iceberg space

81
00:04:15,540 --> 00:04:17,820
where interoperability is
really the golden ticket

82
00:04:17,820 --> 00:04:21,090
of why Iceberg has gained
so much popularity.

83
00:04:21,090 --> 00:04:23,310
You have multiple vendors implementing,

84
00:04:23,310 --> 00:04:25,770
you wanna make sure that
you're adhering to a spec

85
00:04:25,770 --> 00:04:28,500
so that you can have
that single copy of data

86
00:04:28,500 --> 00:04:30,870
and use a bunch of
different compute engines

87
00:04:30,870 --> 00:04:32,430
on top of that data.

88
00:04:32,430 --> 00:04:35,703
So specifications are just that contract.

89
00:04:36,840 --> 00:04:41,190
They have to be voted on by the PMC,

90
00:04:41,190 --> 00:04:42,900
so the project management committee,

91
00:04:42,900 --> 00:04:45,480
before they're officially ratified.

92
00:04:45,480 --> 00:04:48,180
But keep in mind, anybody that wants

93
00:04:48,180 --> 00:04:50,010
to participate in an Apache project

94
00:04:50,010 --> 00:04:51,750
is more than welcome to vote.

95
00:04:51,750 --> 00:04:53,100
Have your voice heard.

96
00:04:53,100 --> 00:04:56,580
Something you really want
to champion, go vote for it.

97
00:04:56,580 --> 00:05:00,450
Something you wanna provide
feedback on, go vote for it.

98
00:05:00,450 --> 00:05:02,223
Everybody's welcome to participate.

99
00:05:03,090 --> 00:05:05,673
Only the PMC member votes count though.

100
00:05:06,720 --> 00:05:09,510
On the release side, this
is where the specs start

101
00:05:09,510 --> 00:05:11,220
to come to life.

102
00:05:11,220 --> 00:05:15,750
So you have spec features that
are implemented in releases,

103
00:05:15,750 --> 00:05:18,900
either through reference
implementations or SDKs,

104
00:05:18,900 --> 00:05:20,190
but the releases go through

105
00:05:20,190 --> 00:05:23,040
a formal Apache Software Foundation

106
00:05:23,040 --> 00:05:24,900
software release process.

107
00:05:24,900 --> 00:05:29,823
Those, again, are voted on by
the PMC just like specs are.

108
00:05:31,260 --> 00:05:33,270
So why is this important?

109
00:05:33,270 --> 00:05:37,110
So a lot of people heard
a lot of marketing buzz,

110
00:05:37,110 --> 00:05:41,940
a lot of talk about the
Iceberg V3 spec being released

111
00:05:41,940 --> 00:05:43,387
back in May, and they were like,

112
00:05:43,387 --> 00:05:45,510
"Well hey, where are my features?

113
00:05:45,510 --> 00:05:47,250
Why can't I go use feature X, Y,

114
00:05:47,250 --> 00:05:49,320
and Z that I heard someone talk about

115
00:05:49,320 --> 00:05:51,420
in the Iceberg V3 spec?"

116
00:05:51,420 --> 00:05:54,030
So since the 1.7 release
all the way through up

117
00:05:54,030 --> 00:05:55,440
to the 1.10 release,

118
00:05:55,440 --> 00:05:57,570
this is where we're
seeing the community start

119
00:05:57,570 --> 00:05:59,520
to implement those features

120
00:05:59,520 --> 00:06:01,290
that were ratified in the spec.

121
00:06:01,290 --> 00:06:03,600
So that's gonna continue to go forward,

122
00:06:03,600 --> 00:06:06,210
but just keep note that there is a lag

123
00:06:06,210 --> 00:06:10,680
between when things are ratified
in a spec in that contract

124
00:06:10,680 --> 00:06:12,240
and when they actually make it

125
00:06:12,240 --> 00:06:14,520
to market in a reference implementation

126
00:06:14,520 --> 00:06:16,650
that you can get your hands on and use.

127
00:06:16,650 --> 00:06:20,310
So that's kind of the
whole point of the slide,

128
00:06:20,310 --> 00:06:24,090
giving some kind of foundation
on where things are at.

129
00:06:24,090 --> 00:06:28,680
The V3 spec is not 100%
implemented in any release yet.

130
00:06:28,680 --> 00:06:32,130
So that's really the kind of dry residue

131
00:06:32,130 --> 00:06:33,840
on this whole slide.

132
00:06:33,840 --> 00:06:38,580
Okay, so with that all out of
the way, let's jump into V3,

133
00:06:38,580 --> 00:06:40,767
and let's start talking
about some of the features.

134
00:06:40,767 --> 00:06:43,863
The first one we'll talk about
is the variant data type.

135
00:06:45,990 --> 00:06:47,490
By the way, real quick show of hands,

136
00:06:47,490 --> 00:06:50,190
how many people are using Iceberg today?

137
00:06:50,190 --> 00:06:52,830
I meant to do this before, so...

138
00:06:52,830 --> 00:06:54,060
Okay.

139
00:06:54,060 --> 00:06:57,423
Anybody had a V3 table created yet?

140
00:07:00,810 --> 00:07:03,011
We have one brave soul.

141
00:07:03,011 --> 00:07:04,650
(audience laughing)

142
00:07:04,650 --> 00:07:06,468
We gotta get this guy a prize.

143
00:07:06,468 --> 00:07:08,506
(laughs) That's awesome, well done.

144
00:07:08,506 --> 00:07:09,540
(audience applauding)

145
00:07:09,540 --> 00:07:11,730
Okay, so come on up and join us.

146
00:07:11,730 --> 00:07:15,150
You can give the rest
of the talk. (laughs)

147
00:07:15,150 --> 00:07:17,430
Okay, so before we jump
into variant data type,

148
00:07:17,430 --> 00:07:18,960
let's just do a quick background.

149
00:07:18,960 --> 00:07:22,560
There's really a few different kinds

150
00:07:22,560 --> 00:07:24,210
of data types in Iceberg.

151
00:07:24,210 --> 00:07:25,710
There's primitive types.

152
00:07:25,710 --> 00:07:29,190
These are atomic data types,
can't be broken down further.

153
00:07:29,190 --> 00:07:30,750
Think of like int, string.

154
00:07:30,750 --> 00:07:34,410
Those are your basic
atomic primitive types.

155
00:07:34,410 --> 00:07:35,980
There's also structured types

156
00:07:37,290 --> 00:07:40,710
where you can actually
stitch together multiple

157
00:07:40,710 --> 00:07:42,270
of the primitive types.

158
00:07:42,270 --> 00:07:45,600
Think list, map, array,
those types of things.

159
00:07:45,600 --> 00:07:48,660
Key here is you have
a fixed schema though.

160
00:07:48,660 --> 00:07:51,900
And then this is where
variant comes in now

161
00:07:51,900 --> 00:07:55,200
with the ability to do
semi-structured data support.

162
00:07:55,200 --> 00:07:56,580
So here you're getting flexibility

163
00:07:56,580 --> 00:08:00,300
to handle a varying schema.

164
00:08:00,300 --> 00:08:03,570
There is no fixed component
to the semi-structured nature

165
00:08:03,570 --> 00:08:05,010
of variant.

166
00:08:05,010 --> 00:08:08,760
We see a lot of people
leveraging variant data type

167
00:08:08,760 --> 00:08:12,093
to handle and process JSON
data that they receive.

168
00:08:15,000 --> 00:08:19,140
So in terms of components
in the variant spec,

169
00:08:19,140 --> 00:08:22,170
for V3, there are three
different components.

170
00:08:22,170 --> 00:08:24,180
There's a metadata component.

171
00:08:24,180 --> 00:08:27,510
This is used to support
things like file pruning.

172
00:08:27,510 --> 00:08:30,780
There's an encoding where
you're taking values out

173
00:08:30,780 --> 00:08:34,893
of the variant data set and
putting a data type on those.

174
00:08:35,940 --> 00:08:38,490
And then the last component is shredding.

175
00:08:38,490 --> 00:08:41,490
So shredding is basically
taking elements out

176
00:08:41,490 --> 00:08:45,600
of the variant data type,
the variant data set,

177
00:08:45,600 --> 00:08:48,150
and materializing those as hidden columns.

178
00:08:48,150 --> 00:08:51,843
We'll talk a bit more about
shredding here in a moment.

179
00:08:54,720 --> 00:08:59,370
So before we get into use
cases of the variant data type,

180
00:08:59,370 --> 00:09:01,880
let's talk a little bit about
some of the pain that some

181
00:09:01,880 --> 00:09:04,890
of you folks that are using Iceberg V2

182
00:09:04,890 --> 00:09:06,540
may be living through
today when you're trying

183
00:09:06,540 --> 00:09:08,403
to handle semi-structured data.

184
00:09:09,420 --> 00:09:12,900
So first thing we saw a lot of
customers doing is basically

185
00:09:12,900 --> 00:09:15,690
trying to make their semi-structured data

186
00:09:15,690 --> 00:09:17,880
look like structured data.

187
00:09:17,880 --> 00:09:20,100
And they're doing that
through transformations,

188
00:09:20,100 --> 00:09:22,710
picking the closest
structured type that they can

189
00:09:22,710 --> 00:09:24,153
to align to their data.

190
00:09:25,350 --> 00:09:28,113
And that's just how they're living, right?

191
00:09:29,280 --> 00:09:33,510
With this, again, you have to
operate with a fixed schema.

192
00:09:33,510 --> 00:09:36,990
There is a little bit less
profile on the performance

193
00:09:36,990 --> 00:09:40,380
that we'll talk a bit
more about later as well.

194
00:09:40,380 --> 00:09:44,550
But we do see some people trying
to operate in this manner,

195
00:09:44,550 --> 00:09:48,390
and you're, again, also
paying for additional costs

196
00:09:48,390 --> 00:09:50,340
around the transformation of that data.

197
00:09:51,990 --> 00:09:55,230
Second pattern we see is
people just materializing out

198
00:09:55,230 --> 00:09:56,280
additional columns.

199
00:09:56,280 --> 00:09:59,640
So they may take in a data set,

200
00:09:59,640 --> 00:10:04,020
find a number of elements
that are key to them,

201
00:10:04,020 --> 00:10:05,610
and then they'll run transformations

202
00:10:05,610 --> 00:10:08,283
and put them out into
separate columns in the table.

203
00:10:09,210 --> 00:10:12,180
So again, you're paying
transformation costs, compute costs,

204
00:10:12,180 --> 00:10:15,330
but you are getting pretty
good performance in this model.

205
00:10:15,330 --> 00:10:18,300
These are legit columns in the
table that you've transformed

206
00:10:18,300 --> 00:10:20,940
and pulled out of a semi-structured set.

207
00:10:20,940 --> 00:10:22,050
They're gonna have stats on them,

208
00:10:22,050 --> 00:10:23,883
they're gonna perform quite well.

209
00:10:25,350 --> 00:10:28,620
And then the last one is string.

210
00:10:28,620 --> 00:10:30,810
I call this kind of
like the give up model.

211
00:10:30,810 --> 00:10:33,450
Like I've got a semi-structured data set,

212
00:10:33,450 --> 00:10:35,190
I can't fit it into a struct,

213
00:10:35,190 --> 00:10:37,020
I don't wanna materialize columns,

214
00:10:37,020 --> 00:10:39,720
ah, let's just throw it
into a string field, right?

215
00:10:39,720 --> 00:10:42,783
And we'll do string parsing, parse JSON,

216
00:10:43,800 --> 00:10:46,110
we'll just grab data out of that thing

217
00:10:46,110 --> 00:10:47,733
whenever we can, right?

218
00:10:49,410 --> 00:10:52,920
This one, it gives you
that flexible schema.

219
00:10:52,920 --> 00:10:55,770
But I do have performance
listed on here twice

220
00:10:55,770 --> 00:10:58,983
because the penalty for this
can be quite substantial.

221
00:10:59,940 --> 00:11:01,500
It really, really can.

222
00:11:01,500 --> 00:11:05,910
So that's what we see
customers doing today.

223
00:11:05,910 --> 00:11:07,110
In terms of use cases,

224
00:11:07,110 --> 00:11:10,950
what we really see customers
going after with variant,

225
00:11:10,950 --> 00:11:13,740
first one I'll talk
about is IoT workloads.

226
00:11:13,740 --> 00:11:17,280
So here, you've got eventing-type models,

227
00:11:17,280 --> 00:11:19,083
you've got different event details

228
00:11:19,083 --> 00:11:21,420
that you may be getting from a number

229
00:11:21,420 --> 00:11:23,880
of different IoT devices.

230
00:11:23,880 --> 00:11:27,420
Schema is really best applied on read,

231
00:11:27,420 --> 00:11:29,340
not trying to figure out a way to get

232
00:11:29,340 --> 00:11:32,493
that data into a fixed schema upfront.

233
00:11:34,140 --> 00:11:37,590
You also have a tendency to add in fields

234
00:11:37,590 --> 00:11:40,320
into your semi-structured
data set more often

235
00:11:40,320 --> 00:11:42,240
with IoT workloads as, again,

236
00:11:42,240 --> 00:11:44,730
you're getting these
varieties of different types.

237
00:11:44,730 --> 00:11:47,310
So this is a case where
variant being semi-structured

238
00:11:47,310 --> 00:11:51,990
in nature can just absorb
that semi-structured type data

239
00:11:51,990 --> 00:11:54,753
and give you an ability to report on it.

240
00:11:56,220 --> 00:11:59,103
The next one is data pipelines.

241
00:12:00,330 --> 00:12:02,910
So here, this is more
about landing the data,

242
00:12:02,910 --> 00:12:05,100
land it into a variant data type,

243
00:12:05,100 --> 00:12:07,740
and instead of doing transformation logic

244
00:12:07,740 --> 00:12:10,410
to get it to fit into a schema,

245
00:12:10,410 --> 00:12:13,770
you're basically building
logic into your data pipeline

246
00:12:13,770 --> 00:12:15,753
to apply a schema.

247
00:12:16,710 --> 00:12:18,750
So we had one customer,

248
00:12:18,750 --> 00:12:22,920
they're taking in data from a
number of different suppliers.

249
00:12:22,920 --> 00:12:25,560
They couldn't scale up the operation

250
00:12:25,560 --> 00:12:29,430
to actually enforce schema
on their data providers.

251
00:12:29,430 --> 00:12:32,730
So they basically had a data set.

252
00:12:32,730 --> 00:12:34,560
If it fit into the schema, great.

253
00:12:34,560 --> 00:12:38,400
It went in to the main
pipelines that they had.

254
00:12:38,400 --> 00:12:40,530
If they didn't have a supplier

255
00:12:40,530 --> 00:12:44,400
that gave them spec-compliant
data or schema-compliant data,

256
00:12:44,400 --> 00:12:45,540
they created something they called

257
00:12:45,540 --> 00:12:47,550
the imparsable fields column.

258
00:12:47,550 --> 00:12:48,660
That was variant.

259
00:12:48,660 --> 00:12:51,630
They would just dump the
data into that variant field

260
00:12:51,630 --> 00:12:54,750
and use pipelines to
extract pipeline logic

261
00:12:54,750 --> 00:12:57,810
to extract the elements
out of that variant field.

262
00:12:57,810 --> 00:13:01,140
That allowed them then
to keep things consistent

263
00:13:01,140 --> 00:13:02,430
as they moved the data

264
00:13:02,430 --> 00:13:04,800
through the different phases in reporting

265
00:13:04,800 --> 00:13:07,863
and analytics for their solution.

266
00:13:10,140 --> 00:13:13,440
The last one is real time analytics.

267
00:13:13,440 --> 00:13:17,280
So again, you want to be able
to query the data as it lands.

268
00:13:17,280 --> 00:13:20,640
You wanna avoid pre-processing of data.

269
00:13:20,640 --> 00:13:23,730
This is in a model where I need access,

270
00:13:23,730 --> 00:13:26,910
and I need analytics on top
of the data immediately.

271
00:13:26,910 --> 00:13:31,533
We had a fintech customer
doing lots of quote analysis:

272
00:13:32,700 --> 00:13:36,420
stocks, bonds, crypto, whatever.

273
00:13:36,420 --> 00:13:37,680
Those quotes all come in

274
00:13:37,680 --> 00:13:39,360
in different shapes and sizes.

275
00:13:39,360 --> 00:13:41,640
They don't have time for
this solution to be able

276
00:13:41,640 --> 00:13:44,640
to do any pre-processing
or transforming up front.

277
00:13:44,640 --> 00:13:46,980
They want to throw it into a variant field

278
00:13:46,980 --> 00:13:48,780
and just let the quants

279
00:13:48,780 --> 00:13:52,863
and the smarter people than
me query on that data quickly.

280
00:13:55,500 --> 00:13:57,540
So that's variant use cases.

281
00:13:57,540 --> 00:14:01,113
Just to distill down some
of the variant benefits.

282
00:14:02,190 --> 00:14:05,880
Performance and cost is
really a big one with variant.

283
00:14:05,880 --> 00:14:09,753
You're getting data put into
columnar in your storage,

284
00:14:11,010 --> 00:14:13,200
which gives you stats,

285
00:14:13,200 --> 00:14:15,600
gives you the ability to
do predicate push down.

286
00:14:16,710 --> 00:14:18,990
We talked a lot about flexible schema

287
00:14:18,990 --> 00:14:22,620
in the different use cases
and build up to this slide.

288
00:14:22,620 --> 00:14:27,620
You're operating now on a
schema that's dynamic for you.

289
00:14:27,960 --> 00:14:31,230
You get in new eventing
scenarios and details.

290
00:14:31,230 --> 00:14:34,260
You almost get an automatic
schema evolution, right?

291
00:14:34,260 --> 00:14:37,770
You can still take things in
that have new elements in them,

292
00:14:37,770 --> 00:14:39,660
and you can still operate on top of them

293
00:14:39,660 --> 00:14:42,933
without having to go through
a formal schema evolution.

294
00:14:45,030 --> 00:14:46,260
Efficient storage.

295
00:14:46,260 --> 00:14:49,080
So you are getting compression
when you do the shredding

296
00:14:49,080 --> 00:14:51,213
and build out the hidden columns.

297
00:14:52,080 --> 00:14:53,760
You're gonna get the
compression done for you

298
00:14:53,760 --> 00:14:55,620
out of the box.

299
00:14:55,620 --> 00:14:58,983
And then last benefit is
around querying the data.

300
00:14:59,940 --> 00:15:01,650
So you get schema navigation

301
00:15:01,650 --> 00:15:03,750
through dot notation on some engines,

302
00:15:03,750 --> 00:15:06,780
some other engines have a
slightly different syntax,

303
00:15:06,780 --> 00:15:08,970
but it's very easy to go through

304
00:15:08,970 --> 00:15:12,600
and navigate down a hierarchy
on a variant data type

305
00:15:12,600 --> 00:15:15,690
compared to wrapping multiple
string parsing functions

306
00:15:15,690 --> 00:15:16,790
or anything like that.

307
00:15:19,230 --> 00:15:22,593
So I've talked a couple
times about shredding.

308
00:15:25,260 --> 00:15:28,800
Spoiler alert, this is my
favorite slide of the entire deck,

309
00:15:28,800 --> 00:15:32,580
so be prepared to be wowed right here.

310
00:15:32,580 --> 00:15:35,310
Okay, so shredding.

311
00:15:35,310 --> 00:15:36,210
What is shredding?

312
00:15:37,260 --> 00:15:41,310
So I have a table here that's
been defined in storage.

313
00:15:41,310 --> 00:15:43,980
It's got event date, timestamp,

314
00:15:43,980 --> 00:15:46,800
it's got source id as an integer.

315
00:15:46,800 --> 00:15:50,970
And then I've created my event
details as a variant field.

316
00:15:50,970 --> 00:15:52,860
So three columns, pretty simple.

317
00:15:52,860 --> 00:15:57,860
I get in this type of data
file from my source system,

318
00:15:58,380 --> 00:16:00,420
and I wanna load that now into the table.

319
00:16:00,420 --> 00:16:04,410
So what happens in this case now with V3?

320
00:16:04,410 --> 00:16:07,530
So the primitive types, those easily map,

321
00:16:07,530 --> 00:16:10,320
get loaded through my
engine, put into the table.

322
00:16:10,320 --> 00:16:13,230
Now with this variant field,

323
00:16:13,230 --> 00:16:15,720
I'm going to pass that through logic

324
00:16:15,720 --> 00:16:18,180
that you would have in
your Iceberg V3 engine

325
00:16:18,180 --> 00:16:20,460
to actually shred that out.

326
00:16:20,460 --> 00:16:23,973
So here's my handy dandy variant shredder.

327
00:16:24,840 --> 00:16:27,270
That data flows through the shredder.

328
00:16:27,270 --> 00:16:31,173
This is where it gets broken
out now into the sub columns.

329
00:16:32,070 --> 00:16:34,650
So that's the engine implementation

330
00:16:34,650 --> 00:16:38,190
of a variant shredding process.

331
00:16:38,190 --> 00:16:42,000
And then those elements then
are put into sub columns,

332
00:16:42,000 --> 00:16:44,133
hidden virtual sub columns in the table.

333
00:16:44,970 --> 00:16:49,410
So you see SKU with integer
account, ID with integer,

334
00:16:49,410 --> 00:16:51,960
and then you have the price in as decimal.

335
00:16:51,960 --> 00:16:55,350
So I've got data types, I've
got these hidden columns.

336
00:16:55,350 --> 00:17:00,350
I still reference that column
as event details, okay?

337
00:17:00,780 --> 00:17:02,070
But now underneath the covers

338
00:17:02,070 --> 00:17:03,750
and the implementation of variant,

339
00:17:03,750 --> 00:17:07,593
I have those hidden sub columns
that I can then query into.

340
00:17:08,746 --> 00:17:10,943
And that query would
look something like this.

341
00:17:12,000 --> 00:17:16,350
Okay, so here with this predicate column,

342
00:17:16,350 --> 00:17:19,440
I'm actually gonna be able
to do pruning on that data

343
00:17:19,440 --> 00:17:23,490
and return just the values
that match that predicate.

344
00:17:23,490 --> 00:17:26,820
Any other type of model
outside of variant,

345
00:17:26,820 --> 00:17:28,710
there's a higher chance
I'm just gonna be doing

346
00:17:28,710 --> 00:17:30,360
a full table scan,

347
00:17:30,360 --> 00:17:32,070
especially if I'm doing
like string parsing

348
00:17:32,070 --> 00:17:34,080
or something like that,

349
00:17:34,080 --> 00:17:37,413
it will certainly be
doing a full table scan.

350
00:17:38,580 --> 00:17:41,190
That's kind of rough
when you think about it.

351
00:17:41,190 --> 00:17:43,920
But then also if you
think about doing joins,

352
00:17:43,920 --> 00:17:48,060
if I wanted to do a join on
account ID to another table

353
00:17:48,060 --> 00:17:49,623
with no statistics,

354
00:17:50,872 --> 00:17:52,650
I think you guys know how
that story ends, right?

355
00:17:52,650 --> 00:17:53,910
It can be pretty ugly.

356
00:17:53,910 --> 00:17:58,230
Mismatched join conditions,
scanning multiple tables,

357
00:17:58,230 --> 00:17:59,930
performance can be very, very bad.

358
00:18:03,060 --> 00:18:04,470
Speaking of performance,

359
00:18:04,470 --> 00:18:07,413
so just to give you kind of a rough sense,

360
00:18:08,580 --> 00:18:12,210
kind of relative query
performance difference

361
00:18:12,210 --> 00:18:13,353
that you may see,

362
00:18:14,940 --> 00:18:17,760
with variant being the 1x baseline,

363
00:18:17,760 --> 00:18:21,990
you could see up to four
times additional degradation

364
00:18:21,990 --> 00:18:24,150
in performance with structured types.

365
00:18:24,150 --> 00:18:26,010
Strings could be 10x worse,

366
00:18:26,010 --> 00:18:29,040
and honestly that 10x number,

367
00:18:29,040 --> 00:18:31,830
I could craft a benchmark
for you that did table scans

368
00:18:31,830 --> 00:18:34,350
to very tight predicate lookups,

369
00:18:34,350 --> 00:18:37,500
and it could be much,
much higher than 10x.

370
00:18:37,500 --> 00:18:39,513
So just some ideas.

371
00:18:40,680 --> 00:18:42,180
You start looking at variant

372
00:18:42,180 --> 00:18:45,450
for potential semi-structured
type data solutions.

373
00:18:45,450 --> 00:18:47,160
This is the type of performance profile

374
00:18:47,160 --> 00:18:48,510
that you can start seeing over some

375
00:18:48,510 --> 00:18:50,973
of those alternatives that we discussed.

376
00:18:52,980 --> 00:18:57,630
Okay, so you're all experts
on variant now hopefully.

377
00:18:57,630 --> 00:19:00,330
Let's move on to the next one

378
00:19:00,330 --> 00:19:02,830
and talk a little bit more
about deletion vectors.

379
00:19:03,870 --> 00:19:06,250
Okay, so deletion vectors

380
00:19:07,440 --> 00:19:10,110
are a write optimization feature.

381
00:19:10,110 --> 00:19:12,630
They're a storage optimization feature

382
00:19:12,630 --> 00:19:16,200
that's been enhanced over
what's been in the spec

383
00:19:16,200 --> 00:19:18,513
since the V2 version of it.

384
00:19:19,560 --> 00:19:24,560
So how does the write modes
inside of Iceberg work?

385
00:19:24,720 --> 00:19:27,060
You have the default
option of copy on write.

386
00:19:27,060 --> 00:19:30,570
That's been there since
day one of Iceberg.

387
00:19:30,570 --> 00:19:33,750
In that model, you have a data file,

388
00:19:33,750 --> 00:19:35,640
you run a delete query against that,

389
00:19:35,640 --> 00:19:38,520
and then you rewrite a new data file

390
00:19:38,520 --> 00:19:42,060
that's got whatever was
deleted out of it removed.

391
00:19:42,060 --> 00:19:44,400
So I use this analogy.

392
00:19:44,400 --> 00:19:46,683
If I've got a table with 10 records in it,

393
00:19:47,970 --> 00:19:51,210
and I want to delete one
of those records out,

394
00:19:51,210 --> 00:19:55,080
I'm gonna rewrite a new data
file with nine records in it.

395
00:19:55,080 --> 00:19:57,750
That's copy on write, okay?

396
00:19:57,750 --> 00:20:00,810
Now that may not seem
like that big of a deal,

397
00:20:00,810 --> 00:20:03,600
but if you do that across
thousands of data files,

398
00:20:03,600 --> 00:20:06,737
your write amplification
will be enormous, right?

399
00:20:06,737 --> 00:20:09,723
You'll be rewriting an awful lot of data.

400
00:20:10,770 --> 00:20:13,500
Conversely with merge on read,

401
00:20:13,500 --> 00:20:16,860
that same model now,
data file, delete query,

402
00:20:16,860 --> 00:20:20,490
I'm just writing a delete
file with one record in it.

403
00:20:20,490 --> 00:20:22,440
One positional delete, right?

404
00:20:22,440 --> 00:20:23,580
Or one delete condition,

405
00:20:23,580 --> 00:20:27,690
depending on the type of row
level delete I've implemented.

406
00:20:27,690 --> 00:20:31,293
So the write amplification
problem is solved.

407
00:20:32,250 --> 00:20:37,250
You're gonna have faster
writes with merge on read.

408
00:20:37,260 --> 00:20:39,900
You're gonna be consuming less storage,

409
00:20:39,900 --> 00:20:42,270
but you're gonna pay a tax on the read.

410
00:20:42,270 --> 00:20:45,000
Okay, now I've gotta
join those delete files

411
00:20:45,000 --> 00:20:46,680
to my full data files

412
00:20:46,680 --> 00:20:49,290
to figure out what's
the filtered result set

413
00:20:49,290 --> 00:20:51,540
to return, okay?

414
00:20:51,540 --> 00:20:55,290
Copy on write, I've got a new
file with nine records in it.

415
00:20:55,290 --> 00:20:57,510
I just need to scan that entire set

416
00:20:57,510 --> 00:21:00,513
and not worry about putting it
together with anything else.

417
00:21:01,770 --> 00:21:04,503
So that's copy on write, merge on read.

418
00:21:07,680 --> 00:21:10,410
So when we talk about the delete types

419
00:21:10,410 --> 00:21:15,030
that are there in Iceberg,
first we had in V2,

420
00:21:15,030 --> 00:21:18,840
we introduced the equality
deletes capability.

421
00:21:18,840 --> 00:21:23,280
So this is writing a
condition to a delete file

422
00:21:23,280 --> 00:21:27,300
as you see here, like
Remove all user_id='ABC'

423
00:21:27,300 --> 00:21:29,220
And then that delete condition is used

424
00:21:29,220 --> 00:21:32,280
on the reads to filter out records.

425
00:21:32,280 --> 00:21:36,420
This is pretty popular for
streaming workloads like Flink.

426
00:21:36,420 --> 00:21:40,743
We see a lot of a equality
delete solutions using Flink.

427
00:21:42,360 --> 00:21:43,440
And then the other piece

428
00:21:43,440 --> 00:21:47,190
that was introduced in V2
was positional deletes.

429
00:21:47,190 --> 00:21:49,590
So this, again, you're
writing a delete position

430
00:21:50,850 --> 00:21:53,190
to a delete file,

431
00:21:53,190 --> 00:21:56,850
and then you're using
that delete file on read

432
00:21:56,850 --> 00:22:01,170
brought into a bitmap to then
filter out the result set.

433
00:22:01,170 --> 00:22:04,230
So equality deletes were there in V2,

434
00:22:04,230 --> 00:22:09,230
equality deletes are still
there in V3 spec, okay?

435
00:22:09,510 --> 00:22:11,970
Positional deletes were there in V2,

436
00:22:11,970 --> 00:22:13,380
those are not in V3 three.

437
00:22:13,380 --> 00:22:14,850
They've been deprecated

438
00:22:14,850 --> 00:22:17,730
and replaced with the
deletion vector feature

439
00:22:17,730 --> 00:22:19,653
now in the V3 spec.

440
00:22:21,240 --> 00:22:24,780
So instead of writing
positional delete files,

441
00:22:24,780 --> 00:22:28,470
deletion vectors is updating
the values on a bitmap

442
00:22:28,470 --> 00:22:31,320
and then persisting that bitmap to disc

443
00:22:31,320 --> 00:22:33,123
in the form of a Puffin file.

444
00:22:33,960 --> 00:22:36,000
So there's a slight nuance to that,

445
00:22:36,000 --> 00:22:37,743
it's still positional deletes,

446
00:22:38,790 --> 00:22:41,520
but it's in a much more optimized format

447
00:22:41,520 --> 00:22:42,993
with deletion vectors.

448
00:22:44,400 --> 00:22:47,460
So let's talk a little bit about use cases

449
00:22:47,460 --> 00:22:51,390
and why you wanna implement
something like deletion vectors.

450
00:22:51,390 --> 00:22:53,310
We talked about the write
amplification piece,

451
00:22:53,310 --> 00:22:55,980
but there are other higher level scenarios

452
00:22:55,980 --> 00:22:58,833
where deletion vectors can come into play.

453
00:23:00,090 --> 00:23:01,620
So GDPR compliance,

454
00:23:01,620 --> 00:23:03,390
we see a lot of people running

455
00:23:03,390 --> 00:23:07,440
into write amplification
issues doing GDPR compliance.

456
00:23:07,440 --> 00:23:10,440
Oftentimes when you're
removing information

457
00:23:10,440 --> 00:23:12,570
about individuals can be different than

458
00:23:12,570 --> 00:23:16,500
how the data's fully consumed
for other business purposes.

459
00:23:16,500 --> 00:23:17,370
And that's where you end up

460
00:23:17,370 --> 00:23:19,860
in these random write scenarios

461
00:23:19,860 --> 00:23:23,220
where your write amplification
is very, very high.

462
00:23:23,220 --> 00:23:25,443
So if you have to do GDPR deletes,

463
00:23:28,500 --> 00:23:30,240
deletion vectors is a feature

464
00:23:30,240 --> 00:23:32,040
that you should certainly look into.

465
00:23:33,270 --> 00:23:35,940
Another one is data cleanup.

466
00:23:35,940 --> 00:23:39,930
So we see people using a
medallion style architecture:

467
00:23:39,930 --> 00:23:41,730
bronze, silver, gold,

468
00:23:41,730 --> 00:23:45,900
where your bronze is more
of like a raw staging area.

469
00:23:45,900 --> 00:23:47,460
You're gonna land a lot of data,

470
00:23:47,460 --> 00:23:49,590
there's gonna be maybe a lot
of noise around that data.

471
00:23:49,590 --> 00:23:51,180
You're gonna wanna do
a lot of data cleanup

472
00:23:51,180 --> 00:23:53,463
on that bronze layer.

473
00:23:54,360 --> 00:23:57,810
So that's where, again, deletion
vectors can come into play

474
00:23:57,810 --> 00:24:00,930
and aid in the the write speeds

475
00:24:00,930 --> 00:24:03,230
for those data cleanup
operations that you do.

476
00:24:04,200 --> 00:24:07,560
And then the last one is
incremental data pipelines.

477
00:24:07,560 --> 00:24:09,970
So if you're running merge statements

478
00:24:10,830 --> 00:24:13,140
in your data pipelines,

479
00:24:13,140 --> 00:24:16,290
you have a potential to be doing, again,

480
00:24:16,290 --> 00:24:18,900
more of those random
write type of operations,

481
00:24:18,900 --> 00:24:23,010
and here's where deletion
vectors can play very nicely

482
00:24:23,010 --> 00:24:24,873
with merge operations.

483
00:24:27,150 --> 00:24:32,150
Okay, so just a slide here then
to kind of understand a bit

484
00:24:34,410 --> 00:24:38,520
how the moving parts operate
within deletion vectors.

485
00:24:38,520 --> 00:24:39,840
So in this example here,

486
00:24:39,840 --> 00:24:42,690
I've got a table with a couple snapshots

487
00:24:42,690 --> 00:24:44,250
that are created on disk.

488
00:24:44,250 --> 00:24:46,950
We've got snapshot one, snapshot two.

489
00:24:46,950 --> 00:24:49,590
If I want to do another delete query,

490
00:24:49,590 --> 00:24:52,320
so now I've got a data file
that I'm gonna produce,

491
00:24:52,320 --> 00:24:54,570
and then I'm gonna have a bitmap

492
00:24:54,570 --> 00:24:56,640
that I'm gonna go ahead and update.

493
00:24:56,640 --> 00:24:59,460
So as part of that S3 transaction then,

494
00:24:59,460 --> 00:25:02,310
we'll make sure that the data
file's committed on disc,

495
00:25:02,310 --> 00:25:05,703
we'll make sure that the bitmap
is also committed on disc,

496
00:25:06,660 --> 00:25:11,660
and then that transaction is
committed into my table, okay?

497
00:25:11,700 --> 00:25:15,270
So when it comes time to
then consume that data,

498
00:25:15,270 --> 00:25:17,490
we'll grab the data files,

499
00:25:17,490 --> 00:25:18,930
we'll grab the bitmap,

500
00:25:18,930 --> 00:25:22,410
and then we'll use that bitmap
to filter out the result set

501
00:25:22,410 --> 00:25:25,263
as we're returning the data
to the calling application.

502
00:25:31,046 --> 00:25:35,046
Okay, so to kind of wrap
up on deletion vectors,

503
00:25:37,440 --> 00:25:40,020
comparing them to V2 positional deletes

504
00:25:40,020 --> 00:25:42,420
and really help hopefully drive home

505
00:25:42,420 --> 00:25:44,760
why this is such a great improvement.

506
00:25:44,760 --> 00:25:46,680
So with positional delete files,

507
00:25:46,680 --> 00:25:49,650
there was a propensity
to have a proliferation

508
00:25:49,650 --> 00:25:50,970
of delete files.

509
00:25:50,970 --> 00:25:52,530
There was no spec,

510
00:25:52,530 --> 00:25:57,530
there was no guidance given
around producing delete files.

511
00:25:57,570 --> 00:25:59,340
So we ended up, and we see with a lot

512
00:25:59,340 --> 00:26:02,160
of customers using position deletes on V2,

513
00:26:02,160 --> 00:26:04,530
they have just tons of these delete files.

514
00:26:04,530 --> 00:26:06,330
Small transactions, small deletes,

515
00:26:06,330 --> 00:26:09,210
small updates, small merge
statements, whatever.

516
00:26:09,210 --> 00:26:11,700
There's tons and tons
of these delete files.

517
00:26:11,700 --> 00:26:14,370
Puts an additional compaction burden.

518
00:26:14,370 --> 00:26:16,260
You have to clean that data up,

519
00:26:16,260 --> 00:26:18,750
fix up your underlying data files.

520
00:26:18,750 --> 00:26:20,310
But that's one of the challenges

521
00:26:20,310 --> 00:26:23,730
that we're certainly seeing with V2

522
00:26:23,730 --> 00:26:26,340
and the positional deletes.

523
00:26:26,340 --> 00:26:30,450
The other is the positional
delete files are translated

524
00:26:30,450 --> 00:26:33,780
from a bitmap into a Parquet file,

525
00:26:33,780 --> 00:26:36,390
and then on read, that is reversed, right?

526
00:26:36,390 --> 00:26:38,790
You're pulling data off of a Parquet file,

527
00:26:38,790 --> 00:26:41,190
and then you're building
a bitmap on the fly

528
00:26:41,190 --> 00:26:42,663
to filter out results set.

529
00:26:44,370 --> 00:26:49,140
With V3 then, a lot of
these challenges are solved.

530
00:26:49,140 --> 00:26:51,150
Per the spec, you're only allowed

531
00:26:51,150 --> 00:26:53,250
to produce the writer implementation.

532
00:26:53,250 --> 00:26:57,420
You're only allowed to produce
one delete file per snapshot.

533
00:26:57,420 --> 00:26:59,040
So it knocks off that problem

534
00:26:59,040 --> 00:27:02,430
of all these small, tiny delete files.

535
00:27:02,430 --> 00:27:06,810
The bitmap itself is also stored
in a Puffin file directly,

536
00:27:06,810 --> 00:27:09,480
so there's no deconstruction
reconstruction process

537
00:27:09,480 --> 00:27:13,803
that happens around that
bitmap and the Puffin file.

538
00:27:14,640 --> 00:27:17,880
And the writers themselves
maintain that bitmap

539
00:27:17,880 --> 00:27:20,040
while the write operations are happening.

540
00:27:20,040 --> 00:27:22,890
So you have a fully efficient bitmap

541
00:27:22,890 --> 00:27:24,590
that's being built and maintained.

542
00:27:28,770 --> 00:27:30,753
Okay, that's deletion vectors.

543
00:27:32,490 --> 00:27:35,240
Let's go ahead and talk a
little bit about row lineage.

544
00:27:36,780 --> 00:27:41,780
So row lineage is really a
great change tracking feature

545
00:27:42,060 --> 00:27:45,660
that's been brought to the V3 spec.

546
00:27:45,660 --> 00:27:48,720
So the components of row
lineage within the spec,

547
00:27:48,720 --> 00:27:50,790
there's a writer specification for you

548
00:27:50,790 --> 00:27:55,320
to be a compliant Iceberg V3 writer.

549
00:27:55,320 --> 00:28:00,033
You have to be producing record
level change information.

550
00:28:01,260 --> 00:28:02,640
You can use, as a writer,

551
00:28:02,640 --> 00:28:04,890
you can use Iceberg V3 metadata

552
00:28:04,890 --> 00:28:07,500
to understand sequence numbers

553
00:28:07,500 --> 00:28:11,160
and row IDs that have been
committed to snapshots.

554
00:28:11,160 --> 00:28:15,600
But there is a responsibility
on the writer's side for you

555
00:28:15,600 --> 00:28:19,230
to produce these change records
that allow for row lineage

556
00:28:19,230 --> 00:28:20,703
to be consumed.

557
00:28:21,870 --> 00:28:23,370
On the reader side,

558
00:28:23,370 --> 00:28:26,460
there are new hidden columns added

559
00:28:26,460 --> 00:28:29,040
to V3 tables that give you

560
00:28:29,040 --> 00:28:31,500
that row lineage
information automatically.

561
00:28:31,500 --> 00:28:34,710
So you get a row ID, and
you get a sequence number,

562
00:28:34,710 --> 00:28:37,863
and that's on every single V3 record.

563
00:28:40,890 --> 00:28:42,890
And then from an operational standpoint,

564
00:28:43,920 --> 00:28:46,860
row lineage is a required feature.

565
00:28:46,860 --> 00:28:50,820
So it's on by default, there's
no knob to turn it off,

566
00:28:50,820 --> 00:28:52,920
and that information is just there,

567
00:28:52,920 --> 00:28:56,553
and it's with you to consume.

568
00:28:57,600 --> 00:29:00,240
The other thing I like about row lineage,

569
00:29:00,240 --> 00:29:01,260
for those that have ever dealt

570
00:29:01,260 --> 00:29:03,030
with like a split brain type scenario

571
00:29:03,030 --> 00:29:05,340
where you've had state information

572
00:29:05,340 --> 00:29:09,600
get out of sync with data,
it's really kind of neat,

573
00:29:09,600 --> 00:29:12,720
a neat little side
benefit with row lineage,

574
00:29:12,720 --> 00:29:15,930
the row lineage information
is stored right on the record.

575
00:29:15,930 --> 00:29:19,590
So you have that row
lineage state information

576
00:29:19,590 --> 00:29:22,680
in the snapshot in the
record in the table itself.

577
00:29:22,680 --> 00:29:24,120
If you want to go do time travel,

578
00:29:24,120 --> 00:29:27,000
you're gonna get the
time travel equivalent

579
00:29:27,000 --> 00:29:30,063
of what that row lineage looked
like at the time you query.

580
00:29:31,800 --> 00:29:36,423
So in terms of row lineage use cases,

581
00:29:38,100 --> 00:29:39,990
the incremental processing, again,

582
00:29:39,990 --> 00:29:43,620
this is really a nice
sweet spot for row lineage.

583
00:29:43,620 --> 00:29:46,140
If you wanna be reading out of V3 tables

584
00:29:46,140 --> 00:29:49,110
as an input into data pipelines,

585
00:29:49,110 --> 00:29:52,650
you can leverage the sequence
numbers, the row IDs,

586
00:29:52,650 --> 00:29:55,110
the information there that's on the rows

587
00:29:55,110 --> 00:29:57,720
to understand what's changed as a source

588
00:29:57,720 --> 00:29:59,943
to feed in to your pipelines.

589
00:30:01,200 --> 00:30:04,740
Event lifecycle tracking
is also another good one.

590
00:30:04,740 --> 00:30:06,840
You think of like an orders table, right?

591
00:30:06,840 --> 00:30:08,790
An orders table goes through,

592
00:30:08,790 --> 00:30:10,170
or an order, I should say,

593
00:30:10,170 --> 00:30:12,780
event goes through a
series of changes, right?

594
00:30:12,780 --> 00:30:16,380
It's been submitted, it's been
billed, it's been processed,

595
00:30:16,380 --> 00:30:21,270
fulfilled, shipped out for
shipment, delivered, right?

596
00:30:21,270 --> 00:30:23,820
You wanna understand what the lifecycle

597
00:30:23,820 --> 00:30:25,890
or the state changes were.

598
00:30:25,890 --> 00:30:28,200
Here, row lineage, each
one of those is gonna be on

599
00:30:28,200 --> 00:30:30,210
that same row ID for that order.

600
00:30:30,210 --> 00:30:33,090
You're gonna see the sequence
number increment each time,

601
00:30:33,090 --> 00:30:34,680
and you'll be able to quickly

602
00:30:34,680 --> 00:30:37,650
and easily stitch together what

603
00:30:37,650 --> 00:30:39,900
that life cycle look like, right?

604
00:30:39,900 --> 00:30:43,770
Slowly changing dimensions,
type two dimensions,

605
00:30:43,770 --> 00:30:45,540
similar sort of model with those.

606
00:30:45,540 --> 00:30:47,730
You could feed in row lineage information

607
00:30:47,730 --> 00:30:50,193
to help build those types
of things out as well.

608
00:30:51,630 --> 00:30:53,820
Debugging is another one.

609
00:30:53,820 --> 00:30:56,550
Maybe not as flashy as some
of the other stuff on here,

610
00:30:56,550 --> 00:31:00,300
but if you keep track of
that row lineage information

611
00:31:00,300 --> 00:31:02,610
within the transformations that you do

612
00:31:02,610 --> 00:31:06,420
and the data that you enrich
and build in your system,

613
00:31:06,420 --> 00:31:07,800
you can then trace back

614
00:31:07,800 --> 00:31:11,550
to how those calculations
came to be, right?

615
00:31:11,550 --> 00:31:14,100
You can use the row id, you
can use the sequence number

616
00:31:14,100 --> 00:31:16,200
for which you did the calculation,

617
00:31:16,200 --> 00:31:19,170
and then you can be able to
trace back over those events

618
00:31:19,170 --> 00:31:23,550
and understand how you got one
plus one equals seven, right?

619
00:31:23,550 --> 00:31:24,550
Something like that.

620
00:31:25,920 --> 00:31:29,040
And then lastly with row lineage,

621
00:31:29,040 --> 00:31:31,503
it's a great compliance capability,

622
00:31:32,370 --> 00:31:33,543
enabler I should say.

623
00:31:34,650 --> 00:31:37,710
So you're gonna have a
sequence number stamped

624
00:31:37,710 --> 00:31:40,530
on each modification to that table.

625
00:31:40,530 --> 00:31:43,170
So you'll be able to
understand if someone went in

626
00:31:43,170 --> 00:31:44,970
and updated my salary

627
00:31:44,970 --> 00:31:47,160
and an employee's table
to $1 billion, right?

628
00:31:47,160 --> 00:31:50,310
You'd be able to see that
sort of change of events

629
00:31:50,310 --> 00:31:52,473
and trace back on those steps.

630
00:31:55,560 --> 00:32:00,093
Okay, so couple clicks here
on how row lineage works.

631
00:32:02,880 --> 00:32:07,290
So I have a table here,
it's got two columns in it:

632
00:32:07,290 --> 00:32:08,700
first name, last name,

633
00:32:08,700 --> 00:32:12,390
and then I've added in this
gray box, on the side of that,

634
00:32:12,390 --> 00:32:16,320
that's the row lineage columns
that come automatically.

635
00:32:16,320 --> 00:32:19,050
If I do a SELECT * star from the table,

636
00:32:19,050 --> 00:32:21,360
I see f_name, l_name.

637
00:32:21,360 --> 00:32:24,720
I don't see the row lineage
columns automatically.

638
00:32:24,720 --> 00:32:28,230
But if I actually key those
in and select four columns,

639
00:32:28,230 --> 00:32:30,000
you'll see all the information

640
00:32:30,000 --> 00:32:32,070
like you see on the screen here.

641
00:32:32,070 --> 00:32:36,270
So initial load, I put
two rows in the table,

642
00:32:36,270 --> 00:32:38,550
and then I have the row ID
and sequence number equal

643
00:32:38,550 --> 00:32:39,573
to one there.

644
00:32:40,440 --> 00:32:45,210
On the next transaction at time
T2, I update Diego to Pablo,

645
00:32:45,210 --> 00:32:49,203
and then we see the sequence_number
increment up to two.

646
00:32:52,098 --> 00:32:54,030
And then for the third transaction here,

647
00:32:54,030 --> 00:32:56,400
we'll ratchet up the
complexity a tiny bit.

648
00:32:56,400 --> 00:32:59,310
You'll get a merge statement
that does a new row,

649
00:32:59,310 --> 00:33:01,173
and then it updates Carlos to Chuck.

650
00:33:02,100 --> 00:33:05,910
So here you see the new record added,

651
00:33:05,910 --> 00:33:08,640
new row ID is generated.

652
00:33:08,640 --> 00:33:10,020
And then the sequence numbers,

653
00:33:10,020 --> 00:33:12,370
both of those come in as
sequence number three.

654
00:33:13,350 --> 00:33:15,337
Some people when we talk
to them about this, like,

655
00:33:15,337 --> 00:33:17,880
"Well hey, why is the new insert,

656
00:33:17,880 --> 00:33:21,030
why shouldn't that be
sequence number one," right?

657
00:33:21,030 --> 00:33:22,800
It's gonna be an increasing,

658
00:33:22,800 --> 00:33:24,780
monotonically increasing number,

659
00:33:24,780 --> 00:33:28,230
but it's gonna start at where
the current sequence number is

660
00:33:28,230 --> 00:33:29,940
within that table,

661
00:33:29,940 --> 00:33:31,620
which is important now when you want

662
00:33:31,620 --> 00:33:34,743
to actually start consuming those changes.

663
00:33:36,450 --> 00:33:37,983
So here on this select query,

664
00:33:39,270 --> 00:33:41,070
if this is like when I was talking about

665
00:33:41,070 --> 00:33:42,450
doing those incremental pipelines,

666
00:33:42,450 --> 00:33:44,190
if this was a source query for you

667
00:33:44,190 --> 00:33:46,260
to feed into a data pipeline,

668
00:33:46,260 --> 00:33:49,830
you can keep track of that
max last sequence number

669
00:33:49,830 --> 00:33:52,620
on each of the batches that
you run through, right?

670
00:33:52,620 --> 00:33:54,750
So now when I come through, if I say hey,

671
00:33:54,750 --> 00:33:57,270
where the sequence number is
greater than the last time

672
00:33:57,270 --> 00:33:58,560
that I pulled,

673
00:33:58,560 --> 00:34:00,210
I'm gonna get those last two records

674
00:34:00,210 --> 00:34:02,310
that we saw come out
of that merge statement

675
00:34:02,310 --> 00:34:04,473
with this example here.

676
00:34:09,210 --> 00:34:11,463
So to close up on row lineage,

677
00:34:12,900 --> 00:34:16,833
in the V2 spec, there was,
in the Spark implementation,

678
00:34:20,100 --> 00:34:23,643
a procedure and a view
implementation for doing changelog.

679
00:34:24,570 --> 00:34:27,450
So changelog is basically a mechanism

680
00:34:27,450 --> 00:34:29,880
that does snapshot diffing.

681
00:34:29,880 --> 00:34:34,880
So you're taking snapshot
at T1 and comparing that,

682
00:34:35,040 --> 00:34:37,530
joining that together with snapshot T2

683
00:34:37,530 --> 00:34:40,500
to understand what the differences are

684
00:34:40,500 --> 00:34:42,870
between those two snapshots.

685
00:34:42,870 --> 00:34:44,777
So if you could compare and contrast that

686
00:34:44,777 --> 00:34:46,020
to the row lineage,

687
00:34:46,020 --> 00:34:48,210
because row lineage is
basically doing the same type

688
00:34:48,210 --> 00:34:50,613
of change tracking for you,

689
00:34:51,900 --> 00:34:54,043
with changelogs in V2,

690
00:34:54,960 --> 00:34:57,600
you're having to pay the compute costs

691
00:34:57,600 --> 00:35:00,447
to do that snapshot diffing, right?

692
00:35:00,447 --> 00:35:02,790
And with row lineage in V3,

693
00:35:02,790 --> 00:35:06,150
those details are just stamped
right on the record for you.

694
00:35:06,150 --> 00:35:09,090
There's no additional
compute, you just query those,

695
00:35:09,090 --> 00:35:11,190
the writers are tracking it automatically.

696
00:35:12,780 --> 00:35:15,570
Changelogs, you have view maintenance.

697
00:35:15,570 --> 00:35:16,920
If you do schema evolution,

698
00:35:16,920 --> 00:35:19,121
you have to make sure the changelogs

699
00:35:19,121 --> 00:35:21,990
and the views are maintained correctly.

700
00:35:21,990 --> 00:35:24,810
Row lineage, again,
that's done automatically.

701
00:35:24,810 --> 00:35:27,060
Those metadata columns, the writers,

702
00:35:27,060 --> 00:35:28,680
everything is done for you,

703
00:35:28,680 --> 00:35:31,593
and that information is
there with zero config.

704
00:35:33,480 --> 00:35:34,533
And then lastly,

705
00:35:35,850 --> 00:35:39,960
if you think about
correlating changes over time,

706
00:35:39,960 --> 00:35:43,980
over time travel queries,
over different snapshots,

707
00:35:43,980 --> 00:35:47,070
if you want to be able to
do that with changelogs,

708
00:35:47,070 --> 00:35:48,540
it can get a bit tricky

709
00:35:48,540 --> 00:35:51,270
where you're having to
do an iterative approach

710
00:35:51,270 --> 00:35:54,390
to comparing multiple snapshots over time.

711
00:35:54,390 --> 00:35:56,910
Whereas with row lineage, now again,

712
00:35:56,910 --> 00:35:59,370
because that information
is stamped on the records,

713
00:35:59,370 --> 00:36:02,040
it's very easy for you to
stitch out the full lifecycle

714
00:36:02,040 --> 00:36:04,950
of changes on straight up queries

715
00:36:04,950 --> 00:36:06,800
as opposed to doing a bunch of joins.

716
00:36:08,430 --> 00:36:10,240
Okay, so

717
00:36:13,290 --> 00:36:14,640
to wrap things up on V3,

718
00:36:14,640 --> 00:36:17,430
we'll cover some of the other stuff

719
00:36:17,430 --> 00:36:19,130
that's been delivered in the spec.

720
00:36:25,380 --> 00:36:27,930
So in terms of additional
features, there's really,

721
00:36:27,930 --> 00:36:30,753
like I mentioned at the
start, there's two buckets.

722
00:36:32,040 --> 00:36:34,650
There's a core
infrastructure set of changes

723
00:36:34,650 --> 00:36:37,890
or features that came in on the V3 spec,

724
00:36:37,890 --> 00:36:40,710
and then there's also some
additional data types.

725
00:36:40,710 --> 00:36:41,580
So we'll take a minute here

726
00:36:41,580 --> 00:36:45,450
to talk about the core
infrastructure pieces.

727
00:36:45,450 --> 00:36:49,170
The first feature that came in the V3 spec

728
00:36:49,170 --> 00:36:51,600
in the core infrastructure
area that talk about here

729
00:36:51,600 --> 00:36:53,613
is default values.

730
00:36:54,660 --> 00:36:55,900
So default values

731
00:36:57,990 --> 00:36:59,640
inserts data

732
00:36:59,640 --> 00:37:02,310
when there's no value specified.

733
00:37:02,310 --> 00:37:03,540
So you're gonna create a table,

734
00:37:03,540 --> 00:37:07,500
you're gonna specify for a
column what the default value is,

735
00:37:07,500 --> 00:37:09,150
and then when you're
running your pipelines

736
00:37:09,150 --> 00:37:12,870
or inserting your data,
if no value is specified,

737
00:37:12,870 --> 00:37:14,520
you're using the default value

738
00:37:14,520 --> 00:37:16,120
that's configured on the schema.

739
00:37:17,640 --> 00:37:20,910
What's interesting about
the Iceberg implementation,

740
00:37:20,910 --> 00:37:23,820
the default value itself
is stored in metadata.

741
00:37:23,820 --> 00:37:27,210
So you're not persisting a constant value

742
00:37:27,210 --> 00:37:28,710
in the file itself.

743
00:37:28,710 --> 00:37:30,990
You're actually using the metadata on read

744
00:37:30,990 --> 00:37:35,310
to replace the value
that wasn't specified.

745
00:37:35,310 --> 00:37:38,700
So that's kind of a nifty little nuance,

746
00:37:38,700 --> 00:37:40,710
being able to leverage Iceberg metadata

747
00:37:40,710 --> 00:37:44,793
to get that default value
populated on the results set.

748
00:37:46,890 --> 00:37:49,650
And then you're gonna get some conformity,

749
00:37:49,650 --> 00:37:52,080
some ease of use around the fact

750
00:37:52,080 --> 00:37:55,023
that the value is persisted
in the table metadata.

751
00:37:55,920 --> 00:37:57,690
I've worked on systems

752
00:37:57,690 --> 00:38:00,480
where you've had multiple
developers getting creative

753
00:38:00,480 --> 00:38:02,460
with how they specify a default value.

754
00:38:02,460 --> 00:38:03,900
Some people use negative one,

755
00:38:03,900 --> 00:38:06,300
some people use some other value,

756
00:38:06,300 --> 00:38:08,670
and it can become a mess, right?

757
00:38:08,670 --> 00:38:12,660
Here, that default value is
stored right with the schema,

758
00:38:12,660 --> 00:38:15,240
and you don't have to worry
about any sort of pipeline logic

759
00:38:15,240 --> 00:38:18,300
or any sort of rules that
you want to implement

760
00:38:18,300 --> 00:38:20,223
for replacing in default values.

761
00:38:22,860 --> 00:38:26,400
Okay, next in the core infrastructure area

762
00:38:26,400 --> 00:38:28,773
is table encryption keys.

763
00:38:30,150 --> 00:38:32,880
So table encryption keys
gives you the ability

764
00:38:32,880 --> 00:38:37,290
to specify an encryption key at the table

765
00:38:37,290 --> 00:38:39,360
and at the metadata level.

766
00:38:39,360 --> 00:38:42,510
It's gonna give you a
granular set of control

767
00:38:42,510 --> 00:38:45,540
across your tables that
you need to encrypt.

768
00:38:45,540 --> 00:38:50,043
You can also integrate that
encryption key with KMS.

769
00:38:51,780 --> 00:38:54,540
It does support key rotation as well.

770
00:38:54,540 --> 00:38:57,060
So you can rotate keys.

771
00:38:57,060 --> 00:39:00,273
And then lastly on table encryption,

772
00:39:01,620 --> 00:39:05,490
this is more of an advanced
feature for you to really align

773
00:39:05,490 --> 00:39:09,240
with some elevated security
compliance requirements

774
00:39:09,240 --> 00:39:10,290
on your data.

775
00:39:10,290 --> 00:39:12,780
So it's gonna give you
more granular control,

776
00:39:12,780 --> 00:39:16,650
allow you to integrate with KMS as well,

777
00:39:16,650 --> 00:39:19,980
but also to get that
higher level of encryption

778
00:39:19,980 --> 00:39:21,573
on your Iceberg data.

779
00:39:24,570 --> 00:39:27,600
The last one I'll talk about
in the core infrastructure area

780
00:39:27,600 --> 00:39:30,420
is the multi-argument transformations.

781
00:39:30,420 --> 00:39:32,280
So with multi-argument transformations,

782
00:39:32,280 --> 00:39:36,544
now you're able to specify
more than one field

783
00:39:36,544 --> 00:39:38,220
in a transformation.

784
00:39:38,220 --> 00:39:41,220
So transformations can
be used for partitioning,

785
00:39:41,220 --> 00:39:43,410
or they can be used for sorting,

786
00:39:43,410 --> 00:39:46,200
but this gives you a bit more control

787
00:39:46,200 --> 00:39:50,190
in scenarios where you have a harder time

788
00:39:50,190 --> 00:39:51,450
picking just one column,

789
00:39:51,450 --> 00:39:53,910
you need more than one
column to actually help

790
00:39:53,910 --> 00:39:57,363
with query scenarios,
help to avoid data skew.

791
00:39:58,530 --> 00:40:02,220
But the net for
multi-argument transformations

792
00:40:02,220 --> 00:40:04,380
is this is a performance feature.

793
00:40:04,380 --> 00:40:06,850
So you'll be able to align

794
00:40:07,860 --> 00:40:10,800
your end user query
patterns more efficiently

795
00:40:10,800 --> 00:40:14,370
with specifying multiple
columns in those transformations

796
00:40:14,370 --> 00:40:16,383
for partitioning as well as sorting.

797
00:40:20,700 --> 00:40:25,050
Okay, and then the last
section we'll talk about on V3

798
00:40:25,050 --> 00:40:26,793
is additional data types.

799
00:40:29,700 --> 00:40:33,990
So the first one in this section
is nanosecond timestamps.

800
00:40:33,990 --> 00:40:36,110
Nanosecond timestamps
have been talked about

801
00:40:36,110 --> 00:40:37,920
in the Iceberg space for quite a while.

802
00:40:37,920 --> 00:40:42,243
Well, probably a year before
the Iceberg V3 spec came to be.

803
00:40:43,530 --> 00:40:45,330
We had customers that I've worked with

804
00:40:45,330 --> 00:40:49,080
that have nanosecond timestamps
in their Parquet data

805
00:40:49,080 --> 00:40:50,430
or from some other system,

806
00:40:50,430 --> 00:40:52,830
and they want to be able
to keep that fidelity,

807
00:40:52,830 --> 00:40:55,500
that precision as they move into Iceberg.

808
00:40:55,500 --> 00:40:57,950
And it's been a pretty
big gap for quite a while.

809
00:40:58,920 --> 00:41:03,390
So what you're getting is the
nanosecond timestamp support

810
00:41:03,390 --> 00:41:07,293
for your high frequency, your
temporal type of workloads.

811
00:41:08,460 --> 00:41:11,670
You're getting the increase
from the microsecond

812
00:41:11,670 --> 00:41:15,810
to the nanosecond fidelity in Iceberg V3.

813
00:41:15,810 --> 00:41:17,487
You're getting this both with

814
00:41:17,487 --> 00:41:20,013
and without timestamp support as well.

815
00:41:22,890 --> 00:41:24,300
In terms of workloads,

816
00:41:24,300 --> 00:41:25,920
I think this one fits very nicely

817
00:41:25,920 --> 00:41:28,290
into the streaming workloads
where you want to be able

818
00:41:28,290 --> 00:41:29,880
to have more precise measurements

819
00:41:29,880 --> 00:41:33,633
on frequently arriving data.

820
00:41:37,230 --> 00:41:40,380
Okay, so the next one is the geo types.

821
00:41:40,380 --> 00:41:43,140
So this is actually two types, one slide:

822
00:41:43,140 --> 00:41:45,960
geography and geometry.

823
00:41:45,960 --> 00:41:49,110
You're getting now support for location

824
00:41:49,110 --> 00:41:52,830
and mapping queries with
the geography data type.

825
00:41:52,830 --> 00:41:56,130
And then you're getting the
ability to do measurements

826
00:41:56,130 --> 00:41:59,280
and shapes with the geometry data type.

827
00:41:59,280 --> 00:42:01,260
So both of these data types,

828
00:42:01,260 --> 00:42:06,260
separate implementation,
separate data types.

829
00:42:06,360 --> 00:42:10,080
They do follow the open OGC standard.

830
00:42:10,080 --> 00:42:13,230
So we have an open standard
following an open standard,

831
00:42:13,230 --> 00:42:14,880
which is great.

832
00:42:14,880 --> 00:42:18,180
And then I said a one plus
one equals three on this.

833
00:42:18,180 --> 00:42:21,630
Like I think having the geography,

834
00:42:21,630 --> 00:42:23,510
the geo data types inside of Iceberg

835
00:42:23,510 --> 00:42:27,090
in V3 is really a killer feature.

836
00:42:27,090 --> 00:42:30,330
You're getting geo enablement now on data

837
00:42:30,330 --> 00:42:32,010
plus all the goodness that comes

838
00:42:32,010 --> 00:42:34,560
with Iceberg around schema evolution,

839
00:42:34,560 --> 00:42:37,260
time travel, and interoperability, right?

840
00:42:37,260 --> 00:42:40,260
So now you have the power of Iceberg

841
00:42:40,260 --> 00:42:44,250
and the power of geospatial on the data,

842
00:42:44,250 --> 00:42:45,750
which I think is really great.

843
00:42:48,900 --> 00:42:51,450
Last one I'll talk about, unknown.

844
00:42:51,450 --> 00:42:55,893
Unknown is a little bit of
a cryptic implementation.

845
00:42:56,940 --> 00:42:59,940
It's a kind of a null placeholder value,

846
00:42:59,940 --> 00:43:01,980
so you're gonna get some protection now

847
00:43:01,980 --> 00:43:04,170
if you're doing schema evolution,

848
00:43:04,170 --> 00:43:08,610
and types or data doesn't exist in files,

849
00:43:08,610 --> 00:43:10,290
you can use the unknown data type

850
00:43:10,290 --> 00:43:14,880
as a known null
placeholder across engines.

851
00:43:14,880 --> 00:43:17,760
Sometimes we see customers
running into issues

852
00:43:17,760 --> 00:43:21,690
where the null handling isn't
always handled gracefully

853
00:43:21,690 --> 00:43:23,220
across their Iceberg engines.

854
00:43:23,220 --> 00:43:27,780
This is something that's
helping to combat that scenario.

855
00:43:27,780 --> 00:43:30,190
And we've actually seen some extreme cases

856
00:43:31,230 --> 00:43:34,080
where customers have
gone to rewriting data

857
00:43:34,080 --> 00:43:36,660
to make sure that they're not breaking

858
00:43:36,660 --> 00:43:38,403
their Iceberg implementations.

859
00:43:40,500 --> 00:43:44,583
Okay, so with that, I
will hand it off to Yuri.

860
00:43:47,010 --> 00:43:48,010
- Thank you.
- Yep.

861
00:43:51,060 --> 00:43:55,140
- Okay, so clearly there's
a lot of features in V3,

862
00:43:55,140 --> 00:43:58,380
and after Ron's excellent deep dive,

863
00:43:58,380 --> 00:44:01,620
I think we have no excuse
not to know about it, right?

864
00:44:01,620 --> 00:44:04,860
And so what I'm gonna do is
I'm just gonna cover some

865
00:44:04,860 --> 00:44:07,770
of the new things that the
community is cooking up in V4,

866
00:44:07,770 --> 00:44:10,120
and then we're gonna just
close out the session

867
00:44:11,040 --> 00:44:12,153
and do conclusions.

868
00:44:13,050 --> 00:44:16,020
One thing to note is that all the stuff

869
00:44:16,020 --> 00:44:19,170
that I'm about to talk about,
it hasn't been ratified.

870
00:44:19,170 --> 00:44:21,690
So these are just proposals.

871
00:44:21,690 --> 00:44:23,940
Community is still
thinking about what to do.

872
00:44:26,649 --> 00:44:28,080
And the way I would actually
characterize the changes

873
00:44:28,080 --> 00:44:30,270
so far is they're very
much performance based.

874
00:44:30,270 --> 00:44:32,370
So focused on raw performance,

875
00:44:32,370 --> 00:44:34,530
not jam packed full of features like V3.

876
00:44:34,530 --> 00:44:37,080
So I'm just gonna quickly
go through some of these.

877
00:44:39,060 --> 00:44:42,840
So let's start off with
improved column stats.

878
00:44:42,840 --> 00:44:44,730
So if you're not familiar
with column statistics,

879
00:44:44,730 --> 00:44:48,960
it's like this special
information about columns

880
00:44:48,960 --> 00:44:52,500
that's stored inside of
the Iceberg metadata files,

881
00:44:52,500 --> 00:44:56,430
and it helps query engines to
effectively scan data, right?

882
00:44:56,430 --> 00:44:58,200
And one of the problems
with them right now is

883
00:44:58,200 --> 00:45:00,690
that the way these stats are implemented

884
00:45:00,690 --> 00:45:02,460
is not super efficient
in certain use cases,

885
00:45:02,460 --> 00:45:04,290
especially if you have lots of columns.

886
00:45:04,290 --> 00:45:08,670
And that's because when query
engines go and read them,

887
00:45:08,670 --> 00:45:10,980
they have to deserialize these large maps,

888
00:45:10,980 --> 00:45:12,960
which then creates memory pressure.

889
00:45:12,960 --> 00:45:15,270
And so one of the things

890
00:45:15,270 --> 00:45:16,560
that the community is thinking about

891
00:45:16,560 --> 00:45:19,143
is just creating a proper
structure for these.

892
00:45:20,430 --> 00:45:23,790
And that's gonna help
engines efficiently look

893
00:45:23,790 --> 00:45:26,790
at particular stats that they care about.

894
00:45:26,790 --> 00:45:28,470
If you're just using Iceberg like me,

895
00:45:28,470 --> 00:45:30,030
all you really need to know is

896
00:45:30,030 --> 00:45:32,190
that you're gonna get better performance

897
00:45:32,190 --> 00:45:33,333
in certain use cases.

898
00:45:36,210 --> 00:45:39,570
Next is the adaptive metadata tree.

899
00:45:39,570 --> 00:45:42,990
So this is motivated by small
write and delete performance,

900
00:45:42,990 --> 00:45:45,030
something that Ron
mentioned earlier, right?

901
00:45:45,030 --> 00:45:48,060
So if you have lots of
small writes and/or deletes,

902
00:45:48,060 --> 00:45:50,400
it can become problematic

903
00:45:50,400 --> 00:45:53,370
because Iceberg kind of
has these layers, right?

904
00:45:53,370 --> 00:45:56,310
Starting from the catalog
layer to the root,

905
00:45:56,310 --> 00:45:58,320
then a JSON layer, and then
there's three more layers

906
00:45:58,320 --> 00:46:01,260
where you got the manifest list, metadata,

907
00:46:01,260 --> 00:46:02,820
and then the actual data file.

908
00:46:02,820 --> 00:46:05,220
So every time you want to insert a file,

909
00:46:05,220 --> 00:46:08,460
or you do some sort of an
update or write into your table,

910
00:46:08,460 --> 00:46:10,110
you kinda have to go
through all of these layers,

911
00:46:10,110 --> 00:46:12,240
and that can be inefficient.

912
00:46:12,240 --> 00:46:17,240
And so the proposal is to
combine the manifest list

913
00:46:17,700 --> 00:46:20,820
and the manifest together
into a single structure

914
00:46:20,820 --> 00:46:24,360
that's gonna have these like
root nodes and leaf nodes.

915
00:46:24,360 --> 00:46:26,940
It's a very long proposal,
which I'm not gonna go into,

916
00:46:26,940 --> 00:46:28,290
but basically at the end of the day,

917
00:46:28,290 --> 00:46:32,940
it's gonna skip one layer for
the small write use cases.

918
00:46:32,940 --> 00:46:35,253
And again, if you're just using Iceberg,

919
00:46:36,180 --> 00:46:38,010
this isn't really something
you're gonna think about.

920
00:46:38,010 --> 00:46:41,520
Just know that you small write
performance is gonna improve

921
00:46:41,520 --> 00:46:44,103
with V4 if this proposal goes through.

922
00:46:47,190 --> 00:46:50,493
And so lastly is relative paths.

923
00:46:51,360 --> 00:46:53,520
So this is actually super useful

924
00:46:53,520 --> 00:46:55,980
because one common
problem with manifests is

925
00:46:55,980 --> 00:47:00,980
that they contain absolute
paths to the Parquet files,

926
00:47:01,007 --> 00:47:03,000
to the data files, right?

927
00:47:03,000 --> 00:47:05,190
And the reason that's problematic

928
00:47:05,190 --> 00:47:07,260
is that if you want to copy your table,

929
00:47:07,260 --> 00:47:10,890
like if you have an Iceberg
table on general purpose S3,

930
00:47:10,890 --> 00:47:12,120
and you just wanna copy it,

931
00:47:12,120 --> 00:47:14,760
could be copying it to
a different S3 bucket,

932
00:47:14,760 --> 00:47:15,780
it could be a different account,

933
00:47:15,780 --> 00:47:20,280
could be a different
cloud storage provider,

934
00:47:20,280 --> 00:47:21,600
you kind of run into this issue

935
00:47:21,600 --> 00:47:23,580
because once you copy the actual files,

936
00:47:23,580 --> 00:47:24,660
you can't actually read them,

937
00:47:24,660 --> 00:47:25,950
or none of the query engines work

938
00:47:25,950 --> 00:47:29,250
because the metadata files are referencing

939
00:47:29,250 --> 00:47:32,040
the previous data files.

940
00:47:32,040 --> 00:47:34,440
And so the fix to that is, well,

941
00:47:34,440 --> 00:47:35,310
like it's in the name, right?

942
00:47:35,310 --> 00:47:37,203
So it's to make paths relative.

943
00:47:38,370 --> 00:47:41,400
And this is gonna be, I
think, a very useful feature,

944
00:47:41,400 --> 00:47:42,840
especially if you're using something

945
00:47:42,840 --> 00:47:44,520
like S3 replication, right,

946
00:47:44,520 --> 00:47:48,480
where you're just replicating
your Iceberg table

947
00:47:48,480 --> 00:47:52,980
to a different bucket for data
protection, backup, whatever.

948
00:47:52,980 --> 00:47:54,570
With the relative paths,

949
00:47:54,570 --> 00:47:56,280
you're gonna be able
to just query that data

950
00:47:56,280 --> 00:47:59,523
without having to muck
around with manifest lists.

951
00:48:02,130 --> 00:48:03,130
And that's about it.

952
00:48:05,010 --> 00:48:07,023
So just to wrap things up,

953
00:48:09,420 --> 00:48:12,510
Ron talked about the key V3 features,

954
00:48:12,510 --> 00:48:15,750
which are deletion vectors, row lineage,

955
00:48:15,750 --> 00:48:19,023
and variant data type.

956
00:48:20,040 --> 00:48:23,010
Then he talked about the core features.

957
00:48:23,010 --> 00:48:25,650
So the core functionalities
such as default values,

958
00:48:25,650 --> 00:48:28,650
multi-argument transforms,
and the new types.

959
00:48:28,650 --> 00:48:31,743
So the geo, nano, and the unknown.

960
00:48:33,180 --> 00:48:35,550
And then I just went
through the V4 proposals.

961
00:48:35,550 --> 00:48:38,130
So that's the improved
stats, adaptive metadata,

962
00:48:38,130 --> 00:48:39,723
and relative paths.

963
00:48:41,910 --> 00:48:44,280
We do invite you to go
and just try things out.

964
00:48:44,280 --> 00:48:46,260
There's only one gentleman
at the beginning of this

965
00:48:46,260 --> 00:48:48,600
that raised their hand when
asked if they tried out V3.

966
00:48:48,600 --> 00:48:51,870
So we hope to kind of move
that number up, right?

967
00:48:51,870 --> 00:48:54,120
So hopefully by at the end of this week,

968
00:48:54,120 --> 00:48:56,040
once you go back, you
go try some of this out.

969
00:48:56,040 --> 00:48:58,680
So whatever vendor you're using with,

970
00:48:58,680 --> 00:49:01,230
ask them what V3 support do they have,

971
00:49:01,230 --> 00:49:05,130
and then try converting
your table from V2 to V3.

972
00:49:05,130 --> 00:49:07,920
And if you're interested more in Iceberg,

973
00:49:07,920 --> 00:49:11,040
do join the community so
you can attend a meetup,

974
00:49:11,040 --> 00:49:14,520
you can join us on the
mailing list on Slack,

975
00:49:14,520 --> 00:49:17,020
or even make a contribution
if you're a developer.

976
00:49:20,850 --> 00:49:22,530
And with that said...

977
00:49:22,530 --> 00:49:24,900
Oops, I pressed the wrong button.

978
00:49:24,900 --> 00:49:26,220
- There we go.
- Thank you for coming

979
00:49:26,220 --> 00:49:28,530
and hope you enjoy the
rest of your re:Invent.

980
00:49:28,530 --> 00:49:29,525
Thank you.
- Thank you.

981
00:49:29,525 --> 00:49:32,692
(audience applauding)

