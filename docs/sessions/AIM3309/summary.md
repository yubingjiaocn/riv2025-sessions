# AWS re:Invent 2025 会议总结：Slack 的开发者体验 AI 之旅

## 会议概述

本次 AWS re:Invent 2025 分会场由 AWS 高级解决方案架构师 Prashant Gapati、Slack 高级软件工程师 Shivani Bi 以及 AWS ISV 战略客户负责人 Moni 共同主讲，深入分享了 Slack 开发者体验团队在过去三年中如何利用生成式 AI 和智能代理技术提升内部开发者生产力的完整历程。

Slack 作为全球领先的企业协作平台，其工程团队面临着快速创新、保持高可靠性和确保安全合规的多重挑战。为了应对这些挑战，Slack 的开发者体验（DevXP）AI 团队从 2023 年第二季度开始探索 AI 技术，经历了从 Amazon SageMaker 到 Amazon Bedrock 的技术演进，最终采用了包括 Claude Code、Cursor 和 Amazon Strands 在内的完整 AI 工具栈。这一转型不仅将基础设施成本降低了 98%，更实现了 99% 的开发者采用率和 25% 的 PR 吞吐量提升，每月处理超过 5000 个升级请求。

会议重点介绍了 Slack 如何通过 Amazon Bedrock 的统一平台、内置安全防护和大规模可扩展性，从简单的代码辅助工具逐步演进到复杂的智能代理系统。特别是通过采用 Amazon Strands 开源框架和模型上下文协议（MCP），Slack 构建了灵活、模型无关且面向未来的智能代理架构，为开发者提供了从文档搜索到事件响应的全方位 AI 辅助能力。

## 详细时间线与关键要点

### 开场介绍（0:00 - 3:30）
- **0:00** - Prashant Gapati 开场，介绍自己是 AWS 高级解决方案架构师，在 AWS 工作 5 年，拥有 20 年解决方案架构经验
- **1:15** - Shivani Bi 自我介绍，Slack 高级软件工程师，在 Slack 工作 7 年，DevXP AI 团队成员 3 年
- **2:00** - Moni 介绍自己负责 AWS 战略 ISV 客户的生成式 AI 项目，在 AWS 工作 5 年
- **2:45** - 现场互动：询问有多少人在来会场前查看了 Slack

### Slack 与 AWS 合作背景（3:30 - 6:00）
- **3:30** - 强调 Slack 是工作真正发生的地方，想法转化为决策和行动的平台
- **4:15** - 说明 Slack 必须保持快速、可靠和安全，同时实现大规模运营
- **5:00** - 引用 Andy Jassy 的话：AWS 和 Slack 共同赋能开发者团队更快协作和创新
- **5:30** - 介绍会议议程：Slack 的 AI 开发者体验之旅、代码辅助工具推出、实际影响、智能代理和 Strands、未来路线图

### Slack 开发者体验团队介绍（6:00 - 8:30）
- **6:00** - 介绍 Slack 开发者体验团队的使命：让 Slack 工程师的工作更轻松
- **6:45** - 团队从构建 Buddy Bot（文档辅助机器人）开始
- **7:15** - 团队规模约 70-80 人，支持整个 Slack 工程团队乃至 Salesforce 组织
- **7:45** - 强调团队的核心方法：快速内部启动，先在小团队试点，验证成功后再推广
- **8:15** - 团队负责构建和发布工具、测试基础设施和开发者工具

### AWS 技术栈介绍（8:30 - 11:00）
- **8:30** - 介绍 AWS AI/ML 技术栈的层次结构
- **9:00** - 基础层：Amazon SageMaker 和 AI 计算，用于构建、训练和部署自定义模型
- **9:30** - 中间层：Amazon Bedrock 全托管基础模型层，提供多种模型选择、内置防护栏、知识库和灵活的托管选项
- **10:00** - 介绍 Agent Core 处理运行时、身份、内存和可观测性
- **10:30** - 顶层：SDK 代理（Strands）和应用层（Hero、QuickSight）
- **10:45** - 总结：SageMaker 用于构建模型，Bedrock 用于安全扩展，Strands 用于实现代理，应用层用于交付

### Slack 的 AI 演进时间线（11:00 - 15:30）
- **11:00** - 展示 Slack 从 2023 年 Q2 到 2025 年 Q3 的完整 AI 之旅
- **11:30** - 2023 年 Q2：使用 SageMaker 开始学习和实验，选择 SageMaker 是因为其 FedRAMP 合规性
- **12:00** - 2023 年 Q3：举办内部黑客马拉松，构建原型，包括 Huddle Summary 功能
- **12:30** - 2024 年 Q1：迁移到 Amazon Bedrock，因为 Bedrock 获得 FedRAMP 认证，提供最新 Anthropic 模型，基础设施更简单，成本降低 98%
- **13:00** - 2024 年 Q2：推出第一个机器人 Buddy Bot，用于文档辅助，开始使用知识库
- **13:30** - 2025 年 Q1：开发者要求代码辅助功能，开始试验 Cursor 和 Claude Code
- **14:00** - 2025 年 Q2：构建智能代理，通过 MCP 服务器访问数据，构建第一个 MCP 服务器
- **14:30** - 2025 年 Q3：引入 Strands 和升级机器人（Escalation Bot）
- **15:00** - 强调 Slack 避免了分析瘫痪，持续实验、交付和学习
- **15:15** - 从每分钟处理数十万个 token 扩展到数百万个 token，得益于 Anthropic 的百万 token 上下文窗口

### 为什么选择 Bedrock（15:30 - 17:30）
- **15:30** - 第一个原因：跨 AWS 的统一平台，一个地方构建、扩展和治理所有内容
- **16:00** - 第二个原因：内置安全性，包括防护栏、安全性和合规性
- **16:30** - 第三个原因：大规模可扩展性，Slack 运行多个 AI 用例，服务数千名员工
- **17:00** - Bedrock 让 Slack 专注于构建出色的开发者体验和用户体验，无需担心基础设施

### 开发者生产力影响（17:30 - 20:00）
- **17:30** - 现场互动：询问观众认为生产力提升了多少（20%？50%？）
- **18:00** - 承诺展示实际数据而非猜测
- **18:30** - 确认加速了开发者生产力，赋能团队更快创新，大幅减少原型开发时间

### 衡量 AI 影响的指标（20:00 - 24:00）
- **20:00** - Shivani 提问：有多少人有衡量 AI 对开发者生产力影响的指标
- **20:30** - 强调衡量 AI 影响是最难回答的问题之一
- **21:00** - 介绍两个基础指标：AI 采用率和对开发者体验指标（DORA 和 SPACE 指标）的影响
- **21:30** - 使用 OpenTelemetry 指标收集 AI 工具使用数据
- **22:00** - 从 GitHub 测量由 AI 共同编写的 PR 和提交
- **22:30** - 这些指标提供了开发者如何受 AI 影响的良好估计

### 实际开发者影响数据（24:00 - 27:30）
- **24:00** - 99% 的开发者正在使用某种形式的 AI 辅助
- **24:30** - 每周持续增长的采用率和每月持续使用
- **25:00** - 主要代码库的 PR 吞吐量每月持续增长 25%
- **25:30** - AI 机器人辅助：**每月处理超过 5000 个升级请求**
- **26:00** - 升级在 Slack 中发生，用户在升级频道提问并升级到相应团队
- **26:30** - AI 辅助帮助工程师缓解待命疲劳
- **27:00** - 最重要的定性指标：直接的开发者反馈确认这些工具确实有帮助
- **27:15** - 也看到了缺点：PR 审查时间增加，因为 AI 帮助编写更多代码，审查面积增加

### 技术演进和经验教训（27:30 - 32:00）
- **27:30** - 强调达到 99% 采用率和 25% PR 吞吐量提升的道路并非直线
- **28:00** - 三年前从纯实验开始
- **28:30** - 使用 SageMaker 和 Triton 构建初始能力，提供最大控制但带来巨大的基础设施维护成本
- **29:00** - 采用 AWS Bedrock 是突破点，这不仅是技术变革，也是理念转变
- **29:30** - Bedrock 简化了基础设施，处理所有 LLM 扩展和基础设施维护
- **30:00** - 立即解决了关键成功因素：安全性（所有内容保留在 AWS 账户内）
- **30:30** - 通过代理 API 提供 LLM 访问，让所有开发者都能试验 AI
- **31:00** - 另一个优势：可观测性，Bedrock 的内置 CloudWatch 日志、指标和警报提供了 LLM 使用洞察
- **31:30** - 面临的主要挑战：不同 LLM 和工具的实验疲劳

### 应对实验疲劳的策略（32:00 - 34:00）
- **32:00** - AI 领域变化太快，难以跟上
- **32:30** - 意识到不断推出新的竞争性内部功能只会给开发者带来困惑，并增加维护开销
- **33:00** - 解决方案：专注于高影响力技术栈 - Amazon Bedrock 和 Anthropic 模型及工具
- **33:30** - 通过将 Claude Code 和 Cursor 与 Bedrock 集成来推动采用
- **33:45** - Slack 是最早在 2025 年 Q2 推出 Claude Code 的公司之一
- **34:00** - 目标：创建无缝体验，最大化吞吐量，减少开发者的决策疲劳

### 智能代理框架介绍（34:00 - 38:00）
- **34:00** - Prashant 接手，讨论智能代理之旅的下一阶段
- **34:30** - 现场互动：有多少人在组织中探索智能代理？有多少人在生产环境中运行智能代理？
- **35:00** - 关键问题：为什么选择智能代理？不仅仅因为其他人都在做
- **35:30** - Slack 使用 LLM API 时，许多操作是临时工作流（例如将日志转储到 Claude Code 询问问题）
- **36:00** - 待命工程师收到大量请求，需要自动化运行手册
- **36:30** - 从临时工作流转向自动化工作流是采用智能代理的原因之一
- **37:00** - 第二个原因：仅使用 LLM 调用不进行复杂推理、规划或适应
- **37:30** - 第三个原因：Slack 在 AWS 服务上构建了许多工具和数据源，需要标准化访问
- **38:00** - 智能代理可以通过 MCP（模型上下文协议）动态使用这些工具和数据源

### Claude Code 和智能代理能力（38:00 - 41:00）
- **38:00** - Slack 已经大量使用 Claude Code
- **38:30** - Claude Code 在 2025 年下半年添加了许多智能代理能力
- **39:00** - 功能包括 Claude Code 子代理、规划能力，现在还有技能（Skills）
- **39:30** - Claude Code 构建了一个带 SDK 的代理，可以有效地用作各种任务的子代理
- **40:00** - 不是从头构建专门任务的代理（这很复杂且难以完善），而是开始使用 Claude Code 子代理
- **40:30** - 第二部分：构建自己的 MCP 服务器以访问各种工具和数据源
- **41:00** - 从 AWS 学习（AWS 为 EKS 构建了 MCP 服务器）

### 当前智能代理实施（41:00 - 43:30）
- **41:00** - 标准化访问所有工具和数据源，无需考虑使用哪个 API
- **41:30** - 探索各种智能代理框架进行集成，这就是 Amazon Strands 的用武之地
- **42:00** - 当前做法：不是构建超级代理，而是采用现有的 LLM 集成工作流并增强它们
- **42:30** - 使用智能代理工作流添加更多能力
- **43:00** - 在 DevOps 环境、事件管理等领域探索新用例
- **43:15** - 这些是小步骤，但正在取得进展并将内容投入生产

### 为什么超越 Claude Code（43:30 - 46:30）
- **43:30** - 关键问题：Claude Code 和子代理如此强大，为什么要寻找其他方案？
- **44:00** - 第一个原因：可能会变得昂贵且不太可预测
- **44:30** - 第二个原因：应该保持模型无关性 - 今天是 Anthropic，明天可能是其他模型
- **45:00** - 第三个原因：成本考虑 - 随着生产使用增加，成本将成为重要因素
- **45:30** - 可能希望为专门任务使用更便宜的 LLM
- **46:00** - 第四个原因：编排代理的概念 - 将编排器从 Claude Code 中抽象出来
- **46:30** - 使用 Claude Code 擅长的部分（子代理执行特定任务），但使用自己的编排器

### 创建模型无关的智能代理框架（46:30 - 48:30）
- **46:30** - 通过抽象编排器，可以今天指向 Claude Code 子代理，明天指向其他代理
- **47:00** - 关键是在框架内控制访问内容和访问时间
- **47:30** - 最终目标：创建一个模型无关的智能代理框架，为生产部署提供未来保障
- **48:00** - 这就是 Strands 的用武之地

### Amazon Strands 介绍（48:30 - 52:00）
- **48:30** - 在深入 Strands 之前，讨论为什么 Amazon 构建并开源 Strands
- **49:00** - 虽然智能代理的潜力令人兴奋，但构建智能代理比想象的更复杂
- **49:30** - 现场互动：有多少人认为构建智能代理比预期更复杂？
- **50:00** - 开发者面临的关键挑战：构建可靠智能代理的学习曲线陡峭
- **50:30** - 客户可以让简单用例工作，但随着复杂性增加和新功能每周发布，很难跟上
- **51:00** - 企业和生产就绪性：在 POC 和演示中工作良好，但投入生产需要满足更多标准
- **51:30** - 复杂的编排逻辑：单个代理或编排器调用一两个代理工作良好，但扩展到数千个代理变得复杂
- **52:00** - 其他挑战：缺乏可见性、缺乏控制、灵活性不足

### Strands 的核心特性（52:00 - 55:30）
- **52:00** - Strands 是一个开源框架，最初发布了 Python SDK
- **52:30** - 只需几行代码即可构建智能代理
- **53:00** - 简单易用，消除了复杂智能代理编排的需求
- **53:30** - 为构建者和开发者设计的代码优先解决方案
- **54:00** - 开发者可以定义提示、选择工具列表、选择 LLM，然后让它运行
- **54:30** - 通过开源，为开发者提供强大、灵活的工具，在快速发展的智能代理领域构建代理
- **55:00** - 关键特性：模型和部署选择 - 虽然默认 LLM 是 Bedrock，但可以选择任何第三方或自定义提供商
- **55:30** - 不限制 LLM 选择，也不限制部署位置

### Strands 的高级功能（55:30 - 58:00）
- **55:30** - 高度灵活，内置防护栏
- **56:00** - 连接到 AWS 的防护栏功能以及外部防护栏功能
- **56:30** - 内置原生可观测性和监控，可以流式传输 OpenTelemetry 指标
- **57:00** - 如果使用 Agent Core，会自动连接这些指标，轻松获得复杂智能代理流的可见性和追踪
- **57:30** - MCP 集成：模型上下文协议已成为连接数据源和工具的行业标准
- **58:00** - Strands 本身提供许多内置工具，可用于许多任务，还可以添加自定义工具

### Strands 集成和多代理模式（58:00 - 62:00）
- **58:00** - 与 Mem0、Ragas、Temporal 等的集成
- **58:30** - 提到 AWS 开源会议中有关于 Temporal 的专场
- **59:00** - 集成许多第三方服务以实现高度灵活性
- **59:30** - 介绍 Strands 中的多代理模式：Swarm、Graph、Workflow 和 Agent as Tools
- **60:00** - Swarm：多个代理之间的协作，具有通信模式、共享内存系统和协调机制
- **60:30** - Graph：代理是节点，通信方式是边，明确定义它们如何通信
- **61:00** - Workflow：结构化方式定义一个代理完成一个任务并传递给下一个代理
- **61:30** - Agent as Tools（最有趣）：Slack 正在使用的模式
- **62:00** - 使用编排代理处理用户交互并决定调用哪个专门工具，这些工具可以是其他代理

### Strands 智能代理循环（62:00 - 64:30）
- **62:00** - Strands 智能代理的核心是"智能代理循环"
- **62:30** - 接收提示和上下文以及可用工具的描述
- **63:00** - 模型推理任务并决定是直接响应还是规划一系列步骤
- **63:30** - 如果可以直接响应，不需要工具；否则，规划步骤、反思先前操作、选择工具
- **64:00** - 从工具获得响应后，决定任务是否完成，否则重复循环直到任务完成
- **64:30** - 这是 Strands 的基本性质

### Strands 代码示例（64:30 - 66:00）
- **64:30** - 展示创建 Strands 代理的简单示例
- **65:00** - 第一个示例：使用默认选择（Bedrock 和 Nova 模型）创建代理，只需几行代码
- **65:30** - 第二个示例：附加工具 - 有许多工具类别可用
- **66:00** - 可以将这些工具附加到代理...（演讲在此处被截断）

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━


注意：本总结基于提供的字幕文本，演讲似乎在代码示例部分被截断。完整会议可能包含更多关于 Strands 实现细节和 Slack 具体用例的内容。