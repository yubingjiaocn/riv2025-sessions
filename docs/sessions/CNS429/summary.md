# AWS re:Invent 2025 - Amazon EKS 超大规模架构深度解析

## 会议概述

本次会议深入探讨了 Amazon EKS（Elastic Kubernetes Service）如何支持超大规模的 AI/ML 工作负载。主讲人 Sheetal Jooshi（AWS EKS 首席解决方案架构师）和 Raghav（EKS 工程负责人）详细介绍了 EKS 的底层架构创新，特别是如何突破传统 Kubernetes 的规模限制。会议重点展示了 Amazon EKS Ultra Scale Clusters 的革命性突破——支持单集群 10 万个节点，可管理 80 万个 Nvidia GPU 或近 160 万个 AWS Trainium 芯片。

这一技术突破源于对 Kubernetes 核心组件 etcd 的彻底重构。团队将共识算法卸载到专用的多可用区事务日志系统，将数据存储从磁盘迁移到内存数据库，并对关键资源类型进行智能分区。这些架构改进使得 EKS 能够在保持毫秒级延迟的同时，处理每秒数千次的读写请求，支持数千万个 Kubernetes 对象。会议还介绍了数据平面的优化，包括容器镜像并行拉取、网络带宽优化和 Karpenter 自动扩展增强，这些创新共同为大规模 AI 训练和推理工作负载提供了坚实的基础设施支撑。

## 详细时间线

### 开场与背景介绍
[00:00 - 05:30] - 会议开场，介绍演讲嘉宾
- Sheetal Jooshi 介绍自己作为 EKS 首席 SA，在 AWS 工作近 5 年
- 介绍 Raghav（EKS 工程负责人）和 Noah（Anthropic 基础设施负责人）
- 展示 Kubernetes 采用率数据：93% 的公司正在生产环境中运行或评估 Kubernetes

[05:30 - 08:45] - Amazon EKS 发展历程回顾
- 2018 年 re:Invent 期间推出 EKS
- 逐步推出托管控制平面、计算管理、插件市场等功能
- 2022 年推出 IPv6 集群支持
- 2024 年推出 Auto Mode 和 Hybrid Nodes
- 特别强调 4 年前推出的 Karpenter 项目并捐赠给 CNCF

### AI/ML 工作负载特性分析
[08:45 - 12:20] - AI/ML 工作负载在 EKS 上的应用场景
- 自动驾驶车辆开发：处理大量传感器数据和感知模型
- 机器人开发：模拟真实物理场景的训练环境
- 生成式 AI：大语言模型训练和推理端点托管
- Agentic AI：支持数千个 AI 代理的大规模编排

[12:20 - 15:40] - AI/ML 工作负载的关键特征
- 依赖海量结构化和非结构化数据
- 计算密集型，需要高带宽低延迟网络互连
- 需要并行读写存储解决方案
- 简化的基础设施管理对高效扩展至关重要
- 依赖广泛的工具、插件和框架生态系统

[15:40 - 17:50] - Gartner 预测与 EKS 优势
- Gartner 预测：到 2028 年，90-95% 的新 AI 部署将使用 Kubernetes（当前仅 30%）
- EKS 选择理由：上游兼容性、工具生态系统、可定制性、AWS 服务集成、混合部署能力、高度可扩展性、成本优化

### EKS 控制平面架构深度解析
[17:50 - 22:30] - EKS 控制平面基础架构
- API Server 实例跨两个可用区部署
- etcd 数据存储跨三个可用区分布
- 控制平面运行在私有 VPC 中提供隔离
- 通过 Network Load Balancer 提供 API 端点访问（支持公有/私有/混合模式）

[22:30 - 26:15] - 确定性弹性机制
- 实现可预测的故障恢复结果
- 故障时的四个关键操作：暂停集群更新保留容量、将 API 流量重定向到健康可用区、重新定位 leader 选举控制器、转移 etcd leadership 维护数据库性能
- 基于静态稳定性原则设计
- 主动测试各种故障场景（数据包丢失、健康检查、依赖服务断开）

[26:15 - 29:40] - 控制平面扩展架构
- 监控多个信号：CPU、内存、节点数、etcd 数据库大小、API Server 性能指标
- 突破性改进：API Server 和 etcd 并行扩展、蓝绿部署优化、预加载文件和 S3 预取
- 智能且保守的扩展策略：渐进式使用更大实例、15 分钟冷却期、自动 QPS 和突发速率调整
- 扩展时间从 50 分钟缩短至 10 分钟，同时保持 99.95% 可用性承诺

### 数据平面与计算选项
[29:40 - 34:20] - EKS 数据平面灵活性
- 从自管理节点组到完全托管的 Auto Mode 节点池
- 支持 Karpenter 节点池进行下一代自动扩展
- Hybrid Nodes 支持本地和边缘基础设施
- Karpenter 特性：支持按需和 Spot 实例、GPU、容量预留、成本优化和工作负载重新平衡

[34:20 - 37:50] - EC2 实例类型支持
- 支持所有 EC2 实例类别：通用、可突增、存储优化、高 I/O、图形密集型
- 多种处理器选择：AWS Graviton、Intel、AMD
- 高内存实例和加速计算实例
- 实例存储选项：HDD 或 NVMe
- 裸金属选项和多种网络配置

[37:50 - 40:30] - GPU 和加速器支持
- 支持 Nvidia GPU 实例（G 系列、P5、最新的 P6.2）
- AWS 自研加速器：Trainium 和 Inferentia
- 多种购买选项：按需、Savings Plan、Spot 实例
- 重要里程碑：客户在 EKS 上使用数百万 GPU 实例，数量较去年翻倍

### EKS 创新功能详解
[40:30 - 45:20] - 计算优化功能
- **Soci OCI 并行拉取**：加速镜像加载和解包，使用高性能 HTTP range 方法
- **容量块预留支持**：支持已购买的 GPU 容量预留
- **加速 AMI**：集成优化的驱动程序和运行时组件，显著减少 GPU 和 AI 加速器的设置时间
- **Kubernetes DRA（动态资源分配）**：实现跨多个 AI 工作负载和 Pod 的细粒度 GPU 资源共享和分配
- **EC2 UltraCluster 集成**：轻松集成超大规模计算集群
- **S3 Mountpoint 驱动**：直接访问 S3 中的训练数据和模型工件，将其视为本地文件系统，显著减少数据加载时间

[45:20 - 48:10] - Day 2 运维功能
- **节点健康和自动修复**：持续监控节点健康状态，集成 EC2 健康检查和 Kubernetes 节点条件
- 检测到不健康节点时自动替换，优雅关闭和驱逐 Pod
- **Container Insights**：为 EKS AI/ML 工作负载提供全面的可观测性

[48:10 - 52:30] - AI/ML 架构模式
- 顶层：AI/ML 开源工具和集成（Jupyter Notebooks、工作流配置、作业调度、模型注册、内存优化、多模型管理、动态批处理）
- 平台层：Kubernetes 作为统一平台，支持数据管道、模型开发、训练和推理
- 基础设施层：与 AWS 服务深度集成
- 生产环境实践：支持多种模型框架、MLOps 工作流、使用 Kubeflow/Ray/MLflow 进行模型训练和验证、模型部署和服务（Nvidia GPU 和 AWS Neuron）

### AI/ML 工作负载挑战与解决方案
[52:30 - 56:40] - AI/ML 工作负载的关键挑战
- 需要大规模协调计算：数千个加速实例作为单一协调系统工作
- 低延迟和高带宽需求
- 工具和框架无法跨集群良好工作
- 跨集群管理框架和映射困难
- 管理多个集群增加运维开销
- 客户需求：减少运维开销、简化集群管理、共享治理、提高成本效率、增加资源利用率、共享容量池

### Amazon EKS Ultra Scale Clusters 重磅发布
[56:40 - 59:20] - Ultra Scale Clusters 核心能力
- **突破性创新**：单集群支持最多 10 万个节点
- 可管理 80 万个 Nvidia GPU
- 相当于近 160 万个 AWS Trainium 芯片
- Raghav 登场，介绍其团队负责运营数千万个大规模集群

### etcd 架构革命性重构（Raghav 主讲）
[59:20 - 1:04:50] - 传统 etcd 架构回顾
- etcd 是 Kubernetes 集群的核心分布式键值存储
- 默认架构：三节点 etcd 集群，使用 Raft 共识算法
- 关键组件：gRPC/HTTP 客户端通信、MVCC 层处理并发读写、BoltDB 数据库引擎、Write-Ahead Log 保证持久性
- 8 年来架构极其可靠，运行数千万集群
- 问题：etcd 从未设计为支持单集群 10 万节点

[1:04:50 - 1:10:30] - etcd 三大核心架构改进
1. 共识卸载：将 Raft 共识算法卸载到 AWS 专用的多可用区事务日志系统
   - 移除 etcd 最大瓶颈之一
   - 使用专门构建的高吞吐量分布式共识系统
   - 支持水平扩展，不再受限于 3/5/7 节点的核心要求

2. 内存数据库：从静态 EBS 卷迁移到内存数据库（使用 tmpfs）
   - 利用已将持久性移至专用系统的优势
   - 解锁数量级更高的读写吞吐量
   - 增加 etcd 数据库总大小限制

3. 智能分区：识别高流量键并分区到独立的 etcd 键值存储
   - 识别出 4-5 个占据大部分流量的键：nodes、pods、leases、events
   - 每个键类型使用专用的 etcd 存储
   - 大幅提升总读写吞吐量

[1:10:30 - 1:13:20] - 新架构设计
- 共识接口连接到多可用区事务日志
- MVCC 层写入内存数据库
- 分区键空间支持水平扩展
- 关键优势：保持与 Kubernetes 期望相同的 API 语义，无需修改 Kubernetes 代码

### Ultra Scale 性能测试结果
[1:13:20 - 1:18:40] - 工作负载规模测试
- **测试场景 1**：在所有 10 万个节点上使用 StatefulSet 提交大规模预训练作业，验证长时间训练运行的可持续性
- **测试场景 2**：混合模式工作负载 - 多个并行微调作业（每个占用 1 万节点）+ 使用 LeaderWorkerSet 在 Llama 3.2 模型上提供实时推理
- 混合工作负载特点：突发性强，对控制平面造成极大压力
- 关键要点：所有工作负载在单个 Ultra Scale 集群中运行，无需跨多集群协调

[1:18:40 - 1:22:10] - 对象规模统计
- 从 0 扩展到 10 万节点再缩减的过程中管理的对象数量：
  - 约 800 万个 Pod
  - 10 万个 Node 对象
  - 600 万个 Lease 对象（用于 leader 选举和协调）
  - 数千万个 Event 对象（表示集群活动）
- 数据库大小：Ultra Scale 集群支持最多 20GB（标准 EKS 的 2.5 倍）
- 挑战：在如此大的工作集下保持毫秒级读写延迟

[1:22:10 - 1:28:30] - 吞吐量性能
- **读吞吐量**：峰值约 7500 次读请求/秒
- 随着节点和 Pod 增加，吞吐量平滑扩展
- 重要性：大量控制器和 Operator 监听集群变化以做出决策（Pod 放置、健康监控、工作负载协调）
- 优化：使用上游改进，从 API Server 缓存提供一致性读请求，避免访问 etcd
- **写吞吐量**：峰值 8000-9000 次写请求/秒
- 写操作更昂贵（需要共识和持久化）
- 主要由下一代数据存储和日志后端驱动
- 在百万对象、10 万节点规模下，始终有 Pod 创建、配置更新、扩展操作

[1:28:30 - 1:33:20] - 延迟性能
- **读/写/删除请求**：P99 延迟在 100 毫秒到 1 秒之间
- 通过精心调优实现：请求超时、重试策略、工作并行度、限流规则
- **List 请求**：P99 延迟 5-20 秒（远低于上游 30 秒 SLO）
- List 请求特殊性：单个请求可返回数千甚至数百万个对象
- 优化：将 List 请求中的对象编码从批处理改为增量编码，显著减少内存消耗，支持更多并行请求

### 数据平面和网络优化
[1:33:20 - 1:39:50] - AI/ML 实例特性优化
- **实例特性**：网络带宽高达 100 Gbps、支持极高 IOPS 和吞吐量的 EBS 卷
- **容器镜像优化**：AI/ML 工作负载容器镜像通常超过 5GB
  - 使用 AWS Soci Snapshot 并行化容器下载和解包操作
  - 显著影响工作负载就绪时间（首次训练或推理请求的启动时间）

- **网络优化**：AI/ML 工作负载启动时从 S3 下载大量数据
  - CNI 改进：允许单个 Pod 连接到实例上的所有网卡
  - 提供对整个网络带宽的访问
  - 组合优化：Pod 从调度到运行并获取所有数据的时间缩短 3 倍

- **节点扩展优化**：Karpenter 已成为客户响应待处理工作负载扩展计算的标准
  - 优化：在节点启动期间预分配 IP 前缀，而非启动后通过 CNI 被动分配
  - 进一步缩短节点启动到就绪的时间
  - 在 0 到 10 万节点扩展时，每一秒都很重要
  - 所有这些功能自 2024 年中期起已在 EKS 中可用

[1:39:50 - 结束] - 总结与展望
- 完整展示了 Ultra Scale 的实现旅程
- 从数据存储的彻底重构到支持大规模的各层优化
- 这些创新代表了 Kubernetes 在存储和检索数据方面的完整架构重新思考
- 为大规模 AI/ML 工作负载提供了坚实的基础设施支撑